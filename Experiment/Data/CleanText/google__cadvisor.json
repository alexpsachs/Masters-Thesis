{
    "vmarmol": "Thanks for the fix! Must have missed it during the migration :)\n. Also, if you don't mind signing the CLA before we merge the PR. Instructions are in CONTRIBUTING.md\n. Thanks for the cleanup! :)\n. I'm not sure we want to always register Docker under /docker. cAdvisor currently lives under the assumption that all containers form a hierarchy with / being the whole machine. In the case that lmctfy is unavailable we'd like Docker's libcontainer to take over stats in /. Currently libcontainer does not understand containers outside of the Docker containers, but arguably it should one day. I believe @rjnagal has mentioned a desire to do this for libcontainer. In this case, if we don't register any manager for /, we won't see any containers in the machine.\nThe argument can also be made to register a no-op manager in this case, which only mentions the existence of /docker and nothing else.\nOr did I misunderstand your PR? :)\n. Ah, @monnand beat me to it :)\n. @kyurtsever that sounds like a great approach. Root is significantly different and we don't handle it well today. The driver could probably use lmctfy for stats (although as we saw in #6 some setups don't support lmctfy well)\n. Thanks!\n. @crosbymichael that's interesting. It seems to have detected a to level Docker container. That or some corruption on the name string. What does the rest of the log say?\n. Closing, we now use the raw driver.\n. Finished the review in person, merging. Thanks @monnand!\n. Thanks for filling the issue! Yeah we need to add support for names, it\nlooks hard to read otherwise. Will track with this issue.\nOn Jun 11, 2014 11:27 PM, \"defender\" notifications@github.com wrote:\n\nHi\nContainer name is not presented, in main page only GUID\nroot docker\n3ba08c450497df5facf35139a02cd1fd3038a3d424d6972531098ed531c9f644\nThanks.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/9.\n. @monnand you can use the --name flag to give the container a unique and memorable name. I think that is what this issue is refering to\n. Assigning to @monnand since he wanted to work on this.\n. It is, but I'm guessing you're running on a systemd system in which it is\nnot working? That is #143 unfortunately.\n\nWas this really fixed? I still only see the ID instead of container name\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/9#issuecomment-56129208.\n. Thanks for the cleanups @proppy !\n. We definitely need to tighter up the density a bit to get more content in the same space, and the graphs could do with more features to make them easier to use. I kinda feel like they should pause when I hover over them or click them and I should be able to scroll back and forth in their history as well.\n. The granularity we currently envision is at the container level (Docker even recommends a 1:1 mapping of processes and containers). We can consider process-level granularity, but I think that will be quite difficult since reporting is not available like it is for containers.\nYou can achieve something very similar though. If you take a look at lmctfy, you can use it to create a subcontainer per process. Then cAdvisor would automatically pick it up and track it.\nI just filed #15 which should help in some regard.\n. Closing in favor of #15\n. We had someone run into this issue yesterday too and I think we had a few issues:\n- Kernel version: Docker officially supports 3.8+ and lmctfy 3.3+ (although it can probably work with much older ones too) and I think this is running 2.6.* no?\n- We don't currently support the LXC execution driver for Docker (sorry!), we should add an issue for that, but we'll probably go for raw support over that (thus support all users).\n- Cgroups were not mounted. Check /proc/mounts and see whether they are.\n. You do seem to have cgroups mounted:\ncgroup /cgroup/cpuset cgroup rw,relatime,cpuset 0 0\ncgroup /cgroup/cpu cgroup rw,relatime,cpu 0 0\ncgroup /cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\ncgroup /cgroup/memory cgroup rw,relatime,memory 0 0\ncgroup /cgroup/devices cgroup rw,relatime,devices 0 0\ncgroup /cgroup/freezer cgroup rw,relatime,freezer 0 0\ncgroup /cgroup/net_cls cgroup rw,relatime,net_cls 0 0\ncgroup /cgroup/blkio cgroup rw,relatime,blkio 0 0\nThere is one issue in lmctfy which we can fix to allow this to work, but you'd only get machine-wide stats. For per-container stats we're gonna need the generic cgroup driver unfortunately. I opened #14 to track it.\n. +1 and @crosbymichael I'm surprised Docker runs in 2.6.32...\n. I'm gonna close this in favor of #14, feel free to re-open if you run into any other issues.\n. The version that supports this is the canary: google/cadvisor:canary not :latest. We should be making it latest later today.\nSorry, I should only update this after we mark :latest :)\n. @tpires just upgraded to :latest. Let us know how it goes!\n. @tpires this is a compatability problem with old versions of Docker. If you upgrade to 1.1.1 it should work. We're working on a fix for older version of Docker today.\n. Specifically: https://github.com/google/cadvisor/issues/127\n. Fixed and pushed to latest\n. Great to hear :)\n. Yep, assigned to you.\n. Last I heard we hadn't started on this. We're planning on re-using libcontainer's cgroups package. ping @rjnagal if we has any updates.\n. Closing. This is in HEAD and in google/cadvisor:canary. Should upgrade to latest today.\n. That SGTM\n. We now only have one, closing.\n. Thanks for the reorg!\n. Sort the list please :)\n. Closing this since we now have the raw driver. Feel free to file another if there is something missing.\n. Assigning to myself. @crosbymichael this is what has been giving you trouble at root\n. We currently report shares, should be good enough for now.\n. Yeah we just broke it :( working on a fix stay tuned...\n. LGTM\n. Thanks for the quick change @monnand!\n. Taking a look\nOn Thu, Jun 12, 2014 at 5:22 PM, monnand notifications@github.com wrote:\n\nping @vmarmol https://github.com/vmarmol. This PR will not solve all\nproblem. But it could be use as a starting point.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/28#issuecomment-45963693.\n. Ping @rjnagal @monnand \n. Thanks for the fixes :)\n. Ping @monnand \n. LGTM\n. @caizixian Vish is indeed @vishh :) bytes/sec in/out sounds completely reasonable.\n. @caizixian today we keep all the data in memory so detailed stats we only keep for a few mins (this is flag configurable). The UI does not expose specific time periods. We expose all the raw data and histograms we've built on the data for other tools to use. No image generator today, but I don't think it is particularly hard to add. Feel free to file separate issues for those if they're important to you :)\n\nCollecting more data and keeping it in disk is on the roadmap, although I'm not sure if at that point we'd like a cluster-wide entity to do the work instead so you get aggregated stats. Any preferences? There is an argument for having it be a simple on-node tool for ad-hoc deployments and letting other tools do the aggregation.\n. Not today, but I think we should integrate with them.\n. Yes, we've actually mentioned influxdb specifically. I think it makes sense\nto have these output plugins.\nOn Fri, Jun 13, 2014 at 10:43 AM, Jonathan Chauncey \nnotifications@github.com wrote:\n\nWould you guys be up to having cadvisor output to different metrics\ncollectors? Like ganglia and influxdb?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/34#issuecomment-46039565.\n. @vishh merges this some time back :)\n. This specifically is to get a jpg/png of a specific graph? or more along the lines of \"get a link to this instance in time\"?\n. Yes, thank you @caizixian. I think we're gonna switch the graphs we use to ones that you'll be able to get an image from.\n. No, I think we were looking to replace the UI layer with one that could do this natively.\n. @caizixian they're dynamic in that they're constantly being updated. @thaJeztah I agree, I wouldn't wanna lose that :) I think we'd keep that but give users the ability to take a snapshot of the current view. I haven't tried getting the image from the canvas object, will try it and see!\n\n+1 to client side (on the UI).\n. @caizixian no alought we arguably should...any recommendations? and anything in particular you want documented?\n. Depends on #37 \n. Fixed by #40 \n. Nice find! The CI is failing because of a Docker dependency. I'm working on that in #17 \n. Definitely! Let us know how we can help.\n. @monnand started work to refactor the existing code to make it easier to add these drivers. So hopefully by next week you'll be able to add many different drivers for storage backends.\n. @erikh that SGTM. Only question I have there is if it can be run within the same process as the rest of cAdvisor\n. I just wanted to keep us from depending on another process running, but if there is nothing we can do then we can go that route.\n. :+1: to configurable. For now we only have one option :D\nWith @monnand's StorageDriver PRs we should be able to take any of these easily\n. The initial version of this is done with @monnand's #74. Feel free to open other issues for details that may be missing :)\n. Ping @monnand this fixes #37 \n. Thanks @monnand!\n. @monnand and please take a look @proppy :)\n. Pushed the new version PTAL @proppy \n. Got it working under google/golang-runtime. It is much simpler now :) PTAL @proppy \n. Done and done. Thanks for the review @proppy!\n. :+1: \n. @rjnagal is working on this, assigning to him.\n. @rjnagal this is done no?\n. Ping @rjnagal \n. LGTM\n. Ping @monnand and @rjnagal \n. Ping @rjnagal. This is a good starting point, I'd like to cleanup the code a bit after this PR.\n. Ping @rjnagal @monnand \n. This may be related to the leak in #26 since we leaked both a thread and memory. I will get a new release of the google/cadvisor image and let you know to try that.\nTomorrow, if we restart the container and the usage is low, I think it verifies that it is #26.\nWe do a bit of work per second housekeeping the containers, but a lot of that is a sleep in libcontainer which we're working on removing. In the end, we really only should need a few ms of CPU time per container for housekeeping. The most expensive part may be talking to Docker.\n. @erikh restarted cAdvisor and the usage went down to expected levels. This points to it likely being a dup of #26. Will build a new cAdvisor and provide it for verification.\n. Pushed google/cadvisor:0.1.0 with the fixes. Let me know if that helps.\nAlso, any ideas how to re-tag latest? Do I just commit an image from a container made with google/cadvisor:0.1.0 and push that?\n. Thanks for the report @erikh! I'll tag the new image latest (thanks @jchauncey and @crosbymichael!)\n. Some comments on this approach :)\nI'm not sure we want to add this is a wrapper around a ContainerHandler, this should be something the Manager understands directly. In essence, each ContainerHandler is how we interact with a container while stats are a Manager-level concept. Your patch today still has us keeping the stats in the manager and then potentially interacting with a database for those stats.\nI imagine what we'd want is for the manager to query the containers for data and store it wherever it'd like. In manager/container.go we see that we have two interactions with stats:\n- Add stat from a call to handler.GetStats() (this is in updateStats()) which stores it in an in-memory linked list today\n- Retrieve stats (done in GetInfo())\nI think that is where we should plugin the storage driver which has those two operations. There we can have an in-memor driver, an influxdb driver, and an other driver people want. The only part that I'm unsure of is that we probably don't want all stats when we retrieved them. We probably want to be able to get a subset. Maybe the interface is to provide all stats within some time range or some number of stats. Time-range seems to be the most common for this type of data. With that model, today's UI would just query the last 60s worth of data for its graphs.\nWDYT?\n. Should I review this PR or wait for the other one?\n. Thanks @jchauncey! I don't think we want to directly export metrics per-say. cAdvisor gathers the data for processing that it needs to do so exposing it seems useful all around, but we also need to get the information back. I'll see how we can take advantage of go-metrics though.\n. Please rebase :)\n. LGTM, the only nit I might have is AddStats() instead of WriteStats(), but up to you. Feel free to merge when Travis returns green.\nThanks Nan!\n. LGTM!\n. LGTM, thanks Rohit!\n. LGTM\n. LGTM, merging\n. LGTM, merging. Thanks @monnand!\n. Some nits, LGTM otherwise. I'm curious if the latency of queries went up\n. :+1: \n. Does this affect /, /docker, /docker/, or all?\n. That may be tricky given freezer support and systemd not using freezer.\n. We will need to change both :)\n. / is handled by lmctfy, everything else is handled by libcontainer. So the\nlibcontainer change should be enough to fix this.\nWDYT of ignoring non-Docker and non-root containers in systemd systems? We\nshould be able to degrade gracefully if that's the case anyways.\nOn Tue, Jun 17, 2014 at 4:07 PM, Rohit Jnagal notifications@github.com\nwrote:\n\n/ is fine. Everything else is broken.\nlibcontainer seems like a simpler change, but I am going to try and change\nboth lmctfy and libcontainer.\nOne problem with systemd systems is that there are many containers (some\nof them just within a named systemd hierarchy). We might want to filter\nsome out.\nI'd be fine if anyone wants to take over either of the changes :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/59#issuecomment-46377458.\n. This is done thanks to the raw driver. We've tested in CoreOS and RHEL. If you find any issues on other systems please file issues :)\n. A simple alias can fix that con :) the godep route SGTM, thanks @monnand!\n\nOn Thu, Jun 19, 2014 at 3:44 PM, monnand notifications@github.com wrote:\n\nI just checked godep https://github.com/tools/godep. It is a good tool\nto do vendoring. According to a recent comment on HN\nhttps://news.ycombinator.com/item?id=7914078, it seems most people\nchoose this tool.\nThe basic idea of godep is to put all dependencies into a directory and\norganize that directory as a GOPATH. When you compile your code, you need\nto wrap any go command with godep, e.g. go build becomes godep go build.\nThen godep will automatically change the GOPATH for go command.\nPros:\n- Easy to manage, you only need to run godep save ./... and godep will\n  automatically find all dependencies and put their code into the directory.\n  Updating dependencies is also easy.\n- No need to change import path. Because godep changed GOPATH when it\n  runs the go command, so the import paths do not need to be changed.\n- It could automatically find the dependencies in parent directories.\n  This means we could run godep go test under storage/memory directory\n  even if all dependencies are under a directory in the project's root\n  directory. (This part makes it better than the scripts used in kubernetes.\n  Because I could test my package individually.)\n- It's a go program which means it is statically linked and can be\n  copied anywhere.\nCons:\n- Every go command has to be wrapped with godep.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/60#issuecomment-46626540.\n. This was done by @kelseyhightower \n. LGTM\n. LGTM\n. LGTM, thanks for putting up with the nits :)\n. LGTM, feel free to merge after adding the comment\n. Grammar nits, LGTM otherwise so feel free to merge when those are fixed :)\n. Don't merge that yet...this change hasn't landed in libcontainer HEAD yet.\n. This is ready to review now :)\n. Thanks @monnand!\n. Fixes #6\n. I think that's because of #67 \n. Should be able to rebase\n. LGTM, merging. Thanks @monnand \n. LGTM\n. Mostly nits, otherwise LGTM. Thanks @monnand, I like how its looking :)\n. Thanks @monnand! Merging once CI passes.\n. This has been fixed at head. Thanks for the feedback!\n. LGTM, merging. Thanks @monnand!\n. LGTM, merging.\n. nit: can you squash your commits :)\n. Ping @monnand?\n. Thanks @monnand! Merging when CI finishes.\n. LGTM, once you fix the two comments feel free to merge. Thanks @monnand!\n. CI passed, merging\n. Ping @monnand \n. We can\n\nOn Tue, Jul 8, 2014 at 5:22 PM, monnand notifications@github.com wrote:\n\nDo we want to wait #80 https://github.com/google/cadvisor/pull/80 ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/81#issuecomment-48416179.\n. Ping @monnand @rjnagal\n. thanks for the uber fast merge @monnand :) I'll tag 0.1.2 and we'll try to do weekly or bi-weekly releases.\n. LGTM\n. Good catch @monnand!\n. Ping @monnand \n. Ping @monnand. I'll build the release with this change.\n. LGTM\n. Ping @monnand. Inspired by @tianon we reduced the image from ~330MB to ~22MB :D\n. Fixes made (except influxdb one since their build is broken and I'd like to make that a separate PR) PTAL and thanks for the review @tianon!\n. Fixes made (except influxdb one since their build is broken and I'd like to make that a separate PR) PTAL and thanks for the review @tianon!\n. Ick, thanks for following up on that @monnand \n. Ick, thanks for following up on that @monnand \n. Yeah we're at a very unhappy place right now because of it :( we filed a bug with influxdb. Hopefully that'll get resolved soon. Prime example of why we should version our dependencies.\n\nThe latest canary was build yesterday from head before we had any issues. In case you wanna try that: google/cadvisor:canary\n. Yeah we're at a very unhappy place right now because of it :( we filed a bug with influxdb. Hopefully that'll get resolved soon. Prime example of why we should version our dependencies.\nThe latest canary was build yesterday from head before we had any issues. In case you wanna try that: google/cadvisor:canary\n. Ping @monnand. I have a couple of followup PRs to this:\n- create a no-op factory to handle the cases when lmctfy is not available\n- lmctfy should determine whether it actually can support that container\n. Ping @monnand. I have a couple of followup PRs to this:\n- create a no-op factory to handle the cases when lmctfy is not available\n- lmctfy should determine whether it actually can support that container\n. LGTM but sad :(\nMaybe we should start versioning our deps...\n. LGTM but sad :(\nMaybe we should start versioning our deps...\n. You're going to hate me, but I thought we were renaming MachineSpec -> MachineInfo to better lineup with ContainerInfo :)\n. You're going to hate me, but I thought we were renaming MachineSpec -> MachineInfo to better lineup with ContainerInfo :)\n. What about calling GetMachineInfo() GetRootInfo() since we are returning a\nContainerInfo\nOn Thu, Jul 17, 2014 at 9:58 AM, monnand notifications@github.com wrote:\n\n@vmarmol https://github.com/vmarmol Ah. That's fine. It's just some\nscripting work.\nThe problem in GoogleCloudPlatform/kubernetes#491\nhttps://github.com/GoogleCloudPlatform/kubernetes/pull/491 is that we\nhave three functions in kubelet:\n- GetContainerInfo(): This will return a container's info, same as in\n  cAdvisor. So the return type of this method is info.ContainerInfo.\n- GetMachineInfo(): This will return the root container's info. So it\n  returns an info.ContainerInfo as well.\n- GetMachineSpec(): This method returns the machine specification\n  (number of cores, memory capacity). But the return type is\n  info.MachineInfo, which may be confused with GetMachineInfo().\nAny suggestion to solve this problem? Or you think it's fine?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/92#issuecomment-49334153.\n. What about calling GetMachineInfo() GetRootInfo() since we are returning a\nContainerInfo\n\nOn Thu, Jul 17, 2014 at 9:58 AM, monnand notifications@github.com wrote:\n\n@vmarmol https://github.com/vmarmol Ah. That's fine. It's just some\nscripting work.\nThe problem in GoogleCloudPlatform/kubernetes#491\nhttps://github.com/GoogleCloudPlatform/kubernetes/pull/491 is that we\nhave three functions in kubelet:\n- GetContainerInfo(): This will return a container's info, same as in\n  cAdvisor. So the return type of this method is info.ContainerInfo.\n- GetMachineInfo(): This will return the root container's info. So it\n  returns an info.ContainerInfo as well.\n- GetMachineSpec(): This method returns the machine specification\n  (number of cores, memory capacity). But the return type is\n  info.MachineInfo, which may be confused with GetMachineInfo().\nAny suggestion to solve this problem? Or you think it's fine?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/92#issuecomment-49334153.\n. Ping @monnand \n. Ping @monnand \n. Ping @monnand \n. Ping @monnand \n. Fixed, merging when CI passes. Thanks @monnand!\n. Fixed, merging when CI passes. Thanks @monnand!\n. Awesome! LGTM\n. Awesome! LGTM\n. Rebase? :) also, Travis is failing it seems. Not sure if that is you or InfluxDB.\n. Rebase? :) also, Travis is failing it seems. Not sure if that is you or InfluxDB.\n. Only a couple of comments, otherwise LGTM :) thanks for putting up with the changes!\n. Only a couple of comments, otherwise LGTM :) thanks for putting up with the changes!\n. LGTM, can we squash the commits and then we can merge :) thanks for making the changes!\n. LGTM, can we squash the commits and then we can merge :) thanks for making the changes!\n. LGTM\n. LGTM\n. Ping @rjnagal @monnand \n. Ping @rjnagal @monnand \n. This doesn't work without https://github.com/docker/libcontainer/pull/114 btw.\n. This doesn't work without https://github.com/docker/libcontainer/pull/114 btw.\n. Ping @vishh @monnand \n. Ping @vishh @monnand \n. Merging since two PRs depend on this and @vishh LGTM'd.\n. Merging since two PRs depend on this and @vishh LGTM'd.\n. Ping @monnand @rjnagal \n. Ping @monnand @rjnagal \n. Seems to be something about InfluxDB not coming up. Will ping @monnand when he comes in.\n. Seems to be something about InfluxDB not coming up. Will ping @monnand when he comes in.\n. Ping @vishh @monnand \n. Ping @vishh @monnand \n. Ping @rjnagal @monnand what do you guys think?\n. Ping @rjnagal @monnand what do you guys think?\n. Ping @vishh \n. Ping @vishh \n. Ping @vishh \n. Ping @vishh \n. Ping @monnand @vishh this is needed to get Docker support working well inside a Docker container.\n. Ping @monnand @vishh this is needed to get Docker support working well inside a Docker container.\n. LGTM\n. LGTM\n. LGTM.\n\nHaha nice, I was thinking we'd need this for something like InfluxDB :)\n. LGTM.\nHaha nice, I was thinking we'd need this for something like InfluxDB :)\n. LGTM and merging. We should consider doing the defaults in the manager's GetInfo() and have the UI inherit that.\n. LGTM and merging. We should consider doing the defaults in the manager's GetInfo() and have the UI inherit that.\n. Thanks @monnand!\n. Thanks @monnand!\n. Ping @vishh @monnand.\n@vishh this is what was breaking us in CoreOS. It now works :)\n. LGTM\n. LGTM, thanks @rjnagal!\n. Nice! Thanks @rjnagal :)\n. LGTM, merging.\n. Ping @vishh @monnand to lower the amount of information we request in the UI.\n. LGTM\n. LGTM\n. LGTM, thanks @monnand!\n. Merging, I'll send some changes in another PR. Thanks @monnand!\n. Ping @monnand @vishh @rjnagal this is the issue we were running into yesterday.\n. LGTM, thanks @monnand!\n. @stigkj thanks for the fix! Even small changes like this require signing the CLA if you don't mind. Details are in CONTRIBUTING.md.\n. Asked around and legal prefers the directory be called third_party. I couldn't find a way to get godep to use that naming instead so unfortunately we may not be able to use it :( unless you have any tricks up your sleeve.\n. @kelseyhightower I think that'd help if they can make it configurable (I guess since they have their config file it should be possible).\nFor the symlink we'd need to have the source of truth in third_party and then everyone to symlink to Godeps. I don't love that people have to do the extra work...I'm okay holding out to see what the godep folks say.\n. Awesome! @kelseyhightower Can you rebase and we'll try to merge? :)\n. LGTM and merging. Thanks @kelseyhightower! We've wanted to get to this for some time :)\n. LGTM, thanks @rjnagal!\n. I think this is pending on: https://github.com/docker/libcontainer/pull/130\n. LGTM\n. LGTM\n. Thanks for the test @monnand! LGTM.\n. Thanks for the fixes @danmcp! Even for small changes likes this we ask that you sign the CLA, details are in CONTRIBUTING.md\n. @danmcp ah sorry, first time I check for a company-wide one :)\nmerging!\n. Mostly LGTM, just those comments above. Can we also add a brief comment on each state to say what it's doing and what it expects?\n. LGTM\n. Thanks @monnand! Merging\n. @rjnagal we have code that should handle that, but it doesn't look like it is working...\n. @bshi do you see this when any service restarts or only when it restarts services that talk to Docker?\n. Ah, that's a good point @rjnagal. I'm not sure we should ignore the delta, maybe we should try to recognize this and re-set our stats. Otherwise the graphs and data will look pretty weird (and a lot of code won't expect the sudden drop). We can check the creation times of the container to verify this I think.\n. SGTM, letme see about sending a PR to do this.\n. Ping @rjnagal \n. Fixed and added a unit test. PTAL.\n. Ping @monnand \n. Ping @monnand and @vishh \n. Huge simplification thanks to @vishh. Ping @monnand and @vishh \n. :( I'll close till then. Feel free to re-open.\n. Ping @monnand \n. It does not, but it can get labels from Docker when those are available. We\ndo talk to the Docker API to get extra info about the containers.\nOn Wed, Feb 25, 2015 at 10:21 AM, Sebastiaan van Stijn \nnotifications@github.com wrote:\n\nDoes cAdvisor use the docker daemon to get the list of containers? If it\ndoes, the \"labels\" proposal (docker/docker#9882\nhttps://github.com/docker/docker/pull/9882) could offer additional ways\nto filter containers; e.g. all containers having a label\ncom.google.cadvisor.group=some-group\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/152#issuecomment-76022804.\n. @rjnagal we merged --dockeronly should we close this?\n. LGTM\n. A few questions to help with debugging:\n- What version of cAdvisor are you running btw?\n- How is it running, just a raw run or in a Docker container?\n- What does the cAdvisor log say?\n. You are at least getting machine-wide data, so it seems to be isolated to Docker containers.\n. @zyndiecate we pushed 0.3.0 wanna give that a shot? It adds support for a lot of systems so it may work for you now.\n. Just filed https://github.com/docker/libcontainer/issues/198 to track that.\n. Obsolete, closing.\n. Closing in favor of #159 since I think all instances of this will be caught by that.\n. From initial profiling we find that we spend:\n- ~55% of time gathering stats\n- ~15% of time in GC\n- ~15% of time in global housekeeping\n\nWithin stats we spend ~50% of our time reading /proc/mounts in libcontainer. We shouldn't need to do this more than once.\nTo reduce GC we plan to reduce allocations by using a ring buffer for stats (our heaviest user of allocations).\nStill need to look more into reducing global housekeeping.\nWe think we may be able to reduce CPU usage by ~33% with the two above.\n. Thanks @rjnagal!\n. LGTM\n. Ping @vishh from our discussion today.\n. Potentially if cAdvisor \"breaks\" and decides to use lots of CPU. It's use is proportional to the number of containers on a system and depends on how it is set up. There is room to grow on making this limit better, but setting it to NumCPU should not be a big issue, it will be no different than an application with the whole machine available.\nA better limit may be max(1, NumCPU() / X), but I don't know what that X is yet :) besides, we have containers to limit the effect on other things on the machine! :D\nGoogle Group: google-containers@googlegroups.com (as well as #google-containers on IRC on freenode).\n. LGTM\n. LGTM\n. LGTM\n. Haha, as I was doing it I knew you'd be thinking of doing it too :)\nAdded the fix, lets see if Travis is happy now.\n. @jchauncey which are those? Sorry, not very familiar with the ones you may be referring to so pointers are welcomed :)\n. Thanks for the numbers and suggestion! We've been seeing similar performance, but didn't have the numbers to back up what \"reasonable\" performance was. We definitely need to improve the performance of our use of InfluxDB and this seems like the best first step.\nWe already fill 60s cache for all containers so it should be feasible to use that to transmit the data periodically (and for all containers at once as you mentioned).\n. It may be nice to let the driver do the batching since it can decide how much of it makes sense for it.\n. Yeah I think that is unexpected, @vishh wanna take a look?\n. No, they should be command line args. The readme has the exact ones you need to set.\n. Adapting your command line, you'd wanna run:\ndocker run --name=cadvisor --volume=/var/run:/var/run:rw --volume=/sys/fs/cgroup/:/sys/fs/cgroup:ro --volume=/var/lib/docker/:/var/lib/docker:ro --publish=8100:8080 --detach=true google/cadvisor:latest --storage_driver=influxdb --log_dir=/\nYou will need to add the flags (to the end of the command line) @rjnagal specified above to specify your InfluxDB instance (you have to run your own).\nIf you're just interested in historical data, you don't need the InfluxDB backend. cAdvisor by default uses an in-memory one which keeps 60s of data (you can configure to increase that). InfluxDB is nice if you want to keep longer amount of stats. Maybe can you give a bit more detail about your usecase? We may be better able to suggest alternatives.\n. @defender how did that go?\n. Closing, let us know if you run into anymore issues.\n. Can you rebase and re-push to force Travis?\n. Thanks for this much needed change @vishh! Just that one suggestion, otherwise LGTM\n. LGTM thanks @vishh!\n. LGTM\n. Weird rebase?\n. Ah, better now. LGTM, waiting on Travis.\n. Thanks @vishh! Merging :)\n. Can you run without --detach=true and see what the log says? Should let us know why it quit :)\n. Couple of questions:\n- Are you using the LXC driver? (run: sudo docker inspect)\n- Are cgroups mounted inside the container? (run: sudo docker run busybox ls / and sudo docker run busybox cat /proc/mounts)\n. Ah, you have one of those systems, also run the docker command with --volume=/cgroup:/cgroup:ro and it should work :) let us know how that goes. (CentOS and RHEL 6 mount the cgroups under /cgroup and our examples assume /sys/fs/cgroup which everyone else uses).\n. Sorry for the delay, mostly nits on my end :) otherwise looks good.\n. LGTM, thanks @rjnagal! Merging.\n. LGTM, thanks @rjnagal!\n. Thanks for the patch @caglar10ur! Only a couple of nits.\n. LGTM. Looks awesome, thanks @caglar10ur!\n. Running again, lets see how it does.\n. Merging this and will comment out the other tests. @monnand how did we fix it last time?\n. Merging to fix build.\n. LGTM, can you rebase to re-run the build now that it is fixed?\n. CI pases, merging. Thanks @rjnagal and @machworks! ;)\n. Can you run it non-detached and logging to stderr:\n$ docker run \\\n--volume=/var/run:/var/run:rw \\\n--volume=/sys:/sys:ro \\\n--volume=/var/lib/docker/:/var/lib/docker:ro \\\n--volume=/cgroup:/cgroup \\\n--publish=8080:8080 \\\n--name=cadvisor \\\ngoogle/cadvisor:latest --logtostderr\nThen we can get the logs and find out what is going on.\n. What is the output of docker info? and can you ls /var/lib/docker/execdriver/native and the dirs in there?\n. Oh also, was there anything interesting in the startup recovery?\n. Hmmm that is very odd. Any ideas where Docker is storing its state then? I didn't know that was configurable in any way. Maybe it is somewhere else in var?\nThe issue is that cAdvisor is not able to get the container info from the Docker state directory.\n@crosbymichael any ideas where Docker could be storing its state in RHEL6?\n. Also weird that my RHEL system does have them under /var/lib/docker\n. @AndyBerman we probably can't do the Docker-specific stuff if we can't find where the Docker state is stored. I'm not familiar with a way of finding that out from Docker today. I guess you could symlink /var/lib/docker -> /docker but we shouldn't need to ask users to do that.\n. Seems like by default it is there but can be placed elsewhere do +1 to flag.\nOn Aug 29, 2014 1:55 PM, \"Vish Kannan\" notifications@github.com wrote:\n\nIs docker using '/var/lib/docker'/ by default on all distros? If that is\nthe case, then a cAdvisor flag sounds like a good idea.\nOn Fri Aug 29 2014 at 1:03:18 PM Andy Berman notifications@github.com\nwrote:\n\nSo as a workaround, I went to /var/lib and created a symbolic link:\nln -s /docker/lib docker\nThat worked, and now cAdvisor works :)\nThanks for your help. Hopefully you can add a switch or something else\nin\nthe future.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/199#issuecomment-53923621.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/199#issuecomment-53928801.\n. Will add the flag, assigning to myself :)\n. Towards what @monnand mentioned. What about always running the memory driver by default (we do anyways with the caching) and then having the storage drivers be optional and take the batched AddStats()? Then we'd have the baseline we need and expect and export the stats when requested.\n. Awesome!\n\nOn Fri, Aug 29, 2014 at 3:29 PM, Vish Kannan notifications@github.com\nwrote:\n\n@vmarmol: I proposed the same exact solution and @monnard is planning to\nsend a PR for that soon. That is the reason for dropping this PR.\nOn Fri, Aug 29, 2014 at 11:27 AM, Victor Marmol notifications@github.com\nwrote:\n\nTowards what @monnand https://github.com/monnand mentioned. What\nabout\nalways running the memory driver by default (we do anyways with the\ncaching) and then having the storage drivers be optional and take the\nbatched AddStats()? Then we'd have the baseline we need and expect and\nexport the stats when requested.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/200#issuecomment-53913201.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/200#issuecomment-53913371.\n. Closing this since I think it is obsolete now. Re-open if that is not the case.\n. Can we give InfluxDB sub-second \"time\" values? May make sense to do it there if it is supported.\n. WDYT of using that instead of a separate field?\n. I think we'll ve okay. The precision should be good enough (better than the\nsecond one we do today).\n\nOn Fri, Aug 29, 2014 at 3:36 PM, monnand notifications@github.com wrote:\n\n@vmarmol https://github.com/vmarmol, @vishh https://github.com/vishh,\nI just tested how influxdb represents time. The time field returned from\nthe client library is in type of float64, not an integer. This may cause\nsome precision problem.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/201#issuecomment-53914209.\n. We've talked about something similar with the \"wrappers\" which would be\nco-scheduled containers. I guess we'd need to see if heapster does the\nstats exporting natively or it also uses co-scheduled containers :) We\ncould use heapster as that wrapper, or have the indivitual wrapper for each\nexporting entity.\n\nOn Fri, Aug 29, 2014 at 4:11 PM, Vish Kannan notifications@github.com\nwrote:\n\nAnother idea I have is that of running heapster\nhttps://github.com/GoogleCloudPlatform/heapster as a buddy container\nalong side cAdvisor whenever someone wants to push data to external DBs.\nHeapster can be very easily configured to talk to just 'locahost:8080' and\nwe need not handle batching there.\nThe issue with that approach will be that all statistical calculations\nthat requires processing a lot of historical data will fail. But I would\nargue that these kind of calculations must be performed on a DB.\nWDYT?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/202#issuecomment-53918252.\n. +1 to having the storage drivers be write only and cAdvisor runs off of the in-memory cache. The plan SGTM (no injecting back into the cache though :) ).\n. I think this refactoring is also superseded by others we did? Closing, re-open if that is not the case.\n. I'm torn between keeping some of these things native (making it very easy for a user to setup and deploy) and making it composable (easier to write your own). Maybe we take a hybird approach? Where we have a set of backends we support natively and have some examples of others we support with co-scheduled containers.\n\nI'm not sure if we want to co-schedule with heapster though, won't it itself forward the data to another helper container? We may just wanna talk to that helper directly (so maybe have heapster and cAdvisor both use the same type of helpers)\n. Ping @vishh\n. @thaJeztah it is based on Kubernetes' healthz\n@vishh thanks! merning\n. 10 Docker containers? Or containers of any kind?\nThe CPU usage is a bit higher than expected with so few containers. We are\nactively looking at reducing CPU usage, but even for today that seems a bit\nhigh. In general, CPU usage scales linearly with the number of containers.\nOn Aug 31, 2014 12:42 PM, \"Frank Rosquin\" notifications@github.com wrote:\n\nUbuntu 14.04 (3.13.0-35-generic amd64)\nand about 10 running containers (all single process)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/206#issuecomment-53999017.\n. We have a couple of open issues tracking different things to improve this.\nWe'll leave this one open to track the issue as a whole.\n\nI have a couple of questions that may help us set goals, if you have some\ntime for them :)\n- What is an acceptable amount of CPU you expect cAdvisor to take?\n- What granularity of data do you want? E.g.: 5min, 1 minute, 30s, 1s\n10 docker containers, and nothing else.\ncAdvisor never really drops below 10%, and is often above 15%\ncontainers\n$ docker ps |wc -l\n10\ntop\nPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND\n14891 root 20 0 269692 13868 4608 S 12.2 0.3 77:57.22 cadvisor\n1698 root 20 0 1450204 28192 8768 S 2.3 0.6 18:25.90 influxdb\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/206#issuecomment-54001807.\n. We recently added two flags to make this configurable and it does have a\nsignificant effect on resource utilization as expected. It not yet in the\nDocker image but they are available from head:\n--housekeeping_interval = how often to get stats\n--global_housekeeping_interval = how often to check for new containers\nThe default for both is 1s.\nOn Sep 4, 2014 2:20 PM, \"Jim Alateras\" notifications@github.com wrote:\n\nAre you able to reduce the frequency at which observations are collected?\nDoes that impact the CPU usage of cdvisor?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/206#issuecomment-54546945.\n. From my tests, --global_housekeeping_interval=5s affects ~0.3% in the\nworst case while --housekeeping_interval=5s can cut the usage in by more\nthan half.\n\nOn Thu, Sep 4, 2014 at 4:01 PM, Frank Rosquin notifications@github.com\nwrote:\n\nAha, very nice! I wonder how much impact each has.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/206#issuecomment-54557668.\n. In CoreOS part of the issue is the sheer number of containers. We're\nworking on allowing you to specify that you only want some of those\nmonitored.\n\nWith the new configurable stats gathering interval, the usage can be\nsignificantly reduced if you want to try again. We're still working on\nreducing usage, but these are some things you can do right now.\nI can say this is one of the reasons I don't use Google CAdvisor.\nI deployed it on a empty CoreOS kvm guest; there was no docker containers\nrunning no activity what so ever and it was using 75% cpu usage.\nNote, Acceptable cpu usage is generally under 1%; profiling has a cost but\nwhen it's cost is too high it's not used.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/206#issuecomment-54767424.\n. Closing this as fixed as of 0.4.0. If you see any issues again, let us know :)\n. Nit: lowercase cluster management\nWe should probably only say that cAdvisor supports cluster level monitoring through heapster and leave the details of what clusters are supported to the heapster page. That way we don't have to update the info in two places when it changes\n. LGTM, merging :)\n. Ping @vishh \n. Ping @rjnagal \n. @cnf any ideas what Go is trying to use? We can always include that in the image.\n. Ping @rjnagal @vishh :)\n. Yep :) thanks @vishh ! Merging\n. Outside of the time question, LGTM\n. LGTM, thanks @monnand! merging.\n. Ping @vishh @rjnagal @monnand \n. This is why Docker containers were not being registered in Systemd systems. It is still broken now because of a bug in libcontainer. Looking into that.\nIt is actually broken even worse now because we'll register the Docker driver, but can't get stats from it. Maybe hold off merging until I figure out the libcontainer issue :)\n. @rjnagal @vishh PTAL. I made us always use the FS stats API even in systemd systems which makes this work. I already sent a PR to libcontainer, I'll update when that goes through.\nFixes #143 \nOnly remaining issue is that the \"system.slice\" does not show the docker container name. One at a time :) all the exported stats and names should work, so it is only a UI issue at this point.\n. Ping @vishh \n. Ping @vishh @rjnagal \n. Fixed (was missing a file), should pass now.\n. Just checked, we actually don't do this.\n. Will work on this today\n. @kateknister\nWe have a ring-buffer impl today at storage/memory/stats_buffer.go\n. Fixed by @kateknister \n. This is obsolete.\n. Ping @vishh @rjnagal \n. Ping @rjnagal @vishh \n. Assigning to @kateknister\n. ping @vishh @rjnagal \n. Fixed the breakage and rebased :)\n. It should be the same issue, it affects any systemd system.\nOn Sep 18, 2014 11:18 PM, \"abilash222\" notifications@github.com wrote:\n\ni am also acing the same issue. it works well for me in ubuntu but fails\nin centos 6.5 . any idea?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/236#issuecomment-56139119.\n. Are we still seeing this btw?\n. Nice :)\n. LGTM, thanks @vishh!\n. Ping @vishh \n. Don't merge until after #241 is merged.\n. Ping @vishh @rjnagal, ready now :)\n. That is definitely unexpected, especially if you're running 0.4.0. At first glance, kipmi0 doesn't seem connected, but maybe that's wrong. How long was this cAdvisor running? How many containers are in the system?\n\nAre you running 0.4.0? If so, we can grab a profiling trace and see what cAdvisor is up to.\n. No worries! Just to verify, you're running 0.4.0? It is the :latest Docker image as well. If so, then it should be pretty easy to profile cAdvisor when it gets into such a state:\n$ go tool pprof http://<cAdvisor host>:<port>/debug/pprof/profile\n... this grabs a 30s profile and drops you into pprof ...\n(pprof) web\nThat should grab a 30s profile and open up a web browser to an .svg file that shows where CPU is going. That trace will be super useful to figure out what's going wrong.\nI'd also be interested in getting the cAdvisor log if possible. (docker log <container> if you're running it as a Docker container).\nThanks!\n. Ping @vishh @rjnagal \n. Thanks @rjnagal! Merging\n. LGTM, thanks for the fixes!\n. Should we do the mock FS? Would it be a lot of work?\n. If you have the cycles, I'd rather we start doing it with mocks. I've started trying to make container and manager more testable so we'd be getting on the right foot. We can do that for now and do the proper change in another PR if you'd like though.\n. Sorry for being a pain :D\n. Ping, how is this going? :)\n. Awesome, thanks @plietar! LGTM\nDo you mind signing the CLA before we merge. Even for small changes we request it, details are in the contributing page:\nhttps://github.com/google/cadvisor/blob/master/CONTRIBUTING.md\n. Thanks @plietar! merging.\n. Ping @rjnagal @vishh \nThis should help with the issues we were seeing where the new cAdvisor triggered the kernel bug that left us stuck on pre 3.12 kernels.\n. Thanks @rjnagal! Changes made, PTAL\n. FWIW, I'm a big fan of all the docs you've added :)\n. I'm okay accepting this compromise :) the camel case const I personally dislike, but the style is clear on it so I won't argue. On the if statement, I've seen both as accepted styles.\n. Thanks for putting up with the nits @satnam6502! Still a few of the if's not rolled back, but probably not a huge deal.\n. Ping @vishh \n. I believe this was fixed and merged already so closing. Re-open if that is not the case.\n. LGTM, thanks for putting up with the nits @vishh!\n. This has already been merged.\n. Also fixed by @dipankar \n. LGTM\n. Ping @vishh :)\n. LGTM, thanks @vishh!\n. Thanks for the patch @ashahab-altiscale! One thing is that this seems to share most of it's code with the raw driver. Can we either move the shared code to a common package or use the raw driver's version?\n. @vishh this should help when analyzing the usage for the next release.\n. Fixed the breakage in the tests. PTAL\n. Thanks @vishh! Changes made, will merge when travis passes.\n. Sorry for all the nits, but overall I really like the patch! Thanks @ashahab-altiscale!\n. Outside of those last few comments it LGTM!\n. Sorry, hotel decided not to have internet yesterday...LGTM. Waiting on LGTM from @vishh and we'll merge.\nThanks for putting up with all the changes @ashahab-altiscale!\n. @andrewwebber are you using godep for the build?\n. Yes, we moved to use godep for building and testing so you'll need to:\ngodep go build github.com/google/cadvisor\n. Hmmm that may be true as inotify is Linux only. cAdvisor depends on that functionality so it probably is failing to build for the test.\nWe really need to beef up the documentation around building cAdvisor. I'll try to get a page together today.\n. @jwalczyk do you see this consistently? As in, every 1s or 60s, or every now and then? Do you see the data for the time period on or before 1107 09:09:48.189645? It may just have dropped that request.\n. We've been considering that and it does feel simpler (we'd also not need to use godep for build or test).\n. Thanks for the doc fix @highlyunavailable!\n. Older versions of Docker don't support mounting /. You can remove that line and it'll work as expected. There is one feature that won't work, but that's not in the Docker image yet anyways.\n. Closing, but feel free to re-open if that doesn't fix it :)\n. Overall happy with the approach. One question: can't we get the list of mounts from LXC somehow? Maybe even Docker directly?\n. Thanks @ashahab-altiscale! LGTM, merging.\n. Thanks! LGTM, merging.\n. Thanks for the docs @kevin1024! Just some nits, otherwise looks good.\n. Thanks @kevin1024! LGTM, Merging.\n. Ping @rjnagal @vishh \n. @vishh this one is for you :)\n. The break here should be pretty minimal. The difference will only be noticed by users who treated these as pointers. I think the fix is small enough that we should be okay.\nThe cost to maintain the old one is way to high IMO and not something we're setup for. WDYT @rjnagal? We haven't been doing a good job of versioning our types and have been doing okay. For 1.0 we probably need to have some way to version those, but pre that I think we are okay.\n. I think that's way overkill for us :) Their API is much broader, changes more often, and the changes are wider. We're okay not doing this pre 1.0 IMO. Our API also keeps the wire format the same. This only breaks users who use our code directly (without something like godep).\n. Merge? :)\nOn Wed, Nov 5, 2014 at 11:03 AM, Vish Kannan notifications@github.com\nwrote:\n\nSince the wire format remains the same, I agree that this change might not\nbe that bad.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/277#issuecomment-61862354.\n. Can we also run gofmt on the code? :)\n. @ashahab-altiscale this gathers stats for the whole system right? This is not per-container correct? If so, we should only do this at the root container.\n\nFeel free to use gofmt on the whole file. We have hooks on save that run gofmt automatically on the whole file so I don't think the changes will be that extensive.\n. I like this patch more since it doesn't make us rely on an external tool. Unless you feel that it give us more data, I think this patch is preferable.\n. Ok, we should definitely expose them in root. I'm guessing in your setup some devices are given exclusively to a specific container? In that case it makes sense to plumb that data in. What I'm worried about is having two containers reporting the stats for a device since they share it. But now that I look closely at it, I think your PR is doing the former :) correct me if that's not the case.\nMy main concern with iostat is when we run cAdvisor in a Docker container. Today we don't have anything in the container. With this change we'll need to include iostat or somehow bind mount it from outside. Not undo able, but more work. If iostat sounds like an overall win, I'm fine going that route and handling the case when it is not available. Do you prefer that PR?\n. Yeah gofmt is awesome. Change looks good, only minor comments.\n. Thanks @ashahab-altiscale! LGTM\n. Actually, looks like one of the manager tests is failing. Can you take a look?\n. On easy way that may work for now is to mock the call to os.Open()\n```\nvar osOpen = os.Open\nfunc foo() {\n  file, err := osOpen(...)\n}\n```\nIn the test:\n```\nfunc mockOpen(...) {\n  return ...\n}\nosOpen = mockOpen()\n// Actual test code\n``\n. Thanks again! Merging.\n. Ping @rjnagal @vishh \n. We'll wait until the libcontainer PR is merged before merging this one btw.\n. @rjnagal and I are two of them :) @mrunalp and @crosbymichael are the other two. I'm taking a look now.\n. LGTM, will wait for travis and then merge.\n. Ping @rjnagal @vishh \n. Thanks @vishh! PTAL\n. Ping @vishh \n. @tpires are you setting a limit on the Docker container? By default Docker containers don't have a limit.\n. @tpires I just verified this on a machine :-\\ we'll target for0.5.1which is for EOW. Thanks for reporting!\n. Ping @rjnagal @vishh \n. Ping @rjnagal @vishh \n. @rjnagal PTAL, added a test for it. We should cleanup and add more integration tests this Friday :)\n. Can you run it with--detach=falseand and add the flag--logtostderr` to see the cAdvisor log and find out where it is hitting issues. Complete command:\nsudo docker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --volume=/cgroup:/cgroup \\\n  --publish=8080:8080 \\\n  --detach=false \\\n  --name=cadvisor \\\n  google/cadvisor:latest --logtostderr\n. Hmm it looks like it can't find your cgroup mounts. What is the output of the following command:\n$ grep cgroup /proc/mounts\n. @ashahab-altiscale I remember we ran into this with you. It seems the issue is Docker with the LXC exec driver. I think LXC doesn't propagate the mounts unless you explicitly add them as @ashahab-altiscale mentions. So try:\nsudo docker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --volume=/cgroup/cpu:/cgroup/cpu \\\n  --volume=/cgroup/cpuacct:/cgroup/cpuacct \\\n  --volume=/cgroup/cpuset:/cgroup/cpuset \\\n  --volume=/cgroup/memory:/cgroup/memory \\\n  --volume=/cgroup/blkio:/cgroup/blkio \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  google/cadvisor:latest\nThis assumes you want cpu, cpuacct, cpuset, memory, and blkio.\n. @vishh can you take a look? I know you made some changes around disks for 0.4.1\n. Ping @rjnagal @vishh \n. Closing this issue, feel free to file another issue if you run into trouble.\n. Thanks for the report @leandrok! in 0.5.0 we introduced the first part of a new Docker API endpoint which will show a Docker-centric view of the world in cAdvisor if you request it. I almost have a PR ready that introduces the second part: an \":8080/docker\" endpoint which shows data as you'd expect. I think that will solve this problem and a few more.\nHope to get that out this week!\n. @leandrok you can see this in HEAD. We will cut 0.6.0 to include this on Friday.\n. Pring @vishh \n. @andrewwebber hopefully this helps future devs. Let us know if anything is still missing that would have helped you today.\n. I'd like to follow up this PR with two more today:\n- Simplifying \"namespace\" management in cAdvisor. Today it is a bit of a mess. Where namespace is /docker -> Docker containers\n- Making / redirect to /docker if the machine has the Docker driver registered.\n. Ping @rjnagal @vishh \n. Ping @vishh is this good? :)\n. Ping @vishh \n. This depends on the /docker/ UI PR so probably worth looking after that one :)\n. Rebased and re-pushed. PTAL @rjnagal @vishh \n. Thanks for the quick review @vishh! All changes made, PTAL.\n. Changes made, PTAL\n. Ping @vishh \n. Thanks @vishh! Changes made, PTAL.\n. Thanks @ibuildthecloud! This looks good. Sorry for the trouble, but can you please sign the CLA before the merge?\nMore details at:\nhttps://github.com/google/cadvisor/blob/master/CONTRIBUTING.md\n. Ping? :)\n. Thanks @ibuildthecloud! LGTM\n. Just two nits, outside of that LGTM\n. Thanks again :) LGTM\n. Yes, unfortunately today we don't do a good job of surfacing Docker containers in systemd. We are also inconsistent in how those are surfaced in systemd and non-systemd systems. It has become difficult to maintain that abstraction so we've opted to instead expose Docker containers as people would expect. #296 should fix this issue and #294. This is targeted for 0.6.0 which will go out next week.\n. Closing this as 0.6.0 is out with more visible view of Docker containers.\n. Also fixed by @dipankar \n. Feel free to run it without mounting that. You will just be missing some fs info. We should add that to the running guide.\n. We're interested in container OOMs and system-wide OOMs. There are two sources of this information:\n- /var/log/messages where the kernel OOM killer writes logs to. Although, this may be hard to parse since the output is per-process.\n- Registering for OOM and OOM kill cgroup notifications (do the system-wide ones work?).\nWe should probably run some experiments with container and system OOMs and see what we get in both of these.\n. @kateknister this is a good next place to go. It will touch a lot of the cAdvisor codebase and it's something we need for Kubernetes.\n. @vishh we need to somehow reconcile the two event streams though since we don't get what container OOM'd from the kernel logs.\n. Not yet :) we're not exposing the events right now.\nOn Fri, Feb 6, 2015 at 12:36 PM, Dawn Chen notifications@github.com wrote:\n\n@@kateknister https://github.com/kateknister should we close this one?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/305#issuecomment-73307811.\n. This is merged and released.\n. @danielkraaij what request were you making when this happened? There was an error with \"Docker containers\" that was fixed by 0.6.2 which I just pushed. Can you try with that and let us know how it went?\n. @danielkraaij does cAdvisor not run at all due to the log space issue? Or can we get the logs?\n. To get the logs to stdout you should be able to give cAdvisor the flag --logtostderr. Docker logs should work as expected then.\n\nThe default place for logs is /tmp (which I think is mounted on /run? which is full). If you try removing some stuff from /run this error may go away.\n. Are you still seeing the issue in 0.7.1? Also, can you provide the output of /validate on the cAdvisor? That was recently added to help debug issues like this.\n. Nothing jumps out from both of those, but yesterday we fixed an issue with a similar error. This fix went into 0.8.0 which we are currently testing. Can you both run google/cadvisor:0.8.0 instead of :latest and give the cAdvisor logs?\nThe fix for that other issue may not be the fix here, but 0.7.1 failed to log certain type of errors. I think one of those errors is what is causing our issues.\nThanks for all your patience in finding the problem here! We always appreciate bug reports :)\n. Do you have the cAdvisor log by any chance?\nOn Mon, Jan 26, 2015 at 4:46 AM, Daniel Kraaij notifications@github.com\nwrote:\n\nTried the newer version, unfortunately still the same problem.\nAnd no error found in the Docker driver setup section\ncAdvisor version: 0.8.0\nOS version: Buildroot 2014.02\nKernel version: [Supported and recommended]\n    Kernel version is 3.13.0-44-generic. Versions >= 2.6 are supported. 3.0+ are recommended.\nCgroup setup: [Supported and recommended]\n    Available cgroups: map[blkio:1 hugetlb:1 cpuacct:1 memory:1 devices:1 freezer:1 cpuset:1 cpu:1 perf_event:1]\n    Following cgroups are required: [cpu cpuacct]\n    Following other cgroups are recommended: [memory blkio cpuset devices freezer]\nCgroup mount setup: [Supported, but not recommended]\n    Cgroups are mounted at /rootfs/sys/fs/cgroup.\n    Cgroup mount directories: blkio cpu cpuacct cpuset devices freezer hugetlb memory perf_event systemd\n    Any cgroup mount point that is detectible and accessible is supported. /sys/fs/cgroup is recommended as a standard location.\n    Cgroup mounts:\n    cgroup /rootfs/sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /rootfs/sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /rootfs/sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /rootfs/sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /rootfs/sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /rootfs/sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /rootfs/sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/rootfs/sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/rootfs/sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/rootfs/sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/rootfs/sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/rootfs/sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/rootfs/sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/rootfs/sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/rootfs/sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/rootfs/sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\nDocker version: [Supported and recommended]\n    Docker version is 1.3.1. Versions >= 1.0 are supported. 1.2+ are recommended.\nDocker driver setup: [Supported and recommended]\n    Docker exec driver is native-0.2. Storage driver is aufs.\n    Cgroups are being created through cgroup filesystem.\n    Docker container state directory is at \"/var/lib/docker/execdriver/native\" and is accessible.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/306#issuecomment-71455468.\n. Sorry for the delay, but I think I found what the issue is. I built a custom google/cadvisor:canary with some extra logging. Can you run that and provide the log's output?\n. No worries and thanks for the verification! We've fixed a lot of misc errors in 0.9.0 so no surprise it works better now.\n\nClosing, and feel free to open another issue if you run into any trouble.\n. Ping @rjnagal @vishh.\nI'm curious what you both think of this change. I think this is what people want 90%+ of the time. And those that don't know what they want.\n. This is not the /containers/docker page. This is the /docker endpoint. This shows only the Docker containers on the machine. We've seen a lot of people get confused with where Docker containers are, how we handle them, and why they vary in systemd systems. This is a way to curb that. The root is still available at /containers.\nI do miss the omission of the root page. We could add a link to it from here, but I think that will get confusing if the user wants to go back to the Docker containers (unless we make it clear somehow that you're viewing non-Docker containers, but that gets tricky)....Maybe we add a machine usage summary in the /docker page?\nOverall, I do think this is better than what we do today because so much of our usage is Docker centric.\n. We spoke offline and the idea of still showing /containers as the root, but having a prominent \"Docker containers\" link seems to be winning. The only concern is user confusion between the /docker subcontainer and the \"Docker containers\" link.\nWhat do you all think?\n. Thanks @thaJeztah @rjnagal and @vishh for all the feedback :)\nChanged to a link. Let me know what you think.\n. After doing this, I realize that a better approach would be to merge both pages. Wherein  there is a \"Docker Containers\" section above \"Subcontainers\". That requires a bunch of changes. So lets start with this PR and go from there :)\n. Technically they're still subcontainers since we don't list their children :)\n. That's an interesting and valid point. I'm torn since 'all containers' or\n'containers' seem misleading. What did you think the first time y\nOn Nov 13, 2014 11:25 AM, \"Sebastiaan van Stijn\" notifications@github.com\nwrote:\n\nTechnically they're still subcontainers since we don't list their children\n:)\nThink that the terminology is a problem here. Depending on what you are\nused to (Docker, LMCTFY), there is or isn't such concept (subcontainers) ..\nor am I wrong here? (Only worked with Docker)\nAnyway, looking forward to see what you come up with, thanks in advance!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/307#issuecomment-62950333.\n. The first time you saw the subcontainer terminology in cAdvisor**\nOn Nov 13, 2014 11:28 AM, \"Victor Marmol\" vmarmol@google.com wrote:\nThat's an interesting and valid point. I'm torn since 'all containers' or\n'containers' seem misleading. What did you think the first time y\nOn Nov 13, 2014 11:25 AM, \"Sebastiaan van Stijn\" notifications@github.com\nwrote:\n\nTechnically they're still subcontainers since we don't list their\nchildren :)\nThink that the terminology is a problem here. Depending on what you are\nused to (Docker, LMCTFY), there is or isn't such concept (subcontainers) ..\nor am I wrong here? (Only worked with Docker)\nAnyway, looking forward to see what you come up with, thanks in advance!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/307#issuecomment-62950333.\n. We do handle both today (and LXC and more probably). We used to use lmctfy\nbut we then implemented a \"raw\" driver which was able to get stats for it\ntoo.\nOn Nov 13, 2014 11:32 AM, \"Sebastiaan van Stijn\" notifications@github.com\nwrote:\n\nFirst time I saw it, I had the advantage that I just read a proposal on\nDocker for LMCTFY and I read up what it was :) but first impression was \"I\nsaw that nice screenshot in the README, so just click around a bit and see\nwhat it gives me\"\nWondering; is a setup possible that is using multiple techniques (ie\nLMCTFY and Docker on a single instance?)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/307#issuecomment-62951674.\n. @mindscratch I would like to personally commend you for this awesome PR! This was indeed something we've wanted to do but never got to actually doing this. We looked into packaging files into Go binaries and the like. But this works great!\n\nOutside of that small nit, this LGTM.\n. A little bit of both if it is not too much trouble :) lets do the versioning for the external dependencies here since we do so today. We can do the versioning for the internal ones in another PR since we don't do that today.\n. Thanks again @mindscratch! Merging.\n. @yesnault I'm guessing the command doesn't work with any image today? As in, swap google/cadvisor --logtostderr for ubuntu\n. Ping @vishh @rjnagal \n. Wait for the merge :) I have a couple of fixes.\n. This is now ready for merging :)\n. Ping @rjnagal @vishh small fix for 0.6.0\n. Merging since travis now passes. Another fix on the way!\n. Ping @rjnagal @vishh this is the last bug I found in 0.6.0.\n. Just uploaded binaries for 0.6.0 and 0.6.1. Will start doing so for all future releases. Thanks!\n. LGTM. Thanks @vishh!\n. 0.6.1 SGTM :)\nOn Nov 18, 2014 11:24 AM, \"Vish Kannan\" notifications@github.com wrote:\n\nMerged #314 https://github.com/google/cadvisor/pull/314.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/314#event-195073641.\n. Ping @rjnagal @vishh this will probably make us do a 0.6.2 cut :(\n\nThe good news is: integration tests coming too!\n. Thanks @rjnagal! Merging, will wait a couple of days for the new release.\n. Ping @rjnagal @vishh sorry, the review is a bit on the huge side but the tests themselves look pretty easy to write! The goal is to be able to run this against a remote cAdvisor instance and eventually to do it from Jenkins.\nAfter we merge this I will try and see how well cAdvisor runs on Travis. Maybe we can run the integration tests on there. Right now Travis skips them.\n. Sorry, it depends on the other PR :) let me merge and rebase.\n. Build is green, PTAL\n. @thockin the framework part of this PR was what we talked about. Probably not particularly useful to you yet, but you can see what the tests using this look like. Some upcoming tests will need to mkdir some cgroup directories which will necessitate the ssh part.\n. Ping @vishh @rjnagal \n. Ping @vishh @rjnagal \n. WDYT @rjnagal @vishh?\n. WDYT @rjnagal?\n. We noticed this bug yesterday too sadly. The fix is in HEAD and we'll be building a 0.6.2 tomorrow which has the fix. Sorry for the churn! Should be working as expected very soon.\n. Ping @vishh @rjnagal\n. Ping @vishh @rjnagal \n. Sorta. The removal is inherently racy between when we detect it, when users make requests for info, and when we housekeep. We've seen the races lose and we printed out an error and added empty stats. When this happens we actually fail the AllDockerContainers requests. This PR will instead not print out the error and not insert empty stats.\n. Ping @rjnagal @vishh \n. Ping @rjnagal @vishh \n. Thanks for the small fix @Emsu! LGTM\n. We've also been meaning to export a /debugz or alike endpoint that has information like this (What we currently have in the VersionInfo struct here)\n. Implemented by @mindscratch so closing.\n. Thanks for the change @mindscratch! LGTM.\n. We don't have any recipe today. It should be easy to make a simple container that gathers from cAdvisor and pushes to Rieman though.\n. What is in /var/lib/docker/execdriver/native? Have you customized your Docker location by any chance?\n. Ah, it seems that your state should be in '/data/docker/execdriver/native' can you ls that to verify please?\nIf that is the case, we'll need to run with a custom --docker_root and include that in the mounts as well:\nsudo docker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/data/docker:/data/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  google/cadvisor:latest --logtostderr --docker_root=/data/docker\nWe should probably try to do a better job of detecting that. If this solves your issue we'll repurpose this issue to better detect custom Docker roots.\n. Excellent, I'll repurposed the issue then :)\nWe need to handle better when users specify a custom Docker root. Today we don't handle it at all. Possible ways to detect it:\n- Check Docker config /etc/default/docker for -g\n- Root Dir of the storage driver. Although this varies per storage driver.\nIf there is a more stable interface from Docker, we should do that instead.\nWe should also add the current flag setup to the running instructions.\n. What is in your /etc/default/docker file?\n. Can you provide the logs of cAdvisor please? From your setup, I think we may be failing connecting to the Docker daemon.\n. @josselin-c what is the output of ls -l /var/lib/docker/execdriver/native (you may need to be root for that).\nAlso, try this with cAdvisor 0.7.0 (docker run google/cadvisor:0.7.0). We've fixed a bunch of issues trying to connect to Docker with 0.7.0.\n. @josselin-c great!\n. Does the directory have space on it?\nCan you also try running with --logtostderr:\ndocker run \\\n--volume=/:/rootfs:ro \\\n--volume=/var/run:/var/run:rw \\\n--volume=/sys:/sys:ro \\\n--volume=/var/lib/docker/:/var/lib/docker:ro \\\n--publish=9090:8080 \\\n--detach=true \\\n--name=cadvisor \\\ngoogle/cadvisor:latest --logtostderr\n. What is the full cAdvisor log?\nOn Dec 3, 2014 6:54 PM, \"Scott\" notifications@github.com wrote:\n\nI am getting the same thing, just wanted to try this out.\nsudo docker run \\\n--volume=/:/rootfs:ro \\\n--volume=/var/run:/var/run:rw \\\n--volume=/sys:/sys:ro \\\n--volume=/sys/fs/cgroup/:/cgroup \\\n--volume=/var/lib/docker/:/var/lib/docker:ro \\\n--publish=8080:8080 \\\n--detach=true \\\n--name=cadvisor \\\ngoogle/cadvisor:latest --logtostderr\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/333#issuecomment-65530379.\n. Hmmm haven't seen that one before. Can you provide the output of ls -l\n/cgroup/cpu?\nOn Dec 3, 2014 9:56 PM, \"Scott\" notifications@github.com wrote:\nMine looks similiar I noticed at the start it said permission denied on\nthe\ndocker.sock\nOn Wed, Dec 3, 2014, 10:10 PM liuyunsh notifications@github.com wrote:\n\n{\"log\":\"I1204 05:06:14.456169 00001 container.go:189] Failed to update\nstats for container \\\"/system.slice\\\": stat /cgroup/cpu/system.slice:\npermission\ndenied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.456234515Z\"}\n{\"log\":\"W1204 05:06:14.456195 00001 container.go:112] Failed to get\nRecentStats(\\\"/system.slice\\\") while determining the next housekeeping:\nunable to find data for container\n/system.slice\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.456234515Z\"}\n{\"log\":\"I1204 05:06:14.462423 00001 container.go:189] Failed to update\nstats for container \\\"/system.slice/iscsiuio.socket\\\": stat\n/cgroup/cpu/system.slice/iscsiuio.socket: permission\ndenied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.462506599Z\"}\n{\"log\":\"W1204 05:06:14.462450 00001 container.go:112] Failed to get\nRecentStats(\\\"/system.slice/iscsiuio.socket\\\") while determining the\nnext\nhousekeeping: unable to find data for container\n/system.slice/iscsiuio.socket\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.462506599Z\"}\n{\"log\":\"I1204 05:06:14.468533 00001 container.go:189] Failed to update\nstats for container \\\"/system.slice/dev-mapper-centos\\x2dswap.swap\\\":\nstat\n/cgroup/cpu/system.slice/dev-mapper-centos\\x2dswap.swap: permission\ndenied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.468576181Z\"}\n{\"log\":\"W1204 05:06:14.468563 00001 container.go:112] Failed to get\nRecentStats(\\\"/system.slice/dev-mapper-centos\\x2dswap.swap\\\") while\ndetermining the next housekeeping: unable to find data for container\n/system.slice/dev-mapper-centos\\x2dswap.swap\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.468613314Z\"}\n{\"log\":\"I1204 05:06:14.474999 00001 container.go:189] Failed to update\nstats for container \\\"/system.slice/sshd.service\\\": stat\n/cgroup/cpu/system.slice/sshd.service: permission\ndenied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.475206764Z\"}\n{\"log\":\"W1204 05:06:14.475040 00001 container.go:112] Failed to get\nRecentStats(\\\"/system.slice/sshd.service\\\") while determining the next\nhousekeeping: unable to find data for container\n/system.slice/sshd.service\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.475206764Z\"}\n{\"log\":\"I1204 05:06:14.481028 00001 container.go:189] Failed to update\nstats for container \\\"/system.slice/systemd-logind.service\\\": stat\n/cgroup/cpu/system.slice/systemd-logind.service: permission\ndenied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.481085612Z\"}\n{\"log\":\"W1204 05:06:14.481068 00001 container.go:112] Failed to get\nRecentStats(\\\"/system.slice/systemd-logind.service\\\") while determining\nthe\nnext housekeeping: unable to find data for container\n/system.slice/systemd-logind.service\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.481129612Z\"}\n{\"log\":\"I1204 05:06:14.487069 00001 container.go:189] Failed to update\nstats for container \\\"/system.slice/NetworkManager.service\\\": stat\n/cgroup/cpu/system.slice/NetworkManager.service: permission\ndenied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.487102662Z\"}\n{\"log\":\"W1204 05:06:14.487092 00001 container.go:112] Failed to get\nRecentStats(\\\"/system.slice/NetworkManager.service\\\") while determining\nthe\nnext housekeeping: unable to find data for container\n/system.slice/NetworkManager.service\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.487137988Z\"}\n{\"log\":\"I1204 05:06:14.493288 00001 container.go:189] Failed to update\nstats for container \\\"/system.slice/avahi-daemon.socket\\\": stat\n/cgroup/cpu/system.slice/avahi-daemon.socket: permission\ndenied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.493401236Z\"}\n{\"log\":\"W1204 05:06:14.493310 00001 container.go:112] Failed to get\nRecentStats(\\\"/system.slice/avahi-daemon.socket\\\") while determining the\nnext housekeeping: unable to find data for container\n/system.slice/avahi-daemon.socket\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.493401236Z\"}\n{\"log\":\"I1204 05:06:14.499541 00001 container.go:189] Failed to update\nstats for container\n\\\"/system.slice/dev-disk-by\\x2duuid-a5d93a86\\x2da084\\x2d4542\\x2dace3\\x2d6fa218904979.swap\\\":\nstat\n/cgroup/cpu/system.slice/dev-disk-by\\x2duuid-a5d93a86\\x2da084\\x2d4542\\x2dace3\\x2d6fa218904979.swap:\npermission\ndenied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.499698731Z\"}\n{\"log\":\"W1204 05:06:14.499563 00001 container.go:112] Failed to get\nRecentStats(\\\"/system.slice/dev-disk-by\\x2duuid-a5d93a86\\x2da084\\x2d4542\\x2dace3\\x2d6fa218904979.swap\\\")\nwhile determining the next housekeeping: unable to find data for\ncontainer\n/system.slice/dev-disk-by\\x2duuid-a5d93a86\\x2da084\\x2d4542\\x2dace3\\x2d6fa218904979.swap\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.499698731Z\"}\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/333#issuecomment-65538806.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/333#issuecomment-65541533.\n. Hmmm so cAdvisor things the cgroups are mounted at /cgroup but you seem to say they're in /sys/fs/cgroup. What is the output of grep cgroup /proc/mounts. Curious why cAdvisor thinks otherwise.\n. @scottscreations are you still seeing the error?\n. @vishh without selinux everything works. Trying to narrow down the failure with selinux on.\n\n@mdshuai looking at the log its clear that we are detecting the containers every 60s as the expected fallback. We get an error when we try to delete any of our inotify watches, but I'm not sure if it is a problem with the watches in general or just the deletes (it is not clear to me that the creates are being run either). What issue are you seeing? Are there some containers that don't show up when expected?\nCan you check /var/log/messages or the equivalent for SELinux messages? Trying to see if SELinux is blocking anything here. It's not clear that is the case.\nDescribing your setup may help a bit more as well :) Thank you for taking the time to help us debug your issue!\n. @liuyunsh I'd be happy to, but I'm not sure how the setup is different from the non selinux setup :-\\ I'd like to try and see if I can reproduce this problem tomorrow. Is this just a vanilla RHEL7 machine or is there any customization I also need to do?\nIf you could answer some of the questions above it would also really help my debugging :)\n. Ping @rjnagal @vishh \n. Ping @rjnagal @vishh \n. Do you know in that Docker version this became true?\n. I just tried this on 1.3.2 and it works file with the single /. What Docker version are you running this on?\n. I think that version doesn't support using / your // trick tricked it enough to keep the check from firing :) \nhttps://github.com/google/cadvisor/blob/master/docs/running.md#invalid-bindmount-\n. Ping @rjnagal \n. Fixed, thanks @dipankar!\n. Closing this since it has been merged into #358 thanks @eparis!\n. Ping @vishh \n. Done, PTAL.\n. Ooops, committed more than I meant to. Fixed, PTAL.\n. LGTM, thanks for the addition!\n. Thanks @dipankar! This has been a commonly requested feature for some time :)\n. LGTM. Thanks! Could you send another PR with some docs on auth usage?\n. That or adding an auth.go linked from the web UI one SGTM.\n. Thanks @dipankar! Been meaning to do this for some time :) Just some minor nits, looks good overall.\n. Thanks @dipankar! LGTM\n. Thanks! This looks great.\n. Can you squash your commits? Outside of that, LGTM\n. Thanks again! LGTM\n. @kateknister looks great! No other comments from what @vishh and @rjnagal have\n. @kateknister this one looks ready to go, or is there anything left? Just squash commits once more if that is the case :)\n. Awesome addition @rjnagal! Should help a ton with debugging.\n. LGTM, can you squash commits please? We're ready to merge!\n. By \"does not show correctly\" do you mean that it doesn't show at all? Most likely this is due to the memory cgroup not being mounted. Can you provide the output of the following please?:\n- cat /proc/cgroup\n- grep cgroup /proc/mounts\n. Sorry, it's /proc/cgroups and it hsould be on the host. Can you also include the cAdvisor log?\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. This picks up from @eparis's PR #340 to update to the latest libcontainer.\nThe main change that had to happen is that we now have to generate the CgroupPaths in case they are not in the state file (as is the case in older Dockers).\nPing @rjnagal @vishh \n. Thanks for the super speedy review @rjnagal :)\n. Thanks @denderello! Even for small patches, we ask that you sign the CLA if it is not too much trouble:\nhttps://github.com/google/cadvisor/blob/master/CONTRIBUTING.md\n. Thanks @denderello! LGTM\n. Ping @rjnagal @vishh \n. Ping @rjnagal @vishh \n. Not a bad thing :D\n. Ping @vishh @rjnagal \n. Even simpler than I thought!\n. Thanks @kateknister! LGTM\n. LGTM\n. Thanks @rjnagal!\n. Ping @vishh @rjnagal \n. Thanks for the quick review @vishh! Merging.\n. Ping @rjnagal @vishh. We were broken by the last update of libcontainer.\n. @vishh any concerns with having this is an alias? Seems like a good place since not all containers will have a hostname.\n. @hinesmr thanks for the patch! Can you squash your commits and sign the CLA please:\nhttps://github.com/google/cadvisor/blob/master/CONTRIBUTING.md\n. What I mean is, most non-Docker containers don't have hostnames so it makes sense to use aliases which specify custom names within a namespace. I think our question is whether we think that hostname should be something you can query a container by. If the answer is yes, aliases is probably the way to go. It'd be similar to the Docker ID and name today.\nI think querying by hostname is a reasonable usecase. WDYT?\n. By query I mean that in the cAdvisor API I can ask for:\n/api/v1.2/docker/container-name\n/api/v1.2/docker/container-id\n/api/v1.2/docker/container-hostname\nand they would all return the information of that container.\nThe map does SGTM long-term as we use aliases more and more. It is a breaking change in the JSON format though.\n. @hinesmr @vishh's suggestion would just add the raw metadata, not make it queryable in the interface (as in, /api/<version>/docker/<hostname> would not get a particular container). You'd need to fetch the information for all Docker containers and then manually sort through it.\nTo better understand: What is your requirement and usecase? You need direct access to a container by hostname? Or would manual parsing be sufficient? Do you have the container name or ID available? Could the hostname be made the name as well?\n. @vishh I think this means we'd need to add it to aliases.\n@hinesmr I'm not sure I understood what you said about the alias? Could your software query by Docker ID or Docker name instead?\n. Any updates on this? :) Were you able to reuse Docker's name field as an identifier?\n. Ping @vishh @rjnagal. New integration tests coming soon :)\n. Needed to add the Godep. Should work now, PTAL.\n. Ping @rjnagal @vishh \n. Towards what Rohit said, you can calculate usage percentage with the data exposed today (we do in the UI for example). CPU usage is exposed as nanoseconds of CPU. Thus NUM_CORES * 1,000,000,000 = total number of nanoseconds that can be computed in a second. With this data, one can calculate percentages. I'm not sure clockspeed goes into that. Or am I missing something?\nThat being said, adding the CPU clock speed sounds reasonable :)\n. Closing with @rjnagal's PR. Thanks for reporting the issue!\n. LGTM, only minor nits.\n. LGTM thanks @rjnagal!\n. Looks like it is using devicemapper. Although, this is not inside a container. This is at the top level, which is what makes me curious as to why it thinks there are no filesystems.\n. It looks like a different issue so feel free to open a new one. Is there a\nline that says \"failed to register Docker factory\" in the log?\nOn Dec 22, 2014 8:07 AM, \"Yvonnick Esnault\" notifications@github.com\nwrote:\n\nWith 0.7.0, I have this error on going on url http://:8080/docker/\n\"Failed to get container \"\" with error: unable to find data for container /docker/af6b7c91c3226cd09839f981e3ddab7d8a5502b5b71364fe881416773b2b583f\"\nWith 0.6.2, no error (so, there a bug in this diff 0.6.2...0.7.0\nhttps://github.com/google/cadvisor/compare/0.6.2...0.7.0 :-) )\nDockerVersion :\n{\"ApiVersion\":\"1.15\",\"Arch\":\"amd64\",\"GitCommit\":\"4e9bbfa\",\"GoVersion\":\"go1.3.3\",\"KernelVersion\":\"3.14.1-1.el6.elrepo.x86_64\",\"Os\":\"linux\",\"Version\":\"1.3.1\"}\nIs it the same issue as yours ? If not, I can open a new one.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/379#issuecomment-67846751.\n. This issue is still present in the 0.8.0 candidate.\n. @furlongm I was able to reproduce the issue in a Docker container with CentOS 7. I was able to fix it by running with --privilege. I suspect the issue is related to SELinux. I will document that on RHEL7 and CentOS 7 users should use --privilege.\n\nLet me know if this doesn't work for you.\n. Verified that this fixes RHEL 7.\n. @furlongm I suspect you're having a different issue. Can you open up another issue and provide the cAdvisor log (--v=2 if you can) and the output of /validate on the cAdvisor UI?\n. Ping @rjnagal @vishh \n. Ping @rjnagal @vishh this should be the last fix for 0.7.0.\n. Awesome, thanks @rjnagal!\n. Just nits. LGTM feel free to merger after those are fixed :)\n. LGTM\n. That's one path to get the stats exported directly. Lately we've been advocating more the route of adapters. These are small applications (typically in docker containers) that read data from cAdvisor's API and export to the data source they care about. This lets you change your data frequency and retention without having to change or configure cAdvisor. It also gives more more control of the code doing the exporting if you need to do some special auth\n. But yes, all you need is a new storage driver if you want to go that route :)\n. This is now in the official 0.8.0 release btw.\n. Assuming this worked for you? Closing in that case :) feel free to re-open if that's not the case.\n. LGTM, consider making a sysinfo subdirectory of utils? Would make it easier on the import so we know what the functions are from/for.\nMerge whenever you'd like :)\n. LGTM\n. Does the graph not show up or the whole memory section?\nIf only the graph doesn't show: Can you open up the JavaScript console in your browser and see if any errors show up? This sounds like something is breaking drawing the graph.\n. Don't review yet, want to make sure the Travis check works as expected.\n. Ping @rjnagal @vishh looks clean!\n. LGTM. Maybe remove the extra ;\nFeel free to merge after that.\n. Yeah, changed it now.\nOn Mon, Jan 5, 2015 at 2:11 PM, Rohit Jnagal notifications@github.com\nwrote:\n\nDo you want to change the location of vet in this PR or separately?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/421#issuecomment-68791286.\n. Thanks @rjnagal! Merging...\n. LGTM\n. Ping @rjnagal this didn't let us log anything since the initial value was always zero.\n. Open to discussion on whether a minute or more/less often. An hour seemed way too long :)\n. Ping @rjnagal @vishh.\n\n@kateknister this will fix the issue you were seeing before.\n. LGTM\n. LGTM\n. I am not proud to say that all those were files I added :P\n. LGTM\n. LGTM\n. LGTM\n. Thanks @cwahl-Treeptik! We've known about the issue but we should document it better. Let me add a pointer to these instructions to the docs.\n. Note sure...we should test :)\nOn Mon, Jan 12, 2015 at 1:35 PM, Rohit Jnagal notifications@github.com\nwrote:\n\nDo we still have the issue with cAdvisor not able to get to docker socket\non Redhat?\nOn Mon, Jan 12, 2015 at 1:23 PM, Vish Kannan notifications@github.com\nwrote:\n\nMerged #434 https://github.com/google/cadvisor/pull/434.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/434#event-217675962.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/434#issuecomment-69650595.\n. Waiting on license...\n. We're probably fine ignoring veth,docker, and loopback. We can then pick the first of those.\n\nWe should also think about making it easier to report multiple i/f in the next version of the API.\n. Lets add it to the proposed 2.0 :D which will be backwards-breaking\n. This looks awesome! Mainly nits on my part :)\n. LGTM, thanks again @rjnagal!\n. Haha awesome :) LGTM\n. LGTM\n. Oh and lets switch it so that the OomParser library is in oomparser and the binary is in oominfo\n. @kateknister can you push your new commit?\n. Looks good! (although waiting on Travis). Can you please sign the CLA though?\nhttps://github.com/google/cadvisor/blob/master/CONTRIBUTING.md#contributor-license-agreements\n. LGTM\n. Thanks again @arkadijs!\n. LGTM, yes please. One commit would be nice :)\n. LGTM\n. LGTM, only minor nits. (and the Travis failure)\n. LGTM\n. LGTM, feel free to merge :)\n. LGTM\n. Not an intended omission. Thanks for the patch @arkadijs!\n. No questions or judgement on your setup :)\nOnly thing I worry about that approach is that we could be tricked into marking some containers as Docker containers (arguably we could today, but less likely). I agree we need to have a better answer for this though. This sounds like a case we should be able to handle. Let me see if we can come up with something that works here too.\n. @rjnagal I think he means that he runs docker+libcontainer under a top-level LXC container. He's running cAdvisor inside that same LXC container and wants it to work as expected.\n. Closing, reopen if it resurfaces.\n. LGTM\n. Sent out #455 to fix this. If you can test and verify from HEAD, we'd really appreciate it :)\n. Ping @rjnagal \n. That's very...interesting. Does this happen everytime you try? Can you send us the output of /validate on cAdvisor and the output of docker info please?\n. This is a panic.\n. Ah yes, sorry:\npanic: runtime error: slice bounds out of range\n. I believe it is. Closing in favor of that. Note that the error paths don't seem to be triggered.\n. Ping @vishh this is all that is missing to allow us to run the integration tests.\n. All changes made and Travis is passing. Merging :)\n. LGTM\n. LGTM, just nits. Thanks for adding in-container support @rjnagal!\n. LGTM\n. Some small changes, otherwise this looks good. Thanks @kateknister!\n. ok to test\n. Travis says that utils/oomparser/oomparser.go is not gofmt'd\n. LGTM, thanks again @kateknister! Merging\n. That test in Jenkins is being flaky. Will try to address.\n. This is different with the netlink implementation no? Are we sure we want to have those be different? Seems like we won't be able to have a stable return in the API if that's the case.\n. SGTM, especially since most of our users run us in a container. Doing the housekeeping in netlink doesn't sound so bad either.\n. LGTM\n. Thanks for the fixes @domluna! LGTM\n. Working on fixing it. It's the first real run.\n. @rjnagal all tests pass :) the e2e were even faster than Travis :P\n. LGTM\n. The integration test failure is a flake. I need to fix that.\n. LGTM\n. +1 on @rjnagal's options 1\nOn Fri, Jan 30, 2015 at 8:48 AM, Rohit Jnagal notifications@github.com\nwrote:\n\nI can think of two ways:\n1. driver type prefixes like docker machine. I think its clean and is much\neasier to handle.\n2. Use --type and --storage_config=file. We can provide a single config\nfile with all the info. We'll parse the config file based on type.\nI like 2 for conciseness. But I think there were some concerns on passing\naround files for deployment. If that is going to be inconvenient, let's go\nwith 1.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/474#issuecomment-72231121.\n. That is the approach that we've been advocating after the initial two\ndrivers were added. By sidecar @vishh means a Docker container running\nalongside the cAdvisor container.\n\nOn Fri, Jan 30, 2015 at 11:25 AM, Vish Kannan notifications@github.com\nwrote:\n\nHow about a third option:\nSeparate storage drivers into separate binaries and run them as sidecars?\nWe can provide the common infrastructure required to implement such side\ncars, and even build and ship them as part of a cadvisor release.\nThis frees cadvisor from any storage driver related issues. Any auth\nrequired for storage drivers can be packed as part of a separate image\nwithout having to wrap the base cadvisor image. It is even possible to\nexport different data to different endpoints without having to modify\ncadvisor.\nOn Fri, Jan 30, 2015 at 11:13 AM, Ankush Agarwal notifications@github.com\nwrote:\n\nCool. Then I will first refactor influxdb and bigquery config options in\na\nPR and then open up another PR sometime next week to add a stable\ngraphite\ndriver.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/474#issuecomment-72253857.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/474#issuecomment-72255660.\n. That is indeed very weird. Seems like it is looking for the cgroups there. Can you provide the output of /validate on cAdvisor? It'll let us know where it thinks the cgroups are.\n. +1, I think this is possible with Go's introspection.\n. LGTM\n. LGTM\n. LGTM\n. Aren't custom metrics out of scope for cAdvisor? Or maybe I'm not sure what metrics you're mentioning :)\n. LGTM\n. Think I finally got it :) it was passing locally but not in Travis for some reason. Those Eq() checks we're doing are hurting more than helping IMO.\n. While working on exposing metrics to GCM when we expose cumulative metrics (like CPU) it it useful to understand when the counter started. This gives us that :)\n. ok to test\n. Just nits, I think this is functionally ready :)\n. LGTM, thanks @kateknister! Merging\n. ok to test\n. LGTM, we can remove the WIP from the commit and submit :)\n. Tested and verified on systemd and non-systemd systems :)\n. Awesome if you could verify @hinesmr, but I do think this should solve your issue.\n. @hinesmr please try again now that #494 went in.\n. Excellent :) this change should go live with 0.9.0 next week.\n. Ping @vishh @rjnagal all integration tests pass on our usual setups.\n. From my tests 3-6% on a system with O(30) containers. Mainly 3-4% with a\nspike every min to 6%. I think that's within bounds, we may need to do an\noptimization round in the near future.\n\nOn Fri, Feb 6, 2015 at 3:31 PM, Rohit Jnagal notifications@github.com\nwrote:\n\nCan we check resource usage on a systemd setup? I was wondering if\nreporting cpu load hierarchically makes dynamic housekeeping slowdown less\nlikely.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/493#issuecomment-73332692.\n. Will test and report back. We should report usage as part of the release candidates.\n. Uuuu, ouch. So the current usage is ~1% with minute spikes to 3%. That's definitely a big jump in CPU usage. I will try to verify if this is due to CPU load. If so, we should add a flag to allow it to be disabled for the users that worry about that.\n\nYou made a good point about the load maybe making it less likely that housekeeping doesn't get lowered. I will check into that as well.\n. Regathered the numbers and removed the UI from the equation:\n- 0.8.0: ~0.7%\n- 0.9.0 without CPU load: ~1.2%\n- 0.9.0 with load: ~1.5% (there are bigger peaks more often than in the two above though)\nDynamic housekeeping did not seem to affect the results. So we're using roughly twice as much CPU. Digging into why without CPU load the usage went up so high.\n. Alright, found the culprits. So relative to 0.8.0:\n- +0.5% CPU usage for Disk IO stats\n- +0.3% CPU usage for CPU load stats\n. Removed load, PTAL.\n. Done, merge after the other PRs.\n. LGTM\nShould this address #492?\n. LGTM\n. Merging to get into 0.9.0. I can address any other comments in a separate PR :) @kateknister is also working on a new API export endpoint so this will be useful.\n. Helps while examining CPU usage since it is so fickle :P\n. On the plus side, once we have the avg and 90%ile stats we can just use that :)\n. The same stats used in the UI for CPU usage are pushed into InfluxDB. It is a cumulative CPU usage. The UI simply takes the difference and divides over the time period to produce the usage in cores.\n@vishh are the null values due to the pushing of different disks?\n. @gdurr, yes we're planning on moving the disk data to its own series.\n. @mboussaa CPU usage is a cumulative metric so it is an ever-increasing integer. The mean of it won't be very interesting. A derivative allows us to show the difference between data points and thus see how much CPU was used in that time period. May be of interest: #679\n. @mboussaa no, memory is an instantaneous metric. It's value will be useful without a derivative.\n. @mboussaa that looks correct.\n. Yeah we certainly should take advantage of this! It's been on our list waiting on 1.5 :)\nAlthough we probably can't remove it just yet since there are some stats that Docker does not yet export that we use and we have to support pre-Docker 1.5.\n. We do detection today so we should be able to auto-detect and do the right thing. Would that work?\n. @jchauncey we re-based libcontainer to support the new Docker. We haven't prioritized moving to the stats API since we'd need to support the current approach for older versions of Docker, extra information Docker doesn't expose, and non-Docker containers.\nThe main driving factor is not wanting to mount the cgroup hierarchy? We do get other stats from other parts of the FS so that would still be useful to cAdvisor I believe.\n. They should be the same :) both use libcontianer to read cgroups under the covers. cAdvisor also does FS stats for root and for some container types. We'd love to have Docker do those too, but it's tricky with the union filesystems.\n. What version of cAdvisor were you running? Any way that you could get how much CPU cAdvisor was using? :) I don't doubt we're doing some silly things around this...\n. This approach makes sense, I agree that we'd rather export the metrics here than have a separate collector process. We've been looking to export other cAdvisor metrics (latency and usage data) through Prometheus so this lines up nicely. It looks like we can register the custom Collector as well as the other collectors we'd want to use, nice! Looking forward to trying this out :) We'll certainly be poking you with questions.\ncAdvisor is primarily container-agnostic, the API exports all containers on the machine. We're able to gather stats on Docker and non-Docker containers simultaneously. We do have some custom code for Docker containers to get some extra information and export those in a separate namespace in the API (since it's a very common use).\nOn a slight side note, the other portion of this is in Heapster. There we'd like to collect Prometheus metrics from clients and possibly re-export them to a Prometheus backend. It seems like there is a library for the extraction (awesome!) and a custom Collector as you mentioned may help with re-exporting.\n. We'd certainly be happy to get the help :) My thinking was that we'd implement a custom collector that would get the information from all containers and synthesize it into metrics. The hope was to be able to do most of this by introspection so that we wouldn't have to change the code each time we changed a field in ContainerStats.\nWDYT?\n. I just did one for Kubelet in Kubernetes and was surprised at how easy it was :)\nFor cAdvisor, with a reference to the manager you can call SubcontainersInfo:\nGo\nm.SubcontainersInfo(\"/\", ContainerInfoRequest{NumStats: 1})\nThat should return the latest stats for all containers on the machine.\n. It will return them all as a flat list. Instantiating in main.go SGTM.\nThe other interesting question will be labels (for identifying Docker containers), but we can tackle that later.\n. It does include them, but I wonder how we expose them through labels (from my experiments they are nested, but maybe that's not the case). ContainerInfo has a ContainerRef which uses the \"docker\" Namespace and gives the ID and name as Aliases:\nhttps://github.com/google/cadvisor/blob/master/info/container.go#L61\n. Given that #545 was merged I believe the work here is complete :)\nFeel free to open other issues if there is anything missing.\n. @discordianfish that SGTM. My only concern is how Prometheus is affected by the explosion of labels. I was under the impression that label values had to be a small set.\n. This is true, and glad to hear it shouldn't affect Prometheus from someone who knows :) thanks for the explanation!\n. Unfortunately, it's not on the roadmap and we probably won't add it. There are already quite a few projects working with logs and it is a big enough work area that it's not something we can go into lightly.\nI don't think we're a full-features Docker UI (which is fine, it wasn't the initial goal :) ) so there are a bunch of stuff missing in that regard already.\n. Hi Nadir,\nWhat is the output of godep go build github.com/google/cadvisor?\nThe function's signature may have changed but all of cAdvisor's dependencies are forked with Godep so those changes should not affect the build. Your error sounds like the forked dependency is not being used.\n. Sorry, we should have specified that you needed to be inside the cAdvisor repo for it to work. Yes, the build succeeded and the cAdvisor binary should be in you current working directory. Let us know if that is not the case.\nIf you're looking for a Docker image, in the deploy directory there is a build.sh that creates a google/cadvisor:canary image for you.\n. Ah I see, you're trying to connect and query a running cAdvisor. For that you will need to use the client as you mentioned. Don't use the manager or pages/container.go those are part of the implementation of cAdvisor and won't be quite useful outside of it. The client will query cAdvisor through the REST API and return the results. For your example you should be able to do something like this:\n``` Go\npackage main\nimport (\"fmt\"\n    \"os\"\n    \"github.com/google/cadvisor/client\"\n)\nfunc checkFail(msg string, err error, terminate bool) {\n    if err != nil {\n        fmt.Printf(\"%s: %s\\n\", msg, err)\n        if terminate {\n            os.Exit(0)\n        }\n    }\n} \nfunc main() {\n    containerName := \"LONG_IMAGE_ID\"\n    request := info.ContainerInfoRequest{2}\nclient, err := client.NewClient(\"http://<address of cAdvisor, localhost for local>:<port for cAdvisor, default is 8080>/\")\ncheckFail(\"Failed to get cAdvisor client\", err, true)\n\n// Since we specifically want a Docker container, else use ContainerInfo().\ncont, err := client.DockerContainer(containerName, &request)\ncheckFail(\"Failed to get container\", err, false)\n\ndisplayName := getContainerDisplayName(cont.ContainerReference)\nfmt.Println(\"\\ndisplayName = \", displayName)\n\n}\n```\nWhat parts of pages/container.go were you interested in? It may make sense to break those appart. We've been moving away from code there and rather do most of the work in the JavaScript UI where we can be more dynamic.\n. We've been waiting to release it as part of 0.10.0 but that has taken longer to get out than expected. We'll build a 0.9.1 with this fix tomorrow.\n. google/cadvisor:0.10.0 was just pushed and should be available. It will go through our release testing and marked latest today/tomorrow.\n. Just pushed to latest, you should see it now.\n. Awesome!\nOn Wed, Feb 25, 2015 at 8:12 PM, pireslaert notifications@github.com\nwrote:\n\nawesome, working now!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/505#issuecomment-76119519.\n. ok to test\n. Thanks for the fix @difro! Would you mind signing the Google CLA before we merge?\n. Awesome, I see it now. Just the one nit and we'll merge.\n. Please merge the commits. Yeah sorry I'm a bit of a pain :)\n. LGTM, thanks again @difro!\n. Similar to what @vishh suggested, you can run a small daemon that queries cAdvisor and exports the data as you'd like. Heapster does this at the cluster level and is somewhat opinionated on how it looks to exports data. Depending on your goal that or your custom exported would work best.\n. LGTM\n. Also testing if the new machine added to the tests works :)\n. All tests pass, we now have Jenkins testing against the CoreOS beta too :) I'll try to add more distros throughout the day.\n. LGTM, merging since the Jenkins errors are me trying something new before a flight and breaking it. Tests pass on all current distros we have in the cluster.\n. LGTM, just some discussion points and nits\n. LGTM\n. Not ready yet :) will report back when it is.\n. Ready for review!\n. LGTM\n. ok to test\n. Looks good for the events list. We should also make access to the watchers list thread-safe though.\n. Can we do that rename, squash commits, and check the failure (in building it seems)\n. LGTM\n. LGTM\n. ok to test\n. Added some small comments. Let me know when those are addressed :)\n. LGTM\n. Looks like it needs a rebase :)\nOn Feb 20, 2015 3:41 PM, \"cadvisorJenkinsBot\" notifications@github.com\nwrote:\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/523#issuecomment-75339913.\n. ok to test\nOn Feb 20, 2015 3:55 PM, wrote:\nLooks like it needs a rebase :)\nOn Feb 20, 2015 3:41 PM, \"cadvisorJenkinsBot\" notifications@github.com\nwrote:\n\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/523#issuecomment-75339913.\n. Yep, shows one commit now :)\n. LGTM\n. Integrations tests pass mainly, failing on CoreOS but it seems unrelated since the other PR fails too. Will take a look there.\n. Tests are passing now :)\n. LGTM\n. Thanks :) I think I found the issue with this.\n. Assigning to myself :)\n. This is fixed at HEAD\n. Not sure why this wasn't caught by the test there...\n. It was very specific and easy to do :)\n. Libcontainer recently updated their API which made a lot of breaking changes. We have not yet updated to the new API. However, since we use godep we are not affected by this change. Our builds should be clean, if not let us know :)\n\n\nMore detail on building: https://github.com/google/cadvisor/blob/master/docs/build.md\n. ok to test\n. Looks good overall :) only minor comments.\n. Tests look good :)\n. LGTM\n. I'm not super familiar with running things on Marathon so bear with me here :)\nTo specify a different port for cAdvisor you need to tell the cAdvisor to listen on a different port through a --port flag. I vaguely remember that Mesos ran Docker containers in --net=host which means that it can't do the port mapping (although you mentioned that it did it for other servers so maybe I'm confused).\nCan you provide the other port as a flag to cAdvisor?\n. Awesome! Thanks for getting back to us btw.\n. Thanks for the PR @DreadPirateShawn! LGTM minus the CLA. Let us know when you've had a chance to sign that :)\n. Ping @DreadPirateShawn  :)\n. lol, thankfully it was just one letter, otherwise it might take longer :)\nNp, let us know.\n. I don't see it yet, but they may not have processes it yet if you submitted recently. I'll check back and let you know.\n. @googlebot are you happy now?\n. Manually verified the CLA for Skytap. Thanks @DreadPirateShawn and sorry for the delay.\n. ok to test\n. ok to test\n. LGTM thanks @kateknister :)\n. ok to test\n. Have we verified the CPU usage with this?\n. Thanks for reporting @lguminski. This looks like a regression due to a new feature. We pulled back 0.10.0 due to a bug yesterday. We'll fix this one too for when we release 0.10.1. If you run cAdvisor 0.9.0 do you see data the container in question? The one with Docker ID 36f4d2ff87ae3793d46dbd50691cd85257e69f2a3711206460e18d967c03b8c9\n. @discordianfish sorry, it's been on my list to take a look and I've been bad at keeping myself to that. I will take a look today :)\n. Thanks for the PR @discordianfish! This looks awesome. +1 to keeping in seconds. As per the name/alias and labels, today we only ever have two so we can split by \"id\", \"name\", and \"alias\" unless there is a better solution.\n. @brian-brazil for Docker containers yes, for non-Docker containers no (we'll only have an ID for those). I'm not sure I understand your second sentence?\n. @brian-brazil we can try that. It does sound slightly painful to write queries for though :) I don't think the names really change over time. Theoretically the alias can, although I haven't seen it in practice.\n. GitHub lost the other comment threads so I'll reply to some here (let me know if I missed anything).\nAbout the image. It is awkward to get them with the API today. We can put this in as is today and see how to get the image data in.\nFor the package structure, can we rename \"exporter\" to \"metrics\". We will probably use it for more metrics.\n. This looks good @discordianfish! Can we rebase, and I think we can merge :)\n. LGTM, will merge on green. A million thanks @discordianfish :)\n. ok to test\n. Merged! Thanks @discordianfish :D\n. Today we have a concept of \"namespaces\" for container names and those come with a list of aliases used to address the container. Docker containers are under the \"docker\" namespace and its aliases are the Docker ID and name. This feature may be served by the \"marathon\" namespace and have the app ID as one of the aliases. This we will then push to the InfluxDB backend.\nWe probably don't have the bandwidth to tackle this anytime soon, but we will gladly take PRs towards that goal if you'd like :)\n. Yes, you need to do both (I think we export the aliases today, if not we should). Here the aliases they're referring to are not a Docker container name or ID, is that correct @samek?\n. We don't read that file, but we read the libcontainer config here. This config does have the env vars so it should have the data you need.\n. Ping @rjnagal. I want to get to the bottom of the problem in #544 but in theory we should be resilient here. If we get a container that only isolates diskio for example (which I have yet to see), this would fail since we only consider CPU and memory.\n. LGTM\n. LGTM\n. @nstott the first error Docker registration failed means we didn't find the Docker daemon so we won't provide Docker-specific data. That's fine, we'll continue running without it.\nThe second error Raw registration failed means we couldn't find the cgroups. That one is not fine since we have no way to detect containers in that case. If you run into this  let us know and we have help debug.\n. LGTM, ick thanks for checking this. Wonder if we have it on in Container VMs.\n. And the answer is no, sigh....will work to get that in.\n. LGTM\n. Will merge on green, thanks @rjnagal :)\n. Yay! :D merging\n. If we go with a different internal struct we have future flexibility but conversions to v1 and v2. If we go with v2 we only need to convert to v1 which should go away over time. I tend to favor the second since I'm not sure how much more different we could make an internal struct. WDYT?\n. Plan SGTM\n. LGTM\n. ok to test\n. LGTM thanks for the change @simon3z! When you have time, could you please sign the CLA? Once that's done we can merge.\n. Thanks for letting me know! I didn't see you in the RedHat org.\n. WDYT of making the flag accept a list of values where to look for the machine-id and make the default values /etc/machine-id and /var/lib/dbus/machine-id. Thus we check both and take the first, if none exists we leave empty.\n. LGTM\n. Grrr need to tackle the flakes. LGTM\n. There are currently two ways to export data from cAdvisor: backends or sidecars. Backends are compiled into cAdvisor and can be used natively. Sidecars are containers that run alongside cAdvisor and fetch data from it. They then push it to the desired backend. We've been advocating for sidecards recently. They're easier to maintain if someone doesn't volunteer to actively maintain the backend :)\n. @fche yes to collectors initially. Eventually cAdvisor will participate.\n. For gathering data from cAdvisor the best thing is to use the REST API or the Go client library\n. Ah I see what you mean. I'm guessing you'd like some way to get where cAdvisor is pushing data to? We don't have an interface for that today unfortunately.\n. An easy hack today is that however you configure cAdvisor you can configure your other collector. That would not require you to get any information from cAdvisor.\n. @simon3z as @vishh mentioned the original plan was to have collectd consume cAdvisor data and export that. I don't think that's a route we're currently pursuing.\n. LGTM\n. The sort order of the containers should be fixes in 0.10.1 via #532. Are you finding that not to be the case? I will re-name this issue to deal with filtering.\n. And retry the operations we see fail.\n. This looks great @kateknister! Only minor comments.\n. LGTM, will merge, feel free to remove the log line in the followup PR.\n. Kicked Jenkins, seems weird that it could not find the godep\n. This is really for use by the Kubelet. Under certain integration tests we run without cgroups. We don't do too much useful work, but it seems easier to just handle these cases by staying up.\nAn alternative is to use build labels for those cases, but that starts to get more complicated I think...\n. This is already surfaced in /validate. I am taking these one at a time as I find them. This is the last one I've found, but I thought that before too :-\\\n. Thanks for the super fast reviews @vishh :) this has really unblocked me.\n+1 to your suggestion of dropping Travis. Esp, at times like this :P\n. WDYT of having the endpoint do: /storage?label=<label> instead of /storage/<label>. I think that in the future we may want to do /storage/<device> or /storage/<path> and it'll be easier to support that way.\nReviewing the rest :)\n. Just small comments. This is gonna be nice to use :)\n. LGTM, thanks @rjnagal!\n. LGTM\n. LGTM, thanks @kateknister!\n. @nashasha1 closing this issue since the fix is in and waiting a release. Feel free to file an issue if you run into any problems and thanks for reporting!\n. WDYT of this option:\n/stats/container_name/maybe_hierarchical?namespace=docker&count=1\nMainly that namespace (or pick another name) is the type of name we have. It lines up nicely with how we structure it today. We can do \"dockerid\", \"dockername\", etc or we can just do the raw \"docker\" or empty (raw) as we do today. Plus side is no need to know whether it is a name of an ID and it gives us flexibility with pods. Roughly:\n/stats/<container name>?namespace=<type of container name>\nUsing v1 SGTM, agreed that we should move off of it eventually :) thankfully it should be transparent to users so no worry there.\nWDYT of having a recursive=true option instead of the /subcontainers of today? Can probably be an option for all our resources. For another PR of course :)\nWill review more thoroughly tomorrow.\n. Only issue with embedding the type is that by default it is usually \"\" which means \"raw\". We'd force users to have to say \"raw\". We can try to do more smart matching (if the first one after the / is not a known factory then it is raw), but that gets brittle and harder to use IMO.\n. Hmmm: filter, container_type, kind?\n. I don't think we have good data either way :) I don't doubt that most are Docker users though.\n@rjnagal didn't like type :P\n. I don't think we have good data either way :) I don't doubt that most are Docker users though.\n@rjnagal didn't like type :P\n. LGTM except that nit :)\nWhy do we want to do dockerid vs dockeralias btw? Why not just have a docker which can handle both? It'd be easier for the user (and for us with how we handle it today).\n. LGTM except that nit :)\nWhy do we want to do dockerid vs dockeralias btw? Why not just have a docker which can handle both? It'd be easier for the user (and for us with how we handle it today).\n. SGTM\n. SGTM\n. Thanks @mikael84! LGTM\n. Thanks @mikael84! LGTM\n. ok to test\n. ok to test\n. Thanks for the fixes @juliusv! LGTM, will merge on green.\n. Thanks for the fixes @juliusv! LGTM, will merge on green.\n. Thanks! Waiting on Jenkins.\n. Thanks! Waiting on Jenkins.\n. After this, the Travis build with cAdvisor passes.\n. After this, the Travis build with cAdvisor passes.\n. LGTM\n. LGTM\n. ok to test\n. ok to test\n. LGTM, thanks @juliusv!\n. LGTM, thanks @juliusv!\n. ok to test\n. ok to test\n. LGTM, thanks again!\n. LGTM, thanks again!\n. Small comments, LGTM overall\n. Small comments, LGTM overall\n. Lets go with one overall /attributes since we all seem to agree that's fine :) Having a separate /version is nice so we can use it directly in clients and scripts.\n. Lets go with one overall /attributes since we all seem to agree that's fine :) Having a separate /version is nice so we can use it directly in clients and scripts.\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. Kicking butt today @rjnagal :)\n. Kicking butt today @rjnagal :)\n. Thanks for reporting! Sent out #593 to fix.\n. Thanks for reporting! Sent out #593 to fix.\n. LGTM\n. LGTM\n. cAdvisor uses godep so our dependencies area version in the \"Godeps\" directory. We haven't updated to the latest libcontainer and probably will soon. We have some instructions about cAdvisor building here, let us know if you run into any trouble with those.\nClosing this issue for now, but feel free to open new issues if you run into trouble.\n. cAdvisor uses godep so our dependencies area version in the \"Godeps\" directory. We haven't updated to the latest libcontainer and probably will soon. We have some instructions about cAdvisor building here, let us know if you run into any trouble with those.\nClosing this issue for now, but feel free to open new issues if you run into trouble.\n. btw:\n$ ./deploy/check_gofmt.sh .\nThe following files are not properly formatted:\napi/versions.go\n. btw:\n$ ./deploy/check_gofmt.sh .\nThe following files are not properly formatted:\napi/versions.go\n. LGTM\n. LGTM\n. Ping @rjnagal\n. Ping @rjnagal\n. Yes you're right, looks like I didn't get this sufficiently :P will revert that tomorrow. Thanks for catching and reporting! There is an issue out for using the docker builds in or integration tests, but we're not there yet.\n. Yes you're right, looks like I didn't get this sufficiently :P will revert that tomorrow. Thanks for catching and reporting! There is an issue out for using the docker builds in or integration tests, but we're not there yet.\n. ok to test\n. ok to test\n. LGTM, will wait for green. Thanks @smarterclayton\n. LGTM, will wait for green. Thanks @smarterclayton\n. We already have a global flag for collection interval. Do you mean: ask for data at 2s intervals? We can do this only if your interval > our collection interval. With dynamic housekeeping it gets interesting, but we can return an error in those cases.\n. We already have a global flag for collection interval. Do you mean: ask for data at 2s intervals? We can do this only if your interval > our collection interval. With dynamic housekeeping it gets interesting, but we can return an error in those cases.\n. The user would not get data that was nice in their resolution interval unless it is a multiple of our collection interval though.\n. The user would not get data that was nice in their resolution interval unless it is a multiple of our collection interval though.\n. Do you mean an API to dynamically adjust the collection frequency? Lower resolution do you mean lower frequency (higher seconds)?\n. Do you mean an API to dynamically adjust the collection frequency? Lower resolution do you mean lower frequency (higher seconds)?\n. Ah I see what you mean. We should be able to as long as the requested resolution is > the collection interval (we can't do 0.5s if we gather every second). If the resolution is not a multiple of the collection interval then you may not get stats as you expect. For example, if we collect every 2s and user asks for resolution of 5s we'd need to interpolate the first or only do every 2 of the user's queries.\nDynamic housekeeping makes it interesting since we may have non-uniform intervals on the same stat: 1s, 2s, 4s\n. @kargakis would you happen to know if the new one is a drop-in replacement?\n. ok to test\n. LGTM, thanks @simon3z!\n. Looks like we don't ever get the token.\n. Thanks @kargakis! This looks good. @rjnagal is there any other testing you'd like to do?\n. @kargakis looks like something is missing from the deps.\n. LGTM, great docs thanks @rjnagal!\n. LGTM\n. LGTM\n. /cc @vishh \n. No, it just fixed an expected behavior (in my opinion). I'm fine leaving a default but having it documented. It is better than returning all...will change this PR for that.\n@vishh, not for that but there is a followup with your name on it :)\n. @vishh I can see the usecase where someone wants to get the 10 stats before a certain time or limit the number of results. But it does feel like a better default for time-based queries is \"all\" rather than an arbitrary limit. That was the logic behind this PR. It would be simpler if we returned \"all\" by default in our cases. WDYT @rjnagal? It may be specific to the flags on the memory storage, but it'll provide consistent behavior.\nAn alternative is to say that the default when specifying both start and end is \"all\" unless otherwise specified.\n. We no longer fetch stats from external drivers. We just use them for export.\nI'm fine with specifying: default NumStats = 60 and if Start and End are specified NumSats is ignored.\n. Updated to document the limit, set the default, and ignore NumStats for time range queries. PTAL @vishh @rjnagal \n. @roldancer it looks like cAdvisor is not able to write to its log files. For some reason it is unable to open a file in the container's /tmp. Since it is an Atomic host, it sounds like an SELinux thing.\nTo prove that theory, can you try running without privileged with --logtostderr:\ndocker run -ti --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro --publish=8080:8080 --name=cadvisor google/cadvisor:latest --logtostderr\n. Yeah cAdvisor tries to look at a lot of places and sometimes we can't without being root. SELinux restricts some of that. This looks like our container detection inotify is failing. We still manually check every minute so containers should show up (do they?) after ~1min.\nI'll take a closer look later today, these errors shouldn't be happening anyways.\n. Sorry for the long delay @roldancer! This looks like a similar problem to #333. We either have a bug in the delete path or SELinux is unhappy (since both see this with SELinux). If you disable SELinux, so you see those log lines?\n. LGTM\n. @perezd you can get this from cpuacct.usage_percpu which is usually under /sys/fs/cgroup/cpuacct\n. We're using numCores in this amount. If you're still around for debugging :) Are we logging all the info messages (--logtostderr --v=2)? This failure is in the UI handling which should only come up after initialization and we're not seeing the log message that prints out MachineInfo.\n. Hi @joelchen,\ncAdvisor may be a bit too low level on the stack for alerting and the like. What we've seen others do is have cAdvisor export data to monitoring systems that can graph, visualize, and alert on that data. It brings more flexibility and scales better for clusters. One that I've heard use a lot recently is Prometheus You should be able to get that setup in CoreOS\n. Heads up @kateknister and @vishh \n. Current assuming #623 is merged.\n. Thanks @rjnagal! Sorry for the delay.\n. LGTM\n. Looking at the referenced files:\nhttps://github.com/GoogleCloudPlatform/kubernetes/blob/master/Godeps/_workspace/src/github.com/google/cadvisor/manager/manager.go#L585\nhttps://github.com/GoogleCloudPlatform/kubernetes/blob/master/Godeps/_workspace/src/github.com/google/cadvisor/storage/memory/memory.go#L41\nWe don't make a copy of the returned stats which triggers the race detector (I think). The source of the data (for the Add() and the get) are under a RWLock which is what makes me think it is the underlying data rather than the structure we get it through. It's a bit odd since our workload write is append only.\n. ok to test\n. Thanks for the patch @jdef! and nice debugging :)\nWe went the ring buffer route to reduce allocations (we were using a list before) and having a ring buffer of pointers is going back to that. The alternative is copying the elements we return, but that also incurs arguably more allocations and realistically we're doing queries a lot on this data. The approach you have here is the better one I think. LGTM.\n. LGTM\n. Thanks @justinsb! LGTM\n. ok to test\n. ok to test\n. ok to test\n. LGTM\n. Network stats are failing on Ubuntu. Will take a look later today when I have a bit more time.\n. Not yet, will get to it today I promise :)\nOn Thu, Apr 9, 2015 at 9:38 AM, Rohit Jnagal notifications@github.com\nwrote:\n\n@vmarmol https://github.com/vmarmol Did you fix the network stats\nalready?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/637#issuecomment-91285742.\n. Tests are passing (not sure why the initial failures, I haven't been able to reproduce them). However, Jenkins is failing due to the OOM test. Did that pass for the previous PR?\n. Jenkins passes: http://104.154.52.74/job/cadvisor-e2e/778/\n. PTAL @rjnagal :)\n. We used to have a bug open for this, but we closed it with some API comments. We should document this in the UI as well.\n. Memory accounting is interesting because different parts of the kernel do it differently. I am not sure what free -m uses. I'm curious what the hot vs. cold memory we were reporting in that case\n. Spoke to @kateknister, I think we have a fix on the way :)\n. LGTM, waiting on green.\n. The Jenkins failure is the same one we've been seeing and expected. This change should help @kateknister send a fix for the issue so I'd recommend we merge :)\n. Thanks for the super quick review @vishh :)\n. @cadvisorJenkinsBot please test\n. LGTM\n. Yes, although we'll need to test how systemd access works within a container. Will change the issue's title to reflect that.\n. cAdvisor degrades its OOM support in these cases (it does not record OOMs), but should otherwise function as expected.\n\nNot sure what the right answer is here, we probably don't want to include our own journalctl in the Docker image. We may end up mounting it from the host.\n. We do today, although we always assume that if systemd is present so will\njournalctl. Let us know if that's not the case :)\nOn Jun 9, 2015 9:22 AM, \"James Cuzella\" notifications@github.com wrote:\n\nJust a hint: It's probably good to gracefully degrade here, and not\nrequire journalctl because older distro versions (e.g.: CentOS / RHEL 6)\ndo not have systemd or journalctl.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/645#issuecomment-110421256.\n. @trinitronx was the file available on the machine? The volume mounting of /var/log should work.\n. We're ready to merge this :)\n. LGTM\n. We should be handling this with the resource path similar to what we do in other parts of the API. So: /api/v1.3/events/container_name?other_params\n\nWill send a PR.\n. @allencloud, do you mean to configure which of the existing metrics to export or to allow cAdvisor to collect custom user metrics we don't today?\n. A regex is probably not a good way to handle this since metrics today are exported as structures JSON objects and any regex against a string ID will be specific to the storage driver it is created.\nAlternatives I can think of are:\n- Define each of the metrics today globally with some string ID instead of the objects we define today\n- Parallel JSON structure that defines whether that metric is exported (default to true) e.g.:\nI think the second is better, easier to configure, and better aligns with what we do today.\n. Fixed\n. Stats, events will have a separate flag.\n. I think those tests are broken at HEAD:\nhttp://104.154.52.74/job/cadvisor-e2e/812/console\nthat's from a clean client. Let me try to debug.\n. Rebased, Travis is green and Jenkins passes: http://104.154.52.74/job/cadvisor-e2e/821/console\n. @rjnagal this should be ready for review and fix head.\n. Integration test is passing: http://104.154.52.74/job/cadvisor-e2e/813/console\n. Travis is green and Jenkins passes: http://104.154.52.74/job/cadvisor-e2e/818/console\n. Jenkins passed: http://104.154.52.74/job/cadvisor-e2e/824/console\n(unsure why it is not updating GitHub :( )\n. /cc @vishh @rjnagal \nVerified this fixes the problem in Kubernetes. Will patch this in once we merge :)\n. We've been moving to time based since its easier to reason about (we keep all events for the last 24hrs rather than the last 1000 events). Alternative we can have some policy to rate-limit the errors or place an absolute limit as well as a time limit. WDYT? We can do either of those in another PR since this one makes the current situation strictly better :)\n. @rjnagal PTAL, all tests pass. The error was on the CI side, the machines had a bunch of leftover crud.\n. I will followup this PR with an absolute limit.\n. Hi @paulczar I'm guessing this happens consistently? Could you run cAdvisor with the flags: --logtostderr --v=4 and provide the logs once more?\nIt seems that the crash is on a dbus call which we only do to check systemd. I may try to get my hands on a 15.04 VM as well.\n. Thanks @frioux! That error is not fatal. The panic is in the dbus library we're using. I think this is the first version of Ubuntu to come with a systemd? Maybe something is a-typical about it (or we just need to update our dbus library).\nWill dig more into this. Thanks for reporting!\n. Sorry, haven't had time to spin up a VM with the new Ubuntu since I mainly use public clouds...Will try to get to this soon.\n. Just got a system up and running and verified the issue. Will look into fixing it. Interestingly, it only happens inside a container.\n. Hi @wololock, yeah this is a pain point we've been seeing a lot lately. I think it is time we tackle it. The suggestion so far has been to make links relative to allow for reverse proxying. This should work in your case too I believe.\nI am closing this as a dup of #596, but I will try to find some time to tackle it this week. Or if you're up for it feel free to send a PR :) it should be a relatively small change and mostly localized to the pages/ directory.\n. @nashasha1 I run the same thing and I get the expected result:\n$ git checkout 4ea9039ff269fc8675409df7f1a192c94eab2f52\nHEAD is now at 4ea9039... Add ChargeProcess to Container API.\nDo you have the latest version of libcontainer?\n. Actually, now that I look around I see that this is a non-existing commit that I have :) Letme rebase cAdvisor and re-do the godeps. Thanks for reporting!\n. grrr godeps and I mixed some up with the previous PR. Will fix.\n. Fixed the go imports issue. PTAL @rjnagal \n. We'll print out each new raw container as a failure. We will also \"detect\" them every 60s and re-print out the failure. We need to somehow tell the manager to ignore the container. Maybe create it but don't house keep it?\n. LGTM, thanks @rjnagal! Merging, the e2e machines are unhappy :(\n. Thanks @rjnagal! PTAL\n. Can you try to remove mounts until it runs? Just trying to figure out what is failing :)\nI imagine it is related to SELinux?\n. Awesome @remoteur! Let us know what you find out, good to know for future issues.\n. Can you run cAdvisor with --logtostderr --v=2 and provide the outputs?\n. Sorry, maybe I wasn't clear :) I wanted to take a look at the cAdvisor logs to see if there were any errors in sending the data.\nCan you also do a list series in InfluxDB?\n. It looks like we're not able to connect to InfluxDB. The issue is that you're pointing to localhost for InfluxDB, that resolves to the container itself which is not running InfluxDB. You'll need to give it the IP of the host itself.\n. Sadly the best we have is the code for now. Just filed #681 to add that.\nClosing this issue, but feel free to open a new issue if you run into any problems :)\n. Great! We're happy to see what's missing and what's harder than it should be :)\n. LGTM\n. Hi @huikang I believe you want to do:\nselect derivative(cpu_cumulative_usage) from stats where container_name='mysql' limit 600 group by time(2s)\n. cAdvisor also outputs in number of cores. The raw data is in billionths of a core so you'll want to divide by 1,000,000,000 to get the result in whole cores. For percentage you'll want to divide over the number of cores on the machine (or do percentage of a core, which is what cAdvisor does.)\n. This seems to be mostly correct (the second value is the derivative I believe). This will be the percentage used over that 5s interval.\n. Great! Will close this @huikang but feel free to open another issue if you run into any trouble.\n. The raw CPU metric is in total nanoseconds used since creation. For the UI\nwith typically take the delta over a time period and divide it by the\nlength of the time period. This gives us instantaneous usage in number of\ncores.\nOn Jul 2, 2015 7:24 AM, \"mboussaa\" notifications@github.com wrote:\n\n@vmarmol https://github.com/vmarmol what the unit of measure for CPU\nutilization. you said that it \"outputs in number of cores\". But how can you\ndefine this measure ? thanks\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/679#issuecomment-118004263.\n. /cc @rjnagal \n. The current output schema could use some love. Recommendations are welcomed :)\n. @rjnagal PTAL, added the right gocap dep\n. Also remove from the info file please :)\n. LGTM! Will merge on green. Thanks @rjnagal\n. It looks like you have multiple network interfaces and we're picking up the wrong one. I'm guessing we're attaching to eth0 and you're downloading using weave?\n\nWe have an outstanding issue to support multiple network interfaces.\n. Closing as a dup of #686.\n. @pchico83 is this being returned as the limit for a container or as available memory on a machine?\n. If for machine, can you please provide the output of these two commands to help debugging:\n$ cat /proc/meminfo\n$ cat /sys/devices/system/node/node*/meminfo\n. Ah, so this is the usage of the root / container correct? If so, can you provide the output of:\n$ cat /sys/fs/cgroup/memory/memory.stat\n$ cat /sys/fs/cgroup/memory/memory.usage_in_bytes\n. To calculate working set we do:\nusage - total_inactive_anon - total_active_file\n267,001,856 - 225,005,568 - 45,862,912 = -3,866,624\nThat probably gets overflowed to intmax somewhere. I will send a PR to cap that at 0, but we also need to look into our logic here. We're probably double-counting some memory.\n. @rjnagal it should (which is another bug)\n. LGTM, thanks @rjnagal! Feel free to update docs/api.md in this PR or another.\n. Ping @rjnagal \ne2e's are failing due to an unrelated issue. Fixing in another PR.\n. LGTM\n. Thanks for the fix @rjnagal!\n. This is ready to review and fixes the e2e. @rjnagal @vishh \n. ok to test\n. this looks great @superzhaoyy, thanks for the PR!\n. Looks like we'll need to add the redis godep btw.\n. Usually what works for me is this (from cAdvisor root):\n$ godep restore\n$ go get -u github.com/garyburd/redigo/redis\n$ godep save ./...\nDoes that return any problems?\n. @superzhaoyy that pretty much means to make your commits into less commits, so that you're \"squashing\" them together. This article does a good job walking you through it:\nhttp://gitready.com/advanced/2009/02/10/squashing-commits-with-rebase.html\nLet us know if you run into any trouble.\n. That works too :)\nIt looks like you got a rebase slightly incorrect. Git is a bit of a pain to get used to for stuff like this...\n. Yes you certainly can close this PR and make a new one. You shouldn't need to delete your repository, just make a new branch:\nAssuming that origin is the upstream fork in GitHub.\n```\nGrab the latest code\n$ git fetch origin master\nMake a new branch based on the current code\n$ git branch new-branch origin/master\nSwitch to that branch\n$ git checkout new-branch\n```\n. LGTM\nNice catch! I remember noticing the issue of us assuming increasing times, but I forgot about it :(\n. LGTM\n. LGTM\n. WDYT of exporting the existing metrics in this format? The concrete types we export today are useful and easy to parse for systems, but using the metrics approach will make it easier to export to other systems.\n. Made requested changes @rjnagal PTAL\n. Travis is taking forever...my tests ran fine. Need to get Jenkins to run Travis tests for cases like this.\n. Hmmm two things to try if you have the time :)\n- Don't mount /sys (but leave /)\n- Run with --privileged\n. This appears to be a bug in Docker 1.6.1. Talking with them to see if there will be a 1.6.2 that fixes it. Libcontainer was already patched.\n. For now, you can remove the /sys mount (but leave the /) and it should work for most stats. A few (like network) will be missing.\n. Update: docker/docker#13133\n. Docker 1.6.2 has been released! Closing this issue. Thanks everyone for reporting!\n. We don't show blkio in the UI. Arguably we should. We'll use this bug to track adding that to the UI.\nThanks for reporting! It's useful for us to know what features people want.\n. @lukasmartinelli thanks! We've been tracking two memory related issues and this is one of them. The reproduction will really help narrow it down.\nWill take a look.\n. /cc @rjnagal \n. Sorry :)\n. LGTM\n. +1 to adding to UI/API and only querying on demand\n. LGTM, just the nit. And run tests please :) since Travis is being mean.\n. LGTM, thanks @rjnagal!\n. LGTM\n. Don't we need to add the gCharts mapping in static.go?\n. Ah kk SGTM and thank you for splitting them out! It was very hard to read how we had it before (and impossible to diff...)\nLGTM\n. LGTM\n. ok to test\n. One thing I noticed, is this just always over-writing the same key? Such that we only store one value? I'm not as familiar with Redis so I apologize for the noob question :)\n. Thanks for explaining @superzhaoyy!\nLGTM, merging.\n. LGTM\n. You'll need to set --enable_load_reader=true when you run cAdvisor. Be warned though, this may cause instability in cAdvisor which is why we don't have it on by default. We've observed periodic crashes when it is enabled and have been looking into them.\n. Closing, feel free to open an issue if you run into any trouble.\n. LGTM\n. LGTM, thanks @rjnagal!\n. @bdanofsky I'm curious, are you consuming the cAdvisor manager as an API instead of using the remote one? In the remote one we are starting to have the concept of \"Docker\" container, the internal interfaces are all raw containers pretty much. We could do a better job in that interface as @rjnagal mentioned. No one has used that API till now so it wasn't prioritized, which is why I ask :)\n. LGTM\n. LGTM\n. LGTM\n. /cc @rjnagal \n. Yep, safe to merge! Now all the tests are run by Jenkins as well.\n. We can wait for the extra fix before we merge #728 :)\n. @tweakmy I'm guessing you recently updated to Docker 1.6+? Docker changed how they handle containers with 1.6. You'll need a more recent version of cAdvisor (0.12+) that knows how to handle the new format.\n. It may be using a locally cached version of it, try a docker pull\n. No worries! Closing, feel free to open another issue if you run into any problems.\n. This is now good to go @rjnagal \n. ok to test\n. I see the corporate CLA, thanks! The bot doesn't know how to check corporate CLA (unless you setup a group for it).\n. btw:\nThe following files are not properly formatted:\nmanager/machine.go\n. LGTM, thanks @rjnagal!\n. Fixed tests, PTAL @rjnagal \n. Could you get a CPU profile of cAdvisor for us to debug:\ngo tool pprof http://localhost:8080/debug/pprof/profile\n$ png > output.png\nIt would also be know how about how many containers you're tracking. 0.14.0 just missed some extra information we added to /validate for that :)\n. @teon could you grab a trace to help us debug?\ngo tool pprof http://localhost:8080/debug/pprof/profile\n$ png > output.png\nWe definitely wanna get this fixed ASAP, but have been unable to reproduce it.\n. @teon sorry, you just need to hit the cAdvisor port so if you expose the port on the host then it is that host's hostname and port. You probably don't want to run it inside the cAdvisor container since it doesn't have Go installed.\n. @teon you can ship us the /tmp/qtt6Z8he7k since that has the raw profile. Otherwise check help in pprof to see how you can output it and share it with us. The profile should be sufficient for us.\nThanks! :)\n. Argh, that isn't working as nicely as I'd like :( @teon if you have some time can you ping me on IRC so we can try some more live debugging? I am vmarmol on Freenode, I'm in #google_containers.\n. Thanks for the patience, this is the first time I try remote pprof profiling via GitHub :) I think I figured out the command line:\n$ go tool pprof -png -output=out.png http://localhost:8080/debug/pprof/profile\nAlternatives to png (if that's not available): svg, gif, pdf, andps\n. LGTM\n. LGTM\n. To get a graph of cores used we usually do something like this:\nselect derivative(cpu_cumulative_usage) from stats where container_name='mysql' limit 600 group by time(2s)\nWhich (outside of the /4) looks like yours. Not quite sure what the error is about...Is it unhappy about selecting a derivative and something else, or doing the math on top of the derivative?\n. As far as percentages. We don't expose either limit today so it is hard to determine percentages :(\n. Thanks @mboussaa! I will re-purpose this issue to add the spec to InfluxDB.\n. Yes, the data is exported in nanoseconds used.\n. Memory is exported in bytes, we also don't push the spec today.\n. Unfortunately no, we're working on refactoring how StorageDrivers are handled before we tackle this.\n. LGTM\n. Eager to see that @mnuessler :)\n. Sorry, we've been swamped this week with the Kubernetes 1.0 launch. Will try to take a look today/tomorrow. Nothing against the PR, have just been swamped :)\n. We started treating memory as a cache and not a storage driver so it should be easier to make the change :)\n. LGTM, nice! removes lots of code.\n. LGTM\n. LGTM, thanks @rjnagal!\n. LGTM\n. Tests failing:\n```\n+ godep go test -v -race -test.short github.com/google/cadvisor/...\ngithub.com/google/cadvisor/fs\nfs/fs.go:82: const initializer map[string]bool literal is not a constant\nfs/fs.go:84: mount.FsType undefined (type *mount.MountInfo has no field or method FsType)\ngithub.com/google/cadvisor/fs\nfs/fs.go:82: const initializer map[string]bool literal is not a constant\nfs/fs.go:84: mount.FsType undefined (type *mount.MountInfo has no field or method FsType)\ngodep: go exit status 2\n```\n. The feature is currently disabled as we work to improve it. We've had some issues with the reliability of the data so have chosen not to expose it just yet.\n. @ananyakumar this is a good one to start with. It should touch a bunch of cAdvisor and all in Go :)\n. We'd just like to remove the data from showing up. In the UI and the API.\nOn Jun 8, 2015 12:16 PM, \"Ananya Kumar\" notifications@github.com wrote:\n\nA clarification: do we want to remove the network stats panel from the\ncAdvisor GUI when Docker containers are run with --net=host? Or do we want\nto find a way to profile the container's network throughput even when it's\nrun with --net=host?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/749#issuecomment-110111148.\n. Niiice :)\n. LGTM, thanks @rjnagal!\n. Fixed @rjnagal, PTAL\n. @Priyanka5 it looks like cAdvisor started fine from the log, but does not have Docker support. Is that the problem you're observing?\n. @Yashu5 can you try running the container with --privileged=true? We've seen some instances of Fedore/RHEL needing that due to SELinux.\n. Awesome! I added a note to the docs. Will close this issue, feel free to open another if you run into any problems.\n. We can probably piggyback off of work on #760.\n\n/cc @AnanyaKumar \n. LGTM, thanks @AnanyaKumar!\n. Yes, we should add the suggestions you mentioned. We tend to work from the latest stable, so Go 1.4 as of now. Not sure about 1.3, but 1.2 is definitely \"too old\".\n. @AnanyaKumar would you like to document the requirements in the building docs page? :)\n. LGTM, thanks :)\n. @AnanyaKumar sorry, didn't mean to make that a challenge :D I just meant that it would be a small change. What you're doing is along the lines of what I had in mind.\n. @AnanyaKumar if there is a simple-ish way to add a test for this then I would certainly encourage it. That is pretty hard today unfortunately...\n. No worries :) Change LGTM, but it looks like you included a few other commits. A rebase gone wrong maybe?\n. LGTM, thanks @AnanyaKumar!\n. Are you running Docker 1.6.1? This sounds like it is the case. This version had a bug where this mounting was not allowed. Unfortunately the only fix is to upgrade to 1.6.2+ :-\\\n. Verified this fixes the leak! Will cut a 0.15.1 with this once we merge.\n. LGTM, thanks @AnanyaKumar!\n. In that case just do: --volume=/sys/fs/cgroup:/sys/fs/cgroup:ro although that should be done with /sys... Can you run cAdvisor with: --logtostderr --v=4 and provide the stdout?\n. Closing in favor of #787\n. Hi! The only question and concern I have is how these translations are updated when changed are made to the english version. Unfortunately, none of the maintainers are fluent in french.\n. Hi @a-pastushenko, do you mean that you now see those services (as containers) or that you no longer do? Systemd runs everything inside containers so you should expect to see the different services reported by cAdvisor.\n. @a-pastushenko where do you see these null values?\n. Hmmm can you post screenshots or example text dumps? Unsure what exactly us returning null values.\n. ok to test\n. @Marmelatze this looks good for the most part. I know image has been requested for a while. Env variables I'm not so sure we want to do however...what is the intended usecase? Just afraid that we're exposing data that is tricky to use and easy to abuse.\n. LGTM, thanks @Marmelatze! Can you rebase and repush to kick the CI?\n. If you're running the latest cAdvisor can you get the output of the /validate HTTP endpoint? I think it is working as implemented, but maybe not as you'd expect :) Let's try to see the setup and see where we can improve.\nFor the filesystem usage, we have no support for getting that explicitly for LXC containers today.\n. Hmmm the validate output indicates that we do see the resources being isolation. This makes me think it may be a UI issue. Can you do two things please:\n- Check for errors in the JavaScript console\n- Get the output of /api/v1.3/containers/machine.slice/machine-lxc-<LXC container name>\n. Thanks for reporting @kenzodeluxe! Just verified the behavior and will look into a fix.\n. Fixed by #783 \n. Will work on a release today :)\nOn Fri, Jun 26, 2015 at 12:31 AM, kenzodeluxe notifications@github.com\nwrote:\n\nWould it be possible to release a new version including the fix?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/782#issuecomment-115561503.\n. Huh, this really should have made it to 0.16.0 (at least as far as we know it did). Are you not seeing the fix when you run it?\n. @rjnagal needed to change the command a bit in the unit test. Fixed. Thanks for the review!\n. @pacuna you will need to inject the required files into a Docker image you derive from the cAdvisor one. Something like:\n\n```\nFROM google/cadvisor:latest\nADD auth.htpasswd /auth.htpasswd\nEXPOSE 8080\nENTRYPOINT [\"/usr/bin/cadvisor\", \"--http_auth_file\", \"auth.htpasswd\", \"--http_auth_realm localhost\"]\n```\nTo generate the file take a look at this doc which has some pointers.\nLet us know if you have any questions or run into trouble.\n. Looks good overall! Minor comments\n. Let me know when this is ready for another look @anushree-n :)\n. LGTM, thanks @anushree-n! Can we squash commits? and we'll merge.\n. The goal is to have an Automated Build with an up-to-date cAdvisor. This is not the official release since it is ~600MB in size, but it'll allow people to test the latest features.\n. It was @afein's idea :D\n. Merging as the build is green!\n. Must have been the recent move of libcontainer to runc that broke the link.\nThe issue mentioned that stats only work if you use the cfq io scheduler.\nOn Jul 2, 2015 7:05 AM, \"mboussaa\" notifications@github.com wrote:\n\nhttps://github.com/docker/libcontainer/issues/165 does not work\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/791#issuecomment-117999443.\n. The code that does the detection is here if you're interested: https://github.com/google/cadvisor/blob/master/utils/sysinfo/sysinfo.go#L87\n\nWhat problems are you running into building cAdvisor? Some help is available here: https://github.com/google/cadvisor/blob/master/docs/build.md\nOutside of that, what is the output of ls /sys/class/net on those machines? That is how we find out about network interfaces today.\n. Did you not get stats with the /sys volume but without --net=host? I would have expected that to work.\n. Thanks @hookenz! That seems like the command on the README, or is there any documentation you think we need to update and make it easier to understand?\n. Failure is due to a different, but known flake. sigh I'd say to ignore it for now, but we should still get this PR in.\n. We're now skipping the known flake. PATL @anushree-n\nWe don't have to wait for Travis as it runs the same things Jenkins already did.\n. @AnanyaKumar yes it will most likely miss a packet and the test will fail. It is a flaky test unfortunately...\nIn practice it doesn't happen often. We need to fix most of the tests in this fixture for cases like this.\n. Is there a cadvisor.service in the repo?\n. That's interesting, looks like an issue on the UI, will take a look. Thanks for reporting!\n. Hmmm the diff is so bit GitHub is choking on it :( can we split the PR into two? Let's try to get the Godeps in first and then the actual changes.\n. LGTM\n. Tests pass! Merging. Thanks @AnanyaKumar!\n. Lets rebase this off of master since we merged the other PR\n. We don't have a knob for it today, and we're wondering what the usecase is for running the API but not the UI. The UI just hits the API anyways :)\n. Hmmm does it ever update? Only way it could be an issue on cAdvisor is if it didn't push the data. Can we find the data in InfluxDB when this happens? We should also check the cAdvisor logs to see if we had any problems pushing the data.\n. Unfortunately we don't export the per-core usage to InfluxDB today. Is that the requested feature?\nSince you're binding to only 1 core, the total utilization should be equivalent though\n. The kernel exports per-CPU usage as well as usage aggregated over all cores. We only export the aggregate CPU usage. We can use this bug to track exporting per-CPU usage as well.\n. Are we also changing the code to depend on the new location?\n. ok to test\n. Thanks :)\n. ok to test\n. LGTM\n. ok to test\n. LGTM, will merge on green. Thanks @anushree-n!\n. Woah, much faster :D\n. LGTM\n. LGTM\n. The network on seems weird, but blockio may be due to the scheduler used. Can you provide the ourpur of /validate on one of these machines? Do we know if they containers are detected as Docker containers by cAdvisor?\n. It looks like only sda supports the cfq diskio scheduler which is the only thing we can get stats for (not sure how device mapper comes into play, but I suspect no stats for that). Since this is Kubernetes, the network namespace (and devices) are owned by a different container so only that one will show the stats. Is that the issue?\n. Are you suggesting to export the stats to something like a CSV file? That's certainly possible, you can add a new StorageDriver that does so. You can look at other drivers for examples: https://github.com/google/cadvisor/blob/master/storage/\n. Sadly we don't export those stats today. We compute them in JavaScript today here:\nhttps://github.com/google/cadvisor/blob/master/pages/static/containers_js.go\nWe could maybe expose them, but different people will want different things so we've gone with letting the users compute it today.\n. We should make this change in the Kubernetes configuration rather than the default for cAdvisor I think.\n. LGTM, thanks @piosz!\n. ok to test\n. I've been resisting adding a Makefile since not having one makes it clear that we're \"just Go\". This is the second request for a Makefile, is this something people expect and prefer?\n. /cc @rjnagal \n. LGTM, will wait for @rjnagal's LGTM\n. @anushree-n is right about the different periods. Long term we want to move everything to metrics which would solve the timing problem and be the most efficient storage mechanism. That will probably take some time though :) @rjnagal can't we coalesce the name,labels into the same metric? The way it is exposed today it should handle multiple points with the same metadata. Maybe we need an intermediate structure as an output to the Collects.\n. LGTM\n. Not directly, but you can calculate it.\nIf 100% == 1 core:\n// Get usage over the time period\nUsageInCores = (end.Usage - start.Usage) / (end.Time - start.Time)\nIf 100% == the whole machine:\nUsageInCores / NumCores (from /machine)\n. LGTM, thanks for the quick fix @rjnagal!\n. Can you provide the output of /validate and the cAdvisor log please?\n. @anushree-n and I spoke on Friday and we thought that we may want to break the custom metrics into their own completely (not rely on Stats). This is more in line with where we want to go long-term with everything being a metric. I'm OK with putting them in Stats as we're doing here in the short-term if we think it'll make things simpler. From some of the issues with storage, it may not.\n. LGTM\n. OK with the change but wondering if this is something we can do on the UI instead. Seems like adding API support is a lot. What metrics do we think this would apply for?\n. ok to test\n. Actually, what if in housekeeping we just set next to now when next is not \"before\" now? Should keep this from happening without needed a configurable jump.\nNice find btw :)\n. Hey @jhjeong-kr I would prefer we make the change. I'm not sure we need to distinguish between the cases unless you strongly disagree.\n. Updating the source SGTM.\n. LGTM, thanks @jhjeong-kr!\n. LGTM\n. LGTM\n. ok to test\n. thanks @Miciah! Could you sign the CLA when you get a chance?\n. Ah I see that. Thanks again! LGTM\n. LGTM, thanks @rjnagal!\n. Do we wanna enable tty and interactive if we're running in the background? +1 to the detach change though!\n. I think we can remove interactive and tty since those default to false\n. Question: How does a user access this data if it is not exported? As in, in ContainerStatsSummary. Its a Go question :)\n. Lets remove Avg since we have percentiles\n. ContainerStatsPercentiles?\n. shouldn't we be able to do this without the floating point ops? Since we're truncating to int anyways.\n. nit: s/%tile/percentile/ :)\n. Shouldn't the handler provide a timestamps for the stats? It seems like we're papering over an issue?\n. Shouldn't we not ignore these errors? Only don't do the update when the sample is nil\n. Lets not do random stats, we should have a static set of stats that we serve. It makes the test deterministic and simpler to read/write. It should also solve your time problem no?\n. thanks!\n. What does this function do? Can we add a comment? Maybe make the arg names more descriptive too :)\n. nit: Lets try to be better about naming vars everywhere. Outside of loop invariants, one or two letter variables are very hard to read. Being more descriptive (while longer) is usually recommended in our styleguides\n. Why not call as: percentiles and bs: values?\n. Why would the sample be nill? Seems like we did something wrong somewhere if that is the case?\n. How about letting Percentiles() return []percentile? Merge this function into the above?\n. nil in the wrong place?\n. wanna use debian like the other to stay consistent?\n. ContainerReference? :)\n. I think you need to rebase?\n. Question: This is accessed as object.ContainerRef?\n. nit: just container\n. nit: s/c/container/\n. rebase with my fixes for these\n. Can we add s go build?\n. We should call this storage rather than db since it more closely matches its role.\n. Why a channel and not just a function call to the storage driver?\n. What about calling this a StorageDriver?\n. Hmm, this seems overly complex and maybe not necessary for all storage drivers (think the in-memory one) WDYT of moving this complexity to the drivers that need/want it?\n. We're still duplicating the data in the storage driver and in the manager's own in-memory list. Should we unify these?\n. Sadly we need libprotobuf8 which doesn't appear to be in wheezy...they only have 7\n. Removed\n. Added a TODO to build from source. My hope is we will lose this dependency soon enough that we don't need to do this anymore.\n. Only issue is it is based on wheezy so we don't have libprotobuf8 (else I would have used it, I copied most of this from yours :) )\n. Removed.\n. It is needed by lmctfy. Maybe if we built it from source we could just use the -dev version.\n. Yep! Removed\n. No, we just use make.\n. Ah good call, done!\n. Removed the chmod line.\n. Maybe not in this PR, but we now have cadvisor/version which has the cAdvisor version. We should display it here too.\n. did this not work?:\nrelease := []byte(uname.Release)\nelse, we can also re-use the i from the loop var:\nfor i, c :=range uname.Release {\n  ...\n}\n. I'm not familiar with that one, it seems like the index is there just for this purpose. But I'm fine either way. @monnand if he knows offhand.\n. SGTM, merging :)\n. nit: Can we add docs documenting these functions?\n. nit: If < 0 lets return all to be consistent with RecentStats()\n. WDYT of naming these Get*()? It seems to matchup with AddStats(), but also seems less Go-like. What do you think?\n. +1 for Go's convention\n. Don't we not need to do the deep copy? I think we've talked about this before :) I don't remember the consensus we reached...\n. Can we just do:\nself.prevStats = &info\n. Check the typecast\n. check cast\n. Another PR, but maybe move the sampling code to storage/memory/sampling?\n. WDYT of having a New() that does the equivalent of FillPercentiles?\n. nit: numRecentStats -> maxNumStats?\n. same here.\n. nit: Do we need to initialize it to a size?\n. We should always have a storage driver. Wanna start enforcing that? i.e.: assume it here and check it in New()\n. nit: For TODOs lets do: TODO(user): comment\n. Documentation for this function?\n. Do we want Close() or Clear()? You seem to be using Clear() in the in memory implementation\n. compact into two lines?\n. This is great for now :) We should also add one for percentiles feel free to do in another PR\n. Either shounds fine. Close seems to be good for our purposed. But we should say in the doc what we expect. If we expect Close() to clear state or leave it implementation-defined (which seems to be what we want to do)\n. It may be nice to break this off into different tests so it is easier to see what is failing during a run.\n. I think we can write a helper that just takes a function to run, Then most of these can be:\nfunc TestPercentilesWithoutSample(t *testing.T) {\n  runStorageTest(test.StorageDriverTestPercentilesWithoutSample, t)\n}\n. Why do they need to be in reverse chronological order?\n. Why are we placing the 100 and 10 restriction? If so we should also document them in the StorageDriver interface\n. What was rthe iddue here?\n. Why switch the order here?\n. for stuff like this I like (with my C++ showing):\nfunc() {\n  lock.Lock()\n  defer lock.Unlock()\n  // Work...\n}()\nfeel free to use or not use :) I know some people worry about defer being 2x slower, but I honestly don't think that affects us here...and I find the readability + lack of deadlock be worth much more than the possible speedup.\n. Sorry, I guess I'm asking why we're making this change :) are we trying to optimize by not re-allocating the slice?\n. typo: something\n. same typo here\n. Doesn't the copy implicitly do a make anyways?\n. We should probably return the stats in chronological order no? WDYT of reversing the slice  in RecentStats()?\n. That's a good point, my concern is what we expose in the REST API. If it should be sorted we should sort it in the manager (which should be able to). I don't think if this PR breaks the UI since it flips the order of the stats :)\n. Lets add a comment as to why we're doing it this way :)\n. Add a comment to the interface please. What it is for and what each function does.\n. How about we replace Atagonism with Interference. So InterferenceDetector and Interference. There can still be an Antagonist and a Victim (or a Source and  Victim)\n. \"The name of the detector used to detect this antagonism. This field should not be empty\"\n. typo: Detector\n. s/string/description of the interference/g\n. Unused?\n. this \"interference\".\n. of \"the\" victims\n. \"Absolute name\" of the \"antagonist\" container.\n. list of \"possible interferences\"\n. Maybe \"TrackContainer\" and \"Tracks\" the behavior of the container?\n. \"antagonist\"\n. which \"interferes\" with a set of \"containers\".\n. Done.\n. nit: s/the pointer/a pointer/\n. nit: \"did not deeply copy the object\"\n. nit: s/works/work/\n. Why not container_name?\n. Why marked optional?\n. nit: influxdbHost?\n. Curious: Why do we do this?\n. Ah, good point.\n. nit: can we make these names const somewhere? Since we're using them more than once.\n. Check typecast and then verify value?\n. case: strings.HasPrefix(...)?\n. nit: const define it please\n. type check?\n. nit: remove empty line\n. same about case\n. I'm not familiar with influxdb, but is this what they'd expect? a point per stat, or they'd rather have them all in one point?\n. nit: add units: millicores same below\n. nit: point[2:2+len(cpuUsagePercentiles)]? Same with memory?\n. why commented out?\n. nit: <=?\n. nit: <=?\n. nit: 10 * time.Millisecond?\n. All this boilerplate makes me miss gMock :) I imagine some of this can be done through introspection (I'm not suggesting we do it though, just wondering whether the library could do it for its users)\n. Verify stats are what we expect?\n. Any way that we can assert that the subcontainers are now tracked?\n. That should be good for now. Maybe have an external API we can query eventually if we think that makes sense.\n. Ah yes.\n. We should do that here :) As in, check the subcontainers here.\n. can we move all this to a storage.New()?\n. nit: storage_driver?\n. table_name?\n. WDYT of \"influxdb.name\" and the same elsewhere? I guess we can re-use if we just go with raw \"db\".\n. I think both of these are percentiles\n. nit: FillDefaults()\n. nit: add docs :)\n. just return fmt.Errorf() of what you Fprintf'd now\n. This will not fail on empty bodies right?\n. nit: type in Received here and below\n. typo: precisio \n. s/ContainerInfoQuery/ContainerInfoRequest/\nand\nContainerInfoRequest specifies what ContainerInfo to query.\n. Also add docs for these :)\n. nit: s/db/storage_driver/\n. s/NewStorage/NewStorageDriver/\n. Move these three to storagedriver.go\n. I think the standard is _, so lets use _ throughout?\n. description: storage driver to use. Options are: memory (default) and influxdb\n. nit: Update stale comment\n. This is only used by test code. Move there?\n. The UI today expects at max 60 data points so we should make a ContainerInfoRequest with that.\n. These tests are getting unmanageable :( I think it's fine for this PR, but lets re-think what we need to make better tests. Needing to pass a function to a function to a function can't be what we should be doing.\n. Remove this commented out code and we can merge :)\nThanks @monnand! No rush, I think I'll just tag 0.1.2 without this.\n. My lack of shell is showing :)\n. Done\n. Done.\n. What errors were you seeing from the HTTP package to make this into a Go routine?\n. Wait, this one is actually super useful :)\n. Maybe try a typecast?:\nval, ok := libcontainer.NetworkStats.(info.NetworkStats)\nif !ok {\n  // Bad cast\n}\n. Remove the useSystemd param since the only user of this call doesn't need it.\n. Alternatively:\ngo log.Fatal(containerManager.Start())\n. This may not mean not active, it may be using another execdriver\n. WDYT of instead of introducing this \"not active\" state from Docker, we try to attach in CanHandle() and if we can't (because it is not active) we say we can't handle it and let the raw driver handle it? The container still exists, just Docker doesn't know about it.\n. Yeah that's fine, if it's not an easy fix what we do now is fine.\n. Why change this?\n. Yes please, I think most of the code doesn't do this actually.\nI'm personally not a fan, but that doesn't matter much :)\n. I don't think we should introduce the concept of \"not active\" container. Can we remove the changes to manager/... for these? Do they affect the rest of the PR?\n. Nice :) you can remove my TODO\n. Can remove from here if we're not exposing it as an API concept.\n. WDYT:\nelse if !strings.HasPrefix(name, \"/docker/\")\n. nit: remove named return, we don't really use it here anyways\n. This should be a member of dockerFactory. We should refactor Register() to make that possible. Not at all for this PR though :) maybe add a TODO so we don't forget it.\n. Lets add what file was not found for these errors\n. nit: s/solID/solid/\n. Sorry, I meant to add \"return false\". So that we ignore non-Docker containers. Today I think we'd actually ask Docker about \"/victor\"\n. nit: /docker/ else it will match things like: /dockeriscool\n. nit: remove TODO\n. Why not just: fmt.Errorf(\"file %q not found\", configPath)?\n. or:\nfunc fileNotFound(path string) error {\n  return fmt.Errorf(\"file %q not found\", path)\n}\n. aaaah I see now from what you use it below. What about a custom error:\ntype FileNotFoundError struct {\n    filepath    string\n}\nfunc (self *FileNotFoundError) Error() string {\n  return fmt.Sprintf(\"file %q not found\", self.path)\n}\nThen you can check for that error type:\nif _, ok := err.(FileNotFoundError); ok {\n  // Was FileNotFound.\n}\n. Shouldn't we only be listing containers at /docker? Since Docker doesn't know about subcontainers, the Docker containers won't have subcontainers. \n. I think in theory that is true. This makes it display as full path names in the UI. Probably what we'd want to do is display relative names in the UI and not have this problem. I'll revert this part.\n. Missing the network errors chart?\n. Lets comment this out for now while we fix the issue so we don't serve JS errors.\n. Lets add it and comment out the JS code that fills it in.\n. nit: these two shouldn't specify the \"panel-title\" class.\n. What about removing this entire function and letting the defaults be 0? We can have the manager apply defaults if it wants and have the UI specify its own. I'm just weary of having defaults in 3 places.\n. We are querying every second and we request the last 60s worth of stats.\nThis could be optimized so that we keep the data, but we don't do that\ntoday.\nOn Thu, Jul 24, 2014 at 11:14 AM, Vish Kannan notifications@github.com\nwrote:\n\nIn pages/static/containers_js.go:\n\n@@ -90,9 +90,14 @@ function getMachineInfo(callback) {\n// Get the container stats for the specified container.\n function getStats(containerName, callback) {\n-   $.getJSON(\"/api/v1.0/containers\" + containerName, function(data) {\n-       callback(data);\n-   // Request 60s of container history and no samples.\n-   var request = JSON.stringify({\n-       \"num_stats\": 60,\n\nIf we are querying every minute, why do we need 1 minute worth of stats?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/118/files#r15362894.\n. Check if spec.Cpu is nil so we can new() it (cpu news it above).\n. This does not change the behavior from today :) the graphs show 60s. We only need like 2s, but the graphs wouldn't be as interesting.\n. +1 to requesting less data and keeping it in the UI though. I'll file an issue.\n. We should check that len(v) is what we'd expect it to be\n. %d?\n. RunWait?\n. nit: s/CPU/Running/\n. This feels slightly weird, it's like we're giving our internal state to a function to manipulate and use. What about having AddProcess() be a member of ProcessSchedStat? I'd even go for accessors, but since this is Go those may be overkill :)\n. Can we add some documentation here? What does this return? Lets add some docs to Load() too. Do we have any restrictions on the arg to Load()? Can it be \"/\"?\n. NewSchedulerLoadReader()?\n. nit: move to top\n. I can see the need for this interface, but what I don't like is that each step can be asked for the LoadsPerCore() when in reality it should only be available at the end no? What about having AddLine() return (schedDebugReaderState, bool, error) where error says if we're done reading the file.\n. Means we could remove this.\n. Also, more docs throughout :)\n\nmaybe rename AddLine -> ProcessLine to make it clearer?\n. Do we need this?\n. can't we do this in-place for context.loadMap?\n. Don't we already do this above?\n. nit: v 1 for all the ones here :)\n. Actually, nvm this is fine\n. nit: not your fault but typo for \"int\"\n. nit: capital F\n. nit: capital F\n. I would make this just Info, it only happens once per container we start.\n. Maybe info too? This usually signifies a problem\n. Yeah but this is a log not an error :)\n. nit: can we add back go vet?\n. Done, PTAL\nOn Thu, Aug 14, 2014 at 12:37 PM, Vish Kannan notifications@github.com\nwrote:\n\nIn CHANGELOG.md:\n\n@@ -9,6 +9,7 @@\n- Make the UI independent of the storage backend by caching recent stats in memory.\n- Switched to glog.\n- Bugfixes and performance improvements.\n  +- Introduced v1.1 remote API with neq \"subcontainers\" resource.\n\ns/neq/new\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/180/files#r16261081.\n. rename to storage_driver_buffer_duration so that we know it is related to that\n. There is also a nice flag.Duration which can accept human-friendly values\n. We should definitely cap the number of samples to bound memory usage, but we can probably do better than what we do today.\n. I'm worried about having the separate Unlocks() and how people handle the code in the future. WDYT of this flow:\n\n```\nseriesToWrite := []*influxdb.Series{}\nfunc() {\n  self.lock.Lock()\n  defer self.lock.Unlock()\n... do all the work ...\n  if self.readyToFush() {\n    ... more work and set seriesToWrite...\n  }\n}()\nif len(seriesToWrite) > 0 {\n  err := self.client.WriteSeries(seriesToWrite)\n  ... error checking ...\n}\n```\n. nit: HousekeepingTick, no K\n. I don't think we should be doing this. What happens if we have less than 60 samples? We don't want to encode this default in multiple places. We're also going to start going into a model where the UI may request less data and keep it locally.\n. Add a TODO please :) so we don't lose track of it.\n. nit: s/cadvisor/cAdvisor/\n. nit: BigQuery\n. We actually only want the \"unit-label\" class applied to the units. We can either separate the unit printing from the number printing or remove the use of \"unit-label\" completely.\n. It'd be valuable to maintain the \"unlimited\" concept since it is much more human friendly than a huge number.\n. I like printing them separately to keep the style. Although it is more work :)\n. I believe in some kernels they are MaxInt64. In newer kernels I think it was switched to MaxUint64. I think keeping the current value should be fine.\n. Do you think it would be useful to expose a \"num_cores\"?\n. nit: i++ here and elsewhere\n. We never actually use these. For later?\n. Instant over what period? Probably 1s?\n. This is for the instant sample? Maybe label with with \"instant\" somehow?\n. Ah, I see the TODO. nvm :)\n. nit: x instead of v.(string)?\n. nit: \"unknown\" Go errors start with a lowercase\n. nit: errors are lowecase, here and elsewhere below\n. Maybe we can share some of this code between the two functions?\n. nit: empty line before these (gofmt will place them before the code.google.com ones)\n. nit: separate the non-Golang imports into their own group\n. Maybe doc on the return and use of this?\n. The sample stuff is a different type of sample from the other discussion.\nThat is Nan's code to get the historical usage percentiles. It'll probably\ngo away as we move to the other format we spoke of earlier.\nOn Tue, Aug 26, 2014 at 4:57 PM, Rohit Jnagal notifications@github.com\nwrote:\n\nIn storage/bigquery/bigquery.go:\n\n\n// Optional: Network stats.\nif stats.Network != nil {\nrow[colRxBytes] = stats.Network.RxBytes\nrow[colRxErrors] = stats.Network.RxErrors\nrow[colTxBytes] = stats.Network.TxBytes\nrow[colTxErrors] = stats.Network.TxErrors\n}\n  +\nsample, err := info.NewSample(self.prevStats, stats)\nif err != nil || sample == nil {\nreturn\n}\n// TODO(jnagal): Handle per-cpu stats.\n  +\n// Optional: sample duration. Unit: Nanosecond.\nrow[colSampleDuration] = sample.Duration\n\n\nThe couple of fields we have now support the Sample() interface for\nstorage driver. There is some inference stuff that uses it. We'll need to\nclean up these callers first.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/190/files#r16738052.\n. nit: do this in a func() {}() block so we can use defer for the Unlock\n. Is the goal to accept stats from multiple containers or multiple stats per container?\n. +1\n. SGTM @monnand\nOn Aug 29, 2014 1:02 PM, \"monnand\" notifications@github.com wrote:\nIn storage/memory/memory.go:\n\n@@ -153,19 +153,27 @@ type InMemoryStorage struct {\n    maxNumStats         int\n }\n-func (self InMemoryStorage) AddStats(ref info.ContainerReference, stats info.ContainerStats) error {\n+func (self *InMemoryStorage) AddStats(pairs ...storage.ContainerRefStatsPair) error {\n\nI have same question. We could have two choices:\n- AddStats(ref info.ContainerReference, stats ...*info.ContainerStats)\n  error: This introduced least change and compatible with old interface.\n  However, it could not maximize the benefit of batch writes: If there're\n  multiple containers, it has to be called several times.\n- AddStats(pairs ...storage.ContainerRefStatsPair) error: This could\n  let the storage driver to one single write operation for all containers\n  with several stats. However, this requires more changes in the old code.\nSince we need to change the code anyway, I chose the second option.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/202/files#r16917979.\n. maybe just return an error?\n. same here, lets not panic\n. stale?\n. why adding this one?\n. debug remove?\n. remove dead code?\n. debug?\n. error :)\n. here too\n. dead code?\n. promote this to Info I think :)\n. Done.\n. is this a floating point in microseconds or seconds? If microseconds we need to divide by 1E6 no? If seconds, we just need to truncate no?\n. Also, parents on the second one please\n. Sorry, my typo :) I meant that if we could add more parenthesis to the\nsecond argument to time.Unix() e.g.:\n\n(int64(f64sec) % 1E3) * 1E6\neasier to read that way\nAlso, in line 214 we say it is in microseconds, am I misunderstanding or\nare we giving and receiving in different units?\nOn Fri, Sep 5, 2014 at 3:51 PM, monnand notifications@github.com wrote:\n\nIn storage/influxdb/influxdb.go:\n\n@@ -169,8 +149,8 @@ func (self influxdbStorage) valuesToContainerStats(columns []string, values []i\n        v := values[i]\n        switch {\n        case col == colTimestamp:\n-           if str, ok := v.(string); ok {\n-               stats.Timestamp, err = time.Parse(time.RFC3339Nano, str)\n-           if f64sec, ok := v.(float64); ok && stats.Timestamp.IsZero() {\n-               stats.Timestamp = time.Unix(int64(f64sec)/1E3, int64(f64sec)%1E31E6)\n\nIt's in milliseconds. What do you mean by \"parents on the second one\"?\nSorry, too many \"second\" here made me confuse.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/218/files#r17202672.\n. I've removed all the cases that do assume it that I know about. The only ones I think may remain are in the sampling code we will be removing soon.\n. This is a fallback. The global housekeeping used to crawl the tree to find new containers. Since we have inotify we don't need to do it now. We still do it just in case we missed any events (can happen if we get overloaded).\n. Yes :D today we only detect creation, deletion, and move but I can see us registering for modifications and exposing those. Today we get the spec on each request, but it may be nice to be able to find when it changes and show that to the user.\n. Clarified. It should be a full name.\n. Done\n. don't we still need this part?\n. rename to build.sh?\n. Doesn't this produce the same behavior as before? If either thread runs into a fatal error it'd glog.Fatal no?\n. Less changes :) honestly it can go either way and this is less typing.\n. We're trying to move to flat structures so we don't allocate so much memory, can we make this a non-pointer?\n. nit: You can actually use a struct as a key:\n\ntype DiskKey struct {\n  Major\n  Minor\n}\n. While I appreciate this type of change, I do think most of the code has this style so I'd rather keep to it :) I know we prefer it as a team. Especially when it is not followed throughout.\n. These are logs so I think they're okay. Errors were the lowercase ones IIRC.\n. Done\n. Good point, done.\n. Interrupt is SIGINT, SIGTERM we'll have to add the syscall.SIGTERM. Not sure if that works. Will try and send a PR if anything.\n. I thought the style was for errors not logs? I'm not familiar with one for logs?\n. The current style in our codebase is that logs start with uppercase (all glog.* calls do this today) and all error types start with lowercase.\nI'm fine changing that :) currently just keeping with the current style as is today.\n. All logs today start with uppercase :)\n. os.Interrupt is SIGINT. They only guarantee those two in Go (accross OSes).\n. nit: I think these can stay uppercase since they're not errors as in the error type\n. looks like an oversight :) feel free to remove if you'd like.\n. Same here.\n. I think this one is not an error, all non-Docker containers will fail here.\n. nit: the Go standard to for error to start with lowercase\n. I'm not sure if we should return here...We should definitely log it, but I think we can gracefully degrade what we do. For example, if the Docker driver fails to attach we can do most of the work from the raw driver.\n. MachineInfo may not be the place to put this now that I think about it. Everything is mostly static. So here may be the DiskSetups Rohit wants to put. Hmm are we sure we don't want to put this in the ContainerStats? It can be empty for most containers until we get them.\n. No, we still have the state so GetStats fetches network stats. I checked just in case :)\n. libcontainer as in container/libcontainer/helpers.go?\nI think the right place for this is here since it is a Docker concept and not a libcontainer one (as in, Docker prefixes it's containers with that).\n. libcontainer does some sad mangling of container names I'm trying to remove in a PR :( once that goes in, Docker will be the one doing the prefixing.\n. Are we sure we want to do this? If we go this route I believe some older versions of Docker don't allow volume mounting /. Also, we wouldn't need the other volume mounts right?\n. The --volume flag should do this for us no?\n. I also never see us accessing this. Did I miss it?\n. nit: FsStats and s/Fs/Filesystem/g? We do with Memory and Network,\n. No, I'm saying is if we go this route we don't need the other mounts. And we have to check what Docker version started to allow / as a volume because some did not before.\n. Let's remove it, it is another layer the user has to download (although tiny) which adds latency to the start time.\n. Do we know how expensive this will be btw?\n. This is going to read /proc/mounts every second. That accounted for 30% of our CPU usage last time. Can we cache these?\n. nit: HasFilesystem and in the json encoded string\n. s/fs/filesystem/\n. nit: fix indent in this section\n. Cool, just what we need :)\n. should we push capacity with every update? This is akin to pushing the CPU limit (or number of CPUs) per update which we don't to today\n. type: Filesystem\n. nit: In order\n. Test for multiple disks? Seems to be the tricky case.\n. What happens when this is not an AUFS system? We should check if AUFS is used and disable it if it is not (similar to today's detection of non-libcontainer exec driver).\n. This is probably fine for now, but we need to be consistent with memory and CPU. Today this setting to capacity is not done.\n. obligatory question: do we know how expensive this is? :)\n. nit: Why not have the tags match the name? It's probably okay to change this for now since we haven't exposed it widely.\n. WDYT of renaming ContainerDescription to ExternalContainerInfo or ContainerHints. Something that implies this is extra information that comes from outside?\n. nit: container_description\nWe should also make the default non-Docker specific. WDYT of /etc/cadvisor/container_description.json?\n. You want:\nAllHosts []containerDesc json:\"all_hosts,omitempty\"\nSame for the rest, add the json:\"NAME,omitempty\" tag. NAME will be the JSON tag used\n. To keep with the cAdvisor API I think we should use full container name here. e.g.: /docker/\n. nit: NetworkInterface\n. We probably want to handle the common case of the file not existing?\n. nit: networkInterface\n. nit: remove test log line here and below\n. nit: you can do:\ndockerlibcontainer.State{\n  NetworkState: network.NetworkState{\n    VethHost: self.network_interface.VethHost,\n    VethChild: self.network_interface.VethChild,\n    NsPath: \"unknown\"\n  }.\n}\n. Shall we rename the file to container_hints.go and container_hints_test.go\n. should this be container_hints.json?\n. nit: s/All_hosts/AllHosts/\n. Hmmm this is the only part of this patch that makes me uneasy. This really should be the full container name. I'm guessing you are wanting to use the Docker ID? I'm also guessing you're not using systemd? Would it be too much trouble to go with /docker/?\nI don't want us to have to do this matching since it lends itself to incorrect collisions.\n. don't most of these fields not exist in ContainerHints?\n. +1\nWe can do this incrementally, but I think we'll use these container hints in more places. It may deserve its own package under \"container\"\n. Done\n. Oh, good point. This is actually a bug, it should be from the interval. Done.\n. nit: s/FullPath/FullName/\n. When this file does not exist this will return an error. We probably want to catch that and return an empty containerHints\n. We should just return nil, err here\n. I think we can remove this case. If it is malformed we will ignore it.\n. I think it should not return an error (return containerHints{}, nil). All functions that call this check the error first, if that exists they assume the operation did not succeed. I'm guessing that's not what we actually want. The file not existing is not an error, it is expected in all systems that don't provide hints. I'm arguing that this function should handle that and return a nil error.\n. I think he meant to return the error, but @vishh let us know if that was not the case :)\n. nit: use _ instead of - here and below for consistency with other JSON fields.\n. For sets we usually use:\nmap[string]struct{}\n. map[string]struct{}\n. use:\nelse if _, ok := devicesSet[device]; !ok\n. shouldn't we do the deviceSet check before the getVfsStats so we don't even make the syscall in that case?\n. nit: s/cadvisor/cAdvisor/g\n. nit: add err instead of _ as you do in the line above\n. link for the MachineInfo struct as you do below?\n. nit: s/mounts/externalMounts/g\n. Nothing uses this? I think we can remove this and the loop below.\n. nit: s/IO/Io/\nI believe it is the Go convention.\n. nit: s/#/Number/ here and elsewhere\n. we should check that len(stats) >= 14 so we don't panic at runtime\n. nit: path.Join(\"/dev\", words[2])\n. nit: we should only need to do this compilation once. Do it in the global namespace outside this function\nvar partitionRegex = regexp.MustCompile(\"...\")\nalso, shouldn't this in theory be [a-z]+ for those with many disks?\n. nit: s/Did/did/g errors should start with lowercase.\n. nit: Because file %q was not available?\n. Done.\n. Yes, the comment on top explains that the stats are swapped with how the veth is hooked up.\n. Sadly no, since this function is not supposed to check Timestamp.\n. We can't today since we don't pump the spec into here. We can, but I'd do it in another PR. Writing 0's should be fine for now. We do the same with CPU and memory above which have the same condition.\n. Same as above.\n. Done.\n. Done.\n. This is the name of the UI endpoint cAdvisor exports. Under the covers it uses the /docker v1.2 API endpoint which only returns Docker containers. That handled teh systemd/non-systemd split.\n. This is the same as the container page :) this code should all go away as we transfer this logic to JS. I can make it a flag, but this only affects the non-updateable graphs (memory and disk breakdown)\n. nit: rename to listen_ip\n. nit: while we're here :) can we change to %d?\n. Today it does not. If it does we'll have to change our logic.\n. Yes, maybe a bit crude but figured it would be unique :)\n. Done.\n. This is an abstraction only kept by the manager and never exposed in the API or any other components so I think it is okay to keep as is. Alternatively, we can have the manager keep a second map with namespaced aliases.\n. Done\n. Animated GIF anyone?\n. nit: gofmt, the spacing is off\n. What update? The flattening? Since that doesn't affect the wire format, should we mention it?\n. Yeah, I'm confused too. We had fixed it in review I thought :)\n. Done.\n. lol :)\nOn Nov 17, 2014 11:14 AM, \"Vish Kannan\" notifications@github.com wrote:\n\nIn manager/manager.go:\n\n@@ -300,7 +300,13 @@ func (self manager) DockerContainersInfo(containerName string, query info.Cont\n        return nil, err\n    }\n-   return self.containerDataSliceToContainerInfoSlice(containers, query)\n-   // Convert to a slice.\n-   containersSlice := make([]*containerData, 0, len(containers))\n-   for _, cont := range containers {\n-       containersSlice = append(containersSlice, cont)\n-   }\n  +\n-   return self.containerDataSliceToContainerInfoSlice(containersSlice, query)\n\nWow. Thats readability to the max [image: :100:]\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/312/files#r20457076.\n. Looks okay for now. It may make sense for these to be returned rather than passed as pointers though.\n. I remember seeing one where the /docker UI endpoint returned an error if there were no containers. This patch does fix that as well. It shows a page with no subcontainers.\n. nit: add \" and exit\"\n. Only reason is to provide partial stats if they're available (i.e.: only CPU and memory)\n. This is fine for now, but we should try to get a nicer name for the disk. The issue is the space I'm guessing.\n. I'm not familiar with \"|\" what does that do? Seems like an \"if error, then 0\", but not sure what error is possible here.\n. Nit; These are technically: TiB, GiB, MiB, etc...\n. nit: rename to \"humanize\" or something along those lines since we don't really return a string :)\n. What are these two files for?\n. I wonder if we could make the return and func arg interface{} to keep from duplicating. Not sure if the result would be less readable though. WDYT?\n. nit: pages digest handler\n. You can register this handler twice if both files were specified. Is only one file allowed? This is something we should check.\n. nit: pages basic handler\n. nit: run gofmt on this file\n. Flag comment is stale.\n. Documenting their format may be better. How about removing them and adding a docs/auth.md? Can be in another PR :)\n. nit: add an info log saying \"Using auth file %q\" same for the digest one please.\n. nit: lowercase basic\n. link to test.htpasswd:\n\n[test.htpasswd](../test.htpasswd)\n. Same here\n. nit: lowercase basic\n. We technically have code that today checks for 0.11+. I'm fine bumping that up to 1.0.\n. I don't think we require memory (since Debian has it off by default). We can technically do without cpu too, but we need some cgroups :)\n. nit: We do this in two places today, we should combine them eventually.\n. Good point. We do have the concept of any of the stats being absent. We'll need to make sure we maintain that.\n. Ah no nvm, it was /proc/self/cgroup :)\n. You should be able to drop this import and since they're in the same package refer to them as StatsBuffer withouth the memory prefix.\n. useSystemd = util.FileExists(mnt + \"/system.slice\")\n. nit: Extra empty line?\n. No, this only updated the subcontainers reported as part of container info (container detection is done in global housekeeping).\n. nit: Add a comment here so we know what it is used for. We haven't been good about it (as you can see in the rest of the struct) but we've started to be better.\n. not important for this PR, but curios: Could we also get the path that it is mounted at?\n. WDYT of having these return strings instead of []byte? Usually simpler to handle.\n. Should this return the int?\n. nit: realSysFs since I don't think we want this exported.\n. nit: lowercase \"could\"\n. consider using: strings.TrimSpace\n. WDYT of having a struct for Major and Minor. In Go we can use that as a string for a map.\n. Yeah...this is like the third such thing we have sadly. Added the TODO.\n. nit: lowercase Could and use %q so we can more easily catch empty contents (here and below)\n. WDYT of using hertz anyway? In the case bellow we'd just convert it to hertz.\n. Cache memory? Maybe add in the comment to make explicit.\n. Nit: use %q\n. Return uint64?\n. Not: errors we usually format with %v. Here and elsewhere.\n. Aaah got you\n. I'll just close this PR, seems minor and not necessary.\n. Done.\n. what about making it a method on the object (uint64Slice)?\n. WDYT:\nfunc NewMean(size uint64) {\n  // .. create ...\n}\ntype Mean struct {\n  size uint64\n  mean uint64\n}\nfunc (self *Mean) Add(value uint64) {\n  // Do what you did here\n}\n. Can we add a comment saying what output is what?\n. And this is memory usage and total CPU usage\n. Ah, it's working set :)\n. Do we want to have a separate netlink in util? It seems like we're starting to make an interface for it. I'm fine leaving here for now and making all private.\n. Can just be New() since it is the only thing exposed by the cpuload package :)\n. glog? or was this for testing? Here and elsewhere.\n. Maybe add the json info for these?\n. I personally like these consts like this, but Go says that they should be camel-case and start with lowercase (since they're private)\n. I'm guessing all these structs are package-private? Should be lowercase if that's the case.\n. TODO to handle big-endian? We may not need to worry for a while, but just have some way to catch it if it happens :P\n. Do we mean for this to be public?\n. \"path is an absolute filesystem path for a container under the CPU cgroup hierarchy\"?\n. nit: errors start with lowercase. Here and below.\n. WDYT of just returning all of LoadStats?\n. nit: can we use a map instead :)\n. Ah, good point.\n. OomKillInfo?\n. pid can be an int\n. We use absolute path container names (so it can be a string):\n/top-level-container\n/top/with-a-subcontainer\n/top/child/even-deeper-subcontainer\n. nit: Let's use // comments :)\n. Consider using regexp for this. You should be able to identify the line and parse the value at once\n. nit: Errors start with lowercase, so:\nerrors.New(\"failed to parse...\")\n. Lets get the main in a different file (with package main) and this library in this filw (with package oomparser). That way we can use the library and compile a tester.\n. and oomparser probably :)\n. fmt.Sprintf() instead for returning a string\n. Call it String() and return a string and lots of things will be able to print it :)\n. Why not error like we do for other parts of the stats?\n. nit: Warning? :)\n. SGTM, TODO?\n. nit: I/O scheduler?\n. Yeah, just the comment :) thanks for putting up with my nits! Merging.\n. I think we can remove this file.\n. nit: compact the empty lines?\n. extra?\n. Yes, but that is more work for something relatively simple I think :) No reason we can't change it over time though.\n. My hope is to eventually run them both ways: outside and inside a container. This only runs them outside for now.\n. Good point. We can start with this and move to the API quickly (remote or local).\n. Hehe I told you they were mostly my fault :D\nAdded.\n. We need to add the Copyright banner to this and the other Go file :)\n. nit: empty line before this one. We separate the Go standard imports from the non-standard ones with an empty line.\n. What about just reading and printing the OOMs:\nfor oomInstance := range outStream {\n  // print oomInstance\n}\n. process ID of the killed process?\n. nit: we can probably remove this and rely on %v\n. GetAllOoms()?\n. lets pull out the compiled regexes to global vars. That way we only have to compile them once when we start up.\n. nit: populateOomInformation() since it is a private method\n. nit: we need to run gofmt over this file and the other Go file :)\n. nit %v for errors, here and below.\n. Can we document these fields please :)\n. quitChan instead?\n. WDYT: having this return an \"error\" and doing the logging in housekeeping where it is called?\n. nit: Now it's a parser of Oom instances :)\n. nit: remove this line and the one below (maybe just remove the file for now).\n. nit: remove this and the String() as we can do it with %v\n. lets remove the slice of all OOM instances and let the caller keep them if they want.\n. nit: lowercase \"Neither\" (errors are lowercase)\n. Also, we can use %s and the consts we have defined\n. Oh, one final thing: make this: path.Join(\"/\", parsedLine[1]) so that it is an absolute container name.\n. use fmt.Errorf() which should let you\n. Only concern I have here is that all of the above stats are instantaneous except this one. We should at least document over what time period or how this is smoothed. Idk if we should export smoothing over different intervals. As long as we do this one over a small enough one we should be okay.\n. Milliload? :)\n. I think the name change and the comment sounds good. Thanks for pointing out that we also export the instantaneous one. SGTM :)\n. lets check that this happens after the first call and is untouched in the second call (we'll have to re-set currentOomInstance)\n. Do these without the else. Then we can get multiple errors. Same below.\n. nit: %s here and below\n. nit:\nOomInstance{\n  Pid: 19667,\n  ProcessName: \"evilprogram5\",\n  TimeOfDeath: correctTime,\n}\n. Lets add a test that uses the official API. Should be similar to the AnalyzeLines above.\n. Lets add some bogus (real looking, but unrelated) lines before and after the OOM.\n. nit: Maybe have a helper that creates the expected OomInstance. We can use it in both of these tests and some of the ones above.\n. nit: remove dead code.\n. Maybe for both of these take the testing.T as an argument and do the t.Error() here. Will make the uses in tests simple since we don't have to check for errors there.\n. Lets separate this tests into two so it is easier to see what is failing:\n- TestAnalyzeLinesSystemOom\n- TestAnalyzeLinesContainerOom\n. nit: couldn't\n. nit: Add a doc as to why we're doing this\n. We already get this information at the start of cAdvisor. Worth it to re-use it here?\n. nit: not public?\n. not public?\n. not public? :)\n. Actually, make this one a t.Fataf since we can't continue with it. Here and below.\n. can we remove the oominfo binary?\n. The quantity stuff sounds pretty good, but I'm unsure about the generic map. We've always preferred strong over weak typing and having the fields spelled out here is stronger and easier to read (than specifying what string values are valid in the map)\n. I buy that, we don't have an \"is present\" in JSON. +1\n(well theres the omitifempty but that doesn't work anywhere close so what we'd like it to work like)\n. I don't think we need to go to floats. Those always lead to trouble as\ndifferent machines/languages treat them in different ways.\nOn Mon, Feb 2, 2015 at 9:47 AM, Vish Kannan notifications@github.com\nwrote:\n\nIn info/container.go\nhttps://github.com/google/cadvisor/pull/481#discussion_r23942396:\n\n+type Percentiles struct {\n-   // Average over the collected sample.\n-   Mean uint64 json:\"mean\"\n-   // Max seen over the collected sample.\n-   Max uint64 json:\"max\"\n-   // 90th percentile over the collected sample.\n-   Ninety uint64 json:\"ninety\"\n  +}\n  +\n  +type Usage struct {\n-   // Indicates amount of data available [0-100].\n-   // If we have data for half a day, we'll still process DayUsage,\n-   // but set PercentComplete to 50.\n-   PercentComplete int32 json:\"percent_complete\"\n-   // Mean, Max, and 90p cpu rate value in milliCpus/seconds. Converted to milliCpus to avoid floats.\n-   CpuUsage Percentiles json:\"cpu_usage\"\n\nWe need something like 'NaN'. If we use float we can do that.\n\u2026 <#14b4b67ea2df0c6f_>\nOn Mon, Feb 2, 2015 at 9:45 AM, Victor Marmol notifications@github.com\nwrote: In info/container.go <\nhttps://github.com/google/cadvisor/pull/481#discussion_r23942180>: >\n+type Percentiles struct { > + // Average over the collected sample. > +\nMean uint64 json:\"mean\" > + // Max seen over the collected sample. > +\nMax uint64 json:\"max\" > + // 90th percentile over the collected sample. >\n- Ninety uint64 json:\"ninety\" > +} > + > +type Usage struct { > + //\n  Indicates amount of data available [0-100]. > + // If we have data for half\n  a day, we'll still process DayUsage, > + // but set PercentComplete to 50.\n\n\nPercentComplete int32 json:\"percent_complete\" > + // Mean, Max, and\n  90p cpu rate value in milliCpus/seconds. Converted to milliCpus to avoid\n  floats. > + CpuUsage Percentiles json:\"cpu_usage\" I buy that, we don't\n  have an \"is present\" in JSON. +1 (well theres the omitifempty but that\n  doesn't work anywhere close so what we'd like it to work like) \u2014 Reply to\n  this email directly or view it on GitHub <\n  https://github.com/google/cadvisor/pull/481/files#r23942180>.\n\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/481/files#r23942396.\n. Capital Valid? WDYT: Present instead?\n. nit: Add: If true, the values below do not have data.\n. nit: use fmt.Sprintf(\"%s %s\", reList[1], stringYear)\n. nit: just handler.go/handler_test.go since we're already in the events directory\n. All these functions should return an error (and GetEvents should return a []Event too)\n. nit: this can be lowercase as we can make it private. With a New() function we can return an EventManager (of which this will be a type)\n. nit: Type and Data\n\nWe can also cann EventCategory EventType\n. nit: lets add some docs on some of these and their interactions :)\n. We can probably omiy this one\n. nit: just NewEventManager()\n. We only need to make the map as long as we assume that missing => false\n. How about: startTime.After(eventTime) and endTime.Before(eventTime)?\n. We can probably treat it as: return up to N events\n. We shouldn't be monitoring for OOMs ourselves. Let another component do that and feed them to you with AddEvent() :)\n. Similar to monitor below?\n. No, we take it from when the cgroup hierarchies were created.\n. I think it would. Those two are also created around the same time as the container would (I don't think they're created ahead of time).\n. No, it would be uninitialized (zero) since we don't know when it was created.\n. Hmmm I was seeing this more like there is one \"Events\" in the system and everyone uses that instead of there being more than one. I also viewed the internal as private to it. What else did you have in mind?\n. Missing s.index++?\n. nit: not important now, but consider using the assert package we started using in the integration tests. Quite useful and makes the tests pretty readable :)\n. Nice tests :D\n. nit: can also do: t1.Sub(t2).UnixNano()\n. nit: can do this with Sub() and time.Duration\nif t1.Sub(t1) < 10 * time.Nanosecond {\n}\n. nit: can move this outside and remove the else :)\n. We should also not assume we get 1 sample per second. With dynamic housekeeping it won't always be 60.\n. We spoke of doing the decay from seconds -> minutes -> hours -> day. How come we decided to just keep all the minutes instead?\n. ah ah yes, sorry my bad :)\n. It's probably fine to keep it stats-specific until we have another user. We save from all the type casting :)\n. Don't worry about putting implementation details here. As they change we'd need to update here which we will forget to do for sure :) Only documenting the public interface should be fine.\n. we can remove the zero inits, that's done by default by the Go runtime\n. WDYT of having an EventType be a slice instead of a map? Then we won't need a New for it.\n. Ah good point, added.\n. This prints the instantaneous usage and then we show the average over the last 60s.\nIt is useful since it gives us an idea of the current usage (and how stable that is) as well as what it means in the \"medium\" (60s) term overall.\n. I think I did what you meant :) PTAL\n. This actually prints the usage every second. So it will print the usage over the last second as well as the average usage over the last 60s.\nMax is probably okay, 90%ile is probably overkill. I don't think we need these for now, so I'll push back unless you feel strongly :)\n. Done.\n. I agree that 4 is the most likely way to go, but I also realized that you refer to WatchEvents() as a way to take events in, it should be a way to get events out. GetEvents() lets you query historical data, while WatchEvents() lets you wait for new events that will occur. The only way to create events are through AddEvent().\nThe manager.Manager will have an eventManager as a member of its struct. On startup the manager.Manager will watchForOoms() and new/deleted containers. When either of those are encountered, it calls eventManager.AddEvent(). I tried adding more detail to the EventManager interface you have to see if that make it clearer.\n``` Go\ntype EventManager interface {\n    // Report the occurrence of an event. This event is not available to GetEvents() \n    // requests. If there are any outstanding WatchEvents() looking for this type of event,\n    // this new event will be delivered to them.\n    AddtEvent(event Event) error\n// Allows the called to wait for events to occur. The caller with use the outChannel to \n// receive any new events of the specified types.\nWatchEvents(outChannel chan Event, types map[EventCategory]struct{}) error\n\n// Allows the caller to fetch historical events. The request argument allows the called to \n// filter the events returned.\nGetEvents(request Request) []Event\n\n}\n```\n. We can abstract away the passToWatch and remove it from WatchEvents() by storing them locally. When someone registers a watch we add it to a slice of chan *Event. We can then look through those when we add an event. Would make it simpler for the caller so they don't have to remember the channel\n. nit: EventType?\n. nit: Timestamp\n. ?\n. nit: two ways to monitor, one is to add :)\n. What do you think of saying you either:\n- Specify a time range\n- Specify last N events\n  It's not clear how both interact, did he mean give him the last N events in that range or the last N events that have occurred?\n. I was suggesting EventType since the documentation calls it that :)\n. This one is unused, we can remove.\n. The StartTime and EndTime parameters may not make sense for a watch. We should document that they must be empty for a watch and check that here.\n. Unfortunately we may not be able to make this a map since we may have more than one user with the same request. Imagine two calls to Watch() that only want OOM events, they should be handled as separate things. Maybe a slice with a request and a channel?\n. Remember that we only want new events for watch, we shouldn't need to send the old ones. That means that we shouldn't need to start any go routines here since we can use the outChannel as the channel that is in self.watchers\n. nit: Do we need to sort? We sort with getMaxEventsReturned\n. this should probably take the outChannel and reuse that.\n. nit: Fatal instead of Error\n. nit: can do:\nGo\nreturn NewEventManager(), NewRequest()\n. use subcontainer instead of Directory?\n. nit: !checkIfIsSubcontainer(...) instead of != true.\nHere and below\n. use t.Fatal. That stops the test. Then below you can assume that we only have one element and thus not have the if statement\n. nit: path.Join(netDir, f.Name())\n. Lets document these fields a bit more.\n. Lets document these fields a bit more.\n. We probably need to append an \"/\" to request.ContainerName, else this can slip by:\nrequest.ContainerName = \"/container\"\nevent.ContainerName = \"/container-that-makes-you-think-is-a-subcontainer\"\n. We can just return, the user will do the actual listening (and I think this will listen and send on the same channel no? Since newWatch() takes outChannel)\n. Add a test for the case we mentioned above.\n. Use the normal Add API here :)\n. Add a test that uses a time range with Start and End\n. Looks like we do a lot of this in common, any way to make a helper for it?\n. Can we simplify this test by registering the watcher and then adding events?\n. nit: Absolute container name, we don't use directory to refer to them usually.\n. Remove use of \"directory\" here and in the one below\n. We can simplify this by not using an oomparser.OomInstance. Since events doesn't care what's inside we can take ContainerName and Timestamp as arguments, make the EventType TypeOom and make EventData int(0)\n. Since we don't check anything, can we just sleep the 5s and then error?\n. Much nicer looking test thank you :)\n. Can we rename? The name is slightly misleading.\n. This is the style of the code above (maybe we should change all of it though).\n. I think v2.0 shouldn't be based on v1.X, it'll probably be separate completely.\nMaybe we keep this for now since it is not complete though. Although having an incomplete WIP API will probably discourage others from using it until it is ready :)\n. nit: return writeResult() \n. unrelated and not for this PR: we do this everywhere, we should get a helper...\n. Only if the size is 0. It does get handled it seems (although by no design of mine :P). When size == 0, start = -1 and end = 0, which means numResults = 0.\nOpen to suggestions on making this behavior more clear :)\n. Rename to: TestWatchEvents since it doesn't add events\n. just sleep and then fail?\n. If the user doesn't report an end time it will set endIndex to 0 too, but good idea on just returning early if we're empty. Makes it much clearer. Added that.\n. nit: rename to eventsLock?\n. We should probably not send with the lock (it may take a long time depending on the user) and can lead to deadlocks. Lets create a slice of the channels we're gonna send on (with the lock) and then send to those channels (after dropping the lock).\n. Rename this :)\n. Lets make this a buffered channel, we can start with 10? See how that goes :)\n. Lets add some extra info: \"Error streaming OOMs: %v\"\n. Actually, can we move this outside of the Go routine? We can just pass the channel to the Go routine.\n. lets make this a V(1) log:\nGo\nglog.V(1).Infof(...)\n. here we can return the error\n. We can just return the result here\n. Glog and error instead. We probably want this to run forever\n. Shouldn't this just be / uint64(elapsed) The difference in CPU is in nanoseconds and we'll divide by the interval (in ns) to get the number of cores used in that interval.\n. cpuRate is unsigned, we shouldn't need this check.\n. I realize we had the code before, so I'm probably just missing something :)\n. I'm okay adding this, do you think it is confusing that we expose the average, 90, max even though they are all the same?\nWould it be okay to say to users, if you want instantaneous data go to the /stats endpoint? WDYT?\n. Aaaaaah that's right :D can we add a comment so I don't bother you a third time? (I think I had the same issue last review lol)\n. I can see what you mean. The single value approach would make it clearer that we actually don't have Max,Mean,90p data that low. I'm +1 to doing that if you prefer.\n. nit: make v2_0 depend on v1_3\n. nit:\nGo\nswitch requestType {\ncase eventsApi:\n...\n}\n. Make it buffered?\n. No need to make this a separate Go routine, one is spun up per HTTP request so it is okay to block it.\n. we can return the error\n. Maybe add some comments on what format you're expecting from the user\n. nit: errors start with lowercase\n. you can do r.URL.Query()\n. isn't the return a url.Values?\n. Ah I see, why don't we do the parsing per field? Since once we get the key we know what type we expect.\n. These are usually in lowercase with _ so:\n- subcontainers=true\n- oom_events=true\n- creation_events=true\n- deletion_events=true\n- start_time=TIME\n- end_time=TIME\n. Lets not initially include the \"all events\".\n. The times we should take as RFC3339 which is what we parse normally in Go too (it is the same format that Go prints to naturally).\n. What about we check each of these instead of checking in a loop:\nGo\nif val, ok := urlMap[\"all_time\"]; ok {\n  getEventsFromAllTime = strconv.ParseBool(val[0])\n}\n. The URL is an object built into the HTTPRequest and saves you from having to do some of the parsing yourself\n. You also shouldn't need to do most of the checking you're doing above. The r is the HTTPRequest object btw\n. can we use \"historical\" instead of \"all_time\"?\n. Lets put this in the tests file since we only use it there.\n. consider using \"github.com/stretchr/testify/assert\" This is used by the integration tests and it makes a lot of thing much easier :)\ne.g.:\nGo\nassert.False(t, shouldGetAllEvents)\nassert.NotNil(t, err)\n. We can probably remove this test, dupe of the above.\n. Looks like this should be moved outside the switch.\n. remove log line since we are returning the error\n. We don't want to do this here since we will create an even each time we restart cAdvisor which is not what we want. We should do this only in detectSubcontainers() or watchForNewContainers(). In detectSubcontainers() we should also only do it if the container creation time was before the cAdvisor start time. Lets start with just watchForNewContainers() and move to detectSubcontainers() in a followup PR.\n. nit: time.Duration and don't use .Nanoseconds() below\n. Should we worry if this is >> 60s, for example if it is 120s we should in theory keep the stats over the 60s. I'm guessing if we add one stat at a time this is not a problem though. Is that what we expect?\n. can we move this logic to analyzeLines instead?\n. This code is getting a bit hard to read, let's try to simplify it. We will never break out of this loop so lets make it an infinite loop with a read at the beginning. We should also only wait while the error is EOF:\n``` Go\nvar line string\nvar err error\nfor true {\n    // Wait for more content to show up in the file.\n    for line, err = ioreader.ReadString('\\n'); err != nil && err == io.EOF; {\n        time.Sleep(100 * time.Millisecond)\n    }\n// Do work...\n\n}\n```\n. We probably don't want to return our internal container data structure here :) How about we return ContainerSpec instead since that is a user-facing structure.\n. We also shouldn't use cont here. What data do you think would be useful here? Spec would be nice, but that's gone by now. Maybe the ContainerReference? That's just the full names of the container. Maybe just blank and leave as is?\n. Ah good point. We should return that then.\n. Actually, can we move this to New?\n. lets move this inside the if since we only use it in that case.\n. Ah right we need the creation time haha, nvm :)\n. This will fail since the container doesn't exist anymore. Lets leave it blank for now. We can consider caching the last spec we got and returning that.\n. \"tasks_count\" broken down by type sounds good.\nAlthough I agree it is a slippery slope. network_bytes broken down by tx, rx. Memory broken down by usage, working set, etc. Those can stay as is :)\n. nit: maybe metric_errors_total? Not to confuse with some other kind of error.\n. It's more useful to expose CPU usage total not broken down since it is significantly more accurate than broken down. We can either have type=total,user,system or have a separate total and breakdown metric. I'm guessing the first is better? If we go that route we can rename to cpu_usage_seconds\n. These are also in nanoseconds\n. nit: s/is/are/\n. nit: s/sectors/sector/\n. nit: s/sectors/sector/\n. \"...while receiving\"\n. \"Cumulative count of packets dropped\"\n. \"...while transmitting\"\n. This is probably ignorance on my part :) but what does Reset() do here?\n. I think its fine to not use image. We never really expose it in cAdvisor.\n. Nice compact collector :)\n. Yeah I removed my other comment and not this one...ignore :)\n. These are cgroup stats. We get total and per-cpu usage with nanosecond precision. System and user breakdown has jiffy precision which is typically ~10ms.\n. Can we add units?\n. Maybe add a TODO to make this a set of sorts? WDYT?\n. SGTM, can we add a TODO to write a small library to do this? :) We already have it, we just need to export it here.\n. Don't we not need \"container\" in the name of all of these due to the namespace?\n. cpu_usage_type_seconds there are 3 labels: total, user, system no?\n. nit: rename to \"streamResults\"\n. lets not use an http error here and below since the API code takes care of it\n. nit: lowercase could\n. nit: lowercase could\n. Change to: \"Error encoding message %+v for result stream: %v\", ev, err\n. lets remove this log\n. not blocking this PR, but we need to detect this root dir better if we're going to start relying on it. Today we assume it is always /var/lib/docker, but users do change it.\n. TODO to make these public and part of the API so people can use them\ncan we also append \"Label\" to them?\n. nit: can document the field? It's a map from label to mount point of the filesystem that has that label?\n. nit: can we document this method and the one below?\n. nit: units\n. Lets remove this (or move it to V(3))\n. Remove?\n. maybe rename to: readLinesFromFile?\n. how about just linefragment += line since if it is empty nothing gets added :)\n. Lets log the error and not exit, we should always stay reading here\n. Remove?\n. Should be an Infof()\n. We can also look at the Docker config file. Customization is usually done there.\n. \"container_per_cpu_usage_seconds_total\"?\n. nit: s/id/name/\n. nit: select :)\n. I'm +1 if you want to remove this behavior :) \"/\" is only valid with recursive == true. Should be fine since it is in the 2.0 API anyways\n. What I mean is\n/stats/?type=docker&recursive=true\nGives you all containers\n/stats/?type=docker\nFails with: no Docker container specified.\nIt should still work with custom cgroups since it'd be like today in systemd systems.\n. lets to strings.TrimPrefix(name, \"/\") so that we don't ignore if the users gives us: /this/is/bad/\n. +1 to /version just since there could be multiple cAdvisors. I'm okay putting some software stuff here too.\n. I like machine more :) but that's just me being picky\n. We need to keep on waiting for data here otherwise we'll deadlock (if no one is reading while the writer wants to write)\n. We can remove channels here as we find that they are stopped.\n. I'd rather us not have to have this is we can. To remove the channels in a timely manner we may need to have the event manager spin up a thread that listens to all stops and does the removal.\nSo we'd start a cleanup thread that listens on a cleanup channel, when someone calls Stop() it sends a request to the cleanup thread to remove that channel. The thread wakes up and removed it.\n. Shouldn't we just call Stop() on the eventChannel?\n. Go uses camelcase: \"lastId\"\n. missing description :)\n. We can replace this with a Stop() on the EventChannel() that way we don't have to go all the way to the manager to call StopWatch().\n. We don't call this anywhere yet btw.\n. you want a write lock, so .Lock(), you probably want to lock over all this too, so we use a pattern like this:\n``` Go\nfunc() {\n  self.watcherLock.Lock()\n  defer self.watcherLock.Unlock()\n...work here\n}()\n```\n. ID generation looks good.\n. containerData wasn't meant to be publicly exposed. If we do this route we should make it safe to do so (doesn't have to be in this PR). I'm just worried about its use in Kubelet :)\nWe can also replace most of the stuff here with a call to this and then getting some data.\n. I'm +1 on exposing a Container object that we can GetSpec() and GetStats() from.\n. Honestly, I would even remove this list and just keep a pointer to the MachineInfo struct. We don't keep a list for other API types and the list is manually updated...\n. I agree, I mean that we don't list all the fields here (they don't in the Docker API either). Long term we should move to go-restfull or swagger which auto-generate these :)\n. This should also be inside the lock below\n. I think this can be a read lock.\n. s/VersionInfo/Attributes/\n. link to v2 file for cadvisor/info/v2/Attributes?\n. Looks like data is never used\n. Hmm wonder why Godep did that...added back.\n. Thanks, forgot about those :)\n. this won't be true in CoreOS machines for example. WDYT of getting the name from a Client().ContainerInfo() and checking that?\n. Ah good point. Let's just do a strings.Contains() of the Docker ID then :)\n. I think we need to do this in container/docker/handler.go. This function is also used by the raw handler so we'll get flipped stats for host.\n. Np, done.\n. Done\n. Done\n. Done\n. I think this one is fine, we check against subcontainer below it. We check:\n- /root subcontainerOf /root == true\n- /root/sub subcontainerOf /root == true\n- /root-different subcontainerOf /root == false\n. stale comment?\n. Yes, the TODO is to make it configurable.\n. (since we'd still like to do the per-event limit before we expose it as a flag)\n. Done.\n. Done.\n. I was more thinking that it'd never be nil although there may be no collectors. I want to re-implement stats as a collector so there should be at least one collector.\n. WDYT?\n. It is not, will remove.\n. Ah thank you. Removed\n. let's marshal inside the if so that we only do it when we are going to write.\n. It's a uint64 so unfortunately I can't check. I check whether the deduction would make it negative beforehand and just set it to zero. That should work I think?\n. nit: it's only one do we need the loop?\n. nit: RSS and VirtualSize\n. nit: PercentCPU\n. Do we need this one? Why not ListProcesses()?\n. Should we parse these values and get a more concrete API? Happy if we add a TODO :)\n. Should be fine for now then :)\n. aaah I didn't realize it was a map.\n. Also add to SupportedRequestTypes(). Realized we can make this interface better and less manual :P\n. Wanna just change the one above to B and use it for both? I don't thinkwe mind :)\n. nit: s/IsRoot/isRoot/\n. I thought about it, but didn't want to take the dependency and encourage other things to be placed in make (instead of the simple Go flow of today)\n. If our flow becomes more complex (like Heapster's build), then I can't complain about moving to make :)\n. We may still have to register the inotify since it could be a different cgroup hierarchy. We register an inotify handle for each cgroup hierarchy but only count the container as registered once.\n. We use it as a set today so it can be. Is there a particular use in mind that you have that'd require it?\n. Yes\n. For correctness we probably should, in practice it hasn't affected us. We do this today so I'll fix in another PR is that's okay.\n. Ah, true. Will change.\n. I'd say the same thing here that @rjnagal suggested. Let's just check all of these and take the first one that provides a UUID\n. If it returns nil, all the operations we use here are handled (get, delete, and len). Went ahead and made it explicit though to make it clearer.\n. nit:\n$(\"#process-top\").text(\"No processes found\")\n. no associated spec?\n. WDYT of renaming this to Cache? It may be confusing to have two storage drivers.\n. I think we can remove the pid map now?\n. nit: s/looks/look/\n. the ps was run in the cAdvisor container, not the container we're examining right now no?\n. Sadly I don't think this will work since cAdvisor may not be running inside namespaces (like in Kubernetes). We should check the mount/pid namespace of us vs init?\n. We should probably make /rootfsa flag eventually, fine for now/\n. I'm okay with that, but won't it block part of the graph if so?\n. I wonder whether we can get this from the libcontainer config?\n. We should check the error like we do above.\n. Nice!\nAnother possible option is to check if the libcontainer config has an empty Network. This would save us the Docker inspect call in every GetSpec() which would be nice. If that's possible, let's do that :)\n. We can also check the type on the Network config.\n. nit: Go style is camelcase, this is fine for the above, but for the others it should be Baremetal and UnknownProvider\n. nit: Same here\n. nit: omitempty that way it is not included when it is not used. Same for CloudProvider:\n`json:\"instance_type,omitempty\"`\n. Although I guess we have Unknown which will make this never happen :P ignore...\n. nit: Just NewCloudInfo()\n. I recommend using: https://godoc.org/google.golang.org/cloud/compute/metadata\nIt's pretty simple code, but useful to use a library by the authors/maintainers :)\n. Why comment out?\n. I'd recommend having this in collector/nginx so we can do: nginx.NewCollector()\n. Rename to NginxCollector and leave the other as CollectorInterface\n. I'd call this GenericCollector or GeneralCollector and leave the interface as Collector :)\n. It looks like we can remove this one as you suggested.\n. You probably don't need to keep track of the last error, it will just be returned by the collector.\n. This should probably be public too.\n. Units are more like: bytes, seconds, nanoseconds per second, etc.\n. We should represent this as a time.Duration or an integer and say it is in seconds.\n. Moved canary -> beta for builds we make manually. The automated build will be the canary. The thought is that users can try a fixed version without us having to make a release. We can probably replace a lot of the \"try vmarmol/cadvisor\" with this.\n. rename file to generic_collector.go?\n. Move this to a config.go. This part is general, this collector is one of possibly many implementations.\n. nit: we tend to use_naming_with_underscores for JSON fields insteadOfCamelcase. So metrics_config and similarly elsewhere.\n. nit: remove \"in seconds\" since it is a duration\n. reword?\n. Maybe add a failure to parse test?\n. You should be able to use a raw string for this:\nhttp://play.golang.org/p/9xLuph6MEQ\nMakes it more readable :)\n. nit: add comments for these\n. These should not be in the reference but rather in the ContainerSpec above (and Metadata should be merged with labels I would think)\n. There are a few things we do per collection that we should be able to do once at initialization:\n- get minPollingFrequency\n- Compile regexps\n. nit: minPollingFrequency?\n. We need to close the body:\ndefer response.Body.Close()\n. Do we need the extra slice copy?\n. We should check the length of the match too\n. Can use: strings.TrimSpace() here and below\n. We shouldn't need to do this, it's the default\n. Maybe simpler:\nmetrics[ind].FloatPoints = []v2.FloatPoint{\n  {\n    Value: regval,\n    Timestamp: currentTime,\n  }\n}\n. We should also check the return of the conversion. Here and below\n. We should probably keep a slice of errors that we then compile together. We try to get all the metrics we can and only return errors at the end. We can use:\nhttps://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/util/errors/errors.go#L31\n. What you have today is correct I think. We'll be closer to collecting every specified interval.\n. +1\n. We should check that there is at least one (we have the TODO above, but we should at least check that so we don't crash)\n. +1 to checking the error\n. Can we space it out a bit to make it easier to read? :)\nGo\nreturn &GenericCollector{\n    name: collectorName,\n    configFile: configInJSON,\n    info: &collectorInfo{\n        minPollingFrequency: minPollFrequency,\n        regexps: regexprs\n    },\n}\n. My thought (and I think Rohit's too), is that here we're able to tell the user that their config is wrong and allow them to fix it.\nYou bring up a good point about trying to fail with reduced functionality (but in that case, we should at least return the error). Only concern is that if some metrics show up and some don't the user would think that something was broken in cAdvisor (or that the metric is empty), whereas a clear yes no to the metric would possibly make them think the config was wrong.\nWe can go either way :) but I think this makes it clear that we should be doing very good reporting of errors on metric collection so users know what's happening. We should export some stats and possibly a page.\n. And it looks like we were on the same page @rjnagal :) lol\n. nit: add a comment and whitespace\n. This would stop getting all regular stats if we don't have any custom stats. Any reason we're integration collection here instead of the callsite above? (the one we commented out)\n. We haven't run these tests in a while. I've been meaning to remove them.\n. In theory it should be the value of minHousekeeping flag :)\n. Are these supposed to be io.cadvisor.metric.<what goes here?>\n. Remove the glog's and merge it into the error. We already log the error in the calling function\n. We should eventually make the /rootfs a flag, we're relying on it in many places. For another PR :)\n. Thanks! I'm guessing we derive no meaning from the name, just an identifier?\n. SGTM\n. The labels should go with the values?\n. We spoke a bit offline, but labels allow us to identify different parameters of the same metric. For example, if I am exporting disk size as the \"disk_size\" metric and I have 2 disk: sda1 and sda2 I can export a label \"disk_id=sda1\" to specify these metrics are for that disk\n. What does this flag do?\n. Is this of the local machine?\n. Looks like we never set a default for this one?\n. WDYT if we count the non-loopback devices? If in the future this changes we don't have to worry about it.\n. ",
    "JasonSwindle": "I think I signed it correct, and got the reply:\ncode.google.com signatories\nThank you.  Your CLA submission will be processed shortly.\n. Thanks for the fast followup and merge!\nOn Tue, Jun 10, 2014 at 4:10 PM, Victor Marmol notifications@github.com\nwrote:\n\nThanks for the cleanup! :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/2#issuecomment-45672394.\n. Very true, let me fix that. :)\n. \n",
    "monnand": "Docker handler could deal with the root container.\nIf there's lmctfy installed on the machine, cadvisor will use it to list the root container information. Otherwise, there must be something to handle root container information, or there will be nil pointer exception when we try to list subcontainers under the root containers.\nCurrently, we are using docker's container handler to do this work. This is why we register docker handler factory under \"/\". Your change may lead to nil pointer panic when we try to access the root container's information.\nA better approach may be implementing a container handler and its corresponding factory specific to deal with root container, so that when there's no lmctfy, we could register this handler to list subcontainers for root and return the machine's information, (i.e. the root container's information.)\n. I'm going to close this PR and let's move our discussion to #6 \n. PTAL. Addressed all except  statssum.go,  line 69.\n. Please take another look.\nAbout timestamp: I added a member in statsSummaryContainerHandlerWrapper which indicates if we should set the time stamp. Since it is not exported, users are not able to change it and it will set times tamp by default. It will only be changed in unit test. We will check this member in GetStats(). Since it will always be the same value in the actual binary, branch predictor may save us on performance.\n. I am going to close this issue because there's no information here. Please feel free to re-open it if you do have anything to say.\n. Probably image names is more helpful?\nAs far as I know, container ids are generated automatically by docker (is there any way to specify a container id in docker run?) This id is not very friendly even if we can use its prefix.\nPeople could recognize their containers by seeing image names, e.g. ubuntu, google/cadvisor, etc.\nHowever, since it is possible to run several containers with same base image, image names will not uniquely identify a running container. Probably we could provide image information about a container on the page?\n. I see. I'm working on it.\n. I think we can close this issue now :).\n. ping @vmarmol. This PR will not solve all problem. But it could be use as a starting point.\n. PTAL.\n. LGTM. ping @rjnagal \n. LGTM\n. ping @vishh \n. PTAL.\n. Will send a PR once you are done on #37 \n. #58 is the last PR to add storage drivers into cAdvisor. Once we got this merged, I will work on other storage backends like influxdb and BigQuery (which is mentioned in #43)\n. @erikh Thanks! You could take a look at the storage package and implement the StorageDriver interface. I probably will add a Close() error method to the interface, which seems useful.\n. @jchauncey Yes. I think that would be better. The only reason I didn't make it configurable is because we only have one storage driver now. Once we add more drivers, we should let users to choose which one they want to use.\n. Working on this issue today. But failed to use the influxdb's Go library. Posted a message on their mailing list. https://groups.google.com/forum/#!topic/influxdb/FDU6ELiICGo\n. LGTM.\n. ping @vmarmol. Travis CI works now. The only problem is that we cannot use race detector. \n. LGTM. ping @proppy\n. LGTM. Thanks!\n. @vmarmol Yes. I agree. manager package could have a global view of the system and is suitable to collect stats because it is able to control some global parameters like output rate, etc.\nI am working on another PR to solve this problem.\n. I think we could discuss both #51 and this one. I don't mind to work on another PR which probably combines #51 and #49 together with some new stuff.\n. LGTM.\n. ping @vmarmol \n. @vmarmol Just simplified the structure. You're right, some logic should be put into the storage driver's implementation. PTAL.\n@jchauncey Thanks you! I'm looking at this package now. Probably we could implement a storage driver using go-metrics.\n. Removed the StorageFactory interface. PTAL.\n. @vmarmol Done. Waiting for the CI.\n. LGTM.\n. Don't merge it. Something wrong with the StorageDriver interface.\n. It works now.\n. PTAL.\n. Rebased\n. LGTM\n. Just realized I set a wrong remote on my git repo and pushed all branches under the google/cadvisor's repo. Let me first merge this PR and change the code in another PR.\n. Just did a grep on the source code. Here is a list of dependencies:\n\"github.com/docker/libcontainer\"\n\"github.com/docker/libcontainer/cgroups\"\n\"github.com/docker/libcontainer/cgroups/fs\"\n\"github.com/fsouza/go-dockerclient\"\n\"github.com/kr/pretty\"\n\"github.com/stretchr/testify/mock\"\n\"code.google.com/p/goprotobuf/proto\"\n. godep is a widely used tool to do this work. I can take a look at it once I finished the kubelet/cAdvisor integration.\n. I just checked godep. It is a good tool to do vendoring. According to a recent comment on HN, it seems most people choose this tool.\nThe basic idea of godep is to put all dependencies into a directory and organize that directory as a GOPATH. When you compile your code, you need to wrap any go command with godep, e.g. go build becomes godep go build. Then godep will automatically change the GOPATH for go command.\nPros:\n- Easy to manage, you only need to run godep save ./... and godep will automatically find all dependencies and put their code into the directory. Updating dependencies is also easy.\n- No need to change import path. Because godep changed GOPATH when it runs the go command, so the import paths do not need to be changed.\n- It could automatically find the dependencies in parent directories. This means we could run godep go test under storage/memory directory even if all dependencies are under a directory in the project's root directory. (This part makes it better than the scripts used in kubernetes. Because I could test my package individually.)\n- It's a go program which means it is statically linked and can be copied anywhere.\nCons:\n- Every go command has to be wrapped with godep.\nBesides godep, there are also lots of other dependency management tools available for Go.\n. Done in #62\n. PTAL. I will let the memory driver use these tests in another PR.\n. PTAL.\n. Fixed. PTAL.\n. I made the following changes:\n- Do not check the order of the returned recent stats. As long as they are the most recent ones, that would be fine.\n- Removed any code which requires a minimum storage space.\n- The return value of Percentile() should guarantee that higher percentage never have smaller value.\nPTAL.\n. Just found another dead lock bug and fixed it in this PR as well (along with unit tests.)\nAdd some other unit test. The coverage for memory package now is 93.6%.\nPTAL.\n. Fixed typo. PTAL.\n. Thanks, @vmarmol . I will merge it once it passed the CI.\n. Done. PTAL.\n. Thanks, @vmarmol !\n. Let me rebuild it on travis first.\n. LGTM. Merged.\n. LGTM. Merged.\n. LMGT. Merging. Thank you, @vmarmol \n. LMGT. Merging. Thank you, @vmarmol \n. PTAL. I also removed the cadvisor binary in the root directory.\n. PTAL. I also removed the cadvisor binary in the root directory.\n. Passed CI. PTAL.\n. Passed CI. PTAL.\n. Add two unit tests for CPU hotplugging. PTAL.\n. Add two unit tests for CPU hotplugging. PTAL.\n. PTAL.\n. PTAL.\n. Changed several bugs. PTAL.\n. Changed several bugs. PTAL.\n. @rjnagal PTAL.\n. @rjnagal PTAL.\n. Don't merge it. I'm working on the Percentiles() function ant it's almost done.\n. Don't merge it. I'm working on the Percentiles() function ant it's almost done.\n. Added Percentiles(). But don't merge it. Lots of comments are not addressed.\n. Added Percentiles(). But don't merge it. Lots of comments are not addressed.\n. @vmarmol PTAL.\n. @vmarmol PTAL.\n. Since I have no other thing to do today, I made the following changes:\n- Changed the .travis.yml:\n  - Added the installation commands for influxdb\n  - Removed the test for go1.2 because of a bug in its race detector, which is mentioned in GoogleCloudPlatform/kubernetes#348\n  - Turned on the race detector by default.\n- Changed the storage/test package, to allow some inaccuracy in the time representation in the backend storage implementation. Currently, we allow 10ms (0.01s) differences at most.\n- Now we could pass CI.\n. Since I have no other thing to do today, I made the following changes:\n- Changed the .travis.yml:\n  - Added the installation commands for influxdb\n  - Removed the test for go1.2 because of a bug in its race detector, which is mentioned in GoogleCloudPlatform/kubernetes#348\n  - Turned on the race detector by default.\n- Changed the storage/test package, to allow some inaccuracy in the time representation in the backend storage implementation. Currently, we allow 10ms (0.01s) differences at most.\n- Now we could pass CI.\n. @vmarmol All comments addressed. PTAL.\n. @vmarmol All comments addressed. PTAL.\n. Thanks for your suggestion, @zfjagann !\nFor the first one, yes, we probably need to expose more information about memory. We are currently discussing which metric is useful for users. In fact, if you use our json API, you could get the working_set of a container, which is the total usage minus pages in inactive LRUs.\nAbout the negative value of page fault: I think it is because of the UI library we used. It smooths the line to make it \"nicer\". However, apparently, we do not need such feature in this case. \nWhat do you mean by cache misses? CPU cache misses?\n. Thanks for your suggestion, @zfjagann !\nFor the first one, yes, we probably need to expose more information about memory. We are currently discussing which metric is useful for users. In fact, if you use our json API, you could get the working_set of a container, which is the total usage minus pages in inactive LRUs.\nAbout the negative value of page fault: I think it is because of the UI library we used. It smooths the line to make it \"nicer\". However, apparently, we do not need such feature in this case. \nWhat do you mean by cache misses? CPU cache misses?\n. @vmarmol PTAL.\n. @vmarmol PTAL.\n. Oops, forgot to change .travis.yml. Will send another PR.\n. Oops, forgot to change .travis.yml. Will send another PR.\n. LGTM.\n. LGTM.\n. All comments addressed. PTAL.\n. All comments addressed. PTAL.\n. PTAL.\n. PTAL.\n. PTAL.\n. PTAL.\n. @vmarmol Sorry, There's a conference for interns yesterday and today.\n. @vmarmol Sorry, There's a conference for interns yesterday and today.\n. @vmarmol All comments addressed except the createManagerAndAddContainers() one. PTAL\n. @vmarmol All comments addressed except the createManagerAndAddContainers() one. PTAL\n. @vmarmol Done. Thank you!\n. @vmarmol Done. Thank you!\n. PTAL.\n. PTAL.\n. Thank you, @vmarmol Just changed the flags. Will merge once CI passed\n. Thank you, @vmarmol Just changed the flags. Will merge once CI passed\n. Thank you, @vmarmol \n. Thank you, @vmarmol \n. Do we want to wait #80 ?\n. Do we want to wait #80 ?\n. Let's merge this PR first. We can always change the binary on GCS if necessary.\n. Let's merge this PR first. We can always change the binary on GCS if necessary.\n. LGTM.\n. LGTM.\n. LGTM. Except one thing I am not quite sure.\n. LGTM. Except one thing I am not quite sure.\n. Just tested it. It works. I don't know why I did not choose filepath.Join().\nMerge it.\n. LGTM.\n. influxdb's Go library is moved to a new path. This really is a bad practice. Do you mind to change its import path to github.com/influxdb/influxdb/client? (Otherwise we could not pass CI.)\n. Merged. Thank you, @vmarmol.\nThe influxdb issue is really bad. Their new client package is not go-get-able. Just filed an issue on their project.\n. Thanks for pointing this. As @vmarmol said, we have filed an issue: influxdb/influxdb#756. The reason it failed to build is because of the .proto file which is not compiled into .go and go get cannot compile it automatically. I'm not sure if influxdb people will fix this bug (if they consider it as a bug.)\n. I think this is fixed.\n. LGTM. Thanks @vmarmol \n. @vmarmol Ah. That's fine. It's just some scripting work.\nThe problem in GoogleCloudPlatform/kubernetes#491 is that we have three functions in kubelet:\n- GetContainerInfo(): This will return a container's info, same as in cAdvisor. So the return type of this method is info.ContainerInfo.\n- GetMachineInfo(): This will return the root container's info. So it returns an info.ContainerInfo as well.\n- GetMachineSpec(): This method returns the machine specification (number of cores, memory capacity). But the return type is info.MachineInfo, which may be confused with GetMachineInfo().\nAny suggestion to solve this problem? Or you think it's fine?\n. That would be fine I think. Thank you @vmarmol.\nClose this issue and will work on  GoogleCloudPlatform/kubernetes#491.\n. LGTM. Merging.\n. LGTM, except a minor point.\n. I think you forgot to add github.com/google/cadvisor/utils :)\n. ping @vmarmol \n. LGTM.\n. Not sure what's happened on Travis side. I'll see what's going on on travis's repo.\n. LGTM. Merging.\n. LGTM.\n. As long as the raw driver works, I'm fine with this change. @rjnagal ?\n. I think it's safe to merge this PR.\n. This PR passed CI now. Let me restart travis and see if it could pass consistently. \n. OK. It failed again.\n. Good news: We may be able to use these unit tests now.\n. It works now. ping @vmarmol, @vishh, @rjnagal \n. #104 is merged. Merging this PR.\n. Thank you, @vmarmol #104 is merged now.\n. LGTM. Merging.\n. Small change. LGTM. Thanks @vmarmol \n. Merged. Thank you, @vishh \n. @vmarmol PTAL. There's also a minor change on a member name.\n. LGTM. Merging.\n. @vmarmol PTAL.\n. LGTM. Merging\n. Oops. I forgot this PR. Sorry about that.\nThank you, @vmarmol!\n. LGTM.\n. LGTM.\n. LGTM.\n. @vmarmol PTAL.\n. LGTM\n. Talked with @vishh on Friday. It seems that we do not need to use SplitName() to calculate nest levels. I tried and it worked. Let me confirm this later today.\n. Thank you, @kelseyhightower ! I was considering to do this. I also discussed about vendoring in GoogleCloudPlatform/kubernetes#402. However, GoogleCloudPlatform/kubernetes#402 is postponed to wait @jbeda back and did not come to a consensus yet. (I think @jbeda may be back this week. If so, we may have a solution by the end of this week.)\nI am leaning toward rewriting import paths which seems idiomatic to Go (i.e. godep save -r). WDYT?\n. My comment was folded:\nYou mean let ProcessLine() to return a bool indicating if the SchedulerLoadReader is ready?\nIt's a bit hard to decide if it is EOF simply by analyzing the content of the file. How about making ProcessLine() return (schedDebugReaderState, SchedulerLoadReader, error), where SchedulerLoadReader is nil if it is not ready?\n. @vmarmol PTAL.\n. @vmarmol PTAL.\n. LGTM. Merging.\n. LGTM.\n. LGTM. Merging.\n. LGTM. Merging.\n. LGTM. Merging.\n. I found the problem of this bug when I was working on #202: In influxdb, the instant cpu usage is calculated based on the prev stats and current stats. However, I just stored one prev stats within the influxdb storage driver. This means all containers are going to use this prev stats as their prev stats, which will definitely cause many problem. This unit test passed because it does not do any concurrent write.\n. If batch writes could help, then we may want to change StorageDriver interface's WriteSeries() method from AddStats(ref info.ContainerReference, stats *info.ContainerStats) error to AddStats(ref info.ContainerReference, stats ...*info.ContainerStats) error. In this case, the old code would still work.\n. I just checked the code. The problem is with the time field which is automatically added by influxdb and may be used by other third party programs. Since we changed to batch mode, the time field is no longer the precise time that the stats generated. We may want to change the time (and seq. number?) of each point maunally. See this\n. I was reading the code and found some potential bugs: In AddStats(), we buffer the stats and do a batch write if some condition is satisfied. However, in RecentStats(), Samples() and other methods, we ignore the stats cached in our buffer not flushed into the storage. In this case, the return values of RecentStats(), Samples() and Percentiles() are not most recent stats. There will be a delay (roughly 1min using our current default configuration.), i.e. the returned value from  RecentStats(), Samples() and Percentiles() is based on the stats 1min ago. The reason that @cboggs did not observe such delay is because of another layer of cache implemented in storage/cache. \nBesides, Close() method does not flush data into the storage, which may lead to some problem.\nI'm surprised that the unit test passed siliently. We may want to add a Flush() method to the StorageDriver interface so that we could control when to flush the data back to the storage in our unit tests. We also need to fix RecentStats() and Samples(), i.e. consider the cached stats before executing the actual query. For Percentiles(), that would be tricky: One solution is to first write all cached stats to the storage and then call percentiles. This will lead to low performance and we need to inform users in advance.\n. I'm considering to move batch writes into the storage/cache package. In that case, we do not need to have any cache-related code in the actual storage driver implementation and we could apply such batch writes operation easily to other storage drivers. WDYT? ping @vmarmol @rjnagal @vishh \n. @vishh I put the 'timestamp' field because the 'time' field is a float64 value and may be not accurate. It seems that influxdb uses float64 for all number fields.\n. Just sent PR #200 to add batch writes in the cache so that we do not need to rewrite this logic in other storage drivers.\n. It's the influxdb again. Restart travis and see if it could be fixed automatically.\n. @vmarmol We did not fix it. Travis worked automatically. I think you can use //+build ignore to ignore all unit tests for influxdb.\n. @vishh Currently, I set BufferDuration to be zero, which disabled the buffer in InfluxDB. I will remove the buffer in another PR. WDYT?\n. @vishh I just replied all your comments. PTAL. (Dinner time here, I will be back later.)\n. Hold on this PR. I'll do two PRs today:\n- Change AddStats() to accept multiple points.\n- Merge storage/cache and storage/memory into one package.\n. @vmarmol @vishh Just sent #202, still working on it but you can take a look first.\n. LGTM. Merging.\n. Oops. @vishh, Just realized this PR broke the influxdb's unit test because it cannot precisely recover the time stamps of stats. See this line, and here. I will add timestamp back as an additional column because we may need sub-second time in the future.\n. @vmarmol I think influxdb supports sub-second time values. However, it seems that influxdb uses float64 to store time, which may have precision problem by itself. (I'm not sure about this. I'll check influxdb's code later.)\n. @vmarmol, @vishh, I just tested how influxdb represents time. The time field returned from the client library is in type of float64, not an integer. This may cause some precision problem.\n. @vishh Currently, the influxdb package could not pass unit test at HEAD, any idea?\n. Let me first work on a fix of the influxdb driver. There are several bugs in the driver. One of them caused #164, which may cause lots of other problems. Let me send a PR to remove the cache layer in influxdb and fix the bugs there. Then I will rebase this PR onto it.\n. @vishh I see.\n\nFor the storage driver interface, I would argue that cAdvisor should not be retrieving stats from an external DB at all.\n\nI have similar thought. I think cAdvisor should keep most recent stats and do any analysis within this window. Anything outside this window should be done as a batch job. This includes things like Percentiles(), Samples(), etc. (Do we need Samples() under this context?)\n\nLets define a new external storage interface which will support only a Write method. This external storage driver can then be optionally injected into the cache.\n\nThen the write could be even async. i.e. let a pool of goroutines do the write operation to the backend storage.\nWell. Then that would be a huge change. Most changes are deleting existing code in storage drivers. If we decided to do that, then I think I do not need to fix influxdb. What do others think? @vmarmol @rjnagal ?\n. OK. I will work on it this afternoon. We will have lots of code to delete now :)\n. I'm not quite familiar with heapster. Still trying to read the code and figure out where it fits into the big picture. Is it tightly coupled with kubernetes? My concern is that if we introduce too much concepts from kubernetes into cAdvisor, users may not be able to run cAdvisor alone.\n. @vishh Does this issue related to the suggestion in #202, i.e. doing all analysis work in memory and making the back-end storage only write-able to cAdvisor.\n. LGTM. Merging.\n. @vmarmol Done. PTAL.\n. @vmarmol PTAL.\n. Since its members are still public, users could access their members. However, percentile itself is private, so users cannot create an object of percentile. In this case, to get percentiles of samples, the only way for users is to call FillPercentiles(). Users will not be able to change CpuUsagePercentiles in ContainerStatsSummary because they cannot create a percentile object.\nHere is an example of iterating all members in CpuUsagePercentiles:\nfor _, p := range s.CpuUsagePercentiles {\n                fmt.Printf(\"%v: %v\\n\", p.Percentage, p.Value)\n        }\n. This part is for unit test. In TestSampleCpuUsage(), I set the timestamp so that we could check the duration of each sample. Any other way to make the test work?\n. No, it does not solve the time problem.\nHere is what I want to test in TestSampleCpuUsage():\nGiven N+1 observations, there should be N samples and each of them should have duration of one second. Using real time will not guarantee the sample duration.\nOne possible solution is to add another container handler decorator which only set the stats' time. This makes the code more clean because statsSummaryContainerHandlerWrapper will only do stats. (Right now, it also sets timestamp.) The downside of this approach is that adding another layer for container handler may affect performance.\n. Yes. Or, if you want to access Name or Aliases, you can use object.Name object.Aliases directly.\n. @rjnagal is right. []int8 cannot directly convert to []byte, you have to manually change it. However, is there any reason why we need to declare i outside the loop?\n. Yes. Get_() is less idiomatic to Go. However, our code base now does have some functions with name Get_(). I think a good thing to do here is to stick with one convention and change other code. Which one would you prefer? I personally think it may be better to following Go's suggestions.\n. The reason why I did a deep copy here is that I don't want the StorageDriver to take the ownership of the stats. To the caller, StorageDriver is something like a file/database. So AddStats should follow the semantic of Write() method. Whatever sends to Write(), the caller should be free to change it after calling the Write() function and it should not affect the following write operations. If it takes the ownership of stats, it seems like Write() function takes the ownership of the buffer passed into it.\nBut I'm open to change it as long as we add some comment to the AddStats() method of StorageDriver\n. We don't have to. But initializing with a size would pre-allocate the memory space and makes the following appending operations faster. Here is a result of a micro-benchmark I just wrote:\n$ go test -bench=.\nPASS\nBenchmarkMapInitWithSize      500000          6505 ns/op\nBenchmarkMapInitWithoutSize   200000         11192 ns/op\nok      github.com/google/cadvisor/storage/memory   5.694s\n. This seems better. However, influxdb now does not support sampling. But I think we could just return most recent samples for influxdb right now.\n. Yes. Probably we could add a Clear() method?\nIn other implementations, e.g. a mysql driver, Close() means close the connection between mysqld and the client; while Clear() means delete all items in the storage.\nFor in-memory storage, I think they are the same.\n. Because of the unit test. In the unit test, we will check the order.\nTo be honest, I'm not sure if we should enforce this requirement to all drivers. I think I could change the unit test and remove this restriction.\n. I'm not quite sure at this point now. If fact, the driver does not need to hold more than 100 samples, as long as it could return some percentiles, that would be fine.\nThe Percentiles() function is actually hard to test: The driver may have their own definition of \"percentiles\", e.g. they have some weighted algorithm which weighs recent stats higher. In some case, the following unit test may fail with some good reason.\nI'm thinking about to remove StorageDriverTestPercentiles, or just test the order of the returned value, i.e. if the 90th %tile is bigger than 80th %tile. What do you think?\n. I think I was trying to fix a bug. The previous version will return the earliest stats when calling RecentStats(). The bug could be reproduced with TestRetrievePartialRecentStats.\n. I don't mind the slow down either. Will change it.\n. Sorry, what do you mean?\n. Done.\n. Yes. Since the number of cores will not change over time, so one memory allocation should be enough.\n. For the in memory storage, yes, we could. For other storage driver, I'm not sure if we should pose this restriction. (Probably, we could provide a function in the storage package to sort stats?)\n. I changed the code in memory package so that it will return stats in chronological order. But I did not check the order in the unit test. As long as they are most recent stats, they will be fine.\n. registeredRoot?\n. I did not expect it. Do they support hot unplugging? I think I can fix this easily.\n. I think we can checked these conditions before calling this method. Just changed the code.\n. The first stats will not have these fields, because it cannot calculate the instant CPU usage.\n. Done.\n. Done\n. Done\n. Done\n. I think it's one point per stat. Otherwise, it would not be able to draw graphs.\n. Well. Just want to have a default value which user could pass 0 and expect some default behavior.\n. Fxied.\n. Oops. Because I wanted to see the data manually when I debug the code. Will remove it.\n. Done\n. Done\n. Exactly. I tried to write a mock library for go and it turned out that testify/mock is the best we can get through Go's reflect package. There's no macro in Go so there's no way to automatically generate those boilerplate code (unless we write a preprocessor for Go.)\n. I think the stats will be checked in line 129. If the argument to AddStats() is different from stats, then it will report error.\n. Maybe we could check the containerData's member?\n. Oh, I just noticed that we've checked the subcontainers in TestContainerGetInfo() through its public method GetInfo(). Do you think it's enough?\n. Done.\n. I did not add this because there may be multiple tables for other databases. So currently, I just hard coded the table name. Any better way to handle this case?\n. What is our convention of flag names? dot separated? Or underscore separated? I think we have both?\n. No, it will not fail. Empty body will lead to io.EOF error, which is handled.\n. I cannot remember why I did not use this method. I thought I tried it but it did not work in this situation. (Maybe it's just a wrong memory somehow in my mind.) Are you sure this works for cAdvisor running inside docker?\n. map[string]struct{} itself is a reference type. So you don't need to pass into a pointer. See here\n. Done\n. You mean let ProcessLine() to return a bool indicating if the SchedulerLoadReader is ready?\nIt's a bit hard to decide if it is EOF simply by analyzing the content of the file. How about making ProcessLine() return (schedDebugReaderState, SchedulerLoadReader, error), where SchedulerLoadReader is nil if it is not ready?\n. Since we are iterating over context.loadMap, I'm not sure if it is safe the change the map in the loop body. Let me check the language specification.\n. Do we query any percentiles from UI now? IIRC, we did not call Percentiles() from UI.\n. Ah, Yes. Let me change it.\n. I see. I think UI does not specify any percentiles by default, i.e. query.CpuUsagePercentiles and query.MemoryUsagePercentiles are all empty slices. If so, then we could check if MemoryUsagePercentiles and CpuUsagePercentiles are empty at  manager/manager.go:141 and only call Percentiles() when any of them is not empty. WDYT?\n. I'm considering to add a method to the StorageDriver interface. How about this: Add a private method called flushUnsafe() which flush data without locking; and export a Flush() method which is thread safe.\n. I've exported a parameter named bufferDuration so that the caller should specify this duration. Changed storagedriver.go in the main package accordingly.\n. I put this logic into the cache. PTAL.\n. Done.\n. Just added a TODO.\n. Done.\n. OK. Rename Flush() to flush() so that we do not export this method.\n. This cache works in Per-container level. i.e. for each container, it maintains maxNumStatsInCache stats and maxNumSamplesInCache samples. whereas dirtyStats maintains stats within bufferDuration. But yes, it is actually duplicated.\n. Renamed Flush() to flush(). It is not exported now.\n. See above.\n. :+1: Do you want me to change it in this PR?\n. @vishh I like the idea of making the cache the default entity. In that case, we will merge storage/memory into storage/cache and any other storage driver will be a backend of the cache.\n. Don't know why I removed that. Will add it back\n. I have same question. We could have two choices:\n- AddStats(ref info.ContainerReference, stats ...*info.ContainerStats) error: This introduced least change and compatible with old interface. However, it could not maximize the benefit of batch writes: If there're multiple containers, it has to be called several times.\n- AddStats(pairs ...storage.ContainerRefStatsPair) error: This could let the storage driver to one single write operation for all containers with several stats. However, this requires more changes in the old code.\nSince we need to change the code anyway, I chose the second option.\n. @vishh What do you mean by splitting the API? Do you mean we need to add AddSingleStats(ref info.ContainerReference, stats *info.ContainerStats) error into the memory storage? I think this function could be implemented outside the StorageDriver interface as AddSingleStats(driver StorageDriver, ref info.ContainerReference, stats *info.ContainerStats) error.\n. Sure. I'll change it. Since this function will be removed in the next PR. So it doesn't matter either it returns an error or panics.\n. @vishh Yes. You can see that I tried to print the time out but still cannot recover the right time with correct precision.\nIt may be caused by my influxdb instance, which only returns one point when I call \"select * from t ...\". Still trying to find what's wrong with it.\n. Yes. Once this PR merged, cAdvisor will never read from a backend storage. Currently, RecentStats() only used for testing purpose. I think I will move it into some test-only package later, make it like StatEq() .\n. Removed\n. Removed\n. Removed\n. Removed\n. Uncommented\n. You mean concurrent writes will lead to error in influxdb?\n. Done\n. Let me write the time in microsecond precision when we call WriteSeriesWithTimePrecision().\n. It's in milliseconds. What do you mean by \"parents on the second one\"? Sorry, too many \"second\" here made me confuse.\n. Ah, Sure. Just did it.\nAbout line 214: In line 64, the time is stored in microseconds. It is the finest granularity allowed by influxdb (or its client API?) The time retrieved is, however, in millisecond.\n. ",
    "kyurtsever": "@vmarmol - we actually always register Docker handler under /docker regardless of whether we register it for root container. In such case at the moment docker handle pretty much only reports docker subcontainer. I think @monnand is right that we should implement root specific handler as at the machine level we have some additional information sources which are not available at container level.\n. Perhaps lmctfy should have configurable job tracking hierarchy and we\nshould use /systemd hierarchy on systemd machines? Using device or cpu\nhierarchy to detect seems somewhat arbitrary?\nOn Tue, Jun 17, 2014 at 2:39 PM, Victor Marmol notifications@github.com\nwrote:\n\nDoes this affect /, /docker, /docker/, or all?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/59#issuecomment-46369156.\n. Changing libcontainer makes sense but at the moment lmctfy gives us more\ndetailed statistics so there is benefit for handling this in lmctfy.\n\nOn Tue, Jun 17, 2014 at 2:45 PM, Vish Kannan notifications@github.com\nwrote:\n\nWhy can't we change libcontainer as Rohit suggested and not change lmctfy?\nOn Tue, Jun 17, 2014 at 2:42 PM, Victor Marmol notifications@github.com\nwrote:\n\nThat may be tricky given freezer support and systemd not using freezer.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/59#issuecomment-46369502.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/59#issuecomment-46369823.\n. \n",
    "crosbymichael": "I'm guessing this is the same issue:\nbash\n2014/06/12 05:23:58 Failed to update stats for container \"/docker-9252dcd2d2357982d833f0b8c2c1499de2f66c3f1d21b062da79d0da183483fa\": unable to run command lmctfy stats full /docker-9252dcd2d2357982d833f0b8c2c1499de2f66c3f1d21b062da79d0da183483fa: exit status 13\n2014/06/12 05:23:59 Failed to update stats for container \"/docker-9252dcd2d2357982d833f0b8c2c1499de2f66c3f1d21b062da79d0da183483fa\": unable to run command lmctfy stats full /docker-9252dcd2d2357982d833f0b8c2c1499de2f66c3f1d21b062da79d0da183483fa: exit status 13\n2014/06/12 05:23:59 Api - Container(/)\n2014/06/12 05:23:59 Get(/)\n2014/06/12 05:23:59 Request took 17.040263ms\n. Sry, this one was my bad. It was running on a system with systemd.\n. You also need docker 1.0.0 because most of the cgroup stats work was merged in for that release\n. Yes, the memory gauge and and usage breakdown are not respecting the host's memory.\n. Ya, just tag the same image id with google/cadvisor:latest and push that\n. I think RHEL 6 is running a custom build of docker 0.11.0-dev \nMichael Crosby\n\nOn Aug 29, 2014, at 12:36 PM, Victor Marmol notifications@github.com wrote:\nAlso weird that my RHEL system does have them under /var/lib/docker\n\u2014\nReply to this email directly or view it on GitHub.\n. maybe you could have a flag on cadvisor for a state dir? \n. \n",
    "serpinuser1": "Not really sure.got hacked on all equipment and can't get functions back to factory\n. ",
    "defender": "Thanks a lot !!!\n\nOn 12 \u05d1\u05d9\u05d5\u05e0 2014, at 09:30, Victor Marmol notifications@github.com wrote:\nThanks for filling the issue! Yeah we need to add support for names, it \nlooks hard to read otherwise. Will track with this issue. \nOn Jun 11, 2014 11:27 PM, \"defender\" notifications@github.com wrote: \n\nHi \nContainer name is not presented, in main page only GUID \nroot docker \n3ba08c450497df5facf35139a02cd1fd3038a3d424d6972531098ed531c9f644 \nThanks. \n\u2014 \nReply to this email directly or view it on GitHub \nhttps://github.com/google/cadvisor/issues/9. \n\u2014\nReply to this email directly or view it on GitHub.\n. Thanks a lot !!!\n\nOn 12 \u05d1\u05d9\u05d5\u05e0 2014, at 09:30, Victor Marmol notifications@github.com wrote:\nThanks for filling the issue! Yeah we need to add support for names, it \nlooks hard to read otherwise. Will track with this issue. \nOn Jun 11, 2014 11:27 PM, \"defender\" notifications@github.com wrote: \n\nHi \nContainer name is not presented, in main page only GUID \nroot docker \n3ba08c450497df5facf35139a02cd1fd3038a3d424d6972531098ed531c9f644 \nThanks. \n\u2014 \nReply to this email directly or view it on GitHub \nhttps://github.com/google/cadvisor/issues/9. \n\u2014\nReply to this email directly or view it on GitHub.\n. No I didn't have one I thought it is part of cadvisor image . I will install one , did the parameters should be pass as environment variables ?\n\nOn 18 \u05d1\u05d0\u05d5\u05d2 2014, at 17:00, Rohit Jnagal notifications@github.com wrote:\nThe optional flags are set to default localhost influxdb instance. Do you \nhave an influxdb instance setup locally? \nThe ip:port of the database. Default is 'localhost:8086'\n-storage_driver_host=ip:port \ndatabase name. Uses db 'cadvisor' by default\n-storage_driver_name \ndatabase username. Default is 'root'\n-storage_driver_user \ndatabase password. Default is 'root'\n-storage_driver_password \nPass these flags to your cadvisor instance with the correct details of your \ninfluxdb backend. \nOn Mon, Aug 18, 2014 at 5:24 AM, defender notifications@github.com wrote: \n\nHi \nI run following command and pass new parameters as environment varaibles \n-e storage_driver=influxdb -e log_dir=/ \ndocker run --name=cadvisor --volume=/var/run:/var/run:rw \n--volume=/sys/fs/cgroup/:/sys/fs/cgroup:ro \n--volume=/var/lib/docker/:/var/lib/docker:ro --publish=8100:8080 \n--detach=true -e storage_driver=influxdb -e log_dir=/ google/cadvisor \nBut I cannot see any information about history , please assists \nThanks, \n\u2014 \nReply to this email directly or view it on GitHub \nhttps://github.com/google/cadvisor/issues/184. \n\u2014\nReply to this email directly or view it on GitHub.\n. No I didn't have one I thought it is part of cadvisor image . I will install one , did the parameters should be pass as environment variables ?\n\nOn 18 \u05d1\u05d0\u05d5\u05d2 2014, at 17:00, Rohit Jnagal notifications@github.com wrote:\nThe optional flags are set to default localhost influxdb instance. Do you \nhave an influxdb instance setup locally? \nThe ip:port of the database. Default is 'localhost:8086'\n-storage_driver_host=ip:port \ndatabase name. Uses db 'cadvisor' by default\n-storage_driver_name \ndatabase username. Default is 'root'\n-storage_driver_user \ndatabase password. Default is 'root'\n-storage_driver_password \nPass these flags to your cadvisor instance with the correct details of your \ninfluxdb backend. \nOn Mon, Aug 18, 2014 at 5:24 AM, defender notifications@github.com wrote: \n\nHi \nI run following command and pass new parameters as environment varaibles \n-e storage_driver=influxdb -e log_dir=/ \ndocker run --name=cadvisor --volume=/var/run:/var/run:rw \n--volume=/sys/fs/cgroup/:/sys/fs/cgroup:ro \n--volume=/var/lib/docker/:/var/lib/docker:ro --publish=8100:8080 \n--detach=true -e storage_driver=influxdb -e log_dir=/ google/cadvisor \nBut I cannot see any information about history , please assists \nThanks, \n\u2014 \nReply to this email directly or view it on GitHub \nhttps://github.com/google/cadvisor/issues/184. \n\u2014\nReply to this email directly or view it on GitHub.\n. Please provide example , what do you mean command line in container perspective ?\n\nOn 18 \u05d1\u05d0\u05d5\u05d2 2014, at 23:19, Victor Marmol notifications@github.com wrote:\nNo, they should be command line args. The readme has the exact ones you need to set.\n\u2014\nReply to this email directly or view it on GitHub.\n. Please provide example , what do you mean command line in container perspective ?\nOn 18 \u05d1\u05d0\u05d5\u05d2 2014, at 23:19, Victor Marmol notifications@github.com wrote:\nNo, they should be command line args. The readme has the exact ones you need to set.\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "jchauncey": "this is particularly useful if you are running a service across several hosts and want to display all runtime metrics on 1 or several graphs. \n. Would you guys be up to having cadvisor output to different metrics collectors? Like ganglia and influxdb?\n. yeah I think im going to take a stab at it. Im definitely not a go expert but think I can get a working prototype on a branch for you guys to look at. \n. Why isnt this configurable?  Its memory by default and you can set a flag to push to influx. \n. i think you have to tag latest by hand, so you tag an image with the version you want and then tag that same image with :latest\n. you guys should take a look at this go framework for generating metrics - https://github.com/rcrowley/go-metrics it already has the ability send stuff to librato and influxdb\n. oh sure in specific we are using a forked version of centurion that we have enhanced =). I am going to add the capability in our fork to specify a custom command at container start but I liked the thought of managing the start options as environment variables. It makes your container more 12 factor app compliant which seems to be a huge plus when using docker containers.\njust a thought.\n. one way you could accomplish this with no code change is to have a shell script be the entry point and it reads in environment variables and builds the run command dynamically (like we typically do with java shell scripts).\n. yeah that's what we would like to have since we are using the container. :+1: \n. i dont think its the correct metric. the metrics reported to the influxdb driver are not the ones used in the cadvisor ui. \n. Yeah I noticed this last week\nOn Feb 9, 2015 9:29 PM, \"Cody Boggs\" notifications@github.com wrote:\n\nSounds like you're onto something... Sequences that have FS stats are\nlacking CPU stats:\n[image: screen shot 2015-02-09 at 21 27 54]\nhttps://cloud.githubusercontent.com/assets/3492944/6121537/95051d6e-b0a2-11e4-9bfd-002894fca077.png\n\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/499#issuecomment-73642662.\n. I think having a series per stat would make the integration with grafana much easier. Then just have columns for hostname and container name for grouping. \n. yeah having a feature flag that allowed you to use docker stats vs cgroup metrics would be a good way to solve that problem once its released.\n. Yeah I think that would work. If I'm running 1.5 use stats and maybe give\nme the option to get extras via mounts if they are not available via the\nend point\nOn Feb 9, 2015 3:37 PM, \"Victor Marmol\" notifications@github.com wrote:\nWe do detection today so we should be able to auto-detect and do the right\nthing. Would that work?\n\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/500#issuecomment-73605877.\n. any movement on this? @vmarmol \n. Well the stats endpoint is pretty expressive in the information it gives\nback and it seems to be more detailed than the cgroups information. So\nthat's why I was asking.\n\nOn Wed, Apr 22, 2015 at 10:54 AM, Victor Marmol notifications@github.com\nwrote:\n\n@jchauncey https://github.com/jchauncey we re-based libcontainer to\nsupport the new Docker. We haven't prioritized moving to the stats API\nsince we'd need to support the current approach for older versions of\nDocker, extra information Docker doesn't expose, and non-Docker containers.\nThe main driving factor is not wanting to mount the cgroup hierarchy? We\ndo get other stats from other parts of the FS so that would still be useful\nto cAdvisor I believe.\n\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/500#issuecomment-95262979.\n. If you were doing this outside of docker and had a long running process you wouldnt graph those values based on PID would you? You would probably annotate the graph when the deploy occurs but graph the values based on process name. I would agree though that passing the container along as a tag is useful (especially if you want to segment on that data). \n. cpu is an ever growing counter. You need to take the derivative (rate of change) when you graph that value. \n. What about using gopsutil? Specifically using this\n. \n",
    "naistran": "Was this really fixed? I still only see the ID instead of container name\n. Was this really fixed? I still only see the ID instead of container name\n. ",
    "cgonzalez": "\nI'm using kernel 2.6.32-431.17.1.el6.x86_64.\n:(\n\"cat /proc/mounts\" shows this:\n\n[root@iridio sgc]# cat /proc/mounts \nrootfs / rootfs rw 0 0\nproc /proc proc rw,relatime 0 0\nsysfs /sys sysfs rw,seclabel,relatime 0 0\ndevtmpfs /dev devtmpfs rw,seclabel,relatime,size=8155256k,nr_inodes=2038814,mode=755 0 0\ndevpts /dev/pts devpts rw,seclabel,relatime,gid=5,mode=620,ptmxmode=000 0 0\ntmpfs /dev/shm tmpfs rw,seclabel,relatime 0 0\n/dev/mapper/vg_iridio-lv_root / ext4 rw,seclabel,relatime,barrier=1,data=ordered 0 0\nnone /selinux selinuxfs rw,relatime 0 0\ndevtmpfs /dev devtmpfs rw,seclabel,relatime,size=8155256k,nr_inodes=2038814,mode=755 0 0\n/proc/bus/usb /proc/bus/usb usbfs rw,relatime 0 0\n/dev/sda1 /boot ext4 rw,seclabel,relatime,barrier=1,data=ordered 0 0\nnone /proc/sys/fs/binfmt_misc binfmt_misc rw,relatime 0 0\ncgroup /cgroup/cpuset cgroup rw,relatime,cpuset 0 0\ncgroup /cgroup/cpu cgroup rw,relatime,cpu 0 0\ncgroup /cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\ncgroup /cgroup/memory cgroup rw,relatime,memory 0 0\ncgroup /cgroup/devices cgroup rw,relatime,devices 0 0\ncgroup /cgroup/freezer cgroup rw,relatime,freezer 0 0\ncgroup /cgroup/net_cls cgroup rw,relatime,net_cls 0 0\ncgroup /cgroup/blkio cgroup rw,relatime,blkio 0 0\n/dev/mapper/vg_iridio-lv_root /var/lib/docker ext4 rw,seclabel,relatime,barrier=1,data=ordered 0 0\n/dev/mapper/docker-253:0-52822119-cbb46300a6950bde8ceae153329c5a0577380d58be4ddd36267e52baa6373a16 /var/lib/docker/devicemapper/mnt/cbb46300a6950bde8ceae153329c5a0577380d58be4ddd36267e52baa6373a16 ext4 rw,seclabel,relatime,barrier=1,stripe=16,data=ordered,discard 0 0\n/dev/mapper/docker-253:0-52822119-d9ccfc866c89e28cbe81596976cdbb6c850aa8922fc7bf01b6eee7d08ab22efa /var/lib/docker/devicemapper/mnt/d9ccfc866c89e28cbe81596976cdbb6c850aa8922fc7bf01b6eee7d08ab22efa ext4 rw,seclabel,relatime,barrier=1,stripe=16,data=ordered,discard 0 0\n/dev/mapper/docker-253:0-52822119-625f30a1b59191a9f46f20afaf49d9c78dc897f0761e84c3f773a48f95aecfbd /var/lib/docker/devicemapper/mnt/625f30a1b59191a9f46f20afaf49d9c78dc897f0761e84c3f773a48f95aecfbd ext4 rw,seclabel,relatime,barrier=1,stripe=16,data=ordered,discard 0 0\n/dev/mapper/docker-253:0-52822119-9950040f0876f237525ddd9d8495be2e23ca4c754a5c34bc02db95196b43e6fd /var/lib/docker/devicemapper/mnt/9950040f0876f237525ddd9d8495be2e23ca4c754a5c34bc02db95196b43e6fd ext4 rw,seclabel,relatime,barrier=1,stripe=16,data=ordered,discard 0 0\n/dev/mapper/docker-253:0-52822119-1db0be156c1f2ca17c9c5eff4ba5526eb96007f576c771a0008b09c89a352dfc /var/lib/docker/devicemapper/mnt/1db0be156c1f2ca17c9c5eff4ba5526eb96007f576c771a0008b09c89a352dfc ext4 rw,seclabel,relatime,barrier=1,stripe=16,data=ordered,discard 0 0\n/dev/mapper/docker-253:0-52822119-f71b1800c7753db9e6defab26ec07da74599bb5f89c76fb172d9f0917acbfec8 /var/lib/docker/devicemapper/mnt/f71b1800c7753db9e6defab26ec07da74599bb5f89c76fb172d9f0917acbfec8 ext4 rw,seclabel,relatime,barrier=1,stripe=16,data=ordered,discard 0 0\n/dev/mapper/docker-253:0-52822119-a6d4bb084b3b459e93f3016c32f2c66b46acd820952e08bf476b37ede7d671ab /var/lib/docker/devicemapper/mnt/a6d4bb084b3b459e93f3016c32f2c66b46acd820952e08bf476b37ede7d671ab ext4 rw,seclabel,relatime,barrier=1,stripe=16,data=ordered,discard 0 0\nI'm not sure whether this shows that cgroups are mounted.\n. OK, I'll keep an eye on #14. Thanks for your answer.\n. ",
    "tpires": "Tried to run on RHEL 6.5 and got this:\n$ sudo docker run --name cadvisor  --volume=/var/run:/var/run:rw   --volume=/cgroup/:/cgroup:ro   --volume=/var/lib/docker/:/var/lib/docker:ro   --publish=8080:8080  --detach=true google/cadvisor:latest\n04f0fc120f11fe660f98c4af8d3f200c7c1380d2e40191af3fa162bd43b154e2\n$ sudo docker logs cadvisor\n2014/07/24 17:53:43 Machine: {NumCores:4 MemoryCapacity:8254861312}\n2014/07/24 17:53:43 Version: {KernelVersion:2.6.32-431.20.3.el6.x86_64 ContainerOsVersion:Debian GNU/Linux 7 (wheezy) DockerVersion:1.0.0 CadvisorVersion:0.1.0}\n2014/07/24 17:53:43 register lmctfy under /\n2014/07/24 17:53:43 Starting cAdvisor version: \"0.1.0\"\n2014/07/24 17:53:43 About to serve on port 8080\n2014/07/24 17:53:43 Container handler factory for / is lmctfy\n2014/07/24 17:53:43 Added container: / (aliases: [])\n2014/07/24 17:53:43 Start housekeeping for container \"/\"\n2014/07/24 17:53:43 Starting recovery of all containers\n2014/07/24 17:53:43 Container handler factory for /docker is docker\n2014/07/24 17:53:43 Added container: /docker (aliases: [])\n2014/07/24 17:53:43 Start housekeeping for container \"/docker\"\n2014/07/24 17:53:43 Container handler factory for /docker/04f0fc120f11fe660f98c4af8d3f200c7c1380d2e40191af3fa162bd43b154e2 is docker\n2014/07/24 17:53:43 Added container: /docker/04f0fc120f11fe660f98c4af8d3f200c7c1380d2e40191af3fa162bd43b154e2 (aliases: [/docker/cadvisor])\n2014/07/24 17:53:43 Failed to update stats for container \"/docker/04f0fc120f11fe660f98c4af8d3f200c7c1380d2e40191af3fa162bd43b154e2\": getting stats for system \"blkio\" open /cgroup/blkio/docker/04f0fc120f11fe660f98c4af8d3f200c7c1380d2e40191af3fa162bd43b154e2/blkio.sectors_recursive: no such file or directory\n2014/07/24 17:53:43 Start housekeeping for container \"/docker/04f0fc120f11fe660f98c4af8d3f200c7c1380d2e40191af3fa162bd43b154e2\"\n2014/07/24 17:53:43 Container handler factory for /docker/338d75e69ccda0129a40fb0afe6c7ff10a347dac1826badbcc5e5756b4f53bd9 is docker\n2014/07/24 17:53:43 Added container: /docker/338d75e69ccda0129a40fb0afe6c7ff10a347dac1826badbcc5e5756b4f53bd9 (aliases: [/docker/jenkins])\n2014/07/24 17:53:43 Failed to update stats for container \"/docker/338d75e69ccda0129a40fb0afe6c7ff10a347dac1826badbcc5e5756b4f53bd9\": getting stats for system \"blkio\" open /cgroup/blkio/docker/338d75e69ccda0129a40fb0afe6c7ff10a347dac1826badbcc5e5756b4f53bd9/blkio.sectors_recursive: no such file or directory\n. Sorry, my bad!! I read latest instead of canary! Going to test it after it's marked to :latest!\n. Got it up and running with:\n$ sudo docker run --volume=/var/run:/var/run:rw --volume=/cgroup:/cgroup:ro --volume=/var/lib/docker/:/var/lib/docker:ro --publish=8080:8080 --detach=true --name=cadvisor   google/cadvisor:latest\nBut when visualizing sub-containers, cadvisor freezes:\n```\n$ sudo docker logs -f cadvisor\n2014/07/25 08:59:20 Machine: {NumCores:4 MemoryCapacity:8254861312}\n2014/07/25 08:59:20 Version: {KernelVersion:2.6.32-431.20.3.el6.x86_64 ContainerOsVersion:Unknown DockerVersion:1.0.0 CadvisorVersion:0.2.0}\n2014/07/25 08:59:20 Registering Docker factory\n2014/07/25 08:59:20 Registering Raw factory\n2014/07/25 08:59:20 Starting cAdvisor version: \"0.2.0\"\n2014/07/25 08:59:20 About to serve on port 8080\n2014/07/25 08:59:20 Using factory \"raw\" for container \"/\"\n2014/07/25 08:59:20 Added container: / (aliases: [])\n2014/07/25 08:59:20 Start housekeeping for container \"/\"\n2014/07/25 08:59:20 Starting recovery of all containers\n2014/07/25 08:59:20 Using factory \"docker\" for container \"/docker/cdbdf0ca926803e74c3eef8f283aec8e2abb48a117f7aeec9d8246cc77013085\"\n2014/07/25 08:59:20 Added container: /docker/cdbdf0ca926803e74c3eef8f283aec8e2abb48a117f7aeec9d8246cc77013085 (aliases: [/docker/nagios])\n2014/07/25 08:59:20 Start housekeeping for container \"/docker/cdbdf0ca926803e74c3eef8f283aec8e2abb48a117f7aeec9d8246cc77013085\"\n2014/07/25 08:59:20 Using factory \"docker\" for container \"/docker/338d75e69ccda0129a40fb0afe6c7ff10a347dac1826badbcc5e5756b4f53bd9\"\n2014/07/25 08:59:20 Added container: /docker/338d75e69ccda0129a40fb0afe6c7ff10a347dac1826badbcc5e5756b4f53bd9 (aliases: [/docker/jenkins])\n2014/07/25 08:59:20 Start housekeeping for container \"/docker/338d75e69ccda0129a40fb0afe6c7ff10a347dac1826badbcc5e5756b4f53bd9\"\n2014/07/25 08:59:20 Using factory \"docker\" for container \"/docker\"\n2014/07/25 08:59:20 Added container: /docker (aliases: [])\n2014/07/25 08:59:20 Start housekeeping for container \"/docker\"\n2014/07/25 08:59:20 Using factory \"docker\" for container \"/docker/72f6e89d3ab234e30aab45e6f7669c34f85eb2f29d064fc385ab4149427770e0\"\n2014/07/25 08:59:20 Added container: /docker/72f6e89d3ab234e30aab45e6f7669c34f85eb2f29d064fc385ab4149427770e0 (aliases: [/docker/postfix])\n2014/07/25 08:59:20 Start housekeeping for container \"/docker/72f6e89d3ab234e30aab45e6f7669c34f85eb2f29d064fc385ab4149427770e0\"\n2014/07/25 08:59:20 Using factory \"docker\" for container \"/docker/23acc760e53bb93717e1b578b84a8cf05f83ce59020af1cb6e316d3b0558ab24\"\n2014/07/25 08:59:20 Added container: /docker/23acc760e53bb93717e1b578b84a8cf05f83ce59020af1cb6e316d3b0558ab24 (aliases: [/docker/cadvisor])\n2014/07/25 08:59:20 Start housekeeping for container \"/docker/23acc760e53bb93717e1b578b84a8cf05f83ce59020af1cb6e316d3b0558ab24\"\n2014/07/25 08:59:20 Recovery completed\n2014/07/25 08:59:26 Get(/); &{NumStats:60 NumSamples:60 CpuUsagePercentiles:[] MemoryUsagePercentiles:[]}\n2014/07/25 08:59:26 Request took 5.694966ms\n2014/07/25 08:59:26 Api - Machine\n2014/07/25 08:59:26 Request took 402.798us\n2014/07/25 08:59:27 Get(/); &{NumStats:60 NumSamples:60 CpuUsagePercentiles:[] MemoryUsagePercentiles:[]}\n2014/07/25 08:59:27 Request took 4.027077ms\n2014/07/25 08:59:27 Api - Container(/)\n2014/07/25 08:59:27 Get(/); &{NumStats:60 NumSamples:0 CpuUsagePercentiles:[] MemoryUsagePercentiles:[]}\n2014/07/25 08:59:27 Request took 3.855463ms\n2014/07/25 08:59:28 Api - Container(/)\n2014/07/25 08:59:28 Get(/); &{NumStats:60 NumSamples:0 CpuUsagePercentiles:[] MemoryUsagePercentiles:[]}\n2014/07/25 08:59:28 Request took 3.390134ms\n2014/07/25 08:59:29 Api - Container(/)\n2014/07/25 08:59:29 Get(/); &{NumStats:60 NumSamples:0 CpuUsagePercentiles:[] MemoryUsagePercentiles:[]}\n2014/07/25 08:59:29 Request took 3.162591ms\n2014/07/25 08:59:30 Api - Container(/)\n2014/07/25 08:59:30 Get(/); &{NumStats:60 NumSamples:0 CpuUsagePercentiles:[] MemoryUsagePercentiles:[]}\n2014/07/25 08:59:30 Request took 5.509357ms\n2014/07/25 08:59:31 Api - Container(/)\n2014/07/25 08:59:31 Get(/); &{NumStats:60 NumSamples:0 CpuUsagePercentiles:[] MemoryUsagePercentiles:[]}\n2014/07/25 08:59:31 Request took 3.988355ms\n2014/07/25 08:59:32 Api - Container(/)\n2014/07/25 08:59:32 Get(/); &{NumStats:60 NumSamples:0 CpuUsagePercentiles:[] MemoryUsagePercentiles:[]}\n2014/07/25 08:59:32 Request took 2.047968ms\n2014/07/25 08:59:33 Api - Container(/)\n2014/07/25 08:59:33 Get(/); &{NumStats:60 NumSamples:0 CpuUsagePercentiles:[] MemoryUsagePercentiles:[]}\n2014/07/25 08:59:33 Request took 2.614232ms\n2014/07/25 08:59:34 Api - Container(/)\n2014/07/25 08:59:34 Get(/); &{NumStats:60 NumSamples:0 CpuUsagePercentiles:[] MemoryUsagePercentiles:[]}\n2014/07/25 08:59:34 Request took 3.060807ms\n2014/07/25 08:59:35 Api - Container(/)\n2014/07/25 08:59:35 Get(/); &{NumStats:60 NumSamples:0 CpuUsagePercentiles:[] MemoryUsagePercentiles:[]}\n2014/07/25 08:59:35 Request took 3.632048ms\n2014/07/25 08:59:36 Api - Container(/)\n2014/07/25 08:59:36 Get(/); &{NumStats:60 NumSamples:0 CpuUsagePercentiles:[] MemoryUsagePercentiles:[]}\n2014/07/25 08:59:36 Request took 3.740146ms\n2014/07/25 08:59:37 Api - Container(/)\n2014/07/25 08:59:37 Get(/); &{NumStats:60 NumSamples:0 CpuUsagePercentiles:[] MemoryUsagePercentiles:[]}\n2014/07/25 08:59:37 Request took 5.529119ms\n2014/07/25 08:59:38 Api - Container(/)\n2014/07/25 08:59:38 Get(/); &{NumStats:60 NumSamples:0 CpuUsagePercentiles:[] MemoryUsagePercentiles:[]}\n2014/07/25 08:59:38 Request took 3.542173ms\n2014/07/25 08:59:39 Get(/docker); &{NumStats:60 NumSamples:60 CpuUsagePercentiles:[] MemoryUsagePercentiles:[]}\n2014/07/25 08:59:39 Request took 6.937376ms\n2014/07/25 08:59:39 Get(/); &{NumStats:60 NumSamples:60 CpuUsagePercentiles:[] MemoryUsagePercentiles:[]}\n2014/07/25 08:59:39 Request took 2.907233ms\n2014/07/25 08:59:41 Get(/docker/23acc760e53bb93717e1b578b84a8cf05f83ce59020af1cb6e316d3b0558ab24); &{NumStats:60 NumSamples:60 CpuUsagePercentiles:[] MemoryUsagePercentiles:[]}\n2014/07/25 08:59:41 http: panic serving 10.112.210.230:51345: runtime error: invalid memory address or nil pointer dereference\ngoroutine 31 [running]:\nnet/http.func\u00b7011()\n    /usr/lib/google-golang/src/pkg/net/http/server.go:1100 +0xb7\nruntime.panic(0x8f3a00, 0xe0d853)\n    /usr/lib/google-golang/src/pkg/runtime/panic.c:248 +0x18d\ntext/template.errRecover(0x4c20811f870)\n    /usr/lib/google-golang/src/pkg/text/template/exec.go:100 +0xb9\nruntime.panic(0x8f3a00, 0xe0d853)\n    /usr/lib/google-golang/src/pkg/runtime/panic.c:248 +0x18d\ngithub.com/google/cadvisor/pages.getHotMemoryPercent(0x4c208119f50, 0x4c208022e70, 0x16, 0x16, 0x4c208119fa0, 0x0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/pages/containers.go:173 +0x39\nreflect.Value.call(0x892de0, 0xa457b8, 0x0, 0x130, 0x949040, 0x4, 0x4c2080d7800, 0x3, 0x3, 0x0, ...)\n    /usr/lib/google-golang/src/pkg/reflect/value.go:563 +0x1210\nreflect.Value.Call(0x892de0, 0xa457b8, 0x0, 0x130, 0x4c2080d7800, 0x3, 0x3, 0x0, 0x0, 0x0)\n    /usr/lib/google-golang/src/pkg/reflect/value.go:411 +0xd7\ntext/template.(state).evalCall(0x4c2081201c0, 0x7e7980, 0x4c208029f10, 0x0, 0x160, 0x892de0, 0xa457b8, 0x0, 0x130, 0x7f9d31197c20, ...)\n    /usr/lib/google-golang/src/pkg/text/template/exec.go:563 +0x9d4\ntext/template.(state).evalFunction(0x4c2081201c0, 0x7e7980, 0x4c208029f10, 0x0, 0x160, 0x4c20803ecc0, 0x7f9d31197c20, 0x4c208084d50, 0x4c208057180, 0x4, ...)\n    /usr/lib/google-golang/src/pkg/text/template/exec.go:455 +0x343\ntext/template.(state).evalCommand(0x4c2081201c0, 0x7e7980, 0x4c208029f10, 0x0, 0x160, 0x4c208084d50, 0x0, 0x0, 0x0, 0x0, ...)\n    /usr/lib/google-golang/src/pkg/text/template/exec.go:359 +0x23e\ntext/template.(state).evalPipeline(0x4c2081201c0, 0x7e7980, 0x4c208029f10, 0x0, 0x160, 0x4c208019220, 0x0, 0x0, 0x0, 0x0)\n    /usr/lib/google-golang/src/pkg/text/template/exec.go:332 +0x1be\ntext/template.(state).walk(0x4c2081201c0, 0x7e7980, 0x4c208029f10, 0x0, 0x160, 0x7f9d3118fd60, 0x4c20803ed00)\n    /usr/lib/google-golang/src/pkg/text/template/exec.go:167 +0x133\ntext/template.(state).walk(0x4c2081201c0, 0x7e7980, 0x4c208029f10, 0x0, 0x160, 0x7f9d31197b90, 0x4c208084cf0)\n    /usr/lib/google-golang/src/pkg/text/template/exec.go:175 +0x848\ntext/template.(state).walkIfOrWith(0x4c2081201c0, 0xa, 0x7e7980, 0x4c208029f10, 0x0, 0x160, 0x4c2080191d0, 0x4c208084cf0, 0x0)\n    /usr/lib/google-golang/src/pkg/text/template/exec.go:205 +0x355\ntext/template.(state).walk(0x4c2081201c0, 0x7e7980, 0x4c208029f10, 0x0, 0x160, 0x7f9d3118ff58, 0x4c2080850b0)\n    /usr/lib/google-golang/src/pkg/text/template/exec.go:172 +0x5d3\ntext/template.(state).walk(0x4c2081201c0, 0x7e7980, 0x4c208029f10, 0x0, 0x160, 0x7f9d31197b90, 0x4c20800f9b0)\n    /usr/lib/google-golang/src/pkg/text/template/exec.go:175 +0x848\ntext/template.(state).walkIfOrWith(0x4c2081201c0, 0xa, 0x7e7980, 0x4c208029f10, 0x0, 0x160, 0x4c208018c30, 0x4c20800f9b0, 0x0)\n    /usr/lib/google-golang/src/pkg/text/template/exec.go:205 +0x355\ntext/template.(state).walk(0x4c2081201c0, 0x7e7980, 0x4c208029f10, 0x0, 0x160, 0x7f9d3118ff58, 0x4c208085230)\n    /usr/lib/google-golang/src/pkg/text/template/exec.go:172 +0x5d3\ntext/template.(state).walk(0x4c2081201c0, 0x7e7980, 0x4c208029f10, 0x0, 0x160, 0x7f9d31197b90, 0x4c20800f260)\n    /usr/lib/google-golang/src/pkg/text/template/exec.go:175 +0x848\ntext/template.(Template).Execute(0x4c208056040, 0x7f9d31197a68, 0x4c2080ad900, 0x7e7980, 0x4c208029f10, 0x0, 0x0)\n    /usr/lib/google-golang/src/pkg/text/template/exec.go:155 +0x332\nhtml/template.(Template).Execute(0x4c20803e080, 0x7f9d31197a68, 0x4c2080ad900, 0x7e7980, 0x4c208029f10, 0x0, 0x0)\n    /usr/lib/google-golang/src/pkg/html/template/template.go:73 +0xa7\ngithub.com/google/cadvisor/pages.ServerContainersPage(0x7f9d31191718, 0x4c20805e700, 0x7f9d31197a08, 0x4c2080ad900, 0x4c208029a40, 0x0, 0x0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/pages/containers.go:251 +0x798\nmain.func\u00b7003(0x7f9d31197a08, 0x4c2080ad900, 0x4c208197450)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/cadvisor.go:80 +0x60\nnet/http.HandlerFunc.ServeHTTP(0x4c2080d26e0, 0x7f9d31197a08, 0x4c2080ad900, 0x4c208197450)\n    /usr/lib/google-golang/src/pkg/net/http/server.go:1235 +0x40\nnet/http.(ServeMux).ServeHTTP(0x4c20800e720, 0x7f9d31197a08, 0x4c2080ad900, 0x4c208197450)\n    /usr/lib/google-golang/src/pkg/net/http/server.go:1511 +0x1a3\nnet/http.serverHandler.ServeHTTP(0x4c208004300, 0x7f9d31197a08, 0x4c2080ad900, 0x4c208197450)\n    /usr/lib/google-golang/src/pkg/net/http/server.go:1673 +0x19f\nnet/http.(conn).serve(0x4c20805f780)\n    /usr/lib/google-golang/src/pkg/net/http/server.go:1174 +0xa7e\ncreated by net/http.(*Server).Serve\n    /usr/lib/google-golang/src/pkg/net/http/server.go:1721 +0x313\n2014/07/25 08:59:41 Get(/); &{NumStats:60 NumSamples:60 CpuUsagePercentiles:[] MemoryUsagePercentiles:[]}\n2014/07/25 08:59:41 Request took 3.195288ms\n```\n. @vmarmol it's up and running for the last 3 days ;) thanks!\n. +1\n. @vmarmol Yes I am. I'm setting --memory on some containers.\n. @rjnagal For root everything's fine. At /containers/docker memory graph appears but memory usage is wrong (too low, ~0MB).\n@vmarmol JavaScript console is showing the error: \"Uncaught TypeError: Cannot read property 'memory' of undefined\". Followed the code to containers.js:1658 and \njavascript\n        memory_limit = machineInfo.spec.memory.limit;;\nShould be \njavascript\n        memory_limit = containerInfo.spec.memory.limit;\nSince this is a JavaScript error, there's no need for the HTTP requests.\n. @rjnagal tested, it's working. thank you :)\n. ",
    "rjnagal": "This is meant to use the driver based on libcontainer/cgroup? If yes, I can take this.\n. Working on the libcontainer changes first. Hoping to get this done soon.\nOn Mon, Jul 7, 2014 at 10:28 AM, Victor Marmol notifications@github.com\nwrote:\n\nLast I heard we hadn't started on this. We're planning on re-using\nlibcontainer's cgroups package. ping @rjnagal https://github.com/rjnagal\nif we has any updates.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/14#issuecomment-48209749.\n. How about publishing a pstree for the container (hidden by default)? For libcontainer, it just needs to call pstree on the id in pid file.\n. We added hot/cold breakdown and legend to the graph. I think its fine now.\n. LGTM\n. Vish is working on this one.\n. @thaJeztah I'd prefer something client-side. There are a few png generator libraries or just plan canvas would do. We can wrap them up with the cadvisor API calls. We can publish a docker container that has both cadvisor and the helper client.\n. @erikh Do you have a patch we can collaborate on?\n. #190 fixes this\n. LGTM\n. LGTM\n. some cgroups are under /system.slice/docker-*. lmctfy stats doesn't detect that. It should use device or cpu cgroup to detect. For libcontainer, one way to track stats is to use nsinit pid and find paths using /proc/pid-nsinit/cgroups\n\n$ ps -efww | grep cadvisor\nroot      3490  3362  0 21:26 ?        00:00:00 /bin/sh -c /usr/bin/cadvisor\nroot      3514  3490  0 21:26 ?        00:00:02 /usr/bin/cadvisor\n$ cat /proc/3490/cgroup \n10:perf_event:/\n9:blkio:/system.slice/docker-d293e6edbf522628caf322c1d603ddbfbd891c7014e7a747e1bb4bfa122d3b1f.scope\n8:net_cls:/\n7:freezer:/\n6:devices:/system.slice/docker-d293e6edbf522628caf322c1d603ddbfbd891c7014e7a747e1bb4bfa122d3b1f.scope\n5:memory:/system.slice/docker-d293e6edbf522628caf322c1d603ddbfbd891c7014e7a747e1bb4bfa122d3b1f.scope\n4:cpu,cpuacct:/system.slice/docker-d293e6edbf522628caf322c1d603ddbfbd891c7014e7a747e1bb4bfa122d3b1f.scope\n3:cpuset:/\n2:name=systemd:/system.slice/docker-d293e6edbf522628caf322c1d603ddbfbd891c7014e7a747e1bb4bfa122d3b1f.scope\n. / is fine. Everything else is broken.\nlibcontainer seems like a simpler change, but I am going to try and change both lmctfy and libcontainer.\nOne problem with systemd systems is that there are many containers (some of them just within a named systemd hierarchy). We might want to filter some out.\nI'd be fine if anyone wants to take over either of the changes :)\n. --docker_only=true would filter cgroups to only report docker and root containers.\n. Build failed:\ngithub.com/google/cadvisor/container/docker\ncontainer/docker/handler.go:121: undefined: libcontainer.Container\ncontainer/docker/handler.go:130: undefined: libcontainer.Container\ncontainer/docker/handler.go:139: undefined: libcontainer.Container\nThe command \"go get -v ./...\" failed. Retrying, 2 of 3.\ngithub.com/google/cadvisor/container/docker\ngithub.com/google/cadvisor/container/docker\ncontainer/docker/handler.go:121: undefined: libcontainer.Container\ncontainer/docker/handler.go:130: undefined: libcontainer.Container\ncontainer/docker/handler.go:139: undefined: libcontainer.Container\nThe command \"go get -v ./...\" failed. Retrying, 3 of 3.\ngithub.com/google/cadvisor/container/docker\ngithub.com/google/cadvisor/container/docker\ncontainer/docker/handler.go:121: undefined: libcontainer.Container\ncontainer/docker/handler.go:130: undefined: libcontainer.Container\ncontainer/docker/handler.go:139: undefined: libcontainer.Container\n. LGTM.\nThanks for the additional tests.\n. LGTM.\nDidn't notice that in the review.\n. LGTM\n. Are the influxdb tests failing due to change in influxdb API that Nan is\nworking on?\nOn Tue, Jul 22, 2014 at 8:46 AM, Victor Marmol notifications@github.com\nwrote:\n\nPing @monnand https://github.com/monnand @rjnagal\nhttps://github.com/rjnagal\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/100#issuecomment-49757575.\n. Is this because of the systemd hierarchy?\n. This is because a container is being re-created too quickly with the same\nname and id? I can see this with raw containers. For docker containers, the\nid should be different.\n\nEasiest way is to ignore the negative values in cpu usage delta.\nOn Thu, Jul 31, 2014 at 2:36 PM, Victor Marmol notifications@github.com\nwrote:\n\n@bshi https://github.com/bshi do you see this when any service restarts\nor only when it restarts services that talk to Docker?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/144#issuecomment-50822281.\n. We have had weird cases where usage drops back to a lower value. But these\nusually only last a single reading. I was suggesting that we report the\nlast cpu usage delta for one reading and then things should return back to\nnormal. We'll save the new reading, so the next time we'll get consistent\nvalues again.\n\nIf we decide to report zero instead, it show a nice drop in graph that\nmakes the restart visible. That would be nice too, if our system can handle\nit.\nTracking cgroup creation time is costlier and would make us disregard the\nhistory before restart. This shouldn't be a problem with docker restart as\nit generates containers with different ids.\nOn Thu, Jul 31, 2014 at 2:50 PM, Victor Marmol notifications@github.com\nwrote:\n\nAh, that's a good point @rjnagal https://github.com/rjnagal. I'm not\nsure we should ignore the delta, maybe we should try to recognize this and\nre-set our stats. Otherwise the graphs and data will look pretty weird (and\na lot of code won't expect the sudden drop). We can check the creation\ntimes of the container to verify this I think.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/144#issuecomment-50823740.\n. Did you get to check why /info tests are failing?\n\nOn Thu, Jul 31, 2014 at 4:06 PM, Victor Marmol notifications@github.com\nwrote:\n\nPing @rjnagal https://github.com/rjnagal\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/145#issuecomment-50830429.\n. Michael, we'd love any patches to support openvz. We don't have an openvz\nsetup handy, so you'll end up doing most of testing and support for openvz\ndriver. Hope that's fine.\n\nOn Mon, Aug 4, 2014 at 10:48 AM, Michael Basnight notifications@github.com\nwrote:\n\nI use openvz and i was wondering if there would be any opposition for me\nto add openvz tracking to cadvisor. I like the tool, and love docker / lxc,\nbut have an existing openvz farm and want to be able to use newer tools\nwith the older tech :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/150.\n. We were planning to implement a regex to decide what containers to track,\nbut it was still going to run as a daemon in its own container.\n\n@crosbymichael: will that work for you?\nOn Tue, Aug 5, 2014 at 2:16 PM, Victor Marmol notifications@github.com\nwrote:\n\nAs @crosbymichael https://github.com/crosbymichael mentioned in IRC and\nalso a way for us to narrow down what we track when we start doing heavier\nthings.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/152.\n. The main motivation for going with regex is our main use-case: track only docker containers and ignore others (like tons of systemd containers). We can do that through environment variables if that is easier to configure than flags.\n. I think the main feature here was to support regexes, so that we can filter out, say only docker containers. That way, we don't need to update the list when a new container shows up. \n\nWe can probably accept the input through container_hints. We can accept aliases too to target specific containers, but doing just aliases makes regex matching harder.\nThe problem I ran into while trying this out was the way cAdvisor is structured now.  To track a container, we need to track all its parents. This means that we need to separate out discovering new containers from tracking. It requires bit of restructuring that we decided not to pursue back in Oct.\nFeel free to take a shot at it. I'll take another look too. Unassigning from me for now.\n. @denderello This is doable. We can add a flag(comma-separated values) or a config file to whitelist columns to push.\n. --docker_only solves part of the problem. I am not going to look at the regex part anytime soon.\nSo we can close for now.\n. I think this works now.\n. This is done.\n. LGTM\n. I was thinking of doing the same :)\nLooks like we need to update travis to add vet tool:\ngo tool: no such tool \"vet\"; to install:\ngo get code.google.com/p/go.tools/cmd/vet\nOn Wed, Aug 13, 2014 at 8:54 AM, Victor Marmol notifications@github.com\nwrote:\n\n\nYou can merge this Pull Request by running\ngit pull https://github.com/vmarmol/cadvisor add-vet\nOr view, comment on, or merge it at:\nhttps://github.com/google/cadvisor/pull/177\nCommit Summary\n- Add go vet to travis.\nFile Changes\n- M .travis.yml\n  https://github.com/google/cadvisor/pull/177/files#diff-0 (1)\nPatch Links:\n- https://github.com/google/cadvisor/pull/177.patch\n- https://github.com/google/cadvisor/pull/177.diff\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/177.\n. LGTM\n. Although the bigquery backend (WIP) doesn't get that loaded, I am changing\nit to batch the writes upto a certain number. We can share the buffering\nlogic with influx driver.\n\nOn Fri, Aug 15, 2014 at 10:13 AM, Vish Kannan notifications@github.com\nwrote:\n\nI have had similar experiences. Influxdb ends up using close to 100% CPU\nwhen running just 3 cadvisor nodes.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/182#issuecomment-52332565.\n. The optional flags are set to default localhost influxdb instance. Do you\nhave an influxdb instance setup locally?\n\n# The ip:port of the database. Default is 'localhost:8086'\n -storage_driver_host=ip:port\n # database name. Uses db 'cadvisor' by default\n -storage_driver_name\n # database username. Default is 'root'\n -storage_driver_user\n # database password. Default is 'root'\n -storage_driver_password\nPass these flags to your cadvisor instance with the correct details of your\ninfluxdb backend.\nOn Mon, Aug 18, 2014 at 5:24 AM, defender notifications@github.com wrote:\n\nHi\nI run following command and pass new parameters as environment varaibles\n-e storage_driver=influxdb -e log_dir=/\ndocker run --name=cadvisor --volume=/var/run:/var/run:rw\n--volume=/sys/fs/cgroup/:/sys/fs/cgroup:ro\n--volume=/var/lib/docker/:/var/lib/docker:ro --publish=8100:8080\n--detach=true -e storage_driver=influxdb -e log_dir=/ google/cadvisor\nBut I cannot see any information about history , please assists\nThanks,\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/184.\n. ping @vmarmol :)\n. Fixed all nits. NumCores and per-core stats will follow in a separate PR.\n. Done.\n\nOn Mon, Aug 25, 2014 at 5:29 PM, Victor Marmol notifications@github.com\nwrote:\n\nLGTM, can you rebase to re-run the build now that it is fixed?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/195#issuecomment-53358114.\n. LGTM. Thanks for the fix. I noticed it while adding BigQuery, but promptly forgot about it :)\n. disk usage is very specific to the implementation of higher layers (docker\nand non-docker use cases).  We have been toying with a few ideas and hoping\nto come up with something that doesn't need to run inside a user container.\nThat said, it will probably be much easier to build it for docker first.\n\nWe'll definitely do something for disk space soon. If you have any ideas or\na patch, please send it our way :)\nOn Thu, Aug 28, 2014 at 9:02 AM, tifayuki notifications@github.com wrote:\n\nHi,\nI wonder if there is a plan to add disk usage metrics for each container\nto cAdvisor. And also may include the volume size of each container?\nBTW, thanks for this awesome cAdvisor project.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/198.\n. Thanks Nan. You can move iterating over queries to the storage driver. I'll\nfix bigquery to batch them. You and Vish can fix the influxdb one.\n\nOn Fri, Aug 29, 2014 at 7:47 AM, monnand notifications@github.com wrote:\n\nHold on this PR. I'll do two PRs today:\n- Change AddStats() to accept multiple points.\n- Merge storage/cache and storage/memory into one package.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/200#issuecomment-53885373.\n. Making storage drivers write-only sounds good. The main interface should be\nAddStats() that can accept multiple stats.\n\nI still liked recentStats for debugging. Its nice to be able to drive the\nUI from storage drivers directly. I might miss that interface.\nOn Fri, Aug 29, 2014 at 8:42 PM, monnand notifications@github.com wrote:\n\n@vishh https://github.com/vishh I see.\nFor the storage driver interface, I would argue that cAdvisor should not\nbe retrieving stats from an external DB at all.\nI have similar thought. I think cAdvisor should keep most recent stats and\ndo any analysis within this window. Anything outside this window should be\ndone as a batch job. This includes things like Percentiles(), Samples(),\netc. (Do we need Samples() under this context?)\nLets define a new external storage interface which will support only a\nWrite method. This external storage driver can then be optionally injected\ninto the cache.\nThen the write could be even async. i.e. let a pool of goroutines do the\nwrite operation to the backend storage.\nWell. Then that would be a huge change. Most changes are deleting existing\ncode in storage drivers. If we decided to do that, then I think I do not\nneed to fix influxdb. What do others think? @vmarmol\nhttps://github.com/vmarmol @rjnagal https://github.com/rjnagal ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/202#issuecomment-53947596.\n. LGTM\n. Nice. LGTM.\n. Could you boot up with lxc driver and got it to work?\n. LGTM\n. LGTM.\n\nMerging\n. LGTM. very cool !\n. LGTM. \n. LGTM\nwaiting on travis.\n. So this is failing on travis as there's no /sys available there. Our test tries to get machine info from the real machine. It works for /proc, but not for /sys.\nNot sure if there's a case where we might run without /sys, but I can make that optional in code. For the unit tests, we'll need a proper fs mock.\n. I can try starting with machine info which just calls /proc and /sys.\nEasier option is to add a test_dir flag and use that as a base path. That's\na couple of lines of change.\nOn Tue, Sep 23, 2014 at 11:55 AM, Victor Marmol notifications@github.com\nwrote:\n\nShould we do the mock FS? Would it be a lot of work?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/249#issuecomment-56571071.\n. OK, I'll start with the mocks now. We'll have to do that some time anyway :)\n\nOn Tue, Sep 23, 2014 at 2:11 PM, Victor Marmol notifications@github.com\nwrote:\n\nIf you have the cycles, I'd rather we start doing it with mocks. I've\nstarted trying to make container and manager more testable so we'd be\ngetting on the right foot. We can do that for now and do the proper change\nin another PR if you'd like though.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/249#issuecomment-56590508.\n. I was updating and adding mocks to this today. This wasn't completely done yet.\nDid the tests pass?\n. Looks like this was an empty merge. I'll send out a new one.\n. LGTM w/ minor nits.\n. LGTM\n. Thanks for the cleanup, Satnam.\n. I'd not argue with the style either :(\nI also like the error check being on its own line, because I can create a\nvim shortcut to stamp it out every other line.\n\nProposed changes sound good to me.\nOn Tue, Sep 23, 2014 at 3:54 PM, Victor Marmol notifications@github.com\nwrote:\n\nI'm okay accepting this compromise :) the camel case const I personally\ndislike, but the style is clear on it so I won't argue. On the if\nstatement, I've seen both as accepted styles.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/252#issuecomment-56602702.\n. I don't think its worth carrying rollback support pre 1.0. It would be better to just mark it as a compat breaking release through versioning and release notes.\n. LGTM\n. Some tests are failing as travis instances doesn't have all /proc files exposed.\n\nFAIL: TestNew (0.02 seconds)\n    manager_test.go:171: Expected manager.New to succeed: open /proc/diskstats: no such file or directory\nI have an outstanding issue to mock /proc and /sys in manager_test.go. Will try to get to it today/tomorrow.\n. Since we have too many files that we touch under /sys and /proc, the\neasiest mock way is to prepend a test_dir to /proc and /sys and read/write\nfrom these files.\nOn Tue, Oct 28, 2014 at 8:58 AM, Victor Marmol notifications@github.com\nwrote:\n\nMerged #279 https://github.com/google/cadvisor/pull/279.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/279#event-184769211.\n. LGTM.\n\nWaiting for Travis...\n. @gmelekh Can you paste the output for a single stats row for that container? It might be that the column and values are mismatched.\n. LGTM, good catch!\n. wow. Is it hard to add a test?\n. Thanks for the test !\nLGTM.\n. I think this might be fixed with the recent changes. @gmelekh is it possible for you to try out cadvisor build from head?\n. LGTM, except nits from @vishh \n. We'll need to have access to /var/log/messages to have persistent OOM events info. We decided to keep an event stream for last 24 hours of sys and container OOMs.\n. I ran into this one today accidentally and the problem was that docker state directory was inaccessible. The newer version of cAdvisor capture the state directory state with /validate. @danielkraaij @jbdalido can you please try out the new version and hit /validate to see if you notice an error in 'Docker driver setup' section. Thanks!\n. I personally like landing on the root page. Is it possible to make docker\ncontainers more prominent on the root page and down play  the\n'subcontainers' section?\nOn Thu, Nov 13, 2014 at 10:05 AM, Sebastiaan van Stijn \nnotifications@github.com wrote:\n\nI like the idea and can confirm that the first time I used cAdvisor, I was\na bit confused (now, where are those containers? Why do I need to pick\n\"docker\"?)\nI do like to have a general overview at the machine level thought, so\nintegrating that is a nice suggestion.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/307#issuecomment-62937612.\n. I'd say punt on the second problem for now. I don't think it will be that\nconfusing. I propose fixing it on the first complaint :)\n\nOn Thu, Nov 13, 2014 at 10:23 AM, Victor Marmol notifications@github.com\nwrote:\n\nWe spoke offline and the idea of still showing /containers as the root,\nbut having a prominent \"Docker containers\" link seems to be winning. The\nonly concern is user confusion between the /docker subcontainer and the\n\"Docker containers\" link.\nWhat do you all think?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/307#issuecomment-62940469.\n. LGTM\n. Merging this one... Add cat gifs in a followup PR :)\n. :+1:  @mindscratch Thanks for fixing a big hole. I don't know when we would have gotten to it.\n. LGTM\n. We do deploy the docker image for each release as we think that's the\neasiest way to try out cadvisor.\n\nNo harm in putting out the binaries too.\nOn Nov 18, 2014 2:34 AM, \"Craig Wickesser\" notifications@github.com wrote:\n\nI believe it would be beneficial to users if pre-built binaries were\nprovided as part of the release. Kubernetes does this\nhttps://github.com/GoogleCloudPlatform/kubernetes/releases/tag/v0.5\nwhich makes it simpler to get started.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/313.\n. LGTM\n. LGTM.\n\nWe should probably wait a day or so before cutting another release. Let it soak a bit.\n. integration/api is failing to build.\n. LGTM\nThis is great. Will save us so much trouble!\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. github.com/google/cadvisor/storage/memory fails to build with head. Does it work on your local branch?\n. Squashed, with help from @vishh \n. LGTM\n. LGTM\n. LGTM\n. I can't seem to get Travis to tell me what failed :/\n. LGTM\n. Adding cpu speed is easy, but calculating usage from there might not be simple.\nMost of today's processors support frequency scaling which results in real cpu clock speed fluctuating a lot. For machine static information, we can add max cpu clockspeed. But that might not be useful in your usage calculations.\nHow are you planning to use clock speed to get usage %?\n. #375 Adds cpu frequency to machine state.\n. Same as #385 . Closing this one.\n. Sorry, I meant to close the CentOs bug.\nRHEL7 issue is still there as docker is running as a service and using systemd socket activation.\nDocker registration fails:\nDocker registration failed: unable to communicate with docker daemon: dial unix /var/run/docker.sock: permission denied.\nIs it because cAdvisor is running in a different IPC namespace?\n. Done. Thanks for the weekend review. I wasn't expecting to merge it till Monday :)\n. @josselin-c Can you check if \"/sys/fs/cgroup/cpu,cpuacct/system.slice\" directory exists? I think CentOS doesn't use systemd to control cgroups yet. Somehow, cAdvisor thinks it does.\nCan you also get the output for /validate from cAdvisor UI? Thanks. \n. I think this issue is same as #386 . cAdvisor is looking for cgroup paths at a weird place.\nI1222 08:14:31.887936 00001 container.go:192] Failed to update stats for container \"/system.slice/docker-7e1d4a00f57a7610d9feb4baabfc76a85ffa6d45561c21e635b54c832de98579.scope\": open /var/lib/docker/devicemapper/mnt/e1f3bea22425ba842c6ceb6d27c4cde6fd499d62e70aafcab17b183c2dfecb7a/rootfs/sys/fs/cgroup/cpu,cpuacct/system.slice/docker-7e1d4a00f57a7610d9feb4baabfc76a85ffa6d45561c21e635b54c832de98579.scope/cpuacct.stat: no such file or directory\n@josselin-c I think the fix for that would fix it too.\nJust to be sure, can you please grab the output for:\ncat /var/lib/docker/execdriver/native/*/state.json\n. @josselin-c Can you please try out 0.7.1 and let us know if that fixes this issue?\n. @yesnault Can you get us the first part of cAdvisor log and output for http://:8080/validate. Thanks.\n. @yesnault Can you also get the output for 'ls /sys/fs/cgroup/cpu'. It might be 'cpu,cpuacct' directory instead of cpu.\n. @yesnault Thanks for the helpful logs!\nI still need to repro this, but based on your logs - it looks like it is the problem with how cAdvisor handle libcontainer state.\nWe try to read libcontainer state here:\nif !utils.FileExists(self.libcontainerStatePath) {\n        if utils.FileExists(self.libcontainerPidPath) {\n            // We don't need the old state, return an empty state and we'll gracefully degrade.\n            return &libcontainer.State{}, nil\n        }\n    }\nFor older versions, we would return empty State and use it directly to report Stats:\n```\nfunc GetStats(state libcontainer.State) (info.ContainerStats, error) {\n    // TODO(vmarmol): Use libcontainer's Stats() in the new API when that is ready.\n    stats := &libcontainer.ContainerStats{}\nvar err error\nstats.CgroupStats, err = cgroupfs.GetStats(state.CgroupPaths)\nif err != nil {\n    return &info.ContainerStats{}, err\n}\n\n```\nWill try to verify it in a while and add in a fix.\n. @vishh does that sound right?\n. Although docker 1.3.1 should create the state file, it seems this is still a problem when we don't map cgroup base directory identically.\nI could reproduce this by mounting /sys/fs/cgroup at a different location with cgroup rootfs.\nWith the new change of reading libcontainer statefile, we try to use the cgroup path as it is visible to docker/libcontainer. eg, for centOS if cgroup outside containers is visible at /cgroup, and we mount /:/rootfs:ro, we would be able to see and access cgroups at /rootfs/cgroup. However docker sees them at /cgroup and that's what we find in the statefile.\nWe don't see this on systems where cgroups are mounted under /sys, because we identity mount /sys:/sys:ro irrespective of where the rootfs is.\n. @yesnault Can you give cadvisor:0.7.1 a try and see if it fixes the failure?\n. Adds an extra line, like:\n \"Cgroup mount directories: blkio cpu cpuacct cpuset devices freezer hugetlb memory perf_event systemd\"\n. Added /proc/mounts. Sample output:\nCgroup mount setup: [Supported and recommended]\n    Cgroups are mounted at /sys/fs/cgroup.\n    Cgroup mount directories: blkio cpu cpuacct cpuset devices freezer hugetlb memory perf_event systemd \n    Any cgroup mount point that is detectible and accessible is supported. /sys/fs/cgroup is recommended as a standard location.\n    Cgroup mounts:\n    systemd /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n. @vmarmol @vishh WDYT of setting GLOG_max_log_size to 100M in the mean time. I think glog just stops logging after the limit is reached. I need to try that out.\n. I tried it out and GLOG_max_log_size doesn't work at all. I looked through glog code and it doesn't seem to exist anymore. We'll need to do something within cAdvisor.\n. yeah, #391 fixes this error instance too.\n. @vishh This would only print the first error message per container. It does not distinguish between multiple error type and is only tied to the container. If a container throws multiple errors, only the first one will be logged.\n. @xinzhige localhost:8080 is going to hit your Mac OS and not the VM that cAdvisor is running in.\nCan you try connecting to cAdvisor through the ip in your DOCKER_HOST env?\neg: on my machine:\n$ echo $DOCKER_HOST \ntcp://192.168.59.103:2376\nI can reach cAdvisor on 192.168.59.103:8080\n. #394 adds explicit instructions on how to get to cAdvisor on boot2docker instance.\n. @vishh @vmarmol I tried this patch on both systemd and non-systemd machines and cAdvisor seems to work fine.  I'd do some more testing on systems with CentOs like setup.\nIs there any special use-case that was enabled by reading state.json paths? I'd like to test against that too.\n. This fixes #385 I think :) Still running through some tests.\n. systemd ones work fine because docker factory gets \"/system.slice/docker-.scope\" as the name of the discovered container. Appending that to the mount paths we found works fine in the cases we have encountered so far.\nThe only other benefit might be that if systemd only used selective cgroups, we would mistakenly tried to read others too. If that's a problem, we can only use the mountpaths that are present in state.json cgroup paths as keys.\n. I am going to merge it now. I have a list of scenarios to check. Will do\nthat before we try to cut a new version. Feel free to build a canary that\nothers can test against too.\nOn Tue, Dec 23, 2014 at 4:39 PM, Vish Kannan notifications@github.com\nwrote:\n\n@rjnagal https://github.com/rjnagal: Feel free to self merge once you\nhave successfully tested it on all supported distros.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/395#issuecomment-68014411.\n. LGTM\n. @sreuter Thanks for the fix.\nEven for small changes like this, we require you to please sign the Google CLA as it is a Google maintained project.\n\nDetails at:\nhttps://github.com/google/cadvisor/blob/master/CONTRIBUTING.md\n. @ahjdzx What information do you want to see for the physical host? I just added information about machine network stats to the root container. Is that the bit you are looking for?\n. We haven't made an official release with the machine network stats? Can you try jnagal/cadvisor:canary image? It should show you root stats on the /containers page.\n. @vishh I am just killing time. You can review these next year :)\n. Merging this one, since its reviewed already.\n. Thanks @vmarmol. Moved to a new dir.\n. Looks like there are two many filesystems detected on this machine :)\n@vishh @vmarmol WDYT of capping the filesystem gauges to 5? We can just use the first 5 for now.\nIf required, we can later change to largest 5.\n. Device mapper creates one or more fs objects per container. Added a static limit for now. It would be nice if we could move these objects from root and on to the page for the corresponding container.\nClosing this issue for now.\n. Fixes #406 \nWe still keep details about all filesystems in the 'Filesystems' section.\n. Thanks for reporting @josh-devops-center. It makes sense to create a simple InfluxDB container with the default settings. We'll work on adding one.\n. Closing this as we fixed the link.\n. Fixes #411 \n. I think I know what's going on. Will send a fix in the evening.\nOn Jan 3, 2015 12:49 PM, \"Vish Kannan\" notifications@github.com wrote:\n\nAssigned #414 https://github.com/google/cadvisor/issues/414 to @rjnagal\nhttps://github.com/rjnagal.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/414#event-213704798.\n. @richp10 Can you please try jnagal/cadvisor:canary image to verify the patch? Thanks.\n. Thanks for verifying @richp10 . We'll cut a release this week.\n. Fixes #414 \n. Can you provide the output for localhost:8080/validate?\nAre you seeing missing stats for root, or all docker containers? \n\nCan you also grab the output for localhost:8080/api/v1.2/containers? Thanks.\n. @tpires you are right. That's the problem. We'll add a fix and share a test image soon.\n. @tpires Can you try out the test image at jnagal/cadvisor:canary to verify the fix?\n. LGTM\n. Fixes #416 \nI could reproduce this problem with 0.7.1 and verify the fix.\n. Fixed the extra semi-colon. Merging.\n. LGTM.\nThis looks fine. I am slightly nervous of mis-using state for anything other than network devices. We should look for a way to hide that from rest of the code.\n. I liked the bigger ones better. But these are fine too - that's just probably my age :)\n. Do you want to change the location of vet in this PR or separately?\n. LGTM\n. LGTM.\nFeel free to merge.\n. doh. One minute is fine for now.\n. LGTM\n. Do we still have the issue with cAdvisor not able to get to docker socket\non Redhat?\nOn Mon, Jan 12, 2015 at 1:23 PM, Vish Kannan notifications@github.com\nwrote:\n\nMerged #434 https://github.com/google/cadvisor/pull/434.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/434#event-217675962.\n. Closing this for now, so it doesn't inadvertently get merged.\n. We can scan all ethernet devices and use the first one, instead of limiting to eth_. Can we ignore docker_, or shall we report them too? The current public API limits us to exporting only one i/f.\n. Reporting multiple i/f is easy. It's just a breaking change to the API, so I was hesitating a bit :)\n. Fixes #436 Verified on CoreOS.\n. Updated the failing tests :)\n. @jzy1688 We disabled load stats as we are re-working the implementation. It needs a few more tweaks before we can enable it again.\n. Sample output:\n\n\"disk_map\":{\n\"252:0\": {\"name\":\"dm-0\",\"major\":252,\"minor\":0,\"size\":107374182400,\"scheduler\":\"none\"},\n\"252:1\": {\"name\":\"dm-1\",\"major\":252,\"minor\":1,\"size\":10737418240,\"scheduler\":\"none\"},\n\"8:0\": {\"name\":\"sda\",\"major\":8,\"minor\":0,\"size\":10737418240,\"scheduler\":\"cfq\"}\n}\n. IMO, we should classify docker+lxc containers under raw handler. The logic we have for docker handler assumes libcontainer. We already capture the type of execdriver docker is using. \nWe might do a better docker+lxc support, but going to raw handler seems like it will fix reporting for these containers, right?\n. Ah, sorry. I completely mis-read that :)\n. LGTM\n. @vishh Can you try with the new head and see what errors are reported?\n. @kateknister This is the bug I was referring to. Please check if you can repro it when you re-enable cpu load.\n. Couldn't repro #452 but this might help get some error message instead of panic.\n. LGTM\n. Is that a panic or error? It looks like it is waiting for kernel to push some data to the netlink message queue. Is that timing out?\n. Can you add the panic string - out of bound error?\nOn Thu, Jan 22, 2015 at 10:10 AM, Victor Marmol notifications@github.com\nwrote:\n\nThis is a panic.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/457#issuecomment-71068356.\n. binary.Read is not returning, so the error paths are not going to be\ntriggered. I'll look at it today.\n\nOn Thu, Jan 22, 2015 at 10:34 AM, Victor Marmol notifications@github.com\nwrote:\n\nClosed #457 https://github.com/google/cadvisor/issues/457.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/457#event-223208021.\n. LGTM\n. That's true. I am trying to figure out a way to make netlink implementation\nhierarchical. One way is to run a similar housekeeping loop in there to\ndiscover all containers and return cached stats.\n\nWithout this fix, we can't use root load information anywhere up the stack.\nHow about switching to sched debug as the primary way till we get\nhierarchical behavior working with netlink?\nOn Mon, Jan 26, 2015 at 12:08 PM, Victor Marmol notifications@github.com\nwrote:\n\nThis is different with the netlink implementation no? Are we sure we want\nto have those be different? Seems like we won't be able to have a stable\nreturn in the API if that's the case.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/463#issuecomment-71528421.\n. Closing this. Sent another PR to make scheddebug the preferred option. Let's do another one to make all load stats hierarchical under scheddebug.\n. Is this expected to fail?\n\ncan't load package: package github.com/google/cadvisor/integration/runner: cannot find package\n. LGTM\n. vmarmol@ said that there's some flakes he's fixing. possibly related.\n. Withdrawing this for now as we integrate #495 \n. I can think of two ways:\n1. driver type prefixes like docker machine. I think its clean and is much easier to handle.\n2. Use --type and --storage_config=file. We can provide a single config file with all the info. We'll parse the config file based on type.\nI like 2 for conciseness. But I think there were some concerns on passing around files for deployment. If that is going to be inconvenient, let's go with 1.\n. ok, 1 it is then. Take it away @ankushagarwal !\n. Haven't looked at it before.\nIt seems like a straightforward storage driver. We'd love to get some patches for it :)\n. We can call this done with the v0.11.0\n. @vishh I think this captures what we discussed around stats summary.\n. LGTM\n. Keeping WIP as I might rip it out if the final stats don't make sense :)\n. LGTM\n. Can we check resource usage on a systemd setup? I was wondering if reporting cpu load hierarchically makes dynamic housekeeping slowdown less likely. \n. Do we know what it was with 0.8.0?\nOn Fri, Feb 6, 2015 at 3:36 PM, Victor Marmol notifications@github.com\nwrote:\n\nFrom my tests 3-6% on a system with O(30) containers. Mainly 3-4% with a\nspike every min to 6%. I think that's within bounds, we may need to do an\noptimization round in the near future.\nOn Fri, Feb 6, 2015 at 3:31 PM, Rohit Jnagal notifications@github.com\nwrote:\n\nCan we check resource usage on a systemd setup? I was wondering if\nreporting cpu load hierarchically makes dynamic housekeeping slowdown\nless\nlikely.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/493#issuecomment-73332692.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/493#issuecomment-73333220.\n. If the load is causing the housekeeping to change, we should just disable\nit rather than add a new flag.\nI think we can fix that easily by changing the dynamic housekeeping check\nto look at cpu and memory usage specifically rather than comparing the\nwhole set.\n\nOn Fri, Feb 6, 2015 at 3:45 PM, Victor Marmol notifications@github.com\nwrote:\n\nUuuu, ouch. So the current usage is ~1% with minute spikes to 3%. That's\ndefinitely a big jump in CPU usage. I will try to verify if this is due to\nCPU load. If so, we should add a flag to allow it to be disabled for the\nusers that worry about that.\nYou made a good point about the load maybe making it less likely that\nhousekeeping doesn't get lowered. I will check into that as well.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/493#issuecomment-73334249.\n. LGTM\n. merging...\n. Yes it should. It looks like shared_cpu_map is supported on older kernels. I didn't see a kernel that supports just shared_cpu_list and not shared_cpu_map. If that happens, se could check for both files.\n. There is no good cheap way to get disk usage from various docker storage drivers today.\n\nWould it help if we added a flag to disable disk usage reporting per container? We can still report the full disks usage and free space. We can add an on-demand endpoint to retrieve diskspace info per container if required.\n. Thanks for reporting @difro. We should filter these out.\n. Fixes #505 \n. Didn't notice #506 that's already out.\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM :)\n. @dchen1107 \n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. If we did a generic change of exporting aliases to storage backend,\nwouldn't it resolve this problem too?\nOn Fri, Feb 27, 2015 at 8:51 AM, Victor Marmol notifications@github.com\nwrote:\n\nToday we have a concept of \"namespaces\" for container names and those come\nwith a list of aliases used to address the container. Docker containers are\nunder the \"docker\" namespace and its aliases are the Docker ID and name.\nThis feature may be served by the \"marathon\" namespace and have the app ID\nas one of the aliases. This we will then push to the InfluxDB backend.\nWe probably don't have the bandwidth to tackle this anytime soon, but we\nwill gladly take PRs towards that goal if you'd like :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/546#issuecomment-76427469.\n. I see, the request is for exposing an env variable. This seems better handle in heapster aggregation where we can add arbitrary tags, or by exposing custom metrics hooks in cAdvisor.\n\n@samek We do want to custom hooks to add extra columns, but we'll probably not get to it in near future.\n. LGTM. I thought we require cpu in other places. I am curious how #544 failed, maybe a stale container. I'll look into that. But non-fatal sounds good.\n. It should work without docker. There is a special docker handler endpoint\n/docker where you won't see anything. cAdvisor will spew some errors that\nit didn't find docker, but would continue on.\nPlease file an issue if you encounter any problems running without docker.\nOn Fri, Feb 27, 2015 at 5:49 PM, Nick Stott notifications@github.com\nwrote:\n\nIs is possible to run cadvisor without docker, i.e. on raw lxc containers\n/ rocket / etc\nIt seems like there are explicit checks that explicitly require docker,\nand It's not clear how to get around that\nThanks,\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/551.\n. Even if its not enabled on root, we'd have to enable it on custom cgroup layout (when that gets it) to get pod usage.\n. LGTM\n. Good point. Will redo stats this week.\n. This is done.\n. LGTM\n. LGTM\n. fields of green :)\n. @vmarmol Let me know what you think about adding conversion layer vs changing the internal structs.\n. Ok. I think I am going to build v2 with v1 conversion and slowly wean off internal usage from v1 to v2.\n. @simon3z Can I change the semantic for machine-id a bit:\n- Make the default value for machine-id as empty.\n- if user sets a machine-id flag, use that.\n- Use /etc/machine-id if flag is not set.\n- if /etc/machine-id doesn't exist, use /var/lib/dbus/machine-id\n- return empty if none of these files exist.\n\nThis will add a machine id to other distros like ubuntu without requiring flag to be changed. SG?\n. @vmarmol that would work too. Let me do that.\n. e2e test seems to be flaky. Running it again fixed it.\n. Collecting custom metrics is a goal for cAdvisor, but right now we do push to collectors like you said.\nThe plugin in roadmap is supposed to add cAdvisor data with all the stuff that collectd already collects and push it out to backend storage.\nSo you are right, there is no pull backend for cAdvisor right now.\n. LGTM\n. discussed offline. closing.\n. LGTM.\nThe tests have been flaky lately. I don't think the failure is related. We should track the test flakiness in another bug.\n. Example /storage output ( I kept minimal information here. Let me know if you think something else should be added):\nubuntu w/ 1 disk:\n[\n{\"device\":\"/dev/disk/by-uuid/0e4daed1-3084-4d1f-be9594c1d5ceaf97\",\n\"mountpoint\":\"/\",\n\"capacity\":10534313984,\n\"usage\":8768860160,\n\"labels\":[\"root\",\"docker-images\"]},\n{\"device\":\"/dev/mapper/docker-8:1-1407781a6ff5d93152e8781d5cd39095c6711d0b4c9aa0d5a22cf341d56f40e2098e6c\",\n\"mountpoint\":\"/var/lib/docker/devicemapper/mnt/1a6ff5d93152e8781d5cd39095c6711d0b4c9aa0d5a22cf341d56f40e2098e6c\",\n\"capacity\":10434699264,\n\"usage\":26365952,\n\"labels\":[]}]\nubuntu w/ 2 disks:\n[\n{\"device\":\"/dev/disk/by-uuid/0e501679-6d73-47fa-9cf9-558872950040\",\n\"mountpoint\":\"/\",\n\"capacity\":10534313984,\n\"usage\":6669844480,\n\"labels\":[\"root\"]},\n{\"device\":\"/dev/sdb1\",\n\"mountpoint\":\"/var/lib/docker\",\n\"capacity\":10534313984,\n\"usage\":3308961792,\n\"labels\":[\"docker-images\"]}]\ncore-os:\n[{\n\"device\":\"/dev/sda9\",\"mountpoint\":\"/\",\"capacity\":7176433664,\"usage\":2078629888,\"labels\":[\"root\",\"docker-images\"]},{\"device\":\"/dev/sda3\",\"mountpoint\":\"/usr\",\"capacity\":1031946240,\"usage\":322973696,\"labels\":[]},\n{\"device\":\"/dev/sda6\",\"mountpoint\":\"/usr/share/oem\",\"capacity\":113229824,\"usage\":39419904,\"labels\":[]}]\n. Changed the API to be /storage?label=. I took the lazy way first as labels were the only static thing users would have. But querying by mountpoint sounds plausible for fs that we don't auto-label.\n. LGTM\n. This is already fixed in head by #570. cAdvisor from head should work. We'll make a release with the fix soon.\n. Looking for suggestions on /stats format.\nAlso I took the lazy option of including v1 into v2 to avoid writing deep conversion methods. Let me know if this is unacceptable :)\n. I was going to add recursive=true [default:false] when converting subcontainers.\nI can go with Victor's naming suggestion. Although I think we need a different term than namespace as it might diverge from the 'namespace' we use internally in the code. eg dockeralias. I don't like 'type' that much either.\n. OK, I'll go with type. There's still about a week to change the name before\nwe start using 2.0 :)\nOn Mar 12, 2015 9:38 AM, \"Victor Marmol\" notifications@github.com wrote:\n\nI don't think we have good data either way :) I don't doubt that most are\nDocker users though.\n@rjnagal https://github.com/rjnagal didn't like type :P\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/578#issuecomment-78520604.\n. Updated the API and introduced 'type'.\n\nGet root container stats as:\n/stats/\n/stats/?type=name\n/stats/?type=name&count=1\nNext additions:\n/stats/81fcc6b7b0b1c2367cb791?type=dockerid&count=1 (matches partial id)\n/stats/docker?type=name&count=1&recursive=true\n/stats/grumpy-netwon?type=dockeralias\nAs always, open to better keywords and option names suggestions :)\n. we can combine id and alias into one. My only argument was better error returns, but that's not a big deal.\n. LGTM\n. LGTM.\nI thought this was the only information we wanted to depend on. But I guess not anymore after kublet integration.\n. you guys are confusing me :) \nSo do I hear three? /machine and /version as it is today and /attribute that combines the two and adds more stuff. /attribute would be the more dynamic one?\n. Ah, I see. I'd still like /attribute to return cpu_frequency, topology, and uuids, if not the capacity fields.\nWe don't have a /version today, but we can add the one as you suggested.\n. ok, new set of changes:\n- /machine : same as v1.2\n- /version : returns cadvisor version\n- /attributes : returns the originally proposed /machine (union of versionInfo and machineInfo)\n. LGTM\nThanks @juliusv \n. LGTM\nThanks for the fix @vmarmol. I noticed the same while making 2.0 changes, where I just used a map.\n. LGTM\n. PTAL. Moved all containerData references back to manager.\n. LGTM.\nSounds good. Let's get the other changes in later.\n. LGTM\n. fixed\n. Let me bring up an instance while travis is still working on it.\n. Hit the same error as travis. @kargakis Can you run godep save ./...\n. testing locally now\n. LGTM.\nWorks great. Thanks @kargakis \n. @kateknister I think this looks good, except the one nit about unused var.\n. LGTM\n. Looks like one of the tests broke:\ngithub.com/google/cadvisor/client\nclient/client.go:50: undefined: events.Event\nclient/client.go:53: undefined: events.Event\ngithub.com/google/cadvisor/client\nclient/client.go:50: undefined: events.Event\nclient/client.go:53: undefined: events.Event\n?       github.com/google/cadvisor  [no test files]\n. Returning all stats doesn't seem like a good default, as it can change based on our memory storage/backend setup.\nDoes it fix a real problem?\n. Separating out time and count queries sounds fine. Not receiving a ton of\nstats by default would be nice. When we are using an external storage, will\nit try to get all stats from backend?\nOn Wed, Mar 25, 2015 at 11:17 AM, Victor Marmol notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh I can see the usecase where someone\nwants to get the 10 stats before a certain time or limit the number of\nresults. But it does feel like a better default for time-based queries is\n\"all\" rather than an arbitrary limit. That was the logic behind this PR. It\nwould be simpler if we returned \"all\" by default in our cases. WDYT\n@rjnagal https://github.com/rjnagal? It may be specific to the flags on\nthe memory storage, but it'll provide consistent behavior.\nAn alternative is to say that the default when specifying both start and\nend is \"all\" unless otherwise specified.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/615#issuecomment-86156727.\n. SG.\n\nOn Wed, Mar 25, 2015 at 12:39 PM, Victor Marmol notifications@github.com\nwrote:\n\nWe no longer fetch stats from external drivers. We just use them for\nexport.\nI'm fine with specifying: default NumStats = 60 and if Start and End are\nspecified NumSats is ignored.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/615#issuecomment-86187427.\n. LGTM\n. LGTM\n. Can you check the number of cores reported by cgroups? Looks like we are picking an invalid number there.\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. Sent a PR to fix head. Will merge after that.\n. @vmarmol all green.\n. You can use bernardo/stress images.\nMemory options:\n-m, --vm N         spawn N workers spinning on malloc()/free()\n     --vm-bytes B   malloc B bytes per vm worker (default is 256MB)\n     --vm-stride B  touch a byte every B bytes (default is 4096)\n     --vm-hang N    sleep N secs before free (default is none, 0 is inf)\n     --vm-keep      redirty memory instead of freeing and reallocating\n\nYou can consistently hit the memory limit of a container with --vm-keep.\n. LGTM.\nWe can improve it further in separate PRs.\n. The container endpoint looks for containers under the specified cgroup path - in your case /docker/2da.... This is the right path under ubuntu and other non-systemd systems, but not for systemd machines like CoreOS.\nYou are better off using the docker endpoint which will work for both type of machines. The following snippet works on both machines:\n```\npackage main\nimport (\n        \"fmt\"\n        \"github.com/google/cadvisor/client\"\n        info \"github.com/google/cadvisor/info/v1\"\n        \"log\"\n)\nvar (\n        cont = \"ef5ab94e8257868f117f942ef004097e0a415a2c66502ce6aeb74f58c55fdeba\"\n)\nfunc main() {\n        cadvisorClient, err := client.NewClient(\"http://localhost:8080/\")\n        if err != nil {\n                log.Fatal(\"Could not create NewClient\")\n        }\n    request := info.ContainerInfoRequest{NumStats: 1}\n\n    cInfo, err := cadvisorClient.DockerContainer(cont, &request)\n    if err != nil {\n            log.Fatalf(\"Could not get container info: %v\", err)\n    }\n    fmt.Println(\"\\ncInfo =\", cInfo)\n\n}\n```\n. awesome! thanks :+1: \n. @feiyang21687 Are you still planning to work on this?\n. @vmarmol Did you fix the network stats already?\n. No rush, I was just wondering why the tests are passing.\n. Hot is the working set - pages that has been recently touched as calculated\nby the kernel.\nTotal includes hot + cold memory - where cold are the pages that have not\nbeen touched in a while and can be reclaimed if there was global memory\npressure.\nI don't think the total on cadvisor page includes swap, but we should\nprobably report that too.\nOn Wed, Apr 8, 2015 at 3:17 PM, Vish Kannan notifications@github.com\nwrote:\n\n\"hot\" refers to the working set, which is held in RAM, and \"total\" refers\nto all the memory used by a process, a part of which may reside on a swap\ndevice.\n+1 to your suggestion of documenting it. AFAIK this isn't documented\nanywhere.\nOn Wed, Apr 8, 2015 at 1:45 PM, Sean Humbarger notifications@github.com\nwrote:\n\nWhen looking at the total memory usage of a running container, the legend\nshows \"hot\" and \"total\". Is there any documentation that explains the\ndefinition \"hot\" and \"total\" and how that is calculated?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/638.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/638#issuecomment-91052608.\n. @kateknister \n. @kateknister The events integration test is failing:\n--- FAIL: TestStreamingEventInformationIsReturned (79.28s)\\n\n\\tevent_test.go:38: Started event streaming with error json: cannot unmarshal array into Go value of type v1.Event\\n\n\\tassertions.go:156: \\r                        \\r\\tLocation:\\tevent_test.go:39\\n\\t\\t\\r\\tError:\\t\\tNo error is expected but got json: cannot unmarshal array into Go value of type v1.Event\\n\\t\\t\\r\\n\n\\tframework.go:294: Ran \\\"sudo\\\" [docker run -m=4M --name test-basic-docker-container-3062 bernardo/stress --vm 4 --vm-keep -q -m 1000 --timeout 10] in \\\"10.240.129.120\\\" and received error: \\\"exit status 159\\\". Stdout: \\\"\\\", Stderr: INFO: Zone for cadvisor-ubuntu-precise detected as us-central1-f.\\n\\t\\tINFO: Running command line: ssh -o UserKnownHostsFile=/dev/null -o CheckHostIP=no -o StrictHostKeyChecking=no -i /var/lib/jenkins/.ssh/google_compute_engine -A -p 22 jenkins@146.148.64.95 -- sudo docker run -m=4M --name test-basic-docker-container-3062 bernardo/stress --vm 4 --vm-keep -q -m 1000 --timeout 10\\n\n\\t\\tWarning: Permanently added '146.148.64.95' (ECDSA) to the list of known hosts.\\r\\n\n\\t\\tWARNING: Your kernel does not support swap limit capabilities. Limitation discarded.\\n\nstress: ../nptl/sysdeps/unix/sysv/linux/x86_64/\n../fork.c:141: __libc_fork: Assertion `({ __typeof (self->tid) __value; if (sizeof (__value) == 1) asm volatile (\\\"movb %%fs:%P2,%b0\\\" : \\\"=q\\\" (__value) : \\\"0\\\" (0), \\\"i\\\" (__builtin_offsetof (struct pthread, tid))); else if (sizeof (__value) == 4) asm volatile (\\\"movl %%fs:%P1,%0\\\" : \\\"=r\\\" (__value) : \\\"i\\\" (__builtin_offsetof (struct pthread, tid))); else { if (sizeof (__value) != 8) abort (); asm volatile (\\\"movq %%fs:%P1,%q0\\\" : \\\"=r\\\" (__value) : \\\"i\\\" (__builtin_offsetof (struct pthread, tid))); } __value; }) != ppid' failed.\\n\\t\\t\n\nstress: ../nptl/sysdeps/unix/sysv/linux/x86_64/\n../fork.c:141: __libc_fork: Assertion `({ __typeof (self->tid) __value; if (sizeof (__value) == 1) asm volatile (\\\"movb %%fs:%P2,%b0\\\" : \\\"=q\\\" (__value) : \\\"0\\\" (0), \\\"i\\\" (__builtin_offsetof (struct pthread, tid))); else if (sizeof (__value) == 4) asm volatile (\\\"movl %%fs:%P1,%0\\\" : \\\"=r\\\" (__value) : \\\"i\\\" (__builtin_offsetof (struct pthread, tid))); else { if (sizeof (__value) != 8) abort (); asm volatile (\\\"movq %%fs:%P1,%q0\\\" : \\\"=r\\\" (__value) : \\\"i\\\" (__builtin_offsetof (struct pthread, tid))); } __value; }) != ppid' failed.\\n\\t\\t\nstress: ../nptl/sysdeps/unix/sysv/linux/x86_64/\n../fork.c:141: __libc_fork: Assertion `({ __typeof (self->tid) __value; if (sizeof (__value) == 1) asm volatile (\\\"movb %%fs:%P2,%b0\\\" : \\\"=q\\\" (__value) : \\\"0\\\" (0), \\\"i\\\" (__builtin_offsetof (struct pthread, tid))); else if (sizeof (__value) == 4) asm volatile (\\\"movl %%fs:%P1,%0\\\" : \\\"=r\\\" (__value) : \\\"i\\\" (__builtin_offsetof (struct pthread, tid))); else { if (sizeof (__value) != 8) abort (); asm volatile (\\\"movq %%fs:%P1,%q0\\\" : \\\"=r\\\" (__value) : \\\"i\\\" (__builtin_offsetof (struct pthread, tid))); } __value; }) != ppid' failed.\\n\\t\\tstress: FAIL: 1 <-- worker 5 got signal 9\\n\\t\\tstress: \nFAIL: 1 <-- worker 6 got signal 9\\n\\t\\tstress: FAIL: 1 <-- worker 7 got signal 9\\n\\t\\tstress: FAIL: 1 <-- worker 8 got signal 9\\n\\t\\tstress: FAIL: 1 <-- worker 9 got signal 9\\n\\t\\tstress: FAIL: 1 <-- worker 10 got signal 9\\n\\t\\tstress: FAIL: 1 <-- worker 11 got signal 9\\n\\t\\t\n. @cadvisorJenkinsBot test this please\n. LGTM.\nMerging it. @kateknister is going to add the missing piece later.\n. I think the message should not say \"Failed to...\"  and not be an error :)\nThe message should make clear that we are degraded gracefully. I'll add a\nfix.\nOn Tue, Jun 9, 2015 at 9:35 AM, James Cuzella notifications@github.com\nwrote:\n\nMy thoughts are that this is \"not a bug\"... It appears that @pbhavsar211\nhttps://github.com/pbhavsar211 just may have not understood that\nboot2docker is not using systemd as it's init system. It also is good to\nknow that the message:\nFailed to start OOM watcher, will not get OOM events: exec: \"journalctl\":\nexecutable file not found in $PATH\nIs really just a warning and that cAdvisor gracefully degrades.\nPerhaps really there is a hidden desire for a feature request here to add\nOOM-watcher support for old SysV-style init systems?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/645#issuecomment-110425274.\n. We enabled load reader and katie saw the panic with iobuf. Can we exclude that from the release?\n. LGTM\n. LGTM\n. I'll take this over, rebase, and re-push. closing for now.\n. LGTM. Thanks!\n. LGTM\n. LGTM\n. LGTM\n. Does this flag applies to stats as well as events?\n. LGTM\n. event tests failed.\n. LGTM\n. LGTM\n. LGTM\n. Its adding the status for 825 to this PR :(\n\nOn Thu, Apr 23, 2015 at 12:00 PM, Victor Marmol notifications@github.com\nwrote:\n\nJenkins passed: http://104.154.52.74/job/cadvisor-e2e/824/console\n(unsure why it is not updating GitHub :( )\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/663#issuecomment-95686261.\n. LGTM\n. Why do it by time? The most common case we want to avoid is the crashlooping containers flooding events.\n. This is better than what we have. But the memory bounds are still worrisome. I'd say we should place an absolute limit based on amount of memory we need to store these.\n. Can you also look at the test failure to see if its related?\n. LGTM\n. LGTM\n. Just ignoring housekeeping still shows the container in all APIs and UI. Its a query-able container that doesn't return anything. \n\nUploaded a different fix that distinguish failed vs ignored containers in manager and silently fails the latter. PTAL.\n. LGTM\n. LGTM, with minor nits.\n. LGTM\n. LGTM.\nHope this will help\n. LGTM\n. LGTM\n. LGTM\n. Updated, thanks for noticing.\n. We only return the first network interface's data right now.Filed issue #686\n. We skip docker0,lo,  and all veth for the root. So it will be one of eth0,\neth1, dummy0, or weave : likely dummy0\nOn Wed, May 6, 2015 at 9:39 AM, Feng Honglin notifications@github.com\nwrote:\n\nIn one of my VMs:\nls /sys/class/net\ndocker0  dummy0  eth0  eth1  lo  veth1e3d936  veth3751dd9  veth3882a0e  veth3f1c930  veth3fda928  veth446bad0  veth537ab23  veth752d712  veth826ff72  veth9b00ba7  vetha5d75f0  vetha6531d3  vethc91e7de  vethf266906  vethf41ce83  vethwepl20171  weave\nthe first interface is docker0\nso cadvisor returns the statistics on docker0, right?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/684#issuecomment-99532446.\n. shouldn't that be total_inactive_file?\n\nOn Mon, May 11, 2015 at 9:37 AM, Victor Marmol notifications@github.com\nwrote:\n\nTo calculate working set we do:\nusage - total_inactive_anon - total_active_file\n267,001,856 - 225,005,568 - 45,862,912 = -3,866,624\nThat probably gets overflowed to intmax somewhere. I will send a PR to cap\nthat at 0, but we also need to look into our logic here. We're probably\ndouble-counting some memory.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/685#issuecomment-100973435.\n. It still looks like an issue, even if we account for atomicity between reading usage and detailed stat. We would still expect (total_cache + total_rss) > (total_inactive_anon + total_inactive_file).\n. updated the docs.\n. LGTM\n. /docker is broken as the change treats it as a container endpoint (container/docker).\nGithub is also not able to render container_js.go file, so something might have gone wrong with the git merge.\n\nWill fix and reapply.\n. LGTM, one nit.\n. LGTM\n. Looks great, just minor nits.\nCan you also squash your commits. Thanks @superzhaoyy !\n. LGTM\n. merging without test, since this is doc only.\n. Fixed\n. Fixes #697\n. I don't know why Jenkins is also pulling in the redis patch with yours:\nstatus 1 and output: \"storage/redis/redis.go:5:2: cannot find package \\\"github.com/garyburd/redigo/redis\\\" in any of:\\n\\t/usr/local/go/src/github.com/garyburd/redigo/redis...\nCan you kick it again?\n. LGTM\n. Thanks for trying out the new docs and fixing them @furlongm \nLGTM\n. Verified with 1.6.2-rc1 :+1: \n. @anushree-n this is another good issue to work on for learning the UI setup.\n. @vmarmol got it already :)\n. I added it to containers_js.go instead:\nconst containersJs = gchartsJs + `\nfunction humanize(num, size, units) {...\nWe still just pull containers_js.  I just separated them out for readability.\n. @anushree-n this might be a good first change to work on. Victor and I can give you more details once you are done with the docker prep. Thanks!\n. kernel uptime seems to start earlier than the one reported in uptime. We also might have a drift due to suspend events. So, the kernel time is still more accurate but its hard to convert back to walltime. The current solution is much simpler. We should just stick to that.\nThanks for running the experiments, @anushree-n !\n. The comments don't make it clear, but all the mentioned interfaces are meant for raw namespace (full path). Since that is the default namespace, whenever a method takes in a container name as a string, it is assumed to be in default namespace and we trivially convert it to a namespaced name by not filling in the namespace.\nI'll add comments around the code to reflect that.\n. closing this one for now.\n. LGTM\n. LGTM\n. Thanks for filing the issue, @rosskukulinski \nDo you collect any container data in statsd today? What's the preferred metric format for statsd\ncadvisor.. ( we can remove cadvisor from the name if that's not useful).\nWe don't know much about statsd. Are there any restrictions on character sets that can go in the metric name? Is \"cadvisor./.cpu_usage\" a valid name?\n. Statsd support is in.\nRemaining items:\n-  [ ] Update storage backend docs (mostly copy-paste from @jmaitrehenry's example above)\n-  [ ] Add docker-compose config to bring up cadvisor+statsd+graphite .\n. Smaller PRs are always better. I'd go with 2.\nAnd thanks for adding the Power support!\nOn May 21, 2015 4:38 AM, \"Pradipta Banerjee\" notifications@github.com\nwrote:\n\nCurrently cadvisor fails to run on Power servers due to differences w.r.to\n/proc/cpuinfo output and location of system UUID data.\nI have two small fixes to get cadvisor running on Power servers. I was\nwondering whether to send one PR with fix for both the issues or different\nPRs ? Would really like to get the community's thoughts on the same.\nFor reference, here are the two commits in my private tree\nbpradipt@7f43a62\nhttps://github.com/bpradipt/cadvisor/commit/7f43a62ab694478a558afc00706771271f16005d\nbpradipt@a8eb38c\nhttps://github.com/bpradipt/cadvisor/commit/a8eb38c39b7c0bd9bf6d308b5821d3f7c82c48c9\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/725.\n. LGTM\n. I think we can redo inotifyWatcher logic to provide a smaller interface later. I am fine with the current code in the meantime.\n. LGTM\n. ok to test\n. LGTM\n\nThanks @bpradipt !\n. LGTM\n. updated.\n. LGTM\n. LGTM\n. LGTM\n. cAdvisor is designed to run on a single machine. We have another tool that collects container data from multiple cAdvisors on different machines. Check out heapster: https://github.com/GoogleCloudPlatform/heapster\n. We just started looking into it. Thanks for reporting the issue. \n@anushree-n we will probably need to consider both new and old influxdb API while updating the metrics  schema.\n. Thanks @mnuessler for taking it up! It can wait a few days.\nWe are in process of removing RecentStats() as we don't use it anymore. I'll send a PR for removing it today. \n. Looks like influxdb broke the older version, so we can ignore that too.\n. fyi @mnuessler \n. yeah, cache seems pretty straightforward. Might get to it today :)\n. LGTM\n. fixed.\n. Fixed the namespace detection logic to look for /rootfs/proc instead.\n. LGTM\n. sysinfo test needs update:\n? github.com/google/cadvisor/utils/sysfs/fakesysfs [no test files]\nFAIL github.com/google/cadvisor/utils/sysinfo [build failed]\n. sysinfo test  needs update:\n?       github.com/google/cadvisor/utils/sysfs/fakesysfs    [no test files]\nFAIL    github.com/google/cadvisor/utils/sysinfo [build failed]\n. LGTM\n. LGTM\nthis is pretty cool and clean! Thanks @vmarmol \n. LGTM\n. LGTM\n. @AnanyaKumar that sounds mostly right.\n- we can do all of it in v2.0 api. We don't need to duplicate it in two places.\n- we'll also need cloudprovider.BAREMETAL. We can look up some h/w info to check if we are on bare-metal.\n- Don't worry about the GUI, just add it to the API. Some day, we'll do a \"machine\" page for cAdvisor. This info can show up there.\nLet me know if you have other questions.\n. +1 send a PR :)\nOn Tue, Jun 9, 2015 at 8:45 AM, Victor Marmol notifications@github.com\nwrote:\n\n@AnanyaKumar https://github.com/AnanyaKumar would you like to document\nthe requirements in the building docs page? :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/761#issuecomment-110408266.\n. helpful for #645 \n. I signed it\n. Thanks for the reminder! I thought Victor merged this already.\n. LGTM\n. wow, this is embarrassing :)\n\nLGTM \n. LGTM\n. minor code movement.\nLGTM, otherwise.\n. I think this is fine.\nCan you squash the commits?\n. This bit of code is in kubelet. It creates a new cgroup and tries to move docker processes there.\nI think the problem is that kubelet is execing pidof docker to find docker processes. In cadvisor, we mount system root at /rootfs and use that to proxy process listing.\nkubelet can do the same with chroot /rootfs pidof docker\nThree lines of change, but needs to go in kubelet :)\nAre the namespaced kubelet changes getting in? I can make the kubelet change if someone is willing to merge it in.\n. Thanks for reporting, @cornelius-keller \nwhat cadvisor version are you running? Can you get host:port/validate for cadvisor?\nIs this  a temporary situation, or does the container fs stays busy till you delete cadvisor? \n. Yeah, I just need the ouput from /validate endpoint on cadvisor UI. You can\nscrub any data that's private in there. Thanks\nOn Fri, Jun 12, 2015 at 9:54 AM, Cornelius Keller notifications@github.com\nwrote:\n\n@rjnagal https://github.com/rjnagal\nCadvisor version is:\n[root@583274-app35 ~]# docker images\nREPOSITORY                                      TAG                 IMAGE ID            CREATED             VIRTUAL SIZEdocker.io/google/cadvisor                       latest              399ae3c46a0e        47 hours ago        19.89 MB\n[root@583274-app35 ~]#\nThis is a permanent situation. The container fs stays busy untill I delete\ncadvisor.\nWhat do you mean by getting host:port/validate for cadvisor? Cadvisor was\nstill running and responsive on the web ui if that is what you mean.\nUnfortunately I can't give you any public host port to validate as cadvisor\nis only exposed via a vpn.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/771#issuecomment-111555689.\n. @vishh Is this fixed if we just stopped tracking disk metrics for these machines? Are there other dependencies?\n. Can we do that by default when we detect devicemapper?\n. Thanks for improving the doc.\n\nJust one minor nit. And you should also squash the two commits into one before we merge.\n. LGTM\n. I think you are missing \"--volume=/cgroup:/cgroup:ro\"\nFrom https://github.com/google/cadvisor/blob/master/docs/running.md (for CentOS):\nYou may need to run the container with --privileged=true and --volume=/cgroup:/cgroup:ro \\ in order for cAdvisor to monitor Docker containers.\nLet us know if that works.\n. ok to test\n. Thanks for the patch @konukhov \nCan you run gofmt on the changes and add a doc on setting up cadvisor+reimman?\n. Thanks for the changes, @konukhov .\nJust an update: Most of us are busy with dockercon this week. I'll probably get to the review by end of week. I also want to try bringing up Reimann and cadvisor as containers together for quick verification. If you have a dockerfile or compose config to test them together, please drop it here too.\n. I feel that this is going to be really hard to maintain. We would need to add versioning to our documentation and the whole process.\nIf you would like to maintain the docs, you can host it somewhere and we can add a link to it.\nThanks for the PR.\n. @Marmelatze Let's split this up. I think image name makes a lot of sense. We can merge that quickly.\nDo the containers in Mesos use labels - something more structured than env variables and intended for exporting? We can pick up runtime image labels too for identifying the apps.\n. This looks fine with the whitelist. Can you rebase? Thanks.\n. LGTM.\nCan you look at test failure?\n. +1. I wanted to get to this one too.\n. LGTM\n. small naming nit, LGTM\n. LGTM\n. Thanks @hookenz We'll update heapster documentation.\n. ok to test\n. Thanks for the PR, @jmaitrehenry \nIt looks great overall. Will try it out soon !\n. LGTM.\nThis works fine' thanks @jmaitrehenry (and sorry for the delay).\nLet's squash the PRs and its ready to merge.\n. Thanks, merging.\n. IIRC, after merging this, we end up dropping support for older influxdb formats. Is that right?\nWe have quite a few users at the older version. @vishh do you think we need to support both versions for a while, or is it okay to drop the old one?\n. Discussed offline. It's fine to drop the older version support.\nI took another shot at reviewing this. I can't make sure if we need all these dependencies (soo many!).\nThe storage driver changes look okay though.\nI'll setup influxdb 0.9 and give it another try. Thanks for your patience!\n. @vishh Do you want to try this out? I am fire-fighting this week :sob: \n. LGTM\n. ok to test\n. @anushree-n Can you rebase and squash?\n. LGTM\n. LGTM\n. ok to test\n. one bit, but looks mostly good.\nThanks for cleaning it up. All that code never belonged in manager :+1: \n. LGTM\nlooks great. Let's squash and merge!\n. You can just ignore the gui as it should not take up much resources if you don't hit it. Is there a specific problem with the gui for your usecase?\nWe currently don't have a way to disable it.\n. oh, you can just use the cadvisor api for that: https://github.com/google/cadvisor/blob/master/docs/api.md\nno need to disable gui.\n. ok to test\n. small nit on not keeping errored collectors around. looks okay otherwise.\n. LGTM\n. LGTM, thanks for fixing it !\n. ok to test\n. @vmarmol yes, mockfs.go\n. this change didn't delete the gomock from code.google.com though.\n. Can you address the nit about adding comments and rebase?\n. LGTM\n. LGTM\n. LGTM\n. LGTM with a small nit.\nI think this changes is good for cAdvisor in general.\nI haven't yet found a good concise way to put these numbers up on cAdvisor UI (plain table is ugly).\nTake a stab at it if you get some time, @mvdan \n. The UI comment was an aside, it can be done separately.\nWe currently get 90p for an hour by using 90p values from last 60 mins sample. If we do the same logic for 95p and 50p, we'll burn a it more memory. I think for now, we only collect up to 60 mins samples, so its not that bad. If we did a day worth of samples per container, then it would become noticeable.\n. ok to test\n. Small nits, but looks good overall.\n. I think we can try to get stats from libnetwork if reading the state file fails.\n@jimmidyson would you like to try a PR for that? Thanks!\n. That's the one reason I slightly liked reading network stats from\n/proc//root/sys/class/net/...\nWe don't have to drop netns for cadvisor.\nBut if we do decide to drop netns, we can also start getting per-container\ncpuload from netlink. I'll let vishh decide.\nOn Fri, Aug 28, 2015 at 7:55 AM, Jimmi Dyson notifications@github.com\nwrote:\n\n@lukaf https://github.com/lukaf Cool! We might revisit this for a\n0.16.1 release if an alternative approach comes along, but it will keep\nworking no matter what now :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/822#issuecomment-135796946.\n. We already have root filesystem mapped in as a volume on /rootfs (so we use\n/rootfs/proc//root/sys/class/net). its used in many places within\ncadvisor.\n\nThe netlink comment isn't relevant for networking stats. There's netlink\ncommand to get taskstats which gives us cpuload per container. It only\nworks when cadvisor is running under root netns, so we have it disabled for\nnow.\nOn Fri, Aug 28, 2015 at 8:19 AM, Jimmi Dyson notifications@github.com\nwrote:\n\n/proc//root/sys/class/net/ won't be available unless we're running\nhost pid namespace anyway so it's no better/safer when running in a\ncontainer. Mounting proc will screw the cadvisor container up, if it's\neven possible. Could mount host /proc somewhere else in the container I\nguess but that doesn't feel nice to do.\nWhat do you mean \"via netlink\"? Perhaps that's an option but don't know\nenough about it.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/822#issuecomment-135803277.\n. @anushree-n  I think this is the model we want to go with. WDYT?\n. yeah, the intention is to not go below the housekeeping interval.\n. self-merging ....\n. I worry that having a makefile would encourage people to make regular \"godep go ...\" not work. We'll also force go purists to install make :)\n\nThese are minor complaints. I won't be too sad if this got in. My build/test alias are still shorter than make test :)\n. This looks great.\nLGTM.\nSquash and lets commit.\n. We did bits of these in #838 \nClosing this one. Lets do the remaining part - handling counter, rate - as a new PR.\n. Thanks for reporting @KevinPike It's missing a conversion. Sending out a PR now.\n. fixes #834 \n. @anushree-n @vmarmol Still a bunch of cleanup to do, but I think this is a more condensed format for stats and would be easier to handle.\n. Chatted with Victor. He has no big issues with the change as such. Merging it and following up with other PRs to resolve issues around namespacing and timestamp alignment.\n. @anushree-n This invalidates existing scripts because:\n- we renamed units to data_type.\n- dropped supporting both int and integer, just using int.\njnagal:nginx has the updated config.\n. LGTM\n. @vmarmol With prometheus, there are tons of metrics that are reported and putting them up in UI isn't that useful.\nI would like to condense UI output in three ways:\n- Add a way to remove a metric from UI (like we do with pgfaults for regular metrics).\n- Add a tag to group multiple metrics on to the same graph (eg read and write connections for nginx)\n- Add a drop down to change metrics in a graph (eg make all labels for a metric grouped as drop-down in prometheus)\nWe should also probably have a cap of graphs we show on a page to keep it responsive.\nLet's discuss ways to do it without API input.\n. Need to think of a different way to do it. Closing this one for now.\n. Thanks for reporting @aecolley. The regex looks okay. Feel free to send a CL, or I'll do it soonish :)\nWe'll cut a new release within a week\n. verified on centos6.\n. Fixes #843 \n. LGTM\n@jimonreal Can you squash your commits? This is ready to merge, thanks.\n. Thanks @jimonreal and @mvdan for multiple iterations.\nLGTM\n. LGTM\n. looks okay... Can you please squash the commits?\n. LGTM\n. ok to test\n. LGTM\n. ok to test\n. go vet error:\n+go vet github.com/google/cadvisor/...\n+godep go test -v -race -test.short github.com/google/cadvisor/...\ngithub.com/google/cadvisor\n./storagedriver.go:91: not enough arguments in call to stdout.New\n. When cadvisor is running in a container, os.hostname() is not going to return the hostname of the physical host. Is that acceptable for this use-case? It will still be different for each cadvisor instance.\n. Thanks @yifan-gu \nCan you add some details on ways to inspect rkt containers?\n- Query to figure out if a given container name was created by rkt.\n- Getting any container spec / config? We get resource limits from cgroups, but network setting etc needs to come from the container provider - an equivalent of docker inspect.\n- Any event stream for rkt container events?\n- Equivalent of labels\n- Way to list containers created by rkt.\n- Any way to get filesystem limits and usage, if applicable.\n- nice to have: Way to know what images are being used by a container, and what rkt images exist on the machine.\n. @yifan-gu By label, I meant if there's a way to add metadata (in image or runtime) for a rkt container, we would want a way to api to discover it. \n. @yifan-gu That sounds ok. Our meta-data use-case should not be a blocker for rkt support. Let us know what you need from us.\n. LGTM.\nThanks for fixing it, @pwittrock !\n. LGTM\n. Are you planning to use the /stats api in kubelet or add a different one for instantaneous stats? I thought you were already change kubelet to have its own native data-structures. If you are going to end up doing translation, I'd prefer to do that with v2. If you are re-using cadvisor structs, then do the minimal work to get you going.\nI'd be fine with both.\n. LGTM\nThanks for fixing the v2 stuff too!\n. fixed by #860 \n. @vmarmol This fixes empty network display for kube containers.\n. I think there's a lighter approach than switching the network namespace.\nIf you can get a pid for a process running in the docker container, we can read stats using the root view of that pid:\neg. from inside a container:\n$ docker exec -ti  3ae8d35423f8 /bin/sh\n   # cat /sys/class/net/eth0/statistics/tx_bytes \n313707668\n  # cat /sys/class/net/eth0/ifindex \n3824\nFrom outside, using a pid (we already get this in cadvisor)\n  # cat /proc/24342/root/sys/class/net/eth0/ifindex \n3824\n  # cat /proc/24342/root/sys/class/net/\neth0/ lo/\n  # /proc/24342/root/sys/class/net/eth0/statistics/tx_bytes \n313708886\nWDYT?\n. LGTM\nThanks @jimmidyson We already expose labels. The only remaining discussion is around env variables, which I am still not convinced is safe to do.\n. This is a warning that we will not be able to report some of the stats that we only get from docker using the libcontainer exec driver. You should still be able to see regular cpu, memory stats, but they show under \"subcontainers\" rather than docker containers (eg. /lxc container in the logs above).\nYou can ignore these errors if the stats you are interested in are still showing up.\n. LGTM\nwish ELK dependency had more fine grained packages for us to import :cry:\n. ok to test\n. LGTM\n. Thanks for reporting @onlyjob \nIt's being addressed in #800 \n. ok to test\n. LGTM\n. @vishh start on application-metrics. Will drop a few more examples and snapshots before linking to the main page.\n. new release 0.18.0 is up. Thanks for your patience!\n. Moving UI to use v2 instead of v1.\nOn Thu, Sep 24, 2015 at 2:56 PM, Vish Kannan notifications@github.com\nwrote:\n\n@rjnagal https://github.com/rjnagal: What is pending to migrate over to\nv2 completely?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/891#issuecomment-143060912.\n. @vishh \n. Will add the tag once this is merged.\n. We never patch branches anyway. It seems like an overkill. I'd expect packaging managers to keep branches of their official releases.\n. @vishh didn't notice we tagged 0.17.0 last week. Re-doing release.\n. +1 to what @jimmidyson said. Many users embed cadvisor into their own products - kubernetes/kubelet being one of them. I'd prefer if we don't add non-trivial auth handling in cadvisor which might conflict or bypass settings for the wrapper. AFAIK, kubelet is in process of  disabling cadvisor port and using kubelet as a proxy for all cadvisor data.\n. Hey @SidneyAn, can you provide the output for api/v1.3/subcontainer and cadvisor logs?\n\nThe custom_metrics: false line is for the root container, which is expected. The custom metrics are only enabled for nginx container in this case.\n. cAdvisor expects the status to have the form : Active connections: ([0-9]+) etc.\nWhat do you see on the status page (http://localhost:2520/nginx_status)\n. That was me reply through mail and expecting github to do the right thing. I can be easily mistaken for a bot though :(\n. The default was 60s because that's what the UI used to plot the graphs.\n. We were using NumStats to bound the amount of memory we use. The intention was that if housekeeping is changed to 10s, we still need 60 samples to populate UI in a meaningful way. Housekeeping can be slowed down if a sample is costly to read, but storage cost is usually the same.\nThe intent was that no matter what timescale we use, we still need enough samples to extract any meaning. The usage has changed since then, but it did make sense at that time :)\n. Thanks @DirectXMan12 this has been sitting in my queue for a while\nLGTM\n. I'd say that passing the whole config is label is a bigger hack  than what we have now. It abuses the labels use and also makes it hard to verify the runtime argument.\nThe intent with the current config mechanism was that we build configs with the images, or wrap an existing image with config you want. Kubelet shouldn't be trying to create config files anyway. The runtime blob is neither extensible or shareable. \n. From the original request, I don't understand why kubelet needs to specify what metrics need to be collected. The user, even if its kube components, should be deciding what metrics we need to collect. Shouldn't we try to keep this a passthrough operation for kubelet?\n. Ack. Will do another PR.\n. uname.Release is int8[]. Direct conversion doesn't work.\nFor local i, I thought the preferred way was to not track a variable if you don't absolutely need it.\nAs long as we need to initialize the outer i, we can just use that.\n. @monnand Not particularly attached to the outer index. It feels safer to fill up the string serially without depending on the index from for loop. It doesn't matter in this case though.\n. s/per_cpu/per_cpu_usage/\n. Since VMs support hotplugging, do we need to take care of this case? Maybe add a TODO.\n. s/PerCpu/PerCpuUsage/.\nHopefully, we'll report cpu loads too sometime in the future.\n. s/kernel/system/\n. Do we need to check if these values are available, and only then copy them over?\n. cpuset is going to return different path for now, even on systemd systems. Maybe remove this for now.\ns/susbsystems/subsystems/\n. Good catch! Fixed.\n. s/data base/database/\n. s/flag/flags/\nI would suggest dropping everything after 'flags to cadvisor:'.\n. Remove packets and dropped for now. We'll add them when we have influxdb scale better.\n. Doesn't it default to localhost:8083 or smth? I never specify this for local testing.\n. s/backedn/backend\n. - mention that we remove data for influxdb. mention network stats.\n. Done.\n. This is cpuacct.usage reading for the attached timestamp. This isn't a rate. We do want to add rate. Right now, it just mirrors what influxdb backend does.\n. These samples are based on diff from previous timestamp (usually 1 sec before). Instant here means readings for a particular timestamp.\n. hmm... I thought gofmt would take care of this. But it doesn't do any reordering. Fixed.\n. Added docs to all methods.\n. experimental because we are going to change the flags quite a bit to make auth simpler.\n. They are almost the same right now, except that bigquery doesn't do all the per_core ones. Cleaner way to do those would be to use a nested record, but I don't think we can do that in influxdb. We might want to push more data to bigquery than influxdb. So, we can't share schema but some of the column names can be same. I can make this work. I'll see how it looks when we have a decent schema done.\n. This matches influxdb column name. We'll need to change at both places.\n. https://github.com/SeanDolphin/bqschema/blob/master/toStructs.go has an example.\nThe downside is that we need to keep the structure relatively flat and simple.\n. The couple of fields we have now support the Sample() interface for storage driver. There is some inference stuff that uses it. We'll need to clean up these callers first.\n. For Bigquery, I can make one call to the backend to flush all requests, instead of all batched rows one after another. Shall we change the storage interface for AddStats() to accept multiple stats. If a driver wants, it can iterate over them, or process them as a single batch.\n. s/found but/found, but/\n. Does it affect the sampling stuff? I'd assume that we have 1s duration (or same duration for all containers) built in at multiple places.\n. s/occured/occurred/\n. maybe \"subcontainer where the event occurred\".\nIs this a relative name to parent container.\n. Can we use this to detect change in spec too? Currently we don't notice updates to resource limits.\n. s/peforms/performs/\ns/cliernt/client/\n. changed.\n. cool. Using the struct key now.\n. s/Pacakge/Package\n. s/recurisvely/recursively/\n. s/indicated/indicates/\n. Is CamelCase the recommended style? I kind of like the all-caps as we use for other languages. Its easier to differentiate enums from local vars etc.\n. Might be better to print it after processing containerManager.Stop()\n. Add a comment that this is no-op for docker driver.\n. s/Error/error/ here and other places.\n. Update comment to mention LXC driver. Does this work with docker+LXC and LXC without docker both?\n. base libraries like strings should go before others with \"fmt\".\n. nit: How about just Docker containers? Just like the one that shows up at /docker now.\n. @vishh pointed that there's a problem with docker endpoint when there are no docker containers. I don't think this patch fixes that.\n. Can you add the minimum docker version good enough to remove this code.\n. I am fine demoting memory :)\nWe'll need to be sure that whatever analysis we build over stats can handle memory signal being absent.\n. Oh, I see it now. 1.0 seems like a cleaner base to start with. I'll bump that.\n. I didn't find any other place. Where else do we parse it?\nThere's one in libcontainer that we can re-use, but it hides all present-but-disabled cgroups, and only reports present and enabled ones.\n. Sorry, got confused as I moved the code around :) fixed.\n. Added.\n. The next step is to reconcile this with the FS info we already have. Note that these are real block devices - like sda and not the mounted partitions (sda1).\n. I kept it same as what actual os interface returns, so we end up running all parsing code through unit tests. string cast isn't a big deal though.\n. I kept the logic of parsing the sys-returned string in unit-testable code. The common code does conversion to int64 and change sectors to bytes.\n. Long time ago, in the first version of this PR, it was a struct :) We moved it to string because one of the operation was not supported with a struct key. I don't remember what it was - maybe some kind of copy. Need to look it up.\n. done.\n. done\n. done.\n. done.\n. Good point. Changes it to KHz to match precision provided by OS.\n. No, this is per-node memory - as in how much memory is attached to each node. This isn't cache information. Still working on that.\n. We have the original memory field in MachineInfo exported as int64. I didn't want to change it here.\n. Good point. Fixed.\n. I want to see what all is there under /sys/fs/cgroups - any kind of named cgroups.\n. Good point about /proc/mounts. I think we should just add the whole output of /proc/mounts at end of the validate output. We ask for it while debugging quite a bit. WDYT?\n. That does sound better. Updated.\n. Done.\n. I didn't change any actual errors in this PR. Personally, I don't like  to make any log warning or error if its not going to bring down cadvisor. Otherwise, we'll log the same error in 3 files (.INFO, .WARN, .ERROR). \n. Updated.\n. Some versions of Parse* wasn't handling these without stripping spaces because of /t and /n returned by sysfs.\n. Nit: Move this comment down with the 'limiting' comment.\n. Made it a method.\n. Yeah, that's nicer. Changed :)\n. Updated comment.\n. I didn't do that yet, because we only implemented the two interfaces we needed for getting cpu load. If we need more, I think we can export a more elaborate netlink implementation. There are few in github, but none of those directly expose the task stats stuff.\nI moved most of the netlink related stuff in a single file, so we can move/replace it if needed.\n. Moved the endianness to a global var for now. I need to test what it does on big-endian machines, but I think the kernel might be reporting these in a specific order. We can always check the machine and set up endianness globally if that's not the case. Added a TODO.\n. done.\n. Done.\n. For testing. Merged with error.\n. Now that these are returned, added json info.\n. Having the same name as kernel consts makes it easier to lookup in kernel sources :(\nChanged it to go fmt, but added kernel equivalent as comments.\n. Done.\n. no :)\n. gah. I am going to add another hook to catch these :)\n. I was debating about it. Maybe it'll be nice for UI display. Updated.\n. I thought it won't help because we have to match just the prefix?\n. This is going to show up as a warning in all namespaced containers. We can update it to warning once we have the sched_debug backend.\n. done.\n. fine, added...\n. just the comment, or field name, or json tag? I am assuming its just the comment :)\n. copy/paste :) fixed.\n. Can't we use docker remote API here?\n. I thought some of the integration tests assume that cadvisor is running inside a container.\n. Talking to remote daemon is easier to work with in case of errors. But we can always do that later.\n. SG.\n. needs boilerplate :)\n. We log error in only one place in refresh(). For others, we still want to continue processing.\nrefresh() is currently called in two places - one in housekeeping and the other in Start(). Logging the errors in refresh() feels more concise as we don't want any caller to act on it.\n. done.\n. Done. Might not be too descriptive, but I tried :)\n. done\n. This is a smoothed avg of number of running threads. I'll update the comment to make it clear.\n. Nooo. Not milliload :(\n. We smooth it over last 10s. If the load changes from 2 to 50 and stays there, this load value should get to 50 within 10s. The instantaneous value is very spiky and not useful for analysis. If a tool wants to do its own calculation or look for bursts, the value is still available in loadstats.NrRunning.\nI can change the field to LoadAverage and add the 10s interval in comments.\n. Added.\n. We call it just once and cache it here. Wanted to keep cAdvisor dependencies on utils at a minimum.\n. Gah updates all the caps!\n. Done.\n. I am following the convention in this whole file. Don't want to update just this one.\n. I think this is the biggest precision we need. We can always use different units.\n. I agree to the part of presence checks and it is probably the best approach when we have some passthrough components that just move the data around. For this one, I have two issues:\n- It hurts readability quite a bit. No one can figure out what data is being returned just looking at the info file.\n- Every component we have in the final summary pipeline actually looks at the data. We'd add up more lookup charges without simplifying much.\n. ok, Present it is.\n. Resources available or not are tracked in Percentiles through Present. PercentComplete only tracks how many Usage samples we gathered irrespective of what type is available.\n. PercentComplete just indicates how many samples cadvisor has collected for that time duration. The primary use for present is when the system is configured to say not have memory isolation or cpu load, for example.\nAre you concerned about cadvisor failing sometimes to collect a single resource data? We'll handle that internally in cadvisor either by ignoring the sample of extrapolating. It should not bubble up to the clients.\n. It's 50% of all stats. That's why it is part of Usage.\nOn Wed, Feb 4, 2015 at 9:22 AM, Vish Kannan notifications@github.com\nwrote:\n\nIn info/container.go\nhttps://github.com/google/cadvisor/pull/481#discussion_r24100741:\n\n+type Percentiles struct {\n-   // Indicates whether the stats are present or not.\n-   Present bool json:\"present\"\n-   // Average over the collected sample.\n-   Mean uint64 json:\"mean\"\n-   // Max seen over the collected sample.\n-   Max uint64 json:\"max\"\n-   // 90th percentile over the collected sample.\n-   Ninety uint64 json:\"ninety\"\n  +}\n  +\n  +type Usage struct {\n-   // Indicates amount of data available [0-100].\n-   // If we have data for half a day, we'll still process DayUsage,\n-   // but set PercentComplete to 50.\n-   PercentComplete int32 json:\"percent_complete\"\n\nMy concern is the latter. When we say 50% data available, is it 50% for\nall stats, which is how I interpret the current API, or 50% of one of the\nstats? To make it unambiguous, we can move the PercentComplete to\n'Percentiles'.\n\u2026 <#14b559dac968e296_>\nOn Wed, Feb 4, 2015 at 6:17 AM, Rohit Jnagal notifications@github.com\nwrote: In info/container.go <\nhttps://github.com/google/cadvisor/pull/481#discussion_r24085185>: >\n+type Percentiles struct { > + // Indicates whether the stats are present\nor not. > + Present bool json:\"present\" > + // Average over the collected\nsample. > + Mean uint64 json:\"mean\" > + // Max seen over the collected\nsample. > + Max uint64 json:\"max\" > + // 90th percentile over the\ncollected sample. > + Ninety uint64 json:\"ninety\" > +} > + > +type Usage\nstruct { > + // Indicates amount of data available [0-100]. > + // If we\nhave data for half a day, we'll still process DayUsage, > + // but set\nPercentComplete to 50. > + PercentComplete int32 json:\"percent_complete\"\nPercentComplete just indicates how many samples cadvisor has collected for\nthat time duration. The primary use for present is when the system is\nconfigured to say not have memory isolation or cpu load, for example. Are\nyou concerned about cadvisor failing sometimes to collect a single resource\ndata? We'll handle that internally in cadvisor either by ignoring the\nsample of extrapolating. It should not bubble up to the clients. \u2014 Reply to\nthis email directly or view it on GitHub <\nhttps://github.com/google/cadvisor/pull/481/files#r24085185>.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/481/files#r24100741.\n. Would this work for things like systemd containers that just join cpu and names hierarchies? Won't it always pick the machine boot time in that case?\n. So if you open a ssh to ubuntu/RH box, it will report boot time as container creation even though we just created cpu and systemd cgroups with right timestamp.\n\nI don't think we care that much for these sessions, so its okay to ignore.\n. we can't because we want to ignore the sample if elapsed is too small.\n. s.index + 1 is doing that, I think.\n. This gives us a simpler hour and day stats with one minute granularity. The other way to do it was to use weighted percentiles - for a day: 30 of minute stats + 0.5(last hour stats). Yesterday, we talked about trying to save the whole series to start with.\n. yes, this is a TODO. To support that, we save timestamps for each second. Instead of checking the number of samples, we need to look at timestamp. I moved it out to later because I wasn't sure how we were going to handle hour and day yet.\n. Discussed offline and limiting data to an hour for now.\n. done\n. done\n. I think we also started reporting diskio stats.\n. made it private.\n. made it private\n. changed secondsToNanoseconds to be secondsToMilliseconds * milliSecondsToNanoseconds\n. Go doesn't like it in constants.\n. will do this in the next PR.\n. I don't think this works for a normal case where we just have symlinks:\n$ ls -l /sys/class/net/\ntotal 0\nlrwxrwxrwx 1 root root 0 Feb 14 18:09 docker0 -> ../../devices/virtual/net/docker0\nlrwxrwxrwx 1 root root 0 Jan 20 06:58 eth0 -> ../../devices/pci0000:00/0000:00:04.0/virtio1/net/eth0\neth0 will pass the test for symlink and fail the one for IsDir(). We'll end up not adding it to the known devices.\n. sorry, I didn't notice that you were modifying 'f'. Getting late :)\nThis should be fine.\n. Can this go to -1? Do we correct for it later?\n. this might not work if the user asks for an end time. Might be better to return early if we don't have any samples yet.\n. Done here and everywhere else.\n. I want to keep it based off 1.2 and replace whatever APIs we need to change. My guess is that some of the APIs will stay the same. If we rip out everything, we can remove the base dependency. \nI am fine going the other way and adding the base versions once we know that some of the endpoints will be replicated.\n. done\n. I was too lazy this time around. Will do a separate PR :)\n. Add a comment that it protects only eventlist and not watchers, if that is the intent.\n. we are reporting rate in millicores. :)\n. I was debating putting in a new type here with just one value. We can do that - one uint64 each for cpu and memory.\nThe goal was to extend the derived stats from day, hour, minute down to second. When cadvisor starts, you just receive a bunch of zeroes for a minute. With seconds data, caller can start seeing usage stat earlier. \n. SG, let me do that.\n. Added a new type.\n. Moved the check before cpuRate calculations to drop bad samples.\n. Added a comment.\n. yeah, we add one stat at a time and also max dynamic housekeeping is currently limited to 1minute (with a flag). The only way this can go way beyond 60s is if we  increase the max allowed housekeeping time. Testing on empty containers generate 7 and 2 samples every other minute (due to backoff).\n. much better. Done.\n. Do we know if watching is better? For many systems, /var/log/messages gets updated more than once per second.\n. /var/log/messages gets debug logs from many sources. The watcher might get invoked more than once per second.\nWere we doing a periodic poll earlier? Can we just check once every 10s and keep an open fd?\n(Sorry, I haven't looked at all details of oom-parser. But I feel like watching might not help some of the machines).\n. I feel like string is more compact and manageable.  Its easier for people looking for visual input and straightforward for advanced users to convert.\n. done. Looks like we don't fill it in yet.\n. I am not sure if we have machine-id on all distro. It might be expected to fail on some hosts.\n. I did some refactoring to get it from docker factory. The docker base dir is chosen by -g flag, but none of the docker APIs return the path they are using. We can either parse docker command line, or infer it from docker client by parsing graph driver info. Easiest way would be to enhance docker to put the base dir somewhere.\nI tried doing it in docker factory, but it was too ugly. Will revisit soon.\n. Updated and made public.\n. Added. It's from label to source device. We can use the output of this to find the partition info in partitions map.\n. Added.\n. Added :)\n. Used id here as it won't always be the name once we extend options to include docker.\n. switched :)\n. I think its important to have a way to get all docker containers. The other option:\n/stats/docker?type=name&recursive=true\nis going to stop working after custom cgroups.\nWe can make it require recursive=true when its added.\n. done.\n. agreed, this is what I meant.\n. I read /machine as machine global information, rather than h/w information. I was thinking that one endpoint to get all machine-wide attributes would be useful. But I am fine splitting it into two endpoints.\n. or we can rename /machine to /attributes \n. We can go back to exposing  manager.GetDerivedStats() and manager.GetSpec() and make containerData private. We'll just need to move that logic from api/ to manager/\nLet me do that in this PR.\n. Do we have access to journalctl from within the cadvisor container?\nOther option might be to communicate with DBus directly eg like github.com/TheCreeper/JournalCheck \n. I think we need to do the generic solution. We can just use the pkg I referred to earlier. I am fine with going with journalctl binary approach first as this would work with kubernetes at least.\n. Are there any new equivalent tests that we can add?\n. Why did we delete this example binary? Can we update instead?\n. The example requests and output do look useful. I'll try to add these.\nRemoving the field descriptions from machine now.\n. It still works with regular combination when container name is used. Only for docker containers, we don't accept docker container id and recursive=true.\n. Can you add comments on why we need to map it to localhost and hardcode port number here?\n. Sorry, I don't have enough context as to what client API is supposed to do. From here, it looks like its rewriting the query to events endpoint and just print out the result. I haven't followed the events stuff in detail before. Can you describe it a bit here.\n. Make the log message V(3).\n. Is this used anywhere?\n. done\n. added\n. fixed.\n. s/2014/2015/\n. remove commented.\n. Removing these is causing the build to fail.\n. Ah, good catch! Fixed. The direction is fine for both host and docker containers now.\n. Can you make these V(4) for the next few weeks while we play with this code?\n. Doesn't this update every docker containers load multiple times in each housekeeping loop - once when we are processing /docker, and once processing /docker/foo?\n. Just tracking the time of last update and comparing might be helpful to avoid double checking.\n. Can we cache this list and update on each subcontainer create/delete?\n. Another problem with this approach is that some of the subcontainers might be idle and we would end up checking load for them. Today,  manager's housekeeping slows down stats check for idle containers.\nThis is more of a concern on systemd systems, where there are billion containers created at startup.\n. s/should have/should not have/\n. I think this check should be checking against subContainerName.\n. The error message is hard to follow. Maybe : \"should have found %v as %v is a subcontainer.\"\n. Do we need to check if loadReader is nil?\n. why 2 seconds? Our normal housekeeping is 1 second. so this will always be true.\n. ah, we check this in the calling fn.\n. Why are we calling GetStats again?\n. Should this be recursive? Calling load stats on \"/\" should sum up stats on all existing containers.\n. Does this add all containers as subcontainers of \"/\". Do we track the immediate children anywhere? We are only interested in immediate children.\n. Ah, I see what this is doing. In my mind, I was assuming that both load and task stats will be hierarchically reported.\nThis approach works too, but I am a little worried if we are doing repetitive work unnecessarily. We might also be maintained a large list of subcontainers for root, even though some of them are empty.\nCan you measure the amount of work we end up doing for a busy machine? We might be able to make the current solution a bit more efficient if required.\n. removed\n. Can we make this a flag?\n. missing arg.\n. missing arg\n. check for nil. For most cases, we want to have collectorManager as nil.\n. ok, SG.\n. Is this common? Is it for dead containers?\n. Hostname() won't return what you expect if cAdvisor is running inside a container. You probably want to accept an IP or hostname as a flag.\n. remove commented.\n. you need to check workingSet to be non-negative after this deduction.\n. ah, this should be okay then.\nLGTM, will merge on green\n. I just kept the original strings, as I would have to convert them back to put on the UI :)\nwill update.\n. Its actually convoluted to change the time fields back as the output changes based on the duration:\napi/v2.0/ps/2ff16760cda11e8568589a9d73fe7e833b83ca3a8f55537157f7ff52195bad70?type=docker\n[{\"user\":\"root\",\"pid\":7003,\"parent_pid\":20768,\"start_time\":\"May06\",\"percent_cpu\":\"0.0\",\"rss\":\"424\",\"virtual_size\":\"7308\",\"status\":\"Ss\",\"running_time\":\"00:00:00\",\"cmd\":\"stress\"},{\"user\":\"root\",\"pid\":7021,\"parent_pid\":7003,\"start_time\":\"May06\",\"percent_cpu\":\"99.5\",\"rss\":\"262284\",\"virtual_size\":\"269456\",\"status\":\"R\",\"running_time\":\"4-23:08:51\",\"cmd\":\"stress\"}]\nstart time can be May06, 20:11, or 2014. I think we can use a different option to print the whole time, but then that will be hard to condense back to be top like :/\n. loop was simpler because conts is a map and containerName isn't necessarily the key.\n. done\n. leftover from before I realized that we already had a ListProcesses() :) removed.\n. done\n. Can you run with go1.4 or just change the version back in this file manually?\n. 2015\n. incomplete sentence.\n. done\n. ok, changed the original one :) I couldn't find any place on the UI where the numbers are low enough to show up as bytes - other than the ps output.\n. fixed.\n. should we early return if the container is already watched?\n. can this be a map to a bool?\n. do we call AddWatch multiple times for the same container, one for each cgroup directory?\n. so we delete the containername on the removal of first path. Should it be when the last path is removed? \n. Its just simpler if we only need to check for presence. struct is fine if you plan to push some data there in future.\n. I would avoid runtime and just check the speed if the x86 test fails. It will make testing it easier with a fake procinfo file.\n. +1\n. We need to check if cgroupsWatched returns nil.\n. sorry, updated.\n. fixed.\n. mention Fedora here too?\n. keep expanded by default?\n. nvmd, I see what you did there\n. I'd suggest we split this method into two:\na private detect() method that detects which cloudprovider the node is running on. This can be called once in NewRealCloudInfo() and we can save the result as this isn't going to change across the machine's lifetime.\nThe public part of the interface should just return the detected type rather than calling detection logic on every invocation. We can also cache instance type as that's mostly the same detect call.\n. If we save cloudprovider and instance type internally, we don't need to accept cloudprovider as a parameter.\n. Its likely we'd would expand cloudprovider utils to return more than just instance type. I'd suggest moving cloudprovider logic to its own file for supported clouds (gce for now). The default/unimplemented ones can stay here.\n. I'd suggest moving the datastructures reported in the API  (CloudProvider, InstanceType) here. That way, consumers of the api don't have to include utils/cloudinfo.\n. I think we don't need this bit. The usual github help applies to cadvisor. Github bootcamp already covers this.\n. I think we don't need Nginx specific collector - just make name as an arg to NewCollector(). This should be the regular config-based collector generator. nginx just happens to be one of the configs.\n. This is supposed to be a generic collector constructed from a config.\n. Document the fields. Add comments about what each field means.\neg. Is err the latest collection error or only set when we fail to parse the config?\n. define enum for metric types - so the users have a way to know what we support.\n. comments? polling frequency in seconds?\n. you can reuse err here, instead of err1.\n. return directly from here and avoid the else part.\nAlso prepend some more information to error message - like printing out the config read.\n. How about passing the config to constructor and failing construction if the config doesn't parse? Would it be cleaner than storing the error state and holding on to a defunct collector?\n. why did we change the tag to beta and have the dir deploy/canary?\n. Can you move open, close, and Send to a separate client file? We are trying to change the metrics format and merge some of the container stats to values methods. It will make the conversion a bit easier.\n. why not merge getSwapCapacity and getMemoryCapacity into parseMemory and move error reporting to the public methods? It will avoid some duplicated code.\n. Can't we rely on the config to know if we are expecting floats or ints?\n. Why not fail collector creation if regex can't be compiled?\n. My thinking is that if regex compile fails, then we should just error out and not create the collector. If we do partial collections, it might be hard for the user to notice that something was actually wrong with supplied regex. Failing to collect a metric is okay and we can continue processing, but invalid regex is probably something user should be reported back to user with a failure.\n. what's the new infra?\n. ah, I see it now.\n. We don't need to test influxdb anymore?\n. I think we don't need to change the housekeeping time. Supporting collection of custom metrics at a rate faster than regular housekeeping will be problematic. \nHow about making UpdateCustomStats() equivalent to UpdateStats(), and calling both of these from tick. We can save the next time for customMetric collection in containerData.\n. Given that we are now saving all stats in one place, it doesn't make sense to collect metrics at this level.\nEither doing it in UpdateStats() or calling UpdateCustomStats() from housekeepingTick() should be fine.\n. In line 68: It looks like we don't check if the user provided a polling frequency for any metric config. We need to set in a default if json returns zero and ignore any metricConfig.PollingFrequency which is zero.\n. optional nit: Can we rename to GetAllPercentiles() instead. It makes searching/reading names easier if they don't differ in just the plural.\n. Do we want the default to be one minute? The default housekeeping is 1s.\n. I think we should move this logic up to replace line 68. Start with the default and then compare in the loop (#72) and lower it if we find a user-defined value.\n. I think this might not be true if a docker container is started with --cgroup_parent flag. parent cgroup name will replace docker in that case.\n. s/has_custommetrics/has_custom_metrics/\n. We are going to add CustomMetricsSpec later though? With metric name and type, so metrics could be discovered.\n. remove the commented out line? Where was this being used?\n. yes, but importing HousekeepingInterval adds a cyclic dependency between manager and collector. I'll clean that up next.\n. more commented out code \n. nit: no omitempty for the boolean fields.\n. An identifying name  -eg . io.cadvisor.metric.nginx. This allows for multiple config to be specified.\nIf there are any processing errors, the name is what users will see in their error logs - \"failed to parse config nginx\"\n. WDYT about moving all flags and consts to a const package?\nRight now, some of the files have circular dependencies. We can have one single file for all tunables.\n. yes, just an identifier.\n. ah, that's right. Fixed.\n. done.\n. ResourcesAvailable is used to draw the spec table. We can skip CustomMetrics as the details won't show up there.\n. We should make this its own h3 heading as Application Metrics.\n. Why not just show cur.name ? Do we also get units in the config?\n. This doesn't seem to work for the api as we end up calling UpdateSpec() and overwriting this.\n. I didn't know what the labels for each metric were supposed to do? For now, I just removed it.\n. I see. Wouldn't we have naming conflicts if we have two stats series that are both called disk_usage and disambiguation depends on the label. I would lean towards pushing that kind of info in the name, in a structured way.\nI think this will be relevant when we implement a prometheus reader. Let's revisit it then.\n. The order of stats and spec doesn't match. In my tests, the units gets mixed up.\nMaps in go don't have a fixed order. I'd suggest changing the loop to iterate over the spec and locate spec.custom_metrics[index].name in metrics map returned by appmetrics.\n. nit: Isn't IO too generic a name? It doesn't imply stdout to me immediately. Why not just say stdout?\n. Let's submit this PR. But we should change the way we detect that its a prometheus config.\nWe can add a \"type\" field to the config which can default to REST and can be overridden to \"prometheus\". That way we don't end up overloading labels.\n. namespace not required anymore.\n. nit: Add to TODO - Might also be helpful to cache, so we can return last known docker version if docker is down at the time of query.\n. I think we shouldn't overload CpuStats to work for both. Dropping units in the base definition is confusing.\nLet's split them and we can evolve them separately. \nOverall supporting instant metrics in cAdvisor SG.\n. Kubelet uses v1 today. If you are not planning to update kubelet to move to v2, you can put it in v1. Else, move it to v2.\n. Sure, changed to counting non-loopback devices.\n. We should probably refuse if the delta is less than 100ms or so.\n. s/convert/convertToRate/ ?\n. Are we using nanocores/s for precision? Will we convert these back to millicores/s in kubelet?\n. Our timestamps are one seconds apart. If we took samples too close, it might exaggerate a small spike to a large number. Best to ignore tiny bursts or idle cycles.\n. ok, nanocores/s is fine.\n. :sob:\n. ",
    "joemiller": "I am interested in the raw driver. Is this work available in a public branch somewhere?\n. ",
    "zohaib1020": "This should be fixed after commit 341f64a4482cb7c2acf042b279d804f42e4a1968\n. :+1: \n. ",
    "caizixian": "@rjnagal Are you referring to @vishh \nAnd can I get something like this?\n\n\n. @vmarmol @rjnagal \nCan I get bandwidth usage between given time period like from 2014/05/06 14:00 to 2014/06/07 11:30 ?\nAnd is there a builtin image generator?Or I can only get the data to gen the img by myself?\n. @vmarmol Got it.\nIs there any easy tool to deal with the raw data and generator image as I post above. Maybe Graphite or Munin?\n. @jchauncey @vmarmol @vishh @rjnagal I think that influxdb will be a perfect db based storage backend.\n. @vishh Brilliant.\n. @vmarmol More detail.\nMaybe like this \n/api/graph/network/day/\n/api/graph/network/week/\n/api/graph/network/hours/6\n/api/graph/network/time/Jun06141200Jun12141523 (maybe other timestamp format)\nwillreturn a png or jpg\nand\n/api/status/network/day/\n/api/status/network/week/\n/api/status/network/hours/6\n/api/status/network/time/Jun06141200Jun12141523\nwill return a status of total usage(json,etc)\nand\n/api/realtime/network/ will return a json,etc \n. @vmarmol Is it clear?\n. @vmarmol Has this finished now?\n. @vmarmol Does the existing UI have dynamic graphs?\n@thaJeztah Maybe in control panel you can have a dynamic graph while you can also get json or static graph.\n. @vmarmol @rjnagal @thaJeztah That's fine.\n. @vishh I'll try.\n. @vishh @vmarmol @rjnagal Do you have a documentation sysytem?\n. @vmarmol Maybe you can just enable GitHub Repo's wiki. Getting start and API documentation are good.\n. @kelseyhightower Good job.\n. ",
    "vishh": "I believe there are plans to export to influxdb. You can open an issue, it\none doesn't already exist.\nOn Fri, Jun 13, 2014 at 10:43 AM, Jonathan Chauncey \nnotifications@github.com wrote:\n\nWould you guys be up to having cadvisor output to different metrics\ncollectors? Like ganglia and influxdb?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/34#issuecomment-46039565.\n. @caizixian cAdvisor now supports network stats. Take a look when you get a chance.\n. @caizixian you can obtain the graphs you want today by running cadvisor\nalong with influxdb and grafana.\n. The plan as you mentioned is to have plugins for different storage\nbackends.\nAFAIK we haven't explored influxdb in detail. Are you interested in working\non adding support for it?\n\nOn Fri, Jun 13, 2014 at 11:34 AM, Jonathan Chauncey \nnotifications@github.com wrote:\n\n\nHow do you tell cAdvisor which influxdb nodes to use?\nShould it push the data in realtime or on a timer?\n\nIm guessing eventually this will be the basis for a plugin setup for\nallowing data to be pushed to other systems like ganglia and graphite.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/39.\n. +1\n\nOn Tue, Jun 17, 2014 at 2:39 PM, Victor Marmol notifications@github.com\nwrote:\n\nDoes this affect /, /docker, /docker/, or all?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/59#issuecomment-46369156.\n. Why can't we change libcontainer as Rohit suggested and not change lmctfy?\n\nOn Tue, Jun 17, 2014 at 2:42 PM, Victor Marmol notifications@github.com\nwrote:\n\nThat may be tricky given freezer support and systemd not using freezer.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/59#issuecomment-46369502.\n. cAdvisor does not ignore any containers as of now.\n\nOn Thu, Jan 28, 2016 at 1:23 AM, Kamba Abudu notifications@github.com\nwrote:\n\n@vmarmol https://github.com/vmarmol Was a patch applied to cadvisor to\nignore non-docker and non-root containers in systemd systems? I'm using the\nlatest tag of google/cadvisor available on docker hub with CentOS Linux\nrelease 7.2.1511 and still seeing systemd containers appearing in the stats\nfrom cadvisor.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/59#issuecomment-176079708.\n. @monnand: done\nPing @vmarmol \n. I have addressed the comments and made some more changes to stabilize the code.\nI have rebased this PR on top of #99 .\nPTAL @vmarmol @monnand\n. Rebased. I tested now and both root and docker containers get detected. Stray docker containers are safely ignored, which might be a bug in one angle. \nThe influx db tests are failing because influx db is not reachable. Ping @monnand \n. PTAL @vmarmol \n. @vmarmol Addressed your comments. PTAL.\n. I don't want to squash all of them. I will see which ones are not important\nand squash them.\n\nOn Tue, Jul 22, 2014 at 12:52 PM, Victor Marmol notifications@github.com\nwrote:\n\nLGTM, can we squash the commits and then we can merge :) thanks for making\nthe changes!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/96#issuecomment-49791346.\n. Squashed a few commits.\n\nOn Tue, Jul 22, 2014 at 12:52 PM, Vishnu Kannan vishnuk@google.com wrote:\n\nI don't want to squash all of them. I will see which ones are not\nimportant and squash them.\nOn Tue, Jul 22, 2014 at 12:52 PM, Victor Marmol notifications@github.com\nwrote:\n\nLGTM, can we squash the commits and then we can merge :) thanks for\nmaking the changes!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/96#issuecomment-49791346.\n. @monnard #96 already has these changes. \nIn any case LGTM\n. LGTM\n. LGTM\n. LGTM\n. CanHandle() needs to be updated to not ignore '/docker-' prefixed docker containers.\n. @vmarmol 'go get' is failing. \n. LGTM\n. This PR now depends on https://github.com/docker/libcontainer/pull/119. \n. Travis is failing because the libcontainer changes are not merged yet ;)\n. LGTM\n. LGTM\n. LGTM\n. Ping @vmarmol \n. LGTM.\n. Ping @stigkj \n. k8s_* containers are created by kubernetes. cAdvisor has no control over\nthe alias of containers.\nAn issue related to network stats was fixed recently. The next release\nshould have the required fix.\n\n\nOn Tue, Sep 8, 2015 at 6:44 AM, bliss notifications@github.com wrote:\n\nGood day! All of a sudden I found out my statistics have gone. After some\nexamination i found that container names in influxdb become like that one\n/system.slice/docker-a89ad7021d73e64de54b8f25fec4bf8a3fb23441b661b9b5a2c6cfbcb000b044.scope\ninstead of more habitual\n/k8s_mariadb2562281893.30a8f541_6839bb45-0ff5-4a68-ae88-7a5203814750-k9349_91ad4704-04c1-490c-bed1-ebb750484d4f_c7a0a08a-52e1-11e5-b267-5\n2540051caca_0cb6d646\nand no network statistics.\nHow can i fix it?\nOS: CentOS7\ncat /etc/redhat-release\nCentOS Linux release 7.1.1503 (Core)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/143#issuecomment-138563342.\n. Thanks for clarifying the issue! Can you post logs from cAdvisor and the\noutput of /validate?\n\nOn Tue, Sep 8, 2015 at 1:08 PM, bliss notifications@github.com wrote:\n\nThank you for your reply.\n\"k8s_* containers are created by kubernetes. cAdvisor has no control over\nthe alias of containers.\" --> But it had, because it definitely fed those\naliases to influxdb. Moreover, when polled via API cadvisor returns only\nsystem slice statistics, no data for docker containers. Looks like it\ncannot get access to cgroups or something like that. I turned selinux on\nand off but no difference. Last time we resolved such issue with cadvisor\nupdate: from 0.6.2 to 0.14.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/143#issuecomment-138685195.\n. Can you try curl -L http://localhost:4194/validate?\nDo check if your unit files have all the necessary mounts as mentioned in\nthe docker running instructions in the repo.\n\nOn Tue, Sep 8, 2015 at 1:53 PM, bliss notifications@github.com wrote:\n\nWhen I started cadvisor from command line as root I got all statistics.\nMaybe the problem arises when cadvisor started by systemd\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/143#issuecomment-138696820.\n. LGTM\n. Fixed.\n. We are currently leaning towards avoiding any sampling in cadvisor itself. The instantaneous values can be obtained from the backend DB easily. Closing this issue.\n. This issue has been fixed. \n. Closing this in favor of #167 \n. Thanks for the rebase @rjnagal. LGTM\n. Looks like travis is Green. LGTM. Merging.\n. Ping @rjnagal \n. @jchauncey it will be difficult to provide a wrapper that will work with all orchestration tools. Like you mentioned, you could place a wrapper around cadvisor. I don't see what we can do to provide a universal solution.\n. LGTM. Merging..\n. @jchauncey you can use our REST API directly instead of the UI.\n. I have had similar experiences. Influxdb ends up using close to 100% CPU when running just 3 cadvisor nodes. \n. Yeah. I will take a look.\n\nOn Mon, Aug 25, 2014 at 7:59 AM, Victor Marmol notifications@github.com\nwrote:\n\nYeah I think that is unexpected, @vishh https://github.com/vishh wanna\ntake a look?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/182#issuecomment-53274965.\n. @cboggs: The resolution is still 1s. Are you using stats.timestamp?\n. @monnard: AFAIK the timestamp column is still accurate. I am trying to get rid of 'timestamp' and use 'time' instead as I write this.\nAbout buffering: Our UI is the main consumer of stats once we have a non-memory storage driver configured. Any other monitoring solution should talk to the backend storage. That is why the memory cache has been configured to hold the at least the last 60 seconds of stats and samples. But Percentiles() requires looking at older data and this will not work until 'buffer_duration' seconds have elapsed since cAdvisor started. We can go around and add more logic to fix this, but I did not do that already because we realized that all the statistical analysis that is done in cAdvisor now can be done outside of it by talking to the backend storage directly.\nThe unit-tests are passing because the unit-tests have been setup to not buffer any stats :)\n. @monnand: Moving the buffering code to storage/cache sounds like a bright idea given that the cache is already buffering data.\n. @cboggs: I have updated the code to push 'time' instead of 'timestamp'. The precision issue should now we solved. We will release a new docker image once @monnand completes his code refactoring.\n. Closing this in favor of #203 \n. We export 'machine' attribute which is the hostname of the container that cadvisor runs in. If you run cadvisor in docker that will be a hash that docker constructs. Does that satisfy your use case?\n. Cadvisor does store the hostname with the column name 'machine'. However\nmachine name is useless when running inside a docker container. We are\nplanning to fix that.\n. This issue is dup of #165. Closing this.\n. Ping @rjnagal @vmarmol \n. This patch does not work when running inside a docker container. Still debugging the issue. It works otherwise. I will look at the test failures soon.\n. The issue with docker containers will be fixed via #186. This patch is good to go as-is except for the test failures.\n. @vmarmol InfluxDb tests are failing if I push to the DB in the background. I will revert that change and re run travis.\n. @vmarmol Travis is happy now :)\n. Fixes #182 \n. Ping @vmarmol @monnand @rjnagal \n. Ping @rjnagal @vmarmol \n. PTAL @vmarmol \n. yeah. I have fixed that now :)\n. Travis is happy now :)\n. You will have to run it without '--detach=false' and with '-logtostderr' to\nsee the logs.\n\nOn Wed, Aug 20, 2014 at 1:51 PM, Victor Marmol notifications@github.com\nwrote:\n\nCan you run without --detach=true and see what the log says? Should let us\nknow why it quit :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/188#issuecomment-52842029.\n. I have seen this error whenever cgroups have not been setup correctly. This\nerror usually disappears once I install the official docker package for\nthat distro since the docker package takes care of setting up the cgroups.\n\nOn Wed, Aug 20, 2014 at 1:54 PM, ray dawg notifications@github.com wrote:\n\ndocker logs aabf0f96bc61\nF0820 20:45:06.554746 00001 cadvisor.go:60] raw registration failed:\nfailed to find cgroup mounts for the raw factory.\ngoroutine 1 [running]:\ngithub.com/golang/glog.stacks(0xc2100c3600, 0xc210074180, 0x77, 0xb2)\n/home/vishnuk/go/src/github.com/golang/glog/glog.go:726 +0xb1\ngithub.com/golang/glog.(_loggingT).output(0xf92120, 0xc200000003,\n0xc2100740c0)\n/home/vishnuk/go/src/github.com/golang/glog/glog.go:677 +0x1ff\ngithub.com/golang/glog.(_loggingT).printf(0xf92120, 0x3, 0x933d70, 0x1c,\n0x7f47d7082e98, ...)\n/home/vishnuk/go/src/github.com/golang/glog/glog.go:635 +0x157\ngithub.com/golang/glog.Fatalf(0x933d70, 0x1c, 0x7f47d7082e98, 0x1, 0x1)\n/home/vishnuk/go/src/github.com/golang/glog/glog.go:1033 +0x64\nmain.main()\n/home/vishnuk/go/src/github.com/google/cadvisor/cadvisor.go:60 +0x42a\nDid a docker rm for this container and recreated with:\nsudo docker run --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro\n--volume=/var/lib/docker/:/var/lib/docker:ro --publish=8080:8080\n--name=cadvisor google/cadvisor:latest\nF0820 20:52:29.666566 00001 cadvisor.go:60] raw registration failed:\nfailed to find cgroup mounts for the raw factory.\ngoroutine 1 [running]:\ngithub.com/golang/glog.stacks(0xc2100bc600, 0xc210075180, 0x77, 0xb2)\n/home/vishnuk/go/src/github.com/golang/glog/glog.go:726 +0xb1\ngithub.com/golang/glog.(_loggingT).output(0xf92120, 0xc200000003,\n0xc2100750c0)\n/home/vishnuk/go/src/github.com/golang/glog/glog.go:677 +0x1ff\ngithub.com/golang/glog.(_loggingT).printf(0xf92120, 0x3, 0x933d70, 0x1c,\n0x7fa717bbee98, ...)\n/home/vishnuk/go/src/github.com/golang/glog/glog.go:635 +0x157\ngithub.com/golang/glog.Fatalf(0x933d70, 0x1c, 0x7fa717bbee98, 0x1, 0x1)\n/home/vishnuk/go/src/github.com/golang/glog/glog.go:1033 +0x64\nmain.main()\n/home/vishnuk/go/src/github.com/google/cadvisor/cadvisor.go:60 +0x42a\n\nI have the same exact issue on another Centos 6.5 host as well, so it's\nrepeatable (at least in our environment.)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/188#issuecomment-52842422.\n. @vmarmol: We should update our README with this information.\n\nOn Wed, Aug 20, 2014 at 2:21 PM, Victor Marmol notifications@github.com\nwrote:\n\nAh, you have one of those systems, also run the docker command with\n--volume=/cgroup:/cgroup and it should work :) let us know how that goes.\n(CentOS and RHEL 6 mount the cgroups under /cgroup and our examples\nassume /sys/fs/cgroup which everyone else uses).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/188#issuecomment-52846193.\n. Opened PR #189 to update the readme.\n\nOn Wed, Aug 20, 2014 at 2:27 PM, ray dawg notifications@github.com wrote:\n\nthanks guys\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/188#issuecomment-52846944.\n. Ping @vmarmol \n. LGTM. Travis is failing.\n. LGTM. Merging\n. Disk usage metrics are already supported. cAdvisor could possibly expose usage of volumes if they happen to be separate filesystems. Shared volumes will be tricky to handle though.\nI'm closing this issue for now.\n. Is docker using '/var/lib/docker'/ by default on all distros? If that is\nthe case, then a cAdvisor flag sounds like a good idea.\n\nOn Fri Aug 29 2014 at 1:03:18 PM Andy Berman notifications@github.com\nwrote:\n\nSo as a workaround, I went to /var/lib and created a symbolic link:\nln -s /docker/lib docker\nThat worked, and now cAdvisor works :)\nThanks for your help. Hopefully you can add a switch or something else in\nthe future.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/199#issuecomment-53923621.\n. @monnand: While you are doing this, you should also get rid of the buffering code in InfluxDB driver. You can do that in a separate PR too. Its just that head will be broken until then.\n. @monnard: Github review system is annoying. My comments keep disappearing as you keep changing the code. Can you do a quick pass over all my comments and respond the ones you haven't already?\n. @vmarmol: I proposed the same exact solution and @monnard is planning to\nsend a PR for that soon. That is the reason for dropping this PR.\n\nOn Fri, Aug 29, 2014 at 11:27 AM, Victor Marmol notifications@github.com\nwrote:\n\nTowards what @monnand https://github.com/monnand mentioned. What about\nalways running the memory driver by default (we do anyways with the\ncaching) and then having the storage drivers be optional and take the\nbatched AddStats()? Then we'd have the baseline we need and expect and\nexport the stats when requested.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/200#issuecomment-53913201.\n. Fixes #182 .\n\nPing @monnand \n. Ok.\nOn Fri, Aug 29, 2014 at 11:02 AM, monnand notifications@github.com wrote:\n\nOops. @vishh https://github.com/vishh, Just realized this PR broke the\ninfluxdb's unit test because it cannot precisely recover the time stamps of\nstats. See this line\nhttps://github.com/google/cadvisor/blob/master/storage/influxdb/influxdb.go#L173,\nand here\nhttps://github.com/google/cadvisor/blob/master/storage/influxdb/influxdb_test.go#L68.\nI will add timestamp back as an additional column because we may need\nsub-second time in the future.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/201#issuecomment-53910037.\n. InfluxDB supports up to micro second granularity.\n\nOn Fri, Aug 29, 2014 at 11:10 AM, monnand notifications@github.com wrote:\n\n@vmarmol https://github.com/vmarmol I think influxdb supports\nsub-second time values. However, it seems that influxdb uses float64 to\nstore time, which may have precision problem by itself. (I'm not sure about\nthis. I'll check influxdb's code later.)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/201#issuecomment-53911140.\n. @monnard: Int64 resolution should be enough for millisecond granularity I\nthink.\n\nOn Fri, Aug 29, 2014 at 11:20 AM, Victor Marmol notifications@github.com\nwrote:\n\nWDYT of using that instead of a separate field?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/201#issuecomment-53912268.\n. @monnard: I haven't looked into it yet. They were passing when my Batching PR got merged.\n. Another idea I have is that of running heapster as a buddy container along side cAdvisor whenever someone wants to push data to external DBs. Heapster can be very easily configured to talk to just 'locahost:8080' and we need not handle batching there.\n\nThe issue with that approach will be that all statistical calculations that requires processing a lot of historical data will fail. But I would argue that these kind of calculations must be performed on a DB. \nWDYT?\n. @monnard: Fixing InfluxDB first SGTM.\nFor the storage driver interface, I would argue that cAdvisor should not be\nretrieving stats from an external DB at all. If cAdvisor is expected to\nperform statistical analysis on historical data, it should be configured to\nstore all that data in memory.\nWith that in mind, lets make the in-memory cache use to the existing API.\nLets define a new external storage interface which will support only a\nWrite method. This external storage driver can then be optionally injected\ninto the cache.\nOn Fri, Aug 29, 2014 at 8:18 PM, monnand notifications@github.com wrote:\n\nLet me first work on a fix of the influxdb driver. There are several bugs\nin the driver. One of them caused #164\nhttps://github.com/google/cadvisor/issues/164, which may cause lots of\nother problems. Let me send a PR to remove the cache layer in influxdb and\nfix the bugs there. Then I will rebase this PR onto it.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/202#issuecomment-53947193.\n. heapster as of now will push data natively to the backend DB. It can be configured to run at a cluster level or at a node level. Heapster does not have to deal with retrieving data, whereas cAdvisor does. So when we enable batching in cAdvisor, we will have to run more logic to ensure correctness. \nThe problem I see with a hybrid approach is that we are anyways going to add and maintain the extra code, so why not add more native drivers.\nDo we want cAdvisor to expose an API that looks at data beyond what it can cache in memory? If the answer is no, then heapster might be a good fit.\n. @monnard: I am working on fully decoupling heapster from k8s. That will be\ndone anytime now.\n\nOn Fri, Aug 29, 2014, 8:01 PM monnand notifications@github.com wrote:\n\nI'm not quite familiar with heapster. Still trying to read the code and\nfigure out where it fits into the big picture. Is it tightly coupled with\nkubernetes? My concern is that if we introduce too much concepts from\nkubernetes into cAdvisor, users may not be able to run cAdvisor alone.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/203#issuecomment-53946949.\n. Yes.\n\nOn Mon, Sep 1, 2014, 2:28 PM monnand notifications@github.com wrote:\n\n@vishh https://github.com/vishh Does this related to the suggestion in\n202 https://github.com/google/cadvisor/pull/202, i.e. doing all\nanalysis work in memory and making the back-end storage only write-able to\ncAdvisor.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/203#issuecomment-54092665.\n. Closing this since the re-factoring is already done following approach '1'.\n. LGTM\n. LGTM. \n. What is the Linux distro? Are there a lot of containers running on the machine?\n. @vmarmol: Done. \n. LGTM.\n@vmarmol I assume you have done basic testing on a few platforms.\n. LGTM. Merging. \n. Good catch. Merging.\n. LGTM. Merging.\n. LGTM\n. I do not see this issue in the latest image. Closing this issue.\n. LGTM\n. LGTM. Merging.\n. LGTM.\n. LGTM\n. LGTM\n. LGTM.\n. This issue is resolved in 0.4.0\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. I did not realize that this PR was already merged :(\n. Minor nits. Otherwise LGTM.\n. LGTM\n. Relevant failure logs:\nI0923 06:37:43.056009 00001 container.go:176] Failed to update stats for container \"/\": Failed to parse 'memory.stat'. Line: total_writeback -32768, Error: Unable to convert param value total_writeback -32768 to uint64: strconv.ParseUint: parsing \"-32768\": invalid syntax\nI0923 06:37:43.096455 00001 container.go:176] Failed to update stats for container \"/docker/756d3e2e240997f5c8c23e54beafd83cfe11516ca8d503ba8204923e03e24b3c\": Failed to parse 'memory.stat'. Line: writeback -32768, Error: Unable to convert param value writeback -32768 to uint64: strconv.ParseUint: parsing \"-32768\": invalid syntax\n. Done.\n. PTAL @vmarmol \n. LGTM\n. Ping @rjnagal @vmarmol \n. LGTM\n. Just a few more comments. Except for the Spec inconsistency, everything else LGTM!\n. LGTM. \n. Is InfluxDB running on your host outside of docker? Since cadvisor is\nrunning in a separate network namespace, it will not be able to connect to\nthe applications running on the host network namespace. You can solve this\nby passing '--net=host' option as part of docker run.\n\ndocker run   --volume=/:/rootfs:ro   --volume=/var/run:/var/run:rw\n--volume=/sys:/sys:ro   --volume=/var/lib/docker/:/var/lib/docker:ro\n--publish=8080:8080   --detach=true   --name=cadvisor   --net=host\ngoogle/cadvisor:latest -storage_driver=influxdb --logtostderr\n. @andrewwebber: Where are you running InfluxDB? Chatting over IRC (#google-containers) might be faster that going back and forth here on github. \n. Awesome. Thanks for the write up. Splitting up grafana into a separate Pod\nis something I have been meaning to do, but there is no native support for\nexternal IPs in Kubernetes yet sadly. Your idea of manually configuring\ngrafana sounds good for the short term.\nRecent versions of heapster exports a new table 'machine' which contains\nall the root cgroup stats. So the grafana dashboard should work you as-is\nwith the latest version.\nIf you are running heapster in Kubernetes, you don't have to run the\nheapster-buddy container, unless you run kubernetes only on a subset of\nmachines in your CoreOS cluster.\nOn Fri, Nov 7, 2014 at 5:23 PM, andrew notifications@github.com wrote:\n\n@vishh https://github.com/vishh Thanks for your support here. I have a\nkubernetes coreos cluster and got this working with the following fleet\nsystemd unit files\nUnfortunatly the documentation for grafana did not work for me at is\ninsisted in looking for a metadata endpoint for influxdb\nhttps://github.com/GoogleCloudPlatform/heapster/blob/master/influx-grafana/grafana/set_influx_db.sh#L11\nand elastic search\nhttps://github.com/GoogleCloudPlatform/heapster/blob/master/influx-grafana/grafana/set_elasticsearch.sh#L14.\nAfter deleting these and manually loading the kubernetes dashboard,\neverything worked (after editing a couple of graphs that were looking for a\n'machines' time series where only 'stats' existed).\ngrafana (manually)\ndocker run -i -t --rm -p 80:80 -e INFLUXDB_HOST=192.168.89.161 -e INFLUXDB_PORT=8086 -e INFLUXDB_NAME=k8s -e INFLUXDB_USER=root -e INFLUXDB_PASS=root tutum/grafana\ncadvisor (globally deployed)\n[Unit]\nDescription=cAdvisor Service\nAfter=docker.service\nRequires=docker.service\n[Service]\nTimeoutStartSec=10m\nRestart=always\nExecStartPre=-/usr/bin/docker kill cadvisor\nExecStartPre=-/usr/bin/docker rm -f cadvisor\nExecStartPre=/usr/bin/docker pull google/cadvisor\nExecStart=/usr/bin/docker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro --publish=4194:4194 --name=cadvisor --net=host google/cadvisor:latest --logtostderr --port=4194\nExecStop=/usr/bin/docker stop -t 2 cadvisor\n[X-Fleet]\nGlobal=true\nMachineMetadata=role=kubernetes\nInfluxdb\n[Unit]\nDescription=InfluxDB Service\nAfter=docker.service\nRequires=docker.service\n[Service]\nTimeoutStartSec=10m\nRestart=always\nExecStartPre=-/usr/bin/docker kill influxdb\nExecStartPre=-/usr/bin/docker rm -f influxdb\nExecStartPre=/usr/bin/docker pull kubernetes/heapster_influxdb\nExecStart=/usr/bin/docker run --name influxdb -p 8083:8083 -p 8086:8086 -p 8090:8090 -p 8099:8099 kubernetes/heapster_influxdb\nExecStop=/usr/bin/docker stop -t 2 influxdb\nHeapster agent (buddy)\n[Unit]\nDescription=Heapster Agent Service\nAfter=docker.service\nRequires=docker.service\n[Service]\nTimeoutStartSec=10m\nRestart=always\nExecStartPre=-/usr/bin/mkdir -p /home/core/heapster\nExecStartPre=-/usr/bin/docker kill heapster-agent\nExecStartPre=-/usr/bin/docker rm -f heapster-agent\nExecStartPre=/usr/bin/docker pull vish/heapster-buddy-coreos\nExecStart=/usr/bin/docker run --name heapster-agent --net host -v /home/core/heapster:/var/run/heapster vish/heapster-buddy-coreos\nExecStop=/usr/bin/docker stop -t 2 heapster-agent\n[X-Fleet]\nMachineOf=influxdb.service\nHeapster\n[Unit]\nDescription=Heapster Agent Service\nAfter=docker.service\nAfter=heapster-agent.service\nRequires=docker.service\nRequires=heapster-agent.service\n[Service]\nTimeoutStartSec=10m\nRestart=always\nExecStartPre=-/usr/bin/docker kill heapster\nExecStartPre=-/usr/bin/docker rm -f heapster\nExecStartPre=/usr/bin/docker pull vish/heapster\nExecStart=/usr/bin/docker run --name heapster --net host -e INFLUXDB_HOST=127.0.0.1:8086 -v /home/core/heapster:/var/run/heapster vish/heapster\nExecStop=/usr/bin/docker stop -t 2 heapster\n[X-Fleet]\nMachineOf=heapster-agent.service\nAt the moment i'm lazy and dont care if my heapster agents move around the\ncluster.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/271#issuecomment-62239188.\n. @MaheshRudrachar: Can you try using 'kubernetes/heapster' image instead of 'vish/heapster' ?\n\nHeapster\n`[Unit]\nDescription=Heapster Agent Service\nAfter=docker.service\nAfter=heapster-agent.service\nRequires=docker.service\nRequires=heapster-agent.service\n[Service]\nTimeoutStartSec=10m\nRestart=always\nExecStartPre=-/usr/bin/docker kill heapster\nExecStartPre=-/usr/bin/docker rm -f heapster\nExecStartPre=/usr/bin/docker pull vish/heapster\nExecStart=/usr/bin/docker run --name heapster --net host -e INFLUXDB_HOST=127.0.0.1:8086 -v /home/core/heapster:/var/run/heapster kubernetes/heapster\nExecStop=/usr/bin/docker stop -t 2 heapster\n[X-Fleet]\nMachineOf=heapster-agent.service`\n. @andrewwebber: I am exploring using a proxy as part of grafana to get to InfluxDB and ElasticSearch containers. This will help split up influxdb, elastic search and grafana. \nSince you mentioned that you run multiple containers on your etcd server, you could try placing resource limits on all the containers, or at least InfluxDB.\n. Adding a flag to the buddy sounds good. I opened\n https://github.com/GoogleCloudPlatform/heapster/issues/11. Lets continue\nthe discussion there.\nI made a small change to the grafana container via\nhttps://github.com/GoogleCloudPlatform/heapster/pull/10. This should make\nthe kubernetes version of heapster work outside of GCE. Give it a try and\nlet me know if you face any issues.\nOn Thu, Nov 13, 2014 at 4:35 AM, andrew notifications@github.com wrote:\n\n@MaheshRudrachar https://github.com/MaheshRudrachar @vishh\nhttps://github.com/vishh This is due to the fact that the heapster\nbuddy assumes fleet is running on the host\nhttps://github.com/GoogleCloudPlatform/heapster/blob/master/clusters/coreos/buddy.go#L35.\nI had this issue and therefore had to run my heapster agents on the etcd\nnode.\nhttps://github.com/GoogleCloudPlatform/heapster/blob/master/clusters/coreos/buddy.go#L35\nI believe we need to add a flag to the buddy to parameterise the fleet\nserver url\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/271#issuecomment-62883967.\n. Merging this would break our API and existing users. Should we add a compat layer  for backwards compatibility and bump up the version? \n. We can place a version label as part of our public types. I see this\npattern in Kubernetes. It will help at least help detect incompatibility.\n\nOn Tue, Nov 4, 2014 at 11:38 AM, Rohit Jnagal notifications@github.com\nwrote:\n\nI don't think its worth carrying rollback support pre 1.0. It would be\nbetter to just mark it as a compat breaking release through versioning and\nrelease notes.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/277#issuecomment-61700629.\n. Since the wire format remains the same, I agree that this change might not be that bad. \n. LGTM\n. LGTM\n. @gmelekh are you referring to the column 'container_name' in Influxdb? \n. LGTM\n. @rmanyari: Can you post logs from cadvisor? \n. cadvisor batches writes to InfluxDB. The default batch duration is 1 minute\n(--storage_driver_buffer_duration). I hope InfluxDB host:port is\naccessible from inside the cadvisor container.\n\nOn Thu, Jan 29, 2015 at 11:47 AM, rmanyari notifications@github.com wrote:\n\nThanks for taking some time to help me debug this @vishh\nhttps://github.com/vishh\nI looked this morning at the logs and found lots of these:\nE0129 19:37:50.348249 00001 memory.go:94] failed to write stats to\ninfluxDb - Post\nhttp://the_right_ip:8086/db/cadvisor/series?u=root&p=the_right_password&time_precision=u:\nwrite tcp 104.131.19.134:8086: connection refused\nE0129 19:37:54.365524 00001 memory.go:94] failed to write stats to\ninfluxDb - Post\nhttp://the_right_ip:8086/db/cadvisor/series?u=root&p=the_right_password&time_precision=u:\nwrite tcp 104.131.19.134:8086: connection timed out\nWhat's curious is that if I take the same url and paste it into postman\n(tool to push http requests) everything works fine. Is it possible that\ncdavisor is trying to write too many series at the time and its overloading\nmy influx at the other end? It's gathering data from a server that has ~350\ncontainers running.\nI also see some of this on the influx server:\nJan 29 14:38:53 influx kernel: [ 75.579469] perf samples too long (2640 >\n2500), lowering kernel.perf_event_max_sample_rate to 50000\nJan 29 14:39:02 influx kernel: [ 85.424844] perf samples too long (5024 >\n5000), lowering kernel.perf_event_max_sample_rate to 25000\nJan 29 14:39:46 influx kernel: [ 129.378845] perf samples too long (10034\n\n10000), lowering kernel.perf_event_max_sample_rate to 12500\nJan 29 14:41:43 influx kernel: [ 245.943875] perf samples too long (20011\n20000), lowering kernel.perf_event_max_sample_rate to 6250\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/293#issuecomment-72091236.\n. LGTM.\n. LGTM\n. Minor nits. LGTM overall\n. LGTM. This is a good feature :+1: \n. LGTM.\n. LGTM.\n. cAdvisor currently exposes only a read only interface and so auth isn't a\npart of our immediate roadmp. That said PRs are always welcome ;)\n\nOn Sun, Nov 9, 2014 at 9:55 AM, Gaston Morixe notifications@github.com\nwrote:\n\nHi guys!\nThis is great! any plan to add basic auth to the web client?\nThanks!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/302.\n. There are no system wide notifications AFAIK. We will have to scrape logs\nand generate events. Running containers could have suffered OOM kills and\nthey might have either dies or continue to live. So I suggest keeping the\nOOM information as a separate event stream.\n\nOn Tue, Dec 16, 2014 at 2:31 PM, Victor Marmol notifications@github.com\nwrote:\n\nWe're interested in container OOMs and system-wide OOMs. There are two\nsources of this information:\n- /var/log/messages where the kernel OOM killer\n  https://github.com/torvalds/linux/blob/master/mm/oom_kill.c writes\n  logs to. Although, this may be hard to parse since the output is\n  per-process.\n- Registering for OOM and OOM kill\n  https://github.com/docker/libcontainer/blob/master/cgroups/fs/notify_linux.go#L16\n  cgroup notifications (do the system-wide ones work?).\nWe should probably run some experiments with container and system OOMs and\nsee what we get in both of these.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/305#issuecomment-67245127.\n. Yeah. That might require tracking every pid and its cgroup. For the long\nrun, we should patch the kernel to provide more information at the least.\n\nOn Tue, Dec 16, 2014 at 2:44 PM, Victor Marmol notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh we need to somehow reconcile the two\nevent streams though since we don't get what container OOM'd from the\nkernel logs.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/305#issuecomment-67246963.\n. I like landing on the root cgroup page. It tells me if the machine is in trouble. I think navigating to the docker page is just a click away. We could may be provide this as a knob to users, if users ask for it.\n. I like that idea @vmarmol. Extending that, how about renaming subcontainers to 'All Containers' on the root cgroup landing page?\n. LGTM\n. LGTM\n. LGTM. Will merge once Travis is happy!\n. Ping @vmarmol @rjnagal \n. Shall we make a 0.6.1 release with this patch?\n. Ping @rjnagal \n. LGTM\n. I tried this change. This is better than the current state. However it might be better to state that there are no docker containers. As of now there is a link to 'Docker Containers' in \"Docker Containers\" page (/docker/) and it does not convey the fact that there are no docker containers.\n. Overall LGTM since this change gives an overview of all the resources docker is consuming on the host. \n. Is this to avoid a race caused by inotify?\n. LGTM.\n. In centOS cgroups are not mounted under \"/sys\". So you need to pass an\nadditional flag \"--volume=/cgroup:/cgroup\". Take a look at this page\nhttps://github.com/google/cadvisor/blob/master/docs/running.md.\n\nOn Thu, Dec 4, 2014 at 7:28 AM, liuyunsh notifications@github.com wrote:\n\nHi vmarmol, thanks for your response.\nYes, the space on that directory is enough.\nI tried the way you suggested, it seems that that container can work now,\nhowever,\nThe web page of http://ip:port come out a error as following:\nFailed to get container \"/\" with error: unable to find data for container /\nAny suggestion?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/333#issuecomment-65526156.\n. Can you confirm that you have indeed turned off Selinux?\n\nOn Wed, Apr 15, 2015 at 6:41 PM, DeShuai Ma notifications@github.com\nwrote:\n\n@vmarmol https://github.com/vmarmol More logs:\nhttp://fpaste.org/211498/91484841/\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/333#issuecomment-93614221.\n. @vmarmol: Just a few more comments. \n. Can you run gofmt on all files you have added/updated?\n\ncAdvisor panics when I test this PR. I ran cadvisor as follows: \n./cadvisor --logtostderr --http_auth_file test.htpasswd --http_digest_file test.htdigest\nHere is the stack trace:\nI1211 17:11:02.000674 13107 factory.go:80] Registering Raw factory\npanic: http: multiple registrations for /static/\ngoroutine 16 [running]:\nruntime.panic(0x7c1ae0, 0xc2080de680)\n        /usr/local/go/src/pkg/runtime/panic.c:279 +0xf5\nnet/http.(_ServeMux).Handle(0xc208022720, 0x9442f0, 0x8, 0x7f80bbc2e228, 0xc2080a70e0)\n        /usr/local/go/src/pkg/net/http/server.go:1527 +0x23c\nnet/http.(_ServeMux).HandleFunc(0xc208022720, 0x9442f0, 0x8, 0xc2080a70e0)\n        /usr/local/go/src/pkg/net/http/server.go:1555 +0x6e\nnet/http.HandleFunc(0x9442f0, 0x8, 0xc2080a70e0)\n        /usr/local/go/src/pkg/net/http/server.go:1567 +0x48\nmain.main()\n        /home/vishnuk/go/src/github.com/google/cadvisor/cadvisor.go:110 +0xa3b\n. Works for me now :+1: Thanks @dipankar!\n. LGTM\n. @kateknister: This is a useful enhancement to cAdvisor :+1: Can you squash your commits?\n. LGTM\n. LGTM\n. Making the table names configurable sounds like a good idea :+1: Go ahead and send out a PR if you have some spare cycles.\n. Works like a charm :+1: LGTM\n. LGTM\n. Wow. We are setting a high bar for PR review latency :100: \n. LGTM\n. LGTM\n. We can extend info.ContainerReference to hold the hostname instead of overloading alias which is meant to be different names assigned to a docker container.\n. @vmarmol, it is non-intuitive. Alias currently refers to the various\nstrings that can be used to refer to a docker container. Won't all docker\ncontainer's have a hostname? Aliases are also useful only with docker\ncontainers as of now.\nOn Wed, Dec 17, 2014 at 7:55 AM, Victor Marmol notifications@github.com\nwrote:\n\n@hinesmr https://github.com/hinesmr thanks for the patch! Can you\nsquash your commits and sign the CLA please:\nhttps://github.com/google/cadvisor/blob/master/CONTRIBUTING.md\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/371#issuecomment-67342874.\n. By query are you referring to the docker APIs? If yes, hostname cannot be used there. hostname is merely a metadata attached to a container. May be we can add a map as @hinesmr suggested to ContainerReference?\n. The new 'map' field should not impact existing users of our API. @hinesmr\nhttps://github.com/hinesmr will adding a metadata map to\nContainerReference satisfy your requirements?\n\nOn Wed, Dec 17, 2014 at 10:35 AM, Victor Marmol notifications@github.com\nwrote:\n\nBy query I mean that in the cAdvisor API I can ask for:\n/api/v1.2/docker/container-name\n/api/v1.2/docker/container-id\n/api/v1.2/docker/container-hostname\nand they would all return the information of that container.\nThe map does SGTM long-term as we use aliases more and more. It is a\nbreaking change in the JSON format though.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/371#issuecomment-67369287.\n. @hinesmr: Does your client have a list of docker container hostnames prior\nto communicating with cadvisor?\n\nOn Fri, Dec 19, 2014 at 9:43 AM, Victor Marmol notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh I think this means we'd need to add it\nto aliases.\n@hinesmr https://github.com/hinesmr I'm not sure I understood what you\nsaid about the alias? Could your software query by Docker ID or Docker name\ninstead?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/371#issuecomment-67670913.\n. LGTM\n. What docker storage driver is used in RHEL7?\n. LGTM\n. LGTM\n. LGTM. \n. LGTM\n. @josselin-c: You can use cadvisor docker image 'google/cadvisor:0.7.1'.\n. @yesnault: You can use cadvisor docker image 'google/cadvisor:0.7.1'.\n. LGTM. This is very useful for debugging!!!!\n. +1\n\nOn Mon, Dec 22, 2014 at 3:07 PM, Rohit Jnagal notifications@github.com\nwrote:\n\nglog doesn't support log rotation. There's an issue filed:\nhttps://code.google.com/p/google-glog/issues/detail?id=22\nIdeally, we would like to rotate logs at a reasonable size and keep the\nfirst and last file.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/389.\n. I like the idea.\n\nOn Mon, Dec 22, 2014 at 3:18 PM, Rohit Jnagal notifications@github.com\nwrote:\n\n@vmarmol https://github.com/vmarmol @vishh https://github.com/vishh\nWDYT of setting GLOG_max_log_size to 100M in the mean time. I think glog\njust stops logging after the limit is reached. I need to try that out.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/389#issuecomment-67903618.\n. Hmm... Do we really have to do something unless cadvisor logs a lot of\ndata?\n\nOn Tue, Dec 23, 2014 at 9:57 AM, Rohit Jnagal notifications@github.com\nwrote:\n\nI tried it out and GLOG_max_log_size doesn't work at all. I looked through\nglog code and it doesn't seem to exist anymore. We'll need to do something\nwithin cAdvisor.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/389#issuecomment-67979595.\n. @rjnagal: Can we close this in favor of #391? \n. How about resetting lastErrorTime whenever the state changes from 'error' to 'active'? This can be done in a separate PR too\n. LGTM\n. LGTM. \n. I am not aware of any other use case other than supporting systemd cgroup hierarchies.\n. I think systemd still uses only a subset of the cgroup wrt docker. But we don't look for stats in all the cgroups. So we might be fine. \n. LGTM\n. @rjnagal: Feel free to self merge once you have successfully tested it on all supported distros.\n. I have pushed a new 'canary' build with version 0.7.1. I tested on my machines and it seems to work fine.\n. I am not sure about deleting it. LGTM otherwise!\n. LGTM\n. Try this query: 'select container_name, derivative(cpu_cumulative_usage)\nfrom \"stats\" group by time(2), container_name order asc'\n\nOn Mon, Dec 29, 2014 at 9:17 AM, Mitchel Kelonye notifications@github.com\nwrote:\n\nCadvisor flushes data with a cpu_cumulative_usage col to influxdb. How can\nI get the % cpu usage from this? Thanks in advance.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/405.\n. Did you miss 'group by time(2)'?\n\nOn Tue, Dec 30, 2014 at 9:59 AM, Mitchel Kelonye notifications@github.com\nwrote:\n\nThanks .. seems I can't get one per container .. select\nderivative(cpu_cumulative_usage) from stats where container_name =\n'/docker/project_monitoring_1'. My use case is, I need metrics to\ngenerate such a graph:\nhttps://camo.githubusercontent.com/a1103d84c42f641713ba659e406d544e19f22e69/68747470733a2f2f646c2e64726f70626f7875736572636f6e74656e742e636f6d2f752f33303136323237382f53637265656e25323053686f74253230323031342d31322d31352532306174253230322e34312e3339253230414d2e706e67\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/405#issuecomment-68379695.\n. Can you post the output of '/api/v1.2/machine'?\n\nOn Tue, Dec 30, 2014 at 3:06 AM, Yvonnick Esnault notifications@github.com\nwrote:\n\nI'm not sure of the value of this information (url: /containers/ ):-)\n[image: screenshot]\nhttps://cloud.githubusercontent.com/assets/395454/5577912/0de73642-901c-11e4-9b57-5c7e6b3c468a.png\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/406.\n. Statically limiting sounds good. I wonder if all those are ext partitions? \n. LGTM\n. LGTM. \n. LGTM\n. LGTM\n. LGTM\n. LGTM.\n. LGTM\n. '/machine' is meant to provide static information only. If looking up two different APIs is an issue, I would suggest placing capacity along with usage in '/container'.\n. \"TestGetDiskStatsMap\" unit test is failing. Can you address the issue?\n. LGTM\n. LGTM\n. Seems to be a dup of #452 \n. LGTM\n. LGTM. Can you take a look at the integration test failures?\n. How about a third option:\nSeparate storage drivers into separate binaries and run them as sidecars?\nWe can provide the common infrastructure required to implement such side\ncars, and even build and ship them as part of a cadvisor release.\n\nThis frees cadvisor from any storage driver related issues. Any auth\nrequired for storage drivers can be packed as part of a separate image\nwithout having to wrap the base cadvisor image. It is even possible to\nexport different data to different endpoints without having to modify\ncadvisor.\nOn Fri, Jan 30, 2015 at 11:13 AM, Ankush Agarwal notifications@github.com\nwrote:\n\nCool. Then I will first refactor influxdb and bigquery config options in a\nPR and then open up another PR sometime next week to add a stable graphite\ndriver.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/474#issuecomment-72253857.\n. @kateknister this might be something you can work on.\n. Overall LGTM. Just a few comments.\n. By custom metrics Rohit is referring to future resources that cadvisor will/might track. Examples: Storage, Network, special hardware\n\n@rjnagal : Will we need derived stats for debugging and perf related metrics?\nThe resource type naming model for kubernetes is here\n. The main argument for an extensible API is that of making the rest of the system generic - in the case of kubernetes, whenever a new derived schedulable stats is exposed by cadvisor, and in-turn by kubelet, which is just a proxy as of now, the stats become immediately usable by the scheduler.\n. LGTM excepting for that one comment.\n. LGTM expect for the one comment. \n. LGTM. @rjnagal any comments from your side?\n. Travis is failing @vmarmol \n. LGTM\n. LGTM\n. Can you list cpu_cumulative_usage per container of host?\nWhich version of InfluxDB are you using? AFAIK there was a bug with derivatives in InfluxDB in the past.\n. Thats what I suspect @vmarmol. @cboggs can you post the entire line in InfluxDB (all the stats) for container 'spoon-daves-dev'? \n. Yup. We push filesystem stats separate from cpu stats. I wonder if that\naffects InfluxDB queries.\nOn Mon, Feb 9, 2015 at 2:41 PM, Cody Boggs notifications@github.com wrote:\n\nSure thing as soon as I get off the bus. Is just a 1-minute sample\nsufficient?\nOn Feb 9, 2015 3:35 PM, \"Vish Kannan\" notifications@github.com wrote:\n\nThats what I suspect @vmarmol https://github.com/vmarmol. @cboggs\nhttps://github.com/cboggs can you post the entire line in InfluxDB\n(all\nthe stats) for container 'spoon-daves-dev'?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/499#issuecomment-73605500.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/499#issuecomment-73606671.\n. Filesystem stats should move to a separate series I guess. I wonder why\nGrafana is not ignoring null values though.\n\nOn Mon, Feb 9, 2015 at 9:45 PM, Jonathan Chauncey notifications@github.com\nwrote:\n\nYeah I noticed this last week\nOn Feb 9, 2015 9:29 PM, \"Cody Boggs\" notifications@github.com wrote:\n\nSounds like you're onto something... Sequences that have FS stats are\nlacking CPU stats:\n[image: screen shot 2015-02-09 at 21 27 54]\n<\nhttps://cloud.githubusercontent.com/assets/3492944/6121537/95051d6e-b0a2-11e4-9bfd-002894fca077.png\n\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/499#issuecomment-73642662.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/499#issuecomment-73647953.\n. True. The reason for starting with a single series was to avoid metadata\nduplication. The usability gains might actually outweigh the cost of\nduplicating metadata.\n\nOn Tue, Feb 10, 2015 at 1:53 PM, Jonathan Chauncey <notifications@github.com\n\nwrote:\nI think having a series per stat would make the integration with grafana\nmuch easier. Then just have columns for hostname and container name for\ngrouping.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/499#issuecomment-73791972.\n. That's a bug. I will fix it soon! Good catch Cameron!\n\nOn Thu, Feb 18, 2016 at 11:23 AM, Cameron Davison notifications@github.com\nwrote:\n\nFYI. I just ran strace on the 'du' that is running, and realized that the\ndu for the cadvisor container disk usage itself is running over every\nsingle docker container for the entire system.\nlstat(\"/var/lib/docker/overlay/e14e4f15e1673095fa6562703280ac11a85d8c0e58cbf469199ed6d29249b130/merged/rootfs/var/lib/docker/overlay/e1b4d58b9cae8953890ffdd493d1f84e751dc0b2b2649d2772b0354d5ced6a10/\nthis seems undesirable to me.\nSince\n--volume=/:/rootfs:ro\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/501#issuecomment-185874392.\n. Is there a reason why the Prometheus server doesn't support a 'push' based\nmodel? The reason I ask is that we can very easily have a cadvisor side-car\nthat can periodically push metrics to prometheus server. This is the model\nwe are trying to move towards.\n\nOn Tue, Feb 10, 2015 at 8:52 AM, Victor Marmol notifications@github.com\nwrote:\n\nThis approach makes sense, I agree that we'd rather export the metrics\nhere than have a separate collector process. We've been looking to export\nother cAdvisor metrics (latency and usage data) through Prometheus so this\nlines up nicely. It looks like we can register the custom Collector as well\nas the other collectors we'd want to use, nice! Looking forward to trying\nthis out :) We'll certainly be poking you with questions.\ncAdvisor is primarily container-agnostic, the API exports all containers\non the machine. We're able to gather stats on Docker and non-Docker\ncontainers simultaneously. We do have some custom code for Docker\ncontainers to get some extra information and export those in a separate\nnamespace in the API (since it's a very common use).\nOn a slight side note, the other portion of this is in Heapster\nhttps://github.com/GoogleCloudPlatform/heapster. There we'd like to\ncollect Prometheus metrics from clients and possibly re-export them to a\nPrometheus backend. It seems like there is a library for the extraction\nhttps://github.com/prometheus/client_golang/tree/master/extraction\n(awesome!) and a custom Collector as you mentioned may help with\nre-exporting.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/502#issuecomment-73735880.\n. You can try using fluentd. \n. 'go get' will try to compile cadvisor. Try running 'go get -d\ngithub.com/google/cadvisor'. To build cadvisor, you need to use 'godep' -\n'godep go build github.com/google/cadvisor'.\n\nTip: Install 'godep' as follows: 'go get github.com/tools/godep'. Make sure\nbinaries inside 'GOBIN' are accessible\nOn Sat, Feb 14, 2015 at 10:55 PM, Nodir Kodirov notifications@github.com\nwrote:\n\nHello,\nI am trying to use Go client\nhttps://github.com/google/cadvisor/tree/master/client to get the status\nof my Docker instances. However, I get following error message when I\nexecute\n$ go get github.com/google/cadvisor/manager\ngithub.com/docker/libcontainer/cgroups/systemd\n.gvm/pkgsets/go1.4/global/src/\ngithub.com/docker/libcontainer/cgroups/systemd/apply_systemd.go:64: not\nenough arguments in call to theConn.StartTransientUnit\n.gvm/pkgsets/go1.4/global/src/\ngithub.com/docker/libcontainer/cgroups/systemd/apply_systemd.go:76: not\nenough arguments in call to theConn.StartTransientUnit\n.gvm/pkgsets/go1.4/global/src/\ngithub.com/docker/libcontainer/cgroups/systemd/apply_systemd.go:145: not\nenough arguments in call to theConn.StartTransientUnit\nI have Docker-maintained\nhttps://docs.docker.com/installation/ubuntulinux/ Ubuntu package. Here\nis the output from $ docker version\nClient version: 1.5.0\nClient API version: 1.17\nGo version (client): go1.4.1\nGit commit (client): a8a31ef\nOS/Arch (client): linux/amd64\nServer version: 1.5.0\nServer API version: 1.17\nGo version (server): go1.4.1\nGit commit (server): a8a31ef\nThe latest version of Go installed via gvm\nhttp://www.hostingadvice.com/how-to/install-golang-on-ubuntu/. Output\nfrom $ go version is\ngo version go1.4 linux/amd64\nAny help is appreciated,\nNodir.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/504.\n. You might want to take a look at heapster. It was built originally to collect data from all cadvisor nodes and attach arbitrary metadata. It might not work out-of-box for your use case, but it should not be too difficult to extend heapster to satisfy your use case.\n. LGTM. \n. LGTM.\n. LGTM. Will merge once the integration test is fixed.\n. LGTM\n. LGTM\n. LGTM. The e2e tests are failing on CoreOS.\n. AFAIK cadvisor will be one of the plugins for collectd. collectd along with\nother application metrics will also be able to export system resource usage\nmetrics exported by cadvisor.\nWhat is your desired behavior? Are you thinking about having collectd\nexport per container metrics?\n\nOn Sat, Mar 7, 2015 at 2:30 PM, Federico Simoncelli \nnotifications@github.com wrote:\n\n@vmarmol https://github.com/vmarmol I think it would be beneficial is\nsomeone can explain what this meant:\n- Use collectd\n  - cAdvisor plugin for collectd is still a WIP\nFrom: GoogleCloudPlatform/kubernetes#990\n  https://github.com/GoogleCloudPlatform/kubernetes/issues/990\n/cc @vishh https://github.com/vishh @lavalamp\nhttps://github.com/lavalamp @thockin https://github.com/thockin\nWhat was the idea there?\nWhat was the cAdvisor plugin for collectd going to do? Get metrics from\ncollectd?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/562#issuecomment-77714095.\n. LGTM\n. LGTM. \n. Can we bubble this error up through the '/validate' handler?\n. What other kernel interfaces will we end up stubbing for cadvisor to work in such environments?\n. Ack. LGTM\n. +1 on the recursive option.\nWhy not embed the namespace in the API itself? /stats/<container_type>/<container_name>?recursive=true? Most of our users are docker users too.\n. @vmarmol: Just curious.. How many of our users really care about raw cgroups? Aren't most of them are docker users? \nAlso why not drop container and just name it type?\n. LGTM\n. LGTM except for that one question.\n. I am suggesting /attributes instead of /version for all software stuff like kernel version, distribution, etc.\n/version can continue to return just cadvisor version.\n. +1 on /attributes\n. LGTM\n. Feel free to self merge once tests pass.\n. Tests are passing. Merging this PR.\n. ok to test\n. LGTM. Will merge once tests pass. \n. Yes. The API should allow querying stats at a lower resolution. What is the\nissue with dynamic housekeeping?\n\nOn Fri, Mar 20, 2015 at 5:30 PM, Victor Marmol notifications@github.com\nwrote:\n\nWe already have a global flag for collection interval. Do you mean: ask\nfor data at 2s intervals? We can do this only if your interval > our\ncollection interval. With dynamic housekeeping it gets interesting, but we\ncan return an error in those cases.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/605#issuecomment-84206769.\n. Nope. I mean just for the query.\n\nOn Fri, Mar 20, 2015 at 5:38 PM, Victor Marmol notifications@github.com\nwrote:\n\nDo you mean an API to dynamically adjust the collection frequency?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/605#issuecomment-84207772.\n. @rjnagal: Any progress on this? New contributors are confused because of this split.\n. ok to test\n\nOn Mon, Mar 23, 2015 at 10:03 AM, cadvisorJenkinsBot \nnotifications@github.com wrote:\n\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/610#issuecomment-85097196.\n. Curious: How does this change help deal with the kubelet part of the issue?\n. LGTM\n. We should separate time range queries from the number of stats. Requiring a max number of stats for time range queries is non intuitive personally.\n. LGTM\n. Is ELK built to handle metrics data?\n\nOn Tue, Apr 7, 2015 at 1:20 AM, \u9648\u98de notifications@github.com wrote:\n\nELK stack is heavily used, but cAdvisor only support influxdb. So we want\nto implement a log stash forward drive base on Lumberjack Protocol.\nIs it a good idea?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/634.\n. Ok! Interesting! Are you interested in posting a PR to add elasticsearch as\na backend?\n\nOn Tue, Apr 7, 2015 at 8:38 PM, wangjingbomail notifications@github.com\nwrote:\n\nIt is a good idea\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/634#issuecomment-90794653.\n. Great!\n\nOn Wed, Apr 8, 2015 at 7:26 PM, \u9648\u98de notifications@github.com wrote:\n\n@vishh https://github.com/vishh I'm working on it.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/634#issuecomment-91095253.\n. \"hot\" refers to the working set, which is held in RAM, and \"total\" refers\nto all the memory used by a process, a part of which may reside on a swap\ndevice.\n\n+1 to your suggestion of documenting it. AFAIK this isn't documented\nanywhere.\nOn Wed, Apr 8, 2015 at 1:45 PM, Sean Humbarger notifications@github.com\nwrote:\n\nWhen looking at the total memory usage of a running container, the legend\nshows \"hot\" and \"total\". Is there any documentation that explains the\ndefinition \"hot\" and \"total\" and how that is calculated?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/638.\n. AFAIK it does not account for swap as of now.\n\nOn Mon, Oct 26, 2015 at 4:08 AM, mboussaa notifications@github.com wrote:\n\n@vishh https://github.com/vishh : the memory usage aggregated by\ncadvisor is the total hot (working set) memory + cold pages + swap that's\nright?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/638#issuecomment-146885813.\n. working_set is not shared between containers. Are you running with swap\nenabled?\n\nOn Wed, Feb 24, 2016 at 7:17 PM, John Yani notifications@github.com wrote:\n\nIs working set shared between containers? Because if I sum up all\ncontainer's working sets it's well above the total memory. And all\ncontainers' working sets are exactly the same.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/638#issuecomment-188584886.\n. LGTM\n. We can add a regex whitelist to control the metrics being pushed by\nindividual storage backends. Is this something you are interested in\nworking on? We are happy to take a PR for this!\n\nOn Wed, Apr 15, 2015 at 11:53 PM, Allen Sun notifications@github.com\nwrote:\n\nconfigure which of the existing metrics to export @vmarmol\nhttps://github.com/vmarmol\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/654#issuecomment-93659109.\n. LGTM\n. LGTM. Lets get '/docker' HTTP endpoint to work and then re-commit this patch.\n. LGTM\n. LGTM\n. Jenkins is happy. Safe to merge?\n. cc @timstclair \n. cAdvisor runs du for aufs and overlayfs storage drivers to calculate\nfilesystem usage per container. We reduce the nice level of du such\nthat it does not affect higher priority tasks and also run it less often.\nI'm surprised it is still a concern.\nWe can either turn it off completely or run it in a sub-cgroup to cap cpu.\n\nOn Tue, Feb 16, 2016 at 10:52 AM, Cameron Davison notifications@github.com\nwrote:\n\nI noticed that at least 1 of my machines has been running a 'du -s' on the\noverlay fs for cAdvisor that seems to be taking a long time.\nps aux output\nroot      9875 29.2  1.9  81824 80908 ?        DN   16:27  41:27 du -s /var/lib/docker/overlay/8c4ffb38e76761b9fa5c29bafa272ea3e8d7509716fc5b0c8c3ace98f6ec998e\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/735#issuecomment-184825899.\n. That is weird. I suspect a slow disk. I'm posting\nhttps://github.com/google/cadvisor/pull/1117 to try to timeout long dus.\n\nOn Tue, Feb 16, 2016 at 10:57 AM, Cameron Davison notifications@github.com\nwrote:\n\nIt is not a concern, I guess I am just trying to help to identify the\nproblem. I just that that it was interesting that it had a TIME of 41:27\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/735#issuecomment-184830197.\n. We are waiting on ext4 support for project quota to stabilize. It's been in\nthe roadmap for nearly a year now.\n\nOn Mon, May 15, 2017 at 6:18 PM, Bryon Roch\u00e9 notifications@github.com\nwrote:\n\nSo i've dealt with this in a decent number of environments before, it\nlooks like in mesos-land they are addressing this sort of thing in\nmesos-1.3 via the xfs-project-quota/group quota isolator, which newer\ndockers also support through corresponding controls -- maybe the solution\nis to detect such a quota isolator and use that, or implemnt that as a\ntoggleable metric vis-a-vis the default disk metrics?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/735#issuecomment-301647951,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKGru6SZWQBiB3GPIYvioCyR-ZZx5ks5r6PlkgaJpZM4ElATY\n.\n. No updates sadly. Support for project quota needs to land in container\nruntimes and k8s for cAdvisor to make use of it.\n\nOn Thu, Jul 19, 2018 at 4:22 AM Andrey Arapov notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh any updates from Google on this issue?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/735#issuecomment-406243196,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKKnDRjo-Bg7OdXXLaY6mltJdIU9xks5uIGwFgaJpZM4ElATY\n.\n. > there is a way to set so called \"project quota\" in some filesystems and it can be done in cadvisor out of the box\n\nFYI: Project quota (or traditional group quota) needs to be set by container management systems and then cAdvisor (or any other monitoring tool) can use the quota APIs to query usage.\n\nIs there a way to disable calculating disk usage per container?\n\nYes. You can set --disable-metrics=disk.\n\nWill disabling du calculation reduce cadvisor CPU load?\n\nThis issue seems to suggest so, but it's not clear under what scenarios that is valid - # of containers, or # of directories within containers, or something else?\n\nIs it specific to ext4? Why can't it be implemented on on xfs only?\n\nProject Quota support exists in both ext4 and xfs. As I mentioned above, container management systems should first start using Quota APIs in-order for cAdvisor to take advantage of it. \n. Here's the one for k8s -\nhttps://github.com/kubernetes/kubernetes/issues/43607\nMoby maybe this one - https://github.com/moby/moby/issues/34702\nQuota assignment could happen underneath OCI, although the overall system\narchitecture needs to be sorted out first.\nAre you using docker directly or via k8s?\nOn Mon, Jul 30, 2018 at 8:57 PM John Yani notifications@github.com wrote:\n\nOk, so the blocker is not in cAdvisor, but in container runtimes, right?\nIs there a tracking issue in any of these projects:\nhttps://github.com/opencontainers/runc/\nhttps://github.com/containerd/containerd\nhttps://github.com/moby/moby\n?\nIs it something that should be handled by OCI runtime spec\nhttps://github.com/opencontainers/runtime-spec?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/735#issuecomment-409086655,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKHH71VWotDHYjm9dYiB0L3c6-54aks5uL9W-gaJpZM4ElATY\n.\n. Don't you have to group by time?\n\nOn Thu, Mar 3, 2016 at 6:46 AM, mboussaa notifications@github.com wrote:\n\nselect derivative(cpu_cumulative_usage) from stats where\ncontainer_name='execution'\ncalculate the derivative over all time stamp ? Because it shows only one\nderivative value\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/740#issuecomment-191795915.\n. We are waiting on InfluxDB client libraries to be cleaned up in their upcoming v0.9.5 release. \n. LGTM\n. ok to test\n\nOn Wed, Jun 10, 2015 at 3:09 PM, cadvisorJenkinsBot \nnotifications@github.com wrote:\n\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/769#issuecomment-110931799.\n. @AnanyaKumar: This PR seems to be mostly fine. \n. @rjnagal: Excepting disk usage calculation, cAdvisor does not poke at any\nof these directories right?\n\nOn Fri, Aug 28, 2015 at 12:26 AM, Jihoon Chung notifications@github.com\nwrote:\n\nsame here with CentOS + Docker 1.8.1(devicemapper)\nHad to remove --volume=/:/rootfs:ro &&\n--volume=/var/lib/docker:/var/lib/docker:ro\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/771#issuecomment-135661164.\n. The best we can do for now is to let users optionally disable filesystem\nusage metrics. We are waiting for some of the new upstream kernel features\nto simplify disk accounting.\n\nOn Tue, Jan 26, 2016 at 2:51 PM, Sven M\u00fcller notifications@github.com\nwrote:\n\nany updates regarding this?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/771#issuecomment-175277349.\n. cAdvisor doesn't mount anything. It runs du periodically to collect\nfilesystem stats. Other than that, it does not touch the container's\nfilesystem at all.\nThe easy fix for this would be to retry docker deletion or disable\nfilesystem aggregation in cadvisor.\n\nOn Wed, Feb 3, 2016 at 2:57 PM, Alex Rhea notifications@github.com wrote:\n\n+1\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/771#issuecomment-179518025.\n. @xbglowx Are you using the latest cadvisor release?\n. cc @timstclair \n. @ashkop Can you try running cAdvisor with --disable_metrics=\"tcp,disk\" and see if that resolves the issue? Note that you will not get docker container filesystem metrics by adding this flag.\n. @timstclair Can we make a v0.23.1 release with the fix for --disable_metrics flag?\n. @rjnagal Disk metrics should be the only dependency. Disabling that by using --disable_metrics=tcp,disk should fix this issue.\n. @rjnagal AFAIK, it is not limited to devicemapper alone. AUFS is also affected. If we need a default solution, we will have to disable per-container disk metrics by default.\n. AFAIK, it should be just the disk usage metrics\n\nOn Tue, Jul 19, 2016 at 2:38 PM, Shane StClair notifications@github.com\nwrote:\n\nThis issue is hitting us often and affecting production container\ndeployments (Debian 8.5 hosts, Docker 1.11.1).\nCan anyone spell out what we lose by omitting the /:/rootfs:ro mount? Is\nit just disk usage metrics?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/771#issuecomment-233774348,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKN3e53lwmDwcVP7hDBloCHdfD_Dsks5qXUO_gaJpZM4FBIxe\n.\n. @Marmelatze: Can you rebase?\n. Yeah. Feel free to work on top of existing patches.\n\nOn Tue, Dec 15, 2015 at 10:34 AM, Daniel, Dao Quang Minh \nnotifications@github.com wrote:\n\n@vishh https://github.com/vishh @rjnagal https://github.com/rjnagal\n@jimmidyson https://github.com/jimmidyson i like to use this patch.\nShould i take over and do a rebase on behalf of @Marmelatze\nhttps://github.com/Marmelatze ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/780#issuecomment-164849159.\n. Nice idea! :+1: LGTM\n. FYI: google/cadvisor:v0.20.5 should include support for InfluxDB v0.9\n\nOn Wed, Feb 3, 2016 at 5:30 PM, Marcello de Sales notifications@github.com\nwrote:\n\n@svenmueller https://github.com/svenmueller I tested a PR and it is now\nsupported... #1040 (comment)\nhttps://github.com/google/cadvisor/pull/1040#issuecomment-176015272\nI built an image to help others at\nhttps://hub.docker.com/r/marcellodesales/google-cadvisor/... I will\nmaintain it until the team releases a new version with it.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/794#issuecomment-179564777.\n. In general, we don't believe in the latest tag. But if you are relying on\nit, we can update that tag as well.\n\nOn Fri, Feb 5, 2016 at 11:33 AM, Marcello de Sales <notifications@github.com\n\nwrote:\n@vishh https://github.com/vishh I see the new image works at the tag\nyou mentioned.\ninfluxdb_1     | [http] 2016/02/05 19:25:47 172.17.0.2 - root [05/Feb/2016:19:25:47 +0000] POST /write?consistency=&db=cadvisor&precision=&rp= HTTP/1.1 204 0 - cAdvisor/0.20.5 41f62cc0-cc3e-11e5-8007-000000000000 24.01661ms\nHowever, the latest tag is not pointing to the v0.20.5 version. What's\nthe process to update the tag?\n$ docker images | grep cadvisor\ngoogle/cadvisor                                           v0.20.5                       cb4f76a7607a        46 hours ago        43.12 MB\ngoogle/cadvisor                                           latest                        cec6ac4b467e        10 days ago         36.46 MB\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/794#issuecomment-180512176.\n. I have updated the latest tag to point to v0.20.5.\n\nOn Fri, Feb 5, 2016 at 3:20 PM, Vishnu Kannan vishnuk@google.com wrote:\n\nIn general, we don't believe in the latest tag. But if you are relying\non it, we can update that tag as well.\nOn Fri, Feb 5, 2016 at 11:33 AM, Marcello de Sales \nnotifications@github.com wrote:\n\n@vishh https://github.com/vishh I see the new image works at the tag\nyou mentioned.\ninfluxdb_1     | [http] 2016/02/05 19:25:47 172.17.0.2 - root [05/Feb/2016:19:25:47 +0000] POST /write?consistency=&db=cadvisor&precision=&rp= HTTP/1.1 204 0 - cAdvisor/0.20.5 41f62cc0-cc3e-11e5-8007-000000000000 24.01661ms\nHowever, the latest tag is not pointing to the v0.20.5 version. What's\nthe process to update the tag?\n$ docker images | grep cadvisor\ngoogle/cadvisor                                           v0.20.5                       cb4f76a7607a        46 hours ago        43.12 MB\ngoogle/cadvisor                                           latest                        cec6ac4b467e        10 days ago         36.46 MB\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/794#issuecomment-180512176.\n. cAdvisor should expose container env variables as part of its API. Have you\nlooked at it?\n\n\nOn Fri, Mar 4, 2016 at 11:20 AM, Anthony Scalisi notifications@github.com\nwrote:\n\n@jmaitrehenry https://github.com/jmaitrehenry do you know if there's a\nway to use a Docker env variable as the docker container name ? Kinda like\nwhat was done with Prometheus backend: #546\nhttps://github.com/google/cadvisor/issues/546\nWhen running in a Mesos environment, containers are just IDs:\n4f7c0aa8d03a        java:8                   \"/bin/sh -c 'JAVA_OPT\"   4 hours ago         Up 4 hours          0.0.0.0:31926->8080/tcp   mesos-29e183be-f611-41b4-824c-2d05b052231b-S6.3dbb1004-5bb8-432f-8fd8-b863bd29341d\n66f2fc8f8056        java:8                   \"/bin/sh -c 'JAVA_OPT\"   4 hours ago         Up 4 hours          0.0.0.0:31939->8080/tcp   mesos-29e183be-f611-41b4-824c-2d05b052231b-S6.60972150-b2b1-45d8-8a55-d63e81b8372a\nf7382f241fce        java:8                   \"/bin/sh -c 'JAVA_OPT\"   4 hours ago         Up 4 hours          0.0.0.0:31656->8080/tcp   mesos-29e183be-f611-41b4-824c-2d05b052231b-S6.39731a2f-d29e-48d1-9927-34ab8c5f557d\n880934c0049e        java:8                   \"/bin/sh -c 'JAVA_OPT\"   23 hours ago        Up 23 hours         0.0.0.0:31371->8080/tcp   mesos-29e183be-f611-41b4-824c-2d05b052231b-S6.23dfe408-ab8f-40be-bf6f-ce27fe885ee0\n5eab1f8dac4a        java:8                   \"/bin/sh -c 'JAVA_OPT\"   44 hours ago        Up 44 hours         0.0.0.0:31500->8080/tcp   mesos-29e183be-f611-41b4-824c-2d05b052231b-S6.5ac75198-283f-4349-a220-9e9645b313e7\nb63740fe56e7        java:8                   \"/bin/sh -c 'JAVA_OPT\"   44 hours ago        Up 44 hours         0.0.0.0:31382->8080/tcp   mesos-29e183be-f611-41b4-824c-2d05b052231b-S6.5d417f16-df24-49d5-a5b0-38a7966460fe\n5c7a9ea77b0e        java:8                   \"/bin/sh -c 'JAVA_OPT\"   2 days ago          Up 2 days           0.0.0.0:31186->8080/tcp   mesos-29e183be-f611-41b4-824c-2d05b052231b-S6.b05043c5-44fc-40bf-aea2-10354e8f5ab4\n53065e7a31ad        java:8                   \"/bin/sh -c 'JAVA_OPT\"   2 days ago          Up 2 days           0.0.0.0:31839->8080/tcp   mesos-29e183be-f611-41b4-824c-2d05b052231b-S6.f0a3f4c5-ecdb-4f97-bede-d744feda670c\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/798#issuecomment-192425291.\n. Yeah. The backend needs to export it.\n. Sure. Will review soon.\n. @mnuessler: I'm not able to review this patch using github. Can you remove the godep changes temporarily? Once the review is complete, you can add it back.\n. Can you squash all non godeps changes into a single commit? It will help\nhaving to juggle between commits.\n\nOn Thu, Sep 17, 2015 at 11:57 AM, Matthias N\u00fc\u00dfler notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh I already split the change into two\ncommits for that reason. There is one commit containing only the godep\nchanges and another one containing only the code changes. Please have a\nlook, maybe that is sufficient?\nUnfortunately the InfluxDB client pulls in a plethora of additional\ndependencies...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/800#issuecomment-141188357.\n. Just a few nits. Overall LGTM. Thanks @mnuessler for working on this! And apologies for the slow review! Do you care to post an upgrade guide? Upgrading to 0.9 is non-trivial AFAIK.\n. There was an issue with network statistics which has been fixed on HEAD.\nThe next release of cadvisor should surface network stats without any\nissues.\n\nOn Fri, Sep 18, 2015 at 4:56 PM, Aman Mangal notifications@github.com\nwrote:\n\nWhy doesn't influxdb show any network interfaces? Is that the expected\nbehavior? I was trying to use these changes in my cadvisor-influxdb-grafana\nsetup.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/800#issuecomment-141597957.\n. @mnuessler: Any updates on this PR?\n. One option until influxDB v0.9.5 is released is to maintain a separate\nbranch for InfluxDB v0.9 support. Will that be helpful?\n\nOn Mon, Oct 19, 2015 at 1:59 AM, Dmitry Smirnov notifications@github.com\nwrote:\n\nI hope they do not release on schedule but given frequency of previous\nreleases and the fact that the last release was about 20 days ago the new\nrelease might be just within weeks from now. Hopefully that's not too long\nand I also hope there is no rush to commit pre-release here...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/800#issuecomment-149151088.\n. @mnuessler: Any updates?\n. +1 for the feature. Unfortunately the core team doesn't have enough cycles\nfor working on this feature. A PR will be much appreciated!\n\nOn Fri, Feb 26, 2016 at 8:33 AM, mboussaa notifications@github.com wrote:\n\nAny advance on Exporting CPU utilization per core to InfluxDB?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/807#issuecomment-189360736.\n. ok to test\n. LGTM\n. May be we can clone old libcontainer code for network stats in cadvisor and\nuse that for older docker versions?\n\nOn Mon, Aug 24, 2015 at 11:47 AM, Jimmi Dyson notifications@github.com\nwrote:\n\n@rjnagal https://github.com/rjnagal One problem of using libnetwork is\nit has a dependency to later version of docker when docker network plugins\nwere introduced. I'm not quite sure how to proceed without updating the\ndocker dependency which I'm sure you don't want to do. Any thoughts?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/822#issuecomment-134335089.\n. ok to test\n. @jimonreal: You can override the default hostname that docker sets. So this PR should work even with docker.\n. We might have to define Pod level APIs in cAdvisor. We wish to do this even\nfor the Docker runtime. We can probably start with hyper Pod support.\n\nOn Thu, Jan 7, 2016 at 5:53 PM, feisky notifications@github.com wrote:\n\n@yifan-gu https://github.com/yifan-gu @yujuhong\nhttps://github.com/yujuhong Any updates on this? I'm working on\ncAdvisor collectors for hyper, though cAdvisor is not so easy to extend to\na new container runtime.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/858#issuecomment-169864047.\n. @feiskyer: I have some idea, but I don't have the time to work on it now. Do you have some spare cycles to work on it? If so, I can help with the design.\n. @feiskyer, @timstclair will be able to help you. He is working on separating cAdvisor from kubelet.\n. The Docker specific APIs can be a no-op on hosts that don't run docker.\nAre there any generic APIs that require docker though?\n\nOn Tue, Jan 12, 2016 at 2:32 PM, Shaya Potter notifications@github.com\nwrote:\n\nHey, in order to try to get rkt working with cadvisor, I've started\nlooking at manager and want to isolate the docker code from the interface.\nWhat I want to do is copy the majority of the implementation code to a new\npackage \"docker\" and have the manager's functions call the docker\nimpelementations functions.\nWe can then implement a rkt implementation as well and have some smarts in\nthe main manager to select which runtime we care about (or both).\nthe main niggle is that some of the API calls seems docker specific (but\nreally aren't), I don't see a way to make it generic at this point in time\nw/o breaking the API so going to leave them the same.\nthoughts?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/858#issuecomment-171081578.\n. LGTM\n. ok to test\n. LGTM. Merging. Thanks for the PR @mikedanese \n. ok to test\n. LGTM\n. retest this please\n. @cadvisorJenkinsBot: retest this please\n. ok to test\n. To summarize the issue, kubelet needs to expose cpu (and possibly network, diskio) usage in units that are similar to how users configure limits, which is in milli cores. One option we had is to perform caching and deriving instantaneous values in kubelet. But that seemed wasteful because cAdvisor can easily (cheaply maybe) calculate instantaneous data and expose it. \nOur dynamic housekeeping might end up varying the resolution for instantaneous usage over time though.\n. #860 \n. ok to test\n. Thanks for this PR @jimmidyson :100: \n. The e2e tests for network stats are failing.\n. The integration tests are run against a few linux distros. Here are the\ninstruction for running it against your VM:\nhttps://github.com/google/cadvisor/blob/master/docs/integration_testing.md#integration-testing-cadvisor\n\nOn Tue, Aug 25, 2015 at 12:51 PM, Jimmi Dyson notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh Looking back looks like the e2e tests\nhave failed on most PRs recently... although I would have expected this PR\nto fix those failures :)\nI have just tested it manually against coreos beta (the reported failure)\n& it works fine. I can't work out how to run the integration tests manually\nagainst a local VM - seems like the integration tests are coupled to GCE...\nnot very friendly IMO. Any way to run these locally against a VM?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/865#issuecomment-134718778.\n. Works for me! Thanks @jimmidyson!! LGTM.\n. We can update cadvisor deps right away in kubernetes. I am not sure when\nthe next Kubernetes release will happen though.\n\nOn Tue, Aug 25, 2015 at 3:44 PM, Jimmi Dyson notifications@github.com\nwrote:\n\nThanks @vishh https://github.com/vishh.\nDon't suppose there's any chance we could get a release cut with this fix\nto update deps in kubernetes & openshift?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/865#issuecomment-134762661.\n. I think tagging makes sense! Created a tag '0.16.0.1'.\n\nOn Tue, Aug 25, 2015 at 3:50 PM, Jimmi Dyson notifications@github.com\nwrote:\n\nAh OK so no need to tag cadvisor first? Just use a non tagged commit in\nkubernetes godeps? As long as it's in next release whenever that is I'm\nhappy.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/865#issuecomment-134763474.\n. Ah! Thats true. It cannot access the procfs of host. Do we have to\nguarantee backwards compat with old kernels? Figuring out veth interfaces\nand getting stats via sysfs seems to be way cheaper than switching\nnamespaces every second for every container.\n\nOn Wed, Aug 26, 2015 at 12:40 AM, Jimmi Dyson notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh One thing I have noticed in further\n(post-merge ;)) testing is when running in a Docker container cadvisor\nneeds to be in the hosts pid namespace (see\nhttps://docs.docker.com/reference/run/#pid-settings-pid) to be able to\nswitch network namespaces based on pid of the docker containers. I don't\nsee this as a problem but something that needs to be documented. Shall I do\nthat?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/865#issuecomment-134879976.\n. Hmm. If we were to be network setup agnostic, I agree that entering the\nnamespace might be the best strategy.\n\nOn Wed, Aug 26, 2015 at 1:28 PM, Jimmi Dyson notifications@github.com\nwrote:\n\nThis is the approach of libnetwork & how docker itself describes how to do\nhigh performance metrics collection - see\nhttps://docs.docker.com/articles/runmetrics/#tips-for-high-performance-metric-collection\n.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/865#issuecomment-135160992.\n. Yes. I agree. What I meant by network agnostic was not having to know if\nthe underlying networking mechanism is veth or ipvlan, etc.\n\nOn Wed, Aug 26, 2015 at 2:00 PM, Jimmi Dyson notifications@github.com\nwrote:\n\nI think we can use this technique for all versions of docker & get rid of\nall the veth handling code.\nI'm not sure what you mean by network setup agnostic? Aren't we? Shouldn't\nwe be?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/865#issuecomment-135168987.\n. ok to test\n. ok to test\n. ok to test\n. LGTM. Thanks again @jimmidyson! :)\n. ok to test\n. LGTM. I don't think we have any e2e tests for prometheus. \n. ok to test\n. cc @vmarmol.\n\nOn Mon, Oct 12, 2015 at 5:00 PM, Dmitry Smirnov notifications@github.com\nwrote:\n\nPing?\nThere are more issues with bundled files. There are no sources included\nfor Bootstrap JS/CSS and jQuery (but at least they can be found in web).\nHowever I could not find origins of containers_css.go and gcharts_js.go.\nAlso contents of google_jsapi_js.go seems to be copied from\nhttps://www.google.com/jsapi but where is the source?? Seems that #304\nhttps://github.com/google/cadvisor/issues/304 did it all wrong. :(\nPlease advise.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/879#issuecomment-147552296.\n. :) I'm fine with removing it from the binary. I'm not sure why it got added\nin the first place.\n\nOn Tue, Oct 13, 2015 at 1:49 AM, Jimmi Dyson notifications@github.com\nwrote:\n\nWhile taking a quick look at this (go rice\nhttps://github.com/GeertJohan/go.rice looks promising btw) I noticed\nthat locally hosting the JS for the charting library breaks Google's ToS:\nsee both https://developers.google.com/chart/interactive/faq#offline &\nhttps://developers.google.com/chart/interactive/faq#localdownload.\n@vishh https://github.com/vishh Thoughts? Do we need to remove the\nlocal copies of gcharts & jsapi so as not to break Google's (your\nemployer's) ToS in one of their own OSS projects?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/879#issuecomment-147650596.\n. ok to test\n. Can you check the output of /validate endpoint of cadvisor? I suspect\nthat it is not able to reach docker daemon. This can happen if cAdvisor is\nrun without the required privileges. ALso try restarting cAdvisor.\n\nOn Fri, Sep 11, 2015 at 6:01 AM, Nikolay Gorylenko <notifications@github.com\n\nwrote:\nHi,\ncurrently \"subcontainers\" view display only container id:\n[image: image]\nhttps://cloud.githubusercontent.com/assets/1862997/9815374/cf2971a2-5895-11e5-9925-35a8d52eb2f5.png\nIt would be awesome to see container name in this table as well.\nOtherwise, when you're looking for specific container's stats, one-by-one\npicking is required.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/883.\n. With the latest version I do see the docker container alias show up in the subcontainers list. So I am not sure why it does not work for you. \n\n\n. Yeah. Raw cgroups based container's don't have an alias. The UI and the\ndependent APIs can be updated to make all the pages include alias, if one\nis available.\nOn Mon, Sep 14, 2015 at 11:11 AM, Nikolay Gorylenko \nnotifications@github.com wrote:\n\n@vishh https://github.com/vishh i finally figured out - you're right,\nit is possible to see containers if you click on \"Docker Containers\"\n(/docker) blue link:\n[image: image]\nhttps://cloud.githubusercontent.com/assets/1862997/9857712/79ddf090-5b1c-11e5-8bba-f2355fcce100.png\nContainer names are not visible when i click \"docker\" (/containers/docker)\nor \"/user\" (/containers/user) in Subcontainers\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/883#issuecomment-140163456.\n. ok to test\n. @hacpai: Thanks for the PR! AFAIK there are no integration tests for elasticsearch driver. It might be worth adding an integration test incase you rely on this feature to be stable.\n. LGTM\n. LGTM.\n. Thanks @rjnagal. Shall we make a new release?\n. ok to test\n\nOn Tue, Sep 15, 2015 at 6:29 AM, cadvisorJenkinsBot \nnotifications@github.com wrote:\n\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/886#issuecomment-140393581.\n. Just a couple of nits. Ping @rjnagal \n. LGTM. @shahidhs-ibm: All the tests are failing. Can you address the test issues?\n. ok to test\n\nOn Tue, Oct 20, 2015 at 2:20 PM, cadvisorJenkinsBot \nnotifications@github.com wrote:\n\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/886#issuecomment-149706888.\n. ok to test\n. I have restarted the test.\n\nOn Fri, Oct 30, 2015 at 6:58 AM, Shahid notifications@github.com wrote:\n\n@vishh https://github.com/vishh\nI tried to reproduce the failure of the 'default' build on my system but\nunable to reproduce it. Please let me know if the issue is related to my\npull request or the environment on which build is running.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/886#issuecomment-152532667.\n. LGTM. Merging now. Thanks for the contribution @shahidhs-ibm!\n. You can setup a CI independently and post the build status to PRs on\ngithub. Typically a bot account is setup on github for CI purposes. I can\ngrant the necessary privileges for the bot account to post build status.\n\nOn Tue, Dec 8, 2015 at 3:47 AM, Shahid notifications@github.com wrote:\n\n@vishh https://github.com/vishh\nI am not sure if this is right channel to discuss this topic but we would\nlike to know if cAdvisor CI can be extended to support z system. Please let\nus know what support will be required from IBM side (we are aware on the\nH/w support).\nLet me know if you want to start this discussion on some other channel.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/886#issuecomment-162859405.\n. > 1. How frequent the CI run on average?\n\nFor every PR push. Take a look at ghprb on Jenkins.\n\n\nWhat is the recommended H/W and configuration requirement\n\n\nNone AFAIK. Just build and test against your platform.\n\n\nSpecial S/W requirements (if any)\n\n\nNone AFAIK.\nOn Wed, Dec 9, 2015 at 5:28 AM, Shahid notifications@github.com wrote:\n\nThanks vish for the information.\nCan you please provide me more details on how the current CI\ninfrastructure is like?\nAlso I need below details in order to setup the CI infrastructure on z\nSystems:\n1. How frequent the CI run on average?\n2. What is the recommended H/W and configuration requirement\n3. Special S/W requirements (if any)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/886#issuecomment-163231602.\n. We cannot expand to test more platforms at the moment. If you are planning on testing Kubernetes, I believe there is an on-going effort to have distributed testing. I would recommend joining the kubernetes sig-testing.\n. https://kubernetes.slack.com/\nhttps://kubernetes.slack.com/messages/sig-testing/ ==>  #sig-testing\nchannel\n. ok to test\n. ok to test\n\nOn Fri, Sep 18, 2015 at 8:57 PM, cadvisorJenkinsBot \nnotifications@github.com wrote:\n\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/888#issuecomment-141617845.\n. @f0: Will you be able to post a PR for this? \n. I'd recommend updating just v2. By import what are you referring to?\n\nOn Tue, Sep 22, 2015 at 11:35 AM, f0 notifications@github.com wrote:\n\n@vishh https://github.com/vishh should info v1 and v2 supported? i see\nonly v1 imported\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/889#issuecomment-142376671.\n. I will make a release before the end of this week.\n\nOn Tue, Sep 22, 2015 at 3:34 PM, Jordan Glassman notifications@github.com\nwrote:\n\nI would be curious if there was a ballpark timeline available,\nparticularly for the 2.0 beta API, for planning purposes.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/890#issuecomment-142440582.\n. @rjnagal: What is pending to migrate over to v2 completely?\n. Until UI transition happens, if we can make v2 REST API independent of v1\nobjects, that by itself will be useful. WDYT?\n\nOn Thu, Sep 24, 2015 at 2:57 PM, Rohit Jnagal notifications@github.com\nwrote:\n\nMoving UI to use v2 instead of v1.\nOn Thu, Sep 24, 2015 at 2:56 PM, Vish Kannan notifications@github.com\nwrote:\n\n@rjnagal https://github.com/rjnagal: What is pending to migrate over\nto\nv2 completely?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/891#issuecomment-143060912.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/891#issuecomment-143061202.\n. @jimmidyson: My previous comment was for @rjnagal. This PR LGTM\n. ok to test\n. LGTM. Instead of a tag, can we create a release branch on HEAD to manage patch releases?\n. @cadvisorJenkinsBot: retest this please\n. LGTM\n. +1 to Rohit's comments. Until we have a kubelet API that can expose all the\ndata that cadvisor provides, we might have to expose an endpoint like\n/cadvisor.\n\nOn Fri, Sep 25, 2015 at 10:25 AM, Jimmi Dyson notifications@github.com\nwrote:\n\nI've submitted a PR for that - kubernetes/kubernetes#14556\nhttps://github.com/kubernetes/kubernetes/pull/14556\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/894#issuecomment-143294505.\n. I think this issue can be closed now. Feel free to re-open if you think otherwise @Oscarzhao.\n. add to whitelist\n. LGTM\n. @jimmidyson: Doesn't --cgroup-parent affect non-systemd systems as well? I don't see this PR addressing that.\n. @cadvisorJenkinsBot: add to whitelist\n. LGTM. ok to test\n. Filesystem stats are useful mainly to figure out which container is hogging up disk space on a given node. \nAs you said @jimmidyson, there is no easy way to make this work across all storage drivers in docker. \nI'm working on a quota based approach, but even that won't work on all deployments since it requires setting up quota. \nI personally think fs usage feature will be useful. \nIf we can, we should probably add support for other storage backends like lvm and overlay for now.\nFor now, reducing the frequency will help. Another option is to place ulimits while exec'ing du.\n. @jimmidyson: We do not use du for out of disk conditions. We use statfs. I intend to add support for devicemapper and overlayfs soon, which should address the \"aufs only\" concern.\nAs @dchen1107 mentioned, identifying and getting rid of a disk hogging container is very useful in reality. \n. I can rebuild and push again today.\n. @jimmidyson: I have updated the latest tag. Can you try once?\n. v1.1 which is still not released.\n. LGTM\n. LGTM\n. I'm having lots of difficulty in debugging integration test failures. To move forward with this PR, I added an unit test as @jimmidyson suggested. I will add the integration tests in a subsequent PR.\n. Looks like the refactoring is breaking cadvisor. While I debug this, I opened #909 to handle the primary issue this PR is intended to solve.\n. I'm closing this for now. \n. cc @yujuhong\n. Jenkins is confused. This PR passed successfully: http://104.154.52.74/job/cadvisor-e2e/1849/\n. @cadvisorJenkinsBot: retest this please\n. @cadvisorJenkinsBot: test this please\n. @yujuhong fixed the nit. \n. Travis is super slow. Unit tests are run on jenkins as well. Ignoring travis for this PR and merging.\n. cc @yujuhong\n. Individual container housekeeping is synchronous. There will not be multiple stat aggregations in flight for a given container.\nThe memory cache is being used for synchronization as of now.\n. @jimmidyson @yujuhong: Thanks for the review. I updated the fs handler to run in a separate go thread. This should prevent default housekeeping from being blocked by du.\n. @cadvisorJenkinsBot: test this please\n. Let's ignore travis since jenkins has already run the unit tests.\n. @yujuhong: comments addressed. PTAL\n. @yujuhong: Got it. Updated.\n. Thanks for the review! Merging this PR.\n. Memory usage is total usage including cold pages which the kernel can reclaim safely under pressure. \nWorking Set is the bytes of memory that the kernel deems to be necessary for continuing to run the processes in a container. Kernel cannot tolerate overcommitment of the sum total of working set.\n. That mostly correct:\nIIUC, cAdvisor is not supporting\nmemory+swap accounting as of now. You should file an issue separately.\n\nAs of now,\nmemory_usage = RAM usage include pages that have not been accessed in a\nlong time.\nworking set = memory usage - inactive memory.\nOn Mon, Oct 26, 2015 at 3:38 AM, mboussaa notifications@github.com wrote:\n\nIf I well understood:\nmemory_usage= Virtual memory= RAM+SWAP\nworking_set= RAM\nThat's right?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/913#issuecomment-151096072.\n. @solomatnikov: I believe there is an implicit assumption that swap is disabled in the current logic. Under memory pressure, the kernel will use hot memory to identify a victim. \n. @jimmidyson: This change LGTM as long as it doesn't change existing functionality.\n. Cadvisor uses docker info in way too many places. So I don't think depending on that is an issue. \nWhile we are at it we should solve this for overlay as well maybe. \n. @mvdan: I meant retrieving the storage driver info using docker info and performing capacity and usage calculations in cAdvisor. Sorry if that was not clear.\n@ncdc: +1 for moving towards a pluggable model for managing docker images partition.\nI prefer not depending on docker's API for getting usage and capacity for a few reasons.\n1. Backwards compatibility. cAdvisor currently works with older versions of docker as well.\n2. I assume we can re-use the dmsetup utility based approach for calculating container's fs usage as well. This will be useful for monitoring and disk scheduling in Kubernetes, until we have support for disk quota. \n. > I'd also propose removing the docker-images label from the fs code in cadvisor & moving the get docker size to the docker handler package. This way we can hopefully use docker packages directly to get storage backend info rather than trying to guess the correct fs as it is doing now. This is really fragile & hard to extend. This would be a breaking change but one I think is necessary. \n\nA huge +1. Why would this be a breaking change? Kubelet uses the Manager interface abstractions and I assume these changes are internal to cAdvisor.\n. If we use docker info for this purpose, how will we handle container filesystem usage measurements?\n. ok to test\nOn Thu, Oct 15, 2015 at 6:46 PM, Tim St. Clair notifications@github.com\nwrote:\n\ncc/ @vishh https://github.com/vishh @dchen1107\nhttps://github.com/dchen1107\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/921#issuecomment-148572303.\n. @ramitsurana: Thanks for the PR! I don't see an issue with the existing docs. Closing this PR. Feel free to open if you think otherwise.\n. Why this change?\n. LGTM\n. @yujuhong: Thanks for reporting the issue. Are you working on a fix, or shall I pick this up?\n. Go for it! Thanks!\n\nOn Mon, Oct 19, 2015 at 10:20 AM, Jimmi Dyson notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh Should be a simple fix - don't mind\npicking it up if you don't have time.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/925#issuecomment-149286960.\n. Docker interactions in cAdvisor needs to be refactored. It is currently\nmessy. I'm in favor of option 2 since it helps with the refactoring in\ngeneral.\ncAdvisor performs read-only operations. From that perspective, will\nthread-safety be an issue?\n\nOn Mon, Oct 19, 2015 at 3:02 PM, Yu-Ju Hong notifications@github.com\nwrote:\n\nHmm...I just checked the cadvisor code and it seems every container\nhandler uses its own docker client as well. Not only does that waste the\nresource, cadvisor never gets to close the connection even after the\ncontainer terminates.\nI think our options are:\n1. Submit a patch to go-dockerclient to allow closing the connection.\n2. Refactor cadvisor to reuse the same docker client for all \"docker\"\n   containers (similar to kubelet's model).\nOption 2 seems reasonable, but it seems go-dockerclient is not\nparticularly thread-safe (e.g., struct field unixHTTPClient\nhttps://github.com/fsouza/go-dockerclient/blob/master/client.go#L677 is\nnot guarded by any sync primitive).\nThoughts? @jimmidyson https://github.com/jimmidyson @vishh\nhttps://github.com/vishh\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/925#issuecomment-149360770.\n. I think CoreOS uses btrfs. I want to switch to a newer version of CoreOS with overlay support. \nI will add a CentOS node today. Do we need to test against fedora as well?\n. @jimmidyson: I have added a CentOS and RHEL v7.1 instances added. Are you targeting this PR for kubernetes v1.1?\n. LGTM. Feel free to self-merge once you test the docker image changes.\n. I have tried it a couple of times now and it seems to fail consistently.\n\nBTW almost all the tests are failing on RHEL v7.1.\nhttp://104.197.39.211/job/cadvisor-e2e/2016/console\nOn Mon, Oct 19, 2015 at 3:46 PM, Jimmi Dyson notifications@github.com\nwrote:\n\nAre you sure this isn't a random flake? 2013 failed in the same way on\ncontainervm I see\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/927#issuecomment-149369127.\n. @jimmidyson: I'm disabling testing on CentOS and RHEL for now. When you get a chance, try running cadvisor against those distros. I can re-add those hosts at any time, now that they are functional.\n. docker 1.8.2. I can try running again on the CentOS host.\n\nOn Tue, Oct 20, 2015 at 4:17 AM, Jimmi Dyson notifications@github.com\nwrote:\n\nI've checked against docker 1.7.1 from CentOS repos & docker 1.8.2 from\ndocker repos - both work fine for me.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/927#issuecomment-149531008.\n. Try opening http://localhost:8080 in your browser.\n\nOn Mon, Oct 19, 2015 at 4:03 PM, prasadnh notifications@github.com wrote:\n\nHi,\nI am new to this...trying to set cadvisor to see all our docker containers\ninfo. i see advisor docker process when I do docker ps. But don't see any\nlogs or info.\nWhen I run curl localhost:8080 I get below message...no clue:(\ncurl localhost:8080\nTemporary Redirect http:///containers/.\nCan somebody help me to fix this?\nI use ubuntu OS.\nThanks,\nPrasad\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/928.\n. Can you run cAdvisor with --logtostderr flag ?\n\nOn Mon, Oct 19, 2015 at 4:40 PM, prasadnh notifications@github.com wrote:\n\nthanks for quick response... tried browser too... shows blank page.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/928#issuecomment-149377736.\n. I suspect you haven't exposed port 8080 from the cadvisor container on to\nyour host.\n\nOn Mon, Oct 19, 2015 at 5:33 PM, prasadnh notifications@github.com wrote:\n\ni see this\nubuntu@ip-10-201-5-134:~$ docker logs -f cadvisor\nI1020 00:25:55.967297 00001 storagedriver.go:111] No backend storage\nselected\nI1020 00:25:55.967348 00001 storagedriver.go:113] Caching stats in memory\nfor 2m0s\nI1020 00:25:55.967421 00001 manager.go:127] cAdvisor running in container:\n\"/docker/44a8e7c9ac4941579f2afbcdf391cb79c5de81fb9e0317aa10361570fa1c96f9\"\nI1020 00:25:55.971370 00001 fs.go:93] Filesystem partitions:\nmap[/dev/disk/by-uuid/e1d70192-1bb0-461d-b89f-b054e45bfa00:{mountpoint:/rootfs\nmajor:202 minor:1} /dev/xvdb:{mountpoint:/rootfs/mnt major:202 minor:16}]\nI1020 00:25:56.724075 00001 machine.go:49] Couldn't collect info from any\nof the files in \"/etc/machine-id,/var/lib/dbus/machine-id\"\nI1020 00:25:56.724177 00001 manager.go:158] Machine: {NumCores:4\nCpuFrequency:2500042 MemoryCapacity:15770574848 MachineID:\nSystemUUID:EC2C6E40-B180-2157-4650-36A78FC5F6DB\nBootID:08726c62-c50e-48ca-bd6c-851e9842080e\nFilesystems:[{Device:/dev/disk/by-uuid/e1d70192-1bb0-461d-b89f-b054e45bfa00\nCapacity:42127835136} {Device:/dev/xvdb Capacity:39490912256}]\nDiskMap:map[202:0:{Name:xvda Major:202 Minor:0 Size:42949672960\nScheduler:deadline} 202:16:{Name:xvdb Major:202 Minor:16 Size:40256929792\nScheduler:deadline}] NetworkDevices:[{Name:eth0\nMacAddress:02:93:d9:92:29:41 Speed:0 Mtu:9001}] Topology:[{Id:0\nMemory:15770574848 Cores:[{Id:0 Threads:[0 2] Caches:[{Size:32768 Type:Data\nLevel:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified\nLevel:2}]} {Id:1 Threads:[1 3] Caches:[{Size:32768 Type:Data Level:1}\n{Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}]\nCaches:[{Size:26214400 Type:Unified Level:3}]}] CloudProvider:Unknown\nInstanceType:Unknown}\nI1020 00:25:56.725213 00001 manager.go:165] Version:\n{KernelVersion:3.13.0-62-generic ContainerOsVersion:Buildroot 2014.02\nDockerVersion:1.8.1 CadvisorVersion:0.18.0}\nI1020 00:25:56.852462 00001 factory.go:234] Registering Docker factory\nI1020 00:25:56.856547 00001 factory.go:89] Registering Raw factory\nI1020 00:25:57.144390 00001 manager.go:998] Started watching for new ooms\nin manager\nW1020 00:25:57.144673 00001 manager.go:234] Could not configure a source\nfor OOM detection, disabling OOM events: exec: \"journalctl\": executable\nfile not found in $PATH\nI1020 00:25:57.159663 00001 manager.go:247] Starting recovery of all\ncontainers\nI1020 00:25:57.207300 00001 manager.go:252] Recovery completed\nI1020 00:25:57.233508 00001 cadvisor.go:94] Starting cAdvisor version:\n\"0.18.0\" on port 8080\nOn Mon, Oct 19, 2015 at 4:57 PM, Vish Kannan notifications@github.com\nwrote:\n\nCan you run cAdvisor with --logtostderr flag ?\nOn Mon, Oct 19, 2015 at 4:40 PM, prasadnh notifications@github.com\nwrote:\n\nthanks for quick response... tried browser too... shows blank page.\n\u2014\nReply to this email directly or view it on GitHub\n<https://github.com/google/cadvisor/issues/928#issuecomment-149377736\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/928#issuecomment-149379885.\n\n\nThanks,\nPrasad\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/928#issuecomment-149385354.\n. Other than the outstanding comment, this PR LGTM.\n\nThanks a lot @jimmidyson for this PR! :)\n. My bad. I forgot about a change that I have been meaning to do. \nAs of now cAdvisor requires docker daemon to be up before it starts up. I want to change this behavior soon, because it makes kubelet internal architecture complex. \nI'm OK with this PR for now, and altering the current behavior should be easy in the future thanks to this PR! \n. Ignore my previous comment. Since client creation does not require the daemon to be up, as long as client creation doesn't fail, we should be fine. Sorry about the noise!\n. LGTM.\n. AFAIK CoreOS does.\nOn Tue, Oct 20, 2015 at 12:12 PM, Jimmi Dyson notifications@github.com\nwrote:\n\nWhat distros use overlayfs as default?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/932#issuecomment-149672247.\n. Kubelet metrics APIs are being developed. Once that's done, network metrics will be exposed as part of POD metrics.\nAs of now, the data that cadvisor exposes per container is assumed to be local to a container. \nYou should see an infra container for every pod in k8s that cadvisor does expose. That container should have network metrics.\n. I pushed a docker image based on 0.19.3 tag. Can you try that out Auston?\n\nOn Wed, Nov 18, 2015 at 11:20 AM, Auston notifications@github.com wrote:\n\n938 https://github.com/google/cadvisor/issues/938 was closed, but I\ndon't see any 0.19.x releases on docker hub:\nhttps://hub.docker.com/r/google/cadvisor/tags/ . is there a different bug\nholding up the push?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/935#issuecomment-157828308.\n. ok to test.\n. LGTM\n. LGTM\n. We usually publish a few checksums for the binary. Some users have explicitly asked for it. Can you update the 0.19.1 release with the checksums?\n. @jimmidyson: This issue is critical and thanks a lot for reporting this! \n@f0 appreciate the quick response. \nI think we need an alternate API for compute intensive metrics. We have one such for process information today. TCP stats probably fits in there.\nIs anyone working on a PR to disable this metric?\n. WOw. That was quick. LGTM\n. Just to clarify, will the current behavior work when a real device is used\ninstead of loopback?\n\nOn Thu, Oct 29, 2015 at 4:27 PM, Jimmi Dyson notifications@github.com\nwrote:\n\nI assume you're talking about devicemapper loopback? Vfs stat doesn't work\nfor devicemapper thinpool lvm (the preferred production use for performance\nreasons). So I would not revert the change but rather extend the check for\nloopback devicemapper to use the underlying partition info.\nAny thoughts @ncdc https://github.com/ncdc?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/944#issuecomment-152355957.\n. Ok :)\nWhen using real volumes instead of loopback, will the size of the pool\ndevice correspond to the size of the data volume or the sum of data and\nmetadata volumes?\n\nDisclaimer: My understanding of dm driver is very limited.\nOn Thu, Oct 29, 2015 at 4:35 PM, Jimmi Dyson notifications@github.com\nwrote:\n\nIt certainly should, yes. At least on my machine :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/944#issuecomment-152357184.\n. Ideally yes. We should support loopback dm devices as well.\n\nOn Thu, Jan 21, 2016 at 11:19 AM, Andy Goldstein notifications@github.com\nwrote:\n\n@jimmidyson https://github.com/jimmidyson do we need to do anything re\n\"So I would not revert the change but rather extend the check for loopback\ndevicemapper to use the underlying partition info.\" ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/944#issuecomment-173679786.\n. Sorry for the delay. Been busy with the release. @ncdc: Have you fixed devicemapper support for both loopback and real device?\n. Awesome. Thanks @ncdc!\n\nOn Tue, Jan 26, 2016 at 10:23 AM, Andy Goldstein notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh yes\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/944#issuecomment-175158621.\n. +1. In addition to these the metrics you suggested, we should track cpu and\nmemory usage. These metrics are exposed by cadvisor already.\n\nThe existing test framework might not be ideal for this purpose. We might\nhave to soak the binary for a few minutes to identify leaks and other\nissues.\nInstead of running against every PR, we can run against HEAD periodically.\nOn Thu, Oct 29, 2015 at 4:54 PM, Jimmi Dyson notifications@github.com\nwrote:\n\n[image: :+1:] This is likely to highlight not just regressions but a\nnumber of performance improvements too.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/945#issuecomment-152359890.\n. #960 is out for review. Once that is merged, I can cherry-pick that into\nK8s v1.1 branch.\n\nOn Thu, Nov 12, 2015 at 1:24 PM, andyzheng0831 notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh May I ask about the status of bug? As\nwe already cut k8s 1.1.1 release, would it possible for us to speed up\nfixing this issue, so that this will unblock our effort ASAP.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/947#issuecomment-156239627.\n. This patch caused the issue. IIUC, thin-provisioning-tools is a package name specific to alpine linux. I have posted a fix.\n. cc @jimmidyson \n. cc @pwittrock \n. Hey @pwittrock, I opened #949 to resolve the same issue. We can either use that PR or merge the changes in that PR into this. \n. LGTM. @graphaelli you will have to sign the CLA to have this PR merged though.\nThanks for fixing this!\n. @jimmidyson: PTAL.\n. @jimmidyson: Makes sense. Updated. \n. I appreciate the thorough review @jimmidyson!\n\nOn Wed, Nov 11, 2015 at 1:17 AM, Jimmi Dyson notifications@github.com\nwrote:\n\nMerged #957 https://github.com/google/cadvisor/pull/957.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/957#event-461084268.\n. If the prometheus exporter where to be a standalone buddy container, I\nthink these use cases will be easier to handle. The buddy can query\ncadvisor periodically and expose prometheus metrics in any manner, without\nrequiring changes to cadvisor.\n\nOn Tue, Nov 10, 2015 at 4:44 PM, Jonathan Chauncey <notifications@github.com\n\nwrote:\nFor example the image isn't particularly useful but the version is. So it\nwould be great if that was separated into its own dimension. The other one\nis the container name. Certain platforms create some what unique container\nnames (mycontainer_v2.web.1 mycontainer_v2.web.2) for instance. It would\nbe great if we could have a way to supply a regular expression to parse out\nthe information we didnt want. In this case I would like my metrics to have\nthe container name mycontainer.web (with version being an added\ndimension).\nI specifically need this for the Prometheus exporter but think it could be\nvaluable for all metrics being sent.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/958.\n. I couldn't get that to work for the container. Can you post an example?\n\nOn Wed, Nov 11, 2015 at 11:31 AM, Jimmi Dyson notifications@github.com\nwrote:\n\nAnother option is to do this with dmsetup as we did with the docker pool\nstorage.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/959#issuecomment-155887207.\n. @jimmidyson: I don't think so. It will require a fork+exec though. \n. @jimmidyson: Can you verify if the dmsetup based approach? If that works, that will be much much better.\n. @jimmidyson: will you be able to work on devicemapper support?\n. cAdvisor surfaces the writable layer usage for docker containers. I hope that the dmsetup based logic mentioned here includes only the writable layer. Including image storage isn't helpful because, images are being shared.\n. Hmm.. From an end user perspective, knowing how local storage is being\nconsumed will definitely help. The fact that image layers are accounted as\npart of every container that uses those layers will be confusing for end\nusers.\n\nOn Mon, Nov 23, 2015 at 5:27 AM, Vivek Goyal notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh above dmsetup based magic will include\nsize of both unerlying image as well as writable layer. It is not writable\nlayer only.\nDm developers had mentioned to me that if I really wanted writable layer,\nI can do following.\n- Take metadata snapshot from thin pool.\n- Do thin_delta\n- Parse output and calculate usage.\nI have not tried it but something along above lines should give us\nwritable layer usage. Its not trivial though.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/959#issuecomment-158933147.\n. Awesome!\n\nOn Tue, Dec 1, 2015 at 3:05 AM, Jimmi Dyson notifications@github.com\nwrote:\n\n@jthornber https://github.com/jthornber Sounds great!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/959#issuecomment-160936282.\n. @jthornber: Has your tool been released?\n. @jimmidyson @rhvgoyal: will one of you be able to integrate @jthornber's thin_ls tool into cadvisor?\n. FYI: If we can get this in anytime soon, we can provide disk usage tracking for device mapper in Kube v1.2.\n. @jthornber: When will the alpine package for thin provisioning tools be updated to include your new tool?\n. @cadvisorJenkinsBot: retest this please\n. @jimmidyson: PTAL.\n. @jimmidyson: An interface sounds good. I'd prefer performing the refactoring in a separate PR though. \n. Rebased to HEAD.\n\nSince devicemapper support is pending, integration tests will need to\nunderstand the underlying docker config.\nAre you planning on adding support for devicemapper anytime soon?\nOn Mon, Nov 30, 2015 at 12:20 PM, Jimmi Dyson notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh OK so can you rebase & let's try to\nsort out integration tests ready to merge this.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/962#issuecomment-160749430.\n. Rebased.\n. Thanks for the review. Self merging now..\n. add to whitelist\n. Ping @f0!\n. +1. RSS is indeed useful.\n\nOn Fri, Nov 27, 2015 at 10:30 AM, f0 notifications@github.com wrote:\n\n@jimmidyson https://github.com/jimmidyson RRS does not include the FS\nCache, so its the real stack size for applications\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/975#issuecomment-160186317.\n. We already expose a lot of metrics. As long as the cost for acquiring these\nmetrics isn't high, I think adding them is fine by me.\n\nOn Fri, Dec 18, 2015 at 3:02 AM, Daniel, Dao Quang Minh \nnotifications@github.com wrote:\n\nNot only rss but i think that all statistics from memory.stat should also\nbe exposed. They are very helpful in many cases.\n@jimmidyson https://github.com/jimmidyson @vishh\nhttps://github.com/vishh wdyt ? Do we worry about exposing too many\nmetrics by default without knobs for turning them off ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/975#issuecomment-165749009.\n. cAdvisor doesn't reconcile the state of a container after it discovers it\nin the beginning. We will have to periodically inspect the container and\nupdate the internal state. Does anyone have some spare cycles to fix this?\n\nOn Thu, Apr 7, 2016 at 5:11 AM, psychoche notifications@github.com wrote:\n\n+\nI have the same problem.\nAfter rename cAdvisor just lost container and only option is restart\ncAdvisor.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/977#issuecomment-206842730\n. IIUC, docker hub builds require a self-sufficient Dockerfile. We can automate canary builds with dockerhub, but I don't see how we can setup release builds since releases require a go build prior to docker build. \n\nThe jenkins project has an build project that is supposed to push canary images on every commit to master. We can extend that to push a new release image whenever a new tag is added.\n. LGTM. Thanks for the cleanup!\n. LGTM.\n. @jimmidyson: I was on vacation for the last two weeks. I will take a look at the e2e infra today. I will also add you to the admin list on the jenkins server.\nAs for dockerhub, I'm not sure if I can grant access to non-googlers since it is a Google owned organization.\nMoving to a new org is a possibility, but I suspect users will still try to access google/cadvisor. \n. I have marked you as an Admin on jenkins. Try logging in again.\n. cAdvisor supports start and end times. Look here. You will have to POST a json query object though. \n. Thanks for the fix @carmark !\n. cc @rjnagal \n. Can Dockerhub support docker builds using Makefiles?\nTo build with just docker build ...., I need access to source code and go\ntools from within the image right?\nOn Fri, Dec 4, 2015 at 3:51 AM, Jimmi Dyson notifications@github.com\nwrote:\n\nWith the new Dockerfile in the root of the repo you can build with docker\nbuild ... & nothing more.\nI'd like to move to this approach & use Docker hub builds with builds from\nregex tags so we can ensure a Docker image is built for every tag\nautomatically when we create a release tag.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/992#issuecomment-161949481.\n. +1 from my side for requiring 1.5\n\nOn Fri, Dec 18, 2015 at 2:06 PM, Jimmi Dyson notifications@github.com\nwrote:\n\nThose warnings are due to the fact that cadvisor has to support with go\n1.4 & 1.5. The only way to get rid of these warnings is to change the\nlinker flags in build scripts in a way that would be incompatible with 1.4.\nI don't mind making go 1.5 a requirement personally.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/992#issuecomment-165907940.\n. That test needs some more love and care :(\n\nOn Wed, Dec 2, 2015 at 12:54 AM, Jimmi Dyson notifications@github.com\nwrote:\n\nClosed #993 https://github.com/google/cadvisor/pull/993.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/993#event-479923341.\n. LGTM. \n. ok to test\n. Thanks for the fix @jonboulle!\n. @timstclair: Can you handle this issue?\n. Jenkins failed with a gcloud error. Tests passed. Self merging this PR for urgency.\n. I'm starting to work on an e2e test that will attempt to catch goroutine leaks. I tested this PR manually.\n. cc @dchen1107 @yujuhong \n. In a given time range, if NumStats is specified, which samples should be returned? Should it be the oldest or the most recent? \n. That change SGTM. We can perhaps send out an announcement on\ngoogle-containers@ regarding this API change.\n\nOn Tue, Dec 8, 2015 at 11:55 AM, Piotr Szczesniak notifications@github.com\nwrote:\n\nThe most recent one.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1010#issuecomment-162996419.\n. ok to test\n. add to whitelist\n\nOn Wed, Dec 9, 2015 at 12:19 AM, Piotr Szczesniak notifications@github.com\nwrote:\n\nReopened #1010 https://github.com/google/cadvisor/pull/1010.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1010#event-486596772.\n. Piotr, can you send out an announcement about this API change to\ngoogle-containers@ ?\n\nOn Wed, Dec 9, 2015 at 11:56 AM, Piotr Szczesniak notifications@github.com\nwrote:\n\nYes, please. Thank you very much in advance.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1010#issuecomment-163372571.\n. -1 for changing the default.\n\nOn Thu, Dec 10, 2015 at 11:06 AM, Tim St. Clair notifications@github.com\nwrote:\n\nWhy should it be changed? Changing the default is a breaking change, and\nif anyone needs another value they can just provide it (especially with\n1010 https://github.com/google/cadvisor/pull/1010 )\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1011#issuecomment-163719409.\n. ok to test\n. @Snorch: Wonder why those cgroups are co-mounted. Is there a specific reason?\n. @ncdc we can use some help here. Current configuration model for custom metrics is meant for Stavanger cadvisor and doesn't make sense in the kubelet case. \n. To be clear, for the kubelet use case, what we need is an internal API that\ncan communicate to the cadvisor core what custom metrics are supposed to be\ncollected.\nThe labels based API is cute, but using labels for storing what is\nAnnotations makes me a bit unhappy.\nIdeally the config should be part of the Docker images.\n\nOn Fri, Dec 11, 2015 at 10:38 AM, Solly Ross notifications@github.com\nwrote:\n\nIt should be fairly quick to implement. I'll see what I can whip up.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1016#issuecomment-164013236.\n. I guess it depends on the use case. Image authors will be aware of what\nmetrics can possibly be relevant for their application and expose them as\npart of the image.\nThere are multiple use cases for those metrics though.\nFor monitoring, we might want all the metrics. But for an orchestration use\ncase, only one or a few metrics will be useful.\nI feel the current interface isn't designed for combining those two use\ncases.\n\nOn Fri, Dec 11, 2015 at 3:35 PM, Solly Ross notifications@github.com\nwrote:\n\nTo be clear, for the kubelet use case, what we need is an internal API\nthat can communicate to the cadvisor core what custom metrics are supposed\nto be collected.\nSure. The labels-based idea is a quick nice-to-have.\nIdeally the config should be part of the Docker images.\nWhy? It seems like it might not be uncommon to have a metrics endpoint\nthat exposes a lot of metrics, but you might only want to use a few. It\nseems silly to have to entirely rebuild your Docker image to change which\nmetrics are exposed, when you're not really making a change that affects\nthe rest of the image itself. It also seems odd to leak cAdvisor details\ninto the filesystem of the container itself.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1016#issuecomment-164078891.\n. To summarize what Rohit mentioned,\nIdeally, an image exposes whatever metrics it thinks will be useful.\nThen components like kubelet should filter out any metrics that is not\ninteresting for the orchestration system.\n\nOn Fri, Dec 11, 2015 at 4:14 PM, Rohit Jnagal notifications@github.com\nwrote:\n\nFrom the original request, I don't understand why kubelet needs to specify\nwhat metrics need to be collected. The user, even if its kube components,\nshould be deciding what metrics we need to collect. Shouldn't we try to\nkeep this a passthrough operation for kubelet?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1016#issuecomment-164086178.\n. We already have two mechanisms.\n1. Embedding the config in the docker container.\n2. Introducing the config during runtime using volumes.\n\nWhy do we need a third mode of configuration?\nOn Mon, Dec 14, 2015 at 8:24 AM, Solly Ross notifications@github.com\nwrote:\n\nThe user, even if its kube components, should be deciding what metrics we\nneed to collect. Shouldn't we try to keep this a passthrough operation for\nkubelet?\nSo, in this case, kubelet would have to receive all the metrics from\ncAdvisor, and then filter before it presents those metrics out as JSON?\nWhat if an application was written to export Prometheus metrics, but\nwasn't written for cAdvisor? It seems, to me, silly to say \"if you want to\nuse cAdvisor to collect your custom metrics, you must build your image\nwith cAdvisor specific configuration data built in to the filesystem\" (you\ncould mount it in, but we discussed why that can be a pain in the kube\ncustom metrics thread).\nI get the desire to provide \"sharable\" configurations, but I also think\nthe user should have some flexibility -- if you decide to build the config\nin, you can use that. If you want to control exposed metrics at runtime, or\nyou want to use an \"off-the-shelf\" image that exposes metrics but doesn't\nhave built-in cadvisor configuration, then you can use a label. I don't see\nthe harm in allowing this, and it seems like a nice feature to have.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1016#issuecomment-164483708.\n. The image author is the one who knows about the metrics available in their\nimage. As a user I'd expect cAdvisor based monitoring to just work, instead\nof probing the image, figuring out how metrics are exposed and then setting\nup a runtime config.\nI still don't see how that desire is invalid.\n\nOn Tue, Dec 15, 2015 at 8:10 AM, Solly Ross notifications@github.com\nwrote:\n\nIt was argued in the kube discussion that option 2 is clunky (which is\nwhat spawned this issue), and I'm inclined to agree.\nFurthermore, I'd argue that configuration for how an external service\nshould treat the container should not be embedded in the container's\nfilesystem -- the filesystem should be used for information that the\ncontainer processes themselves need to access (whether that's configuration\nfor the container processes, or information about where they're running),\nwhile labels/annotations/external APIs/etc should be used to hold metadata\nthat needs to be accessed by services outside the container.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1016#issuecomment-164810869.\n. Ok. Let's try to collect the requirements.\n1. Image authors should be able to expose what ever metrics they deem to be necessary.\n2. Image users should be able to add metrics configuration if one doesn't exist already.\n3. Image users need ability to query only a subset of all the available metrics if required.\n\n1 is supported.\n2 can be achieved today using volumes. If volumes doesn't seem appealing, how about creating a global config directory on the host? We can have cAdvisor pick up configs either based on image name, or specific labels.\n3 is already supported. cAdvisor exposes an endpoint that lists all the available metrics.\nDid I miss any other requirement?\nUsing labels for passing configuration data doesn't sound right. For example, in Kube that is the reason why Annotations were introduced. \n. Through this process of supporting custom metrics in kubelet, we realized\nthat cadvisor should probably be separated from kubelet itself.\nLet's hold off on adding any APIs until the overall system design is clear.\nSorry for the confusion. This feature needs more thought.\nOn Wed, Dec 16, 2015 at 7:33 AM, Solly Ross notifications@github.com\nwrote:\n\nAt this point, I'm also fine dropping the \"label\" angle, and just working\non a more direct internal API, but I'm unsure based on your comments if you\nwant that, either.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1016#issuecomment-165144403.\n. https://github.com/kubernetes/kubernetes/issues/18770\n\nOn Wed, Dec 16, 2015 at 10:36 AM, Jimmi Dyson notifications@github.com\nwrote:\n\nCan you link to the discussion on separating out cadvisor from kubelet\nplease?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1016#issuecomment-165204136.\n. @pwittrock: I wonder if it is possible to delete previous posts of k8s-bot whenever it posts a new update? In its current state its too spammy :(\n. Just a nit. Otherwise LGTM. Feel free to self-merge @pwittrock. Thanks for the fix!! Much appreciated.\n. Which configuration will take precedence? File based or label based?\n. Embedding custom metrics configuration as part of the image makes it\npossible to share the config.\nIf the primary use case is to query specific custom metrics from cAdvisor,\nthat use case is already supported.\n\nOn Fri, Dec 11, 2015 at 5:38 PM, Kubernetes Bot notifications@github.com\nwrote:\n\nEXPERIMENTAL - CAN IGNORE\nBuild/test failed for commit 0f770ff\nhttps://github.com/google/cadvisor/commit/0f770ffadd2532dcf81307dc740e728cb0a06070\n.\n- Build Log\n  https://storage.cloud.google.com/kubernetes-jenkins/pr-logs/0f770ffadd2532dcf81307dc740e728cb0a06070/heapster-pull-build-test-e2e/1/build-log.txt\n- Test Artifacts\n  https://console.developers.google.com/storage/browser/kubernetes-jenkins/pr-logs/0f770ffadd2532dcf81307dc740e728cb0a06070/heapster-pull-build-test-e2e/1/_artifacts/\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1018#issuecomment-164094542.\n. What about handling capacity and usage for the base filesystem under zfs? Is it already working ?\n. We should add an e2e test for this. Can you file an issue?\n\nOn Tue, Dec 15, 2015 at 12:45 PM, Miguel Perez notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh Yeah this set as well.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1020#issuecomment-164891096.\n. ok to test\n\nOn Tue, Dec 15, 2015 at 12:46 PM, Vishnu Kannan vishnuk@google.com wrote:\n\nWe should add an e2e test for this. Can you file an issue?\nOn Tue, Dec 15, 2015 at 12:45 PM, Miguel Perez notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh Yeah this set as well.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1020#issuecomment-164891096.\n. ok to test\n. @jimmidyson: I wonder why the network tests are flaky :(\n\n\nThe e2e failures are unrelated to this PR. So this PR LGTM\n. Oops. This fell out of my radar. Sorry!\n. @cadvisorJenkinsBot: test this please\n. ok to test\n. LGTM\n. add to whitelist\n. @timstclair: Add yourself to the admin list for triggering builds.\n. Feel free to self-merge once build passes.\n. Just a couple of nits. Otherwise LGTM! Thanks @dqminh \n. LGTM\n. can you squash your commits?\n. LGTM\n. @k8s-bot test this please\n. @k8s-bot test this please\n. Merging this since e2e has passed.\n. https://github.com/google/cadvisor/blob/master/storage/common_flags.go#L28\n. @pwittrock: Can we update the cAdvisor canary jenkins job to push images whenever a new tag is created?\n. Thanks for the cleanup.\n. ok to test\n. Travis failure is unrelated to this failure. Once #1031 is merged, this PR should hopefully pass all tests.\n. ok to test\n. LGTM. Thanks for the quick fix @pwittrock!\n. Feel free to self merge once Travis is happy.\nBTW don't we run unit tests as part of jenkins as well?\n. On Tue, Jan 5, 2016 at 2:27 AM, wangzhezhe notifications@github.com wrote:\n\nHi !\nI want to know if it is necessary for cadvisor to add the label-selector\nmechanism ?\nSure. If your nodes have a lot of containers (cgroups) that can be helpful\nor if you want to run multiple cadvisor instances.\nIt seems we do need to monitor all the containers on the machine , because\nthe containers come and go so fast just as this article\nhttps://wwwdatadoghqcom/blog/the-docker-monitoring-problem/ mentioned\nThe churn issue can be solved by pushing metrics from cadvisor instead of\npolling.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1034.\n. This PR has not been tested yet.\n. So I assume the API reworking is OK. I will address the comments, test and then update the PR.\n\nThanks for the review.\n. @timstclair @pwittrock: updated PR and added unit and integration tests. PTAL.\n. @timstclair: Splitting up the PR would be a bit time consuming. I just completed rebasing and it took quite some time. I'd much rather spend my energy improving the existing API machinery.\n. Merged the files @timstclair \n. @timstclair: Tests are passing. Merging.\n. Not as of now. We should add support for storing labels in the InfluxDB sink. cAdvisor already picks up labels from Docker.\n. Sounds like cAdvisor is having trouble collecting stats. 0 doesn't seem like a valid value for memory usage.\n. Check the logs. \n. -v=0 means minimal logging is enabled. You can try setting --v=4 to get\nmore logs.\nOn Fri, Jan 8, 2016 at 1:31 PM, Achille notifications@github.com wrote:\n\nSorry I forgot to mention that, there's about nothing in the logs:\ndocker logs --tail=10 -f container-exporter\nI0108 21:21:58.873729 00001 manager.go:158] Machine: {NumCores:4 CpuFrequency:2400072 MemoryCapacity:16863678464 MachineID: SystemUUID:* BootID:6a8d7a36-e177-44ec-a42a-7562c3677d15 Filesystems:[{Device:/dev/xvda1 Capacity:8316706816} {Device:/dev/xvdb Capacity:270565117952}] DiskMap:map[202:0:{Name:xvda Major:202 Minor:0 Size:8589934592 Scheduler:cfq} 202:16:{Name:xvdb Major:202 Minor:16 Size:274877906944 Scheduler:cfq}] NetworkDevices:[{Name:eth0 MacAddress:* Speed:10000 Mtu:9001}] Topology:[{Id:0 Memory:16863678464 Cores:[{Id:0 Threads:[0 2] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[1 3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:31457280 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown}\nI0108 21:21:58.874528 00001 manager.go:164] Version: {KernelVersion:3.16.0-4-amd64 ContainerOsVersion:Buildroot 2014.02 DockerVersion:1.8.3 CadvisorVersion:0.18.0}\nI0108 21:21:58.968268 00001 factory.go:227] System is using systemd\nI0108 21:21:58.971804 00001 factory.go:235] Registering Docker factory\nI0108 21:21:58.975366 00001 factory.go:93] Registering Raw factory\nI0108 21:21:59.122014 00001 manager.go:1001] Started watching for new ooms in manager\nW0108 21:21:59.122174 00001 manager.go:232] Could not configure a source for OOM detection, disabling OOM events: exec: \"journalctl\": executable file not found in $PATH\nI0108 21:21:59.141765 00001 manager.go:245] Starting recovery of all containers\nI0108 21:21:59.215566 00001 manager.go:250] Recovery completed\nI0108 21:21:59.249463 00001 cadvisor.go:94] Starting cAdvisor version: \"0.18.0\" on port 8080\nNothing more than this is shown, Prometheus is pulling every 15 seconds\nand I forced generating a few HTTP requests as well but the logs stay\nsilent.\nI'm also checking the command line options, the -stderrthreshold and -v\nare at 0 which as I understand means all logs will be printed... I'm not\nsure how to get more data on what's going on.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1039#issuecomment-170132389.\n. > I0111 20:20:05.495421 00001 machine.go:49] Couldn't collect info from any of the files in \"/etc/machine-id,/var/lib/dbus/machine-id\"\n\nThis is not an issue. This file is distro specific. \n\nI0111 20:20:05.749432 00001 factory.go:71] Error trying to work out if we can handle /: error inspecting container: unexpected end of JSON input\n\nThis log line sounds like a bug in the code. It should be harmless in any case.\nThe logs you posted is for the setup phase and does not seem to highlight any errors. Are there any more logs that refer to metrics collection after cAdvisor is up logically?\nIs cAdvisor running in privileged mode?\n. Ah! You need to enable memory cgroup. This is a kernel configuration.\nAdd a kernel boot option \"cgroup_enable=memory\" to your bootloader setting\n(e.g. /etc/default/grub) to enable it.\nOn Mon, Jan 11, 2016 at 3:01 PM, Achille notifications@github.com wrote:\n\nSo as far as I understand the memory stats are pulled from this\nhttps://github.com/opencontainers/runc/blob/master/libcontainer/cgroups/fs/memory.go#L101\nI've scanned the host file system and couldn't find any trace of a\nmemory.stat file, do you know if that could be the cause of the problem?\nIt's weird that no error is reported by cAdvisor tho.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1039#issuecomment-170725848.\n. I'm closing this issue. Feel free to re-open if there are any more concerns.\n. IN general LGTM. We can fix those comments later.\n. LGTM\n. As long as the standard Linux APIs are available, cAdvisor should work on\nRasberry devices as well.\nI assume the docker APIs don't change on those devices. If so, cAdvisor's\ndocker integration should also work.\nI don't have a Rasberry device to validate my claims though.\n\nOn Thu, Jan 7, 2016 at 5:21 AM, mboussaa notifications@github.com wrote:\n\nIs cadvisor capable of gathering performance data even for RPI docker\ncontainers running in Raspberry devices?\nlike in\nhttp://blog.hypriot.com/getting-started-with-docker-on-your-arm-device/\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1042.\n. Are you referring to remote monitoring? If so, that is not possible. cAdvisor should be able to run as a standalone process on most flavors of Linux.\n. Remote monitoring is not possible\n\nOn Fri, Jan 8, 2016 at 11:46 AM, mboussaa notifications@github.com wrote:\n\nYes I mean from my linux machine is it possible to monitor RPI containers ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1042#issuecomment-170104747.\n. @timstclair: My PR is not yet complete. So you win :) Reviewing this now.\n. Overall code LGTM. Just a couple of comments about the API and the structure.\n. LGTM\n. Tests are failing. Not sure why.\n. Just one comment. Otherwise LGTM.\n. cc @jimmidyson \n. Why this PR? Can you explain what you are trying to do? \n. BTW sorry for the delay.\n. Ok. Most of the code in here will be shared with other runtimes right? Are\nyou planning on factoring them in a separate PR?\nWhat about the APIs? Is each manager going to expose their own set of APIs?\nWhich manager will handle raw cgroups?\nWill docker and rkt support co-exist in a single cadvisor instance?\n\nOn Wed, Jan 20, 2016 at 4:34 PM, Shaya Potter notifications@github.com\nwrote:\n\nable to plug in rkt (and other container engines). Want to proceed in\nsteps. so this was refactoring docker out to enable other containers to\nwrite their own managers.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1049#issuecomment-173412797.\n. SGTM. This PR LGTM in that case. I'd like to see factoring out of raw\ncgroups driver before adding new code for other runtimes though.\n\nOn Wed, Jan 20, 2016 at 6:37 PM, feisky notifications@github.com wrote:\n\nI think raw cgroups should be separated from docker/rkt/hyper container\nruntimes, which means\n- raw manager should be coexist with other container runtime manager\n- container manager could be configurable to select runtime\n- api should be changed not specific to docker\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1049#issuecomment-173432967.\n. cc @timstclair \n. @jimmidyson: Can you describe the issue you are trying to fix with this PR?\n. @jimmidyson: That was quick :) LGTM. I don't like the additional Start and Cleanup to be frank. I will try to find alternative methods to avoid them. For now, let's merge this PR.\n. LGTM\n. ok to test\n. @guoshimin: Kindly fix the test issues. \n. Thanks for tackling this @pwittrock \n. LGTM.\n. ok to test\n. Self merging to create a tag asap\n. Thanks @timstclair. Missed this while rebasing. \n. We added a flag recently to disable systemd assumption. IIRC, the flag was\n--nosystemd=true.\n\nOn Mon, Jan 18, 2016 at 8:14 AM, Daniel, Dao Quang Minh \nnotifications@github.com wrote:\n\ncadvisor seems to mishandling the containers cgroup in a systemd host ( at\nleast with 1.10-dev ). Given a host that has the following cgroup layout\n/sys/fs/cgroup\n-> /cpu\n  -> /docker\n    -> /2eedaaf84987b9297c012a2acb2aa114029b924d3216562653affba409f16333\n  -> /system.slice\n-> /systemd\nThis is the layout created when we are on a systemd host, but use the fs\ncgroup and set cgroup-parent to docker, and is the default setting. This\nsetup will make UseSystemd returns true, while containers are actually in\na different cgroup, so we miss the docker container data.\nPerhaps we should just remove UseSystemd as running on a systemd host\ndoesnt mean that it uses the systemd cgroup approach, or maybe docker\nshould tell us which cgroup driver it is using via /info so we can check\nfor it in UseSystemd\nping @vishh https://github.com/vishh\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1062.\n. Does docker expose the cgroupfs driver uses?\n\nOn Wed, Jan 20, 2016 at 9:27 AM, Daniel, Dao Quang Minh \nnotifications@github.com wrote:\n\nMakes sense. Its sad that there isnt a nice way to auto-detect this though\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1062#issuecomment-173290088.\n. ok to test\n. The email you used does not show up in the CLA list :( ?\n. +1.\n\nOn Tue, Jan 19, 2016 at 11:12 AM, krhubert notifications@github.com wrote:\n\nThere are MemoryCapacity and WorkingSet which represents information about\nmemory, but one is defined as int64 and second one is defined as uint64. I\nthink those two type should be the same.\ninfo/v1/machine.go\n// The amount of memory (in bytes) in this machine\n  MemoryCapacity int64 `json:\"memory_capacity\"\ninfo/v1/container.go\n// dirty memory, and kernel memory. Working set is <= \"usage\".\n  // Units: Bytes.\n  WorkingSet uint64 json:\"working_set\"\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1065.\n. Thanks!\n\nOn Tue, Jan 19, 2016 at 1:46 PM, krhubert notifications@github.com wrote:\n\nI can create PR in next 2 days.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1065#issuecomment-172997583.\n. ok to test\n. @krhubert: Tests are failing.\n. /var/run is needed to access the docker socket. cAdvisor needs read-only\naccess to docker API. Maybe mount just that in.\n\nOn Wed, Jan 20, 2016 at 7:04 AM, Julian Ospald notifications@github.com\nwrote:\n\nI tried cadvisor and the README suggests to do:\n--volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\nwhich, from a security standpoint, is a really strong no-go. /var/run is\nmounted writable... there will be more on the host system that is writing\nto that directory including crucial system services. Then the whole root\nfile system is mounted in read-only. That exposes all data of the host.\nIs there no way to run cadvisor in a more restricted way?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1069.\n. cc @jimmidyson \n. I need this merged to be able to update the deps in kubernetes repo.\n. @k8s-bot: retest this please\n. Security update \n. @k8s-bot ok to test\n. @jimmidyson: It removes non-existent dependencies which is currently breaking godep restore.\n\nI will fix revert the update to docker deps\n. Thanks for the pointer. I ended up writing a similar script. Vendoring in cadvisor into kube is failing because of this non-existent dependency. I'm attempting to move to glide from godeps to avoid similar issues in the future.\nI have reverted the update to docker deps\n. Self-merging to make progress on the kube side\n. I have updated cadvisor to not use a non-existent package. I don't see why\ngodep fails when one of the vendored dependencies doesn't exist. What is\nthe point of vendoring?\nIn any case, the pre-load option doesn't seem to work when updating godeps\nin the kube side. That is the reason for fixing this.\nOn Thu, Jan 21, 2016 at 3:28 PM, Jimmi Dyson notifications@github.com\nwrote:\n\nBut there is nothing wrong with these godeps afaik. Why didn't you just\nuse kubes preload deps like I pointed out?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1071#issuecomment-173749018.\n. @k8s-bot: ok to test\n. Tip: Ignore all files under vendor/ directory.\n. There are a couple of issues with glide that should ideally be resolved\nbefore we can switch to it.\n\nOn Mon, May 16, 2016 at 1:40 PM, Kubernetes Bot notifications@github.com\nwrote:\n\nJenkins GCE e2e\nBuild/test failed for commit 4f892c4\nhttps://github.com/google/cadvisor/commit/4f892c4a2605ee6f1f5912f21a54d8602dc78d49\n.\n- Build Log\n  https://storage.cloud.google.com/kubernetes-jenkins/pr-logs/pull/1073/cadvisor-pull-build-test-e2e/570/build-log.txt\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1073#issuecomment-219540958\n. AFAIK, you can set that flag to a shorter duration. Is that not working for\nyou?\n\nOn Thu, Jan 21, 2016 at 9:41 PM, Daria Mehra notifications@github.com\nwrote:\n\nI'm running cadvisor-canary:latest build, sending data to InfluxDB, and\ntrying to set -storage_driver_buffer_duration to a lower value than the\ndefault 1 minute, so I'd be able to chart current data from influx rather\nthan having a delay. I thought that was working when I ran a release build\nin Nov-Dec (with influx v0.8, setting -storage_driver_buffer_duration=2s.\nThe reason I'm now running canary is to have influx v0.9 support, since\nthat's what I need. When I tried setting the duration to 2s, the actual\ndelay I saw was 2 minutes. That seems to match the setting to\n60*time.Second in this code\nhttps://github.com/google/cadvisor/blob/master/storage/common_flags.go\nunless I'm misreading it. Trying a fractional value\n-storage_driver_buffer_duration=0.1 resulted in not seeing data in\ninfluxdb for several minutes (then i stopped waiting).\nI'm now running with -storage_driver_buffer_duration=1m but would like a\nshorter delay.\nThis is the exact command line in my docker-compose file:\nimage: google/cadvisor-canary:latest\n  command: -storage_driver=influxdb -storage_driver_db=cadvisor -storage_driver_host=influxdb:8086 -logtostderr=true -v=9 -stderrthreshold=9 -storage_driver_buffer_duration=1m\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1074.\n. Do the logs give any clues?\n\nOn Fri, Jan 22, 2016, 6:47 PM Daria Mehra notifications@github.com wrote:\n\nCorrect, when I set -storage_driver_buffer_duration=2s the actual delay\nis 2 minutes, not seconds.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1074#issuecomment-174113092.\n. I have fixed this manually for now. \n@pwittrock: can we extend the jenkins pusher to sync the latest tag to the most recent release?\n. Oops. Operator error. Can you check again?\n\nOn Mon, Jan 25, 2016 at 1:36 PM, Sven M\u00fcller notifications@github.com\nwrote:\n\njust pulled the current latest tag. when running the container, the logs\nshows:\nI0125 21:31:10.290451 00001 cadvisor.go:94] Starting cAdvisor version: \"0.18.0\" on port 8080\nis that the latest stable version? when looking at the \"releases\" section,\ni expected version 0.20.1 (https://github.com/google/cadvisor/releases)\nThx,\nSven\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1075#issuecomment-174674408.\n. Err. Looks like the build/release scripts have changed. Apologies. Updated\nagain.\n$ docker run --rm -it google/cadvisor:latest\n...\nI0125 22:10:48.852680       1 manager.go:254] Recovery completed\nI0125 22:10:48.853630       1 cadvisor.go:106] Starting cAdvisor version:\n0.20.1-634965a on port 8080\n...\n\nOn Mon, Jan 25, 2016 at 2:00 PM, Sven M\u00fcller notifications@github.com\nwrote:\n\nHi,\nStill the same version for me (although a new iamge was pulled).\n$ docker images\nREPOSITORY                                 TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\ngoogle/cadvisor                            latest              3f6443d29d3e        11 minutes ago      35 MB\ndocker logs -f cadvisor\n...\nI0125 21:56:03.652602 00001 cadvisor.go:94] Starting cAdvisor version: \"0.18.0\" on port 8080\n...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1075#issuecomment-174686162.\n. I have updated google/cadvisor:0.19.3 and verified that google/cadvisor:v0.20.4 contains the right version.\n. I pushed 0.20.5 as well.\n\nOn Wed, Feb 3, 2016, 1:11 PM Jimmi Dyson notifications@github.com wrote:\n\nHow about 0.20.5?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1075#issuecomment-179470353.\n. LGTM\n. LGTM\n\nOn Wed, Jan 27, 2016 at 2:11 AM Jimmi Dyson notifications@github.com\nwrote:\n\nMerged #1079 https://github.com/google/cadvisor/pull/1079.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1079#event-527846370.\n. cc @jimmidyson\n\nOn Thu, Jan 28, 2016 at 10:35 AM, Mike notifications@github.com wrote:\n\nsee https://githubcom/prometheus/prometheus/issues/1352\ni can not tell if it is really on cadvisors side to change labels for the\nprometheus endpoint or prometheus should allow dots again\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1082.\n. Yup. On it.\n\nOn Fri, Jan 29, 2016 at 1:16 AM, Jimmi Dyson notifications@github.com\nwrote:\n\nI tagged 0.20.5 that includes that fix yesterday, but I have no access to\npush Docker images to Docker hub for this project I'm afraid.\n@vishh https://github.com/vishh Is this something you could take care\nof please?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1082#issuecomment-176655795.\n. @k8s-bot: ok to test\n. Surprised that the tests passed without any changes to the PR. The e2e suite is now pretty flaky.\n. This PR should have failed on CoreOS nodes. Not sure how/why it passed. Reverting this PR.\n. On retrospect, defaulting to flag defined value might be better. Making that change here \n. FYI: Opened https://github.com/docker/docker/pull/19986 to fix this for real.\n. I noticed this and tried to fix in the next PR of mine. I defaulted to\nusing the flat value of root device wasn't found.\n\nOn Fri, Feb 5, 2016, 3:55 AM Jimmi Dyson notifications@github.com wrote:\n\n@vishh https://github.com/vishh This needs reverting too. Checking the\ntest logs it only passed because they timed out... This breaks stuff for\ndevicemapper storage as it doesn't necessarily have a root storage dir.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1083#issuecomment-180319249.\n. This PR might be failing on old versions of docker. I'm planning to update\nour test infra to output docker version of the tests fail.\n\nOn Fri, Jan 29, 2016, 5:34 PM Kubernetes Bot notifications@github.com\nwrote:\n\nJenkins GCE e2e\nBuild/test failed for commit ae9c2e4\nhttps://github.com/google/cadvisor/commit/ae9c2e420b687d152622573eee785ffe70f5e24d\n.\n- Build Log\n  https://storage.cloud.google.com/kubernetes-jenkins/pr-logs/pull/1084/cadvisor-pull-build-test-e2e/142/build-log.txt\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1084#issuecomment-177043000.\n. Tested this PR locally with docker v1.10-rc2 and it works. e2e passed. AFter this PR goes in, we can add a node with docker v1.10 to the e2e suite.\n. @pwittrock: I have made several changes to PR including that of adding a much needed integration test. PTAL.\nI have also added a check to include docker major version as well.\n. @pwittrock: Addressed your comments. PTAL\n. Ohh no. That's bad. Time out should have been an error :(\n\nOn Fri, Feb 5, 2016, 2:54 AM Jimmi Dyson notifications@github.com wrote:\n\nTests seemed to pass even though there were errors in the logs... Test\nkilled with quit: ran too long (10m0s). So no confidence that this\nactually passed integration tests.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1084#issuecomment-180293754.\n. xref #969\n\nOn Mon, Feb 1, 2016 at 7:44 AM Robert \u00c5kerblom-Andersson \nnotifications@github.com wrote:\n\nHi,\nI would like cadvisor to support containers using CFS\nTo reproduce, start a container like this (allowed to use 10% of host CPU):\nsudo docker run -it --cpu-period=10000 --cpu-quota=1000 ubuntu /bin/bash\nThen create a while loop inside (or something else that would use close to\n100% CPU):\nwhile [ 1 ] ; do echo \"Creating a load\" ; done\nIn this scenario I would expect cadvisor to show (in the web UI) how much\nCPU the container used based on it's allowed quota (in the cadvisor view\nfor the container) Right now the percentage showed in cadvisor is\ncalculated with the hosts maximum CPU rather than the containers set\nmaximum CPU, so the above example show ~10% usage for the container (while\nit's in fact using most of it's capacity)\nWhat are your views on this? It feels it should be possible to calculate\nthe percentage based on the period and quota parameters to get an accurate\ndisplay for the container?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1085.\n. @Scorpiion none of the maintainers have cycles to work on this feature atm. We can review and take patches though. . Nope. make build doesn't work either.\n\nOn Mon, Feb 1, 2016 at 12:57 PM Jimmi Dyson notifications@github.com\nwrote:\n\nDoes make build work for you? Confused how this can't work if it works on\nCI...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1086#issuecomment-178185889.\n. Nope. Building on linux only. AFAIK @timstclair has issues building as well.\n. Err. Broken godeps :( Sorry for the confusion. I need to get back to switching over to glide.\n. @pwittrock: Can we merge this soon? I need this PR to figure out the test failures on my other PRs.\n. @pwittrock: I like the changes you are proposing. It requires a bit more refactoring. Let's merge this for now and clean things up in a subsequent PR.\n. Merging since this is just a version bump\n. I think #1095 fixed this issue. nosytemdflag should no longer be a necessity.\n. LGTM.\n. The disk usage test that I added recently is causing timeouts. I have temporarily disabled it. I will re-enable it in a subsequent PR. For now, this PR seems to work on aufs and devicemapper with docker v1.10. I'm planning to add a node for overlay with docker v1.10 and one with zfs. I think this is good to go for now, unless you have any objections @jimmidyson.\n. @jimmidyson: Matrix build is what we need. Any ideas on how to get there?\n. We will have to periodically upgrade docker version and the base distributions. @pwittrock and I were brainstorming today to crowdsource testing if possible, similar to Go's build dashboard. WDYT?\n\nI will address the comments in this PR now. This PR will help with the process until we have a better test infra. Disagree?\n. @timstclair @timstclair: PTAL.\n. I want to merge this PR before adding a docker v1.10 node.\n. LGTM\n. LGTM. Thanks @timstclair !\n. @pwittrock: gcloud is still being flaky :( Time to switch to pure ssh I guess.\n. LGTM\n. I will let @jimmidyson and @pwittrock review this PR before merging.\n. Let's ignore the tests\nOn Wed, Feb 10, 2016 at 1:12 PM, Phillip Wittrock notifications@github.com\nwrote:\n\nMore credentials failures :(\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1102#issuecomment-182583586.\n. LGTM\n. ok to test\n. @mwielgus: Build is failing.\n. @k8s-bot ok to test\n. @k8s-bot ok to test\n. Can you rebase against upstream master? git fetch upstream && git rebase\n-i upstream/master\n\nOn Tue, Feb 9, 2016 at 2:40 PM, Kubernetes Bot notifications@github.com\nwrote:\n\nCan one of the admins verify that this patch is reasonable to test? (reply\n\"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1104#issuecomment-182111794.\n. Closing this in favor of #1120\n. LGTM\n. LGTM\n. Yayy... Thanks @pwittrock for making this change :bow: \n. ok to test\n. Overall lgtm\n. Sorry. This somehow fell off my radar. LGTM again\n. Can we move the mock/stubs to a separate package?\n. This change is fine by me, although less ideal. \n. Got it. I'm fine with this change. It does not affect regular workflow unless we use go tools directly.\n. Build is still failing. I suspect this is because the jenkins has not been updated to use make yet. I will make that change now.\n. LGTM\n. Are you referring to the Docker Containers page in the UI or\nsubContainers in the REST API?\n\nOn Sun, Feb 14, 2016 at 8:36 PM, hridyeshpant notifications@github.com\nwrote:\n\nwe recently moved docker 1.7.1 to docker 1.9.1 and seems cAdvisor not\nshowing Subcontainers sections with docker 1.9.1.\nattaching the image for both 1.7.1 and 1.9.1.\nCOMMAND run : docker run --volume=/var/run:/var/run:rw\n--volume=/var/lib/docker/:/var/lib/docker:ro --publish=8080:8080\n--detach=true --name=cadvisor google/cadvisor:latest\nlog with docker 1.9.1>\nI0215 04:19:44.580674 1 storagedriver.go:44] Caching stats in memory for\n2m0s\nI0215 04:19:44.581071 1 manager.go:131] cAdvisor running in container:\n\"/docker/fa9ceba4782af64adb42d3ee1089f56c8cea8481fc7e5330db44b8b09554955d\"\nW0215 04:19:44.708814 1 fs.go:159] Could not get Docker devicemapper\ndevice: exit status 1\nI0215 04:19:44.708853 1 fs.go:107] Filesystem partitions:\nmap[/dev/mapper/docker-202:1-263179-fa9ceba4782af64adb42d3ee1089f56c8cea8481fc7e5330db44b8b09554955d:{mountpoint:/\nmajor:253 minor:28 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-7704baba9fe1f722b6eb874b41a4f90537b04fd2c2246a25b0c0c461bc8fba2d:{mountpoint:/var/lib/docker/devicemapper/mnt/7704baba9fe1f722b6eb874b41a4f90537b04fd2c2246a25b0c0c461bc8fba2d\nmajor:253 minor:16 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-775b04fe4d43c95c673ea2c6059eac7ff16da9b4ef8ef94e483fb45ed51fca31:{mountpoint:/var/lib/docker/devicemapper/mnt/775b04fe4d43c95c673ea2c6059eac7ff16da9b4ef8ef94e483fb45ed51fca31\nmajor:253 minor:5 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-bcca1940e3190016ce4665dbe3df7563621a25f1383e535aa0820cb13118577d:{mountpoint:/var/lib/docker/devicemapper/mnt/bcca1940e3190016ce4665dbe3df7563621a25f1383e535aa0820cb13118577d\nmajor:253 minor:8 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-11676189\nd23786081612c6cd3bbe6d27c4923faaaab8290a2ff752466ce96759:{mountpoint:/var/lib/docker/devicemapper/mnt/11676189d23786081612c6cd3bbe6d27c4923faaaab8290a2ff752466ce96759\nmajor:253 minor:34 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-480a181947a7812d8d7249efe1c408734aeafef0cecc1c46c8694ee2cbc67c0e:{mountpoint:/var/lib/docker/devicemapper/mnt/480a181947a7812d8d7249efe1c408734aeafef0cecc1c46c8694ee2cbc67c0e\nmajor:253 minor:21 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-72b862e58b63ff2b5d8631f3ceb3b669153e4aa08f7d0444d6508956a4adaa38:{mountpoint:/var/lib/docker/devicemapper/mnt/72b862e58b63ff2b5d8631f3ceb3b669153e4aa08f7d0444d6508956a4adaa38\nmajor:253 minor:6 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-eda24965c0223f7cc871b0707ea37c102d3e35724b69066014f60d107f50277e:{mountpoint:/var/lib/docker/devicemapper/mnt/eda24965c0223f7cc871b0707ea37c102d3e35724b69066014f60d107f50277e\nmajor:253 minor:24 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-847247b1652fbb34ce\n18e05dfe72eb3b65e1f2e780624842858f308d19fcb25b:{mountpoint:/var/lib/docker/devicemapper/mnt/847247b1652fbb34ce18e05dfe72eb3b65e1f2e780624842858f308d19fcb25b\nmajor:253 minor:17 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-f3e24c7684f52d1b3e6a18de4df3937be54c138773db425c2195268a1328b40f:{mountpoint:/var/lib/docker/devicemapper/mnt/f3e24c7684f52d1b3e6a18de4df3937be54c138773db425c2195268a1328b40f\nmajor:253 minor:22 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-176b90b691dddb1f479cad4ea73ee710fcbab48c4d72c047472d958a07d0f870:{mountpoint:/var/lib/docker/devicemapper/mnt/176b90b691dddb1f479cad4ea73ee710fcbab48c4d72c047472d958a07d0f870\nmajor:253 minor:15 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-9949780c489ee56380b12581570a770b2afcb2f2300c0d56d4dc44d752812670:{mountpoint:/var/lib/docker/devicemapper/mnt/9949780c489ee56380b12581570a770b2afcb2f2300c0d56d4dc44d752812670\nmajor:253 minor:9 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-75572eac115f352db4fcb302f9cd\nd864a74f0498fce92f4c67f5689735e8b933:{mountpoint:/var/lib/docker/devicemapper/mnt/75572eac115f352db4fcb302f9cdd864a74f0498fce92f4c67f5689735e8b933\nmajor:253 minor:23 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-1248d2b401d7034d3db1311a3ef1dc280d1ea38083b6df72cad86a26169a2b3c:{mountpoint:/var/lib/docker/devicemapper/mnt/1248d2b401d7034d3db1311a3ef1dc280d1ea38083b6df72cad86a26169a2b3c\nmajor:253 minor:7 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-e02616764254712faf5b22eacd2d6a0790c8aceed7b3805c36a450ae42a6325e:{mountpoint:/var/lib/docker/devicemapper/mnt/e02616764254712faf5b22eacd2d6a0790c8aceed7b3805c36a450ae42a6325e\nmajor:253 minor:3 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-f037773cc126640f43114100a4c5730e061581ed08d773079eb1c273eb69fa6a:{mountpoint:/var/lib/docker/devicemapper/mnt/f037773cc126640f43114100a4c5730e061581ed08d773079eb1c273eb69fa6a\nmajor:253 minor:18 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-4db3d26d9db9947bf9787d894596b15c31a47cd\n416df0f7a8a333a8d71fc6c69:{mountpoint:/var/lib/docker/devicemapper/mnt/4db3d26d9db9947bf9787d894596b15c31a47cd416df0f7a8a333a8d71fc6c69\nmajor:253 minor:19 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-9b1cd35faed02264603b4d7039752a52b96fffce2a9e7f035a4096793d4cbf42:{mountpoint:/var/lib/docker/devicemapper/mnt/9b1cd35faed02264603b4d7039752a52b96fffce2a9e7f035a4096793d4cbf42\nmajor:253 minor:25 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-df6e86d10c4df644fd87ed46d9a8347f0f439dc28d52602cd6d458745c79a432:{mountpoint:/var/lib/docker/devicemapper/mnt/df6e86d10c4df644fd87ed46d9a8347f0f439dc28d52602cd6d458745c79a432\nmajor:253 minor:13 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-dad6b57dcd338b61d2841355c73e7213b69e38c0c6fd61701d62e675ab936d30:{mountpoint:/var/lib/docker/devicemapper/mnt/dad6b57dcd338b61d2841355c73e7213b69e38c0c6fd61701d62e675ab936d30\nmajor:253 minor:14 fsType: blockSize:0} /dev/xvda1:{mountpoint:/var/run\nmajor:202 minor:1 fsType: blockSize:0} /dev/mapp\ner/docker-202:1-263179-09310b4809384d3d731aeba4d9226c11afd6e30137e3cc05b50d649f7a2e5841:{mountpoint:/var/lib/docker/devicemapper/mnt/09310b4809384d3d731aeba4d9226c11afd6e30137e3cc05b50d649f7a2e5841\nmajor:253 minor:11 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-b5904346f03f597304012a7fbc261d31e3ad9b9e282e2c4770cd76121aae4f1e:{mountpoint:/var/lib/docker/devicemapper/mnt/b5904346f03f597304012a7fbc261d31e3ad9b9e282e2c4770cd76121aae4f1e\nmajor:253 minor:10 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-87cedb9ea464f70b92af822593c2cd13f863b7f21f50327a8645dd9466722d44:{mountpoint:/var/lib/docker/devicemapper/mnt/87cedb9ea464f70b92af822593c2cd13f863b7f21f50327a8645dd9466722d44\nmajor:253 minor:12 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-bb29927bb61eb21d4094b8c9d4c2b31492fe6b3bb913580b10fd3eb51eb58d36:{mountpoint:/var/lib/docker/devicemapper/mnt/bb29927bb61eb21d4094b8c9d4c2b31492fe6b3bb913580b10fd3eb51eb58d36\nmajor:253 minor:20 fsType: blockSize:0} /dev/mapper/docker\n-202:1-263179-5075228d883a883a44c7680e57638515620d55d590173f3ba3c632686dc8bd0c:{mountpoint:/var/lib/docker/devicemapper/mnt/5075228d883a883a44c7680e57638515620d55d590173f3ba3c632686dc8bd0c\nmajor:253 minor:26 fsType: blockSize:0}\n/dev/mapper/docker-202:1-263179-724e6737f484b655cc5a498845b620314ec2f84ac098e82c4644159883716422:{mountpoint:/var/lib/docker/devicemapper/mnt/724e6737f484b655cc5a498845b620314ec2f84ac098e82c4644159883716422\nmajor:253 minor:27 fsType: blockSize:0}]\nI0215 04:19:44.801700 1 machine.go:50] Couldn't collect info from any of\nthe files in \"/etc/machine-id,/var/lib/dbus/machine-id\"\nI0215 04:19:44.801742 1 manager.go:166] Machine: {NumCores:16\nCpuFrequency:2900092 MemoryCapacity:31612305408 MachineID:\nSystemUUID:EC21D58A-9992-0A49-677A-8754A392114F\nBootID:26a8edd2-e5ad-4083-a54b-58b37b5915f9\nFilesystems:[{Device:/dev/mapper/docker-202:1-263179-5075228d883a883a44c7680e57638515620d55d590173f3ba3c632686dc8bd0c\nCapacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-724e6737f484b655cc5a498845b620314ec2f84ac098e82c4644159883716422\nCapacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-87cedb9ea464f70b92af822593c2cd13f863b7f21f50327a8645dd9466722d44\nCapacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-bb29927bb61eb21d4094b8c9d4c2b31492fe6b3bb913580b10fd3eb51eb58d36\nCapacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-fa9ceba4782af64adb42d3ee1089f56c8cea8481fc7e5330db44b8b09554955d\nCapacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-bcca1940e3190016ce4665dbe3df7563621a25f1383e535aa0820cb13118577d\nCapacity:107319656448 }\n{Device:/dev/mapper/docker-202:1-263179-11676189d23786081612c6cd3bbe6d27c4923faaaab8290a2ff752466ce96759\nCapacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-7704baba9fe1f722b6eb874b41a4f90537b04fd2c2246a25b0c0c461bc8fba2d\nCapacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-775b04fe4d43c95c673ea2c6059eac7ff16da9b4ef8ef94e483fb45ed51fca31\nCapacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-eda24965c0223f7cc871b0707ea37c102d3e35724b69066014f60d107f50277e\nCapacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-480a181947a7812d8d7249efe1c408734aeafef0cecc1c46c8694ee2cbc67c0e\nCapacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-72b862e58b63ff2b5d8631f3ceb3b669153e4aa08f7d0444d6508956a4adaa38\nCapacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-f3e24c7684f52d1b3e6a18de4df3937be54c138773db425c2195268a1328b40f\nCapacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-176b90b691dddb1f479cad4ea73ee710fcbab48c4d72c047472d958a07d\n0f870 Capacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-847247b1652fbb34ce18e05dfe72eb3b65e1f2e780624842858f308d19fcb25b\nCapacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-1248d2b401d7034d3db1311a3ef1dc280d1ea38083b6df72cad86a26169a2b3c\nCapacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-9949780c489ee56380b12581570a770b2afcb2f2300c0d56d4dc44d752812670\nCapacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-75572eac115f352db4fcb302f9cdd864a74f0498fce92f4c67f5689735e8b933\nCapacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-4db3d26d9db9947bf9787d894596b15c31a47cd416df0f7a8a333a8d71fc6c69\nCapacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-9b1cd35faed02264603b4d7039752a52b96fffce2a9e7f035a4096793d4cbf42\nCapacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-e02616764254712faf5b22eacd2d6a0790c8aceed7b3805c36a450ae42a6325e\nCapacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-f037773cc126640f43114100a4c5730e\n061581ed08d773079eb1c273eb69fa6a Capacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-09310b4809384d3d731aeba4d9226c11afd6e30137e3cc05b50d649f7a2e5841\nCapacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-df6e86d10c4df644fd87ed46d9a8347f0f439dc28d52602cd6d458745c79a432\nCapacity:107319656448}\n{Device:/dev/mapper/docker-202:1-263179-dad6b57dcd338b61d2841355c73e7213b69e38c0c6fd61701d62e675ab936d30\nCapacity:107319656448} {Device:/dev/xvda1 Capacity:8318783488}\n{Device:/dev/mapper/docker-202:1-263179-b5904346f03f597304012a7fbc261d31e3ad9b9e282e2c4770cd76121aae4f1e\nCapacity:107319656448}] DiskMap:map[253:1:{Name:dm-1 Major:253 Minor:1\nSize:23488102400 Scheduler:none} 253:2:{Name:dm-2 Major:253 Minor:2\nSize:23488102400 Scheduler:none} 253:26:{Name:dm-26 Major:253 Minor:26\nSize:107374182400 Scheduler:none} 253:11:{Name:dm-11 Major:253 Minor:11\nSize:107374182400 Scheduler:none} 253:14:{Name:dm-14 Major:253 Minor:14\nSize:107374182400 Scheduler:none} 253:16:{Name:dm-16 Major :253 Minor:16\nSize:107374182400 Scheduler:none} 253:18:{Name:dm-18 Major:253 Minor:18\nSize:107374182400 Scheduler:none} 253:20:{Name:dm-20 Major:253 Minor:20\nSize:107374182400 Scheduler:none} 253:23:{Name:dm-23 Major:253 Minor:23\nSize:107374182400 Scheduler:none} 253:24:{Name:dm-24 Major:253 Minor:24\nSize:107374182400 Scheduler:none} 253:4:{Name:dm-4 Major:253 Minor:4\nSize:107374182400 Scheduler:none} 202:26368:{Name:xvdcz Major:202\nMinor:26368 Size:23622320128 Scheduler:noop} 253:34:{Name:dm-34 Major:253\nMinor:34 Size:107374182400 Scheduler:none} 253:0:{Name:dm-0 Major:253\nMinor:0 Size:25165824 Scheduler:none} 253:13:{Name:dm-13 Major:253 Minor:13\nSize:107374182400 Scheduler:none} 253:3:{Name:dm-3 Major:253 Minor:3\nSize:107374182400 Scheduler:none} 253:9:{Name:dm-9 Major:253 Minor:9\nSize:107374182400 Scheduler:none} 253:17:{Name:dm-17 Major:253 Minor:17\nSize:107374182400 Scheduler:none} 253:27:{Name:dm-27 Major:253 Minor:27\nSize:107374182400 Scheduler:none} 253:5:{Name:dm-5 Major:2 53 Minor:5\nSize:107374182400 Scheduler:none} 202:0:{Name:xvda Major:202 Minor:0\nSize:8589934592 Scheduler:noop} 253:6:{Name:dm-6 Major:253 Minor:6\nSize:107374182400 Scheduler:none} 253:8:{Name:dm-8 Major:253 Minor:8\nSize:107374182400 Scheduler:none} 253:10:{Name:dm-10 Major:253 Minor:10\nSize:107374182400 Scheduler:none} 253:15:{Name:dm-15 Major:253 Minor:15\nSize:107374182400 Scheduler:none} 253:19:{Name:dm-19 Major:253 Minor:19\nSize:107374182400 Scheduler:none} 253:25:{Name:dm-25 Major:253 Minor:25\nSize:107374182400 Scheduler:none} 253:7:{Name:dm-7 Major:253 Minor:7\nSize:107374182400 Scheduler:none} 253:12:{Name:dm-12 Major:253 Minor:12\nSize:107374182400 Scheduler:none} 253:21:{Name:dm-21 Major:253 Minor:21\nSize:107374182400 Scheduler:none} 253:22:{Name:dm-22 Major:253 Minor:22\nSize:107374182400 Scheduler:none} 253:28:{Name:dm-28 Major:253 Minor:28\nSize:107374182400 Scheduler:none}] NetworkDevices:[{Name:eth0\nMacAddress:02:42:ac:11:00:03 Speed:10000 Mtu:9001}] Topology:[{Id:0 Memory\n:31612305408 Cores:[{Id:0 Threads:[0 8] Caches:[{Size:32768 Type:Data\nLevel:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified\nLevel:2}]} {Id:1 Threads:[1 9] Caches:[{Size:32768 Type:Data Level:1}\n{Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}\n{Id:2 Threads:[2 10] Caches:[{Size:32768 Type:Data Level:1} {Size:32768\nType:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:3\nThreads:[3 11] Caches:[{Size:32768 Type:Data Level:1} {Size:32768\nType:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:4\nThreads:[4 12] Caches:[{Size:32768 Type:Data Level:1} {Size:32768\nType:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:5\nThreads:[5 13] Caches:[{Size:32768 Type:Data Level:1} {Size:32768\nType:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:6\nThreads:[6 14] Caches:[{Size:32768 Type:Data Level:1} {Size:32768\nType:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:7\nThreads:[7 15] Caches:[{Size :32768 Type:Data Level:1} {Size:32768\nType:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}]\nCaches:[{Size:26214400 Type:Unified Level:3}]}] CloudProvider:Unknown\nInstanceType:Unknown}\nI0215 04:19:44.803743 1 manager.go:172] Version:\n{KernelVersion:4.1.13-19.30.amzn1.x86_64 ContainerOsVersion:Alpine Linux\nv3.2 DockerVersion:1.9.1 CadvisorVersion:0.20.5 CadvisorRevision:9aa348f}\nI0215 04:19:44.916812 1 factory.go:252] Registering Docker factory\nI0215 04:19:44.919849 1 factory.go:94] Registering Raw factory\nI0215 04:19:45.117911 1 manager.go:1000] Started watching for new ooms in\nmanager\nW0215 04:19:45.118675 1 manager.go:239] Could not configure a source for\nOOM detection, disabling OOM events: exec: \"journalctl\": executable file\nnot found in $PATH\nI0215 04:19:45.146574 1 manager.go:252] Starting recovery of all containers\nI0215 04:19:45.146963 1 manager.go:257] Recovery completed\nI0215 04:19:45.147596 1 cadvisor.go:106] Starting cAdvisor version:\n0.20.5-9aa348f on port 8080\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1115.\n. Can you try the latest version v0.22.0?\n\nOn Sun, Feb 28, 2016 at 1:00 AM hridyeshpant notifications@github.com\nwrote:\n\nyes @vishh https://github.com/vishh i am referring the UI page. I tried\nremoving \"/var/lib/docker/:/var/lib/docker:ro\" in my command line, but\nstill Cadvisor Ui is not showing subContainers section . Same is working\nfine with docker 1.7.1. Please see the image attached, missing\nsubContainers when running with 1.9.1 in UI.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1115#issuecomment-189823785.\n. @k8s-bot test this\n\nOn Tue, Feb 16, 2016 at 12:34 PM, Kubernetes Bot notifications@github.com\nwrote:\n\nJenkins GCE e2e\nBuild/test failed for commit dd46a5b\nhttps://github.com/google/cadvisor/commit/dd46a5b58cdb83bfd8b04c18ec0f2961ddcf4583\n.\n- Build Log\n  https://storage.cloud.google.com/kubernetes-jenkins/pr-logs/pull/1117/cadvisor-pull-build-test-e2e/256/build-log.txt\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1117#issuecomment-184864821.\n. How do we differentiate between transient and permanent failures? A disk\ncould become slow temporarily.\n\nOn Tue, Feb 16, 2016 at 12:49 PM, Tim St. Clair notifications@github.com\nwrote:\n\nLGTM, but I'm concerned that if du times out, it may indicate a recurring\nproblem (i.e. rerunning du may also timeout). If this seems to be the case,\nwe should consider marking it as a permanent failure (maybe give it a few\ntries).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1117#issuecomment-184869905.\n. @timstclair: I added exponential backoff. Limited it to a factor of 20 for now.\n. @timstclair: I fixed #501 as part of this PR and re-enabled the disk e2e. PTAL.\n. @timstclair: Addressed comments.\n. @timstclair: This PR is ready to be reviewed again.\n. We will try to make a release by the end of this week.\n\nOn Tue, Feb 23, 2016 at 6:51 AM, Cameron Davison notifications@github.com\nwrote:\n\nAny idea when this will be released, and/or if there is a good work around\nfor the mean time?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1117#issuecomment-187727139.\n. @timstclair: Addressed the two pending comments. This PR is ready to be merged now.\n. Can you try running cadvisor v0.22?\n\nOn Tue, Mar 8, 2016 at 5:14 AM, Aditya Patawari notifications@github.com\nwrote:\n\nI am facing the same problem. Is there any workaround?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1118#issuecomment-193780319.\n. Hmm. A rebase against master should fix that for now.\n\nOn Mon, Feb 22, 2016 at 4:18 PM, googlebot notifications@github.com wrote:\n\nCLAs look good, thanks!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1120#issuecomment-187447546.\n. It will help if we can split this PR into several ones - Create a common package, move docker and raw handlers to use that and then add rkt. \n. @timstclair: Addressed all comments. PTAL.\n. @timstclair: PTAL\n. Fixed the nits. Will merge once tests are green\n. @timstclair: This PR fixes the build.\n. PTAL @timstclair \n. ok to test\n. It has to be a new release. This PR has to be cherry picked into 0.21\nrelease branch and then we can build a release. Actually there are several\nPRs marked with cherrypick-candidate label that needs to go into the 0.21\nrelease train.\n\nOn Wed, Feb 24, 2016 at 1:59 PM, Tim St. Clair notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh can you rebuild the docker image?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1123#issuecomment-188471204.\n. Merging. Tests passed after the recent update.\n. I'd like this PR to use docker info + statfs for getting docker filesystem\nstats. That way these labels don't matter.\n. ok to test\n. This change LGTM except for one nit.\n. @enoodle: There are quite a few patches with cherrypick-candidate label that have to be pulled into the release-0.21 branch before releasing a tag. We can either include all of that in this PR or wait for me to post a PR with them.\n. This PR LGTM\n. ok to test\n. @enoodle: Looks like there are a lot of PRs to cherry pick. I ended up making a new release. I will release a tag tomorrow. Sorry for the confusion.\n. Closing this in favor of #1129\n. cAdvisor will expose the Name of the docker container to InfluxDB. Is\nCadvisor having issues communicating with docker in your setup? Does\n/docker endpoint on cAdvisor work for you?\n\nOn Thu, Feb 25, 2016 at 10:42 AM, Marcello de Sales \nnotifications@github.com wrote:\n\nProblem\nThe container names captured by cAdvisor are the container IDs instead of\nthe names...\n[image: screen shot 2016-02-25 at 10 31 37 am]\nhttps://cloud.githubusercontent.com/assets/131457/13329967/991a6b54-dbab-11e5-904b-330819d8a306.png\nThis is directly reflected in the InfluxDB values of the container_name\nas shown below:\n\nSHOW TAG VALUES FROM \"cpu_usage_total\" WITH KEY = \"container_name\"\nname: container_nameTagValues\ncontainer_name\n/\n/docker\n/docker/01d931a43dcdca1ba71f26ef3eadf946aaec2c6c791785101bb4260b5c21ad40\n/docker/02cc515f2cb6234d80ab4f8615936241ab71f4e6a4e58810282a543e1044387f\n/docker/0e1e84b6291f9e6b8b328b43eab7901bb2b82fff78bbf41248299bfc1a9fc112\n\nDetails\nThe docker-engine lists the proper names that are intended to be\nsubmitted...\n[root@pprdnpmas300 npmo-server]# docker ps\nCONTAINER ID        IMAGE                                            COMMAND                  CREATED             STATUS              PORTS                            NAMES\n3f3a458b1863        npmoserver_newww                                 \"/etc/npme/start.sh\"     2 days ago          Up 2 days           5005/tcp, 0.0.0.0:80->8081/tcp   npmoserver_newww_1\n78a032e4a4b4        bcoe/policy-follower:1.0.18                      \"/etc/npme/start.sh\"     2 days ago          Up 2 days                                            npmoserver_policyfollower_1\ne206626d1b51        npmoserver_rrfollower                            \"/etc/npme/start.sh\"     2 days ago          Up 2 days                                            npmoserver_rrfollower_1\n4f903da5431d        bcoe/es-follower:1.0.3                           \"/etc/npme/start.sh\"     2 days ago          Up 2 days                                            npmoserver_esfollower_1\nThe expected names are npmoserver_newww_1, npmoserver_policyfollower_1 \u2026.\nQuestion\n- What value does cAdvisor use to send to influxdb?\n- Do I need to specify any Docker Label?\n- What's the suggested way of naming containers, given that those\n  containers may be running on multiple machines? Use the Hostname? IP\n  Address? The problem is that we need to distinguish them from different\n  machines...\ncAdvisor Version\nUsing the v0.20.5 version.\n[root@pprdnpmas300 npmo-server]# docker ps | grep cadvisor\n7c7e52082f6c        google/cadvisor:v0.20.5                          \"/usr/bin/cadvisor -l\"   2 days ago          Up 2 days           0.0.0.0:6090->8080/tcp           services_cadvisor_1\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1127.\n. Can you post the output of http://<cadvisor_ip:port>/api/v2.1/stats/docker/\n01d931a43dcdca1ba71f26ef3eadf946aaec2c6c791785101bb4260b5c21ad40 or some\nother docker container that is still running?\n\nOn Thu, Feb 25, 2016 at 1:52 PM, Marcello de Sales <notifications@github.com\n\nwrote:\n@vishh https://github.com/vishh The screenshot above is from the /docker\nendpoint... That shows the same docker ID... The query above is in the\nproduction remote influxDB host, so no communication problem so far...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1127#issuecomment-189002679.\n. I still suspect that cadvisor is not able to talk to docker and so is\ntreating your containers as cgroups-only containers.\n\nOn Thu, Feb 25, 2016 at 2:32 PM, Marcello de Sales <notifications@github.com\n\nwrote:\nHi @vishh https://github.com/vishh\nHere's the output of\nhttp://pprdnpmas300.corp.ccc.net:6090/api/v2.1/stats/docker/0e1e84b6291f9e6b8b328b43eab7901bb2b82fff78bbf41248299bfc1a9fc112\nhttps://gist.github.com/marcellodesales/ca8922b8fa6cf3d42f7c\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1127#issuecomment-189013953.\n. Can you post cAdvisor logs? If its having trouble connecting to docker it\nwould have logged it.\n\nOn Thu, Feb 25, 2016 at 2:50 PM, Marcello de Sales <notifications@github.com\n\nwrote:\n@vishh https://github.com/vishh even running on the same host, what\ndoes cadvisor needs to talk to the daemon? I'm starting cadvisor from\nDocker Compose... Is that a possible problem?\ncadvisor:\n  image: google/cadvisor:v0.20.5\n  command: -storage_driver=influxdb -storage_driver_db=cadvisor -storage_driver_host=xyz.abc.194.188:8086 -storage_driver_user=cadvisor -storage_driver_password=xxxxxxx -storage_driver_secure=False\n  restart: always\n  ports:\n    - \"6090:8080\"\n  volumes:\n    - /:/rootfs:ro\n    - /var/run:/var/run:rw\n    - /sys:/sys:ro\n    - /var/lib/docker/:/var/lib/docker:ro\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1127#issuecomment-189018367.\n. No. Can you try running with --nosystemd flag? We fixed a systemd\nintegration issue on HEAD. I suspect that might be the cause of the issue.\n\nOn Thu, Feb 25, 2016 at 3:16 PM, Marcello de Sales <notifications@github.com\n\nwrote:\n@vishh https://github.com/vishh let me collect those...\n[root@pprdnpmas300 npmo-server]# DATA_CENTER=qydc NPMO_ROLE=primary ENV=prd LOG_FORMAT=docker docker-compose -f services/ccc-ops.yml logs cadvisor\nAttaching to services_cadvisor_1\ncadvisor_1 | I0223 10:20:31.083785       1 storagedriver.go:42] Using backend storage type \"influxdb\"\ncadvisor_1 | I0223 10:20:31.084018       1 storagedriver.go:44] Caching stats in memory for 2m0s\ncadvisor_1 | I0223 10:20:31.084147       1 manager.go:131] cAdvisor running in container: \"/docker/7c7e52082f6c4f0d8b96f3a55f0f423a291df0f70a53725dd8802af22904c672\"\ncadvisor_1 | I0223 10:20:31.330678       1 fs.go:107] Filesystem partitions: map[/dev/mapper/docker-253:1-134296580-53464f81ac6740416c0e9ab0d66d551a102823e1dd3761308522004419ca3e62:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/53464f81ac6740416c0e9ab0d66d551a102823e1dd3761308522004419ca3e62 major:253 minor:7 fsType: blockSize:0} /dev/mapper/vg_root-lv_root:{mountpoint:/rootfs major:253 minor:1 fsType: blockSize:0} /dev/mapper/vg_appdata-lvm_appdata:{mountpoint:/rootfs/app major:253 minor:0 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-b54edf37a7493b3aa7ad081c44f3153c1137c0dc75935061f65784a2de4a5c31:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/b54edf37a7493b3aa7ad081c44f3153c1137c0dc75935061f65784a2de4a5c31 major:253 minor:5 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-7761918b90b9d879aaa4dbbaa8fcc237b4a8c398ce8005102f6fbeaaa9364153:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/7761918b90b9d879aaa4dbbaa8fcc237b4a8c398ce8005102f6fbeaaa9364153 maj\n or:253 minor:6 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-2bff51cd0def977b86089b215b5ea60f42fb2a9f1b16f811621218fceee6690c:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/2bff51cd0def977b86089b215b5ea60f42fb2a9f1b16f811621218fceee6690c major:253 minor:9 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-dd34e8042e298836e9d1557f74428f69a1d20b6b1dc9560c2d799cfad0ce9c52:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/dd34e8042e298836e9d1557f74428f69a1d20b6b1dc9560c2d799cfad0ce9c52 major:253 minor:10 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-76a8bf6d879db577f40cba5537e4af0dd6e9d262cdc8062ca9735e53da764113:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/76a8bf6d879db577f40cba5537e4af0dd6e9d262cdc8062ca9735e53da764113 major:253 minor:12 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-7c7e52082f6c4f0d8b96f3a55f0f423a291df0f70a53725dd8802af22904c672:{mountpoint:/ major:253 minor:17 fsType: blockSize:0} /dev/sda1:{mountpoint:/rootfs/boot ma\n jor:8 minor:1 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-e15e615b909cea26ed32345363d86a13adde5599bea79f60f282035f9aa73e5f:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/e15e615b909cea26ed32345363d86a13adde5599bea79f60f282035f9aa73e5f major:253 minor:4 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-03800a7b7a98d17a31fc6fe041fe0871edfa89086e606f4d02baf6338cbc6978:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/03800a7b7a98d17a31fc6fe041fe0871edfa89086e606f4d02baf6338cbc6978 major:253 minor:3 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-72d562779765fd40e4ea4f483de33a52cb08809480a300091d3ba4eb9390ea6b:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/72d562779765fd40e4ea4f483de33a52cb08809480a300091d3ba4eb9390ea6b major:253 minor:8 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-7efd1ec3fcb5f528204acf8c46a4535fd4dc77fb6103b90fb3e2aea0e62bff73:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/7efd1ec3fcb5f528204acf8c46a4535fd4dc77fb61\n 03b90fb3e2aea0e62bff73 major:253 minor:13 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-0e1e84b6291f9e6b8b328b43eab7901bb2b82fff78bbf41248299bfc1a9fc112:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/0e1e84b6291f9e6b8b328b43eab7901bb2b82fff78bbf41248299bfc1a9fc112 major:253 minor:16 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-0b140c1ae4d2aa4f40b2cfe5aa9e74277945bc1abfd090321bf79de0c8a47e67:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/0b140c1ae4d2aa4f40b2cfe5aa9e74277945bc1abfd090321bf79de0c8a47e67 major:253 minor:11 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-27564bd828e2ac540ce14cb8d4fbf63418afe60a76fc847be0fcc9e5a7a40025:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/27564bd828e2ac540ce14cb8d4fbf63418afe60a76fc847be0fcc9e5a7a40025 major:253 minor:14 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-1e4476a9a20d2d27a9afed0d7dd924285531488af1baaa5226c5747b71f6b342:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/1e4476a9a20\n d2d27a9afed0d7dd924285531488af1baaa5226c5747b71f6b342 major:253 minor:15 fsType: blockSize:0}]\ncadvisor_1 | I0223 10:20:31.352636       1 manager.go:166] Machine: {NumCores:4 CpuFrequency:2128000 MemoryCapacity:16651481088 MachineID:0855d19bf2cf40d7a70cb45145950327 SystemUUID:42066107-97D5-B986-DDC9-8E3CE26464A0 BootID:42e1e5fe-d80e-4a42-8a20-5018316c8343 Filesystems:[{Device:/dev/mapper/docker-253:1-134296580-03800a7b7a98d17a31fc6fe041fe0871edfa89086e606f4d02baf6338cbc6978 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-72d562779765fd40e4ea4f483de33a52cb08809480a300091d3ba4eb9390ea6b Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-7efd1ec3fcb5f528204acf8c46a4535fd4dc77fb6103b90fb3e2aea0e62bff73 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-0b140c1ae4d2aa4f40b2cfe5aa9e74277945bc1abfd090321bf79de0c8a47e67 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-27564bd828e2ac540ce14cb8d4fbf63418afe60a76fc847be0fcc9e5a7a40025 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-1e4476a9a20d2d27a9afed0d\n 7dd924285531488af1baaa5226c5747b71f6b342 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-0e1e84b6291f9e6b8b328b43eab7901bb2b82fff78bbf41248299bfc1a9fc112 Capacity:107320705024} {Device:/dev/mapper/vg_root-lv_root Capacity:40488808448} {Device:/dev/mapper/vg_appdata-lvm_appdata Capacity:31568424960} {Device:/dev/mapper/docker-253:1-134296580-b54edf37a7493b3aa7ad081c44f3153c1137c0dc75935061f65784a2de4a5c31 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-53464f81ac6740416c0e9ab0d66d551a102823e1dd3761308522004419ca3e62 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-2bff51cd0def977b86089b215b5ea60f42fb2a9f1b16f811621218fceee6690c Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-dd34e8042e298836e9d1557f74428f69a1d20b6b1dc9560c2d799cfad0ce9c52 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-76a8bf6d879db577f40cba5537e4af0dd6e9d262cdc8062ca9735e53da764113 Capacity:107320705024} {Device:/dev/mapper/dock\n er-253:1-134296580-7c7e52082f6c4f0d8b96f3a55f0f423a291df0f70a53725dd8802af22904c672 Capacity:107320705024} {Device:/dev/sda1 Capacity:264941568} {Device:/dev/mapper/docker-253:1-134296580-e15e615b909cea26ed32345363d86a13adde5599bea79f60f282035f9aa73e5f Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-7761918b90b9d879aaa4dbbaa8fcc237b4a8c398ce8005102f6fbeaaa9364153 Capacity:107320705024}] DiskMap:map[253:13:{Name:dm-13 Major:253 Minor:13 Size:107374182400 Scheduler:none} 253:14:{Name:dm-14 Major:253 Minor:14 Size:107374182400 Scheduler:none} 253:15:{Name:dm-15 Major:253 Minor:15 Size:107374182400 Scheduler:none} 253:8:{Name:dm-8 Major:253 Minor:8 Size:107374182400 Scheduler:none} 253:9:{Name:dm-9 Major:253 Minor:9 Size:107374182400 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:deadline} 253:10:{Name:dm-10 Major:253 Minor:10 Size:107374182400 Scheduler:none} 253:17:{Name:dm-17 Major:253 Minor:17 Size:107374182400 Scheduler:none} 253:6:{Name:\n dm-6 Major:253 Minor:6 Size:107374182400 Scheduler:none} 253:7:{Name:dm-7 Major:253 Minor:7 Size:107374182400 Scheduler:none} 253:16:{Name:dm-16 Major:253 Minor:16 Size:107374182400 Scheduler:none} 253:2:{Name:dm-2 Major:253 Minor:2 Size:107374182400 Scheduler:none} 253:4:{Name:dm-4 Major:253 Minor:4 Size:107374182400 Scheduler:none} 253:5:{Name:dm-5 Major:253 Minor:5 Size:107374182400 Scheduler:none} 8:16:{Name:sdb Major:8 Minor:16 Size:32212254720 Scheduler:deadline} 253:12:{Name:dm-12 Major:253 Minor:12 Size:107374182400 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:40508588032 Scheduler:none} 253:11:{Name:dm-11 Major:253 Minor:11 Size:107374182400 Scheduler:none} 253:3:{Name:dm-3 Major:253 Minor:3 Size:107374182400 Scheduler:none} 2:0:{Name:fd0 Major:2 Minor:0 Size:4096 Scheduler:deadline} 253:0:{Name:dm-0 Major:253 Minor:0 Size:32208060416 Scheduler:none}] NetworkDevices:[{Name:bridge0 MacAddress:12:32:bd:42:78:86 Speed:0 Mtu:1500} {Name:eth0 MacAddress:00:50:56:86:01\n :9b Speed:10000 Mtu:1500} {Name:eth1 MacAddress:00:50:56:86:25:f5 Speed:10000 Mtu:1500}] Topology:[{Id:0 Memory:17179336704 Cores:[{Id:0 Threads:[0] Caches:[]} {Id:1 Threads:[1] Caches:[]} {Id:2 Threads:[2] Caches:[]} {Id:3 Threads:[3] Caches:[]}] Caches:[{Size:25165824 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown}\ncadvisor_1 | I0223 10:20:31.354104       1 manager.go:172] Version: {KernelVersion:3.10.0-327.10.1.el7.x86_64 ContainerOsVersion:Alpine Linux v3.2 DockerVersion:1.9.1 CadvisorVersion:0.20.5 CadvisorRevision:9aa348f}\ncadvisor_1 | I0223 10:20:31.355327       1 factory.go:210] System is using systemd\ncadvisor_1 | I0223 10:20:31.633518       1 factory.go:252] Registering Docker factory\ncadvisor_1 | I0223 10:20:31.644199       1 factory.go:94] Registering Raw factory\ncadvisor_1 | I0223 10:20:32.006032       1 manager.go:1000] Started watching for new ooms in manager\ncadvisor_1 | W0223 10:20:32.006974       1 manager.go:239] Could not configure a source for OOM detection, disabling OOM events: exec: \"journalctl\": executable file not found in $PATH\ncadvisor_1 | I0223 10:20:32.007723       1 manager.go:252] Starting recovery of all containers\ncadvisor_1 | I0223 10:20:32.031139       1 manager.go:257] Recovery completed\ncadvisor_1 | I0223 10:20:32.060334       1 cadvisor.go:106] Starting cAdvisor version: 0.20.5-9aa348f on port 8080\nFrom here, I see the first error... cadvisor_1 | E0224 23:10:26.798628 1\nmemory.go:91] failed to write stats to influxDb - Post\nhttp://10.180.194.188:8086/write?consistency=&db=cadvisor&precision=&rp=:\ndial tcp\n10.180.194.188:8086: getsockopt: connection refused\ncadvisor_1 | W0223 10:33:32.173015       1 container.go:354] Failed to get RecentStats(\"/docker/53464f81ac6740416c0e9ab0d66d551a102823e1dd3761308522004419ca3e62\") while determining the next housekeeping: unable to find data for container /docker/53464f81ac6740416c0e9ab0d66d551a102823e1dd3761308522004419ca3e62\ncadvisor_1 | W0223 10:37:12.719889       1 container.go:341] Failed to create summary reader for \"/docker/69145724a3bc855385119e2552c70064bb1c42be7857ad6413dff4cfcb41793b\": none of the resources are being tracked.\ncadvisor_1 | W0223 10:37:15.991376       1 container.go:341] Failed to create summary reader for \"/docker/64cba7065673a8025785ead336e5c405dc97ec1a2ca6d66fe5b758f59bcc765b\": none of the resources are being tracked.\ncadvisor_1 | W0223 10:37:17.376885       1 container.go:341] Failed to create summary reader for \"/docker/978117c3fbf2e814dd1907f8b109cf1b1b683325b4f250799fa462eec9ed820f\": none of the resources are being tracked.\ncadvisor_1 | W0223 10:37:29.095995       1 container.go:341] Failed to create summary reader for \"/docker/78a032e4a4b4332238427ea20b4cdb44ee8ed44ac483131e5cba18b746513c0c\": none of the resources are being tracked.\ncadvisor_1 | E0224 09:39:55.290111       1 memory.go:91] failed to write stats to influxDb - Post http://10.180.194.188:8086/write?consistency=&db=cadvisor&precision=&rp=: dial tcp 10.180.194.188:8086: i/o timeout\ncadvisor_1 | E0224 09:40:55.385761       1 memory.go:91] failed to write stats to influxDb - Post http://10.180.194.188:8086/write?consistency=&db=cadvisor&precision=&rp=: dial tcp 10.180.194.188:8086: i/o timeout\ncadvisor_1 | E0224 09:41:25.464999       1 memory.go:91] failed to write stats to influxDb - Post http://10.180.194.188:8086/write?consistency=&db=cadvisor&precision=&rp=: dial tcp 10.180.194.188:8086: getsockopt: connection refused\ncadvisor_1 | E0224 09:42:24.102757       1 memory.go:91] failed to write stats to influxDb - Post http://10.180.194.188:8086/write?consistency=&db=cadvisor&precision=&rp=: read tcp 192.168.4.15:45426->10.180.194.188:8086: read: connection reset by peer\ncadvisor_1 | E0224 21:50:30.219446       1 memory.go:91] failed to write stats to influxDb - {\"error\":\"timeout\"}\ncadvisor_1 | E0224 23:07:26.649126       1 memory.go:91] failed to write stats to influxDb - Post http://10.180.194.188:8086/write?consistency=&db=cadvisor&precision=&rp=: dial tcp 10.180.194.188:8086: getsockopt: connection refused\ncadvisor_1 | E0224 23:08:26.655138       1 memory.go:91] failed to write stats to influxDb - Post http://10.180.194.188:8086/write?consistency=&db=cadvisor&precision=&rp=: dial tcp 10.180.194.188:8086: getsockopt: connection refused\ncadvisor_1 | E0224 23:09:26.696369       1 memory.go:91] failed to write stats to influxDb - Post http://10.180.194.188:8086/write?consistency=&db=cadvisor&precision=&rp=: dial tcp 10.180.194.188:8086: getsockopt: connection refused\ncadvisor_1 | E0224 23:10:26.798628       1 memory.go:91] failed to write stats to influxDb - Post http://10.180.194.188:8086/write?consistency=&db=cadvisor&precision=&rp=: dial tcp 10.180.194.188:8086: getsockopt: connection refused\nThen, I think the server went up again, and lost the database... {\"error\":\"database\nnot found: \\\"cadvisor\\\"\"}\ncadvisor_1 | E0225 01:42:39.394727       1 memory.go:91] failed to write stats to influxDb - Post http://10.180.194.188:8086/write?consistency=&db=cadvisor&precision=&rp=: dial tcp 10.180.194.188:8086: getsockopt: connection refused\ncadvisor_1 | E0225 01:43:40.328899       1 memory.go:91] failed to write stats to influxDb - {\"error\":\"database not found: \\\"cadvisor\\\"\"}\ncadvisor_1 | E0225 01:44:39.815301       1 memory.go:91] failed to write stats to influxDb - {\"error\":\"database not found: \\\"cadvisor\\\"\"}\ncadvisor_1 | E0225 01:45:39.809165       1 memory.go:91] failed to write stats to influxDb - {\"error\":\"database not found: \\\"cadvisor\\\"\"}\nQuestion\nCould this be the source of the problems? We used to see the full\ncontainer names before... And now only the IDs...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1127#issuecomment-189030166.\n. It looks an InfluxDB rate limiter. You can set the syncing period to be\nhigher. It is not clear from the logs why cAdvisor is not detecting docker\ncontainers. One option is to set --vmodule=*=4 flag to get additional\ninformation.\nIf everything works, you should see the container pseudo names in the UI\n(http:///docker)\n\nOn Thu, Feb 25, 2016 at 3:50 PM, Marcello de Sales <notifications@github.com\n\nwrote:\nAlso, I started seeing the following:\ncadvisor_1 | E0225 23:43:38.044736       1 memory.go:91] failed to write stats to influxDb - {\"error\":\"write failed: engine: write points: write throughput too high. backoff and retry\"}\ncadvisor_1 | E0225 23:44:38.171171       1 memory.go:91] failed to write stats to influxDb - {\"error\":\"write failed: engine: write points: write throughput too high. backoff and retry\"}\ncadvisor_1 | E0225 23:45:38.365485       1 memory.go:91] failed to write stats to influxDb - {\"error\":\"write failed: engine: write points: write throughput too high. backoff and retry\"}\ncadvisor_1 | E0225 23:46:38.780019       1 memory.go:91] failed to write stats to influxDb - {\"error\":\"write failed: engine: write points: write throughput too high. backoff and retry\"}\ncadvisor_1 | E0225 23:47:38.909916       1 memory.go:91] failed to write stats to influxDb - {\"error\":\"write failed: engine: write points: write throughput too high. backoff and retry\"}\ncadvisor_1 | E0225 23:48:39.045459       1 memory.go:91] failed to write stats to influxDb - {\"error\":\"write failed: engine: write points: write throughput too high. backoff and retry\"}\ncadvisor_1 | E0225 23:49:39.002101       1 memory.go:91] failed to write stats to influxDb - {\"error\":\"write failed: engine: write points: write throughput too high. backoff and retry\"}\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1127#issuecomment-189038495.\n. FYI: You can reduce the write rate from cadvisor side\n\nOn Thu, Feb 25, 2016 at 4:30 PM, Marcello de Sales <notifications@github.com\n\nwrote:\n@vishh https://github.com/vishh Hummm Yeah our influxdb team might have\nadded something to it as they mentioned that our cadvisor from only 4 hosts\nwas pushing events at the rate of 500/s.\nOk, let me add the flag and see if we can collect anything else...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1127#issuecomment-189050815.\n. --storage_driver_buffer_duration\n\nOn Thu, Feb 25, 2016 at 4:32 PM, Marcello de Sales <notifications@github.com\n\nwrote:\n@vishh https://github.com/vishh how? :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1127#issuecomment-189051321.\n. Ignore the du logs it will be fixed in the next release. Can you post all\nthe logs?\n\nOn Fri, Feb 26, 2016, 12:02 PM Marcello de Sales notifications@github.com\nwrote:\n\nI will be adding the rate as described at #1074 (comment)\nhttps://github.com/google/cadvisor/issues/1074#issue-128088993...\n@vishh https://github.com/vishh Any points from the logs above?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1127#issuecomment-189458349.\n. Can you try the latest cadvisor version - google/cadvisor:v0.23.0?\n. We added support for docker v1.11 in cadvisor v:0.23.0. We will soon\nswitch latest.\n\nOn Sun, May 1, 2016 at 1:50 PM, Santi Lertsumran notifications@github.com\nwrote:\n\nI used cadvisor tag :lastest\nNot work for me :(\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1127#issuecomment-216071717\n. LGTM\n. LGTM. https://github.com/google/cadvisor/pull/1128/files#r54950999 can be resolved in this PR or in a separate PR.\n. ping @sjpotter \n. cc @ncdc \n. @ncdc mentioned that RHEL7 doesn't support go 1.5 yet. We can fix the build scripts to make builds just work on RHEL. \nThe error after the fix still seems to indicate some issues with running the test container. \n. ok to test\n. LGTM\n. ok to test\n. IIRC, this PR will end up blocking the test on sleep 1000 part. \n. cAdvisor stops monitoring a container as soon as the container process exits. So until the test completes, we need the container to be running. \n. @sjenning: docker run -d busybox /bin/sh -c 'dd if=/dev/zero of=/dev/null bs=1M count=10 & sleep 100' should work. It does for me. Wonder why it breaks on RHEL though.\n. @sjenning: The test infra wraps the shell command and passes them through ssh. That should be the part that's ending up not working as expected on RHEL nodes. \n. Yeah. Passing args over ssh is being a PITA. Possible solutions are\nbundling the tests into a tarball and running them on the remote host, or\nswitching to docker remote API with ssh tunnel. I'm trying to test a hacky\nfix to get us going for now. This needs proper cleanup though.\n\nOn Mon, Feb 29, 2016 at 2:46 PM, Seth Jennings notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh it seems that with the single quotes,\nrunning the e2e works but running the integration tests locally with \"make\nintegration\" does not. This correct? And if you remove the single quotes,\nit runs locally but the e2e fails?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1136#issuecomment-190433878.\n. ok to test\n. LGTM\n. ok to test\n. If I parse correctly, you want a global view that is sorted based on CPU or\nMemory usage. This does sound really useful. I don't think the core\nmaintainers have any spare cycles to work on this feature though. I'm\nadding a help-wanted label and we will gladly accept PRs.\n\nOn Mon, Feb 29, 2016 at 2:03 PM, Andr\u00e9 Caron notifications@github.com\nwrote:\n\nI'm using cadvisor to monitor Docker containers for multiple application\ndatabases (MySQL, Redis, Mongo, Memcached) for a complex application.\nI can see the memory & CPU usage for each database container individually,\nand I can also see the global memory & CPU usage per CPU core.\nHowever, one thing I'm often interested in is seeing which container is\nstressed more heavily than the others. There is currently no graph to plot\nthis information, but it seems to me like all the information is available.\nAny chance you can add this graph?\nThanks!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1142.\n. Got it. Most of the UI elements are in javascript. The templates are in Go\nthough. Feel free to poke us if you run into any issues.\n\nOn Mon, Feb 29, 2016 at 4:49 PM, Andr\u00e9 Caron notifications@github.com\nwrote:\n\nI don't necessarily want the containers to be sorted.\nI would mostly like a timeline graph showing CPU usage by container. The\nhome page shows this type of graph, but the usage is by CPU core (which\nonly helps determine whether the docker daemon is distributing containers\nequally across all cores).\nThere is a detailed view by container where you can see the CPU usage for\na single container, so I guess the information is already available and\nthat we only need to combine the different \"feeds\" into a single graph on\nthe home page.\nI might consider putting some time into this, but I don't know anything\nabout Go...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1142#issuecomment-190472653.\n. There are no tests :(\n\nSyntax highlighting is not possible. I usually copy the JS contents into a\nseparate file and then move it to its original location.\nOn Sun, Mar 6, 2016 at 11:44 AM, Andr\u00e9 Caron notifications@github.com\nwrote:\n\nDid a quick look into this today. If I understand this correctly,\npages/containers_html.go contains the page markup and\npages/static/containers_js.go contains the JS code that powers it. I\ndon't see any tests for these files, are they stored elsewhere?\nAlso, sorry if this is a newbie question (I'm more fluent in Python), but\nwhy is the all the HTML and JS code in strings in Go files and how do you\nget syntax highlighting in your editor for this?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1142#issuecomment-192972071.\n. What version of cAdvisor are you running? Can you try the latest version?\n\nOn Tue, Mar 8, 2016 at 11:51 AM, Roch Devost notifications@github.com\nwrote:\n\nI am getting the same issue.\nDocker Version 1.10.2\nKernel Version 4.1.18-boot2docker\nOS Version Boot2Docker 1.10.2 (TCL 6.4.1); master : 611be10 - Mon Feb 22\n22:47:06 UTC 2016\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1150#issuecomment-193940507.\n. I updated latest to v0.22.0. Can you try again?\n\nOn Fri, Mar 11, 2016 at 9:01 AM, Luca Critelli notifications@github.com\nwrote:\n\nSame error for me running on Docker version 1.10.3, build 20f81dd\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1150#issuecomment-195454590.\n. Yes\n. What interface are you referring to?\n\nOn Thu, Mar 10, 2016 at 10:54 AM, fahimeh2010 notifications@github.com\nwrote:\n\nthanks, but where in the its interface?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1151#issuecomment-194999825.\n. Sure. By interface are you referring to the REST APIs?\n\nOn Fri, Mar 11, 2016 at 8:47 PM, fahimeh2010 notifications@github.com\nwrote:\n\nI downloaded the latest image on docker hub.(google/cadvisor:latest)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1151#issuecomment-195660197.\n. ok to test\n. LGTM\n. The raw driver has been simple so far - it handles any cgroups that cannot be handled by other handlers. Any specific reason to change that?\n. Ping @sjpotter.. \n. The godeps changes are making it a pain to review. Can we squash all the non godep changes into a single commit?\n. Completed another review pass. Ping me once you address all the comments @sjpotter. \nThe main issue with the PR is the lack of resiliency to rkt apiservice failures. We can address that in a subsequent PR though...\nHow are we planning to add test coverage for the rkt integration?\n. LGTM. I didn't get a chance to test this PR yet. @sjpotter: What is the plan for testing rkt integration?\n. We can add a few nodes to the e2e setup that run rkt and have the e2e framework run either docker or rkt tests based on the environment.\nThe other option is for you to setup an independent e2e and post the status on github.\n. @k8s-bot test this\n. Tests are passing. Merging this PR. We can continue iterating on it.\n. cc @dchen1107 \n. Isn't memory Limit is already exported - https://github.com/google/cadvisor/blob/master/info/v2/container.go#L48 ?\n. Yeah. The description has to be fixed.\n\nOn Mon, Mar 14, 2016 at 3:16 PM, Sam Ghods notifications@github.com wrote:\n\nYes that's right - I was looking for it in the wrong place.\nOne thing that may want to be fixed is that the comment says \"Default is\nunlimited (-1)\", but the field is uint64 so \"default\" is actually MaxUint64\n(i.e. 18446744073709551615). You may want to either make it an actual int\nor change the documentation.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1157#issuecomment-196544632.\n. @derekwaynecarr: Yes.\n. ping @timstclair \n. PTAL @timstclair \n. @k8s-bot test this\n. @k8s-bot test this\n. CI is happy now. PTAL @timstclair @ncdc \n. It should not be necessary since we set the pid. \n. @derekwaynecarr: Can you verify this fix on a systemd machine? It should be straight forward. Run with --profiling flag to hit the pprof endpoints.\n. Merging this. @timstclair I can address comments in a separate PR.\n. Just one nit. Otherwise LGTM\n. LGTM. \n. AFAIK godep got better with identifying dependencies. Open a PR to fix godeps. \n. ok to test\n. What version of godeps was used?\n. Yeah. Given that godep is not compatible across versions, we should document it.\n. This PR LGTM. Merging it..\n. cc @timstclair \n. We run CI on HEAD. I believe the PR was merged before the CI was happy with\nthe most recent commit.\n\nOn Mon, Mar 14, 2016 at 5:47 PM, Andy Goldstein notifications@github.com\nwrote:\n\nWhat CI failed - I'm guessing you have something besides the Jenkins GCE\ne2e?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/google/cadvisor/pull/1163#issuecomment-196588210\n. @k8s-bot test again\n. That flag is part of the integration test framework. Why would the PR\naffect the test framework?\n\nOn Tue, Mar 15, 2016, 6:41 AM Andy Goldstein notifications@github.com\nwrote:\n\nIt appears there's an issue with the way that CI is running the\nintegration tests. It's invoking like this:\n\"godep\" [\"go\" \"test\" \"--timeout\" \"15m0s\" \"\" \"github.com/google/cadvisor/integration/tests/...\" \"--host\" \"e2e-cadvisor-container-vm-v20160127\" \"--port\" \"8080\" \"--ssh-options\" \"-i /home/jenkins/.ssh/google_compute_engine -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o CheckHostIP=no -o StrictHostKeyChecking=no\"]\nNotice the empty string argument in between 15m0s and\ngithub.com/google/cadvisor/integration/tests/... . Because of that empty\nstring, CI is basically just running godep go test ... --host ....\nThe reason that CI is failing is because --host is not a valid flag for\nthe new cadvisor_test.go that was added in Seth's PR in the project root.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/google/cadvisor/pull/1163#issuecomment-196822547\n. It should be possible to look up using partial IDs. Feel free to post a PR\nto fix this..\n\nOn Tue, Mar 15, 2016 at 5:38 AM, Olexandr notifications@github.com wrote:\n\nIn the api docs examples shortened ids are used.\nBut when I tried this\nhttp://localhost:8080/api/v2.0/stats/a56e78bb4b8e?type=docker the\nresponse is\nunable to find Docker container \"a56e78bb4b8e\"\nBut when I use the full id it works as expected.\nThe same is for api v.1.3.\nDocker version is 1.9.1, cadvisor version is 0.22\nI have copy-pasted docker command from the README.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/google/cadvisor/issues/1164\n. @pwittrock: This PR needs your help :pray: \n. @k8s-bot test this\n. Try the latest version.\n\nOn Tue, Mar 15, 2016 at 11:50 PM, JiangJun notifications@github.com wrote:\n\nNewer version (about >= v0.20.4) cAdvisor can support for InfluxDB 0.9,\nbut I'm not sure with 0.10.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1167#issuecomment-197183545\n. LGTM\n. The same trick can be used for GCE as well. \n\nshell\n$ cat /sys/class/dmi/id/product_name \nGoogle\n. ok to test\n. @k8s-bot test again\n. @justinsb @simon3z Are there any additional checks we can perform using the AWS client in the slow path to avoid ambiguity against openstack?\n. @k8s-bot test again\n. This is WAI. cAdvisor looks only at the writable layer and the log files neither of which are expected to have mounts. Since it does not enter the container's filesystem, I don't think there is an issue. False alarm..\n. Wouldn't having an internal version also help here?\n. App metrics needs to be added to the sinks. Got some time to work on a patch?\n. @cirocosta Application metrics collected by cAdvisor are not exported to all sinks. Take a look at the appication metrics available via cAdvisor APIs and check which sinks are currently missing the metrics.. memory limit - working set is a close enough approximation of free.\n. I don't think client code is expected to be as stable as the wire format\nitself. So we can change it. Users should have vendored the existing client\ncode.\nOn Thu, Mar 24, 2016, 4:02 AM Thibaut Rousseau notifications@github.com\nwrote:\n\nGreat!\nHere are a few expectations I have from the client:\n- Every REST endpoint should be exposed in the client\n  - List of the available endpoints\n    https://github.com/google/cadvisor/blob/master/api/versions.go\n  - The methods should take a nilable/optional v2.RequestOptions\n    whenever it makes sense\n- It would be nice to have a DefaultRequestOptions somewhere\n   with the default values\n   <https://github.com/google/cadvisor/blob/05068c196a3c3343329258662ce624a9464d6b6f/api/versions.go#L518-L522>\n  - The method names should match the endpoint name (machine ->\n    Machine() instead of MachineInfo()\n  - Obviously add tests for every method\n    - Currently Stats\n      https://github.com/google/cadvisor/blob/master/client/v2/client.go#L91\n      is broken, it should return a\n      map[string][]v2.DeprecatedContainerStats instead of a\n      map[string]v2.ContainerInfo\n      - And maybe it'd be better for the resource to be a\n        map[string][]v2.ContainerInfo, this \"Deprecated\" is scary :)\n    - When creating a client, the version shouldn't be hardcoded\n      https://github.com/google/cadvisor/blob/master/client/v2/client.go#L45\n      but parameterizable\n  - Maybe this would result in a single client able to handle both v1\n    and v2? I don't know if it's doable or a good idea\n- At least httpGetJsonData\n  https://github.com/google/cadvisor/blob/master/client/v2/client.go#L166\n  should be publicly exposed to allow users to access non-exposed endpoints\n  (in case the development of the API moves faster than the client)\n  - This method should be more specialized and handle baseUrl\n    https://github.com/google/cadvisor/blob/master/client/v2/client.go#L35\n    internally, it would allow to get rid of the *Url helper methods\n    https://github.com/google/cadvisor/blob/master/client/v2/client.go#L107-L125.\n    Thus it would just need an endpoint string parameter instead of url\n    string\nThese modifications would have helped me a lot when using it!\nAs for moving client to client/v1, won't it result in breaking the\nexisting builds? Or is it not an issue since cAdvisor is still 0.x.x?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1180#issuecomment-200788139\n. I think it might be ok to deprecate v2.0 completely and support only v2.1.\n\nOn Tue, Apr 5, 2016 at 2:26 PM, Tim St. Clair notifications@github.com\nwrote:\n\nAs I mentioned above, I think we should hide the details of the minor\nversions in the client. In other words, let's provide a single V2 client\ninterface which provides the latest v2 API. The client should handle\nbackwards compatability, and upconvert the deprecated version to the latest\nversion. Does that make sense?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1180#issuecomment-205990046\n. I will make some time this week to wrap up #1073.\n\nOn Mon, Mar 28, 2016 at 1:15 PM, Tim St. Clair notifications@github.com\nwrote:\n\nxref: #1073 https://github.com/google/cadvisor/pull/1073\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1181#issuecomment-202564114\n. --port=12345 should work.\n\nOn Wed, Mar 30, 2016 at 8:00 AM, mokshpooja notifications@github.com\nwrote:\n\nWhere could change port for cAdvisor from 8080 to something else or how to\nsupply port as a config?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1184\n. Docker does not support exec driver anymore. So cAdvisor needs to stop\nchecking for the native exec driver.\n\nOn Thu, Mar 31, 2016 at 11:13 AM, Tim St. Clair notifications@github.com\nwrote:\n\nLooks like the docker inspect API changed in docker/docker@68fb7f4\nhttps://github.com/docker/docker/commit/68fb7f4b744bf71206898d32fe203556a6261e5d,\nwe might just need to update Godeps to the latest docker client (to include\nfsouza/go-dockerclient#107\nhttps://github.com/fsouza/go-dockerclient/pull/107)\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1185#issuecomment-204061078\n. Just one comment. Otherwise LGTM. Thanks for the cleanup @sjpotter \n. @krallin \nThis ensures each goroutine is given its own Netlink connection, and presumably avoids having a message destined for one goroutine read by another goroutine.\n\nCan you explain this statement a bit more? How is central cpu load collection an issue?\n. Running load reader every second does sounds like an overkill. I don't think a second granularity is necessary for such a resource. As you are suggesting we can run the load reader less frequently instead.\nThe existing cpu load logic seems to be using a 10 second decaying interval for smoothing. We can CPU load once every 10 seconds at the least.\nI also think that the entire cpu load logic should be pushed to be within each container handler - container/raw or container/docker. \n. ok to test\n. I'm ok with this change for now. \n. @timstclair This PR is ready to go if you are OK with this PR.\n. @krallin The polling frequency is always an issue when deriving value from instantaneous metrics. Thanks for sharing your experience. It might be helpful to run pprof on cadvisor while doing performance testing, to evaluate the resource consumption of cpu load calculations. If it is trivial, we could even run it every second.\n. > ... continuing on that comment: a dynamic probing interval works great with the load reader (it'll avoid wasting resources on containers that are sleeping, all the while providing accurate data for others), but I think the code that calculates the load average needs to be adjusted a little bit in order to account for the fact that the decay should depend on how old the last point was (I actually have a patch for that in my patch set as well).\nDo post that patch. As part of that, I'd like to move the cpu load calculations to the respective container handler though. \n. This LGTM. @timstclair any thoughts?\n. @krallin Ping. Can you rebase this PR?\n. LGTM\n. LGTM\n. LGTM\n. A couple of comments, otherwise LGTM\n. LGTM\n. cc @sjpotter \n. ok to test\n. Thanks for the PR. Unfortunately, we don't have e2e tests for influxdb integration. Can you post a sample output from InfluxDB that shows the newly added labels?\n. @elisehuard The commit seems to be authored by acron1@gmail.com. Thats why the CLA bot is not happy, and I checked that acron1@gmail.com is not in the CLA signers list.\n. LGTM. Thanks for the PR @elisehuard \n. LGTM\n. Would it ok if we just suppress those errors? From a usability stand point, it is great if cadvisor just works on a node that is running rkt.\n. I think having the ability to register and re-register a handler is a\nnecessity. We have been punting on that for a while.\nOn Mon, Apr 11, 2016 at 1:50 PM, Tim St. Clair notifications@github.com\nwrote:\n\nHappier, yes, but it would be nice to have some data around connection\ntimes rather than just picking an arbitrary numbers. Since we're dialing\nlocalhost, I imagine the success case is very fast. Is there an option to\nmark \"connection refused\" as a permanent error, or any other way to\ndetermine if rkt is running?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1201#issuecomment-208556000\n. Completed initial review pass. I did not look at the code in detail yet. \n. Completed another review pass. \n. ok to test\n. @pmorie This version LGTM. Kindly go ahead and complete this PR! Apologies for the slow review.\n. Reviewing this PR now @pmorie \n. @pmorie This PR is good to go, except for a few pending comments. :tada: \n. It will be very very helpful if you can add an integration test for all filesystem stats. All along, we couldn't add one because of the fact that devicemapper wasn't supported. \n. LGTM. @timstclair ok to merge?\n. ok to test\n. LGTM\n. Can we instead have a systemd plugin in cAdvisor? \n. Existing rkt integration will be a good place to start - https://github.com/google/cadvisor/tree/master/container/rkt\n. Yeah. The raw drive is the fallback option. I'd like to see more systemd\nspecific code go into the systemd handler.\n\nOn Wed, Apr 20, 2016 at 11:27 AM, Derek Carr notifications@github.com\nwrote:\n\nI guess that is what the code currently is doing... ok, I can buy that\nthis may work.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1211#issuecomment-212546039\n. Switching network namespaces has error prone in the past. Is it fair to\nexpect the metrics to be available in the container/pod IP:port?\n\nOn Thu, Apr 14, 2016 at 1:29 PM, Solly Ross notifications@github.com\nwrote:\n\ncc @mwielgus https://github.com/mwielgus @piosz\nhttps://github.com/piosz\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1212#issuecomment-210134973\n. > @vishh you could put in on the pod IP, but then you'd have to know the pod IP when writing the configuration, so you'd basically be back to have kube write the configuration. \n\nDepends on when the configuration is added. It is binded at the last minute, the kubelet can generate configs dynamically for example. We could also compose the configuration and have provisions for overriding specific fields in the config. \nIn addition to that, we can have cadvisor identify the IP of a given container in a pod as well.\n. @DirectXMan12 To be clear, we have tried switching namespaces and it has caused bugs in the past. Otherwise your idea of exposing stats only on localhost does make a lot of sense.\n. > With the exception of the \"exposing metrics to anyone with the pod IP\" issue mention above, this seems pretty reasonable to me. However, I will note that this was originally part of how custom metrics worked, and I recall that there were objections to having Kubelet autogenerate custom metrics config on the fl\nThere are multiple means to achieve this. For example, a new feature called \"init containers\" can be used to dynamically generate configuration for cAdvisor. We could support variable substitution and get pod IP from env vars.\n\nAh, ok. I'd be curious to hear what they were\n\nGo threads were in random network namespaces. On top of that exec'ing is also not cheap and adds latency. \n. > So, I think I'd prefer just having the kubelet generate the config.\n\nSomewhat separate from this issue, I've a suspicion that the whole\nconfigmap-volume-manually-writing-json workflow will end up being confusing\nand complicated for user. If we switched to just having Kubelet generate\nthe config, we could kill two birds with one stone here, so to speak.\n\nWe can create a generic cadvisor plugin container that generates configs\nfor most open source applications.\n\nMy worry with the \"init containers\" idea is that a) it appears to just be\nat the proposal phase, and b) unless the init container is generated\nautomatically (at which point why not just have kubelet generate the config\ndirectly), it makes things more complicated, not less (see above). Thus, if\nwe can't just have kubelet generate the config, I'd probably lean towards\nenv vars.\n\nReg a - @smarterclayton is working on it actively.\nReg b - Generating configs is a problem that is common to many apps. We\nshould try to build and support common patterns. We can create a standard\ncAdvisor configuration container that users can run. I can also imagine\ncluster admins injecting init containers into pods as part of admission\ncontrol.\nOn Tue, Apr 19, 2016 at 12:02 PM, Solly Ross notifications@github.com\nwrote:\n\nGo threads were in random network namespaces.\nYeah, that's mainly why I wasn't thinking of just using the system calls,\netc.\nOn top of that exec'ing is also not cheap and adds latency.\nFair. It seems like the \"expose metrics on the pod IP\" is the way we\nshould go for the moment, then.\nFor example, a new feature called \"init containers\" can be used to\ndynamically generate configuration for cAdvisor. We could support variable\nsubstitution and get pod IP from env vars.\nSo, I think I'd prefer just having the kubelet generate the config.\nSomewhat separate from this issue, I've a suspicion that the whole\nconfigmap-volume-manually-writing-json workflow will end up being confusing\nand complicated for user. If we switched to just having Kubelet generate\nthe config, we could kill two birds with one stone here, so to speak.\nMy worry with the \"init containers\" idea is that a) it appears to just be\nat the proposal phase, and b) unless the init container is generated\nautomatically (at which point why not just have kubelet generate the config\ndirectly), it makes things more complicated, not less (see above). Thus, if\nwe can't just have kubelet generate the config, I'd probably lean towards\nenv vars.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1212#issuecomment-212070529\n. LGTM\n. Just two comments. Otherwise LGTM\n. lgtm\n. LGTM\n. ok to test\n. Just one nit, otherwise LGTM\n. cc @pmorie\n. ok to test\n. @krallin Thanks for the PR. I won't have time to look at this until Wednesday though.\n. I agree that returning nil is the right approach. The v1 API is not\ncompatible with nil values and that is the reason why the existing logic\ndoesn't use nil instead of 0. I'm ok with updating the test to wait\nuntil the output is non-zero.\n\nOn Fri, Apr 22, 2016 at 12:16 AM, Thomas Orozco notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh\nI think the failing test is due to a change in behavior I made that causes\nthe fsHandler to stop reporting stats if none are available. Indeed, the\nfsHandler runs on its own schedule in a separate goroutine, so if the\ncontainer is asked for metrics before the fsHandler is done running, then\nthe fsHandler has two options:\n- Return incomplete / invalid data. This is what cAdvisor used to do.\n  It would report usage as 0, although it did correctly report disk size.\n  Unfortunately, that's not really a viable option once you stop pulling disk\n  size from a static cache (which we used to). In practice, 0 is not nil, so\n  that would explain why the test used to pass.\n- Return nothing. The fsHandler will eventually update, and it'll\n  eventually return data, but it'll won't return any until it has updated.\n  This is what it does with this patch. For clarity, that's what I was\n  referring to in my \"Second, as of this PR, the FsHandler complains when\n  it's asked for metrics it doesn't have\" comment.\n\nI'm not sure how I can reproduce the test failure to confirm. Perhaps we\nshould update the test code to provide more relevant information\nhttps://github.com/google/cadvisor/blob/bbd05e4baa6fd3675a1f5bffb382f0e60842bf1c/integration/tests/api/docker_test.go#L360,\ne.g. use:\nrequire.NotNil(t, info.Stats[0].Filesystem, \"got info: %+v\", info.Stats[0])\nLet me know what you think the next steps are here?\nThanks!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1226#issuecomment-213297257\n. If you can push a patch with this change, we can have the CI run all the tests.\n. LGTM\n. +1 for customizing the es storage driver.\n\nOn Tue, Apr 19, 2016 at 9:17 AM, Kristof L\u00fcnenschlo\u00df \nnotifications@github.com wrote:\n\nAs cadvisor can generate a lot of data, there must be an efficient way to\nset retention times for stored data.\nRight now when using the elasticsearch storage driver, cadvisor stores all\ndata in a single index (defined with --storage_driver_es_index). This\nmakes deleting old data unnecessarily difficult.\nApparently, the recommended way\nhttps://www.elastic.co/guide/en/elasticsearch/guide/current/time-based.html\nto handle time-based data in elasticsearch is to create separate indices\nfor regular time frames.\nThe way logstash handles this is by creating one index per day (e.g. as\nlogstash-2016.04.19).\nIt would be nice if cadvisor did the same by default or at least provided\nan option to do so (maybe as an option like\n--storage_driver_es_timestamped_index=cadvisor-, which would create\nindices like cadvisor-2016.04.19\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1230\n. ok to test\n. LGTM\n. @k8s-bot test this\n. LGTM\n. ok to test\n. LGTM\n. I wonder if cross compilation for arm just works.. cc @luxas\n. It might help to automate the docker image release process in cadvisor to\nensure that arm images are built for every release.\n\nOn Thu, Apr 21, 2016 at 10:48 AM, Lucas K\u00e4ldstr\u00f6m notifications@github.com\nwrote:\n\nI would just add GOARCH=arm to go build command in build/build.sh.\nIt should just work.\nHowever, if we want to cross-build a docker image, we probably have to\nbase it on debian for other arches than amd64. See the hyperkube image\nhow to do this:\nhttps://github.com/kubernetes/kubernetes/blob/master/cluster/images/hyperkube/Makefile\nIf we decide on a naming convention for non-amd64 cAdvisor and think we\nshould do it, I may send a PR that builds cAdvisor images for arm, arm64,\nppc64le.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1236#issuecomment-213035619\n. @luxas: That will be awesome!!\n\nOn Thu, Apr 21, 2016 at 10:55 AM, Lucas K\u00e4ldstr\u00f6m notifications@github.com\nwrote:\n\nI may do it if you want it\n\u2014\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1236#issuecomment-213038189\n. ok to test\n\nOn Fri, Apr 22, 2016 at 1:04 PM, Lucas K\u00e4ldstr\u00f6m notifications@github.com\nwrote:\n\nAhh, doesn't it recognize me? :)\nAlso cc @timstclair https://github.com/timstclair\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1237#issuecomment-213571400\n. @luxas The tests are not using the makefiles unfortunately. Look here. We need to make the test runner use the makefiles inorder to test this PR.\n. Updating runner.go is the way to go.\n\nOn Fri, Apr 22, 2016 at 1:55 PM, Lucas K\u00e4ldstr\u00f6m notifications@github.com\nwrote:\n\nArgh, I see.\nWill you set up an other CI or should I update the line you linked to?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1237#issuecomment-213585664\n. @pwittrock ssh is failing (again)...\n. google/cadvisor:v0.23.0 should work with docker v1.11. Once we are\nconfident with the release, we will switch that to be the latest.\n\nOn Sun, Apr 24, 2016 at 8:09 PM, Selvi K notifications@github.com wrote:\n\n@nickvanw https://github.com/nickvanw @timstclair\nhttps://github.com/timstclair I forked cadvisor a couple of months ago\nand have some local changes. I'll rebase with the latest version and try\nagain. Great to know that support is already in! Thank you very much for\nthe responses.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1239#issuecomment-214103379\n. LGTM. Sorry for the delay!\n. The reality is that we will seldom add elements out of order. Do you think its worth optimizing the sort?\n. My understanding is that kubelet mainly needs higher resolution for machine level stats and not for container stats. thin_ls matters only to containers and not the machine itself.\n@timstclair suggested adding a query param maxAge to /api/v2.1/machineStats endpoint that will help us get more recent stats on demand.\n. You need to pass in the entire container ID. cAdvisor does not understand partial IDs. If you are running containers via Kubelet, you can also query the /stats/summary endpoint on the kubelet. It is exposed on port 10255 by default. \n. @aaskey Kubernetes will expose basic cpu and memory usage metrics soon via its APIs. It will not be equivalent to that of cAdvisor or kubelet. \nAs for the summary API, code is the document as of now. @timstclair is working on making it ready for prime time with better documentation.\n. @aaskey The API has not changed. We have only introduced new revisions. The UI for instance, uses the same APIs. \n\nOne thing to note is that kubelet organizes docker containers using non-default cgroup hierarchies. To work around that you can try using /api/v1.2/docker/3ff966ba487fe2bb9f775913cff2f16e09ac34934b7f6c3c23ef0a9e593da671\n. @timstclair @thaJeztah PTAL\n. @timstclair made a few changes\n- docker based builds is now optional - I wish we make it the default because it will just work for everyone (assuming docker is available)\n- no rebuilds will be triggered for the image unless the build image Dockerfile changes\n- re-use makefile inside docker container\nPTAL\n. @timstclair \nThe thing I'm having trouble with is having make not run docker run unless the sources files have changes. I got it working for the build image, but not for source and test files. Inputs welcome :)\nIn any case we can now safely improve this.\n. @chenchun good suggestion. I went too deep in one path and did not think of having an alternate build rule \ud83d\udc4d \n. @thaJeztah \nMy intention was to make it possible to build and test cAdvisor outside of linux. Since godep is available across OSes and platforms, I did not' bother running format inside a docker container. But I can do that too :)\n@timstclair \nI'm starting to like make rules over conditionals. If we can get make to not invoke docker run when no files have changes, I'd be interested in switching the default to use docker. \n. ok to test\n. This change LGTM! Can you clarify the rationale for this change in this PR?\n. Given that there are still other distros that are not running systemd yet, it will still be helpful to fix the log rotation issue.\n. LGTM. I was thinking about this yesterday \ud83d\udc4d \n. LGTM. Thanks for the cleanup @timstclair!\n. There is an API for this already - /api/v2.0/ps/<containerName>\n. Good catch. LGTM\n. Not yet. We can support it though.\nOn Thu, May 5, 2016 at 1:14 PM, Selvi K notifications@github.com wrote:\n\nCan the App metrics facility be used to push host-level metrics instead of\nthe default container level metrics?\nAlso is there an example to be able to read from a file instead of a REST\nendpoint as shown here:\nhttps://github.com/google/cadvisor/blob/master/docs/application_metrics.md\n-Selvi\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1271\n. ok to test\n. LGTM. Although a better change would be to implement it :smile: \n. Yeah. We haven't had an explicit request for it yet. So this PR makes\nsense for now...\n\nOn Mon, May 9, 2016 at 11:59 AM, Tim St. Clair notifications@github.com\nwrote:\n\na better change would be to implement it\nIt's not just a matter of implementing it though. To be useful, it also\nneeds to be wired through to all the endpoints: REST API, clients, and UI.\nIf you think it's valuable we should open an issue for it, but as is I\nthink it was just a confusing bit for new runtimes (rkt) to deal with.\n\u2014\nYou are receiving this because you were assigned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1274#issuecomment-217956653\n. LGTM\n. @shilpamayanna Thanks for the PR. I don't think the logic is valid. Take a look at #1204 which is an attempt to add filesystem metrics support for device mapper storage backend.\n. @shilpamayanna IIUC, you intend to add support for getting stats for volumes mounted into containers. Given that volumes are essentially mounts, there is no guarantee that these mounts will be backed by devices. They can be arbitrary directories on disk, that are potentially shared between containers.\n\nIf you are using kubernetes, take a look at the output of summary API - http://localhost:10255/stats/summary on your nodes. Kubelet collects volumes stats. We can extend that to collect stats for your volume type easily. Its basically a TODO.\n. @shilpamayanna This PR is incompatible with the v2 version of the API. In v2, Filesystem stats do not include Disk metrics. We decided to expose disk metrics as part of the entire machine only. \nWhy do you need disk stats for volumes? \n. @shilpamayanna DiskIO stats are already exposed per container.\n. @shilpamayanna As part of overall machine stats you get a dump of all devices with major and minor. You can use that information to identify devices.\nI'd suggest opening an issue against heapster to support DiskIO stats.\n. /api/v2.1/machinestats\n. I'm closing this PR under the assumption that there is no change required from cAdvisor. Feel free to re-open if you disagree @shilpamayanna!\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. Should we mark the flag as deprecated before removing it?\n. Should we mark the flag as deprecated before removing it?\n. We use golang flags package. So deprecation will have to handled manually. Just post a message saying that the flag will be removed in a subsequent release and switch to using the new flag should do.\n. We use golang flags package. So deprecation will have to handled manually. Just post a message saying that the flag will be removed in a subsequent release and switch to using the new flag should do.\n. LGTM\n. LGTM\n. Just one nit. Otherwise LGTM!\n. Just one nit. Otherwise LGTM!\n. cc @krallin \n. cc @krallin \n. LGTM. Feel free to self-merge based on what you decide for https://github.com/google/cadvisor/pull/1288#discussion_r63938199\n. LGTM. Feel free to self-merge based on what you decide for https://github.com/google/cadvisor/pull/1288#discussion_r63938199\n. LGTM\n. LGTM\n. Device mapper needs a kernel module I think. It is worth mentioning that module in the release notes... cc @pmorie @sjenning \nOtherwise lgtm \n. Device mapper needs a kernel module I think. It is worth mentioning that module in the release notes... cc @pmorie @sjenning \nOtherwise lgtm \n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. Just run cAdvisor with --disable-metrics=\"\" flag. By default that flag\nhas tcp set.\nOn Fri, May 20, 2016 at 4:10 AM, f0 notifications@github.com wrote:\n\nHi,\nthe TCP Metrics are disabled per default, is there a way to enable them\nwithout patching cadvisor?\n// Metrics to be ignored.\n// Tcp metrics are ignored by default.\nignoreMetrics metricSetValue = metricSetValue{container.MetricSet{container.NetworkTcpUsageMetrics: struct{}{}}}\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1298\n. Just run cAdvisor with --disable-metrics=\"\" flag. By default that flag\nhas tcp set.\n\nOn Fri, May 20, 2016 at 4:10 AM, f0 notifications@github.com wrote:\n\nHi,\nthe TCP Metrics are disabled per default, is there a way to enable them\nwithout patching cadvisor?\n// Metrics to be ignored.\n// Tcp metrics are ignored by default.\nignoreMetrics metricSetValue = metricSetValue{container.MetricSet{container.NetworkTcpUsageMetrics: struct{}{}}}\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1298\n. It makes it possible to distribute cAdvisor & kubelet without requiring a\ncontrolled base image.\n\nOn Fri, May 20, 2016 at 2:39 PM, Tim St. Clair notifications@github.com\nwrote:\n\nWhat are the concerns around building this way? Is it just that it\nincreases our shipping binary size?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1299#issuecomment-220724942\n. It makes it possible to distribute cAdvisor & kubelet without requiring a\ncontrolled base image.\n\nOn Fri, May 20, 2016 at 2:39 PM, Tim St. Clair notifications@github.com\nwrote:\n\nWhat are the concerns around building this way? Is it just that it\nincreases our shipping binary size?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1299#issuecomment-220724942\n. This PR won't. I'm working on a separate patch for k8s. AFAIK cAdvisor was\nthe main reason why we did not link statically.\n\nOn Fri, May 20, 2016 at 3:08 PM, Tim St. Clair notifications@github.com\nwrote:\n\nThis won't affect kubelet though.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1299#issuecomment-220730145\n. This PR won't. I'm working on a separate patch for k8s. AFAIK cAdvisor was\nthe main reason why we did not link statically.\n\nOn Fri, May 20, 2016 at 3:08 PM, Tim St. Clair notifications@github.com\nwrote:\n\nThis won't affect kubelet though.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1299#issuecomment-220730145\n. Oops. Apologies. I sent out the PR while discussing kubelet distribution\nissues with my colleagues.\nI'm happy to revert this PR in favor of #1237 :)\n\nOn Sat, May 21, 2016 at 11:27 AM, Lucas K\u00e4ldstr\u00f6m notifications@github.com\nwrote:\n\nSeems like you forgot #1237 https://github.com/google/cadvisor/pull/1237,\nwhich was opened ages ago :( and failed due to internal flakes\nHowever, I'm glad this is in, and I've been experimenting with linking\nkubelet statically...\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1299#issuecomment-220793176\n. Oops. Apologies. I sent out the PR while discussing kubelet distribution\nissues with my colleagues.\nI'm happy to revert this PR in favor of #1237 :)\n\nOn Sat, May 21, 2016 at 11:27 AM, Lucas K\u00e4ldstr\u00f6m notifications@github.com\nwrote:\n\nSeems like you forgot #1237 https://github.com/google/cadvisor/pull/1237,\nwhich was opened ages ago :( and failed due to internal flakes\nHowever, I'm glad this is in, and I've been experimenting with linking\nkubelet statically...\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1299#issuecomment-220793176\n. +1\n. +1\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. cAdvisor needs to be run as root to be able to access cgroupfs.\n\nOn Mon, May 23, 2016 at 5:25 AM, f0 notifications@github.com wrote:\n\nHi, if i try to run cadvisor as a non root user (e.g core on coreos) i got\nthis error and cadvisor crashes, if i run this with root , it does work\nNode the rkt container runs with the fly stage 1\nF0523 14:22:06.227025 73579 cadvisor.go:145] Failed to start container\nmanager: inotify_add_watch\n/var/lib/rkt/pods/run/24c516a3-d62d-4372-9f4b-da782bd8c686/stage1/rootfs/opt/stage2/prometheus-container-backend/rootfs/sys/fs/cgroup/cpu,cpuacct:\npermission denied\nany ideas?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1304\n. cAdvisor needs to be run as root to be able to access cgroupfs.\n\nOn Mon, May 23, 2016 at 5:25 AM, f0 notifications@github.com wrote:\n\nHi, if i try to run cadvisor as a non root user (e.g core on coreos) i got\nthis error and cadvisor crashes, if i run this with root , it does work\nNode the rkt container runs with the fly stage 1\nF0523 14:22:06.227025 73579 cadvisor.go:145] Failed to start container\nmanager: inotify_add_watch\n/var/lib/rkt/pods/run/24c516a3-d62d-4372-9f4b-da782bd8c686/stage1/rootfs/opt/stage2/prometheus-container-backend/rootfs/sys/fs/cgroup/cpu,cpuacct:\npermission denied\nany ideas?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1304\n. Can you post the output of \"/proc/pidof cadvisor/status? I suspect that\nrkt fly is dropping perms.\n\nOn Mon, May 23, 2016 at 9:52 AM, f0 notifications@github.com wrote:\n\n@vishh https://github.com/vishh hm this only happens with such a rkt\nfly container, not with a normal rkt container or with a systemd-nspawn\ncontainer\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1304#issuecomment-221029671\n. Can you post the output of \"/proc/pidof cadvisor/status? I suspect that\nrkt fly is dropping perms.\n\nOn Mon, May 23, 2016 at 9:52 AM, f0 notifications@github.com wrote:\n\n@vishh https://github.com/vishh hm this only happens with such a rkt\nfly container, not with a normal rkt container or with a systemd-nspawn\ncontainer\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1304#issuecomment-221029671\n. cc @sjpotter \n. cc @sjpotter \n. We might have to install the relevant kernel modules and packages to have\nthin_ls work on our test nodes.\n\nOn Mon, May 23, 2016 at 4:51 PM, Paul Morie notifications@github.com\nwrote:\n\nI'm on PTO until Thurs. Will look at it then.\nOn Mon, May 23, 2016 at 6:23 PM, Tim St. Clair notifications@github.com\nwrote:\n\nActually, the actual flake looks like #1228\nhttps://github.com/google/cadvisor/issues/1228:\nF0523 15:08:38.158517 10157 runner.go:290] Error 0: error on host\ne2e-cadvisor-rhel-7: command \"godep\" [\"go\" \"test\" \"--timeout\" \"15m0s\" \"\ngithub.com/google/cadvisor/integration/tests/...\" \"--host\"\n\"e2e-cadvisor-rhel-7\" \"--port\" \"8080\" \"--ssh-options\" \"-i\n/home/jenkins/.ssh/google_compute_engine -o UserKnownHostsFile=/dev/null -o\nIdentitiesOnly=yes -o CheckHostIP=no -o StrictHostKeyChecking=no\"] failed\nwith error: exit status 1 and output: godep: WARNING: Go version (go1.6) &\n$GO15VENDOREXPERIMENT= wants to enable the vendor experiment, but disabling\nbecause a Godep workspace (Godeps/_workspace) exists\n--- FAIL: TestDockerContainerCpuStats (1.87s)\nframework.go:338: Failed to run \"sudo\" [docker rm -f\n3348d8eca4c0b22dd53f626ec3387662581f865f1ace484b99792f55811e741c] in\n\"e2e-cadvisor-rhel-7\" with error: \"exit status 255\". Stdout: \"\", Stderr:\nWarning: Permanently added 'e2e-cadvisor-rhel-7' (ECDSA) to the list of\nknown hosts.\nPermission denied (publickey,gssapi-keyex,gssapi-with-mic).\nFAIL\nFAIL github.com/google/cadvisor/integration/tests/api 90.315s\nok github.com/google/cadvisor/integration/tests/healthz 0.011s\ngodep: go exit status 1\nI think the errors reported in the logs should still be investigated,\neven\nif it just means turning down the log spam.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1306#issuecomment-221113903\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1306#issuecomment-221129046\n. We might have to install the relevant kernel modules and packages to have\nthin_ls work on our test nodes.\n\nOn Mon, May 23, 2016 at 4:51 PM, Paul Morie notifications@github.com\nwrote:\n\nI'm on PTO until Thurs. Will look at it then.\nOn Mon, May 23, 2016 at 6:23 PM, Tim St. Clair notifications@github.com\nwrote:\n\nActually, the actual flake looks like #1228\nhttps://github.com/google/cadvisor/issues/1228:\nF0523 15:08:38.158517 10157 runner.go:290] Error 0: error on host\ne2e-cadvisor-rhel-7: command \"godep\" [\"go\" \"test\" \"--timeout\" \"15m0s\" \"\ngithub.com/google/cadvisor/integration/tests/...\" \"--host\"\n\"e2e-cadvisor-rhel-7\" \"--port\" \"8080\" \"--ssh-options\" \"-i\n/home/jenkins/.ssh/google_compute_engine -o UserKnownHostsFile=/dev/null -o\nIdentitiesOnly=yes -o CheckHostIP=no -o StrictHostKeyChecking=no\"] failed\nwith error: exit status 1 and output: godep: WARNING: Go version (go1.6) &\n$GO15VENDOREXPERIMENT= wants to enable the vendor experiment, but disabling\nbecause a Godep workspace (Godeps/_workspace) exists\n--- FAIL: TestDockerContainerCpuStats (1.87s)\nframework.go:338: Failed to run \"sudo\" [docker rm -f\n3348d8eca4c0b22dd53f626ec3387662581f865f1ace484b99792f55811e741c] in\n\"e2e-cadvisor-rhel-7\" with error: \"exit status 255\". Stdout: \"\", Stderr:\nWarning: Permanently added 'e2e-cadvisor-rhel-7' (ECDSA) to the list of\nknown hosts.\nPermission denied (publickey,gssapi-keyex,gssapi-with-mic).\nFAIL\nFAIL github.com/google/cadvisor/integration/tests/api 90.315s\nok github.com/google/cadvisor/integration/tests/healthz 0.011s\ngodep: go exit status 1\nI think the errors reported in the logs should still be investigated,\neven\nif it just means turning down the log spam.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1306#issuecomment-221113903\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1306#issuecomment-221129046\n. cc @dchen1107. @pmorie wants this fixed in v1.3\n. cc @dchen1107. @pmorie wants this fixed in v1.3\n. Isn't total_ meant for hierarchical stats instead of just this cgroups' stats?\n. Isn't total_ meant for hierarchical stats instead of just this cgroups' stats?\n. They can. Current cAdvisor API doesn't provide hierarchical stats AFAIK.\n\nOn Thu, May 26, 2016 at 12:17 PM, f0 notifications@github.com wrote:\n\n@vishh https://github.com/vishh\nthats true, but the cgroup casn have children cgroups or not?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1309#issuecomment-221967560\n. They can. Current cAdvisor API doesn't provide hierarchical stats AFAIK.\n\nOn Thu, May 26, 2016 at 12:17 PM, f0 notifications@github.com wrote:\n\n@vishh https://github.com/vishh\nthats true, but the cgroup casn have children cgroups or not?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1309#issuecomment-221967560\n. Looks like its crashing on DNS resolution.\n\nIf you compiling on Unix and using go v1.5+, I'd expect cAdvisor to not\ninvoke cgo at all - More information https://golang.org/doc/go1.5#net\nOn Tue, May 24, 2016 at 1:00 PM, f0 notifications@github.com wrote:\n\nthe current master (49f3d7e\nhttps://github.com/google/cadvisor/commit/49f3d7ed3d786202ce0ae8d7ff7aa962fea53b93)\ncrashes , the version 0.23.2-7ddf6eb does work, go version go1.6.2\nlinux/amd64\nif i remove -extldflags '-static' it does work, any ideas?\nI0524 21:24:47.455933   36794 storagedriver.go:50] Caching stats in memory for 2m0s\nI0524 21:24:47.456139   36794 manager.go:138] cAdvisor running in container: \"/system.slice/system-sshd.slice/sshd@22685-10.61.112.241\"\nfatal error: unexpected signal during runtime execution\n[signal 0xb code=0x1 addr=0x0 pc=0x0]\nruntime stack:\nruntime.throw(0xfba1a0, 0x2a)\n        /usr/local/go/src/runtime/panic.go:547 +0x90\nruntime.sigpanic()\n        /usr/local/go/src/runtime/sigpanic_unix.go:12 +0x5a\ngoroutine 16 [syscall, locked to thread]:\nruntime.cgocall(0xaac3c0, 0xc820046bd8, 0x0)\n        /usr/local/go/src/runtime/cgocall.go:123 +0x11b fp=0xc820046b78 sp=0xc820046b48\nnet._C2func_getaddrinfo(0x7f13280008c0, 0x0, 0xc8202f0810, 0xc82003a058, 0x0, 0x0, 0x0)\n        ??:0 +0x55 fp=0xc820046bd8 sp=0xc820046b78\nnet.cgoLookupIPCNAME(0xea0c40, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/cgo_unix.go:111 +0x448 fp=0xc820046d50 sp=0xc820046bd8\nnet.cgoLookupIP(0xea0c40, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0, 0x653d5d706f72702e)\n        /usr/local/go/src/net/cgo_unix.go:163 +0x56 fp=0xc820046da8 sp=0xc820046d50\nnet.lookupIP(0xea0c40, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/lookup_unix.go:67 +0x94 fp=0xc820046e18 sp=0xc820046da8\nnet.glob.func16(0x104c5d0, 0xea0c40, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/hook.go:10 +0x4d fp=0xc820046e58 sp=0xc820046e18\nnet.lookupIPDeadline.func1(0x0, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/lookup.go:106 +0x71 fp=0xc820046ed8 sp=0xc820046e58\ninternal/singleflight.(_Group).doCall(0x15e2840, 0xc820316d70, 0xea0c40, 0x9, 0xc820204140)\n        /usr/local/go/src/internal/singleflight/singleflight.go:93 +0x2c fp=0xc820046f88 sp=0xc820046ed8\nruntime.goexit()\n        /usr/local/go/src/runtime/asm_amd64.s:1998 +0x1 fp=0xc820046f90 sp=0xc820046f88\ncreated by internal/singleflight.(_Group).DoChan                                                                                                                       [36/1958]\n        /usr/local/go/src/internal/singleflight/singleflight.go:86 +0x3ee\ngoroutine 1 [select]:\nnet.lookupIPDeadline(0xea0c40, 0x9, 0xeced6a181, 0x29413356, 0x15e37a0, 0x0, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/lookup.go:109 +0x6a6\nnet.internetAddrList(0xe86630, 0x3, 0xea0c40, 0xf, 0xeced6a181, 0x29413356, 0x15e37a0, 0x0, 0x0, 0x0, ...)\n        /usr/local/go/src/net/ipsock.go:252 +0x6ee\nnet.resolveAddrList(0xe81f08, 0x4, 0xe86630, 0x3, 0xea0c40, 0xf, 0xeced6a181, 0xc829413356, 0x15e37a0, 0x0, ...)\n        /usr/local/go/src/net/dial.go:158 +0x466\nnet.(_Dialer).Dial(0xc8202375e8, 0xe86630, 0x3, 0xea0c40, 0xf, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/dial.go:216 +0x124\nnet.DialTimeout(0xe86630, 0x3, 0xea0c40, 0xf, 0x77359400, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/dial.go:200 +0xa3github.com/google/cadvisor/container/rkt.Client.func1()\n        /home/fkoch/src/go/src/github.com/google/cadvisor/container/rkt/client.go:44 +0x59\nsync.(_Once).Do(0x1608b20, 0x104bcc0)\n        /usr/local/go/src/sync/once.go:44 +0xe4github.com/google/cadvisor/container/rkt.Client(0x0, 0x0, 0x0, 0x0)\n        /home/fkoch/src/go/src/github.com/google/cadvisor/container/rkt/client.go:79 +0x47github.com/google/cadvisor/container/rkt.RktPath(0x0, 0x0, 0x0, 0x0)\n        /home/fkoch/src/go/src/github.com/google/cadvisor/container/rkt/client.go:85 +0x48github.com/google/cadvisor/manager.New(0xc8202f9a40, 0x7f134c4bbad8, 0x1608a68, 0xdf8475800, 0xc8202f9a01, 0xc8203498f0, 0x0, 0x0, 0x0, 0x0)\n        /home/fkoch/src/go/src/github.com/google/cadvisor/manager/manager.go:144 +0x341\nmain.main()\n        /home/fkoch/src/go/src/github.com/google/cadvisor/cadvisor.go:121 +0x57c\ngoroutine 17 [syscall, locked to thread]:\nruntime.goexit()\n        /usr/local/go/src/runtime/asm_amd64.s:1998 +0x1\ngoroutine 6 [syscall]:\nos/signal.signal_recv(0x0)\n        /usr/local/go/src/runtime/sigqueue.go:116 +0x132\nos/signal.loop()\n        /usr/local/go/src/os/signal/signal_unix.go:22 +0x18\ncreated by os/signal.init.1\n        /usr/local/go/src/os/signal/signal_unix.go:28 +0x37\ngoroutine 7 [chan receive]:github.com/golang/glog.(*loggingT).flushDaemon(0x15e3b00)\n        /home/fkoch/src/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:882 +0x67\ncreated by github.com/golang/glog.init.1\n        /home/fkoch/src/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:410 +0x297\ngoroutine 58 [IO wait]:\nnet.runtime_pollWait(0x7f134c4bcc18, 0x72, 0xc8202b8000)\n        /usr/local/go/src/runtime/netpoll.go:160 +0x60\nnet.(_pollDesc).Wait(0xc82037a920, 0x72, 0x0, 0x0)\n        /usr/local/go/src/net/fd_poll_runtime.go:73 +0x3a\nnet.(_pollDesc).WaitRead(0xc82037a920, 0x0, 0x0)\n        /usr/local/go/src/net/fd_poll_runtime.go:78 +0x36\nnet.(_netFD).Read(0xc82037a8c0, 0xc8202b8000, 0x1000, 0x1000, 0x0, 0x7f134eddf050, 0xc82000e1b0)\n        /usr/local/go/src/net/fd_unix.go:250 +0x23a\nnet.(_conn).Read(0xc82013a1b8, 0xc8202b8000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/net.go:172 +0xe4\nnet/http.noteEOFReader.Read(0x7f134c4bccd8, 0xc82013a1b8, 0xc8202f2af8, 0xc8202b8000, 0x1000, 0x1000, 0x4082c3, 0x0, 0x0)\n        /usr/local/go/src/net/http/transport.go:1687 +0x67\nnet/http.(_noteEOFReader).Read(0xc8203714e0, 0xc8202b8000, 0x1000, 0x1000, 0xc820197d1d, 0x0, 0x0)\n        :284 +0xd0\nbufio.(_Reader).fill(0xc820072a80)\n        /usr/local/go/src/bufio/bufio.go:97 +0x1e9\nbufio.(_Reader).Peek(0xc820072a80, 0x1, 0x0, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/go/src/bufio/bufio.go:132 +0xcc\nnet/http.(_persistConn).readLoop(0xc8202f2a90)\n        /usr/local/go/src/net/http/transport.go:1073 +0x177\ncreated by net/http.(*Transport).dialConn\n        /usr/local/go/src/net/http/transport.go:857 +0x10a6\ngoroutine 59 [select]:\nnet/http.(_persistConn).writeLoop(0xc8202f2a90)\n        /usr/local/go/src/net/http/transport.go:1277 +0x472\ncreated by net/http.(_Transport).dialConn\n        /usr/local/go/src/net/http/transport.go:858 +0x10cb\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1310\n. Looks like its crashing on DNS resolution.\n\nIf you compiling on Unix and using go v1.5+, I'd expect cAdvisor to not\ninvoke cgo at all - More information https://golang.org/doc/go1.5#net\nOn Tue, May 24, 2016 at 1:00 PM, f0 notifications@github.com wrote:\n\nthe current master (49f3d7e\nhttps://github.com/google/cadvisor/commit/49f3d7ed3d786202ce0ae8d7ff7aa962fea53b93)\ncrashes , the version 0.23.2-7ddf6eb does work, go version go1.6.2\nlinux/amd64\nif i remove -extldflags '-static' it does work, any ideas?\nI0524 21:24:47.455933   36794 storagedriver.go:50] Caching stats in memory for 2m0s\nI0524 21:24:47.456139   36794 manager.go:138] cAdvisor running in container: \"/system.slice/system-sshd.slice/sshd@22685-10.61.112.241\"\nfatal error: unexpected signal during runtime execution\n[signal 0xb code=0x1 addr=0x0 pc=0x0]\nruntime stack:\nruntime.throw(0xfba1a0, 0x2a)\n        /usr/local/go/src/runtime/panic.go:547 +0x90\nruntime.sigpanic()\n        /usr/local/go/src/runtime/sigpanic_unix.go:12 +0x5a\ngoroutine 16 [syscall, locked to thread]:\nruntime.cgocall(0xaac3c0, 0xc820046bd8, 0x0)\n        /usr/local/go/src/runtime/cgocall.go:123 +0x11b fp=0xc820046b78 sp=0xc820046b48\nnet._C2func_getaddrinfo(0x7f13280008c0, 0x0, 0xc8202f0810, 0xc82003a058, 0x0, 0x0, 0x0)\n        ??:0 +0x55 fp=0xc820046bd8 sp=0xc820046b78\nnet.cgoLookupIPCNAME(0xea0c40, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/cgo_unix.go:111 +0x448 fp=0xc820046d50 sp=0xc820046bd8\nnet.cgoLookupIP(0xea0c40, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0, 0x653d5d706f72702e)\n        /usr/local/go/src/net/cgo_unix.go:163 +0x56 fp=0xc820046da8 sp=0xc820046d50\nnet.lookupIP(0xea0c40, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/lookup_unix.go:67 +0x94 fp=0xc820046e18 sp=0xc820046da8\nnet.glob.func16(0x104c5d0, 0xea0c40, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/hook.go:10 +0x4d fp=0xc820046e58 sp=0xc820046e18\nnet.lookupIPDeadline.func1(0x0, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/lookup.go:106 +0x71 fp=0xc820046ed8 sp=0xc820046e58\ninternal/singleflight.(_Group).doCall(0x15e2840, 0xc820316d70, 0xea0c40, 0x9, 0xc820204140)\n        /usr/local/go/src/internal/singleflight/singleflight.go:93 +0x2c fp=0xc820046f88 sp=0xc820046ed8\nruntime.goexit()\n        /usr/local/go/src/runtime/asm_amd64.s:1998 +0x1 fp=0xc820046f90 sp=0xc820046f88\ncreated by internal/singleflight.(_Group).DoChan                                                                                                                       [36/1958]\n        /usr/local/go/src/internal/singleflight/singleflight.go:86 +0x3ee\ngoroutine 1 [select]:\nnet.lookupIPDeadline(0xea0c40, 0x9, 0xeced6a181, 0x29413356, 0x15e37a0, 0x0, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/lookup.go:109 +0x6a6\nnet.internetAddrList(0xe86630, 0x3, 0xea0c40, 0xf, 0xeced6a181, 0x29413356, 0x15e37a0, 0x0, 0x0, 0x0, ...)\n        /usr/local/go/src/net/ipsock.go:252 +0x6ee\nnet.resolveAddrList(0xe81f08, 0x4, 0xe86630, 0x3, 0xea0c40, 0xf, 0xeced6a181, 0xc829413356, 0x15e37a0, 0x0, ...)\n        /usr/local/go/src/net/dial.go:158 +0x466\nnet.(_Dialer).Dial(0xc8202375e8, 0xe86630, 0x3, 0xea0c40, 0xf, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/dial.go:216 +0x124\nnet.DialTimeout(0xe86630, 0x3, 0xea0c40, 0xf, 0x77359400, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/dial.go:200 +0xa3github.com/google/cadvisor/container/rkt.Client.func1()\n        /home/fkoch/src/go/src/github.com/google/cadvisor/container/rkt/client.go:44 +0x59\nsync.(_Once).Do(0x1608b20, 0x104bcc0)\n        /usr/local/go/src/sync/once.go:44 +0xe4github.com/google/cadvisor/container/rkt.Client(0x0, 0x0, 0x0, 0x0)\n        /home/fkoch/src/go/src/github.com/google/cadvisor/container/rkt/client.go:79 +0x47github.com/google/cadvisor/container/rkt.RktPath(0x0, 0x0, 0x0, 0x0)\n        /home/fkoch/src/go/src/github.com/google/cadvisor/container/rkt/client.go:85 +0x48github.com/google/cadvisor/manager.New(0xc8202f9a40, 0x7f134c4bbad8, 0x1608a68, 0xdf8475800, 0xc8202f9a01, 0xc8203498f0, 0x0, 0x0, 0x0, 0x0)\n        /home/fkoch/src/go/src/github.com/google/cadvisor/manager/manager.go:144 +0x341\nmain.main()\n        /home/fkoch/src/go/src/github.com/google/cadvisor/cadvisor.go:121 +0x57c\ngoroutine 17 [syscall, locked to thread]:\nruntime.goexit()\n        /usr/local/go/src/runtime/asm_amd64.s:1998 +0x1\ngoroutine 6 [syscall]:\nos/signal.signal_recv(0x0)\n        /usr/local/go/src/runtime/sigqueue.go:116 +0x132\nos/signal.loop()\n        /usr/local/go/src/os/signal/signal_unix.go:22 +0x18\ncreated by os/signal.init.1\n        /usr/local/go/src/os/signal/signal_unix.go:28 +0x37\ngoroutine 7 [chan receive]:github.com/golang/glog.(*loggingT).flushDaemon(0x15e3b00)\n        /home/fkoch/src/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:882 +0x67\ncreated by github.com/golang/glog.init.1\n        /home/fkoch/src/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:410 +0x297\ngoroutine 58 [IO wait]:\nnet.runtime_pollWait(0x7f134c4bcc18, 0x72, 0xc8202b8000)\n        /usr/local/go/src/runtime/netpoll.go:160 +0x60\nnet.(_pollDesc).Wait(0xc82037a920, 0x72, 0x0, 0x0)\n        /usr/local/go/src/net/fd_poll_runtime.go:73 +0x3a\nnet.(_pollDesc).WaitRead(0xc82037a920, 0x0, 0x0)\n        /usr/local/go/src/net/fd_poll_runtime.go:78 +0x36\nnet.(_netFD).Read(0xc82037a8c0, 0xc8202b8000, 0x1000, 0x1000, 0x0, 0x7f134eddf050, 0xc82000e1b0)\n        /usr/local/go/src/net/fd_unix.go:250 +0x23a\nnet.(_conn).Read(0xc82013a1b8, 0xc8202b8000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/net.go:172 +0xe4\nnet/http.noteEOFReader.Read(0x7f134c4bccd8, 0xc82013a1b8, 0xc8202f2af8, 0xc8202b8000, 0x1000, 0x1000, 0x4082c3, 0x0, 0x0)\n        /usr/local/go/src/net/http/transport.go:1687 +0x67\nnet/http.(_noteEOFReader).Read(0xc8203714e0, 0xc8202b8000, 0x1000, 0x1000, 0xc820197d1d, 0x0, 0x0)\n        :284 +0xd0\nbufio.(_Reader).fill(0xc820072a80)\n        /usr/local/go/src/bufio/bufio.go:97 +0x1e9\nbufio.(_Reader).Peek(0xc820072a80, 0x1, 0x0, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/go/src/bufio/bufio.go:132 +0xcc\nnet/http.(_persistConn).readLoop(0xc8202f2a90)\n        /usr/local/go/src/net/http/transport.go:1073 +0x177\ncreated by net/http.(*Transport).dialConn\n        /usr/local/go/src/net/http/transport.go:857 +0x10a6\ngoroutine 59 [select]:\nnet/http.(_persistConn).writeLoop(0xc8202f2a90)\n        /usr/local/go/src/net/http/transport.go:1277 +0x472\ncreated by net/http.(_Transport).dialConn\n        /usr/local/go/src/net/http/transport.go:858 +0x10cb\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1310\n. It should & it does for me.\n\nshell\n$ go version\ngo version go1.6.2 linux/amd64\nOn Tue, May 24, 2016 at 8:36 PM, f0 notifications@github.com wrote:\n\n@vishh https://github.com/vishh @timstclair\nhttps://github.com/timstclair yes i know this, i use the default\nbuild.sh to build the binary and i think this should be work per default,\nor not?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1310#issuecomment-221465002\n. It should & it does for me.\n\nshell\n$ go version\ngo version go1.6.2 linux/amd64\nOn Tue, May 24, 2016 at 8:36 PM, f0 notifications@github.com wrote:\n\n@vishh https://github.com/vishh @timstclair\nhttps://github.com/timstclair yes i know this, i use the default\nbuild.sh to build the binary and i think this should be work per default,\nor not?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1310#issuecomment-221465002\n. cc @sjpotter \n. cc @sjpotter \n. +1\n\nOn Thu, May 26, 2016 at 9:05 AM, Fabian Reinartz notifications@github.com\nwrote:\n\nThe number of restarts is attached as a label to all metrics for a\ncontainer. This causes all time series for a container to break as it\nrestarts as identity is defined by the full set of labels.\nThe restart count is a time series itself and should only be exported as\nsuch.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1312\n. I tried a docker container and it did not work inside the container either!\n\nOn Fri, May 27, 2016 at 9:33 AM, Lucas K\u00e4ldstr\u00f6m notifications@github.com\nwrote:\n\n(Just a small note, I don't think the case was inside a container, it was\nin the localkube vm)\nBut it might be a bug nonetheless. At least it's confusing in case it's\nnot required\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1313#issuecomment-222193228,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AGvIKMuoI9DBKf9SxgNLJYDNIhURtwqmks5qFxzkgaJpZM4IonQd\n.\n. I think cAdvisor is not looking at the relative root path while inside the\ncontainer. On the outside, I suspect that not finding journalctl binary\nis the cause of the failure.\n\nOn Fri, May 27, 2016 at 10:23 AM, Lucas K\u00e4ldstr\u00f6m notifications@github.com\nwrote:\n\nAny clue how to solve?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1313#issuecomment-222204701,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AGvIKPEG5M_3dC_OlyyqsVkQ2ODg7xrAks5qFyh3gaJpZM4IonQd\n.\n. cAdvisor from HEAD works fine from outside the container. I haven't gotten\naround to test it inside a container yet.\n. cAdvisor is used as a library as well. So we need to be careful while\nadding external dependencies.\n\nOn Mon, Jan 9, 2017 at 5:10 AM, goettl79 notifications@github.com wrote:\n\ntestet it, mounting the binary with --volume=/usr/bin/journalctl:/usr/bin/journalctl:ro\n\\ makes the oom parser at least start manager.go:1082] Started watching\nfor new ooms in manager\nhowever the shared libraries for journalctl as well as the data is missing\ninside the container. I'd suggest to install journalctl in the cadvisor\nimage and actually change the logic based on programm arguments, not on\nexistence of a binary ...\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1313#issuecomment-271280534,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKK4HpoWlZxhCeeuxlOpW--hudwuVks5rQjHMgaJpZM4IonQd\n.\n. My point is that Dockerfile is an optional deployment model. cAdvisor can\nrun outside of Docker too.\n\nOn Mon, Jan 9, 2017 at 10:06 AM, goettl79 notifications@github.com wrote:\n\nI think this would not be a dependency to cadvisor, rather a change in the\ndockerfile.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1313#issuecomment-271358352,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKJjVLm5By6zz39HiZLKUJvqGsxetks5rQncegaJpZM4IonQd\n.\n. I'm fine with this change. We should document it though in the next release.\n. @k8s-bot test this\n. We can ignore CLA here. I think we can add a cla human approved label like in the case of kubernetes.\n. LGTM\n. LGTM\n. LGTM\n. ok to test\n. LGTM\n. @waleedsamy needs a rebase\n. ok to test\n. @ronnielai needs a rebase. \n. @ronnielai Your rebase wasn't successful. You pulled in an extra commit.\n. @googlebot check CLA\n. @dashpole can you shepherd this PR through or close it if it's no longer relevant?. +1. A PR is welcome!\n\nOn Mon, Jun 20, 2016 at 1:30 AM, Christopher Batey <notifications@github.com\n\nwrote:\nAs far as I can tell cadvisor doesn't currently expose when processes in a\ncgroup are throttled:\nhttps://github.com/google/cadvisor/blob/master/container/libcontainer/helpers.go\nIs there a reason for that? Otherwise I am happy to raise PR to add it as\nwe'd really like this data aggregated in prometheus.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1337, or mute the thread\nhttps://github.com/notifications/unsubscribe/AGvIKAL4XzAp8tDyIscAuuWU7XDE7HZBks5qNk-ngaJpZM4I5d_U\n.\n. ok to test\n. +1 for not moving away from godep for now.\n\nOn Tue, Jun 21, 2016 at 1:33 PM, Tim St. Clair notifications@github.com\nwrote:\n\nIt looks like you're doing 2 different things here:\n1. Move Godeps/_workspace to vendor\n2. Replace godep with trash for dependency management\nThe first sounds reasonable to me, as godep now supports the vendor\ndirectory and Kubernetes has already made the move. As for the second, I\nwould like to keep the cAdvisor tooling as close to the Kubernetes tooling\nas possible. Some people have been investigating replacing godep with\nglide, but I'm not sure what the status is. If you think trash is a\nsupperior option, I recommend opening a discussion on the kubernetes\nhttps://github.com/kubernetes/kubernetes repo.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1340#issuecomment-227562925, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe/AGvIKJMTBG8hPpAz87jB_kopcerca7xgks5qOEqkgaJpZM4I6XZ0\n.\n. Are you able to access http://localhost:8080 from your browser?\n. You need to expose the cAdvisor port, which is port 8080 based on your\ndescription, on your VM's public IP. Then you can access cAdvisor UI from\nyour local machine.\n\nOn Tue, Jun 21, 2016 at 10:59 PM, nitinmulchandani <notifications@github.com\n\nwrote:\nThanks for you reply.. No.. i can't\ni have just setup a ubuntu server (with docker and cadvisor setup) on\ncloud and then from my windows machine i want to access the the API's.. so\nhow to make this possible ?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1341#issuecomment-227651056,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AGvIKI7pmQ7M8VNz3PxK_0REWlveoWroks5qOM8qgaJpZM4I6lID\n.\n. LGTM. Once the existing comments are addressed, this is good to go. @ncdc is this supposed to be integrated into kube v1.3?\n. That would require a cherrypick against the release-0.23 branch. A new\ntag needs to be cut against v0.23 and then vendor that tag into kube.\n\nOn Tue, Jun 21, 2016 at 1:17 PM, Andy Goldstein notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh if this can make 1.3, that would be\ngreat\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1343#issuecomment-227558569, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe/AGvIKDUW_IS_t9rVFZGz7Taj32mr5Hkdks5qOEbWgaJpZM4I7F0V\n.\n. @k8s-bot test this\n. LGTM\n. @timstclair Were able to root cause this issue?\n. We can disable CoreOS node from the e2e suite until this issue is resolved.\nWDYT?\n\nOn Wed, Jun 22, 2016 at 10:42 AM, Tim St. Clair notifications@github.com\nwrote:\n\nLet me see if I can reproduce on a new GCE coreos instance.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1344#issuecomment-227821097,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AGvIKMebpzqb8hCjGqUgzMRqXjP98252ks5qOXPzgaJpZM4I7Hq7\n.\n. ok to test\n. LGTM\n. @k8s-bot test this\n. ok to test\n\nOn Fri, Jun 24, 2016 at 2:31 PM, Kubernetes Bot notifications@github.com\nwrote:\n\nCan one of the admins verify that this patch is reasonable to test? If so,\nplease reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update\nconfigurations in\nkubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull\nhttps://github.com/kubernetes/test-infra/tree/master/jenkins/job-configs/kubernetes-jenkins-pull\ninstead.)\nThis message may repeat a few times in short succession due to\njenkinsci/ghprb-plugin#292\nhttps://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1353#issuecomment-228467008, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe/AGvIKP3cTm_t65KlJNEUo3R77WLZm-FJks5qPEyvgaJpZM4I-JBd\n.\n. Does this mean that a device mapper fs will get filled up completely over\ntime?\n\nOn Mon, Jun 27, 2016 at 10:38 AM, Vivek Goyal notifications@github.com\nwrote:\n\nUpon file deletion, discards are not sent to thin pool (by default). That\nmeans thin pool never knows that block space consumed by file has been\nfreed. That's why you see same space.\nYou have two options.\n- Either enable online discards while mounting file system. That will\n  slow down fs and is not a very good operation.\n- Run fstrim on container rootfs and that should generate discards and\n  free up space.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1354#issuecomment-228818028,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AGvIKKo17OGZfwdPL40igQXNobQwqOoTks5qQAqcgaJpZM4I-xPG\n.\n. In that case, should we track the usage of filesystem in cAdvisor instead?\n\nOn Mon, Jun 27, 2016 at 12:20 PM, Vivek Goyal notifications@github.com\nwrote:\n\nFile system knows about the free blocks and hopefully it exhausts free\nblocks first before it asks for more blocks from thin pool. So I think\nfreed blocks will be reused. Just that these free blocks are sitting with\nfilesystem and have not been returned to thin pool.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1354#issuecomment-228847392,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AGvIKOwIaZ_8qIQtB1zEeJY-GLzKvuFTks5qQCJrgaJpZM4I-xPG\n.\n. Ack @ncdc\n\nOn Mon, Jun 27, 2016 at 12:45 PM, Andy Goldstein notifications@github.com\nwrote:\n\ni.e. I don't think this is a bug\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1354#issuecomment-228853656,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AGvIKDBnJD7hBspFLexrf5ha5P-9pxZVks5qQChEgaJpZM4I-xPG\n.\n. This is great! LGTM @timstclair !\n. @ncdc What about the overall capacity and usage that is independent of per container usage.\n. Ah! LGTM!\n\nOn Wed, Jun 29, 2016 at 1:47 PM, Andy Goldstein notifications@github.com\nwrote:\n\nWith this change, on my VM:\nI0629 16:22:30.041129    7662 fs.go:116] Filesystem partitions: map[/dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:ext4 blockSize:0} andydocker-docker--pool:{mountpoint: major:253 minor:3 fsType:devicemapper blockSize:1024} /dev/mapper/vg_vagrant-lv_root:{mountpoint:/ major:253 minor:0 fsType:xfs blockSize:0}]\nSee \"andydocker-docker--pool\"\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1358#issuecomment-229483526, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe/AGvIKCqFb-VeQYTrg3kBAgnORN_uvK7Oks5qQtnkgaJpZM4JBiId\n.\n. @jimmidyson Thanks for the tip on protected branched. I went ahead and updated the settings. This should no longer be a problem!!\n\n:100: \n. LGTM\n. +1. This will reduce maintenance overhead.\nOn Thu, Jun 30, 2016 at 5:04 PM, Tim St. Clair notifications@github.com\nwrote:\n\nIf possible, we should simply reuse the e2e-node images. If not, we should\nat least add a containervm-v20160321 node to cAdvisor, and move coreos-beta\nto coreos-stable.\ncAdvisor image list\nhttps://github.com/google/cadvisor/blob/5a28a0c718760985524c073a1848777a980fd7b4/build/jenkins_e2e.sh#L28\n:\n- e2e-cadvisor-ubuntu-trusty\n- e2e-cadvisor-container-vm-v20151215\n- e2e-cadvisor-container-vm-v20160127\n- e2e-cadvisor-rhel-7\n- e2e-cadvisor-coreos-beta (temporarily disabled)\nk8s e2e_node list\nhttps://github.com/kubernetes/kubernetes/blob/23e7b6653fe37cf597c1ed4a11b8c66c771059a0/test/e2e_node/jenkins/jenkins-ci.properties#L6\n:\n- e2e-node-ubuntu-trusty-docker9-v1-image\n- e2e-node-ubuntu-trusty-docker10-v1-image\n- e2e-node-coreos-stable20160622-image\n- e2e-node-containervm-v20160321-image\n/cc @vishh https://github.com/vishh @pwittrock\nhttps://github.com/pwittrock\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1361, or mute the thread\nhttps://github.com/notifications/unsubscribe/AGvIKLBKHOGA7DZcL7sJF8PAxRBBekw3ks5qRFl0gaJpZM4JCtFp\n.\n. LGTM\n. What platform and OS are you building against? If you build against amd64\nand linux it should work.\n\nOn Sat, Jul 2, 2016 at 3:20 AM, Yogi Hardi notifications@github.com wrote:\n\nHi All,\nI'm following the doc to build this project\ngodep go build .\nbut, I got error message like this:\ncontainer/common/inotify_watcher.go:20:2: no buildable Go source files in\n/Users/yogi/Documents/workspaces/go/cadvisor/src/\ngithub.com/google/cadvisor/Godeps/_workspace/src/golang.org/x/exp/inotify\nit seems cann't load golang.org/x/exp/inotify libs, is there anyone got\nthis error resolved? if so, please share with me.\nThanks.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1363, or mute the thread\nhttps://github.com/notifications/unsubscribe/AGvIKNuTmDYphxyAx51IDCl7QuGpPgwrks5qRjt_gaJpZM4JDpWB\n.\n. https://github.com/google/cadvisor/issues/1366#issuecomment-230896690 SGTM\n. cc @timstclair \n. @timstclair posted a new patch. PTAL\n. Depends on #1369\n. ok to test\n. cc @timstclair \n. @k8s-bot test this\n. @timstclair This patch is ready for review now. \n. That looks like processing/converting disk io metrics. We cannot disable those metrics on-demand as of now. If that memory usage is of concern, feel free to post a patch to ignore diskio metrics.\n. https://github.com/kubernetes/test-infra/pull/273 should fix jenkins CI.\n. @timstclair Portable option is to use a docker container for builds. We need to separate builds from real containers though. So I think this PR is still safe to be merged. I have a pending PR to run builds inside a docker container. If I can get that to work, then we have hit the holy grail.\n. @timstclair Given that a single CI failure is not a critical failure, do you still think it is necessary to break this PR?\n. I do. Let's give the test infra team a heads up before merging. WDYT?\n. @timstclair Feel free to self merge. \n. It might be helpful for the community to help us implement #1458. WDYT?\n\nOn Mon, Oct 24, 2016 at 11:48 AM, Tim St. Clair notifications@github.com\nwrote:\n\nWe're reconsidering how we want to handle storage drivers in cAdvisor\nright now (see #1458 https://github.com/google/cadvisor/issues/1458).\nFor the moment, storage driver additions are on-hold.\n/cc @dchen1107 https://github.com/dchen1107 @vishh\nhttps://github.com/vishh\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1378#issuecomment-255830229, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKHmQ-Dj31zweqYZtRoWDJv3VUaTNks5q3P2PgaJpZM4JM6p_\n.\n. LGTM\n. cc @mtaufen @timstclair \n. That SGTM. Feel free to close this PR.\n\nOn Mon, Jul 18, 2016 at 12:09 PM, Tim St. Clair notifications@github.com\nwrote:\n\nThe underlying issue is that we changed the jenkins jobs to use the\nbuild/jenkins_e2e.sh script, but the release-v0.23 branch doesn't have a\nfunctional version of that script.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1383#issuecomment-233427571, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKGWvz42LoKX5_7a85c0JxvD4U2yBks5qW89ugaJpZM4JO_-7\n.\n. ok to test\n. @jimmidyson I can fix the maintainer issue. \n. @jimmidyson The jenkins admin list was updated recently. Now you are in that list. \n. lgtm\n. Closing #1370\n. I'm not sure when runc will post a tag. Meanwhile, head has some build fixes and will also help with unifying dependencies across cadvisor and kube.\n. @Random-Liu this pr is ready to go\n. @mwringe Added a few comments. Thanks for this PR! Is this collector applicable to jvm based apps? Can you also update the application metrics doc to include a section on jolokia?\n. Apologies. This fell through the cracks. The only other comment I have is that of including a version field in the Spec. This will be helpful to rev the configs independently. \nI will let @timstclair review this PR before merging it. \n. My choice would be to take this PR in for now, while we figure out a\nre-design, to continue our MVP model.\n\nOn Fri, Sep 30, 2016 at 11:34 AM, Tim St. Clair notifications@github.com\nwrote:\n\n@timstclair requested changes on this pull request.\nIn collector/jolokia_collector.go\nhttps://github.com/google/cadvisor/pull/1392#pullrequestreview-2377962:\n\n@@ -0,0 +1,211 @@\n+// Copyright 2016 Google Inc. All Rights Reserved.\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//     http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package collector\n\nYes, please. We might also consider the factory model used by the storage\nbackends, but that doesn't need to be done in this PR. Lets keep it in a\nseparate package though.\nIn collector/util.go\nhttps://github.com/google/cadvisor/pull/1392#pullrequestreview-2377962:\n\n//If the exact URL was not specified, generate it based on the ip address of the container.\nendpoint := endpointConfig\n  if endpoint.URL == \"\" {\n+\n-     if endpoint.URLConfig.Protocol == \"\" || endpoint.URLConfig.Port.String() == \"\" {\n-         return fmt.Errorf(\"The endpoint URL was not specified but either the protocol [%v] or port [%v] was not specified.\", endpoint.URLConfig.Protocol, endpoint.URLConfig.Port)\n\nnit: errors start with lowercase\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1392#pullrequestreview-2377962,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKC7p39wqIkRkGCb5NO250zHYAgtqks5qvVY0gaJpZM4JSNiv\n.\n. What is the purpose of the integration? is it to enable windows support in\nk8s or monitoring in general?\n\nOn Fri, Jul 22, 2016 at 4:05 PM, Tim St. Clair notifications@github.com\nwrote:\n\nI do not think we should make cAdvisor run on windows. The model is too\ndifferent. Instead, we should wait for the container runtime interface\nhttps://github.com/kubernetes/kubernetes/blob/master/docs/proposals/container-runtime-interface-v1.md\nto define a stats interface, and implement that interface through another\nmodel (not cAdvisor).\nIt would be good to keep windows support in mind when we're defining the\nstats interface, so we can choose the right level of abstraction (i.e. not\ncgroups).\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1394#issuecomment-234677894,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKMWOlxK--5spS3HULJbC_gxLZZY8ks5qYUzXgaJpZM4JTGIp\n.\n. > Implement some rudimentary monitoring using straight Windows Perf Monitors to provide sufficient capability for k8s needs\n\nI think this is what @timstclair is suggesting. We are introducing a Container Runtime API in kubelet, and how metrics are being collected wouldn't matter to the kubelet. \nIf the plan is to make kubelet work with the existing docker integration, then the cAdvisor interfaces have to work for windows too.\n\nLeverage another monitoring solution like snap since it's been proven it can already work with k8s\n\nOur plan as of now is to not use an external monitoring solution for internal core metrics. For full-fledged monitoring there are already multiple options like snap, sysdig, etc. \n\nWait for the container runtime interface to provide a stats interface but it seems like this would take too long.\n\nI'm optimistic about this interface. There are quite a few people working towards implementing this interface. \n\n@vishh the original driver for wanting to do this was to enable windows support in k8s. \n\nOk. I was wondering if providing monitoring for windows containers is also a goal. For example, cAdvisor is already being used widely for monitoring docker containers on Linux, even outside of k8s. \n. > Do you have an idea of the ETA for this? This may be the easiest route to implement if we are simply trying to unblock the requirements for k8s at this point. Also, is the goal to ship this as part of 1.4? from the conversation on #27097 and the comment on that issue by @timstclair it seems like this is not currently prioritized.\nI'd recommend opening an issue on k8s side to understand the timelines. AFAIK, people are already working towards making this API fully functional in the next couple of months. \n. Good catch. LGTM\n. We need to improve testing around the critical APIs.. \n. Lol. Yeah. Time for :beers: :beers: \n. LGTM\n. LGTM. I wish we can test this on RHEL or CentOS once before merging.\nPing @derekwaynecarr \n. The /sys/ mount is necessary. Libcontainer had a few fixes around cgroups\nmounts discovery. When was the last time we updated libcontainer deps?\nOn Thu, Aug 4, 2016 at 12:40 PM, Tim St. Clair notifications@github.com\nwrote:\n\nMount /proc/mounts is correct, as are the symlinks. Interestingly, if I\nomit --volume=/sys:/sys:ro that error goes away, but the tests fail with\na different error:\n--- FAIL: TestDockerContainerCpuStats (5.82s)\n    Location:   docker_test.go:63\n    Error:      No error is expected but got request \"http://localhost:8080/api/v1.3/docker/578645cef7dad17177ada1bb7041320e00b4895bec4de22a618d27177ad70249\" failed with error: \"failed to get Docker container \\\"578645cef7dad17177ada1bb7041320e00b4895bec4de22a618d27177ad70249\\\" with error: unable to find Docker container \\\"578645cef7dad17177ada1bb7041320e00b4895bec4de22a618d27177ad70249\\\"\"\n    Messages:   Timed out waiting for container \"578645cef7dad17177ada1bb7041320e00b4895bec4de22a618d27177ad70249\" to be available in cAdvisor: request \"http://localhost:8080/api/v1.3/docker/578645cef7dad17177ada1bb7041320e00b4895bec4de22a618d27177ad70249\" failed with error: \"failed to get Docker container \\\"578645cef7dad17177ada1bb7041320e00b4895bec4de22a618d27177ad70249\\\" with error: unable to find Docker container \\\"578645cef7dad17177ada1bb7041320e00b4895bec4de22a618d27177ad70249\\\"\"\n\u2014\nYou are receiving this because you were assigned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1407#issuecomment-237660943, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKK6Rshr7N4tFxiUKKy9qEsk-X9Guks5qckAigaJpZM4JbHrD\n.\n. @timstclair I tried google/cadvisor:canary which is essentially a nightly build and cadvisor fails on RHEL. So the failure is itself not related to this PR. \n. It looks like docker on RHEL is incorrectly mounting cgroups. cpu,cpuacct getting mounted inside the container as cpuacct,cpu. A libcontainer bug. \n\n\n$ sudo docker run --rm -it  --volume=/:/rootfs:ro   --volume=/var/run:/var/run:rw   --volume=/sys:/sys:ro   --volume=/var/lib/docker/:/var/lib/docker:ro --privileged=true busybox mount\nrootfs on / type rootfs (rw)\n/dev/mapper/docker-8:1-50821573-09336c024e0c765d042901fd0de3dc4383b8c064c3a3c7f947683379d9841ce7 on / type xfs (rw,seclabel,relatime,nouuid,attr2,inode64,logbsize=64k,sunit=128,swidth=128,noquota)\nproc on /proc type proc (rw,nosuid,nodev,noexec,relatime)\ntmpfs on /dev type tmpfs (rw,seclabel,nosuid,mode=755)\ndevpts on /dev/pts type devpts (rw,seclabel,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=666)\nmqueue on /dev/mqueue type mqueue (rw,seclabel,nosuid,nodev,noexec,relatime)\ntmpfs on /sys/fs/cgroup type tmpfs (rw,seclabel,nosuid,nodev,noexec,relatime,mode=755)\ncgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)\ncgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)\ncgroup on /sys/fs/cgroup/cpuacct,cpu type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu)\ncgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)\ncgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)\ncgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)\ncgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)\ncgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)\ncgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)\ncgroup on /sys/fs/cgroup/net_cls type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls)\nsysfs on /sys type sysfs (ro,seclabel,relatime)\nsecurityfs on /sys/kernel/security type securityfs (rw,nosuid,nodev,noexec,relatime)\ntmpfs on /sys/fs/cgroup type tmpfs (ro,seclabel,nosuid,nodev,noexec,mode=755)\ncgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)\ncgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)\ncgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu)\ncgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)\ncgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)\ncgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)\ncgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)\ncgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)\ncgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)\ncgroup on /sys/fs/cgroup/net_cls type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls)\npstore on /sys/fs/pstore type pstore (rw,nosuid,nodev,noexec,relatime)\n\n\n. Yup. I'm merging this for now!\n. There are two scenarios as @timstclair mentioned:\n1. cAdvisor as a standalone monitoring tool\n2. cAdvisor as a metrics library that Kubelet uses\ncAdvisor is better positioned for collecting node level container metrics. For example, it can discover containers dynamically & also acquire metadata from corresponding runtimes. This makes it an attractive solution for handling application level metrics as well.\nThe real answer to this issue will come from users. If users find cAdvisor collecting application metrics to be useful, then why not support it as part of cAdvisor?\nAs for the issues that @DirectXMan12 mentioned, I find them to be bugs and not fundamental issues. For instance, cAdvisor can identify the IP of a container and use that instead of requiring host ports. \nAs for what Kubernetes should do by default, it is beyond cAdvisor. Metrics in kubernetes is a larger, more complex problem. So let's restrict this discussion to just cAdvisor as a standalone entity.\n. Extend machine.go to include GPU related information. PRs are welcome!\nOn Mon, Aug 22, 2016 at 11:20 PM, Jian Zhang notifications@github.com\nwrote:\n\nHi Guys,\nI refer to the docs cAdvisor Remote REST API\nhttps://github.com/google/cadvisor/blob/master/docs/api.md#machine-information,\nand also read related code machine.go\nhttps://github.com/google/cadvisor/blob/master/info/v1/machine.go, but\ndidn't find any API about GPU in MachineInfo struct. So, if we want to\ndisplay the GPU info on cAdvisor, how can we do to implement that?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1436, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKEzOzZVt8HkKVSGDaoqCahxl-Xm-ks5qipE2gaJpZM4JqluA\n.\n. I'd prefer avoiding additional executable dependencies to cAdvisor if\npossible. Are there any nvidia libraries that we can leverage to get the\nnecessary metrics?\n\nOn Wed, Feb 8, 2017 at 6:14 PM, Yubo Li notifications@github.com wrote:\n\n@cmluciano https://github.com/cmluciano Yes, you need either\nlibnvidia-ml.so lib or nvidia-smi binary to get live metrics. Both of\nthem are in nvidia GPU driver. But if you want to get GPU count only, a\nhack is to check /dev/nvidiaX.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1436#issuecomment-278526447,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKAYLS2RyE-s7z7_MncaYR7L4RQWBks5ranZ3gaJpZM4JqluA\n.\n. @therc any thoughts?. +1. I can help shepherd/review PRs that add GPU metrics.\n\nOn Thu, Mar 9, 2017 at 5:28 PM, Seetharami Seelam notifications@github.com\nwrote:\n\nIrrespective of the GPU sharing between containers, developers need to\nknow how much GPU memory is used by their application, total memory, gpu\nutilization, temperature, pcie link bandwidth etc. Nvidia-smi provides a\nrich collection of these metrics, I suggest we capture as many metrics as\npossible.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1436#issuecomment-285545669,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKECS7vw_Fx6d4sovBBP32hxK4_T2ks5rkKdDgaJpZM4JqluA\n.\n. @3XX0 cadvisor is used not just for collecting core kubernetes metrics, but also for general purpose container monitoring. AFAIK, we do not need any core metrics for GPUs in k8s as of now. There is a need for monitoring metrics though. So, if cAdvisor needs to support multiple means for collecting metrics, that's OK. We might disable GPU monitoring metrics in the compiled-in version of cadvisor in kubelet.. -1 for adding new binary dependencies. . I would not recommend option 1. We already have issues with CPU consumption.\nOption 3 is complex from a deployment standpoint. cAdvisor being a single process that you can drop on any node is a great UX IMHO.\n\nIs a native golang implementation possible for nvml?. Given the options, cgo seems to least intrusive. . What do you mean by non-container? Run outside of a container? or track\nresources that are not in containers?\nOn Mon, Aug 29, 2016 at 2:43 AM, mgdsxhy notifications@github.com wrote:\n\nHI\ncould cadvisor run as non-container mode? How do that?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1441, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKJuXBiPqq-w8IH9STkMpXyrxRoz8ks5qkqnfgaJpZM4JvT_p\n.\n. This is a docker issue and it should have been resolved in the more recent\nversions cAdvisor.\n\nOn Tue, Sep 6, 2016 at 3:05 AM, Atanas Mirchev notifications@github.com\nwrote:\n\n\ud83d\udc4d just stumbled upon the same problem\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1444#issuecomment-244906537,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKMxRkbQvpYg4wSYJ64JczlW4vXmsks5qnTrvgaJpZM4JxdUg\n.\n. Currently canary images include go build env. We need to change the canary\nbuild process to create canary images similar to how we build official\nimages.\n\nOn Mon, Sep 12, 2016 at 2:21 PM, Tim St. Clair notifications@github.com\nwrote:\n\nThe canary image builds cAdvisor from HEAD (in the container), so it has a\nlot more tool requirements. I'm sure it could be optimized more, but I'm\nnot sure it's worth it (we'd still welcome a PR though). If the need is\njust to have a recent image built from head, it might be easier to just\nautomate building with the same dockerfile used for the production images.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1452#issuecomment-246497805,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKOk5j2uLSLsu6s1NlYrXhTQtfsLqks5qpcJGgaJpZM4J67HN\n.\n. LGTM\n. LGTM\n. lgtm\n. +1 for 1 as well. It should be fairly straightforward to make the\nexisting storage plugins logic a standalone entity.\nPolling might introduce time sync issues, but we can hopefully address that\nwith a general purpose library that all storage plugins can re-use.\n\nOn Wed, Sep 14, 2016 at 10:19 AM, Andy Goldstein notifications@github.com\nwrote:\n\nI'm leaning towards option 1.\nHaving had the experience we have being a downstream consumer of\nKubernetes in OpenShift, I am really scared of what it would entail to keep\nforks up to date as cadvisor continues to evolve. If we do end up choosing\noption 2, we would need to have explicit guarantees about API\ncompatibility, so fork maintainers have as few headaches as possible when\nit comes time to rebase cadvisor into their fork.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1458#issuecomment-247088355,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKGw_AL7eKIUrIJx9fmsv0BcrygP2ks5qqCyogaJpZM4J8O7l\n.\n. Why not use the existing REST API? We have enough trouble maintaining that\none. We are yet to deprecate the v1 API and move to a v2 after completing\nit.\n\nOn Wed, Sep 14, 2016 at 2:46 PM, Tim St. Clair notifications@github.com\nwrote:\n\nI'll put together a PoC. Any objections to using go's standard net/rpc\npackage over a unix socket?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1458#issuecomment-247165731,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKBhAcEs-eYa64IPjhq-cB36Dnwjtks5qqGtMgaJpZM4J8O7l\n.\n. Ahh ok! Just as a thought experiment, would it be easier from a maintenance\nstandpoint if we were to handle polling and the issues with polling in a\nlibrary and not add a new API?\n\nOn Wed, Sep 14, 2016 at 3:56 PM, Tim St. Clair notifications@github.com\nwrote:\n\nI think using the existing rest API would require polling. I was\nenvisioning a push model where cAdvisor connects to a \"server\" and sends an\n\"AddStats\" RPC every time AddStats would normally be called on the driver.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1458#issuecomment-247182114,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKAsQVC_n_AJHgnB_doO0ErjVzxOzks5qqHusgaJpZM4J8O7l\n.\n. proto is definitely better than json. Do we need an RPC interface though?\nIf we go down the rpc + protobuf route you might want to check out\nhttps://capnproto.org/\n\nOn Wed, Sep 14, 2016 at 4:16 PM, Tim St. Clair notifications@github.com\nwrote:\n\nSorry, I sent that too soon :)\nYes, I share your concerns about the maintenance problem, specifically\naround version skew. However, I'm not sure that using the REST endpoints\nmakes it that much easier, since it would still be a problem to deprecate\nv1. What if instead we include a Version rpc call that returns the API\nversion supported? That way the client can handle different versions, and\nwe can eventually deprecate the older ones.\nI think the advantages of implementing it this way are:\n- Stats are pushed immediately, no lag from polling\n- Scraping the REST endpoint would require significantly more CPU &\n  memory, since it needs to marshal & unmarshal the JSON for all containers\n  at each polling interval, and check which stats are new.\n- The RPC implementation is actually much simpler, for the above\n  reasons.\nThe initial server RPCs would be:\ntype Server interface {\n  AddStats(info v1.ContainerInfo, _ struct{}) error\n  Version(_ struct{}, version *string) error\n}\nWDYT?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1458#issuecomment-247185882,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKIdzULUJPh463hbYZGJuLofhOC-Zks5qqIBAgaJpZM4J8O7l\n.\n. IIRC, this was fixed via a libcontainer update. Libcontainer was mounting\ncgroups with non-identify mappings inside the container. I'm surprised this\nis showing up in k8s v1.4.0\n\nOn Wed, Sep 14, 2016 at 3:03 PM, Mrunal Patel notifications@github.com\nwrote:\n\n@smarterclayton https://github.com/smarterclayton Yeah, the system\norder is different from what we see in the container when we bind mount\n/sys over /sys. IMO, cadvisor should check which path exists and set the\ninotify watch accordingly.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1461#issuecomment-247170313,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKC_FHeWF4mZKCThrV401yj9dJmcyks5qqG88gaJpZM4J9SiV\n.\n. cAdvisor uses libcontainer to identify the mount points. IIRC, libcontainer\nused the first set of mount paths it identifies. I don't have time to dig\ninto it now, but in essence, libcontainer should be able to pick the right\nset of mount points.\n\nOn Wed, Sep 14, 2016 at 3:22 PM, Mrunal Patel notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh This is what libcontainer mounts inside\na container (for e.g.):\ndrwxr-xr-x. 2 root root  0 Sep 14 22:18 blkio\nlrwxrwxrwx. 1 root root 11 Sep 14 22:18 cpu -> cpu,cpuacct\ndrwxr-xr-x. 2 root root  0 Sep 14 22:18 cpu,cpuacct\nlrwxrwxrwx. 1 root root 11 Sep 14 22:18 cpuacct -> cpu,cpuacct\ndrwxr-xr-x. 2 root root  0 Sep 14 22:18 cpuset\ndrwxr-xr-x. 2 root root  0 Sep 14 22:18 devices\ndrwxr-xr-x. 2 root root  0 Sep 14 22:18 freezer\ndrwxr-xr-x. 2 root root  0 Sep 14 22:18 hugetlb\ndrwxr-xr-x. 2 root root  0 Sep 14 22:18 memory\nlrwxrwxrwx. 1 root root 16 Sep 14 22:18 net_cls -> net_cls,net_prio\ndrwxr-xr-x. 2 root root  0 Sep 14 22:18 net_cls,net_prio\nlrwxrwxrwx. 1 root root 16 Sep 14 22:18 net_prio -> net_cls,net_prio\ndrwxr-xr-x. 2 root root  0 Sep 14 22:18 perf_event\ndrwxr-xr-x. 2 root root  0 Sep 14 22:18 pids\ndrwxr-xr-x. 2 root root  0 Sep 14 22:18 systemd\nNow, when we bind mount /sys:/sys we are overwriting that with the order\non the system which happens to be the opposite on RHEL and hence the issue.\nOne thing that cadvisor could do is not use combined paths as symlinks are\nprovided for each subsystem to a combined path.\nNot sure how we can fix this in libcontainer.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1461#issuecomment-247175270,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKCPlMtjvkaxzSG8dzuA-xbn6cfsrks5qqHOZgaJpZM4J9SiV\n.\n. It expects all the cgroups mounts from the host to be mounted. It uses\nlibcontainer to detect the cgroup mounts.\n\nOn Wed, Sep 14, 2016 at 4:10 PM, Mrunal Patel notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh Yes, libcontainer looks at the first\nmount points that it finds. How is cadvisor expected to be launched in a\ncontainer? What host paths are expected to be mounted inside? This will\nhelp identify how to fix this.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1461#issuecomment-247184681,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKHHJ0rWbzBkuwzXDHd5t49NN_Kxuks5qqH7RgaJpZM4J9SiV\n.\n. sgtm!\n\nOn Wed, Sep 14, 2016 at 4:48 PM, Mrunal Patel notifications@github.com\nwrote:\n\n@vishh https://github.com/vishh I went through the code and I think it\nwill be easier if we discuss this over a call. What you really want is the\nlater mount point from mountinfo but that isn't always the correct\nassumption that could be made in libcontainer especially when nesting\ncontainers (like docker-in-docker).\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1461#issuecomment-247191515,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKEZhk1QYdyQfKETWW9Oc35zvIbXMks5qqIfTgaJpZM4J9SiV\n.\n. lgtm\n. cc @derekwaynecarr @ncdc\n\nOn Mon, Feb 13, 2017 at 6:33 AM, bmouthrob notifications@github.com wrote:\n\nHi,\nI don't think the above issue is fixed, I have tried both cadvisor and\ncadvisor-canary and am getting similar symptoms on an Amazon Linux\ninstance...\nI0208 22:08:06.528378 1 storagedriver.go:50] Caching stats in memory for\n2m0s\nI0208 22:08:06.528672 1 manager.go:140] cAdvisor running in container:\n\"/docker/aeca51a748adbe6c12dc08550ed15024194544b4235fcb66144bee3dbac08334\"\nW0208 22:08:06.542050 1 manager.go:148] unable to connect to Rkt api\nservice: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441:\ngetsockopt: connection refused\nI0208 22:08:06.566575 1 fs.go:116] Filesystem partitions:\nmap[/dev/mapper/docker-202:1-395786-98d5d321dc375464f3fb4cdc61dfc0\n9e97c813ce39fe7bde32c1cdef15a7f7b7:{mountpoint:/ major:253 minor:13\nfsType:xfs blockSize:0} /dev/xvda1:{mountpoint:/var/lib/docker/devicemapper\nmajor:202 minor:1 fsType:ext4 blockSize:0}]\nI0208 22:08:06.570021 1 info.go:47] Couldn't collect info from any of the\nfiles in \"/etc/machine-id,/var/lib/dbus/machine-id\"\nI0208 22:08:06.570107 1 manager.go:195] Machine: {NumCores:2\nCpuFrequency:3000000 MemoryCapacity:16037875712 MachineID:\nSystemUUID:EC2FE94F-E16E-29BC-DC54-897C42293EDC BootID:e22f450d-be25-4472-a042-b290c9e76394\nFilesystems:[{Device:/dev/mapper/docker-202:1-395786-\n98d5d321dc375464f3fb4cdc61dfc09e97c813ce39fe7bde32c1cdef15a7f7b7\nCapacity:10725883904 Type:vfs Inodes:10484736 HasInodes:true}\n{Device:/dev/xvda1 Capacity:8318783488 Type:vfs Inodes:524288\nHasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0\nSize:107374182400 Scheduler:none} 253:3:{Name:dm-3 Major:253 Minor:3\nSize:10737418240 Scheduler:none} 253:7:{Name:dm-7 Major:253 Minor:7\nSize:10737418240 Scheduler:none} 253:8:{Name:dm-8 Major:253 Minor:8\nSize:10737418240 Scheduler:none} 253:9:{Name:dm-9 Major:253 Minor:9\nSize:10737418240 Scheduler:none} 202:0:{Name:xvda Major:202 Minor:0\nSize:8589934592 Scheduler:noop} 253:10:{Name:dm-10 Major:253 Minor:10\nSize:10737418240 Scheduler:none} 253:6:{Name:dm-6 Major:253 Minor:6\nSize:10737418240 Scheduler:none} 253:12:{Name:dm-12 Major:253 Minor:12\nSize:10737418240 Scheduler:none} 253:4:{Name:dm-4 Major:253 Minor:4\nSize:10737418240 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1\nSize:10737418240 Scheduler:none} 253:11:{Name:dm-11 Major:253 Minor:11\nSize:10737418240 Scheduler:none} 253:13:{Name:dm-13 Major:253 Minor:13\nSize:10737418240 Scheduler:none} 253:2:{Name:dm-2 Major:253 Minor:2\nSize:10737418240 Scheduler:none} 253:5:{Name:dm-5 Major:253 Minor:5\nSize:10737418240 Scheduler:none}] NetworkDevices:[{Name:eth0\nMacAddress:12:18:27:50:94:b8 Speed:0 Mtu:9001}] Topology:[{Id:0\nMemory:16037875712 Cores:[{Id:0 Threads:[0 1] Caches:[{Size:32768 Type:Data\nLevel:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified\nLevel:2}]}] Caches:[{Size:47185920 Type:Unified Level:3}]}]\nCloudProvider:AWS InstanceType:r4.large InstanceID:i-0176c03a9ed8c4cb3}\nI0208 22:08:06.570741 1 manager.go:201] Version:\n{KernelVersion:4.4.41-36.55.amzn1.x86_64 ContainerOsVersion:Alpine Linux\nv3.4 DockerVersion:1.12.6 CadvisorVersion:v0.24.1 CadvisorRevision:ae6934c}\nE0208 22:08:06.580310 1 factory.go:291] devicemapper filesystem stats will\nnot be reported: unable to find thin_ls binary\nI0208 22:08:06.580328 1 factory.go:295] Registering Docker factory\nW0208 22:08:06.580342 1 manager.go:244] Registration of the rkt container\nfactory failed: unable to communicate with Rkt api service: rkt: cannot tcp\nDial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused\nI0208 22:08:06.580349 1 factory.go:54] Registering systemd factory\nI0208 22:08:06.580781 1 factory.go:86] Registering Raw factory\nI0208 22:08:06.581134 1 manager.go:1082] Started watching for new ooms in\nmanager\nW0208 22:08:06.581430 1 manager.go:272] Could not configure a source for\nOOM detection, disabling OOM events: unable to find any kernel log file\navailable from our set: [/var/log/kern.log /var/log/messages\n/var/log/syslog]\nI0208 22:08:06.581769 1 manager.go:285] Starting recovery of all containers\nI0208 22:08:06.581837 1 manager.go:290] Recovery completed\nF0208 22:08:06.581865 1 cadvisor.go:151] Failed to start container\nmanager: inotify_add_watch /var/lib/docker/devicemapper/mnt/\n98d5d321dc375464f3fb4cdc61dfc09e97c813ce39fe7bde32c1cdef15a7f7b7/rootfs/sys/fs/cgroup/cpu:\nno such file or directory\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1476#issuecomment-279408294, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKKyH2mUtoPLgR0l5bZG4kFXmDrTGks5rcGnAgaJpZM4KEPoZ\n.\n. @tstclair we need to add a couple of tests for this feature. I don't think\nwe test on xenial yet.\n\nOn Wed, Sep 28, 2016 at 7:44 AM, pkrolikowski notifications@github.com\nwrote:\n\nHello,\nI'm trying to use cAdvisor to catch OOM events but I failed (probably I'm\ndoing something wrong).\nHere's my setup:\nlsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 16.04 LTS\nRelease:    16.04\nCodename:   xenial\ndocker version\nClient:\n Version:      1.12.0\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   8eab29e\n Built:        Thu Jul 28 22:11:10 2016\n OS/Arch:      linux/amd64\nServer:\n Version:      1.12.0\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   8eab29e\n Built:        Thu Jul 28 22:11:10 2016\n OS/Arch:      linux/amd64\nAnd I'm running cAdvisor as follows:\ndocker run -it --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro --volume=/var/log:/var/log:ro --publish=8888:8080 google/cadvisor:latest -disable_metrics disk -logtostderr\nI0928 14:02:52.419328       1 storagedriver.go:50] Caching stats in memory for 2m0s\nI0928 14:02:52.421050       1 manager.go:140] cAdvisor running in container: \"/docker/8220f6518546f643bbaf2e22d6ce1438d8fcd85c11e556504c5047b196fd040c\"\nW0928 14:02:52.478840       1 manager.go:148] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused\nI0928 14:02:52.529191       1 fs.go:116] Filesystem partitions: map[/dev/sda1:{mountpoint:/var/lib/docker/overlay major:8 minor:1 fsType:ext4 blockSize:0}]\nI0928 14:02:52.537331       1 manager.go:195] Machine: {NumCores:2 CpuFrequency:2599998 MemoryCapacity:13656510464 MachineID:af03dcd1dca6264136598dbff549c369 SystemUUID:AF03DCD1-DCA6-2641-3659-8DBFF549C369 BootID:0861dcdb-cfe0-43a2-bfee-1705cda0227d Filesystems:[{Device:overlay Capacity:31158935552 Type:vfs Inodes:3840000 HasInodes:true} {Device:/dev/sda1 Capacity:31158935552 Type:vfs Inodes:3840000 HasInodes:true}] DiskMap:map[8:0:{Name:sda Major:8 Minor:0 Size:32212254720 Scheduler:noop}] NetworkDevices:[{Name:ens4 MacAddress:42:01:0a:f0:00:39 Speed:-1 Mtu:1460}] Topology:[{Id:0 Memory:13656510464 Cores:[{Id:0 Threads:[0 1] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:20971520 Type:Unified Level:3}]}] CloudProvider:GCE InstanceType:n1-highmem-2 InstanceID:8555047159068902293}\nI0928 14:02:52.545440       1 manager.go:201] Version: {KernelVersion:4.4.0-34-generic ContainerOsVersion:Alpine Linux v3.4 DockerVersion:1.12.0 CadvisorVersion:v0.24.0 CadvisorRevision:0cdf491}\nI0928 14:02:52.598527       1 factory.go:295] Registering Docker factory\nW0928 14:02:52.598608       1 manager.go:244] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused\nI0928 14:02:52.598631       1 factory.go:54] Registering systemd factory\nI0928 14:02:52.598971       1 factory.go:86] Registering Raw factory\nI0928 14:02:52.599374       1 manager.go:1082] Started watching for new ooms in manager\nI0928 14:02:52.599635       1 oomparser.go:200] OOM parser using kernel log file: \"/var/log/kern.log\"\nI0928 14:02:52.601033       1 manager.go:285] Starting recovery of all containers\nI0928 14:02:53.262367       1 manager.go:290] Recovery completed\nI0928 14:02:53.778738       1 cadvisor.go:157] Starting cAdvisor version: v0.24.0-0cdf491 on port 8080\nThe problem is I can't find any OOM events in cAdvisor. At the same time\nI'm using mtail to keep track of /var/log/kern.log file (OOM events are\nthere).\nI tried to use API, without any result (/api/v1.3/events?oom_events=\ntrue&oom_kill_events=true).\nI would appreciate any help :)\nPK\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1484, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKACShHFSejFxg3SorWEUwszfIhT0ks5qun08gaJpZM4KI6Z-\n.\n. Overall this change LGTM\n. @timstclair SGTM. +1 to exposing numa info. NUMA support in k8s should be dealt with in a k8s proposal though.\n. lgtm. @euank I can help review this. is this ready for a review?. One more future optimization to consider is that of using inotify to update\nusage instead of having to periodically walk the filesystem. inotify has\nbeen a bit unreliable, but its worth a try since the resource cost would\nbecome much smaller and we can get more accurate disk stats.\n\nOn Tue, Mar 14, 2017 at 12:10 PM, David Ashpole notifications@github.com\nwrote:\n\n@euank https://github.com/euank Ill try and get some numbers on \"du\"\nlatency. Sounds like that may be slower than find, since it needs fstat. If\nwe can collect both inodes and disk usage in comparable time to du, then\nthis obviously would be a great thing to add.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1576#issuecomment-286528971, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKAPdkn6x8IUvN9uvb6iE_Ai8aisIks5rluYigaJpZM4Lk1-s\n.\n. Ideally, we would run thin_ls in a container :) That is very difficult in the case of k8s. \nWe might want to define a new RunCommand(cmd, timeout, cpu, memory) interface where we setup sub-containers (cgroups) to run commands. We can kill commands that do not complete by timeout. \n\nWe can such an interface for many binary execs we do across cadvisor and kubelet.. lgtm. sgtm. It would be useful if we can point to an example setup for nginx\nproxy.\nOn Fri, Mar 17, 2017 at 2:38 PM, Tim St. Clair notifications@github.com\nwrote:\n\ncAdvisors authentication is not implemented correctly, and in it's current\nstate is worse than no auth (can give a false sense of security). The\nobvious problem is that only some of the endpoints are actually\nauthenticated, but the same information can be accessed from an\nunauthenticated endpoint. There are also issues with error handling, and\npossible non-enforcement issues (#1554\nhttps://github.com/google/cadvisor/issues/1554).\nI think we should just remove auth entirely for now. Users who require\nauth can set it up using an nginx proxy.\n/cc @vishh https://github.com/vishh @dashpole\nhttps://github.com/dashpole\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1621, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKDkqKH00R2UxFXfIg_PPOM0oWVxjks5rmv1QgaJpZM4MhITQ\n.\n. Fixed github ACLs. David you can review and merge PRs henceforth.\n\nOn Fri, Aug 25, 2017 at 3:41 PM, David Ashpole notifications@github.com\nwrote:\n\nAssigned #1732 https://github.com/google/cadvisor/pull/1732 to @vishh\nhttps://github.com/vishh.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1732#event-1222246174, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AGvIKOBw0Gapn6erSLA9yTRu2EYvAjUrks5sb02HgaJpZM4PDCGh\n.\n. @dashpole this PR LGTM. If you are happy with it, let's get it merged. . LGTM. LGTM again. @dashpole WDYT?. /lgtm. If this change is primarily meant to make cAdvisor expose system topology for introspection purposes, then I can go ahead and review this PR.\nIf this PR is meant to tackle HW topology in k8s, then I'd suggest holding off this PR until we get to finalize the overall design on k8s side. \nFor example, we currently expect PCI device topology to be exposed relative to CPU sockets by device plugins. So I'm not sure for cAdvisor data will help.\nExposing adequate CPU and memory topology from cAdvisor will be useful though if the current information isn't adequate. . I see you are adding a list of PCI device IDs per NUMA node. Is that adequate or should each device hold more metadata than just it's PCI ID?. This casting is not possible unless info.NetworkStats is an alias of libcontainer.NetworkStats correct?\nWe can do a deep copy to be on the safer side. How about dealing with this in a separate PR?\n. Done.\n. Done.\n. our current driver depends heavily on libcontainer. So until we have docker export stats, cadvisor's lxc support is broken.\n. I reverted the change.\n. Retained it.\n. I am not able to detect the exec driver used for a container as of now. We can deal with that in a separate PR. Added a TODO for now\n. Modified CanHandle() to check if a docker container is active.\n. This seems to be the pattern in the rest of the code. Do you want me revert back?\n. Removed NotActive.\n. Done.\n. Done.\n. Thats incorrect. We want to ignore /docker/* containers which are not active.\n. Done.\n. Done.\n. I plan to add a {error_type, error_string} type for errors in cadvisor. Once I add that it will be easy to modify this error. I have placed a TODO for now.\n. Good catch :)\n. Done.\n. Done. \n. Done. I don't know the actual purpose of this TODO. But I am not able to think of any thing else we will want to do here.\n. Look below. fileNotFound is treated as a special error. I could do string matching, but I don't like that approach. This error is being consumed by its caller.\n. Done.\n. Why are you changing the alias? Iisn't that independent of the container name?\n. That is intentional. As discussed offline, the graph for errors was not\ncorrect. I will add that once I figure out the issue with errors.\n\nOn Wed, Jul 23, 2014 at 1:25 PM, Victor Marmol notifications@github.com\nwrote:\n\nIn pages/containers_html.go:\n\n@@ -149,6 +149,14 @@ const containersHtmlTemplate = `\n         \n\n     {{end}}\n-    \n-      \n-        Network\n-      \n-      \n-        \n\nMissing the network errors chart?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/111/files#r15313235.\n. I fixed the errors issue. It works now.\n. Done\n. If we are querying every minute, why do we need 1 minute worth of stats?\n. Do we need the last 60 seconds or would just the last 10 seconds be enough?\n. Got it :) \n. Done\n. doesn't golang recommend lower caps for errors?\n. Same as above.\n. Done.\n. Done.\n. Done.\n. It does default to localhost:8086. I will move that to optional\n\nOn Tue, Aug 12, 2014 at 3:08 PM, Rohit Jnagal notifications@github.com\nwrote:\n\nIn README.md:\n\n@@ -29,12 +29,26 @@ If you want to build your own cAdvisor Docker image, take a look at the Dockerfi\ncAdvisor now also supports InfluxDB to store stats. To use InfluxDB, you need to pass some additional flags to cAdvisor:\n-- -storage_driver: storage driver to use. Options are: memory (default) and influxdb. Use influxdb.\n-- -storage_driver_host: The host:port of the database.\n-- -storage_driver_name: database name.\n-- -storage_driver_user: database username.\n-- -storage_driver_password: database password.\n-- -storage_driver_secure: use secure connection with database. False by default\n+Required\n+```\n- # storage driver to use. Options are: memory (default) and influxdb\n- -storage_driver=influxdb\n- # The ip:port of the database\n- -storage_driver_host=ip:port\n\nDoesn't it default to localhost:8083 or smth? I never specify this for\nlocal testing.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/176/files#r16146257.\n. Done\n. Done\n. Done\n. s/neq/new\n. Done\n. Since we buffer data in influxdb driver based on time duration, we have to hold stats in the cache for that duration. The number of samples can vary based on the housekeeping duration. To the end user, it is easier to think it terms of duration rather than number of samples * housekeeping duration. \nThe cache is only used by the UI and so holding just 60 seconds of data might be enough even if the buffering duration for influxdb is higher.\nBut if a user were to hit our REST API directly, then caching all uncommitted stats in memory will help avoid complexity in the storage drivers.\n. Valid suggestion. func() {}() did not occur to me when I was writing that piece of code :)\nDone.\n. If the buffer duration were to be set to 30s, then we will end up querying the db for every second.  We want to cache at least what the UI wants. Once we improve the client side UI code, we can bring down the default or may be not have a default at all.\nIt might help to make the client and the server agree upon a value using a global constant. Remember that the cache exists mainly for the UI as of now.\n. Done.\n. Done\n. nit: Should we call it 'beta' instead of experimental?\n. nit: s/machine/hostname? \n. Would it make sense to share these column names across different storage backends? May be just a TODO for now.\n. +1\n. Din't we decide to perform these calculations on the DB and get rid of these from cAdvisor?\n. Do we have to batch writes for big query as well?\n. Since the UI queries every second, we will end up invoking Flush() every second which defeats the purpose of buffering. We should we get rid of percentiles from basic ContainerInfo before making this change.\n. I would prefer this being in terms of time duration that maximum number of stats. If we had a lot of containers to track, we will end up writing often to the DB. I think we should re-use the code in influxdb driver here.\n. Take a look at manager/manager.go:141. \n. Yes. That will alleviate the problem for now. We should put a note on our main page stating that querying percentiles often with a non-memory storage backend might result in frequent writes to the backend.\n. How about not releasing the lock here and getting rid of locking from Flush() ?\n. This should be equivalent to the 'BufferDuration' that is currently used by the influxDB driver.\n. Do we ever need to invoke Flush() from outside of the storage driver? We decided that buffering will be something internal to storage drivers.\n. nit: rename this to cachePeriod or bufferDuration.\n. Ideally we just need to know the precision and we can derive the number of stats: bufferDuration/precision. Precision  will be manager.HousekeepingTick. I am OK with a TODO for now.\n. What value does this cache add? We are already caching data in 'dirtyStats'\n. Why do we need this cache? We are already storing recent stats in 'dirtyStats'. We are duplicating memory here.\n. Do we ever need to invoke Flush() from outside of the storage driver? We decided that buffering will be something internal to storage drivers.\n. Ack.\n. Ack.\n. I am not too happy with memory duplication. We are doing this because the current storage driver interface does not support all requirements. On the other hand, the additional memory usage might be negligible in reality. I wonder that @rjnagal and @vmarmol think about this.\n\nIf we care about memory, we can refactor the storage driver abstraction, or better move non-memory storage drivers out of cAdvisor completely.\n. This is possible even in the case of InfluxDB. So :+1: for changing the interface.\n. One idea that comes to my mind is to make the cache the default entity that the rest of cadvisor will interact with and the cache can optionally manage another storage backend to which it will flush to based on various policies.\n. Ok. Lets do that then. I feel bad for dragging this PR this far. But I think by making this change, we will simplify the code a lot and bring down the corner cases.\n. The memory storage driver can still take in stats for a single container. Only the external storage drivers like InfluxDB and BigQuery needs to handle batch writes. So can we split the API?\n. +1\n. @monnand: Are we facing issues any with 'time' resolution?\n. Dint't we decide not to read back from the DB?\n. Nice :)\n. I tried this once and influxdb threw some errors. FYI.\n. FWIU influxdb's default resolution is milliseconds. In any case, we do not plan to read back data from influxdb correct?\n. Why change it to a minute?\n. No. We have /tmp now.\n. No. glog.Fatal() does not bring down the entire process :(\n. Done.\n. Why not just embed CpuSpec?\n. Same here and for network.\n. I think embedding has its benefits. It will help with the storage drivers as well. \n. +1\n. nit: Merge this part of the sentence to the previous line :)\n. nit: s/cAdvosor/cAdvisor\n. s/manager support/management support. And the same description applies to manager/manager.go\n. What about SigTerm or SigInt?\n. nit: Shall we stick to go's logging style guide and start the logs with small caps? I am saying this in light of other refactoring PRs that are making the same change.\n. Not trying to nit picky, but these are logically errors. It would be better to stick to one style so that we can avoid nit picks in our PRs. I am fine with choosing the existing style.\n. I would prefer choosing one style for all logs. Less cognitive burden and consistency. \n. Do we also care about SIGINT?\n. Ok. Lets stick to that.\n. Ack.\n. Should we instead export \"docker-\" prefix from libcontainer?\n. Doesn't this mean we will loose network stats? If so, why not make this change only on systemd systems?\n. Ahh ok :)\n. I meant docker/libcontainer. If libcontainer is not the source of the prefix, then the current code is fine as is.\n. The problem with moving into ContainerStats is that we cannot detect usage of all raw containers. But this might work for docker containers, once we figure out how to perform the accounting.\n. By bringing in root, we make all the root namespace mounts visible inside the container. Running cadvisor in a docker container is convenient, but a hack. cAdvisor needs access to the host namespace. We can coe up with some other hack to avoid this, but why?\n. I believe it does. But the general format I have observed for dockerfiles is to create all the directories that are needed in the dockerfile. \n. As discussed offline, cadvisor will work even without this option, but it will not detect all the disks. I am adding a note to clarify the behavior.\n. Done\n. Done.\n. It is obtaining information from the 'vfs' layer and not the block device itself. So it should not be too expensive.\n. Done.\n. Done.\n. What is wrong with the indentation? Thats how the rest of the content is indented.\n. Done.\n. We don't push the Spec as of now. If we start doing that, then we can move capacity to the Spec table and just add usage here. I think we need to push the Spec somewhere. CPU and Memory limits should also be pushed. I will work on that sometime soon. For now, I prefer to leave it as-is.\n. Done\n. Done.\n. Its a bug. Fixed it.\n. Nope. We can run pprof once this patch goes in and add a longer housekeeping for this if we find it to be expensive. \n. Ok. Lets change the memory and cpu to logical limits in a separate PR.\n. Done.\n. nit: float64(usageCpuNs) / float64(time.Second)\n. nit: \"...cores, %s of memory\"\n. allHosts actually since the struct is anyways local to the package.\n. Another possibility is to make this a string in the unit test file. We can create a temporary file from the unit test instead of having a test-only file in the repo. \n. nit: Can you return a more detailed error message? These errors get logged elsewhere and it sometimes hard to debug.\n. Why is this a public method? Also can we give it a more descriptive name - getContainerHintsFromFile() or some such?\n. Can all the variables inside a package local struct start with lower caps?\n. Just curious, why is this field needed?\n. Victor just clarified that they need to be pubic for json parsing. \n. I think this can be replaced in favor of stats, err = libcontainer.GetStats(self.cgroup, &dockerlibcontainer.State{}). The API will do the right thing if network state is not available.\n. You also need to set 'spec.HasNetwork' to true to be consistent with the cAdvisor API.\n. \"ContainerHints\" file based interface to inject...\n. Will reflect.DeepEqual(a, b) suffice?\n. We should check if network stats is available for this container before pushing it to influxdb. May be check spec.HasNetwork is true?\n. We should check if network stats is available.\n. s/1.1/1.2\n. nit: remove 'also'\n. will \"/docker\" match all the docker containers?\n. Why hardcode to \"60\" ?\n. Ok :)\n. Can docker run containers somewhere other than \"/docker\"?\n. Is '//' indicating a namespace instead of filesystem path? \n. Should we have a helper to generate Namespaced container names just to avoid bugs?\n. One has to be careful when treating these names as paths. Are there any alternatives to overloading the container name?\n. Mention ContainerStats API update.\n. I think we should mention it.\nOn Mon, Nov 17, 2014 at 9:54 AM, Victor Marmol notifications@github.com\nwrote:\n\nIn CHANGELOG.md:\n\n@@ -1,5 +1,12 @@\n # Changelog\n+## 0.6.0 (2014-11-17)\n+- Adding /docker UI endpoint for Docker containers.\n+- Fixes around handling Docker containers.\n+- Performance enhancements.\n+- Embed all external dependencies.\n+- Misc bugfixes and cleanups.\n\nWhat update? The flattening? Since that doesn't affect the wire format,\nshould we mention it?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/310/files#r20451543.\n. Ahh this bug.. We talked about it, but somehow it sneaked in :)\n. Wow. Thats readability to the max :100: \n. Ack.\n. Why not return immediately if there is an error? \n. Hmm. That would complicate the API. If we intend to ignore network errors, why not just log it instead of propagating it?\n. s/insped/inspect?\n. Two arguments passed, but only is expected.\n. second argument is missing.\n. nit: Why not ret[i] = &e?\n. Why not use self.recentStats.FirstN(numStats)?\n. Is this import necessary? Isn't this test part of the memory package already? \n. Isn't it already false?\n. nit: s/\"table name\"/\"database table name\"\n. Would this mean that we will not expose new containers or remove dead containers for up to 5 seconds?\n. This is ugly. May be put a TODO to remove it a few months from now.\n. Can these be made private?\n. What if the cgroup mount point also had some other dirs or files? Won;t everything would be listed here?\n. Why not print the mount point for all the cgroups mounted on the system? I mean a user friendly version of \"/proc/mounts\" with just cgroup mounts.\n. That file can be lengthy at times and we usually just care about cgroup\nmounts right? Do you have any other use cases in mind?\n\nOn Mon, Dec 22, 2014 at 12:09 PM, Rohit Jnagal notifications@github.com\nwrote:\n\nIn validate/validate.go\nhttps://github.com/google/cadvisor/pull/387#discussion-diff-22187032:\n\n@@ -194,7 +194,19 @@ func validateCgroupMounts() (string, string) {\n        out += desc\n        return Unsupported, out\n    }\n-   mounts, err := ioutil.ReadDir(mnt)\n-   if err != nil {\n-       out := fmt.Sprintf(\"Could not read cgroup mount directory %s.\\n\", mnt)\n-       out += desc\n-       return Unsupported, out\n-   }\n-   mountNames := \"\\tCgroup mount directories: \"\n-   for _, mount := range mounts {\n-       mountNames += mount.Name() + \" \"\n\nGood point about /proc/mounts. I think we should just add the whole output\nof /proc/mounts at end of the validate output. We ask for it while\ndebugging quite a bit. WDYT?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/387/files#r22187032.\n. How about path.Dir(mnt)?\n. nit: V(1) maybe since its an error?\n. Isn't this an error? Why Info log?\n. ack\n. :)\n. Call it mac instead or macAddress?\n. This should probably go into \"root\" cgroup stats. I am curious as to why this is necessary since you can compute 'free' from 'capacity' which is here and 'usage' which is in \"root\" cgroup stats.\n. why use fmt.Sprintf here and below when they are just raw strings?\n. Cant you provide a '-h' option to the docker client to make it spawn a container on a remote docker server?\n. nit: s/lime/line :)\n. Curious: Why will the number of runnable threads be a float?\n. How about moving the comment as a 'description' as part of the json name?\n. Will uint64 be appropriate for all stats? What prevents us from providing more precision?\n. How about map[ResourceName]ResourceQuantity instead of explicit fields? Take a look at type quantity that is part of kubernetes. That might help avoiding having to infer scaling factors from comments or json description.\n. nit: Why not just Memory instead of MemoryUsage? Ditto for CpuUsage.\n. Having strong type requires us to also specify whether a field is valid.\nWith a map we don't have to do that.\n\nOn Mon, Feb 2, 2015 at 9:29 AM, Victor Marmol notifications@github.com\nwrote:\n\nIn info/container.go\nhttps://github.com/google/cadvisor/pull/481#discussion_r23940887:\n\n+type Percentiles struct {\n-   // Average over the collected sample.\n-   Mean uint64 json:\"mean\"\n-   // Max seen over the collected sample.\n-   Max uint64 json:\"max\"\n-   // 90th percentile over the collected sample.\n-   Ninety uint64 json:\"ninety\"\n  +}\n  +\n  +type Usage struct {\n-   // Indicates amount of data available [0-100].\n-   // If we have data for half a day, we'll still process DayUsage,\n-   // but set PercentComplete to 50.\n-   PercentComplete int32 json:\"percent_complete\"\n-   // Mean, Max, and 90p cpu rate value in milliCpus/seconds. Converted to milliCpus to avoid floats.\n-   CpuUsage Percentiles json:\"cpu_usage\"\n\nThe quantity stuff sounds pretty good, but I'm unsure about the generic\nmap. We've always preferred strong over weak typing and having the fields\nspelled out here is stronger and easier to read (than specifying what\nstring values are valid in the map)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/481/files#r23940887.\n. We need something like 'NaN'. If we use float we can do that.\n\nOn Mon, Feb 2, 2015 at 9:45 AM, Victor Marmol notifications@github.com\nwrote:\n\nIn info/container.go\nhttps://github.com/google/cadvisor/pull/481#discussion_r23942180:\n\n+type Percentiles struct {\n-   // Average over the collected sample.\n-   Mean uint64 json:\"mean\"\n-   // Max seen over the collected sample.\n-   Max uint64 json:\"max\"\n-   // 90th percentile over the collected sample.\n-   Ninety uint64 json:\"ninety\"\n  +}\n  +\n  +type Usage struct {\n-   // Indicates amount of data available [0-100].\n-   // If we have data for half a day, we'll still process DayUsage,\n-   // but set PercentComplete to 50.\n-   PercentComplete int32 json:\"percent_complete\"\n-   // Mean, Max, and 90p cpu rate value in milliCpus/seconds. Converted to milliCpus to avoid floats.\n-   CpuUsage Percentiles json:\"cpu_usage\"\n\nI buy that, we don't have an \"is present\" in JSON. +1\n(well theres the omitifempty but that doesn't work anywhere close so what\nwe'd like it to work like)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/481/files#r23942180.\n. We should make it explicit in this error message that we went ahead and pushed stats even though an error was encountered.\n. Do we foresee a scenario where we will have some stats but not all? \n. If one of the resources here is not available or is partially available, what will PercentComplete refer to? % of Cpu or % of Memory or aggregate?\n\nAlso how about renaming this to accuracy? \n. Wouldn't Percent then vary for every resource? How can a user then\ninterpret this value?\nOn Tue, Feb 3, 2015 at 5:00 PM, Rohit Jnagal notifications@github.com\nwrote:\n\nIn info/container.go\nhttps://github.com/google/cadvisor/pull/481#discussion_r24055220:\n\n+type Percentiles struct {\n-   // Indicates whether the stats are present or not.\n-   Present bool json:\"present\"\n-   // Average over the collected sample.\n-   Mean uint64 json:\"mean\"\n-   // Max seen over the collected sample.\n-   Max uint64 json:\"max\"\n-   // 90th percentile over the collected sample.\n-   Ninety uint64 json:\"ninety\"\n  +}\n  +\n  +type Usage struct {\n-   // Indicates amount of data available [0-100].\n-   // If we have data for half a day, we'll still process DayUsage,\n-   // but set PercentComplete to 50.\n-   PercentComplete int32 json:\"percent_complete\"\n\nResources available or not are tracked in Percentiles through Present.\nPercentComplete only tracks how many Usage samples we gathered irrespective\nof what type is available.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/481/files#r24055220.\n. My concern is the latter. When we say 50% data available, is it 50% for all\nstats, which is how I interpret the current API, or 50% of one of the\nstats? To make it unambiguous, we can move the PercentComplete to\n'Percentiles'.\n\nOn Wed, Feb 4, 2015 at 6:17 AM, Rohit Jnagal notifications@github.com\nwrote:\n\nIn info/container.go\nhttps://github.com/google/cadvisor/pull/481#discussion_r24085185:\n\n+type Percentiles struct {\n-   // Indicates whether the stats are present or not.\n-   Present bool json:\"present\"\n-   // Average over the collected sample.\n-   Mean uint64 json:\"mean\"\n-   // Max seen over the collected sample.\n-   Max uint64 json:\"max\"\n-   // 90th percentile over the collected sample.\n-   Ninety uint64 json:\"ninety\"\n  +}\n  +\n  +type Usage struct {\n-   // Indicates amount of data available [0-100].\n-   // If we have data for half a day, we'll still process DayUsage,\n-   // but set PercentComplete to 50.\n-   PercentComplete int32 json:\"percent_complete\"\n\nPercentComplete just indicates how many samples cadvisor has collected for\nthat time duration. The primary use for present is when the system is\nconfigured to say not have memory isolation or cpu load, for example.\nAre you concerned about cadvisor failing sometimes to collect a single\nresource data? We'll handle that internally in cadvisor either by ignoring\nthe sample of extrapolating. It should not bubble up to the clients.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/481/files#r24085185.\n. Ok. If we are restricting ourselves to just basic resources which cannot dynamically appear or disappear, this model should work.\nLGTM\n. Is CreationTime being set elsewhere to now?\n. It looks like spec.CreationTime is not being set if the condition is not satisfied. Is it being initialized elsewhere?\n. Can we make this generic? Instead of info.Usage can it be an interface?\n. +1. Assert and Require seem pretty useful.\n. Can this be uint(time.Millisecond)\n. nit: Can this this be uint(time.Second)\n. nit: secondsToNanoSeconds / milliSecondsToNanoSeconds\n. Does this have to be a public type?\n. Does this have to be a public type?\n. I was planning on using this in heapster :)\n\nWe can do this outside of this PR too :)\n. How about defininf consts for various parts of the api request to make the code more readable?\n. Why is this useful? This is cpu usage calculated every 60 seconds right? If we were to plot it for over a 60 second period I can see the usecase.\n. Instead, how about printing max and maybe 90%ile which might be more\nuseful. The reason I say is that this instantaneous value only looks at 1\nsecond period every 60 seconds and does not care about cadvisor's cpu usage\nduring the other 59 seconds in the same period.\nOn Mon, Feb 9, 2015 at 1:46 PM, Victor Marmol notifications@github.com\nwrote:\n\nIn manager/container.go\nhttps://github.com/google/cadvisor/pull/497#discussion_r24368424:\n\n\nusageInCores := float64(usageCpuNs) / float64(stats[1].Timestamp.Sub(stats[0].Timestamp).Nanoseconds())\ninstantUsageInCores := float64(stats[numSamples-1].Cpu.Usage.Total-stats[numSamples-2].Cpu.Usage.Total) / float64(stats[numSamples-1].Timestamp.Sub(stats[numSamples-2].Timestamp).Nanoseconds())\n\n\nThis prints the instantaneous usage and then we show the average over the\nlast 60s.\nIt is useful since it gives us an idea of the current usage (and how\nstable that is) as well as what it means in the \"medium\" (60s) term overall.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/497/files#r24368424.\n. I was referring to something like the following:\nconst (\napiVersion = iota + 1\napiRequestType\napiRequestArgs\n), instead of using numeric indexes.\nSorry if that was not clear in the previous comment.\n. Ahh my bad. I misread the code. The current implementation makes sense if it is printed every second.\n. Can we be specific here instead of more? I guess we support btrfs in addition to existing \"ext4\" support?\n. One other option is to run gcloud compute instances list <instance-name> --format=yaml which makes it easy to extract the IP. \n. why not switch requestType { case summaryApi: ... }\n. +1. I was thinking the same thing.\n\nOn Wed, Feb 18, 2015 at 10:33 AM, Victor Marmol notifications@github.com\nwrote:\n\nIn manager/manager.go\nhttps://github.com/google/cadvisor/pull/515#discussion_r24926758:\n\n@@ -364,6 +367,20 @@ func (self _manager) containerDataSliceToContainerInfoSlice(containers []_contai\n    return output, nil\n }\n+func (self manager) GetContainerDerivedStats(containerName string) (info.DerivedStats, error) {\n-   var ok bool\n-   var cont containerData\n-   func() {\n\nunrelated and not for this PR: we do this everywhere, we should get a\nhelper...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/515/files#r24926758.\n. nit: Why not report an error instead of info?\n. Should this really belong in /machine? \n. To me, /machine seems to imply the hardware config. Should we have a /version endpoint that provides all the software version info?\n. I like /attributes. But I would still vote for a /machine with static info and a separate /attributes which provide more dynamic info.\n. Should we include the field name here?\n. What about getting stats for all subcontainers under '/'?\n. I feel the docker remote API documentation is pretty useful. That is the basis for my comment.\n. Ahh got it. May be add a sentence about 'recursive' option for regular\ncgroup stats as well?\n\nOn Mon, Mar 23, 2015 at 12:38 PM, Rohit Jnagal notifications@github.com\nwrote:\n\nIn docs/api_v2.md\nhttps://github.com/google/cadvisor/pull/611#discussion_r26971391:\n\n+Note that the root container (/) contains usage for the entire machine. All Docker containers are listed under /docker. Also, type=name is not required in the examples above as name is the default type.\n+\n+### Docker Containers\n+\n+When container identifier is of type docker, the identifier is interpreted as docker id. For example:\n+\n+\n+| Docker container     | Resource Name                             |\n+|----------------------|-------------------------------------------|\n+| All docker containers| /api/v2.0/stats?type=docker&recursive=true|\n+| clever_colden        | /api/v2.0/stats/clever_colden?type=docker |\n+| 2c4dee605d22         | /api/v2.0/stats/2c4dee605d22?type=docker  |\n+\n+The Docker name can be either the UUID or the short name of the container. It returns the information of the specified container(s).\n+\n+Note that recursive is only valid when docker root is specified. It is used to get stats for all docker containers.\n\nIt still works with regular combination when container name is used. Only\nfor docker containers, we don't accept docker container id and\nrecursive=true.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/611/files#r26971391.\n. V(3) or V(4)?\n. How about a Makefile instead?\n. Ack. An advantage with make is that you can remember a few build rule names\nand not have to type the entire build, test and vet commands. But as you\nsay its not a big deal.\n\nOn Wed, May 20, 2015 at 11:01 AM, Victor Marmol notifications@github.com\nwrote:\n\nIn build/presubmit.sh\nhttps://github.com/google/cadvisor/pull/722#discussion_r30730446:\n\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+set -e\n+set -x\n+\n+./build/check_gofmt.sh .\n\nIf our flow becomes more complex (like Heapster's build), then I can't\ncomplain about moving to make :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/722/files#r30730446.\n. Could this be \"unknown\" instead?\n. nit: The line wrap isn't necessary. \n. Consider creating an interface for this method. It will be useful for unit testing.\n. nit: s/self/driver/g \n. What does namespace mean here?\n. How can a user initialize this driver? Are you planning to integrating this with the CLI in a subsequent PR?\n. This driver needs to be initialized. You need to plumb it through cadvisor.\n. Look here\n. why not milli or micro cores? \n. Also the unit is cores per second right?\n. Let's do it in v2. Kubelet can switch to using v2. We should consider doing\nthis for other instantaneous metrics as well.\n\nOn Fri, Aug 21, 2015 at 11:58 AM, Daniel Mart\u00ed notifications@github.com\nwrote:\n\nIn info/v2/container.go\nhttps://github.com/google/cadvisor/pull/861#discussion_r37666630:\n\n@@ -86,8 +86,11 @@ type ContainerStats struct {\n    // The time of this stat point.\n    Timestamp time.Time json:\"timestamp\"\n    // CPU statistics\n-   HasCpu bool        json:\"has_cpu\"\n-   Cpu    v1.CpuStats json:\"cpu,omitempty\"\n-   HasCpu bool json:\"has_cpu\"\n-   // In nanoseconds (aggregated)\n-   Cpu v1.CpuStats json:\"cpu,omitempty\"\n-   // In nanocores per second (instantaneous)\n-   CpuInst v1.CpuStats json:\"cpu_inst,omitempty\"\n\nI would have assumed that v1 was frozen by now. Otherwise, shouldn't\nchanges in v1 be in v2 too?\nWould moving kubelet to v2 be an issue?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/861/files#r37666630.\n. How about parsing the entire file once, and index it based on the Interface name?\n. nit: Sniff or Sniffer\n. Are you referring to elasticsearch finding its peers here?\n. Does this have to be Itoa() ?\n. This nested if loop isn't ideal. While you are here, can you refactor the code to not return id if any of the if conditions pass?\n. Did you intend to add this comment?\n. Can you add a pointer that describes these fields?\n. Can we move this to NetworkStats defined below?\n. Did you test this on a systemd distro?\n. Setting nice values is a good idea. \n. Done\n. Fixed\n. Done. I added an integration test. \n. Done\n. Done.\n. Done\n. Done\n. Done\n. I don't think this is a typo. The original statement was essentially conveying the fact that the test runs queries against a running cadvisor instance.\n. Just curious: Has docker every changed the value for this key in the past? \nPreviously the code was performing string.Contains and now we use the value as-is. Hence the concern. \n. nit: case aufsStorageDriver\n. nit: self.storageDriver == aufsStorageDriver\n. Question: Should this be relative to /rootfs?\n. Is dmsetup available on the docker base image that cAdvisor is using?\n. Why run this command again?\n. Nvm. I misread the code.\n. Should we cache the value of dataBlkSize from the previous call to dmsetup table <poolName> instead?\n. Thanks for the cleanup :exclamation: \n. Acknowledged. \n. We can add it. \nI wonder if switching to alpine linux base image is necessary here. There is an alpine linux package for this purpose\n. Did you get a chance to test this change?\n\nAFAIK our integration tests do not use the docker images as of now.\n. Wouldn't Sscanf return an error if the expected number of elements, as specified in the format, is not present?\n. Why not use mixedCaps? Is this change related to any golang conventions?\n. Any pointers to this convention?\nOn Mon, Nov 2, 2015 at 9:55 AM, Jimmi Dyson notifications@github.com\nwrote:\n\nIn api/handler.go\nhttps://github.com/google/cadvisor/pull/936#discussion_r43658177:\n\n@@ -30,7 +30,7 @@ import (\n\"github.com/golang/glog\"\n\"github.com/google/cadvisor/events\"\n-   httpMux \"github.com/google/cadvisor/http/mux\"\n\nYeah golang conventions\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/936/files#r43658177.\n. Thanks!\n. Yeah. Fixed it now. \n. Done\n. Done\n. I don't get your comment? Why is it a no-op?\n. nit: old doesn't really mean anything once we face this issue again. \nShall we rename this to v1Cgroup or something similar to that? Let's also add a few comments describing the versions of docker this version is compatible with.\n. What request? docker run? waitForContainer is supposed to ensure that the container is running right?\nAre you trying to give some time for cadvisor to gather metrics? \n. +1. I'd expect higher layers to be resilient to nil values.\n. Why delete this file? This Dockerfile makes it possible to build images using just docker build ...?\nOnce we get the jenkins automatic builder to work reliably using makefiles, I'd be comfortable with removing this Dockerfile.\n. How is this ordering guaranteed? If its through a special golang tool, that should be part of the build infra. \n. In any case, I fixed it for now. \n. nit: s/Defazult/default\n. What is the value when it is disabled? 0?\n. Why add this metric only to the raw driver?\nIs it possible to extract this metrics even for docker containers?\n. With this change will I be able to run docker build -t .. without requiring make and golang?\n. Ok. My assumption is that this canary Dockerfile is still functional. \n. Mentioned the original issue number. PTAL\n. I tried this test a few times and it works. The number of threads do change a bit before it stabilizes. Hence the delay.\n\nIrrespective of how many containers we start, the purpose of the test is to check that the initial and final number of goroutines match.\n. Not that I can think of? Any suggestions?\n. That's how it usually works. In this test though, I want to explicitly delete the containers. \n. Golang is yet to formalize a format for pprof data. I don't expect golang to change this format anytime soon. \nThis is a test. As long as the data is valid, do we really care about how we get the data?\n. Sure.\nOn Wed, Dec 9, 2015 at 1:35 PM, Yu-Ju Hong notifications@github.com wrote:\n\nIn integration/tests/api/docker_test.go\nhttps://github.com/google/cadvisor/pull/1008#discussion_r47155479:\n\n@@ -270,3 +270,22 @@ func TestDockerContainerNetworkStats(t testing.T) {\n    assert.NotEqual(stat.Network.RxBytes, stat.Network.TxBytes, \"Network tx and rx bytes should not be equal\")\n    assert.NotEqual(stat.Network.RxPackets, stat.Network.TxPackets, \"Network tx and rx packets should not be equal\")\n }\n+\n+// Tests goroutine leaks in docker handler.\n+func TestGoRoutineSanity(t testing.T) {\n-   fm := framework.New(t)\n  +\n-   assert := assert.New(t)\n-   initialGoRoutines, err := fm.Cadvisor().GetGoRoutines()\n-   assert.NoError(err)\n-   containerId1 := fm.Docker().RunPause()\n-   containerId2 := fm.Docker().RunPause()\n-   waitForContainer(containerId1, fm)\n-   waitForContainer(containerId2, fm)\n-   fm.Cleanup()\n-   // Adding the sleep here to let cAdvisor stabilize internally.\n-   time.Sleep(5 * time.Second)\n\nI don't have any good idea, but I'll throw something out. How about\nletting the test check periodically for a longer timeout until #goroutines\n<= initial#goroutines is true? This way we end the test quickly when\ncadvisor responds faster, but wait for a longer timeout if cadvisor is slow\nfor other reasons.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1008/files#r47155479.\n. AFAIK Cleanup() is idempotent. It will attempt to cleanup and fail, which\nis probably OK. I really don't think that this is an issue with this test\nthough.\n\nOn Wed, Dec 9, 2015 at 1:38 PM, Yu-Ju Hong notifications@github.com wrote:\n\nIn integration/tests/api/docker_test.go\nhttps://github.com/google/cadvisor/pull/1008#discussion_r47155912:\n\n@@ -270,3 +270,22 @@ func TestDockerContainerNetworkStats(t testing.T) {\n    assert.NotEqual(stat.Network.RxBytes, stat.Network.TxBytes, \"Network tx and rx bytes should not be equal\")\n    assert.NotEqual(stat.Network.RxPackets, stat.Network.TxPackets, \"Network tx and rx packets should not be equal\")\n }\n+\n+// Tests goroutine leaks in docker handler.\n+func TestGoRoutineSanity(t testing.T) {\n-   fm := framework.New(t)\n  +\n-   assert := assert.New(t)\n-   initialGoRoutines, err := fm.Cadvisor().GetGoRoutines()\n-   assert.NoError(err)\n-   containerId1 := fm.Docker().RunPause()\n-   containerId2 := fm.Docker().RunPause()\n-   waitForContainer(containerId1, fm)\n-   waitForContainer(containerId2, fm)\n-   fm.Cleanup()\n\nIs there any undesirable side-effect of calling the Cleanup() function\ntwice? If not, it might still be worth adding defer fm.Cleanup() to\nhandle misbehaved tests\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1008/files#r47155912.\n. nit: To avoid having two commands that are expected to be identical, can we increase the retry count to be 6 instead and run the command only inside the loop or cache the command in a separate var?\n. Is ZFS a storage driver in docker?\nI thought we can use ZFS instead of ext4 as the base fs.\n. nvm. Just noticed this page: https://docs.docker.com/engine/userguide/storagedriver/zfs-driver/\n. nit: Can we move this to fh.trackUsage?\n. Should we keep env as a separate field?\n. Sure. The data generated here can be exposed in many different ways - REST API, prometheus endpoint, storage sinks, etc. I'd prefer keeping env and labels separate at this layer. \nIf the prometheus endpoint chooses to combine labels and env, that's fine by me.\n. This feels more like labels. Why not expose this additional metadata as labels instead?\n. Infact why not add it to docker containers directly? Is this needed to provide backwards compat with old docker versions?\n. Ok. Why add a flag to cadvisor?\n. Why is this needed? Does it make sense to enable it by default?\n. Thanks for explaining @dqminh. The flag name doesn't seem to convey the fact that it is a filter. Can you fix the naming and description to make it clear that it is a filter?\n. We need to add this to API v2 as well.\n. How about --docker-env-metadata-whitelist: a comma-separated list of environment variable keys that needs to be collected for docker containers?\nAnd BTW will this default to all env variables? To me that feels like a good default.\n. yes. good catch. \n. Sure. I don't like this model either. It is very cumbersome. We should scope out the changes required for refactoring the existing API framework.\n. :) Other includes logs, container metadata, etc. I can add better documentation.\n. It is useful for detecting containers that thrash the disk. \n. No. DiskStats is not scoped at the cgroup. It is at the node level.\n. With uint64 a value of 0 will be treated as unset. With a pointer, we can use nil to express that the value is unset or unknown.\n. Sure. That will be possible here, but existing APIs will still expose them as uint64.\n. Can we use - instead? Also why drop the cumulative keyword?\n. What does instance mean here?\n. I wonder what will happen to values that are UINT64_MAX? For example, I think the default memory limit is UINT64_MAX.\n. Ohh ok :) +1 for device.\n. Ok. May be we can document the schema instead of including the cumulative keyword.\n. Is the limitation of using just int64 valid? Did you cross-verify against InfluxDB docs?\n. I do agree with you. Env variables sometimes contain auth information. Disabled by default SGTM.\nIf we were to support collecting all env variables, then we might have to support wildcards in the flag though.\n. nit: dockerEnvWhitelist would be a more readable name for this variable.\n. nit: Update the description. The existing description is valid for Labels field. Let's add a new one for Envs and mention that only whitelisted env vars are exposed, to avoid users filing issues that this field is always empty.\n. We already include some of the stats from memory.stat. For example WorkingSet is derived using data from memory.stat.\nCan we instead expose specific fields? Explicit fields will help with understanding of the API as well. \n. We will have to update v2 API as well.\n. Yeah. Let's limit to RSS for now. We get quite a few questions/issues around the data exposed by cAdvisor. Honestly, all the data that cAdvisor exposes is documented as part of kernel APIs. \n. My bad. :+1: \n. They are not really the same. But we can start with dropping DiskIO for now. \n. Fixed\n. I changed it to duration. \n. Should these conversion methods reside in the \"v2\" package since they are specific for v2?\n. Given that we are essentially breaking the existing API, which I think is ok, shall we get rid of some of the fields in Spec? I want to get rid of all the Has* fields in place of pointer values in Stats.\n. I'm fine with splitting it up, but I won't have time to work on it until\nThursday. So we can either merge this in if it's a blocker out wait for two\nmore days..\nOn Jan 11, 2016 7:52 PM, \"Tim St. Clair\" notifications@github.com wrote:\nIn info/v2/container.go\nhttps://github.com/google/cadvisor/pull/1035#discussion_r49413412:\n\n@@ -85,7 +85,7 @@ type ContainerSpec struct {\n    Image string json:\"image,omitempty\"\n }\n-type ContainerStats struct {\n+type DeprecatedContainerStats struct {\n\nWhile I like this change, I wonder if it would be better to split it into\na separate PR? Both because it seems a little unrelated to the disk stats,\nand we could make sure the disk changes are not blocked while dealing with\nmerge conflicts etc. from the API change. If you think it's easier to keep\nit in this PR, I won't stop you...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1035/files#r49413412.\n. #1035 doesn't change ContainerStats. This PR is changing the wire format for the /stats API right?\n. Ah got it.. Sorry I got confused. I really wish we separate the internal API from the wire format soon. \n. As an alternative, if we were to invoke handler.Cleanup() here, would that also fix the issue?\n. If we were to check if the container exists before invoking this Factory method, wouldn't that be better?\n. The problem IIUC is that of not invoking Cleanup() before letting go GC the container object. \nI intended to say that if alreadyExists is true, then we can invoke Cleanup() right away.\nBut I prefer not creating the object if it already exists and invoking Cleanup() iif we return error instead of starting housekeeping. \nThat would let us not add the new Start method.\n. We should cap the housekeeping at maxHousekeepingInternal. I'd recommend not changing this.\n. So the range of values will be [_housekeepingInterval, 3_HousekeepingInterval)?\nThat will mean for a 10 second housekeeping, it will be [10, 30), which is not acceptable.\nIsn't the goal to spread housekeeping evenly. So wouldn't a factor of 1 be enough?\n. Isn't it enough to introduce jitter here alone and ceil it at maxHousekeepingInterval?\n. Makes sense. Wrap cont.Start() inside of go thread then?\n\nOn Fri, Jan 15, 2016 at 1:30 PM, Timothy St. Clair <notifications@github.com\n\nwrote:\nIn manager/container.go\nhttps://github.com/google/cadvisor/pull/1057#discussion_r49907396:\n\n@@ -356,7 +372,7 @@ func (self *containerData) nextHousekeeping(lastHousekeeping time.Time) time.Tim\n        }\n    }\n-   return lastHousekeeping.Add(self.housekeepingInterval)\n-   return lastHousekeeping.Add(jitter(self.housekeepingInterval, JitterFactor))\n\nI think your initial starting point should also have jitter in the case of\ninitial burst of containers.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1057/files#r49907396.\n. Making it variable might invalidate assumptions made over cAdvisor APIs.\nFor example, heapster's internal logic might break if the housekeeping\njitter is arbitrarily large.\n\nOn Fri, Jan 15, 2016 at 2:54 PM, Timothy St. Clair <notifications@github.com\n\nwrote:\nIn manager/container.go\nhttps://github.com/google/cadvisor/pull/1057#discussion_r49915178:\n\n@@ -78,6 +83,17 @@ type containerData struct {\n    collectorManager collector.CollectorManager\n }\n+// jitter returns a time.Duration between duration and duration + maxFactor * duration,\n+// to allow clients to avoid converging on periodic behavior.  If maxFactor is 0.0, a\n+// suggested default value will be chosen.\n+func jitter(duration time.Duration, maxFactor float64) time.Duration {\n\nI think we should keep the function parameter at minimum, and if we want\nto remove the constant and simply default the argument that's fine. But at\nthis point we're splitting hairs.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1057/files#r49915178.\n. This change makes sense, but it is a breaking API change :( Its not clear if it will break any users though. So let's try rolling this out and then revert if required.\n. Kindly follow golang style guidelines. _ should be replaced with camelCase.\n. Why do we need an explicit cast here and above?\n. Thats weird. The kernel expects it to be a uint64. So thats an issue against runc I guess. \nLGTM then.\n. setting the log directory will result in multiple log files. I tried it locally and for some reason the INFO logs did not have all the log data. For the sake of expediency, I went with this approach. Happy to reconsider once this PR works.\n. Err. My bad. Fixing it.\n. Updated code. PTAL\n. Tests are supposed to fail if gofmt fails\n. Jenkins runs ./build/presubmit.sh\n. But the string format is that of a slice right? How can we unmarshal a slice into a map?\nIsn't the serialized format for map \"key\": \"value\"?\n. Yup done.\n. Makes sense. Done.\n. Yeah. \n. This is not yet deprecated. I was thinking of getting rid of this flag altogether. Attempting to be backwards compatible with that will only make the code messier.\n. Yeah it is :(\n. Good catch. Late night coding is clearly bad. Fixed it. Added a unit test as well.\n. Yeah. Thats possible. Updated. I followed the existing templates, which are weird in retrospect. \n. Fixed\n. Sure. Updated.\n. Sweet. Updated.\n. Is this method failing with devicemapper backend?\n. This is not always true since we can enable systemd support optionally. How do we handle that?\n. AFAIK fedora and rhel will have systemd support enabled by default.\n. sudo docker daemon --exec-opt=\"native.cgroupdriver=systemd\" -s\ndevicemapper will make the daemon use systemd. I wonder if we can use\ndocker info to figure this out.\n\nOn Fri, Feb 5, 2016 at 12:56 PM, Jimmi Dyson notifications@github.com\nwrote:\n\nIn container/docker/factory.go\nhttps://github.com/google/cadvisor/pull/1095#discussion_r52071462:\n\n@@ -273,6 +269,9 @@ func Register(factory info.MachineInfoFactory, fsInfo fs.FsInfo) error {\n                return fmt.Errorf(\"cAdvisor requires docker version %v or above but we have found version %v reported as \\\"%v\\\"\", expected_version, dockerVersion, version_string)\n            }\n        }\n-       if dockerVersion[0] == 1 && dockerVersion[1] >= 10 {\n\nI run fedora & this works for me. Docker has changed cgroup hierarchy in\n1.10 it seems.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1095/files#r52071462.\n. This logic is common to all storage drivers AFAIK.\n. Yeah. That is possible. Any specific reason other than it being a no-op as of now?\n. @jimmidyson: Do you have access to a zfs machine? I'm trying to add a zfs machine to the test fleet.\n. Makes sense. I went with hard coding the path for now and avoided the constant.\n. Done.\n. Great thought :clap: :+1: \n. The exit code is the reason for exec. I can catch the exit code and return that too, if exec has issues.\n. We should include a note on assigning issues to contributors outside of the Maintainers list.\nA simple comment might suffice.\n. Fixed.\n. 110 is the valid name.\n. Done\n. Valid comments. Updated. PTAL.\n. nit: s/scaped/scraped/g\n. Hmm... shouldn't this align with the existing storage duration (2m by default)? Is there a need for treating custom metrics differently? The APIs combine them anyways.\n. IIUC, this check will fail monitoring completely for a container. Is that the behavior that we want?\nWould it be better to just fail collection of user defined metrics?\n. Is this possible given that we fail to consume configs that define more metrics than what the limit permits?\n. Acknowledged. Minor nit: I prefer not including the keyword custom here. Let's just use application metrics to store per container.\n\nBTW how did you arrive at this limit? \n. nit: Can we pass in a new Config object instead of expanding this interface? I'd like to see all the existing bool knobs be a part of that.\n. Yeah. My comment applies to both the collectors. As of now, we will not collect any metrics for a container with a application metrics configuration that includes more than 100 metrics. \nMy gut feeling is that we might want to skip application metrics and continue to collect the rest of the metrics like cpu, memory, network, etc. WDYT?\n. Let's remove it.\n. It is kind of weird to make addDockerImagesLabel update labels for rkt as well. This logic might require a more thorough refactoring.\n. nit: The current code base is complex because flags are configured alongside regular code. Since you are adding new (and better) code, can we move the flags to a top level file and inject it into this package? \nSo instead of providing a static method in this package, the path will only be returned from a rkt factory object.\nI intend to perform a similar surgery on the docker factory.\nAlso, is it possible to auto-detect this path?\n. If there is a plan to switch to the service, then how will the existing\nmethod be modified to get the path from the service? We have the exact same\nissue with docker factory :(\nOn Wed, Feb 10, 2016 at 3:59 PM, Shaya Potter notifications@github.com\nwrote:\n\nIn container/rkt/factory.go\nhttps://github.com/google/cadvisor/pull/1105#discussion_r52547311:\n\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package rkt\n+\n+import (\n-   \"flag\"\n-   \"fmt\"\n  +\n-   \"github.com/google/cadvisor/container\"\n-   \"github.com/google/cadvisor/fs\"\n-   info \"github.com/google/cadvisor/info/v1\"\n  +)\n  +\n  +// Basepath to all container specific information that libcontainer stores.\n  +var rktPath = flag.String(\"rkt_path\", \"/var/lib/rkt\", \"Absolute path to the Rkt root directory\")\n\nI'm hoping to get this info from the rkt api-service at some point in\ntime, but right now that service is limited.\n\u2026 <#-1722709039_>\nOn Wed, Feb 10, 2016 at 3:53 PM, Vish Kannan notifications@github.com\nwrote: In container/rkt/factory.go <#1105 (comment)\nhttps://github.com/google/cadvisor/pull/1105#discussion_r52546728>: >\n+// See the License for the specific language governing permissions and >\n+// limitations under the License. > + > +package rkt > + > +import ( > +\n\"flag\" > + \"fmt\" > + > + \"github.com/google/cadvisor/container\" > + \"\ngithub.com/google/cadvisor/fs\" > + info \"\ngithub.com/google/cadvisor/info/v1\" > +) > + > +// Basepath to all\ncontainer specific information that libcontainer stores. > +var rktPath =\nflag.String(\"rkt_path\", \"/var/lib/rkt\", \"Absolute path to the Rkt root\ndirectory\") nit: The current code base is complex because flags are\nconfigured alongside regular code. Since you are adding new (and better)\ncode, can we move the flags to a top level file and inject it into this\npackage? So instead of providing a static method in this package, the path\nwill only be returned from a rkt factory object. I intend to perform a\nsimilar surgery on the docker factory. Also, is it possible to auto-detect\nthis path? \u2014 Reply to this email directly or view it on GitHub <\nhttps://github.com/google/cadvisor/pull/1105/files#r52546728>.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1105/files#r52547311.\n. Removing https://github.com/google/cadvisor/blob/master/manager/manager.go#L787 SGTM.\n. nit: Why is it called GceInstanceName?\n. done\n\nOn Tue, Feb 16, 2016 at 12:41 PM, Tim St. Clair notifications@github.com\nwrote:\n\nIn container/docker/fsHandler.go\nhttps://github.com/google/cadvisor/pull/1117#discussion_r53074788:\n\nif err != nil {\n    return err\n}\n-   extraDirUsage, err := fh.fsInfo.GetDirUsage(fh.extraDir)\n-   extraDirUsage, err := fh.fsInfo.GetDirUsage(fh.extraDir, duTimeout)\n\nI think we should log these errors at Error level (here:\nhttps://github.com/vishh/cadvisor/blob/timeout-du/container/docker/fsHandler.go#L99\n- I can't figure out how to comment there on GH)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1117/files#r53074788.\n. Fixing\n. Yup. Fixing it\n. bool is misleading since the zero value for an unknown key is also bool. I'm adding a complex type to make this cleaner.\n. Yeah. Good idea. Doing it\n. Deleting.\n. +1 updating \n. Yeah. Thats true.\n. input.\n. updating it to metric\n. Yeah. Deleting this logic\n. Yeah. Sorry. Removed it.\n. Ok. Updated.\n. Fixing\n. Done\n. UPdating.\n. This will result in all the overlay filesystems being created for containers from being included in the list of filesystems on the host. It will make the API pretty much unusable. \n. Ideally docker root fs should be detected using docker's APIs (info) and a statfs on that should be good enough. \nThis code just needs some love.\nThis logic here was originally meant for discovering filesystems of interest at a global level. \nLayered filesystems that are ephemeral are not of interest to most users in the global context.\n. I'd like this PR to use docker info + statfs for getting docker filesystem\nstats. That way these labels don't matter.\n\nOn Fri, Feb 26, 2016 at 2:44 PM, Shaya Potter notifications@github.com\nwrote:\n\nIn fs/fs.go\nhttps://github.com/google/cadvisor/pull/1125#discussion_r54312866:\n\n@@ -78,9 +78,10 @@ func NewFsInfo(context Context) (FsInfo, error) {\n    }\n    supportedFsType := map[string]bool{\n        // all ext systems are checked through prefix.\n-       \"btrfs\": true,\n-       \"xfs\":   true,\n-       \"zfs\":   true,\n-       \"btrfs\":   true,\n-       \"overlay\": true,\n\nso for now perhaps say if mount point is / and its overlay we dont ignore\nit?\n\u2026 <#398886326_>\nOn Fri, Feb 26, 2016 at 2:43 PM, Vish Kannan notifications@github.com\nwrote: In fs/fs.go <#1125 (comment)\nhttps://github.com/google/cadvisor/pull/1125#discussion_r54312699>: >\n@@ -78,9 +78,10 @@ func NewFsInfo(context Context) (FsInfo, error) { > } >\nsupportedFsType := map[string]bool{ > // all ext systems are checked\nthrough prefix. > - \"btrfs\": true, > - \"xfs\": true, > - \"zfs\": true, > +\n\"btrfs\": true, > + \"overlay\": true, Ideally docker root fs should be\ndetected using docker's APIs (info) and a statfs on that should be good\nenough. This code just needs some love. This logic here was originally\nmeant for discovering filesystems of interest at a global level. Layered\nfilesystems that are ephemeral are not of interest to most users in the\nglobal context. \u2014 Reply to this email directly or view it on GitHub <\nhttps://github.com/google/cadvisor/pull/1125/files#r54312699>.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1125/files#r54312866.\n. Is this necessary? otherStorageDir directory is common to all storage drivers and we do want to surface usage from that. \n. Yeah. rootfsStorageDir is storage driver dependent.\n\nOn Mon, Feb 29, 2016 at 2:58 PM, Seth Jennings notifications@github.com\nwrote:\n\nIn container/docker/handler.go\nhttps://github.com/google/cadvisor/pull/1143#discussion_r54494773:\n\n@@ -168,7 +168,8 @@ func newDockerContainerHandler(\n        ignoreMetrics:      ignoreMetrics,\n    }\n-   if !ignoreMetrics.Has(container.DiskUsageMetrics) {\n-   if !ignoreMetrics.Has(container.DiskUsageMetrics) && storageDriver != devicemapperStorageDriver {\n\nYou are saying that we should only not gather stats for the\nrootfsStorageDir?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1143/files#r54494773.\n. Logs usage is still supported with devicemapper. If you look below, only the root filesystem usage part is skipped for devicemapper.\n. I don't see why container hints will be common to all implementations..\n. This is not a test flag. It is being used AFAIK.\nAlso as part of this change, can we move this flag further up and pass it as an arg?\n. Got it. Ideally, we should push the flag up since you are introducing new shiny and clean code. But I'm fine with punting that to a separate PR.\n. nit: To avoid this loop, can we make getDockerImagePaths() return a map[string]struct{} ?\n. nit: Why is there a comment about not running on aws here?\n. I wonder why the e2e tests did not catch this.\n. Can we also reduce the default timeout?\n. I'd hope that the local metadata server can be resolved locally and that 2s\nshould be good enough.\n\nOn Wed, Mar 9, 2016 at 12:25 PM, Derek Carr notifications@github.com\nwrote:\n\nIn utils/cloudinfo/aws.go\nhttps://github.com/google/cadvisor/pull/1152#discussion_r55583061:\n\n@@ -23,7 +23,10 @@ import (\n )\nfunc onAWS() bool {\n-   client := ec2metadata.New(session.New(&aws.Config{}))\n-   // the default client behavior retried the operation 4 times with a 5s timeout per attempt.\n-   // if you were not on aws, you would block for 20s when invoking this operation.\n-   // we reduce retries to 0 to reduce the time this blocks when not on aws.\n-   client := ec2metadata.New(session.New(&aws.Config{MaxRetries: aws.Int(0)}))\n\n@justinsb https://github.com/justinsb - do you think its safe to\ndecrease the default timeout to 2s?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1152/files#r55583061.\n. Why is the rkt handler watching for new containers? I'd expect to have a global watcher which then hands off new containers to one of the handlers.\n. Is this configurable? Can users set it to an alternative path?\n. nit: Reduce this log to at-least V(4).\n. I don't think this logic is needed for rkt handler. This logic was added for lxc integration a while back to the raw handler.\n. I assume logs will be pod level usage?\n. Yup. Filesystem limits aren't available.\n. Is rkt planning to implement labels?\n. TODO(SJP): I guess...\n. This PR needs to include the metrics selection (opt-out) patch that I posted recently.\n. It is not needed here. IIUC, this logic is needed in the raw container handler mainly to watch all the sub-containers of /. \nThis PR exposes one more abstraction issue. The raw handler must be made specialized, or the watching of subcontainers must be moved out of the handler interface into a something else, which the raw handler can implement.\n. Good idea. Done..\n. nit: Can we keep this test around?\n. Yeah. Done.\n. @derekwaynecarr @timstclair: I had incorrectly set the pid to be that of cAdvisor. I updated it to the init pid. FYI!\n. Yeah. Makes sense. I updated the patch to do that. \n. :no_mouth: Fixed it.\n. Yeah. Sorry. Too much of context switching isn't helping with PRs..\n. Resiliency I guess. I can try changing that behavior in a new PR and see if any one complains..\n. Would we want to retry? What if the api service were to be restarting while cAdvisor is being started?\nNot for this PR - This is currently an issue with docker client as well, and it needs to be fixed there too.\n. nit: Why not use RktPath() instead?\n. nit: Give more context here. Skipped which container?\n. You don't have to include this logic here. It is used by some users to account for externally mounted volumes.\n. No. I think the abstractions are incorrect. Only the raw handler that handler the machine itself is supposed to implement this method.\n. Are logs stored in some well known path?\n. nit: Fix this and other comments/TODOs in this PR.\n. nit: Give more context. This line in the log file should make sense just by looking at it.\n. Recovery is needed. It has bitten us several times in the past.\n. I assume you will be removing this log then...\n. Yeah. That sounds good.\n. Yeah.\n. Should we check for ignore before checking if handler != nil ?\n. Ok. As an API consumer, the following seems easier to understand to me:\n\ngo\nif ignore {\n return nil\n}\nif handler == nil {\n  return err\n}\n. +1\n. Let's use the --disable_metrics flag to disable cpu load.\n. Apologies for overlooking that. The flag should ideally be deprecated. We\ndon't have a good policy around configuration as of now.\nOn Mon, Apr 4, 2016 at 2:54 PM, Thomas Orozco notifications@github.com\nwrote:\n\nIn manager/container.go\nhttps://github.com/google/cadvisor/pull/1188#discussion_r58455971:\n\n@@ -42,6 +42,7 @@ import (\n )\n// Housekeeping interval.\n+var enableLoadReader = flag.Bool(\"enable_load_reader\", false, \"Whether to enable cpu load reader\")\n\nI actually just moved that flag over from manager.go (i.e. it already\nexists in cAdvisor).\nWe could definitely use disable_metrics to disable CPU load monitoring,\nbut that'd break CLI compatibility with past versions.\nLet me know what you think,\n\u2014\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1188/files/002d8aae35acb23a69ddca2c1eedc29c75c36e2f#r58455971\n. this should not be necessary. I suspect that your goimports binary is causing trouble.\n. FYI: With docker v1.11, we should ignore the exec driver option all together. \n. I like it too for the record. Its just that we don't enforce it everywhere.\n\nOn Tue, Apr 5, 2016 at 1:26 PM, Tim St. Clair notifications@github.com\nwrote:\n\nIn container/docker/factory.go\nhttps://github.com/google/cadvisor/pull/1190#discussion_r58608544:\n\n@@ -27,7 +27,7 @@ import (\n    \"github.com/google/cadvisor/fs\"\n    info \"github.com/google/cadvisor/info/v1\"\n-   \"github.com/fsouza/go-dockerclient\"\n-   docker \"github.com/fsouza/go-dockerclient\"\n\nIt's not necessary, but I think it makes it easier to parse the code (and\nfigure out what docker refers to). We do it in some other places, but I\ncan revert it if you prefer.\n\u2014\nYou are receiving this because you were assigned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1190/files/d2bb061da0404e9bcc5330d7b3792011af75e6f3..d9c864324b03ef9d62620990d267df00039d77d3#r58608544\n. why remove this? \n. Shouldn't this be committed to go-dockerclient first?\n. nit: we should move the input args into a struct and group them appropriately. \n. I'd prefer not overloading FsInfo for exposing device mapper usage. Can we instead have the fsHandler include this logic?\n. Another option is to define a new device mapper usage object and inject that into the fs handler if required.\n. The docker driver should perform this initialization ideally. The rest of cadvisor should not depend on docker semantics (ideally)\n. For the future, we should see if we can get all the information we need via containerd APIs.\n. Why do we care about the state anymore? \n. Device mapper outside of docker lacks context. How do you envision users\nconsuming the device mapper data?\nIf cAdvisor can support desktop sandbox containers for example which uses\ndevice mapper outside of the docker context, then we can extend device\nmapper support to such a plugin.\n\nOn Mon, Apr 18, 2016 at 9:25 AM, Paul Morie notifications@github.com\nwrote:\n\nIn fs/fs.go\nhttps://github.com/google/cadvisor/pull/1204#discussion_r60089023:\n\n@@ -58,12 +60,15 @@ type RealFsInfo struct {\n    labels map[string]string\ndmsetup dmsetupClient\n+\n-   thinPoolWatcher *volume.ThinPoolWatcher\n\n@vishh https://github.com/vishh also -- is there a reason to keep\ndevicemapper as a docker-only notion?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1204/files/ae32fd2351c70119c092810e6d3265706f691b0a#r60089023\n. nit: Rename this to argUnixSocketPath\n. nit: What does this comment mean?\n. Why are you adding the fs logic here, instead of abstracting all of it in FsHandler?\nWhy can't fsHandler.GetRootUsage() and fsHandler.GetLogsUsage() work across storage drivers?\n. This might result in circular dependency, resulting in vendoring all of kube.\n. Will this result in a while(1) loop?\n. Is it safe to use os.Stderr here? Would it be polluted with data from other exec's or cadvisor itself?\n. Apologies If I wasn't clear when we discussed in person @pmorie.\n\nAlternative option 1 you proposed makes sense. Let's do that. \n. Lol \ud83d\ude09 \n. Ahh. That makes sense. Updating PR now.\n. Updating!\n. Given that kubelet consumes these methods, should the API types be kept in a separate package?\n. nit: Why push this to utils package? Why not a top level machine package? \n. @timstclair \n1253 will still be in effect. Adding a Dockerfile makes sense. I'm updating the PR now.\n. Given that we build entirely using vendor'ed code, I'm weary of mapping in\nthe entire $GOPATH/pkg directory. On top of that since we install godep,\nit will overwrite the existing godep in the host's $GOBIN path, which is\nundesirable.\nOn Tue, May 3, 2016 at 10:48 AM, Tim St. Clair notifications@github.com\nwrote:\n\nIn Makefile\nhttps://github.com/google/cadvisor/pull/1251#discussion_r61924373:\n\n@@ -32,9 +33,8 @@ vet:\n    @$(GO) vet $(pkgs)\nbuild:\n-   @echo \">> building binaries\"\n-   @./build/assets.sh\n-   @./build/build.sh\n-   @echo \">> building binaries using docker\"\n-   @docker run -i -v $(PWD):/go/src/github.com/google/cadvisor golang:$(goversion) /bin/sh -c \"cd /go/src/github.com/google/cadvisor && go get github.com/tools/godep && ./build/assets.sh && ./build/build.sh\"\n\nI don't think it is. I think you need to mount the $GOPATH/pkg directory\nfor it to work.\nWith your changes (irrelevant output omitted):\n[(c262eb7...)] cadvisor: time make build\n\n\nbuilding binaries using docker\n  cadvisor\n\n\nreal    0m14.201s\nuser    0m0.066s\nsys 0m0.032s\n[(c262eb7...)] cadvisor: time make build\n\n\nbuilding binaries using docker\n  cadvisor\n\n\nreal    0m14.279s\nuser    0m0.073s\nsys 0m0.027s\nWithout your changes:\n[install] cadvisor: time make build\n\n\nbuilding binaries\n  cadvisor\n\n\nreal    0m6.658s\nuser    0m13.909s\nsys 0m1.622s\n[install] cadvisor: time make build\n\n\nbuilding binaries\n  cadvisor\n\n\nreal    0m0.781s\nuser    0m0.343s\nsys 0m0.188s\nNotice how the second build is super fast once everything is cached.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1251/files/97ac208588777d15872e39ffe615760b853c91c4#r61924373\n. Did you try running make build?\n\nOn Tue, May 3, 2016 at 12:23 PM, Lucas K\u00e4ldstr\u00f6m notifications@github.com\nwrote:\n\nIn integration/runner/runner.go\nhttps://github.com/google/cadvisor/pull/1237#discussion_r61940055:\n\n@@ -204,7 +204,7 @@ func Run() error {\n// Build cAdvisor.\nglog.Infof(\"Building cAdvisor...\")\n-   err := RunCommand(\"godep\", \"go\", \"build\", \"github.com/google/cadvisor\")\n-   err := RunCommand(\"godep\", \"go\", \"build\", \"--ldflags='-extldflags \\\"-static\\\"'\", \"github.com/google/cadvisor\")\n\n@vishh https://github.com/vishh Do you have any clue what to write here\nto make it work?\nI tried a lot of different combinations, but the linker just didn't parse\nit.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1237/files/c7e2e113a9084142964556bd76b43ffb64b9e0e9#r61940055\n. Done\n. Question: Does mi.Filesystems include dm devices ?\n. Nit: I prefer returning an interface over a struct. That way the methods available are explicit. \n. Why are we running this method twice. Instead why not move all the label additions to this line?\n. Ideally, we should get rid of this flag and use docker CLI directly. That requires a retry from other consumers until docker is available though.\n. I meant that we should not fallback at all ideally. If docker is not even around, why do we even need docker's root directory. The current code structure where pkg fs depends on container/docker makes this difficult. \nWe cannot deprecate it until the clients of this package can deal with the fact that docker might not be available when cAdvisor is initialized.\n. Why not append a string form of the input instead?\n. Did you intend to log Refresh() calls that take too long?\n. Without @ doesn't make echo all these lines?\n. This fails on GCE VMs. Can we just remove it?\n. Why is the line above commented?\n. Are we sure that a rename is what logrotation does? Sounds most likely, but just checking..\n. The name of the scripts are not really that descriptive. instead we can have these scripts log what they are doing..\n. #sig-node might be a more appropriate forum. \n. I respond to questions there whenever I can. We can remove it though.\n\nOn Tue, Jun 7, 2016 at 2:58 PM, Tim St. Clair notifications@github.com\nwrote:\n\nIn README.md\nhttps://github.com/google/cadvisor/pull/1323#discussion_r66160638:\n\n@@ -59,4 +59,4 @@ cAdvisor aims to improve the resource usage and performance characteristics of r\n## Community\n-Contributions, questions, and comments are all welcomed and encouraged! cAdvisor developers hang out on Freenode IRC in the #google-containers room & Slack (get an invitation here). We also have the google-containers Google Groups mailing list.\n+Contributions, questions, and comments are all welcomed and encouraged! cAdvisor developers hang out on Freenode IRC in the #google-containers room and Slack #sig-autoscaling channel (get an invitation here). We also have the google-containers Google Groups mailing list.\n\nIs the freenode IRC line still accurate?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1323/files/936d1db3f500c1b73648df84afc52e7091877bd0#r66160638,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AGvIKEpcnYhsgYHqGPd1-sTkBB4vjjB1ks5qJel-gaJpZM4IwVsw\n.\n. package as in a go package or rpm or deb?\n. Can you update the comments? Actually, having a note about the package in the global Readme.md might be super helpful. Not necessary for this PR though.\n. How about declaring these variables upfront instead?\n. go\nvar (\n  dockerStatus info.DockerStatus\n  rktPath string\n)\n. Great idea. Can we do that in a separate PR? \n. Good catch. Fixing it. I'm going to make go 1.6 minimum requirement.\n. Honestly, I have no idea how I managed to pull in a newer version of this dep. Do you think its worth fixing?\n. This is for our jenkins automation that builds a canary docker image from HEAD every night. We can update it I guess to use existing make rules. Again can we do that in a separate PR?\n. Ack. Removing this section.\n. Depends on the context I think. We definitely want contributors to run make all before posting patches. But it is not necessary for building though. \n. I will clean this up in a separate PR since it needs some work on the jenkins side as well\n. Ok. I'm doing it in this patch itself :)\n. https://github.com/google/cadvisor/pull/1372\n. Can you update v2 APi as well? We plan on deprecating v1 sometime soon. \n. +1\n. What is mbean?\n. Are all these fields required? If not, add the omitempty json string.\n. What is the difference between Name and Attribute?\n. I don't think this is a good idea. If defaults exist, they should be per collector and not generic. For example, mysql metrics might be available at a well known socket. nginx metrics need not be available there.\n. @timstclair Should we move each collector into a separate sub-package? \n. Should the next collection time depend on whether the current collection succeeded? Like retry in a second if it fails, and retry in a minute if it succeeds?\n. -1 for these defaults. I'd prefer the per-collector factories fail instead of applying global defaults.\n. nit: Can you change this code to be strings.HasPrefix(strings.ToLower(k), \"prometheus\") here and apply the same logic below to jolokia as well?\n. Are any of these fields optional? If yes, add an omitempty tag\n. Ignore my comment. What is the enforcement interval?\n. From user perspective, grouping it under cpu sgtm. After all it is related to CPU. \n. Ah. My bad. I thought we had cleaned that up :(\n. Also, would it make sense to instead expose CPU hard limits based on quota settings and then expose the number and duration of hardcapping events?\nEssentially hide the cgroups API a bit for make the cAdvisor API user friendly\n. Ok. If we expect that looking up kernel documentation is the best option as of now, I'm ok with this change. \n. Only for the node right and not for all containers?\n. AFAIK, inodes cannot be tracked for devicemapper.\n. It should not be doing that. dir here is expected to be the writable layer of a container. We need to test this.\n. +1\n. Wouldn't it better to make the local and jenkins build be the same? \n. I'd prefer implementing the logic that is there in numactl instead of adding a dependency on this binary.\n. Can this step be hidden behind a make rule?\n. Alternatively we can have make release output the shas\n. Upload the binary?\n. One option is to detect if we are in a containerized environment by setting an environment variable. If we are inside a container, then run the make command directly. If not, then invoke make run_container with the actual make command as an argument. . duty cycle is the fraction of one period in which a signal or system is active\nSource. Would it be helpful to define an AcceleratorHandler interface and place nvidia handler in there? That way adding AMD or other GPU metrics will be isolated to one module (and APIs). @flx42 Are there any plans to move away from cgroups interface anytime soon for Nvidia GPUs?. @flx42 Are these APIs reliable enough for at-least a year?. What is this field supposed to contain? List of PCI IDs of devices? \nAlso add to v2 API as well.. \n",
    "thaJeztah": "Could this be done client-side, i.e., using canvas.toDataURL()? I'd hate to see the dynamic graphs being replaced with old-fashioned static images\n. @vmarmol thanks, basically, that was my only concern :smile: any additional feature is fine with me.\n. Does cAdvisor use the docker daemon to get the list of containers? If it does, the \"labels\" proposal (https://github.com/docker/docker/pull/9882) could offer additional ways to filter containers; e.g. all containers having a label com.google.cadvisor.group=some-group\n. @vmarmol Does this offer the same functionality as the Docker _ping endpoint? Just mentioning this, because if _ping is a name that is used on other platforms as well, it would be nice to use consistent naming (so that it can become a somewhat \"de-facto\" standard)\n. @vmarmol alrighty then. Pity that there's no consistency, but good to hear the endpoint wasn't picked 'at random', so :thumbsup:\n. I like the idea and can confirm that the first time I used cAdvisor, I was a bit confused (now, where are those containers? Why do I need to pick \"docker\"?)\nI do like to have a general overview at the machine level thought, so integrating that is a nice suggestion.\n. Sounds ok, but I'm a \"visual\" guy, so don't hold me to it until after I've seen it :)\nIn all cases, I don't think it's something \"set in stone\", so I'm not worried :smile: \n. > Technically they're still subcontainers since we don't list their children :)\nThink that the terminology is a problem here. Depending on what you are used to (Docker, LMCTFY), there is or isn't such concept (subcontainers) .. or am I wrong here? (Only worked with Docker)\nAnyway, looking forward to see what you come up with, thanks in advance!\n. First time I saw it, I had the advantage that I just read a proposal on Docker for LMCTFY and I read up what it was :) but first impression was \"I saw that nice screenshot in the README, so just click around a bit and see what it gives me\"\nWondering; is a setup possible that is using multiple techniques (ie LMCTFY and Docker on a single instance?)\n. Nice! One nit from my side; is there a way to add the version to the URLs? This gives the opportunity to cache-bust the browser cache when a new version is used. Either;\n- the version of the CSS/JS used (jquery-1.10.2)\n- the version/release of cAdvisor (could be the commit or released version)\nBTW, I noticed that the versions of bootstrap and jQuery are a bit older (bootstrap is now at 3.3.1, jQuery at 1.11.1 or 2.1.1 if you don't care about oldie browsers)\n. > newer versions of bootstrap and jQuery could be done through a separate issue?\nFully agreed, I was just a side-note, should definitely be a separate PR\nAny ideas on versioning the URLs, to allow cache busting?\n. @mindscratch yes, that sounds like the good approach.\n\nWould you prefer to see that in this PR or another?\n\nIf it's not too much trouble, I'd like it in this PR to have the feature 'complete' \nand keep the current \"behavior\" (currently external resources are versioned). \nBUT I think Victor or Vish should make that decision, because I'm just a \"passers-by\" :smile:\n. @mindscratch great, thanks in advance!\n. Ah thanks, awesome job! LGTM (for what it's worth :))\nNote to the maintainers; upgrading a resource version will have to be changed in two locations. I don't think this should be a big problem; it just requires some discipline when making those changes, so not worth changing stuff that would only overcomplicate a small issue.\n. :clap: now, go and enjoy your weekend!\n. Nice looking screenshot! Definitely interesting\n. @timstclair I don't think that would make much difference; the docker build cache should make building near instant. The only thing that can be done to improve is to add a .dockerignore file, so that docker build doesn't upload the source files to the daemon \n. Tried this in a \"clean\" environment; not sure to what extend you want it to \"just work\", but make all-docker still triggers format, which calls godep on the host.\n``` bash\n$ make all-docker\nmake: godep: Command not found\n\n\nformatting code\nmake: godep: No such file or directory\nmake: *** [format] Error 1\n```\n\n\nOh, not related to this pull request, but now that there's quite some options in the Makefile, this is a really cool trick to implement a make help: https://github.com/docker/docker/blob/9e7651db4d464649bc32cf346fdee8c6900ebe0f/Makefile#L114-L116\n. @vishh alright, feel free to ignore then, I was just wondering to what extend you wanted to make it \"work\" anywhere. Making that work would be a \"nice to have\" then, not strictly necessary \n. +1 yes, the \"can be found here\" may be a bit too much. \nHm maybe a <blink> or <marquee>Come inside and find your Docker containers here. Free entrance!</marquee>\n. \n. Perhaps use -w (workdir) instead of \"cd /go/src/github.com/google/cadvisor\", and add --rm so that the container is removed after it's finished. Probably don't need -i;\nMakefile\n@docker run --rm -v $(PWD):/go/src/github.com/google/cadvisor -w /go/src/github.com/google/cadvisor golang:$(goversion) sh -c \"go get github.com/tools/godep && $(GO) test -tags test -short -race $(pkgs)\"\n. same here;\nMakefile\n@docker run --rm -v $(PWD):/go/src/github.com/google/cadvisor -w /go/src/github.com/google/cadvisor golang:$(goversion) sh -c \"go get github.com/tools/godep && ./build/assets.sh && ./build/build.sh\"\nor (with \"workdir\" variable)\nMakefile\n@docker run --rm -v $(PWD):$(workdir) -w $(workdir) golang:$(goversion) sh -c \"go get github.com/tools/godep && ./build/assets.sh && ./build/build.sh\"\n. or (slightly less DRY), use a variable for workdir;\nMakefile\n@docker run --rm -v $(PWD):$(workdir) -w $(workdir) golang:$(goversion) sh -c \"go get github.com/tools/godep && $(GO) test -tags test -short -race $(pkgs)\"\n. For speed, that would probably help yes\n. you can remove this now\n. ",
    "erikh": "Here's a basic shipper between the cadvisor and influx DB APIs: https://gist.github.com/erikh/fbfe6dadc938cf815435\nI'll be working on this a bit as well. If you'd like to collaborate @jchauncey, please lmk.\n. On Jun 17, 2014, at 1:55 PM, monnand notifications@github.com wrote:\n\n58 is the last PR to add storage drivers into cAdvisor. Once we got this merged, I will work on other storage backends like influxdb and BigQuery (which is mentioned in #43)\nCan I take influx? I can probably have it banged out later this week.\n\n-Erik\n. On Jun 17, 2014, at 2:15 PM, Victor Marmol notifications@github.com wrote:\n\n@erikh that SGTM. Only question I have there is if it can be run within the same process as the rest of cAdvisor\nI\u2019ll investigate this but it might not be possible. What would you suggest in lieu of that?\n\n-Erik\n. Unfortunately I have not had time to do this.\nIf you want to take it, go for it.\n-Erik\n\nOn Jul 1, 2014, at 1:40 PM, Rohit Jnagal notifications@github.com wrote:\n@erikh Do you have a patch we can collaborate on?\n\u2014\nReply to this email directly or view it on GitHub.\n. thanks!\n. Looks like this was resolved with the latest image. Thanks guys!\n. \n",
    "josephruscio": "+1 to plugins for different storage backends, we could put a @librato plugin together then :-) \n. ",
    "tamsky": ":+1: to not restricting the implementation of this to a single service.\nBeing able to actively push to multiple distinct backend services, concurrently, is super desirable.\n. ",
    "proppy": "LGTM it be great to build lmctfy from source later and get rid of the custom debs install. Can you add a TODO? \n. Also note that because ONBUILD triggers are inserted just after FROM, the lmctfy layers cache will get invalidated every time you change something in the context (cadvisor source tree)\n. use google/debian:wheezy?\n. don't apt-get upgrade (that's the job of the base image to be up to date)\n. can you build lmctfy from source? or is there a docker base image for it?\n. maybe use google/golang as a base: it already include this\n. I think the ADD below will create the dir\n. go get before should already have compiled it in GOPATH/bin\n. Is it a hard dep? What doesn't work with libprotobuf7?\nSince you're building from source don't you also need libprotobuf-dev?\n. How do you build the binary of lmctfy? Do you have a Dockerfile?\n. Can you fold this with the previous line with && it will Dave you 1 layer.\n. Maybe just ln -s instead, it should have the good rights.\n. --no-install-recommends?\n. Just a note that removing them won't actually save space (since they are still in the previous layer).\nIf you wanted to get them out of it, you would have have a shell script that does everything in 1 RUN instructions.\n. ",
    "kabudu": "@vmarmol Was a patch applied to cadvisor to ignore non-docker and non-root containers in systemd systems? I'm using the latest tag of google/cadvisor available on docker hub with CentOS Linux release 7.2.1511 and still seeing systemd containers appearing in the stats from cadvisor. \n. ",
    "andyxning": "\n--docker_only=true would filter cgroups to only report docker and root containers.\n\nThis is really use to reduce the output from stats/container when used with heapster legacy kubelet source.. Any processing about oom kill log files rotate? As far as i can see that i can not find code processing log rotation in this PR. \n. the supporting version relationship between cAdvisor and Docker should be list somewhere explicitly. \n. @LudicrousSpeed This may be related to GO15VENDOREXPERIMENT. As for common usage, go should use the vendor dir to get dependency first. And also, vendor contains older version of influxdb package which should work properly, going for the influxdb package in GOPATH is most properly due to GO15VENDOREXPERIMENT. \nThis error is reproduce when running env GO15VENDOREXPERIMENT=\"0\" go get -v -d github.com/google/cadvisor with go 1.5 or go1.6(For go1.7 and above, GO15VENDOREXPERIMENT has been removed and the behavior is enabled by default). :). @jianzhangbjz @vishh any progress on this? Or any material about GPU. I think we need more info. . @jianzhangbjz I have no idea about GPU now. Just be willing to give a try. :). @liyubobj \n\nGPU core and GPU memory utility\n\nCould you please explain GPU memory utility. IIUC, we should only expose GPU core utility. :). @liyubobj What if we use other GPU brands, such as AMD. It seems that we need to adapt to all major GPU brands. . @liyubobj Not really. We should keep it generic enough to make cAdvisor easily query GPU metrics from all major brands if possible. :). According to gpu impl in kubernetes, we may give NVML a try.. /cc @vmarmol @vishh @piosz . @timstclair Will change it later. Could you please take a look at the Jenkins e2e test failure. It seems nothing to do with this PR.\nE0103 17:52:43.377] FAIL: pull-cadvisor-e2e.. @timstclair Done. Thanks @dashpole \ud83d\ude04 . @timstclair Done again.. @dashpole IIUC, Kubernetes has separate libraries for getting disk io statistics. Any reference?. This maybe complex when we get many network interface data and choose which one to use.. I have proposed a PR for heapster to summarize all the network interface values instead of just picking up the first one. PTAL @xiangpengzhao @CBR09 . @aaronjwood Does current go1.6.3 version cadvisor uses too much resource for you? Just curious. :). @aaronjwood I am afraid that updating golang version to 1.8 may not solve your problem. IIUC, what you concern is decreasing CPU usage, however, cadvisor has not so much memory that will utiling more CPU for GC. Updating golang version to 1.8 will improve build speed and make cadvisor binary more smaller. I guess we may get less earnings updating to golang 1.8 for CPU utilization. :). @aaronjwood Seems reasonable to me. I agree that we should at least use golang 1.7.5 since golang 1.8 has released a few days before. :)\n@timstclair WDYT. It seems that the google/cadvisor image is built by you. :). @timstclair \n\nThis error is expected when running on a non-GCE host.\n\nThe error can still be recorded.\n\nIf you would like to add a comment to that effect I would accept that change.\n\nCould you please make it more clear, i can not understand it.  :). @dashpole Actually, because kubelet has added cAdvisor as a lib, heapster can retrieve cAdvisor stats endpoint to access cpu/memory/disk io data and emit metrics to different backends.. Even kubelet and heapster can use kubelet summary endpoint, the data for summary endpoint, is also retrieved from cAdvisor.. @dashpole Yes. Even summary api need this device name when adding disk io metrics.. @dashpole FYI: https://github.com/kubernetes/heapster/blob/master/metrics/sources/kubelet/kubelet_client.go#L134-L148. @dashpole Thanks for pointing this and correct my wrong understanding. :). /cc @derekwaynecarr . /ping @dashpole @tallclair . @dashpole \n\nI think labels make more sense as part of the reference, rather than the spec. In kubernetes, labels are part of metadata, rather than spec.\n\nYes. I have thought about this before making this PR. And it is true that this is an backwards-incompatible change. What i prefer spec instead of reference is that heapster actually use spec to access labels. This is my selfishness just not to change heapster and labels in spec has been first added and it maybe used widely than the labels in reference which is later added in https://github.com/google/cadvisor/pull/1033. \nBut i agree that labels should be added to reference instead of spec.. @dashpole Thanks for the review. @tallclair PTAL.\nOnce this is merged, i will make a PR to update the cadvisor dep in Kubernetes.. Ping @tallclair in case you are swallowed in bunch of notifications. :). @dashpole Could we merge this as seems @tallclair is not available recently. :) \nOr should we ping someone else to give this another review?. > we think the best path is to put a deprecation notice in for this coming release (which isnt too far off)\nThis seems good to me. BTW, can you please add a release label or milestone label for this PR in case we forget it. :) I have no auth to do so for now.. @dashpole Can we merge this for now as the v0.29.0 release for kubernetes v1.10 has been released, we can move on for next release of cadvisor. :). > Do you have users who are asking for these metrics?\nActually i have added cpu/load metrics to heapster in https://github.com/kubernetes/heapster/pull/1989. I have some demands about container/cgroup loads to meet the user habits when they are in bare metal/vm.. Close this as it seems that there are not so many people currently using cpu load metrics. Will see this later. . > how would you feel about making this an argument to manager.New()? \nI have also thought about this since rawPrefixWhitelist is a inter package variable. What i think about integrate this cAdvisor functionality into Kubernetes is to only pass this raw prefix whitelist when docker_only is specified automatically. Adding rawPrefixWhitelist should help us to easily integrate this functionality into Kubernetes.\n\nThe only downside is that if people are using cadvisor metrics for particular cgroups that are outside of what we plan to monitor, those would no longer be accessible.\n\nYep. That is the truth when make rawPrefixWhitelist an argument to manager.New(). IMHO, it should be fine as no users have complained about lacking the ability to customize the whitelist when docker_only has been specified. Since we have no clear plan about how to evolve the related settings, leaving customizing whitelist from command line seems fine. If we need this later, it should also not difficult to add the support.\n. /retest. @dashpole @tallclair PTAL. I have moved the configuration from command line to manager field. This should be fine for us to integrate cadvisor into Kubernetes.. Ping @dashpole @tallclair \nBTW, after this PR is merged, i will prepare an PR to bump cAdvisor dep to Kubernetes to utilizing this new change.. > have you brought this up at sig-instrumentation at all? It may be valuable to hear more opinions on limiting the cgroups that are scraped.\nNo. I have not talk about this in sig-instrumentation. Do we need to do a email to sig-instrumentation to describe this PR.. @dashpole Seems @tallclair has temporarily no response. Do anyone else can help us to review\n this? . Ping @dashpole @tallclair . Ping @dashpole @tallclair in case the notification emails are lost in your inbox. :). /cc @dashpole @tallclair . Ping @dashpole @tallclair. @dashpole @tallclair PTAL. \ud83d\ude06. /cc @dashpole @tallclair PTAL.. /cc @dashpole @tallclair PTAL.. Will make a PR to Kubernetes once the dependency for cAdvisor is updated to make use of the new functionality. :). @serathius No. Will work on this later this week.. @timstclair done.. Will do.. @timstclair PTAL.. Sounds good to me. Will do.. > We can make setting --docker_only set this to [ ] during creation of the raw factory.\nWe should not do this, imo. What we want is to respect with whitelist settings even docker_only is set and  we should customize whitelist settings to at least []string{\"/kubepods\"} to allow pod level metrics in Kubernetes.. What i think is about this: \n by default setting rawPrefixWhiteList to []string{\"/\"} to allow all raw cgroups to be collected or be compatible with backwards\n always filter whitelist raw cgroups and respect with the whitelist settings.. PTAL.. ",
    "tianon": "Be careful who you summon for unsolicited review. >:)\n. Rock on, :heart:\n. \"from\"\n. Don't you want to add set -e here to protect later commands against failures?\n. Oh man, now the fact that I reviewed this from my phone is showing.  You do || echo ..., so it'll still be truthy :x\n. You could remove the failure echos and just add set -x which prints the commands as they're run, as a possible alternative.\n. Wouldn't it be more consistent to use arm64v8/alpine:3.7 here so this matches Dockerfile.amd64?\n(If you visit https://hub.docker.com/_/alpine/ and click on arm64v8 under \"Supported architectures:\", it takes you to https://hub.docker.com/r/arm64v8/alpine/, which is the architecture-specific push location for official images and shows that 3.7 is supported there.). ",
    "byxorna": "@vmarmol @monnand thanks!\n. ",
    "kelseyhightower": "@monnand I think it's fine either way. Personally I just vendor with godep and avoid rewriting import paths. It just seems to look \"better\" that way, but using a \"standard\" tool for this type of thing is the real victory.\n. Symlink\n. @vmarmol I'm going to issue a PR to godep to make that directory configurable \n. Looks like the Kubernetes project just pushed support for godep as is. Would be nice to do the same here. The _workspace is ignored by the go tool, and seems to be pretty standard in the Go community at this point. \n. Yes.\n. Rebased. PTAL\n. Please take another look\n. Ah, yes.\n. ",
    "danmcp": "@vmarmol This has already been filled out for Red Hat.  What's the process given that?\n. @vmarmol This has already been filled out for Red Hat.  What's the process given that?\n. ",
    "bliss": "Good day! All of a sudden I found out my statistics have gone. After some examination i found that container names in influxdb became like that one\n/system.slice/docker-a89ad7021d73e64de54b8f25fec4bf8a3fb23441b661b9b5a2c6cfbcb000b044.scope\ninstead of more habitual\n/k8s_mariadb2562281893.30a8f541_6839bb45-0ff5-4a68-ae88-7a5203814750-k9349_91ad4704-04c1-490c-bed1-ebb750484d4f_c7a0a08a-52e1-11e5-b267-5\n2540051caca_0cb6d646\nand no network statistics.\nHow can i fix it?\nOS: CentOS7\ncat /etc/redhat-release\nCentOS Linux release 7.1.1503 (Core)\nstandalone cAdvisor\ncadvisor --version\ncAdvisor version 0.14.0\n. Thank you for your reply.\n\"k8s_* containers are created by kubernetes. cAdvisor has no control over the alias of containers.\" --> But it had, because it definitely fed those aliases to influxdb. Moreover, when polled via API cadvisor returns only system slice statistics, no data for docker containers. Looks like it cannot get access to cgroups or something like that. I turned selinux on and off but no difference. Last time we resolved such issue with cadvisor update: from 0.6.2 to 0.14.\n. @vishh could please make it some more clear? Even with --logtostderr=true --v=2 I got no log output except start/stop info. And let me please know where can I get the '/validate' ? I tried\ncurl http://localhost:4194/validate\n<a href=\"/validate/\">Moved Permanently</a>.\nThank you!\n. When I started cadvisor from command line as root I got all statistics. Maybe the problem arises when cadvisor started by systemd\n. ```\ncAdvisor version: 0.14.0\nOS version: CentOS Linux 7 (Core)\nKernel version: [Supported and recommended]\n        Kernel version is 3.10.0-229.11.1.el7.centos.x86_64. Versions >= 2.6 are supported. 3.0+ are recommended.\nCgroup setup: [Supported and recommended]\n        Available cgroups: map[cpu:1 memory:1 cpuset:1 devices:1 freezer:1 net_cls:1 blkio:1 perf_event:1 hugetlb:1 cpuacct:1]\n        Following cgroups are required: [cpu cpuacct]\n        Following other cgroups are recommended: [memory blkio cpuset devices freezer]\n        Hierarchical memory accounting enabled. Reported memory usage includes memory used by child containers.\nCgroup mount setup: [Supported and recommended]\n        Cgroups are mounted at /sys/fs/cgroup.\n        Cgroup mount directories: blkio cpu cpu,cpuacct cpuacct cpuset devices freezer hugetlb memory net_cls perf_event systemd \n        Any cgroup mount point that is detectible and accessible is supported. /sys/fs/cgroup is recommended as a standard location.\n        Cgroup mounts:\n        cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n        cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n        cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpuacct,cpu 0 0\n        cgroup /sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0\n        cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n        cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n        cgroup /sys/fs/cgroup/net_cls cgroup rw,nosuid,nodev,noexec,relatime,net_cls 0 0\n        cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n        cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n        cgroup /sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0\nDocker version: [Unknown]\n        Could not parse docker version. Docker version is Unknown. Versions >= 1.0 are supported. 1.2+ are recommended.\nDocker driver setup: [Supported and recommended]\n        Docker exec driver is native-0.2. Storage driver is overlay.\n        systemd is being used to create cgroups.\n        Docker container state directory is at \"/var/lib/docker/containers\" and is accessible.\nBlock device setup: [Supported, but not recommended]\n        None of the devices support 'cfq' I/O scheduler. No disk stats can be reported.\n         Disk \"vda\" Scheduler type \"none\".\n         Disk \"vdb\" Scheduler type \"none\".\n```\nLooks pretty ridiculous but after I ran cadvisor from under root on one of my nodes it got self-healed and started to return required stats being run as usually by systemd. The 'validate' output above from another node with the same problems. Diff between /validate calls on the 'not-healed' node and 'healed' one below:\n```\n diff n1.log n2.log\n10c10\n<       Available cgroups: map[cpu:1 memory:1 cpuset:1 devices:1 freezer:1 net_cls:1 blkio:1 perf_event:1 hugetlb:1 cpuacct:1]\n\n\n  Available cgroups: map[cpu:1 freezer:1 net_cls:1 blkio:1 perf_event:1 hugetlb:1 cpuset:1 cpuacct:1 memory:1 devices:1]\n\n33,34c33,34\n< Docker version: [Unknown]\n<       Could not parse docker version. Docker version is Unknown. Versions >= 1.0 are supported. 1.2+ are recommended.\n\n\n\nDocker version: [Supported and recommended]\n      Docker version is 1.6.2. Versions >= 1.0 are supported. 1.2+ are recommended.\n36d35\n< \n```\n\nOn all nodes docker is installed, functioning and of the same version:\n$ docker --version\nDocker version 1.6.2, build ba1f6c3/1.6.2\n. ",
    "hub-cap": "yea i dont mind that. let me run it by the powers that be and figure out the CLA stuff and actually get it properly prioritized and all that standard mumbo jumbo. ill get back w/ ya. Dunno if u want to nuke this issue or leave it open for others to see.. your call. ty.\n. This project was canned by my mgmt, /me cries. Id love to see someone else take up the work, or once i convince mgmt to let me do it again ill restart my work..\n. ",
    "charliek": "Another option might be to leverage environment variables as an opt in/out mechanism.  I've seen this approach used in this project https://github.com/progrium/registrator and a couple others. This seems like it could be a little cleaner, since I've seen regex based methods for things like this get unmanageable.\n. ",
    "dipankar": "@rjnagal Was thinking about implementing filtering by container aliases, any pointers as to what you guys decided approach wise?\n. Hey @yesnault just wrote up a script which sends the container load stats to riemann. You can check it out here. Hope it helps!\n. Great! Let me know if I can be of any help there.\n. Added the filesystem gauge, put in the PR. \nHave signed the CLA form. No clue how much time that will take.\n. Just realized that the js is much \n. Have made the updates @vmarmol @vishh . Thanks for the feedback!\n. Added log info for both!\n. Sure, letting you know when thats done. Are you okay with re-arranging the docs a bit? Or do you feel its okay to just put this in the web UI section. \n. @vmarmol Made the humanize updates! Hope this is better.\n. Done :)\n. Yep squashed the commits.\n. Yep, thats definitely a better name!\n. These are some test files, with some default users baked in. Can remove them, if they are an issue?\n. The duplication was something that I was trying to minimize :), got greedy with trying to put in both HTTP auth and digest! I do feel the current flow is easier to understand. \n. ",
    "denderello": "We had a discussion about this topic today and it would help us a lot to lower the amount of data that is transferred to InfluxDB because we are not interested in all data collected by cAdvisor. What's your opinion on this?\n. @rjnagal sounds like a good approach. Judging from our use cases multiple expressions could definitely make it easier instead of trying to come up with one big expressions that tries to fit all matches.\n. Hey @vmarmol, just signed it.\n. @vishh I selfishly used stats as default name for the table from InfluxDB but that would break with the current default for BigQuery. Any objections against this or should we find a different solution?\n. ",
    "daniel-garcia": "Does defaulting to number of processors have any performance penalties on system with a large number of cores? Also, is there a google group for cAdvisor?\n. I had forgotten that cAdvisor also runs in a container. Thanks.\n. ",
    "marcellodesales": "@vishh I'm having the same issue... I tried to set the variables using docker-compose through variable replacement https://docs.docker.com/engine/reference/builder/#environment-replacement. \nI tried to add CADVISOR_HOST as a variable in the environment and that did not work.\ncadvisor_1        | E0217 06:27:54.241068       1 memory.go:91] failed to write stats to influxDb - parse http://$%7BCADVISOR_HOST%7D/write: percent-encoded characters in host\nMaybe instead of the command instruction, we could pass env vars and all would be good.\n. :+1: @vmarmol Happy New Year!!! It is a promising 2016 and a lot of people is waiting for this to be merged... 6 months ago you mentioned you would be taking a look at it... What needs to be reviewed? Can someone on this thread help?\n. @svenmueller I tested a PR and it is now supported...  https://github.com/google/cadvisor/pull/1040#issuecomment-176015272 \nI built an image to help others at https://hub.docker.com/r/marcellodesales/google-cadvisor/... I will maintain it until the team releases a new version with it.\n. @vishh I did not verify it :S Well that's nice then I will remove my image... thanks!\n. @vishh I see the new image works at the tag you mentioned. \n@mboussaa Use the tag v0.20.5 and everything is working on the latest.\ninfluxdb_1     | [http] 2016/02/05 19:25:47 172.17.0.2 - root [05/Feb/2016:19:25:47 +0000] POST /write?consistency=&db=cadvisor&precision=&rp= HTTP/1.1 204 0 - cAdvisor/0.20.5 41f62cc0-cc3e-11e5-8007-000000000000 24.01661ms\nHowever, the latest tag is not pointing to the v0.20.5 version. What's the process to update the tag?\n$ docker images | grep cadvisor\ngoogle/cadvisor                                           v0.20.5                       cb4f76a7607a        46 hours ago        43.12 MB\ngoogle/cadvisor                                           latest                        cec6ac4b467e        10 days ago         36.46 MB\n. @vishh Although I can definitely agree with your point, most documentation I found uses the the tag latest... It confuses a lot of users, including myself... It since there was not Git Tag on this merge request, it is hard to find which version this is released...\nCould you guys please tag the merge as part of the release process, in case the docker latest tag is not used? that would simplify and help users finding features...!\nThanks a lot!\n. @mnuessler @christianhuening @carmark Is there a way to revisit this PR and get it fixed? Is there a way to help?\n. @jimmidyson Thanks a lot for getting this! I hope I can do any code review after a  PR that passes tests!\n. @berglh Just tested the mergePR #1040 and I published an image till cAdvisor releases a new version https://hub.docker.com/r/marcellodesales/google-cadvisor/\n. This is still happening with the latest, but not with the tag v0.20.5, as shown at https://github.com/google/cadvisor/pull/1040#issuecomment-180518519.\ninfluxdb_1     | [http] 2016/02/05 19:25:47 172.17.0.2 - root [05/Feb/2016:19:25:47 +0000] POST /write?consistency=&db=cadvisor&precision=&rp= HTTP/1.1 204 0 - cAdvisor/0.20.5 41f62cc0-cc3e-11e5-8007-000000000000 24.01661ms\nHowever, the latest tag is not pointing to the v0.20.5 version. \n$ docker images | grep cadvisor\ngoogle/cadvisor                                           v0.20.5                       cb4f76a7607a        46 hours ago        43.12 MB\ngoogle/cadvisor                                           latest                        cec6ac4b467e        10 days ago         36.46 MB\nThe current workaround for this is to use google/cadvisor:v0.20.5.\n. @jimmidyson I hope I get some time to build local and verify in case this tests pass...\n. @jimmidyson I will try to build and verify this weekend! Thanks a lot for merging this!\n. @svenmueller apparently not released yet...\nSorry... took me a while to try to verify this... \n@jimmidyson, I apparently can't get it to work yet... I just built an image at https://hub.docker.com/r/marcellodesales/google-cadvisor/, tag influxdb-0.9, based on commit 040bdd3 (see step-by-step)... \nUPDATE: IT WORKS! -storage_driver=influxdb was missing in the command instruction below.\nUPDATE: Docker-compose at https://github.com/google/cadvisor/pull/1040#issuecomment-179568126\nHow I built\nCould you please verify if that's correct? I did the following (https://hub.docker.com/r/marcellodesales/google-cadvisor/)\n- Checked out commit 040bdd3\n- Built locally\n- Ran local tests\n- Built Docker Image\n- Tagged and Pushed\nBuild Instructions\nGot the code\n\nhttps://github.com/google/cadvisor/blob/master/docs/build.md\n\n$ go get -d github.com/google/cadvisor\npackage golang.org/x/net/context/ctxhttp: /home/mdesales/go/src/golang.org/x/net exists but /home/mdesales/go/src/golang.org/x/net/.git does not - stale checkout?\ny\n$ go get github.com/tools/godep\nChecked out the merge commit\n```\n $ git checkout 040bdd3c\nNote: checking out '040bdd3c'.\n$ git log\ncommit 040bdd3cb1b038f2b6aca48c62fa4a1a8cb8da6c\nMerge: 1b62cef 706a954\nAuthor: Vish Kannan vishh@users.noreply.github.com\nDate:   Fri Jan 8 10:27:33 2016 -0800\nMerge pull request #1040 from jimmidyson/influxdb-0.9\n\nUpgrade InfluxDB storage to InfluxDB 0.9\n\n```\nBuilt, verified\n```\n$ godep go build .\n$ ls -la cadvisor\n-rwxr-xr-x 1 mdesales mdesales 19507383 Jan 27 16:15 cadvisor\n$ file cadvisor\ncadvisor: ELF 64-bit LSB  executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.24, BuildID[sha1]=befe7321680d19977d844d01e08f67af328eabe8, not stripped\n```\nRan Unit test cases\n$ godep go test ./... -test.short\n?       github.com/google/cadvisor  [no test files]\nok      github.com/google/cadvisor/api  0.018s\n?       github.com/google/cadvisor/cache    [no test files]\nok      github.com/google/cadvisor/cache/memory 0.324s\nok      github.com/google/cadvisor/client   0.685s\n?       github.com/google/cadvisor/client/clientexample [no test files]\nok      github.com/google/cadvisor/client/v2    0.042s\nok      github.com/google/cadvisor/collector    0.043s\nok      github.com/google/cadvisor/container    0.010s\n?       github.com/google/cadvisor/container/docker [no test files]\nok      github.com/google/cadvisor/container/libcontainer   0.044s\nok      github.com/google/cadvisor/container/raw    0.030s\nok      github.com/google/cadvisor/events   0.011s\nok      github.com/google/cadvisor/fs   0.740s\n?       github.com/google/cadvisor/healthz  [no test files]\n?       github.com/google/cadvisor/http [no test files]\n?       github.com/google/cadvisor/http/mux [no test files]\nok      github.com/google/cadvisor/info/v1  0.072s\n?       github.com/google/cadvisor/info/v1/test [no test files]\n?       github.com/google/cadvisor/info/v2  [no test files]\n?       github.com/google/cadvisor/integration/common   [no test files]\n?       github.com/google/cadvisor/integration/framework    [no test files]\n?       github.com/google/cadvisor/integration/runner   [no test files]\nok      github.com/google/cadvisor/integration/tests/api    0.023s\nok      github.com/google/cadvisor/integration/tests/healthz    0.003s\nok      github.com/google/cadvisor/manager  0.036s\nok      github.com/google/cadvisor/metrics  0.007s\n?       github.com/google/cadvisor/pages    [no test files]\n?       github.com/google/cadvisor/pages/static [no test files]\n?       github.com/google/cadvisor/storage  [no test files]\n?       github.com/google/cadvisor/storage/bigquery [no test files]\n?       github.com/google/cadvisor/storage/bigquery/client  [no test files]\n?       github.com/google/cadvisor/storage/bigquery/client/example  [no test files]\n?       github.com/google/cadvisor/storage/elasticsearch    [no test files]\n?       github.com/google/cadvisor/storage/influxdb [no test files]\n?       github.com/google/cadvisor/storage/redis    [no test files]\n?       github.com/google/cadvisor/storage/statsd   [no test files]\n?       github.com/google/cadvisor/storage/statsd/client    [no test files]\n?       github.com/google/cadvisor/storage/stdout   [no test files]\n?       github.com/google/cadvisor/storage/test [no test files]\nok      github.com/google/cadvisor/summary  0.026s\nok      github.com/google/cadvisor/utils    0.003s\n?       github.com/google/cadvisor/utils/cloudinfo  [no test files]\n?       github.com/google/cadvisor/utils/cpuload    [no test files]\n?       github.com/google/cadvisor/utils/cpuload/netlink    [no test files]\n?       github.com/google/cadvisor/utils/cpuload/netlink/example    [no test files]\nok      github.com/google/cadvisor/utils/machine    0.005s\nok      github.com/google/cadvisor/utils/oomparser  0.005s\n?       github.com/google/cadvisor/utils/oomparser/oomexample   [no test files]\n?       github.com/google/cadvisor/utils/procfs [no test files]\n?       github.com/google/cadvisor/utils/sysfs  [no test files]\n?       github.com/google/cadvisor/utils/sysfs/fakesysfs    [no test files]\nok      github.com/google/cadvisor/utils/sysinfo    0.002s\nok      github.com/google/cadvisor/validate 0.008s\n?       github.com/google/cadvisor/version  [no test files]\nGenerated the new Docker Image to push to https://hub.docker.com/r/marcellodesales/google-cadvisor/.\n```\n$ make docker\nSending build context to Docker daemon 46.42 MB\nStep 1 : FROM alpine:3.2\n ---> ab7e84202862\nStep 2 : MAINTAINER dengnan@google.com vmarmol@google.com vishnuk@google.com jimmidyson@gmail.com\n ---> Using cache\n ---> 88671c112252\nStep 3 : RUN apk add --update ca-certificates device-mapper &&     wget https://circle-artifacts.com/gh/andyshinn/alpine-pkg-glibc/8/artifacts/0/home/ubuntu/alpine-pkg-glibc/packages/x86_64/glibc-2.21-r2.apk &&     wget https://circle-artifacts.com/gh/andyshinn/alpine-pkg-glibc/8/artifacts/0/home/ubuntu/alpine-pkg-glibc/packages/x86_64/glibc-bin-2.21-r2.apk &&     apk add --allow-untrusted glibc-2.21-r2.apk glibc-bin-2.21-r2.apk &&     /usr/glibc/usr/bin/ldconfig /lib /usr/glibc/usr/lib &&     echo 'hosts: files mdns4_minimal [NOTFOUND=return] dns mdns4' >> /etc/nsswitch.conf &&     rm -rf /var/cache/apk/*\n ---> Running in da7cd7d9b138\nfetch http://dl-4.alpinelinux.org/alpine/v3.2/main/x86_64/APKINDEX.tar.gz\n(1/9) Installing run-parts (4.4-r0)\n(2/9) Installing openssl (1.0.2e-r0)\n(3/9) Installing lua5.2-libs (5.2.4-r0)\n(4/9) Installing lua5.2 (5.2.4-r0)\n(5/9) Installing ncurses-terminfo-base (5.9-r3)\n(6/9) Installing ncurses-widec-libs (5.9-r3)\n(7/9) Installing lua5.2-posix (33.3.1-r2)\n(8/9) Installing ca-certificates (20141019-r2)\n(9/9) Installing device-mapper (2.02.122-r3)\nExecuting busybox-1.23.2-r0.trigger\nExecuting ca-certificates-20141019-r2.trigger\nOK: 8 MiB in 24 packages\nConnecting to circle-artifacts.com (52.72.7.255:443)\nglibc-2.21-r2.apk    100% |****|  1813k  0:00:00 ETA\nConnecting to circle-artifacts.com (52.71.104.154:443)\nglibc-bin-2.21-r2.ap 100% |****|  1018k  0:00:00 ETA\n(1/3) Installing libgcc (4.9.2-r5)\n(2/3) Installing glibc (2.21-r2)\n(3/3) Installing glibc-bin (2.21-r2)\nExecuting glibc-bin-2.21-r2.trigger\nOK: 15 MiB in 27 packages\n ---> dd061e106fba\nRemoving intermediate container da7cd7d9b138\nStep 4 : ADD cadvisor /usr/bin/cadvisor\n ---> aa66b7ae0141\nRemoving intermediate container e73877928c84\nStep 5 : EXPOSE 8080\n ---> Running in dafae0aa3f2e\n ---> 58e6ab9cc383\nRemoving intermediate container dafae0aa3f2e\nStep 6 : ENTRYPOINT /usr/bin/cadvisor -logtostderr\n ---> Running in 8eff24631902\n ---> b6495090befb\nRemoving intermediate container 8eff24631902\nSuccessfully built b6495090befb\n$ docker images | grep cadvisor\ncadvisor                                                  040bdd3                       b6495090befb        8 seconds ago       36.52 MB\n$ docker tag b6495090befb marcellodesales/google-cadvisor:influxdb-0.9\n```\nInfluxDB setup\n\nStarted a regular latest InfluxDB with default paramers\n\n$ sudo docker run -d -p 8083:8083 -p 8086:8086 --expose 8090 --expose 8099 --name influxsrv tutum/influxdb\nI created the default user root:root as it was not created at bootstrap.\n\nI also created the default database cadvisor\n\n```\n$ docker logs influxsrv\ninfluxdb configuration: \nWelcome to the InfluxDB configuration file.\nOnce every 24 hours InfluxDB will report anonymous data to m.influxdb.com\nThe data includes raft id (random 8 bytes), os, arch, version, and metadata.\nWe don't track ip addresses of servers reporting. This is only used\nto track the number of instances running and the versions, which\nis very helpful for us.\nChange this option to true to disable reporting.\nreporting-disabled = false\n\n[meta]\n\nControls the parameters for the Raft consensus group that stores metadata\nabout the InfluxDB cluster.\n\n[meta]\n  dir = \"/data/meta\"\n  hostname = \"localhost\"\n  bind-address = \":8088\"\n  retention-autocreate = true\n  election-timeout = \"1s\"\n  heartbeat-timeout = \"1s\"\n  leader-lease-timeout = \"500ms\"\n  commit-timeout = \"50ms\"\n\n[data]\n\nControls where the actual shard data for InfluxDB lives and how it is\nflushed from the WAL. \"dir\" may need to be changed to a suitable place\nfor your system, but the WAL settings are an advanced configuration. The\ndefaults should work for most systems.\n\n[data]\n  dir = \"/data/db\"\n# The following WAL settings are for the b1 storage engine used in 0.9.2. They won't\n  # apply to any new shards created after upgrading to a version > 0.9.3.\n  max-wal-size = 104857600 # Maximum size the WAL can reach before a flush. Defaults to 100MB.\n  wal-flush-interval = \"10m0s\" # Maximum time data can sit in WAL before a flush.\n  wal-partition-flush-delay = \"2s\" # The delay time between each WAL partition being flushed.\n# These are the WAL settings for the storage engine >= 0.9.3\n  wal-dir = \"/data/wal\"\n  wal-enable-logging = true\n# When a series in the WAL in-memory cache reaches this size in bytes it is marked as ready to\n  # flush to the index\n  # wal-ready-series-size = 25600\n# Flush and compact a partition once this ratio of series are over the ready size\n  # wal-compaction-threshold = 0.6\n# Force a flush and compaction if any series in a partition gets above this size in bytes\n  # wal-max-series-size = 2097152\n# Force a flush of all series and full compaction if there have been no writes in this\n  # amount of time. This is useful for ensuring that shards that are cold for writes don't\n  # keep a bunch of data cached in memory and in the WAL.\n  # wal-flush-cold-interval = \"10m\"\n# Force a partition to flush its largest series if it reaches this approximate size in\n  # bytes. Remember there are 5 partitions so you'll need at least 5x this amount of memory.\n  # The more memory you have, the bigger this can be.\n  # wal-partition-size-threshold = 20971520\n..\n..\n```\ncAdvisor Logs\nSo, I tried to run locally and I still cannot see anything in InfluxDB, default parameters... \n$ sudo docker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro --publish=9080:8080 --detach=true --name=cadvisor-local2 marcellodesales/google-cadvisor:influxdb-0.9 -storage_driver_db=cadvisor -storage_driver_host=localhost:8086\nHere's the output... Is there any switch to turn on debugger around the connector?\n$ docker logs cadvisor\nI0128 01:31:19.446159       1 storagedriver.go:44] Caching stats in memory for 2m0s\nI0128 01:31:19.446414       1 manager.go:128] cAdvisor running in container: \"/docker/6a522685230ed83cabc1097bb19106353f38821cd99455567655908d74926b3a\"\nI0128 01:31:19.622739       1 fs.go:105] Filesystem partitions: map[/dev/disk/by-uuid/aef7d545-5fd5-488b-b69a-2b460a8daaef:{mountpoint:/rootfs major:8 minor:1 fsType: blockSize:0} /dev/sda3:{mountpoint:/rootfs/more-space major:8 minor:3 fsType: blockSize:0}]\nI0128 01:31:19.754555       1 machine.go:50] Couldn't collect info from any of the files in \"/rootfs/etc/machine-id,/var/lib/dbus/machine-id\"\nI0128 01:31:19.754676       1 manager.go:163] Machine: {NumCores:4 CpuFrequency:2793746 MemoryCapacity:7172173824 MachineID: SystemUUID:564DB8D8-A255-24BE-28A2-370A36553D36 BootID:2dfc9f7d-ece4-479c-921c-ca71e3fa067d Filesystems:[{Device:/dev/disk/by-uuid/aef7d545-5fd5-488b-b69a-2b460a8daaef Capacity:19945680896} {Device:/dev/sda3 Capacity:52710469632}] DiskMap:map[2:0:{Name:fd0 Major:2 Minor:0 Size:0 Scheduler:deadline} 8:0:{Name:sda Major:8 Minor:0 Size:75161927680 Scheduler:deadline} 8:16:{Name:sdb Major:8 Minor:16 Size:107374182400 Scheduler:deadline}] NetworkDevices:[{Name:br-82cc809b176a MacAddress:02:42:30:9f:da:23 Speed:0 Mtu:1500} {Name:br-cb5e14d68103 MacAddress:02:42:af:8b:78:a5 Speed:0 Mtu:1500} {Name:eth0 MacAddress:00:0c:29:55:3d:36 Speed:1000 Mtu:1500}] Topology:[{Id:0 Memory:7172173824 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:6291456 Type:Unified Level:3} {Size:134217728 Type:Unified Level:4}]} {Id:2 Memory:0 Cores:[{Id:0 Threads:[1] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:6291456 Type:Unified Level:3} {Size:134217728 Type:Unified Level:4}]} {Id:4 Memory:0 Cores:[{Id:0 Threads:[2] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:6291456 Type:Unified Level:3} {Size:134217728 Type:Unified Level:4}]} {Id:6 Memory:0 Cores:[{Id:0 Threads:[3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:6291456 Type:Unified Level:3} {Size:134217728 Type:Unified Level:4}]}] CloudProvider:Unknown InstanceType:Unknown}\nI0128 01:31:19.756000       1 manager.go:169] Version: {KernelVersion:3.13.0-76-generic ContainerOsVersion:Alpine Linux v3.2 DockerVersion:1.9.1 CadvisorVersion: CadvisorRevision:}\nI0128 01:31:19.883034       1 factory.go:245] Registering Docker factory\nI0128 01:31:19.886669       1 factory.go:94] Registering Raw factory\nI0128 01:31:20.154208       1 manager.go:1005] Started watching for new ooms in manager\nW0128 01:31:20.154409       1 manager.go:236] Could not configure a source for OOM detection, disabling OOM events: exec: \"journalctl\": executable file not found in $PATH\nI0128 01:31:20.174621       1 manager.go:249] Starting recovery of all containers\nI0128 01:31:20.192585       1 manager.go:254] Recovery completed\nI0128 01:31:20.201919       1 cadvisor.go:106] Starting cAdvisor version: - on port 8080\nI do see activity on the cAdvisor container through the UI, \n\nbut I don't see any measurements on the influxDB side.\n\nI do see the measurements for the _internal database...\n\n@jimmidyson, @carmark, @vishh, @christianhuening @carmark @mnuessler Is there anything that can help me debug this?\n. @mnuessler I finally can confirm the PR is working as expected! Thanks a lot for spotting this mistake :+1: \n@jimmidyson I can finally confirm this is working for me!!! Thanks a lot!\nI do se the first line of the logs 1 storagedriver.go:42] Using backend storage type \"influxdb\". Here's the full logs...\n$ docker logs -f cadvisor-0.9\nI0128 09:15:32.838890       1 storagedriver.go:42] Using backend storage type \"influxdb\"\nI0128 09:15:32.838962       1 storagedriver.go:44] Caching stats in memory for 2m0s\nI0128 09:15:32.839018       1 manager.go:128] cAdvisor running in container: \"/docker/4e56068ff373fc4fee042f052055bb3839c8568cb0fc2347fe411c34fc4efb01\"\nI0128 09:15:32.975282       1 fs.go:105] Filesystem partitions: map[/dev/disk/by-uuid/aef7d545-5fd5-488b-b69a-2b460a8daaef:{mountpoint:/rootfs major:8 minor:1 fsType: blockSize:0} /dev/sda3:{mountpoint:/rootfs/more-space major:8 minor:3 fsType: blockSize:0}]\nI0128 09:15:33.733564       1 machine.go:50] Couldn't collect info from any of the files in \"/rootfs/etc/machine-id,/var/lib/dbus/machine-id\"\nI0128 09:15:33.733781       1 manager.go:163] Machine: {NumCores:4 CpuFrequency:2793746 MemoryCapacity:7172173824 MachineID: SystemUUID:564DB8D8-A255-24BE-28A2-370A36553D36 BootID:2dfc9f7d-ece4-479c-921c-ca71e3fa067d Filesystems:[{Device:/dev/sda3 Capacity:52710469632} {Device:/dev/disk/by-uuid/aef7d545-5fd5-488b-b69a-2b460a8daaef Capacity:19945680896}] DiskMap:map[2:0:{Name:fd0 Major:2 Minor:0 Size:0 Scheduler:deadline} 8:0:{Name:sda Major:8 Minor:0 Size:75161927680 Scheduler:deadline} 8:16:{Name:sdb Major:8 Minor:16 Size:107374182400 Scheduler:deadline}] NetworkDevices:[{Name:br-82cc809b176a MacAddress:02:42:30:9f:da:23 Speed:0 Mtu:1500} {Name:br-cb5e14d68103 MacAddress:02:42:af:8b:78:a5 Speed:0 Mtu:1500} {Name:eth0 MacAddress:00:0c:29:55:3d:36 Speed:1000 Mtu:1500}] Topology:[{Id:0 Memory:7172173824 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:6291456 Type:Unified Level:3} {Size:134217728 Type:Unified Level:4}]} {Id:2 Memory:0 Cores:[{Id:0 Threads:[1] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:6291456 Type:Unified Level:3} {Size:134217728 Type:Unified Level:4}]} {Id:4 Memory:0 Cores:[{Id:0 Threads:[2] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:6291456 Type:Unified Level:3} {Size:134217728 Type:Unified Level:4}]} {Id:6 Memory:0 Cores:[{Id:0 Threads:[3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:6291456 Type:Unified Level:3} {Size:134217728 Type:Unified Level:4}]}] CloudProvider:Unknown InstanceType:Unknown}\nI0128 09:15:33.734994       1 manager.go:169] Version: {KernelVersion:3.13.0-76-generic ContainerOsVersion:Alpine Linux v3.2 DockerVersion:1.9.1 CadvisorVersion: CadvisorRevision:}\nI0128 09:15:33.875937       1 factory.go:245] Registering Docker factory\nI0128 09:15:33.878857       1 factory.go:94] Registering Raw factory\nI0128 09:15:34.161698       1 manager.go:1005] Started watching for new ooms in manager\nW0128 09:15:34.161878       1 manager.go:236] Could not configure a source for OOM detection, disabling OOM events: exec: \"journalctl\": executable file not found in $PATH\nI0128 09:15:34.173012       1 manager.go:249] Starting recovery of all containers\nI0128 09:15:34.190601       1 manager.go:254] Recovery completed\nI0128 09:15:34.200440       1 cadvisor.go:106] Starting cAdvisor version: - on port 8080\nNow I can see the measuremnts.\n\n. @jimmidyson No problem!!! Any plans to release a version with this?\n. Here's a Docker-compose if anyone wants to use it until this is released.\n``` yml\ninfluxdbData:\n  image: busybox\n  volumes:\n    - ./data/influxdb:/data\ninfluxdb:\n  image: tutum/influxdb\n  environment:\n    - PRE_CREATE_DB=cadvisor\n  ports:\n    - \"8083:8083\"\n    - \"8086:8086\"\n  expose:\n    - \"8090\"\n    - \"8099\"\n  volumes_from:\n    - \"influxdbData\"\ncadvisor:\n  image: marcellodesales/google-cadvisor:influxdb-0.9\n  command: -storage_driver=influxdb -storage_driver_db=cadvisor -storage_driver_host=influxsrv:8086\n  ports:\n    - \"9090:8080\"\n  volumes:\n    - /:/rootfs:ro\n    - /var/run:/var/run:rw\n    - /sys:/sys:ro\n    - /var/lib/docker/:/var/lib/docker:ro\n  links:\n    - influxdb:influxsrv\n``\n. As discussed at https://github.com/google/cadvisor/issues/794#issuecomment-180512176, the fix is apparently only applied to tagv0.20.5`. So, here's an updated docker-compose.yml with the changes.\n``` yml\ninfluxdbData:\n  image: busybox\n  volumes:\n    - ./data/influxdb:/data\ninfluxdb:\n  image: tutum/influxdb\n  environment:\n    - PRE_CREATE_DB=cadvisor\n  ports:\n    - \"8083:8083\"\n    - \"8086:8086\"\n  expose:\n    - \"8090\"\n    - \"8099\"\n  volumes_from:\n    - \"influxdbData\"\ncadvisor:\n  image: google/cadvisor:v0.20.5\n  command: -storage_driver=influxdb -storage_driver_db=cadvisor -storage_driver_host=influxsrv:8086\n  ports:\n    - \"9090:8080\"\n  volumes:\n    - /:/rootfs:ro\n    - /var/run:/var/run:rw\n    - /sys:/sys:ro\n    - /var/lib/docker/:/var/lib/docker:ro\n  links:\n    - influxdb:influxsrv\n```\nThis worked properly and the logs show the events being properly written.\ninfluxdb_1     | [http] 2016/02/05 19:25:47 172.17.0.2 - root [05/Feb/2016:19:25:47 +0000] POST /write?consistency=&db=cadvisor&precision=&rp= HTTP/1.1 204 0 - cAdvisor/0.20.5 41f62cc0-cc3e-11e5-8007-000000000000 24.01661ms\nThe latest tag still results in the 404 errors.\ninfluxdb_1     | [wal] 2016/02/05 18:41:22 write to index of partition 1 took 1.451106ms\ncadvisor_1     | E0205 18:41:26.374037       1 memory.go:91] failed to write stats to influxDb - Server returned (404): 404 page not found\ninfluxdb_1     | [wal] 2016/02/05 18:41:32 Flush due to idle. Flushing 10 series with 10 points and 494 bytes from partition 1\n.  You're very welcome @nuxman!\n. @vishh The screenshot above is from the /docker endpoint... That shows the same docker ID... The query above is in the production remote influxDB host, so no communication problem so far... \n. @vishh sure! Let me collect the values and report...\n. Hi @vishh \nHere's the output of pprdnpmas300.corp.ccc.net:6090/api/v2.1/stats/docker/0e1e84b6291f9e6b8b328b43eab7901bb2b82fff78bbf41248299bfc1a9fc112\nhttps://gist.github.com/marcellodesales/ca8922b8fa6cf3d42f7c\n. @vishh even running on the same host, what does cadvisor needs to talk to the daemon? I'm starting cadvisor from Docker Compose... Is that a possible problem?\ncadvisor:\n  image: google/cadvisor:v0.20.5\n  command: -storage_driver=influxdb \n    -storage_driver_db=cadvisor -storage_driver_host=xyz.abc.194.188:8086 \n    -storage_driver_user=cadvisor -storage_driver_password=xxxxxxx \n    -storage_driver_secure=False\n  restart: always\n  ports:\n    - \"6090:8080\"\n  volumes:\n    - /:/rootfs:ro\n    - /var/run:/var/run:rw\n    - /sys:/sys:ro\n    - /var/lib/docker/:/var/lib/docker:ro\n. @vishh let me collect those... \n[root@pprdnpmas300 npmo-server]# DATA_CENTER=qydc NPMO_ROLE=primary ENV=prd LOG_FORMAT=docker docker-compose -f services/ccc-ops.yml logs cadvisor\nAttaching to services_cadvisor_1\ncadvisor_1 | I0223 10:20:31.083785       1 storagedriver.go:42] Using backend storage type \"influxdb\"\ncadvisor_1 | I0223 10:20:31.084018       1 storagedriver.go:44] Caching stats in memory for 2m0s\ncadvisor_1 | I0223 10:20:31.084147       1 manager.go:131] cAdvisor running in container: \"/docker/7c7e52082f6c4f0d8b96f3a55f0f423a291df0f70a53725dd8802af22904c672\"\ncadvisor_1 | I0223 10:20:31.330678       1 fs.go:107] Filesystem partitions: map[/dev/mapper/docker-253:1-134296580-53464f81ac6740416c0e9ab0d66d551a102823e1dd3761308522004419ca3e62:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/53464f81ac6740416c0e9ab0d66d551a102823e1dd3761308522004419ca3e62 major:253 minor:7 fsType: blockSize:0} /dev/mapper/vg_root-lv_root:{mountpoint:/rootfs major:253 minor:1 fsType: blockSize:0} /dev/mapper/vg_appdata-lvm_appdata:{mountpoint:/rootfs/app major:253 minor:0 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-b54edf37a7493b3aa7ad081c44f3153c1137c0dc75935061f65784a2de4a5c31:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/b54edf37a7493b3aa7ad081c44f3153c1137c0dc75935061f65784a2de4a5c31 major:253 minor:5 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-7761918b90b9d879aaa4dbbaa8fcc237b4a8c398ce8005102f6fbeaaa9364153:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/7761918b90b9d879aaa4dbbaa8fcc237b4a8c398ce8005102f6fbeaaa9364153 major:253 minor:6 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-2bff51cd0def977b86089b215b5ea60f42fb2a9f1b16f811621218fceee6690c:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/2bff51cd0def977b86089b215b5ea60f42fb2a9f1b16f811621218fceee6690c major:253 minor:9 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-dd34e8042e298836e9d1557f74428f69a1d20b6b1dc9560c2d799cfad0ce9c52:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/dd34e8042e298836e9d1557f74428f69a1d20b6b1dc9560c2d799cfad0ce9c52 major:253 minor:10 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-76a8bf6d879db577f40cba5537e4af0dd6e9d262cdc8062ca9735e53da764113:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/76a8bf6d879db577f40cba5537e4af0dd6e9d262cdc8062ca9735e53da764113 major:253 minor:12 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-7c7e52082f6c4f0d8b96f3a55f0f423a291df0f70a53725dd8802af22904c672:{mountpoint:/ major:253 minor:17 fsType: blockSize:0} /dev/sda1:{mountpoint:/rootfs/boot major:8 minor:1 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-e15e615b909cea26ed32345363d86a13adde5599bea79f60f282035f9aa73e5f:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/e15e615b909cea26ed32345363d86a13adde5599bea79f60f282035f9aa73e5f major:253 minor:4 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-03800a7b7a98d17a31fc6fe041fe0871edfa89086e606f4d02baf6338cbc6978:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/03800a7b7a98d17a31fc6fe041fe0871edfa89086e606f4d02baf6338cbc6978 major:253 minor:3 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-72d562779765fd40e4ea4f483de33a52cb08809480a300091d3ba4eb9390ea6b:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/72d562779765fd40e4ea4f483de33a52cb08809480a300091d3ba4eb9390ea6b major:253 minor:8 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-7efd1ec3fcb5f528204acf8c46a4535fd4dc77fb6103b90fb3e2aea0e62bff73:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/7efd1ec3fcb5f528204acf8c46a4535fd4dc77fb6103b90fb3e2aea0e62bff73 major:253 minor:13 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-0e1e84b6291f9e6b8b328b43eab7901bb2b82fff78bbf41248299bfc1a9fc112:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/0e1e84b6291f9e6b8b328b43eab7901bb2b82fff78bbf41248299bfc1a9fc112 major:253 minor:16 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-0b140c1ae4d2aa4f40b2cfe5aa9e74277945bc1abfd090321bf79de0c8a47e67:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/0b140c1ae4d2aa4f40b2cfe5aa9e74277945bc1abfd090321bf79de0c8a47e67 major:253 minor:11 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-27564bd828e2ac540ce14cb8d4fbf63418afe60a76fc847be0fcc9e5a7a40025:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/27564bd828e2ac540ce14cb8d4fbf63418afe60a76fc847be0fcc9e5a7a40025 major:253 minor:14 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-1e4476a9a20d2d27a9afed0d7dd924285531488af1baaa5226c5747b71f6b342:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/1e4476a9a20d2d27a9afed0d7dd924285531488af1baaa5226c5747b71f6b342 major:253 minor:15 fsType: blockSize:0}]\ncadvisor_1 | I0223 10:20:31.352636       1 manager.go:166] Machine: {NumCores:4 CpuFrequency:2128000 MemoryCapacity:16651481088 MachineID:0855d19bf2cf40d7a70cb45145950327 SystemUUID:42066107-97D5-B986-DDC9-8E3CE26464A0 BootID:42e1e5fe-d80e-4a42-8a20-5018316c8343 Filesystems:[{Device:/dev/mapper/docker-253:1-134296580-03800a7b7a98d17a31fc6fe041fe0871edfa89086e606f4d02baf6338cbc6978 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-72d562779765fd40e4ea4f483de33a52cb08809480a300091d3ba4eb9390ea6b Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-7efd1ec3fcb5f528204acf8c46a4535fd4dc77fb6103b90fb3e2aea0e62bff73 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-0b140c1ae4d2aa4f40b2cfe5aa9e74277945bc1abfd090321bf79de0c8a47e67 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-27564bd828e2ac540ce14cb8d4fbf63418afe60a76fc847be0fcc9e5a7a40025 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-1e4476a9a20d2d27a9afed0d7dd924285531488af1baaa5226c5747b71f6b342 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-0e1e84b6291f9e6b8b328b43eab7901bb2b82fff78bbf41248299bfc1a9fc112 Capacity:107320705024} {Device:/dev/mapper/vg_root-lv_root Capacity:40488808448} {Device:/dev/mapper/vg_appdata-lvm_appdata Capacity:31568424960} {Device:/dev/mapper/docker-253:1-134296580-b54edf37a7493b3aa7ad081c44f3153c1137c0dc75935061f65784a2de4a5c31 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-53464f81ac6740416c0e9ab0d66d551a102823e1dd3761308522004419ca3e62 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-2bff51cd0def977b86089b215b5ea60f42fb2a9f1b16f811621218fceee6690c Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-dd34e8042e298836e9d1557f74428f69a1d20b6b1dc9560c2d799cfad0ce9c52 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-76a8bf6d879db577f40cba5537e4af0dd6e9d262cdc8062ca9735e53da764113 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-7c7e52082f6c4f0d8b96f3a55f0f423a291df0f70a53725dd8802af22904c672 Capacity:107320705024} {Device:/dev/sda1 Capacity:264941568} {Device:/dev/mapper/docker-253:1-134296580-e15e615b909cea26ed32345363d86a13adde5599bea79f60f282035f9aa73e5f Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-7761918b90b9d879aaa4dbbaa8fcc237b4a8c398ce8005102f6fbeaaa9364153 Capacity:107320705024}] DiskMap:map[253:13:{Name:dm-13 Major:253 Minor:13 Size:107374182400 Scheduler:none} 253:14:{Name:dm-14 Major:253 Minor:14 Size:107374182400 Scheduler:none} 253:15:{Name:dm-15 Major:253 Minor:15 Size:107374182400 Scheduler:none} 253:8:{Name:dm-8 Major:253 Minor:8 Size:107374182400 Scheduler:none} 253:9:{Name:dm-9 Major:253 Minor:9 Size:107374182400 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:deadline} 253:10:{Name:dm-10 Major:253 Minor:10 Size:107374182400 Scheduler:none} 253:17:{Name:dm-17 Major:253 Minor:17 Size:107374182400 Scheduler:none} 253:6:{Name:dm-6 Major:253 Minor:6 Size:107374182400 Scheduler:none} 253:7:{Name:dm-7 Major:253 Minor:7 Size:107374182400 Scheduler:none} 253:16:{Name:dm-16 Major:253 Minor:16 Size:107374182400 Scheduler:none} 253:2:{Name:dm-2 Major:253 Minor:2 Size:107374182400 Scheduler:none} 253:4:{Name:dm-4 Major:253 Minor:4 Size:107374182400 Scheduler:none} 253:5:{Name:dm-5 Major:253 Minor:5 Size:107374182400 Scheduler:none} 8:16:{Name:sdb Major:8 Minor:16 Size:32212254720 Scheduler:deadline} 253:12:{Name:dm-12 Major:253 Minor:12 Size:107374182400 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:40508588032 Scheduler:none} 253:11:{Name:dm-11 Major:253 Minor:11 Size:107374182400 Scheduler:none} 253:3:{Name:dm-3 Major:253 Minor:3 Size:107374182400 Scheduler:none} 2:0:{Name:fd0 Major:2 Minor:0 Size:4096 Scheduler:deadline} 253:0:{Name:dm-0 Major:253 Minor:0 Size:32208060416 Scheduler:none}] NetworkDevices:[{Name:bridge0 MacAddress:12:32:bd:42:78:86 Speed:0 Mtu:1500} {Name:eth0 MacAddress:00:50:56:86:01:9b Speed:10000 Mtu:1500} {Name:eth1 MacAddress:00:50:56:86:25:f5 Speed:10000 Mtu:1500}] Topology:[{Id:0 Memory:17179336704 Cores:[{Id:0 Threads:[0] Caches:[]} {Id:1 Threads:[1] Caches:[]} {Id:2 Threads:[2] Caches:[]} {Id:3 Threads:[3] Caches:[]}] Caches:[{Size:25165824 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown}\ncadvisor_1 | I0223 10:20:31.354104       1 manager.go:172] Version: {KernelVersion:3.10.0-327.10.1.el7.x86_64 ContainerOsVersion:Alpine Linux v3.2 DockerVersion:1.9.1 CadvisorVersion:0.20.5 CadvisorRevision:9aa348f}\ncadvisor_1 | I0223 10:20:31.355327       1 factory.go:210] System is using systemd\ncadvisor_1 | I0223 10:20:31.633518       1 factory.go:252] Registering Docker factory\ncadvisor_1 | I0223 10:20:31.644199       1 factory.go:94] Registering Raw factory\ncadvisor_1 | I0223 10:20:32.006032       1 manager.go:1000] Started watching for new ooms in manager\ncadvisor_1 | W0223 10:20:32.006974       1 manager.go:239] Could not configure a source for OOM detection, disabling OOM events: exec: \"journalctl\": executable file not found in $PATH\ncadvisor_1 | I0223 10:20:32.007723       1 manager.go:252] Starting recovery of all containers\ncadvisor_1 | I0223 10:20:32.031139       1 manager.go:257] Recovery completed\ncadvisor_1 | I0223 10:20:32.060334       1 cadvisor.go:106] Starting cAdvisor version: 0.20.5-9aa348f on port 8080\nFrom here, I see the first error... cadvisor_1 | E0224 23:10:26.798628       1 \nmemory.go:91] failed to write stats to influxDb - Post \nhttp://10.180.194.188:8086/write?consistency=&db=cadvisor&precision=&rp=: dial tcp \n10.180.194.188:8086: getsockopt: connection refused\ncadvisor_1 | W0223 10:33:32.173015       1 container.go:354] Failed to get RecentStats(\"/docker/53464f81ac6740416c0e9ab0d66d551a102823e1dd3761308522004419ca3e62\") while determining the next housekeeping: unable to find data for container /docker/53464f81ac6740416c0e9ab0d66d551a102823e1dd3761308522004419ca3e62\ncadvisor_1 | W0223 10:37:12.719889       1 container.go:341] Failed to create summary reader for \"/docker/69145724a3bc855385119e2552c70064bb1c42be7857ad6413dff4cfcb41793b\": none of the resources are being tracked.\ncadvisor_1 | W0223 10:37:15.991376       1 container.go:341] Failed to create summary reader for \"/docker/64cba7065673a8025785ead336e5c405dc97ec1a2ca6d66fe5b758f59bcc765b\": none of the resources are being tracked.\ncadvisor_1 | W0223 10:37:17.376885       1 container.go:341] Failed to create summary reader for \"/docker/978117c3fbf2e814dd1907f8b109cf1b1b683325b4f250799fa462eec9ed820f\": none of the resources are being tracked.\ncadvisor_1 | W0223 10:37:29.095995       1 container.go:341] Failed to create summary reader for \"/docker/78a032e4a4b4332238427ea20b4cdb44ee8ed44ac483131e5cba18b746513c0c\": none of the resources are being tracked.\ncadvisor_1 | E0224 09:39:55.290111       1 memory.go:91] failed to write stats to influxDb - Post http://10.180.194.188:8086/write?consistency=&db=cadvisor&precision=&rp=: dial tcp 10.180.194.188:8086: i/o timeout\ncadvisor_1 | E0224 09:40:55.385761       1 memory.go:91] failed to write stats to influxDb - Post http://10.180.194.188:8086/write?consistency=&db=cadvisor&precision=&rp=: dial tcp 10.180.194.188:8086: i/o timeout\ncadvisor_1 | E0224 09:41:25.464999       1 memory.go:91] failed to write stats to influxDb - Post http://10.180.194.188:8086/write?consistency=&db=cadvisor&precision=&rp=: dial tcp 10.180.194.188:8086: getsockopt: connection refused\ncadvisor_1 | E0224 09:42:24.102757       1 memory.go:91] failed to write stats to influxDb - Post http://10.180.194.188:8086/write?consistency=&db=cadvisor&precision=&rp=: read tcp 192.168.4.15:45426->10.180.194.188:8086: read: connection reset by peer\ncadvisor_1 | E0224 21:50:30.219446       1 memory.go:91] failed to write stats to influxDb - {\"error\":\"timeout\"}\ncadvisor_1 | E0224 23:07:26.649126       1 memory.go:91] failed to write stats to influxDb - Post http://10.180.194.188:8086/write?consistency=&db=cadvisor&precision=&rp=: dial tcp 10.180.194.188:8086: getsockopt: connection refused\ncadvisor_1 | E0224 23:08:26.655138       1 memory.go:91] failed to write stats to influxDb - Post http://10.180.194.188:8086/write?consistency=&db=cadvisor&precision=&rp=: dial tcp 10.180.194.188:8086: getsockopt: connection refused\ncadvisor_1 | E0224 23:09:26.696369       1 memory.go:91] failed to write stats to influxDb - Post http://10.180.194.188:8086/write?consistency=&db=cadvisor&precision=&rp=: dial tcp 10.180.194.188:8086: getsockopt: connection refused\ncadvisor_1 | E0224 23:10:26.798628       1 memory.go:91] failed to write stats to influxDb - Post http://10.180.194.188:8086/write?consistency=&db=cadvisor&precision=&rp=: dial tcp 10.180.194.188:8086: getsockopt: connection refused\nThen, I think the server went up again, and lost the database... {\"error\":\"database not found: \\\"cadvisor\\\"\"}\ncadvisor_1 | E0225 01:42:39.394727       1 memory.go:91] failed to write stats to influxDb - Post http://10.180.194.188:8086/write?consistency=&db=cadvisor&precision=&rp=: dial tcp 10.180.194.188:8086: getsockopt: connection refused\ncadvisor_1 | E0225 01:43:40.328899       1 memory.go:91] failed to write stats to influxDb - {\"error\":\"database not found: \\\"cadvisor\\\"\"}\ncadvisor_1 | E0225 01:44:39.815301       1 memory.go:91] failed to write stats to influxDb - {\"error\":\"database not found: \\\"cadvisor\\\"\"}\ncadvisor_1 | E0225 01:45:39.809165       1 memory.go:91] failed to write stats to influxDb - {\"error\":\"database not found: \\\"cadvisor\\\"\"}\nQuestion\nCould this be the source of the problems? We used to see the full container names before... And now only the IDs... \n. @vishh Let me try... \n. @vishh I just restarted it... \n- I don't see any information about connecting to influxDB in the beginning... Shouldn't we add such a log entry during the bootstrap?\n- I don't see the systemd entry in the log anymore...\n- How do I verify \n[root@pprdnpmas300 npmo-server]# DATA_CENTER=qydc NPMO_ROLE=primary ENV=prd LOG_FORMAT=docker docker-compose -f services/ccc-ops.yml logs cadvisor\nAttaching to services_cadvisor_1\ncadvisor_1 | I0225 23:37:37.344136       1 storagedriver.go:42] Using backend storage type \"influxdb\"\ncadvisor_1 | I0225 23:37:37.344471       1 storagedriver.go:44] Caching stats in memory for 2m0s\ncadvisor_1 | I0225 23:37:37.344614       1 manager.go:131] cAdvisor running in container: \"/docker/99c215438f9a5c4e1f1e7575ecbf851d94770d61ed2a00123355feda6c2cd91a\"\ncadvisor_1 | I0225 23:37:37.669296       1 fs.go:107] Filesystem partitions: map[/dev/mapper/docker-253:1-134296580-25aa982db0a4752ad913be7340abc85dfa8349d91618cd42f09f7a5751dfbc46:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/25aa982db0a4752ad913be7340abc85dfa8349d91618cd42f09f7a5751dfbc46 major:253 minor:23 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-978117c3fbf2e814dd1907f8b109cf1b1b683325b4f250799fa462eec9ed820f:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/978117c3fbf2e814dd1907f8b109cf1b1b683325b4f250799fa462eec9ed820f major:253 minor:25 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-2fc22b33b5c67a1db9a814c5fed1aa3e8799968763bdf905e1df8b29cfac7b59:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/2fc22b33b5c67a1db9a814c5fed1aa3e8799968763bdf905e1df8b29cfac7b59 major:253 minor:28 fsType: blockSize:0} /dev/mapper/vg_root-lv_root:{mountpoint:/rootfs major:253 minor:1 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-69145724a3bc855385119e2552c70064bb1c42be7857ad6413dff4cfcb41793b:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/69145724a3bc855385119e2552c70064bb1c42be7857ad6413dff4cfcb41793b major:253 minor:20 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-e206626d1b51575662fc2c014c15fe31e6d10c0f47897a732398ca5562cd7250:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/e206626d1b51575662fc2c014c15fe31e6d10c0f47897a732398ca5562cd7250 major:253 minor:30 fsType: blockSize:0} /dev/mapper/vg_appdata-lvm_appdata:{mountpoint:/rootfs/app major:253 minor:0 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-9db0ea44969ebd620684e4356cec163e6e077474191ad30ed3f78c8234a9b847:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/9db0ea44969ebd620684e4356cec163e6e077474191ad30ed3f78c8234a9b847 major:253 minor:19 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-6d56b1ff2c8e121aefd0409b1b4425554239cea15555b43fcb2eb09637b06327:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/6d56b1ff2c8e121aefd0409b1b4425554239cea15555b43fcb2eb09637b06327 major:253 minor:27 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-b5fc4d10a46d8b4d804ae1c436a33e0596afa4924342f95c6d8f5ab4b9b44101:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/b5fc4d10a46d8b4d804ae1c436a33e0596afa4924342f95c6d8f5ab4b9b44101 major:253 minor:22 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-4f903da5431d21781d920fbd4fd4171caf1406f1bc339961245eefb09cd17053:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/4f903da5431d21781d920fbd4fd4171caf1406f1bc339961245eefb09cd17053 major:253 minor:29 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-3f3a458b1863d150968f00fbd33b301724b37918a83392747c7828d5a10ac422:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/3f3a458b1863d150968f00fbd33b301724b37918a83392747c7828d5a10ac422 major:253 minor:31 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-78a032e4a4b4332238427ea20b4cdb44ee8ed44ac483131e5cba18b746513c0c:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/78a032e4a4b4332238427ea20b4cdb44ee8ed44ac483131e5cba18b746513c0c major:253 minor:32 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-99c215438f9a5c4e1f1e7575ecbf851d94770d61ed2a00123355feda6c2cd91a:{mountpoint:/ major:253 minor:34 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-64cba7065673a8025785ead336e5c405dc97ec1a2ca6d66fe5b758f59bcc765b:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/64cba7065673a8025785ead336e5c405dc97ec1a2ca6d66fe5b758f59bcc765b major:253 minor:24 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-f2b8d149a4017325daa7c6f4db48703bcafc2d3e10dd5240465907c797b97628:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/f2b8d149a4017325daa7c6f4db48703bcafc2d3e10dd5240465907c797b97628 major:253 minor:26 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-93e89198343ea01fa98a53a7131d7d4f8c43e75da1fd80023c85e764560d5c37:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/93e89198343ea01fa98a53a7131d7d4f8c43e75da1fd80023c85e764560d5c37 major:253 minor:33 fsType: blockSize:0} /dev/sda1:{mountpoint:/rootfs/boot major:8 minor:1 fsType: blockSize:0} /dev/mapper/docker-253:1-134296580-486efb1a8a5a2740258fc3641a9c580d3e3c04acba53f6546138cb196aeb102f:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/486efb1a8a5a2740258fc3641a9c580d3e3c04acba53f6546138cb196aeb102f major:253 minor:21 fsType: blockSize:0}]\ncadvisor_1 | I0225 23:37:37.692723       1 manager.go:166] Machine: {NumCores:4 CpuFrequency:2128000 MemoryCapacity:16651481088 MachineID:0855d19bf2cf40d7a70cb45145950327 SystemUUID:42066107-97D5-B986-DDC9-8E3CE26464A0 BootID:42e1e5fe-d80e-4a42-8a20-5018316c8343 Filesystems:[{Device:/dev/mapper/docker-253:1-134296580-64cba7065673a8025785ead336e5c405dc97ec1a2ca6d66fe5b758f59bcc765b Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-6d56b1ff2c8e121aefd0409b1b4425554239cea15555b43fcb2eb09637b06327 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-b5fc4d10a46d8b4d804ae1c436a33e0596afa4924342f95c6d8f5ab4b9b44101 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-4f903da5431d21781d920fbd4fd4171caf1406f1bc339961245eefb09cd17053 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-3f3a458b1863d150968f00fbd33b301724b37918a83392747c7828d5a10ac422 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-78a032e4a4b4332238427ea20b4cdb44ee8ed44ac483131e5cba18b746513c0c Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-99c215438f9a5c4e1f1e7575ecbf851d94770d61ed2a00123355feda6c2cd91a Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-486efb1a8a5a2740258fc3641a9c580d3e3c04acba53f6546138cb196aeb102f Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-f2b8d149a4017325daa7c6f4db48703bcafc2d3e10dd5240465907c797b97628 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-93e89198343ea01fa98a53a7131d7d4f8c43e75da1fd80023c85e764560d5c37 Capacity:107320705024} {Device:/dev/sda1 Capacity:264941568} {Device:/dev/mapper/docker-253:1-134296580-69145724a3bc855385119e2552c70064bb1c42be7857ad6413dff4cfcb41793b Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-25aa982db0a4752ad913be7340abc85dfa8349d91618cd42f09f7a5751dfbc46 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-978117c3fbf2e814dd1907f8b109cf1b1b683325b4f250799fa462eec9ed820f Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-2fc22b33b5c67a1db9a814c5fed1aa3e8799968763bdf905e1df8b29cfac7b59 Capacity:107320705024} {Device:/dev/mapper/vg_root-lv_root Capacity:40488808448} {Device:/dev/mapper/docker-253:1-134296580-9db0ea44969ebd620684e4356cec163e6e077474191ad30ed3f78c8234a9b847 Capacity:107320705024} {Device:/dev/mapper/docker-253:1-134296580-e206626d1b51575662fc2c014c15fe31e6d10c0f47897a732398ca5562cd7250 Capacity:107320705024} {Device:/dev/mapper/vg_appdata-lvm_appdata Capacity:31568424960}] DiskMap:map[253:12:{Name:dm-12 Major:253 Minor:12 Size:107374182400 Scheduler:none} 253:16:{Name:dm-16 Major:253 Minor:16 Size:107374182400 Scheduler:none} 253:18:{Name:dm-18 Major:253 Minor:18 Size:107374182400 Scheduler:none} 253:21:{Name:dm-21 Major:253 Minor:21 Size:107374182400 Scheduler:none} 253:10:{Name:dm-10 Major:253 Minor:10 Size:107374182400 Scheduler:none} 253:17:{Name:dm-17 Major:253 Minor:17 Size:107374182400 Scheduler:none} 253:20:{Name:dm-20 Major:253 Minor:20 Size:107374182400 Scheduler:none} 253:28:{Name:dm-28 Major:253 Minor:28 Size:107374182400 Scheduler:none} 253:11:{Name:dm-11 Major:253 Minor:11 Size:107374182400 Scheduler:none} 253:2:{Name:dm-2 Major:253 Minor:2 Size:107374182400 Scheduler:none} 253:24:{Name:dm-24 Major:253 Minor:24 Size:107374182400 Scheduler:none} 253:5:{Name:dm-5 Major:253 Minor:5 Size:107374182400 Scheduler:none} 253:26:{Name:dm-26 Major:253 Minor:26 Size:107374182400 Scheduler:none} 253:27:{Name:dm-27 Major:253 Minor:27 Size:107374182400 Scheduler:none} 253:8:{Name:dm-8 Major:253 Minor:8 Size:107374182400 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:deadline} 8:16:{Name:sdb Major:8 Minor:16 Size:32212254720 Scheduler:deadline} 253:0:{Name:dm-0 Major:253 Minor:0 Size:32208060416 Scheduler:none} 253:13:{Name:dm-13 Major:253 Minor:13 Size:107374182400 Scheduler:none} 253:3:{Name:dm-3 Major:253 Minor:3 Size:107374182400 Scheduler:none} 253:30:{Name:dm-30 Major:253 Minor:30 Size:107374182400 Scheduler:none} 253:14:{Name:dm-14 Major:253 Minor:14 Size:107374182400 Scheduler:none} 253:25:{Name:dm-25 Major:253 Minor:25 Size:107374182400 Scheduler:none} 253:32:{Name:dm-32 Major:253 Minor:32 Size:107374182400 Scheduler:none} 253:4:{Name:dm-4 Major:253 Minor:4 Size:107374182400 Scheduler:none} 253:6:{Name:dm-6 Major:253 Minor:6 Size:107374182400 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:40508588032 Scheduler:none} 253:19:{Name:dm-19 Major:253 Minor:19 Size:107374182400 Scheduler:none} 253:29:{Name:dm-29 Major:253 Minor:29 Size:107374182400 Scheduler:none} 253:31:{Name:dm-31 Major:253 Minor:31 Size:107374182400 Scheduler:none} 253:34:{Name:dm-34 Major:253 Minor:34 Size:107374182400 Scheduler:none} 253:9:{Name:dm-9 Major:253 Minor:9 Size:107374182400 Scheduler:none} 2:0:{Name:fd0 Major:2 Minor:0 Size:4096 Scheduler:deadline} 253:15:{Name:dm-15 Major:253 Minor:15 Size:107374182400 Scheduler:none} 253:22:{Name:dm-22 Major:253 Minor:22 Size:107374182400 Scheduler:none} 253:23:{Name:dm-23 Major:253 Minor:23 Size:107374182400 Scheduler:none} 253:33:{Name:dm-33 Major:253 Minor:33 Size:107374182400 Scheduler:none} 253:7:{Name:dm-7 Major:253 Minor:7 Size:107374182400 Scheduler:none}] NetworkDevices:[{Name:bridge0 MacAddress:22:27:d2:62:84:d9 Speed:0 Mtu:1500} {Name:eth0 MacAddress:00:50:56:86:01:9b Speed:10000 Mtu:1500} {Name:eth1 MacAddress:00:50:56:86:25:f5 Speed:10000 Mtu:1500}] Topology:[{Id:0 Memory:17179336704 Cores:[{Id:0 Threads:[0] Caches:[]} {Id:1 Threads:[1] Caches:[]} {Id:2 Threads:[2] Caches:[]} {Id:3 Threads:[3] Caches:[]}] Caches:[{Size:25165824 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown}\ncadvisor_1 | I0225 23:37:37.694066       1 manager.go:172] Version: {KernelVersion:3.10.0-327.10.1.el7.x86_64 ContainerOsVersion:Alpine Linux v3.2 DockerVersion:1.9.1 CadvisorVersion:0.20.5 CadvisorRevision:9aa348f}\ncadvisor_1 | I0225 23:37:37.967196       1 factory.go:252] Registering Docker factory\ncadvisor_1 | I0225 23:37:37.978313       1 factory.go:94] Registering Raw factory\ncadvisor_1 | I0225 23:37:38.643410       1 manager.go:1000] Started watching for new ooms in manager\ncadvisor_1 | W0225 23:37:38.644505       1 manager.go:239] Could not configure a source for OOM detection, disabling OOM events: exec: \"journalctl\": executable file not found in $PATH\ncadvisor_1 | I0225 23:37:38.678312       1 manager.go:252] Starting recovery of all containers\ncadvisor_1 | I0225 23:37:38.778898       1 manager.go:257] Recovery completed\ncadvisor_1 | I0225 23:37:38.812340       1 cadvisor.go:106] Starting cAdvisor version: 0.20.5-9aa348f on port 8080\nThe /docker/ cadvisor endpoint still not showing the name of the containers...\n/docker\nroot docker\nSubcontainers\n/docker/25aa982db0a4752ad913be7340abc85dfa8349d91618cd42f09f7a5751dfbc46\n/docker/2fc22b33b5c67a1db9a814c5fed1aa3e8799968763bdf905e1df8b29cfac7b59\n/docker/3f3a458b1863d150968f00fbd33b301724b37918a83392747c7828d5a10ac422\n/docker/486efb1a8a5a2740258fc3641a9c580d3e3c04acba53f6546138cb196aeb102f\n/docker/4f903da5431d21781d920fbd4fd4171caf1406f1bc339961245eefb09cd17053\n. Also, I started seeing the following:\nIs there a to setup the rate and maybe decrease the throughput?\ncadvisor_1 | E0225 23:43:38.044736       1 memory.go:91] failed to write stats to influxDb - {\"error\":\"write failed: engine: write points: write throughput too high. backoff and retry\"}\ncadvisor_1 | E0225 23:44:38.171171       1 memory.go:91] failed to write stats to influxDb - {\"error\":\"write failed: engine: write points: write throughput too high. backoff and retry\"}\ncadvisor_1 | E0225 23:45:38.365485       1 memory.go:91] failed to write stats to influxDb - {\"error\":\"write failed: engine: write points: write throughput too high. backoff and retry\"}\ncadvisor_1 | E0225 23:46:38.780019       1 memory.go:91] failed to write stats to influxDb - {\"error\":\"write failed: engine: write points: write throughput too high. backoff and retry\"}\ncadvisor_1 | E0225 23:47:38.909916       1 memory.go:91] failed to write stats to influxDb - {\"error\":\"write failed: engine: write points: write throughput too high. backoff and retry\"}\ncadvisor_1 | E0225 23:48:39.045459       1 memory.go:91] failed to write stats to influxDb - {\"error\":\"write failed: engine: write points: write throughput too high. backoff and retry\"}\ncadvisor_1 | E0225 23:49:39.002101       1 memory.go:91] failed to write stats to influxDb - {\"error\":\"write failed: engine: write points: write throughput too high. backoff and retry\"}\n. @vishh Hummm Yeah our influxdb team might have added something to it as they mentioned that our cadvisor from only 4 hosts was pushing events at the rate of 500/s.\nOk, let me add the flag and see if we can collect anything else...\n. @vishh how? :) Any pointers?\n. @vishh awesome\n. @vishh I added the flag --vmodule=*=4 and I'm getting a failure... failed to collect filesystem stats - du command failed on  with output du: : No such file or directory.\nThe current command is as follows:\n-storage_driver=influxdb -storage_driver_db=cadvisor -storage_driver_host=10.180.194.188:8086 -storage_driver_user=cadvisor -storage_driver_passwor\nd=yH70hxwgSI -storage_driver_secure=False --nosystemd --vmodule=*=4\nHere's the log...\ncadvisor_1 | I0226 00:34:16.858215       1 manager.go:257] Recovery completed\ncadvisor_1 | I0226 00:34:16.858488       1 container.go:386] Start housekeeping for container \"/docker/e206626d1b51575662fc2c014c15fe31e6d10c0f47897a732398ca5562cd7250\"\ncadvisor_1 | I0226 00:34:16.899940       1 cadvisor.go:106] Starting cAdvisor version: 0.20.5-9aa348f on port 8080\ncadvisor_1 | I0226 00:35:11.174489       1 container.go:403] [/docker/4f903da5431d21781d920fbd4fd4171caf1406f1bc339961245eefb09cd17053] Housekeeping took 1.090983293s\ncadvisor_1 | I0226 00:35:16.783319       1 fsHandler.go:96] failed to collect filesystem stats - du command failed on  with output du: : No such file or directory\ncadvisor_1 |  - exit status 1\ncadvisor_1 | I0226 00:35:16.784036       1 fsHandler.go:96] failed to collect filesystem stats - du command failed on  with output du: : No such file or directory\ncadvisor_1 |  - exit status 1\ncadvisor_1 | I0226 00:35:16.789255       1 fsHandler.go:96] failed to collect filesystem stats - du command failed on  with output du: : No such file or directory\ncadvisor_1 |  - exit status 1\ncadvisor_1 | I0226 00:35:16.793966       1 fsHandler.go:96] failed to collect filesystem stats - du command failed on  with output du: : No such file or directory\ncadvisor_1 |  - exit status 1\ncadvisor_1 | I0226 00:35:16.802140       1 fsHandler.go:96] failed to\n. I will be adding the rate as described at https://github.com/google/cadvisor/issues/1074#issue-128088993... \n@vishh Any points from the logs above?\n. @vishh Sounds good... I will be busy for the next 1-2hs but I will post another Gist with it... Thanks a lot Vish!\n. ",
    "matcornic": "@vishh one solution would be using Viper instead of flags. It would be more flexible than just simple flags. But it's a breaking change, because with pflag, single dashes does not have same meaning than classic flags. Equivalent would be using double dashed --option\n. I signed it!. ",
    "cboggs": "Awesome! I imagine we could overpower this issue with more nodes on better storage, but that seems less than ideal.\nHappy to help and experiment if I can!\n. We pulled the canary build of cadvisor last Friday and noticed that while metrics are indeed only emitted once every 60 seconds per host now, we've lost the 1-second resolution that we had before. Was this intentional? A 1-minute resolution could work, but we were diggin' the 1s. :-)\nThanks!\n. Good point. I think that actually helps show what the issue might be. Are you passing in an explicit \"time\" value for each data point when POSTing them to InfluxDB? If not, InfluxDB will just assign the timestamp at which it receives the POST. Had a similar issue with our tool for pumping JMX metrics in. \nThese are the results I'm seeing:\nselect timestamp from stats where container_name='/docker/8edd06d4a3b86c216892ea2c71ab2f5edb18a4d3066fba4cbe16df2bd5aa7d79' and time > now() - 1m\nstats\ntime    sequence_number timestamp\n1409239410529   908817450001    2014-08-28T15:23:29.759912942Z\n1409239410529   908817040001    2014-08-28T15:23:28.755808325Z\n1409239410529   908816630001    2014-08-28T15:23:27.751775712Z\n1409239410529   908816220001    2014-08-28T15:23:26.747419999Z\n1409239410529   908815810001    2014-08-28T15:23:25.740713973Z\n1409239410529   908815390001    2014-08-28T15:23:24.724002711Z\nThe stats.timestamp values are legit, but InfluxDB just assigns the same epoch to all of that minute's data points since they don't specify a \"time\" value on the way in. :-)\nThanks!\n. That's what I suspected too, but then #405 seemed to suggest otherwise.\nHere's the result set for a single container... surprised to see null values interleaved with valid values. I overlooked that last time in a rush. Oops. I imagine that is a contributing factor, would kind of explain the wild swings...\n[Edit: tried without the unnecessary 'group by' as well, still see the null values, just didn't grab a new screenshot]\n\n. Sure thing as soon as I get off the bus. Is just a 1-minute sample\nsufficient?\nOn Feb 9, 2015 3:35 PM, \"Vish Kannan\" notifications@github.com wrote:\n\nThats what I suspect @vmarmol https://github.com/vmarmol. @cboggs\nhttps://github.com/cboggs can you post the entire line in InfluxDB (all\nthe stats) for container 'spoon-daves-dev'?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/499#issuecomment-73605500.\n. Sorry for the delay, forgot about errands to run. :-)\nSounds like you're onto something... Sequences that have FS stats are lacking CPU stats:\n\n\n. Yea, not sure why Grafana is acting that way. Will see if I can figure anything out there.\nAlso :+1:  to what @jchauncey suggested. I've generally found much better query performance and flexibility when every metric is its own series. I'd love to give something more than an anecdote, but I haven't run any sort of benchmarks. Another day perhaps...\n. :+1: \nI noticed a pretty hefty load increase when I had Cadvisor running yesterday on a docker host with a couple TB of local storage, and a couple dozen du's running pretty much all the time. Stopped the Cadvisor container and the du's went away, and load returned to expected values.\nHappy to gather more detailed stats if needed.\n. ",
    "ntquyen": "@vishh I know this issue is old and it must have been fixed. But I still don't know how to replace the cadvisor container id in the machine column with the hostname. Can you give me a hint? I'm using cAdvisor v0.22.0 and influxdb 0.10.3 in CoreOS host v899.13.0\n. Never mind, I have figured out to overwrite the container hostname using --hostname flag in docker run\n. I'm running google/cadvisor:v0.25.0 and got the same issue:\nI0425 08:24:04.857137       1 factory.go:115] Factory \"docker\" was unable to handle container \"/system.slice/run-docker-netns-a05ba53671e3.mount\"\nI0425 08:24:04.857152       1 factory.go:108] Factory \"systemd\" can handle container \"/system.slice/run-docker-netns-a05ba53671e3.mount\", but ignoring.\nI0425 08:24:04.857169       1 manager.go:867] ignoring container \"/system.slice/run-docker-netns-a05ba53671e3.mount\"\nI0425 08:24:04.857415       1 factory.go:115] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-containers-3cb6d9ece7247f29df946bba4735753092be13089a3f3c30552f54e0bdfa3171-shm.mount\"\ndocker version:\n```\nClient:\n Version:      1.12.6\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   d5236f0\n Built:        Fri Mar 31 02:09:07 2017\n OS/Arch:      linux/amd64\nServer:\n Version:      1.12.6\n API version:  1.24\n Go version:   go1.6.3\n Git commit:   d5236f0\n Built:        Fri Mar 31 02:09:07 2017\n OS/Arch:      linux/amd64\ndocker info:\nContainers: 38\n Running: 38\n Paused: 0\n Stopped: 0\nImages: 36\nServer Version: 1.12.6\nStorage Driver: overlay2\n Backing Filesystem: extfs\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: bridge null overlay host\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: seccomp selinux\nKernel Version: 4.9.16-coreos-r1\nOperating System: Container Linux by CoreOS 1298.7.0 (Ladybug)\nOSType: linux\nArchitecture: x86_64\nCPUs: 4\nTotal Memory: 4.722 GiB\nName: coreos_k8s2_33\nID: NVAQ:W7FV:VJFC:XEI7:GCWR:PCKX:LDCU:TWJ3:Y65U:CSPX:N4IL:343R\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): false\nRegistry: https://index.docker.io/v1/\nInsecure Registries:\n 127.0.0.0/8\n```. ",
    "raydawg1": "docker logs aabf0f96bc61\nF0820 20:45:06.554746 00001 cadvisor.go:60] raw registration failed: failed to find cgroup mounts for the raw factory.\ngoroutine 1 [running]:\ngithub.com/golang/glog.stacks(0xc2100c3600, 0xc210074180, 0x77, 0xb2)\n        /home/vishnuk/go/src/github.com/golang/glog/glog.go:726 +0xb1\ngithub.com/golang/glog.(_loggingT).output(0xf92120, 0xc200000003, 0xc2100740c0)\n        /home/vishnuk/go/src/github.com/golang/glog/glog.go:677 +0x1ff\ngithub.com/golang/glog.(_loggingT).printf(0xf92120, 0x3, 0x933d70, 0x1c, 0x7f47d7082e98, ...)\n        /home/vishnuk/go/src/github.com/golang/glog/glog.go:635 +0x157\ngithub.com/golang/glog.Fatalf(0x933d70, 0x1c, 0x7f47d7082e98, 0x1, 0x1)\n        /home/vishnuk/go/src/github.com/golang/glog/glog.go:1033 +0x64\nmain.main()\n        /home/vishnuk/go/src/github.com/google/cadvisor/cadvisor.go:60 +0x42a\nDid a docker rm for this container and recreated with:\nsudo docker run   --volume=/var/run:/var/run:rw   --volume=/sys:/sys:ro   --volume=/var/lib/docker/:/var/lib/docker:ro   --publish=8080:8080   --name=cadvisor   google/cadvisor:latest\nF0820 20:52:29.666566 00001 cadvisor.go:60] raw registration failed: failed to find cgroup mounts for the raw factory.\ngoroutine 1 [running]:\ngithub.com/golang/glog.stacks(0xc2100bc600, 0xc210075180, 0x77, 0xb2)\n        /home/vishnuk/go/src/github.com/golang/glog/glog.go:726 +0xb1\ngithub.com/golang/glog.(_loggingT).output(0xf92120, 0xc200000003, 0xc2100750c0)\n        /home/vishnuk/go/src/github.com/golang/glog/glog.go:677 +0x1ff\ngithub.com/golang/glog.(_loggingT).printf(0xf92120, 0x3, 0x933d70, 0x1c, 0x7fa717bbee98, ...)\n        /home/vishnuk/go/src/github.com/golang/glog/glog.go:635 +0x157\ngithub.com/golang/glog.Fatalf(0x933d70, 0x1c, 0x7fa717bbee98, 0x1, 0x1)\n        /home/vishnuk/go/src/github.com/golang/glog/glog.go:1033 +0x64\nmain.main()\n        /home/vishnuk/go/src/github.com/google/cadvisor/cadvisor.go:60 +0x42a\n\nI have the same exact issue on another Centos 6.5 host as well, so it's repeatable (at least in our environment.)\n. Got the rpm from epel, do you recommend a different repo?\n. I don't see LXC mentioned in docker inspect:\ndocker inspect google/cadvisor\n[{\n    \"Architecture\": \"amd64\",\n    \"Author\": \"dengnan@google.com vmarmol@google.com\",\n    \"Comment\": \"\",\n    \"Config\": {\n        \"AttachStderr\": false,\n        \"AttachStdin\": false,\n        \"AttachStdout\": false,\n        \"Cmd\": [\n            \"-log_dir\",\n            \"/\"\n        ],\n        \"CpuShares\": 0,\n        \"Cpuset\": \"\",\n        \"Domainname\": \"\",\n        \"Entrypoint\": [\n            \"/usr/bin/cadvisor\"\n        ],\n        \"Env\": [\n            \"HOME=/\",\n            \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n        ],\n        \"ExposedPorts\": {\n            \"8080/tcp\": {}\n        },\n        \"Hostname\": \"127c64114b9a\",\n        \"Image\": \"0600ad28425dac3004aed611b73e4fd3d7824ee37f2ea550d3f527cf9cf130af\",\n        \"Memory\": 0,\n        \"MemorySwap\": 0,\n        \"NetworkDisabled\": false,\n        \"OnBuild\": [],\n        \"OpenStdin\": false,\n        \"PortSpecs\": null,\n        \"StdinOnce\": false,\n        \"Tty\": false,\n        \"User\": \"\",\n        \"Volumes\": null,\n        \"WorkingDir\": \"\"\n    },\n    \"Container\": \"b51403b2065201ded07d906f7ed22e93aa5ed92fefc6fe26327b378bb1162700\",\n    \"ContainerConfig\": {\n        \"AttachStderr\": false,\n        \"AttachStdin\": false,\n        \"AttachStdout\": false,\n        \"Cmd\": [\n            \"/bin/sh\",\n            \"-c\",\n            \"#(nop) CMD [-log_dir /]\"\n        ],\n        \"CpuShares\": 0,\n        \"Cpuset\": \"\",\n        \"Domainname\": \"\",\n        \"Entrypoint\": [\n            \"/usr/bin/cadvisor\"\n        ],\n        \"Env\": [\n            \"HOME=/\",\n            \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n        ],\n        \"ExposedPorts\": {\n            \"8080/tcp\": {}\n        },\n        \"Hostname\": \"127c64114b9a\",\n        \"Image\": \"0600ad28425dac3004aed611b73e4fd3d7824ee37f2ea550d3f527cf9cf130af\",\n        \"Memory\": 0,\n        \"MemorySwap\": 0,\n        \"NetworkDisabled\": false,\n        \"OnBuild\": [],\n        \"OpenStdin\": false,\n        \"PortSpecs\": null,\n        \"StdinOnce\": false,\n        \"Tty\": false,\n        \"User\": \"\",\n        \"Volumes\": null,\n        \"WorkingDir\": \"\"\n    },\n    \"Created\": \"2014-08-12T21:07:59.396602027Z\",\n    \"DockerVersion\": \"1.1.2\",\n    \"Id\": \"8624e82072990386c61e44fa79ecd8e1d8fa9080e11570652c4d4c01d2937aaf\",\n    \"Os\": \"linux\",\n    \"Parent\": \"0600ad28425dac3004aed611b73e4fd3d7824ee37f2ea550d3f527cf9cf130af\",\n    \"Size\": 0\n}\n/proc/mounts shows:\ndocker run busybox cat /proc/mounts\nrootfs / rootfs rw 0 0\n/dev/mapper/docker-253:3-9273414-c25674571112dacfa34458bc03acdfacc8f4acf756ad0e89e1123b8e0cd9585c / ext4 rw,relatime,barrier=1,stripe=16,data=ordered,discard 0 0\nproc /proc proc rw,nosuid,nodev,noexec,relatime 0 0\nsysfs /sys sysfs ro,relatime 0 0\ntmpfs /dev tmpfs rw,nosuid,mode=755 0 0\nshm /dev/shm tmpfs rw,nosuid,nodev,noexec,relatime,size=65536k 0 0\ndevpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=666 0 0\n/dev/mapper/VolGroup00-VarVol /.dockerinit ext3 ro,relatime,errors=continue,user_xattr,acl,barrier=1,data=ordered 0 0\n/dev/mapper/VolGroup00-RootVol /etc/resolv.conf ext3 ro,relatime,errors=continue,user_xattr,acl,barrier=1,data=ordered 0 0\n/dev/mapper/VolGroup00-VarVol /etc/hostname ext3 ro,relatime,errors=continue,user_xattr,acl,barrier=1,data=ordered 0 0\n/dev/mapper/VolGroup00-VarVol /etc/hosts ext3 ro,relatime,errors=continue,user_xattr,acl,barrier=1,data=ordered 0 0\nproc /proc/sys proc ro,nosuid,nodev,noexec,relatime 0 0\nproc /proc/sysrq-trigger proc ro,nosuid,nodev,noexec,relatime 0 0\nproc /proc/irq proc ro,nosuid,nodev,noexec,relatime 0 0\nproc /proc/bus proc ro,nosuid,nodev,noexec,relatime 0 0\ntmpfs /proc/kcore tmpfs rw,nosuid,mode=755 0 0\n. I do see dirs inside of /cgroups on the host but there's no mount such as:\nnone        /cgroup        cgroup        defaults    0    0\n. ll /sys/fs/cgroup\nls: cannot access /sys/fs/cgroup: No such file or directory\n[root@infra001 /]# docker run   --volume=/var/run:/var/run:rw  --volume=/sys:/sys:ro  --volume=/var/lib/docker/:/var/lib/docker:ro --volume=/cgroup:/cgroup  --publish=8080:8080 --detach=true --name=cadvisor  google/cadvisor:latest\ne4b97a6b29a74239928b0e039c68cbc22ea1a0027b38ebc4031bb6e98bc3dc4c\n[root@infra001 /]# docker ps\nCONTAINER ID        IMAGE                           COMMAND                CREATED             STATUS              PORTS                    NAMES\ne4b97a6b29a7        google/cadvisor:latest          /usr/bin/cadvisor -l   4 seconds ago       Up 3 seconds        0.0.0.0:8080->8080/tcp   cadvisor                                                               \nBingo!  I guess I do have one of those systems. :)\n. thanks guys\n. ",
    "caglar10ur": "ping @vmarmol :)\n. Sure, which one do you prefer?\n. The default limit is 16EB but I'm fine showing it as \"unlimited\" if you prefer that.\nQuestion is, do you want to keep the current value (MaxInt64 - which is half of the default value aka 8EB) or want me to replace it with MaxUint64 (16EB)? Or maybe we should set some other upper limit cause EB (or PB) is still huge.\n. OK got it. Will address these two comments and refresh the pull request - hopefully today.\n. ",
    "tifayuki": "I was thinking that we can do docker ps -s to get the current size of each container. Then look up its image tree to get the size of its each dependent layers. Then the disk usage is the sum of those sizes. But I don't know if we should count the shared layer size or not.\n. I have a weave interface in my vm, but I think the downloading is through eth0 not the weave\n one \n```\ndocker0   Link encap:Ethernet  HWaddr 56:84:7a:fe:97:99\n          inet addr:172.17.42.1  Bcast:0.0.0.0  Mask:255.255.0.0\n          inet6 addr: fe80::5484:7aff:fefe:9799/64 Scope:Link\n          UP BROADCAST MULTICAST  MTU:1500  Metric:1\n          RX packets:587874 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:654651 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:446995998 (446.9 MB)  TX bytes:2743470508 (2.7 GB)\neth0      Link encap:Ethernet  HWaddr 08:00:27:4a:9d:51\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\n          inet6 addr: fe80::a00:27ff:fe4a:9d51/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:6780704 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:1703466 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000 \n          RX bytes:7762621321 (7.7 GB)  TX bytes:228248162 (228.2 MB)\nlo        Link encap:Local Loopback\n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:222993 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:222993 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:432554810 (432.5 MB)  TX bytes:432554810 (432.5 MB)\nweave     Link encap:Ethernet  HWaddr 7a:4e:b1:2d:c1:b7\n          inet6 addr: fe80::784e:b1ff:fe2d:c1b7/64 Scope:Link\n          UP BROADCAST MULTICAST  MTU:65535  Metric:1\n          RX packets:8 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:31 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:536 (536.0 B)  TX bytes:4757 (4.7 KB)\n```\n. In one of my VMs:\nls /sys/class/net\ndocker0  dummy0  eth0  eth1  lo  veth1e3d936  veth3751dd9  veth3882a0e  veth3f1c930  veth3fda928  veth446bad0  veth537ab23  veth752d712  veth826ff72  veth9b00ba7  vetha5d75f0  vetha6531d3  vethc91e7de  vethf266906  vethf41ce83  vethwepl20171  weave\nthe first interface is docker0\nso cadvisor returns the statistics on docker0, right?\n. Got it.\nThank you\n. 3558  docker run \\\\\n 3559    --volume=/:/rootfs:ro \\\\\n 3560    --volume=/var/run:/var/run:rw \\\\\n 3561    --volume=/sys:/sys:ro \\\\\n 3562    --volume=/var/lib/docker/:/var/lib/docker:ro \\\\\n 3563    --publish=8080:8080 \\\\\n 3564    --name=cadvisor \\\\\n 3565    -ti --rm\\\\\n 3566    google/cadvisor:latest\nthe distro is ubuntu:trusty\nLinux hfeng-VirtualBox 3.16.0-37-generic #49~14.04.1-Ubuntu SMP Thu Apr 30 10:52:28 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\n\u279c  ~  docker ps -a\nCONTAINER ID        IMAGE                    COMMAND               CREATED             STATUS              PORTS                    NAMES\nb6e78e54cba2        google/cadvisor:latest   \"/usr/bin/cadvisor\"   33 minutes ago      Up 8 seconds        0.0.0.0:8080->8080/tcp   cadvisor\n\u279c  ~  docker inspect -f '{{.HostConfig.NetworkMode}}' b6e78\ndefault\n. ~  docker info\nContainers: 1\nImages: 258\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 276\n Dirperm1 Supported: true\nExecution Driver: native-0.2\nLogging Driver: json-file\nKernel Version: 3.16.0-37-generic\nOperating System: Ubuntu 14.04.2 LTS\nCPUs: 2\nTotal Memory: 7.612 GiB\nName: hfeng-VirtualBox\nID: AM7D:YLWM:I5QN:MKPN:JOPI:SOMT:WVYD:ON5Y:NC6S:73YA:WUSJ:VUGD\nUsername: xxxx\nRegistry: https://index.docker.io/v1/\nWARNING: No swap limit support\n. Thank you for the update.\nBTW: is the binary on https://github.com/google/cadvisor/releases/tag/0.18.0 the correct version?\n. @jimmidyson \nThe new binary works fine. \nThank you\n. ",
    "discordianfish": "@vishh Are they? At least they aren't available on GKE's k8s. I have container_fs_usage_bytes but without a total or free space metric this isn't useful.. @vmarmol Let me know if I can help. I've wrote the prometheus container exporter: https://github.com/docker-infra/container_exporter and would be happy to deprecated it in favor our prometheus support in cadvisor since using libcontainer in a 'engine agnostic' way isn't trivial and the api breaks every other day..\n. @vmarmol Yes, that sounds good. Can't say much about the cavisor internals (introspection or not) but from a prometheus perspective implementing a Collector would be the right way (similar to container_exporter).\n. @vmarmol Ah nice! Ok, I think this all looks pretty straight forward:\n- Implement a Collector which holds a reference to the manager to get the stats\n- Instantiate the Collector in main.go(?) and register the metrics handler\nOne thing though: Does SubcontainersInfo() return all containers as a flat list or do I need to somehow recursively traversal all Subcontainers?\n. Oh right, the ContainerInfo doens't include the Docker metadata. Yes, we need to solve that somehow, otherwise the metrics won't be very useful.\n. @vmarmol I'm working on a PR now and we can discuss details there. Once thing that I think would be useful is to have the image name and tag as labels. That's something we probably need to add somehow.\n. @vmarmol I still think having the container name as label would be useful, probably also https://docs.docker.com/userguide/labels-custom-metadata/\nIf you think this makes sense, I can open an issue and maybe start work on this if time permits.\n. Opened #688\n. @brian-brazil I don't think we want such 'map metric' in this case. The docker container name in aliases is what you are usually use when refering to a container. Always through through that map sounds painful.\n. @vmarmol Do you have some input before I work on the remaining bits and look into getting a CLA signed?\n. > >  I don't think the names really change over time. Theoretically the alias can, although I haven't seen it in practice.\n\nOkay, sounds like we can get away with the name as a label on everything and an alias as I suggest.\n\n@brian-brazil What's exactly wrong with changing label values? We still have the id in here which exists as long as the container exists, aka this gets stale after the container is gone. But I assume that's okay, isn't it?:\n. Okay, I've changed the PR according to the discussion. I also changed it as @beorn7 suggested and did some moving around.\n@vmarmol Are you okay with the package structure as it is right now?\nBeside that, we submitted a CLA with me (Johannes Ziemke) listed some time ago for a kubernetes contribution. Do I need to somehow sign-off the commits? The googlebot sounds like identification happens via email addresses but IIRC back then we were asked to submit a real name.\n. @brian-brazil Hrm.. maybe this is a misunderstanding. The timeseries, for example 'memory usage of container X' will have the same labels for the lifetime of 'container X'. But if you, for example, delete and recreate the container with the same name etc the id and therfor the label changes. I assume this is okay because it's in fact a new container.\n. Nice, thanks @juliusv - LGTM!\n. Awesome, then it can be closed!\n. If I'm not mistake, this should be in 0.26.1, correct? But I still don't have the metrics:\n$ curl -s http://10.116.2.7:8080/metrics|egrep '^cadvisor_version_info|^container_fs_io'|sed 's/container_fs_io.*} //'\ncadvisor_version_info{cadvisorRevision=\"d19cc94\",cadvisorVersion=\"v0.26.1\",dockerVersion=\"1.11.2\",kernelVersion=\"4.4.52+\",osVersion=\"Alpine Linux v3.4\"} 1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0. @ZhiqinYang Maybe differences in how the cgroups are mounted?. Good point, will do that.\n. It's always hard to answer those things.. Any other opinions? But I'm fine with using one metric.\n. You mean instead of creating them via NewXX in NewPrometheusCollector() I would create them in here? But I assume I need to create them before Describe() gets called?\n. Just realized that CpuStats's PerCpu doesn't include system vs user usage. Only the overall cpu usage is split into user / system.\n- Should we keep both metrics?\n- Ignore the overall usage and live without system usage?\n- Change CpuStats to include system / user usage in PerCpu (obviously the hardest option to implement)\n. It is? Because this kind of error isn't about processing some items for which a error rate would be useful but something affecting overall scrape? We probably should add that to the best practices in the prometheus docs.\n. Keep in mind it's still prefixed by container, so would be 'container_errors_total'. Still, probably not the best name but maybe something like 'container_exporter_errors_total' is better than metric_errors? Other opinions? @brian-brazil usually has a good feeling for those things :)\n. Ah, then ignore my comment above and let's just keep both metrics for now? Seem the best we can do. @brian-brazil Naming, do you have some suggestion? \n. Hrmm I think having the image as a dimension is actually very useful. Usually people have their version numbers / git-shas in the image tag, so you often want to group by that to compare one version of your containers to another version.  Do you think it's hard to get the image name with the current design of cadvisor?\nFor now, I'll remove the image and we can look later into introducing it somehow.\n. yay :)\n. I've changed it to (container_)export_errors (removed the total) because it's not a Gauge as suggestd by @brian-brazil \n. I've added it back now. Let me know if you want me to change the name. \n. But it's not really an error while scraping? But I'm fine with changing it.\n. I've changed it now to scrape_error with same description as in node-exporter/textfile\n. Good catch, I've fixed it. I think it's important to differentiate user vs system so would rather keep it. On the long run we could try to get the user vs system dimension into the per_cpu metric but not sure that's even exposed by the kernel. For now, I think it's fine to keep both.\n. Oh sure, thanks!\n. Good catch, seems like I missed that when rebasing. I'm not very happy with container_cpu(usagetype)_seconds_total but it's probably better the best option.\n. ",
    "AndyBerman": "I ran the container that way and every second it spits this error out (even if I'm not using Cadvisor).\nE0829 19:18:54.697272 00001 handler.go:200] Libcontainer config not found for container \"/docker/861df8d1edb83bd7bb37e5f478f440689eb6475fdada8f2adac08a49d75d95be\"\nE0829 19:18:55.697105 00001 handler.go:200] Libcontainer config not found for container \"/docker/861df8d1edb83bd7bb37e5f478f440689eb6475fdada8f2adac08a49d75d95be\"\nAlso, I was able to get this error with docker 1.12 too.\n. docker info:\nContainers: 9\nImages: 83\nStorage Driver: devicemapper\n Pool Name: docker-253:4-1638402-pool\n Pool Blocksize: 64 Kb\n Data file: /docker/lib/devicemapper/devicemapper/data\n Metadata file: /docker/lib/devicemapper/devicemapper/metadata\n Data Space Used: 4010.4 Mb\n Data Space Total: 102400.0 Mb\n Metadata Space Used: 5.4 Mb\n Metadata Space Total: 2048.0 Mb\nExecution Driver: native-0.2\nKernel Version: 2.6.32-431.17.1.el6.x86_64\nOperating System: \nAs root, I see no directories underneath /var/lib/docker.  That directory is empty.\n. Ah, I've started docker on a different mount point - /docker\nThis path does exist:  /docker/lib/execdriver/native,   But it's empty.\nSo will cAdvisor work if docker is started on another file system?\n. So as a workaround, I went to /var/lib and created a symbolic link:\n    ln -s /docker/lib docker\nThat worked, and now cAdvisor works :)\nThanks for your help.  Hopefully you can add a switch or something else in the future.\n. Awesome, thanks for the help.\n. Oops, reopening since it's assigned.\n. ",
    "mboussaa": "How can I change the default timestamp granularity? by default it is 1s right? \n. @rjnagal how did you do finally to get the %cpu usage of one container? \nDid you consider cpu frequency to compute it? \n. what does express exactly Derivative of cpu_cumulative_usage? Why use derivate and not mean?\n. Thank you. For the memory usage should I use the same behavior ?\n. In that case, \"select mean(memory_usage) from stats where container_name='foo' group by time(2s)\" \nis it correct ? \n. @vishh  :  the memory usage aggregated by cadvisor is the total hot (working set) memory + cold pages + swap that's right?\n. Working set memory == RSS ?\n. the hot or total memory is sent  to backend storage like influxdb ? \n. So memory_usage (total) = RAM + Swap ? because @vishh said that cAdvisor is not supporting memory+swap accounting as of now here https://github.com/google/cadvisor/issues/913 \nIf hot is RSS + cache what would be the total memory ?\n. \nYes I did that and as you can see, cAdvisor is working very well however, it does not dump data to the specified db in influxdb.\nIf I run something like \"select count(*) from stats\" in influxdb Web UI it shows \" ERROR : can't find series : stats\"\nI don't know if the problem is more clear.\n. Thank you for the clarification.\nWhen trying to get logs : \n\nFor influx DB :\n\nNB: I am using a VirtualBoxVM with Ubunto as a virtual machine. That may help to understand the problem.\n. I0504 17:59:04.438751 00001 storagedriver.go:90] Using backend storage type \"influxdb\"\nI0504 17:59:04.442380 00001 storagedriver.go:94] Caching 60 stats in memory\nI0504 17:59:04.442776 00001 manager.go:106] cAdvisor running in container: \"/docker/91e3c58b9a69174c65244a774a0ee01f32820e3bcf162561922a8350d9484be4\"\nI0504 17:59:04.445607 00001 fs.go:87] Filesystem partitions: map[/dev/mapper/ubuntu--vg-root:{mountpoint:/rootfs major:252 minor:0} /dev/sda1:{mountpoint:/rootfs/boot major:8 minor:1}]\nI0504 17:59:04.453717 00001 machine.go:223] Couldn't collect info from any of the files in \"/etc/machine-id,/var/lib/dbus/machine-id\"\nI0504 17:59:04.469639 00001 manager.go:127] Machine: {NumCores:1 CpuFrequency:2587640 MemoryCapacity:10290925568 MachineID: SystemUUID:E8E478D3-3092-49A8-A802-13BA926862AE BootID:20331896-31b4-466a-8684-ae8cbb954470 Filesystems:[{Device:/dev/mapper/ubuntu--vg-root Capacity:22072389632} {Device:/dev/sda1 Capacity:246755328}] DiskMap:map[252:0:{Name:dm-0 Major:252 Minor:0 Size:22561161216 Scheduler:none} 252:1:{Name:dm-1 Major:252 Minor:1 Size:10536091648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:33388118016 Scheduler:deadline}] NetworkDevices:[{Name:eth0 MacAddress:08:00:27:19:78:32 Speed:1000 Mtu:1500}] Topology:[{Id:0 Memory:10290925568 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}]}\nI0504 17:59:04.477555 00001 manager.go:134] Version: {KernelVersion:3.16.0-31-generic ContainerOsVersion:Buildroot 2014.02 DockerVersion:1.6.0 CadvisorVersion:0.12.0}\nI0504 17:59:04.509139 00001 factory.go:226] Registering Docker factory\nI0504 17:59:04.524481 00001 factory.go:60] Registering Raw factory\nI0504 17:59:04.525402 00001 manager.go:872] Started watching for new ooms in manager\nE0504 17:59:04.525517 00001 manager.go:199] Failed to start OOM watcher, will not get OOM events: exec: \"journalctl\": executable file not found in $PATH\nI0504 17:59:04.544673 00001 manager.go:212] Starting recovery of all containers\nI0504 17:59:04.582335 00001 manager.go:217] Recovery completed\nI0504 17:59:04.597763 00001 cadvisor.go:90] Starting cAdvisor version: \"0.12.0\" on port 8080\nE0504 18:00:04.562568 00001 memory.go:84] failed to write stats to influxDb - Post http://127.0.0.1:8086/db/databaseName/series?u=root&p=root&time_precision=u: dial tcp 127.0.0.1:8086: connection refused\nE0504 18:01:04.581246 00001 memory.go:84] failed to write stats to influxDb - Post http://127.0.0.1:8086/db/databaseName/series?u=root&p=root&time_precision=u: dial tcp 127.0.0.1:8086: connection refused\nE0504 18:02:04.572020 00001 memory.go:84] failed to write stats to influxDb - Post http://127.0.0.1:8086/db/databaseName/series?u=root&p=root&time_precision=u: dial tcp 127.0.0.1:8086: connection refused\nE0504 18:03:04.579496 00001 memory.go:84] failed to write stats to influxDb - Post http://127.0.0.1:8086/db/databaseName/series?u=root&p=root&time_precision=u: dial tcp 127.0.0.1:8086: connection refused\nE0504 18:04:04.579458 00001 memory.go:84] failed to write stats to influxDb - Post http://127.0.0.1:8086/db/databaseName/series?u=root&p=root&time_precision=u: dial tcp 127.0.0.1:8086: connection refused\nE0504 18:05:04.595141 00001 memory.go:84] failed to write stats to influxDb - Post http://127.0.0.1:8086/db/databaseName/series?u=root&p=root&time_precision=u: dial tcp 127.0.0.1:8086: connection refused\nE0504 18:06:04.603950 00001 memory.go:84] failed to write stats to influxDb - Post http://127.0.0.1:8086/db/databaseName/series?u=root&p=root&time_precision=u: dial tcp 127.0.0.1:8086: connection refused\nE0504 18:07:04.608570 00001 memory.go:84] failed to write stats to influxDb - Post http://127.0.0.1:8086/db/databaseName/series?u=root&p=root&time_precision=u: dial tcp 127.0.0.1:8086: connection refused\nE0504 18:08:05.565142 00001 memory.go:84] failed to write stats to influxDb - Post http://127.0.0.1:8086/db/databaseName/series?u=root&p=root&time_precision=u: dial tcp 127.0.0.1:8086: connection refused\nE0504 18:09:05.573991 00001 memory.go:84] failed to write stats to influxDb - Post http://127.0.0.1:8086/db/databaseName/series?u=root&p=root&time_precision=u: dial tcp 127.0.0.1:8086: connection refused\nE0504 18:10:05.581251 00001 memory.go:84] failed to write stats to influxDb - Post http://127.0.0.1:8086/db/databaseName/series?u=root&p=root&time_precision=u: dial tcp 127.0.0.1:8086: connection refused\nE0504 18:11:05.603416 00001 memory.go:84] failed to write stats to influxDb - Post http://127.0.0.1:8086/db/databaseName/series?u=root&p=root&time_precision=u: dial tcp 127.0.0.1:8086: connection refused\nE0504 18:12:05.583855 00001 memory.go:84] failed to write stats to influxDb - Post http://127.0.0.1:8086/db/databaseName/series?u=root&p=root&time_precision=u: dial tcp 127.0.0.1:8086: connection refused\nE0504 18:13:05.598008 00001 memory.go:84] failed to write stats to influxDb - Post http://127.0.0.1:8086/db/databaseName/series?u=root&p=root&time_precision=u: dial tcp 127.0.0.1:8086: connection refused\nE0504 18:14:05.604288 00001 memory.go:84] failed to write stats to influxDb - Post http://127.0.0.1:8086/db/databaseName/series?u=root&p=root&time_precision=u: dial tcp 127.0.0.1:8086: connection refused\n. +1. In fact, yes that's true. I should write the Host IP. It works right now :) That's great. I am planning to link these data to grafana in order to display the graphs of each query. \nIs there any documentation of the data structure? such as columns names, definition...\n. Yes, I think it is very important to document the data structure that does store cAdvisor. In the testing community, these values are very useful for evaluating test suites. Any way, I making some experiments using cAdvisor. For sure, I will open many new issues :)\n. @vmarmol what the unit of measure for CPU utilization. you said that it \"outputs in number of cores\". But how can you define this measure ? thanks\n. So as @huikang said, is that the way of calculating the %CPU? @vmarmol  is the % accurate ? \n. I divided by the total number of cores in my machine which is 4. I followed the discussion here https://github.com/google/cadvisor/issues/679 . \nAnd to answer your question, doing the math on top of the derivative causes the error\n. So far, cAdvisor can only report raw data in billionths of a core is it right? \nGetting the percentage of CPU utilized is not so obvious I guess \n. for the memory, it is the same thing?\n. Any progress on this issue ? \n. when I would like to get the derivative value of cpu usage over the container's lifetime and without defining a group by (time) like:\nselect derivative(cpu_cumulative_usage) from stats where container_name='execution'\nit calculates the derivative over all time stamp ((cpu_cum_end - cpu_cum_start)/(time_end - time_start))? Because it shows only one derivative value\nafter that, I am dividing  this value by 1000,000,000 to get the % of cpu consumption\nI am not sure that is right\ncc @vmarmol  @vishh \nThanks\n. according to this https://github.com/google/cadvisor/issues/679#issuecomment-99529852\nyou know, I need to get one single value that represents the cpu usage of one container.\nIs that right @vishh ? \nThanks\n. https://github.com/docker/libcontainer/issues/165 does not work\n. Thank you for your answer. I changed the image however now, when I create a new database on influxDB. I get this message:\n\nI can not understand this issue. The only thing I have changed is the influxDB docker image.\ndocker logs influxdb doesn't report any issue.\n. Ok so the solution is to clear the web browser cache. \nYou can close this issue.\nThank you for your help \n. Yes. In fact I am running a container with ressources limitation. For instance, my container runs on cpu 0 and 1 (--cpuset=0,1) so the \"cpu_cumulative_usage\" in influxdb refers to which core ? mean of cpu 0 and 1?\nFor example, my machine has 4 cores: \n%CPU0=90%\n%CPU1=100%\n%CPU1=0%\n%CPU2=0%\nHow are you calculating cpu_cumulative_usage?\nThanks in advance.\n. Any advance on Exporting CPU utilization per core to InfluxDB?\n. If I well understood:\nmemory_usage= RAM (hot+cold)+SWAP\nworking_set= RAM (hot)\nThat's right? \nI am not sure if SWAP memory is included in \"memory_usage\"\nCan you confirm that?\n. @solomatnikov You can enable/disable swapping for your container and check the memory consumption using cAdvisor\n. any updates on that integration?\n. @vishh  +1. Thank you. It would be great to add that to the storage documentation. You can close this issue.\n. These stats are related to disk metrics. The are is no issue I think\n. So I have to run a cadvisor container within Rasberry device. it is not possible to gather performance of rasberry containers from my host machine?\n. Yes I mean from my linux machine is it possible to monitor RPI containers ?\n. the only way I found to get the cpu usage per container is doing like following in Java:\n```\n public static double getInstantPercentCPUUsage(JSONObject obj0) throws ParseException {\n    JSONObject obj = obj0.getJSONObject(obj0.keys().next()) ;\n    JSONObject firstValue = obj.getJSONArray(\"stats\").getJSONObject(obj.getJSONArray(\"stats\").length()-1);\n    JSONObject secondValue = obj.getJSONArray(\"stats\").getJSONObject(obj.getJSONArray(\"stats\").length()-2);\n    String ts1 = firstValue.getString(\"timestamp\");\n    String ts2 = secondValue.getString(\"timestamp\");\n\n    int cpuUsage1 = firstValue.getJSONObject(\"cpu\").getJSONObject(\"usage\").getInt(\"total\");\n    int cpuUsage2 = secondValue.getJSONObject(\"cpu\").getJSONObject(\"usage\").getInt(\"total\");\n\n    DateTimeFormatter parser = ISODateTimeFormat.dateTime();\n    DateTime dt1 = parser.parseDateTime(ts1);\n    DateTime dt2 = parser.parseDateTime(ts2);\n\n    DateTimeFormatter formatter = DateTimeFormat.mediumDateTime();\n    return (cpuUsage1-cpuUsage2)/((dt1.getSecondOfDay()-dt2.getSecondOfDay())*10000000) ;\n}\n\n```\nIt is the derivative cpu usage over time. I don't know if that right !!\n. So for instance cAdvisor image does not handle ARM image? \n. Sorry but this did not solve the problem. \n. it is the same error. previously it was working like I described. I don't know why it does not work any more. I think the link is needed when using docker compose \n. @RRAlex I think nobody tested the impact of cAdvisor on system overhead. We have to capture resource usage metrics from cAdvisor and compare them to linux monitoring tools. Perfect. From my side, I tested the memory overhead induced by plugging my containers to cadvisor and influxdb. It was very negligible. \nI compared the following values:\n- the memory usage of my job gathered from cadvisor and saved in influxdb: select max(memory_usage) from stats where...\n- non-containerized solution: time -v (my job) ... and I gathered the \"max resident size\"  \nThese 2 values where roughly equal. Sounds good like this?. @RRAlex can you provide us your new settings? So that I can use it for the future. ",
    "cnf": "Ubuntu 14.04 (3.13.0-35-generic amd64)\nand about 10 running containers (all single process)\n. 10 docker containers, and nothing else.\ncAdvisor never really drops below 10%, and is often above 15%\n(if you need any specific information / data gathering on this host, please do let me know. it's a test setup so nothing important lives there)\ncontainers\n$ docker ps | awk '{print $2}'\ndockerfile/elasticsearch:latest\ngoogle/cadvisor:beta\narcus/kibana:latest\ntutum/grafana:latest\nhekatest:latest\ntutum/influxdb:latest\nfrosquin/softether:latest\nfrosquin/nginx:latest\ncrosbymichael/dockerui:latest\ntop\nPID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND           \n14891 root      20   0  269692  13868   4608 S  12.2  0.3  77:57.22 cadvisor          \n 1698 root      20   0 1450204  28192   8768 S   2.3  0.6  18:25.90 influxdb\n. 1%, maybe? 10% to 15% is an awful lot...\nand more is better, of course!\n. Aha, very nice! I wonder how much impact each has.\n. I have the same problem. It seems cAdvisor isn't capable of any dns lookups.\nI would like SRV lookups, but I'll settle for just any sort of DNS lookups, for now!\n. I think this is because the container is so bare, it has no mechanisms for DNS resolution. And go is trying to use the OS resolving mechanisms.\n. @emmanuel If it is not actually connecting to your influx database with a resolvable name, and it connects with an IP address, would you not call that failing?\nIt seems to fail silently, but failing none the less...\n. Right, this makes sense.\nWithout an IP go can't actually do the name resolution itself, so it needs to fall back to the host...\nCan you strace cadvisor and see what exactly it needs, so it could be included in the official image?\n. @emmanuel I tried your container, and it runs fine for a few minutes and then starts crashing.\ncrash log\n. Linux seshat 3.13.0-35-generic #62-Ubuntu SMP Fri Aug 15 01:58:42 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux\ndefault ubuntu 14.04.1 LTS kernel, booted with swapaccount=1 cgroup_enable=memory for docker \n. ",
    "jalateras": "Are you able to reduce the frequency at which observations are collected? Does that impact the CPU usage of cdvisor?\n. ",
    "damm": "I can say this is one of the reasons I don't use Google CAdvisor.\nI deployed it on a empty CoreOS kvm guest; there was no docker containers running no activity what so ever and it was using 75% cpu usage.\nNote, Acceptable cpu usage is generally under 1%; profiling has a cost but when it's cost is too high it's not used.\n. ",
    "aquam8": "Me too, it's been a bit painful to find the problem. Looking forward to a (re)solution - excuse the punt ;-)\n. ",
    "emmanuel": "I have confirmed that cadvisor doesn't fail on name resolution when a (statically-linked) binary built on a Linux host is placed into a Docker image based on Ubuntu Core 14.04. \nAt least, it doesn't fail in the same way; data is not being written to InfluxDB, but that's a different issue (I trust). \nThe image I built is available on Docker hub: nordstrom/cadvisor:0.3.0-snapshot.\n. BTW, Go outputted these (hopefully relevant/enlightening) warnings when I built the binary:\n```\nvagrant@vagrant-ubuntu-trusty-64:~/go/src/github.com/google/cadvisor$ $GOPATH/bin/godep go build -a --ldflags '-extldflags \"-static\"' github.com/google/cadvisor\ngithub.com/google/cadvisor\n/var/tmp/go-link-CeHFDD/000001.o: In function mygetpwuid_r':\n/usr/local/go/src/pkg/os/user/lookup_unix.go:72: warning: Using 'getpwnam_r' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking\n/var/tmp/go-link-CeHFDD/000001.o: In function_cgo_0db69e1b5445_Cfunc_mygetpwuid_r':\n/usr/local/go/src/pkg/os/user/lookup_unix.go:27: warning: Using 'getpwuid_r' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking\n/var/tmp/go-link-CeHFDD/000000.o: In function `_cgo_14616c423265_Cfunc_freeaddrinfo':\n/usr/local/go/src/pkg/net/cgo_unix.go:53: warning: Using 'getaddrinfo' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking\n```\nWhich I got by copying the build command that appears in deploy/prepare.sh:\nshell\ngodep go build -a --ldflags '-extldflags \"-static\"' github.com/google/cadvisor\n. @cnf \nHere's what I know so far. The report from @payneio above is w/r/t google/cadvisor image (which contains nothing but the cadvisor binary). My hunch was that the empty container had something to do with cadvisor failing to resolve DNS names (as you commented), so I went about re-compiling cadvisor in order to place it in a non-empty container image and see if DNS works.\nThe log output I posted was from me compiling a new binary, which I then placed inside a container image based on Ubuntu Core 14.04. That binary didn't fail on name resolution, but it didn't connect to InfluxDB at all (no data written to influxdb, and nothing in the cadvisor log about connection failures or attempts). \nFinally, I extracted the cadvisor binary from the official google/cadvisor distribution image, and added that into an image based on Ubuntu Core 14.04. Finally, it works: cadvisor is writing data into influxdb! The resulting image is available as nordstrom/cadvisor on Docker Hub, if you'd like to take a look.\nMy conclusion is that there is something from the OS that go binaries need to perform DNS resolution (libc?). Mainly I draw that conclusion from a couple of articles I've read on the internet about statically linked Go binaries still dynamically linking in some circumstances, and from this line in the log output I copied from my attempt to compile cadvisor:\n/usr/local/go/src/pkg/net/cgo_unix.go:53: warning: Using 'getaddrinfo' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking\n. Thanks for pointing that out. I checked again, and my cadvisor containers are crashing as well :frowning: . But I am seeing (plausible) data in InfluxDB (in the 'cadvisor' database), which is confusing. There are dropouts in the data. \nAt this point, there are multiple interacting pieces that I don't well understand. I can't really afford to invest more time on this issue. I'm going to implement a work-around by looking up the InfluxDB IP address before starting cadvisor. \n. @hinesmr gliderlabs/registrator reads environment variables from (other) containers as a form of arbitrary metadata. Metadata in that form is not necessarily easily accessible (depending on how you want to access it), but the approach might be worth a look.\n. ",
    "abilash222": "i am also facing the same issue. it works well for me in ubuntu but fails in centos 6.5 . any idea?\n. ",
    "reasonerjt": "@vmarmol No, it's not a custom version.\nAny idea that I can further debug?  Not sure where/how it gets the docker version...\n. ",
    "plietar": "Same issue here on a Debian stable host (no systemd). \nDocker is installed using repositories on get.docker.io. AUFS and native driver used.\n$ docker version\nClient version: 1.2.0\nClient API version: 1.14\nGo version (client): go1.3.1\nGit commit (client): fa7b24f\nOS/Arch (client): linux/amd64\nServer version: 1.2.0\nServer API version: 1.14\nGo version (server): go1.3.1\nGit commit (server): fa7b24f\n$ docker info\nContainers: 10\nImages: 135\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Dirs: 155\nExecution Driver: native-0.2\nKernel Version: 3.14.0-0.bpo.2-amd64\nOperating System: Debian GNU/Linux 7 (wheezy)\nWARNING: No memory limit support\nWARNING: No swap limit support\n$ docker run --volume=/var/run/:/var/run/:rw --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro -p 8080:8080 --name=cadvisor google/cadvisor:0.4.0\nE0923 13:02:51.456853 00001 cadvisor.go:57] Docker registration failed: Couldn't parse docker version: Version string \"\" doesn't match expected regular expression: \"(\\d+)\\.(\\d+)\\.(\\d+)\".\nThanks\n. Bumping the vendored go-dockerclient fixed it for me. Anything newer than 2dc7f6c67ce works.\nSee PR #250 \n. @vmarmol Done.\n. ",
    "burakyenier": "Thanks for the follow up. We did run into it a couple more times, but since then our interaction with cAdvisor has been minimal (not due to this potential issue). Will ping you if it pops up again. \n. It's also strange that kipmi0 is consuming as much resource as it is, I will look into that, too... Is it possible that the two are related?\n. Victor, some of these are fairly vague since the system has been restarted since, apologies. Following the restart, cAdvisor is active again, and the usage appears normal. \nThe cAdvisor was running for about 2 days when the issue occurred. There were about 5 other containers running on the system. The machine was performing heavy calculations; was pegged at 100% CPU utilization and heavy I/O between cores using MPI. \nWe do run similar workloads frequently and I will keep my eyes open. I realize that we destroyed much of the troubleshooting info which we could have collected for you. Please give me a list of info to collect if this should reoccur. \nBurak\n. ",
    "satnam6502": "Ah, OK. I see. Perhaps I should discard this pull request.\n. So how about this as a compromise:\n1. I revert the changes that involve moving code inside the if.\n2. I keep the change to the enum because according to Go style it really ought to be camel case.\n3. I fix up my typos in the comments, keep the linining changes etc.\nHow does that sound?\n. Assuming I have done the right thing with Git, I think I have fixed my typos and reverted the if statements. Thank you.\n. I talked to Go folks and they pointed out https://code.google.com/p/go-wiki/wiki/CodeReviewComments#Mixed_Caps\n. Yes, I wondered about that. Changed to warning.\n. ",
    "ashahab-altiscale": "Hi Victor,\nI thought about the refactoring you suggested, and then decided to add the Network stats to the raw handler for now: https://github.com/google/cadvisor/pull/270\nThe raw handler can handle lxc sufficiently for now, and we may defer the separate handler until it becomes too lxc specific.\n. Closing as #270 is doing the same thing.\n. Ok, I've made all the review changes.\n. I've made the spec changes and handled the file existence error.\n. Vishnu, Victor, do you have any other comments?\n. Well, How generic did you want \"raw\" driver to be? Should it depend on lxc\nand/or docker?\nOn Fri, Oct 17, 2014 at 1:04 PM, Victor Marmol notifications@github.com\nwrote:\n\nOverall happy with the approach. One question: can't we get the list of\nmounts from LXC somehow? Maybe even Docker directly?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/274#issuecomment-59567470.\n. I've made the changes you requested. So far I've tried to steer clear of using lxc or docker client to get information about the container(so raw driver remains agnostic). \n. Hi @vishh , @vmarmol , @rjnagal , do you have additional feedback on this?\n. I've made the changes  you requested. Let me know if there are anything else.\n. I see. Thanks @rjnagal , @vmarmol . \nOn gofmt, can I run it on older code like the raw-handler? I didn't want it to show up with a whole bunch of formatting changes.\nShould I call iostats -xd  instead of reading /proc/diskstats directly? It may still fail if /proc/diskstats is not available but it provides low level stats such as: \n\n```\n         await\n                 The average time (in milliseconds)  for  I/O  requests\n                 issued  to  the device to be served. This includes the\n                 time spent by the requests in queue and the time spent\n                 servicing them.\n      svctm\n             The  average  service  time  (in milliseconds) for I/O\n             requests that were issued to the device.  Warning!  Do\n             not  trust  this  field  any  more. This field will be\n             removed in a future sysstat version.\n\n      %util\n             Percentage of CPU time during which I/O requests  were\n             issued  to  the  device (bandwidth utilization for the\n             device). Device saturation occurs when this  value  is\n             close to 100%.\n\n```\n. @vmarmol , yes this is for the whole system, but the containerHints allow cadvisor to map filesystems to containers, and so a user can get container-specific filesystem stats. Yes, they can manually make the link, but when someone is knee-deep in performance debugging (like us :)), it's very convenient for containers to tell us their own disk-stats.\nAs for depending on iostat, I think iostat/sysstat are prevalent in most OS-es(correct me if I'm wrong) where containers can run. Each version of iostat can handle os-specific changes in the data(which is why I think the iostat pull request is passing in travis). \n. Ok, formatted everything. I really like the fact that we can format everything.\nSo right now it works both in root and in containers(exactly like the filesystem usage data). Let me know if you have other questions about this approach. \nFor now, I like not having to depend on iostat. If we need it later, we can try the other approach.\n. I've made all the changes you requested. Let me know if there are other things you'd like changed.\n. Absolutely! I'll take a look now.\n. @vmarmol @rjnagal , the test is failing because /proc/diskstats is absent(see Rohit's comment above).\nAll the tests need to mock out /proc/diskstats. How to do that?\n. I found mocking to be difficult in this case as I have to pass the mock function pointer to \nmanager.New  --> fsInfo.GetGlobalFsInfo() --> fsInfo.GetFsInfoForPath --> fsInfo.getDiskStatsMap\nSo for now I took the same approach as the container_hints file. If /proc/diskstats is missing a warning is logged, but those stats are not collected.\n. Finally, travis passed.\n. Let me know if you have other questions on this.\n. Alternative to https://github.com/google/cadvisor/pull/279\n. Closing, as decided to proceed with #279\n. @vmarmol Do you know who is the maintainer of libcontainer's cgroups package?\n. Rebased from master.\n. I'd suggest mounting the subdirectories/subsystems you want\nOn Oct 30, 2014 6:24 PM, \"wiwengweng\" notifications@github.com wrote:\n\nand this kernel is upgraded from an old one, probably 2.6.3**, using yum\nupgrade to do this.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/290#issuecomment-61200595.\n. Meaning, mount /cgroup/cpu(if you want cpu), instead of mounting just\n/cgroup.\n\nOn Thu, Oct 30, 2014 at 7:04 PM, Abin Shahab ashahab@altiscale.com wrote:\n\nI'd suggest mounting the subdirectories/subsystems you want\nOn Oct 30, 2014 6:24 PM, \"wiwengweng\" notifications@github.com wrote:\n\nand this kernel is upgraded from an old one, probably 2.6.3**, using yum\nupgrade to do this.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/290#issuecomment-61200595.\n. Hi,\nYou can add the following options when you start cadvisor(see https://github.com/google/cadvisor/blob/master/README.md):\n -storage_driver=influxdb\n -log_dir=/\n. Thanks for the error message, it's helpful. What's happening is, docker thinks -storage_driver is a flag to docker, but actually it has to be a flag to cadvisor:\n\n\nsudo docker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  google/cadvisor:latest -storage_driver=influxdb -log_dir=/\n. Can you start another container with the lxc-driver?\ndocker run -itd ubuntu bash\ndocker ps\ndo you see a container up after this?\n. I'm trying to see if this issue is the same as: https://github.com/docker/docker/pull/9039\n. Yes, this works with both. I will update the comment. Sorry I just saw your review.\n. Good point.\n. Done. It's now called ContainerHints\n. I've added support for both types now. if full container name is provided(Id has /something), it will still work.\n. I get the following error for a bogus file:\nFailed to start container manager: open /etc/docker/cdeaasa.json: no such file or directory\nSeems like golang ioutils emits really nice errors. Should this suffice?\n. Done, thanks!\n. That won't work. Golang json won't parse private fields: http://stackoverflow.com/questions/11126793/golang-json-and-dealing-with-unexported-fields\nThe types are private and so it won't show in the api.\n. Done, and I have made the id match change you requested.\n. Vishnu asked to add this error msg I think, but I can remove it.\nOn Oct 15, 2014 2:46 PM, \"Victor Marmol\" notifications@github.com wrote:\n\nIn container/raw/handler.go:\n\n@@ -49,6 +52,20 @@ func newRawContainerHandler(name string, cgroupSubsystems cgroupSubsystems, mac\n    if err != nil {\n        return nil, err\n    }\n-   cHints, err := getContainerHintsFromFile(argContainerHints)\n-   if err != nil {\n-       glog.Fatalf(\"Error unmarshalling json %s Error: %s\", *argContainerHints, err)\n\nWe should just return nil, err here\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/270/files#r18925466.\n. It is doing that, actually. It will return a nil containerHint and the\nerror in case of an issue. This seems like the standard behavior of most\nfunctions.\nOn Oct 15, 2014 2:46 PM, \"Victor Marmol\" notifications@github.com wrote:\nIn container/raw/container_hints.go:\n\n\nAllHosts []containerHint json:\"all_hosts,omitempty\"\n  +}\n  +\n  +type containerHint struct {\nFullPath                string json:\"full_path,omitempty\"\nNetworkInterface *networkInterface json:\"network_interface,omitempty\"\n  +}\n  +\n  +type networkInterface struct {\nVethHost  string json:\"veth_host,omitempty\"\nVethChild string json:\"veth_child,omitempty\"\nNsPath    string json:\"ns_path,omitempty\"\n  +}\n  +\n  +func getContainerHintsFromFile(containerHintsFile string) (containerHints, error) {\ndat, err := ioutil.ReadFile(containerHintsFile)\n\n\nWhen this file does not exist this will return an error. We probably want\nto catch that and return an empty containerHints\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/270/files#r18925463.\n. Oh I see.\nI'll add this: \nif os.IsNotExist(err) {\n    return containerHints{}, nil\n}\n. Are you sure tx_bytes emits RxBytes?\n. \n",
    "xiangflytang": "i met the same problem,do you create the \"cadvisor\" manually\n. if this: \ncurl --noproxy localhost -X POST -d '[{\"name\":\"foo\",\"columns\":[\"val\"],\"points\":[[23]]}]' 'http://localhost:8086/db/mydb/series?u=root&p=root'\nworks\nmaybe this problem comes from proxy.\n. ",
    "andrewwebber": "I have the same issue. Event hosting an influxdb on the same host results in the same error. I am not able to run the /storage/influxdb tests as I cant build the project\ncan't load package: package code.google.com/p/go.exp/inotify\n. no, i presume i should be?... i just do a go get \n. i noticed within i notify only _linux go files. does this mean i cant build on my mac?\nI assumed running the influxdb storage tests to be no linux related.\n. the fact is i believe it is important to test the version of the influxdb go dependency with the latest version of influxdb. This is what i am trying to achieve. I will create a go program with the latests version of the client library for influxdb and try to execute a post within a docker container on a CoreOS machine. This would validate if the library works. Then do the same test with the version of the library specified in godep. This would help investigate if it is a library version issue or a networking issue\n. This is consistently because there is never any data logged to influxdb.\n. my work around is now to mine cadvisor like heapster but implement a new sink in heapster to log to logstash\n. Can confirm i am now seeing data, however this is all running in one host (boot2docker)\n. @vishh Thanks for your support here. I have a kubernetes coreos cluster and got this working with the following fleet systemd unit files. \nUnfortunatly the documentation for grafana did not work for me at is insisted in looking for a metadata endpoint for influxdb and elastic search. After deleting these and manually loading the kubernetes dashboard, everything worked (after editing a couple of graphs that were looking for a 'machines' time series where only 'stats' existed).\ngrafana (manually)\ndocker run -i -t --rm -p 80:80 -e INFLUXDB_HOST=192.168.89.161 -e INFLUXDB_PORT=8086 -e INFLUXDB_NAME=k8s -e INFLUXDB_USER=root -e INFLUXDB_PASS=root tutum/grafana\ncadvisor (globally deployed)\n```\n[Unit]\nDescription=cAdvisor Service\nAfter=docker.service\nRequires=docker.service\n[Service]\nTimeoutStartSec=10m\nRestart=always\nExecStartPre=-/usr/bin/docker kill cadvisor\nExecStartPre=-/usr/bin/docker rm -f cadvisor\nExecStartPre=/usr/bin/docker pull google/cadvisor\nExecStart=/usr/bin/docker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro --publish=4194:4194 --name=cadvisor --net=host google/cadvisor:latest --logtostderr --port=4194\nExecStop=/usr/bin/docker stop -t 2 cadvisor\n[X-Fleet]\nGlobal=true\nMachineMetadata=role=kubernetes\n```\nInfluxdb \n```\n[Unit]\nDescription=InfluxDB Service\nAfter=docker.service\nRequires=docker.service\n[Service]\nTimeoutStartSec=10m\nRestart=always\nExecStartPre=-/usr/bin/docker kill influxdb\nExecStartPre=-/usr/bin/docker rm -f influxdb\nExecStartPre=/usr/bin/docker pull kubernetes/heapster_influxdb\nExecStart=/usr/bin/docker run --name influxdb -p 8083:8083 -p 8086:8086 -p 8090:8090 -p 8099:8099 kubernetes/heapster_influxdb\nExecStop=/usr/bin/docker stop -t 2 influxdb\n```\nHeapster agent (buddy)\n```\n[Unit]\nDescription=Heapster Agent Service\nAfter=docker.service\nRequires=docker.service\n[Service]\nTimeoutStartSec=10m\nRestart=always\nExecStartPre=-/usr/bin/mkdir -p /home/core/heapster\nExecStartPre=-/usr/bin/docker kill heapster-agent\nExecStartPre=-/usr/bin/docker rm -f heapster-agent\nExecStartPre=/usr/bin/docker pull vish/heapster-buddy-coreos\nExecStart=/usr/bin/docker run --name heapster-agent --net host -v /home/core/heapster:/var/run/heapster vish/heapster-buddy-coreos\nExecStop=/usr/bin/docker stop -t 2 heapster-agent\n[X-Fleet]\nMachineOf=influxdb.service\n```\nHeapster\n```\n[Unit]\nDescription=Heapster Agent Service\nAfter=docker.service\nAfter=heapster-agent.service\nRequires=docker.service\nRequires=heapster-agent.service\n[Service]\nTimeoutStartSec=10m\nRestart=always\nExecStartPre=-/usr/bin/docker kill heapster\nExecStartPre=-/usr/bin/docker rm -f heapster\nExecStartPre=/usr/bin/docker pull vish/heapster\nExecStart=/usr/bin/docker run --name heapster --net host -e INFLUXDB_HOST=127.0.0.1:8086 -v /home/core/heapster:/var/run/heapster vish/heapster\nExecStop=/usr/bin/docker stop -t 2 heapster\n[X-Fleet]\nMachineOf=heapster-agent.service\n```\nAt the moment i'm lazy and dont care if my heapster agents move around the cluster.\n. With respect to the discovery issue of the influxdb database for grafana i am think about the following approaches.\nOptions 1:\n- Setup with a buddy pod container (kubernetes) \\ systemd (bindsto=influxdb.service) unit file (coreos) that 'announces' the influxdb IP address to etcd when the service starts or stops.\n- Have an additional systemd unit file for grafana that on ExecStart reads from etcd for the ip address of the influx db container\nOption 2:\n- Setup with a buddy pod container (kubernetes) \\ systemd (bindsto=influxdb.service) unit file (coreos) that 'announces' the influxdb IP address to etcd when the service starts or stops.\n- Modify the grafana container to take a new environment variable ($ETC_PEERS) which it will use to read\\watch etcd for discovery values related to influxdb\nOption 3:\n- Setup with a buddy pod container (kubernetes) \\ systemd (bindsto=influxdb.service) unit file (coreos) that 'announces' the influxdb IP address to etcd when the service starts or stops.\n- Modify the grafana container to take a new environment variable ($ETC_PEERS) which it will use to read\\watch etcd for discovery values related to influxdb\n- Within the container use something like confd to re-write grafan config file within the container \n. @MaheshRudrachar I have not run specifically into any issues you mentioned. However indirect issues probably due to the fact that i am low on hardware in my lab environment. I have influxdb, the heapster agents and grafana all running on my single etcd serving my cluster which also runs my private docker registry container :-).\nUltimately it makes sense to split these machines up into dedicated roles to better independently isolate from where issues might be originating. For example my etcd server suddenly was unable to start docker. In reality I shouldn't care because a production setup of an etcd cluster probably should not even be running docker containers.\nI am also running the alpha branch with automatic update (reboot when update found) which doesn't always help. So I guess my tip would be to move out at least your units of the etcd servers\nAlso in my case i dont really care if my cadvisors crash or even if my influxdb, heapsters gets rebuild and run. Sacred are the etcd servers, if they go down your whole kubernetes cluster goes down too and you need to redeploy all of your pods, replication controllers and services. \n. @MaheshRudrachar @vishh This is due to the fact that the heapster buddy assumes fleet is running on the host. I had this issue and therefore had to run my heapster agents on the etcd node. \nhttps://github.com/GoogleCloudPlatform/heapster/blob/master/clusters/coreos/buddy.go#L35\nI believe we need to add a flag to the buddy to parameterise the fleet server url\n. ",
    "alessioguglielmo": "... maybe the solution for my post?\n. already tried...\nsudo docker run \\\n  -storage_driver=influxdb \\\n  -log_dir=/ \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  google/cadvisor:latest\nIt shows:\nflag provided but not defined: -storage_driver\nUsage: docker run [OPTIONS] IMAGE [COMMAND] [ARG...]\n. Ok, seems to be running correctly. Now how can I test that influxdb works fine?\ncurl localhost:8086 returns connection refused.\ngoogle/cadvisor:0.5.0   \"/usr/bin/cadvisor -   6 minutes ago       Up 6 minutes        0.0.0.0:8080->8080/tcp   cadvisor\nport 8086 doesn't seem to be exposed\nthanks for help.\nedit Ok, perhaps I've forgotten a publish command...\n. I've published 8086 port, but seems I can't talk with server. It returns empty reply.\n. ",
    "jwalczyk": "I'm new to both cadvisor and influxdb; also having this problem.  On a Mac, I run google/cadvisor:0.5.0 in a docker container - Docker version 1.3.1, build 4e9bbfa\n$docker logs  \nE1107 09:09:48.189645 00001 memory.go:109] failed to write stats to influxDb - Post http://localhost:8086/db/cadvisor/series?u=root&p=root&time_precision=u: dial tcp 127.0.0.1:8086: connection refused\nThis works fine:  curl --noproxy localhost -X POST -d '[{\"name\":\"foo\",\"columns\":[\"val\"],\"points\":[[123]]}]' 'http://localhost:8086/db/cadvisor/series?u=root&p=root'\nI can view metrics in the InfluxDB GUI via 'select * from foo' ,etc.\n@xiangflytang -  What proxy are you reffering to?\n. Thank you for the comments! I got it working.  Silly mistake.  @vishh got it and I had to change localhost to my host IP during cadvisor container startup.\n--storage_driver_host=\"my_host_ip:8086\"\nI then tested this by running nc -kl 8086 on my host and saw all the nice data dumped.  Thanks!\n. ",
    "MaheshRudrachar": "@andrewwebber:  Heapster Agent and Heapster Service getting restarted abruptly when I followed above Units using Fleet.\nI am running all those Units in one CoreOS instance which is configured as cluster in Kubernetes.\nCould you explain whether we need different CoreOS instances or all Units needs to run on one CoreOS instance.\nThanks in advance....\n. @andrewwebber: BTW, I am running Kubernetes cluster with CoreOS on EC2 instance...\n. @andrewwebber & @vishh\nThanks for your inputs. Still not able to resolve this issue.\nHere is my Kubernetes Setup Details:\nI have setup 5 CoreOS instances on AWS and followed kelseyhightower kubernetes-fleet-tutorial.\nBasically I have \n1 ETCD dedicated Server, \n3 Minions with 1 Minion acting as API Server and \n1 dedicated Minion for setting up Heapster. \nAll Minions are pointing to ETCD dedicated server.\nNow when I ran units which are mentioned above:\n1. cAdvisor, Influxdb & Grafana - Works fine with status running\n1. Heapster Agent: Fails with log message as follows:\n   Starting Heapster Agent Service...\n   heapster-agent\n   Pulling repository vish/heapster-buddy-coreos\n   Started Heapster Agent Service.\n   INFO log.go:73: Failed getting response from http://127.0.0.1:4001/: dial tcp 127.0.0.1:4001:\n   ERROR log.go:81: Unable to get result for {Get /_coreos.com/fleet/machines}, retrying in 100m\n   timeout reached\n   heapster-agent.service: main process exited, code=exited, status=1/FAILURE\n   Unit heapster-agent.service entered failed state.\n2. Heapster: Fails with log message as follows:\n   Pulling repository kubernetes/heapster\n   Started Heapster Agent Service.\n   /usr/bin/heapster --sink influxdb --sink_influxdb_host 127.0.0.1:8086\n   Heapster version 0.2\n   Cannot stat hosts_file /var/run/heapster/hosts. Error: stat /var/run/heapster/hosts: no such file or directory\n   heapster.service: main process exited, code=exited, status=1/FAILURE\n   heapster\n   Unit heapster.service entered failed state.\n   heapster.service holdoff time over, scheduling restart.\n   Stopping Heapster Agent Service...\n   Starting Heapster Agent Service...\nNeed your help in resolving this.\nThanks\n. Thanks @vishh and @andrewwebber. I will give a try with latest version.\n. ",
    "jzelinskie": "@vmarmol to avoid confusion about building your project, you can use godep -r to rewrite your imports. It makes it so that your godep project can still be \"go get-able\"\n. ",
    "kevin1024": "OK, I rebased and added the changes recommended in 803a288a6265c0b3b349b408812da7c16beb5ea6\n. ",
    "wiwengweng": "Hi, vmarmol. here is what I have:\n```\n[root@localhost ~]# sudo docker run   --volume=/:/rootfs:ro   --volume=/var/run:/var/run:rw   --volume=/sys:/sys:ro   --volume=/var/lib/docker/:/var/lib/docker:ro   --volume=/cgroup:/cgroup   --publish=8888:8080   --detach=false   --name=cadvisor   google/cadvisor:latest --logtostderr\nI1030 08:49:46.862402 00001 storagedriver.go:89] Caching 60 recent stats in memory; using \"\" storage driver\nI1030 08:49:46.862812 00001 manager.go:75] cAdvisor running in container: \"/lxc/2b2d6ce70d55b89ec6a2518722cc713b7c166771bd8230873d20897670ce6f4a\"\nI1030 08:49:46.864356 00001 manager.go:89] Machine: {NumCores:1 MemoryCapacity:2498093056 Filesystems:[{Device:/dev/mapper/docker-253:0-3145730-2b2d6ce70d55b89ec6a2518722cc713b7c166771bd8230873d20897670ce6f4a Capacity:10434699264} {Device:/dev/mapper/VolGroup-lv_root Capacity:52710469632}]}\nI1030 08:49:46.866283 00001 manager.go:96] Version: {KernelVersion:2.6.32-504.el6.x86_64 ContainerOsVersion:Buildroot 2014.02 DockerVersion:1.1.2 CadvisorVersion:0.5.0}\nE1030 08:49:47.268904 00001 cadvisor.go:61] Docker registration failed: Docker found, but not using native exec driver.\nF1030 08:49:47.269625 00001 cadvisor.go:66] raw registration failed: failed to find cgroup mounts for the raw factory.\nlxc-start: The container failed to start.\nlxc-start: Additional information can be obtained by setting the --logfile and --logpriority options.\n```\nI have a kernel of 2.6.32 \n2.6.32-504.el6.x86_64\nis it compatible with docker? Or should I upgrade to centos 7?\n. @vmarmol I can see the following output:\n[root@localhost ~]# grep cgroup /proc/mounts \ncgroup /cgroup/cpuset cgroup rw,relatime,cpuset 0 0\ncgroup /cgroup/cpu cgroup rw,relatime,cpu 0 0\ncgroup /cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\ncgroup /cgroup/devices cgroup rw,relatime,devices 0 0\ncgroup /cgroup/freezer cgroup rw,relatime,freezer 0 0\ncgroup /cgroup/net_cls cgroup rw,relatime,net_cls 0 0\ncgroup /cgroup/blkio cgroup rw,relatime,blkio 0 0\nby the way, seems memory cgroup is not here, right?\nand in the /cgroup folder, I also list them here\n[root@localhost ~]# cd /cgroup/\n[root@localhost cgroup]# ls\nblkio  cpu  cpuacct  cpuset  devices  freezer  memory  net_cls\n[root@localhost cgroup]# ll\ntotal 4\ndrwxr-xr-x  3 root root    0 Oct 30 15:28 blkio\ndrwxr-xr-x  3 root root    0 Oct 30 15:28 cpu\ndrwxr-xr-x  3 root root    0 Oct 30 15:28 cpuacct\ndrwxr-xr-x  3 root root    0 Oct 30 15:28 cpuset\ndrwxr-xr-x  3 root root    0 Oct 30 15:28 devices\ndrwxr-xr-x  3 root root    0 Oct 30 15:28 freezer\ndrwxr-xr-x. 2 root root 4096 Oct 28 16:31 memory\ndrwxr-xr-x  3 root root    0 Oct 30 15:28 net_cls\n[root@localhost cgroup]# cd memory/\n[root@localhost memory]# ls\n[root@localhost memory]# cd ..\n[root@localhost cgroup]# cd cpu\n[root@localhost cpu]# ls\ncgroup.event_control  cgroup.procs  cpu.cfs_period_us  cpu.cfs_quota_us  cpu.rt_period_us  cpu.rt_runtime_us  cpu.shares  cpu.stat  lxc  notify_on_release  release_agent  tasks\n. and this kernel is upgraded from an old one, probably 2.6.3**, using yum upgrade to do this.\n. ok, I don't know what I want, so I just mount every thing. ;D And it works now. Hope that you can update the README\n. ",
    "gmelekh": "I tried .mismatch of this column caused me to use 0.4.1 cadvisor version.\nOn Nov 3, 2014 6:49 PM, \"Rohit Jnagal\" notifications@github.com wrote:\n\nI think this might be fixed with the recent changes. @gmelekh\nhttps://github.com/gmelekh is it possible for you to try out cadvisor\nbuild from head?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/291#issuecomment-61507998.\n. Yes.I filter by this column building my grafana dashboard.\nOn Nov 4, 2014 3:13 AM, \"Vish Kannan\" notifications@github.com wrote:\n@gmelekh https://github.com/gmelekh are you referring to the column\n'container_name' in Influxdb?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/291#issuecomment-61578513.\n. \n",
    "leg100": "I'm also seeing a lack of headers, not just container_name but machine too. \nI built cAdvisor from HEAD, and running ngrep confirms those headers are not being sent.\n- on CoreOS 444.5.0\n. ",
    "burkostya": "cadvisor does not include InfluxDB. You must setup it somewhere else\n. cadvisor does not include InfluxDB. You must setup it somewhere else\n. ",
    "rmanyari": "Thanks for taking some time to help me debug this @vishh \nI looked this morning at the logs and found lots of these:\nE0129 19:37:50.348249 00001 memory.go:94] failed to write stats to influxDb - Post http://the_right_ip:8086/db/cadvisor/series?u=root&p=the_right_password&time_precision=u: write tcp 104.131.19.134:8086: connection refused\nE0129 19:37:54.365524 00001 memory.go:94] failed to write stats to influxDb - Post http://the_right_ip:8086/db/cadvisor/series?u=root&p=the_right_password&time_precision=u: write tcp 104.131.19.134:8086: connection timed out\nWhat's curious is that if I take the same url and paste it into postman (tool I use to push http requests) everything works fine. Is it possible that cdavisor is trying to write too many series at the time and its overloading my influx at the other end? It's gathering data from a server that has ~350 containers running.\nI also see some of this on the influx server (4gb Ubuntu droplet on DO):\nJan 29 14:38:53 influx kernel: [   75.579469] perf samples too long (2640 > 2500), lowering kernel.perf_event_max_sample_rate to 50000\nJan 29 14:39:02 influx kernel: [   85.424844] perf samples too long (5024 > 5000), lowering kernel.perf_event_max_sample_rate to 25000\nJan 29 14:39:46 influx kernel: [  129.378845] perf samples too long (10034 > 10000), lowering kernel.perf_event_max_sample_rate to 12500\nJan 29 14:41:43 influx kernel: [  245.943875] perf samples too long (20011 > 20000), lowering kernel.perf_event_max_sample_rate to 6250\n. ",
    "leandrok": "Thank you Victor! That's great. I think that cAdvisor is a very useful tool. I was searching for a lightweight Docker monitoring tool and cAdvisor is perfect for my needs. Glad to know that this enhancement will be added so soon. Thanks!\n. @vmarmol @vishh Thank you very much!\n. @vmarmol Great! Thank you very much Victor.\n. ",
    "ibuildthecloud": "Sorry for the slow response, I just signed the CLA.\n. I updated the PR to address the comments.  Thanks!\n. ",
    "docteurklein": "might be fixed by #296 \nand related to #294\n. thanks for the quick answer. Great work!\n. works perfectly. Thanks!\n. ",
    "mindscratch": "It may not be ideal or even the best way to solve the problem, but it does work.\n. Closing since the pull-reuest was merged.\n. @vmarmol glad I could help. I've fixed the formatting.\n@thaJeztah I noticed there were newer versions but I didn't want to make potentially breaking changes when doing this, perhaps upgrading to newer versions of bootstrap and jQuery could be done through a separate issue?\n. @thaJeztah I like the idea of using the version of the library in the URL for the resource. For external dependencies (jquery, bootstrap) the version of that library should be used. For internal resources (containers.css) the cAdvisor could be used (by leveraging the VERSION constant in version.go). Would you prefer to see that in this PR or another?\n. @vmarmol Sounds good, I'll work on that this evening/weekend, shouldn't be much work.\n. @vmarmol @thaJeztah I had some time so I've included the version numbers of jQuery and Bootstrap in the URL.\n. Thanks for the feedback and encouragement, glad I could contribute back.\n. The docker image is great, if your server has a connection to the Internet. Providing the binaries can help, or even the .tar of the image might work too.\n. @vmarmol that's greatly appreciated, excited to start running 0.6.1 today!\n. Done.\n. ",
    "onlyjob": "Is there a more elegant way to embed static resources? Perhaps serving .js and .css files from /usr/share/cadvisor instead of stitching resources straight into executable? Current approach strikes me as remarkably ugly (let alone unnecessary use of obfuscated/minified code).   :-(\n. Do not ever use anything \"latest\", \"from master\" etc. Please stick to formal release. See list of InfluxDB releases. The latest released version of InfluxDB is v0.9.4.2.\n. I'm not concerned about extra dependencies. But if v0.9.4.2 is not good enough then by all means wait till 0.9.5 please. Release numbers are there precisely to define dependencies as well as to use software that is less likely to break comparing to unreleased development version from repository.\nDepending on unreleased software (and even worse, bundling it) is a nightmare for downstream package maintainers. Please don't do that.\n. I hope they do not release on schedule but given frequency of previous releases and the fact that the last release was about 20 days ago the new release might be just within weeks from now. Hopefully that's not too long and I also hope there is no rush to commit pre-release here...\n. From downstream maintainer prospective a separate branch based on pre-release software would not be harmful as long as it is not used to produce a tagged release...\n. @jimmidyson I'm not sure why you are asking me... Pull request is not mine and I'm not qualified to update it nor have any time for it...\n. Ping?\nThere are more issues with bundled files. There are no sources included for Bootstrap JS/CSS and jQuery (but at least they can be found in web). However I could not find origins of containers_css.go and gcharts_js.go. Also contents of google_jsapi_js.go seems to be copied from https://www.google.com/jsapi but where is the source?? Seems that #304 did it all wrong. :(\nPlease advise.\n. Another pitfall of embedding is that it makes build fragile. For instance, Bootstrap v3.3.5 contains\nthrow new Error('`selector` option must be specified when initializing ' + this.type + ' on the window.document object!')\nwhich breaks compilation as follows if Bootstrap is upgraded in pages/static/bootstrap_min_js.go (due to '`' characters):\nsrc/github.com/google/cadvisor/pages/static/bootstrap_min_js.go:1309: syntax error: unexpected name, expecting semicolon or newline\n. \"Making distribution easy\" is not worth it. Why not leave distribution to GNU/Linux distros?\nAnyway I'm illiterate in Golang and I'm against bundling resources like this (see https://github.com/google/cadvisor/issues/304#issuecomment-147118251) so I can't prepare a pull request.\nBesides there are unidentified undocumented source-less resources so even if I could take care of technical issues I wouldn't know where to get missing sources from...\n. > ... This is a pretty common thing to do in the go world.\nFrankly it is a terrible practice -- too bad if it is \"common\". We do need proper packages and personally I won't install anything that is not packaged. I find it uncomfortable to have humongous \"everything-included\" executables with no security support... :(\nBy the way this problem was discovered during packaging...\n\nTotally agree that the way cadvisor currently does this needs improvement. I'll see if I get time to take a look.\n\nThank you.\n. This is still an outstanding issue... :(\n. Thanks, @vishh. :-)\n. I did not test this approach but from the brief look at 9f52cad it appears to be much nicer indeed. The only minor concern is that there are 5 new go(.rice) dependencies in Godeps but that's not an issue.\nAs for charts I'm quite happy that source-less files are no longer bundled.\nToS compliance is quite important as well but privacy breach (i.e. unconditional load of 3rd party JS) is still there. Maybe some other charting library like \"D3\" or \"Flot\" could be a potential replacement? Or something from this list as long as free and open source alternative is chosen. Google JSAPI and Charts are not free software so I hope they can be replaced eventually...\n. ",
    "dchen1107": "@@kateknister should we close this one?\n. cc/ @dchen1107\n. LGTM\n. @jimmidyson du is a temporary workaround for disk usage tracking in cAdvisor without disk quota. We are working on proposal / prototype on better disk usage tracking. One proposal is using disk quota tracking. But before we get there, we need signals at least to detect out-of-disk condition, and propagate such information to upstream layers for management. Thus increasing the interval for filesystem stats might be a ok workaround for short-term. \n. LGTM\n. @jimmidyson Are you going to cut a release with this one or you want to include devicemapper fix too? \n. https://github.com/kubernetes/kubernetes/issues/15805\n. Thanks for status updates here. \n. LGTM\n. small nits.\nLGTM\n. LGTM\n. LGTM\n. cc/ @justinsb on aws support. \nLGTM\n. @justinsb and @vishh It is not a blocker, but it would be nice to get this into 1.2 release. Thanks!\n. LGTM thanks!\n. The cAdvisor pr builder job doesn't run for a couple of days. @fejta and I looked into why ...\n. @pmorie Looks like the test failed due to your change:\n16:18:35 --- FAIL: TestRefresh (0.00s)\n16:18:35    thin_pool_watcher_test.go:147: no existing reservation - ok: actual usage did not match expected usage: expected: 23456 got: 0\n16:18:35    thin_pool_watcher_test.go:147: existing reservation - ok: actual usage did not match expected usage: expected: 34567 got: 0\n16:18:35 === RUN   TestCheckReservation\n16:18:35 --- PASS: TestCheckReservation (0.00s)\n16:18:35 FAIL\n16:18:35 FAIL   github.com/google/cadvisor/devicemapper 0.052s\n. LGTM\n. @k8s-bot test this\n. LGTM\n. LGTM\n. Keyboard locking issue. Not mean to send this out. \n. https://github.com/kubernetes/kubernetes/issues/27573\n. @k8s-bot test this\n. @euank I am assigning this one to you for delegating. We need better support for coreos as one of basic images for us. Re-assign it back to us or ask for help if you need. Thanks!\n. Agreed with @derekwaynecarr on that this shouldn't be the blocker for 1.4 release. \n. cc/ @timstclair @dashpole @vishh . @bamarni, can we have some cadvisor tests for validating overlay2 driver? Some basic sanity check tests here? . The code is pretty clean and lgtm. . /lgtm. /lgtm. Can you also have the issue # here? The release note you put here is pretty confusion. \n. ",
    "danielkraaij": "I hate to bump issues, but does anyone have an idea?\n. @vmarmol Simply clicking through the interface to look at a specific docker container. I just updated to 0.6.2 and problem remains.\nFailed to get container \"/docker/02542e45b912653a587540d47b955947c4fc82e14e33ee9a0227e8bcdfc44315\" with error: unable to find data for container /docker/02542e45b912653a587540d47b955947c4fc82e14e33ee9a0227e8bcdfc44315\nSomething else i noticed, on the main dashboard the filesystem section is empty.\nIt would suggest that cadvisor is having problems reading my filesystem?\nIf there is any more information you need, please let me know. I will provide it.\n. Just noticed the following\nroot@par-staging:~# docker logs cadvisor \nI1124 14:42:01.649371 00001 storagedriver.go:89] Caching 60 recent stats in memory; using \"\" storage driver\nlog: exiting because of error: write /tmp/cadvisor.d997971eb9be.root.log.INFO.20141124-144201.1: no space left on device\nthere is enough diskspace on the server for the container to expand into.\n. @vmarmol No, not at this moment. It did run however in the past. \nI have also tried older versions of cadvisor which i kept running in the past (although still faulty with the above error) \nI unfortunatly don't know how to access the filesystem of an docker container that won't start. (other than looking in /var/lib/docker/aufs/) So i don't have log files for you.\nHere is some additional information that might be usefull\n```\nroot@par-staging:~# docker ps -a\nCONTAINER ID        IMAGE                                                  COMMAND               CREATED             STATUS                     PORTS                                                                     NAMES\n04815f9d4f27        google/cadvisor:0.5.0                                  \"/usr/bin/cadvisor\"   2 seconds ago       Exited (2) 1 seconds ago                                                                             cadvisor             \nb049808bfa07        xxx-xxx-api:latest                                  \"/sbin/my_init\"       21 hours ago        Up 21 hours                0.0.0.0:2231->22/tcp, 0.0.0.0:9001->80/tcp                                mobile01             \ne8200d2b2a57        registry.xxx.nl:2124/xxx/memcached:latest   \"/sbin/my_init\"       21 hours ago        Up 21 hours                0.0.0.0:49191->22/tcp, 0.0.0.0:49192->11211/tcp                           xxx-mobile-memcached \n8d4d69029412        paradise/loadbalancer:latest                           \"/sbin/my_init\"       4 days ago          Up 4 days                  0.0.0.0:80->80/tcp, 0.0.0.0:443->443/tcp, 0.0.0.0:49190->22/tcp           lb01                 \n74f10ac16480        paradise/cache:latest                                  \"/sbin/my_init\"       4 days ago          Up 4 days                  0.0.0.0:2212->22/tcp, 0.0.0.0:8082->80/tcp, 0.0.0.0:49189->81/tcp         cache02              \n4d0445f9afa5        paradise/cache:latest                                  \"/sbin/my_init\"       4 days ago          Up 4 days                  0.0.0.0:2211->22/tcp, 0.0.0.0:8081->80/tcp, 0.0.0.0:49188->81/tcp         cache01              \n02542e45b912        paradise/front-end:latest                              \"/sbin/my_init\"       4 days ago          Up 4 days                  0.0.0.0:2204->22/tcp, 0.0.0.0:8004->80/tcp                                web04                \n2602e369463b        paradise/front-end:latest                              \"/sbin/my_init\"       4 days ago          Up 4 days                  0.0.0.0:2203->22/tcp, 0.0.0.0:8003->80/tcp                                web03                \nf98637ff412b        paradise/front-end:latest                              \"/sbin/my_init\"       4 days ago          Up 4 days                  0.0.0.0:2202->22/tcp, 0.0.0.0:8002->80/tcp                                web02                \ndbcac66ac6bb        paradise/front-end:latest                              \"/sbin/my_init\"       4 days ago          Up 4 days                  0.0.0.0:2201->22/tcp, 0.0.0.0:8001->80/tcp                                web01                \n2dc6b0db171d        xxx-xxx-api:latest                                  \"/sbin/my_init\"       4 days ago          Up 4 days                  0.0.0.0:2222->22/tcp, 0.0.0.0:8802->80/tcp                                api02                \ncd8e9930f163        xxx-xxx-api:latest                                  \"/sbin/my_init\"       4 days ago          Up 4 days                  0.0.0.0:2221->22/tcp, 0.0.0.0:8801->80/tcp                                api01                \nd7fdc950659c        paradise/elasticsearch:latest                          \"/sbin/my_init\"       7 days ago          Up 6 days                  0.0.0.0:49159->22/tcp, 0.0.0.0:49160->9200/tcp, 0.0.0.0:49161->9300/tcp   search02             \n65111b78f9c0        paradise/elasticsearch:latest                          \"/sbin/my_init\"       7 days ago          Up 6 days                  0.0.0.0:49156->22/tcp, 0.0.0.0:49157->9200/tcp, 0.0.0.0:49158->9300/tcp   search01             \n611d1adb0651        xxx-xxx-mysql:latest                                \"/sbin/my_init\"       7 days ago          Up 6 days                  0.0.0.0:3306->3306/tcp, 0.0.0.0:49155->22/tcp                             mysql01              \nd0e7cc44b729        registry.xxx.nl:2124/xxx/memcached:latest   \"/sbin/my_init\"       12 days ago         Up 6 days                  0.0.0.0:49153->11211/tcp, 0.0.0.0:49154->22/tcp                           xxx-memcached          \nroot@par-staging:~# docker logs cadvisor \nI1125 12:57:08.496141 00001 storagedriver.go:89] Caching 60 recent stats in memory; using \"\" storage driver\nlog: exiting because of error: write /tmp/cadvisor.04815f9d4f27.root.log.INFO.20141125-125708.1: no space left on device\nroot@par-staging:~# df -h \nFilesystem                                                    Size  Used Avail Use% Mounted on\n/dev/mapper/ubuntu-root                                        78G   37G   41G  48% /\nnone                                                          4.0K     0  4.0K   0% /sys/fs/cgroup\nudev                                                          3.9G  4.0K  3.9G   1% /dev\ntmpfs                                                         799M  799M     0 100% /run\nnone                                                          5.0M     0  5.0M   0% /run/lock\nnone                                                          3.9G   27M  3.9G   1% /run/shm\nnone                                                          100M     0  100M   0% /run/user\n/dev/vda1                                                     496M  119M  378M  24% /boot\n109.72.84.42:/storage2/joomla/domains/bax-shop.nl/htdocs/www  234G  164G   71G  71% /mnt/media\nroot@par-staging:~# free -m \n             total       used       free     shared    buffers     cached\nMem:          7985       7756        228        902          0       3148\n-/+ buffers/cache:       4608       3376\nSwap:         1951        773       1178\n```\nI found a sort of stack trace in dmesg\n[536215.289782] device veth6653c8a entered promiscuous mode\n[536215.290349] IPv6: ADDRCONF(NETDEV_UP): veth6653c8a: link is not ready\n[536215.344988] IPv6: ADDRCONF(NETDEV_CHANGE): veth6653c8a: link becomes ready\n[536215.345019] docker0: port 16(veth6653c8a) entered forwarding state\n[536215.345027] docker0: port 16(veth6653c8a) entered forwarding state\n[536215.436362] docker0: port 16(veth6653c8a) entered disabled state\n[536215.437151] device veth6653c8a left promiscuous mode\n[536215.437170] docker0: port 16(veth6653c8a) entered disabled state\n[536215.748355] ------------[ cut here ]------------\n[536215.748365] WARNING: CPU: 3 PID: 20724 at /build/buildd/linux-3.13.0/fs/proc/generic.c:511 remove_proc_entry+0x139/0x1b0()\n[536215.748367] name 'fs/nfsfs'\n[536215.748368] Modules linked in: veth xt_nat xt_tcpudp xt_addrtype xt_conntrack ipt_MASQUERADE iptable_nat nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack bridge stp llc aufs iptable_filter ip_tables x_tables nfsv3 qxl ttm drm_kms_helper serio_raw drm i2c_piix4 mac_hid nfsd auth_rpcgss nfs_acl nfs lockd sunrpc lp fscache parport xfs libcrc32c psmouse floppy\n[536215.748397] CPU: 3 PID: 20724 Comm: kworker/u8:0 Tainted: G        W     3.13.0-39-generic #66-Ubuntu\n[536215.748399] Hardware name: Red Hat RHEV Hypervisor, BIOS 0.5.1 01/01/2007\n[536215.748404] Workqueue: netns cleanup_net\n[536215.748406]  0000000000000009 ffff88010b191c80 ffffffff8171ece7 ffff88010b191cc8\n[536215.748410]  ffff88010b191cb8 ffffffff8106773d 0000000000000000 0000000000000005\n[536215.748413]  ffffffffa01cd8a8 ffff88021701da30 0000000000000100 ffff88010b191d18\n[536215.748417] Call Trace:\n[536215.748422]  [<ffffffff8171ece7>] dump_stack+0x45/0x56\n[536215.748427]  [<ffffffff8106773d>] warn_slowpath_common+0x7d/0xa0\n[536215.748431]  [<ffffffff810677ac>] warn_slowpath_fmt+0x4c/0x50\n[536215.748436]  [<ffffffff81229889>] remove_proc_entry+0x139/0x1b0\n[536215.748450]  [<ffffffffa01ad4e2>] nfs_fs_proc_net_exit+0x62/0x70 [nfs]\n[536215.748460]  [<ffffffffa01b35b2>] nfs_net_exit+0x12/0x20 [nfs]\n[536215.748463]  [<ffffffff8161b3a9>] ops_exit_list.isra.1+0x39/0x60\n[536215.748467]  [<ffffffff8161bc30>] cleanup_net+0x110/0x250\n[536215.748471]  [<ffffffff810839c2>] process_one_work+0x182/0x450\n[536215.748475]  [<ffffffff810847b1>] worker_thread+0x121/0x410\n[536215.748478]  [<ffffffff81084690>] ? rescuer_thread+0x430/0x430\n[536215.748482]  [<ffffffff8108b492>] kthread+0xd2/0xf0\n[536215.748485]  [<ffffffff8108b3c0>] ? kthread_create_on_node+0x1c0/0x1c0\n[536215.748489]  [<ffffffff8172f73c>] ret_from_fork+0x7c/0xb0\n[536215.748492]  [<ffffffff8108b3c0>] ? kthread_create_on_node+0x1c0/0x1c0\n[536215.748494] ---[ end trace cb51192a6e5a74bf ]---\n. @vmarmol Sorry for the very late reply.\nThe --logtostderr options outputs nothin.\nI removed all the cadvisor.* files from /var/run and cadvisor started running again. Althought the initial problem remained.\nHere is the log output from /var/run/cadvisor.INFO\n```\nroot@par-staging:/var/run# tail -f cadvisor.INFO\nLog file created at: 2015/01/08 14:29:56\nRunning on machine: 399e0e6bc19a\nBinary: Built with gc go1.3.3 for linux/amd64\nLog line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\nI0108 14:29:56.603698 00001 storagedriver.go:89] Caching 60 recent stats in memory; using \"\" storage driver\nI0108 14:29:56.604587 00001 manager.go:78] cAdvisor running in container: \"/docker/399e0e6bc19ad226f497e13cf89784c1abfecdf563bf07cb1744aa6645a37e67\"\nI0108 14:29:56.611551 00001 manager.go:92] Machine: {NumCores:4 CpuFrequency:2599998 MemoryCapacity:8373039104 Filesystems:[] DiskMap:map[252:0:{Name:dm-0 Major:252 Minor:0 Size:2046820352} 252:1:{Name:dm-1 Major:252 Minor:1 Size:83324043264} 11:0:{Name:sr0 Major:11 Minor:0 Size:1073741312} 253:0:{Name:vda Major:253 Minor:0 Size:85899345920}] Topology:[{Id:0 Memory:8373039104 Cores:[{Id:0 Threads:[0] Caches:[{Size:65536 Type:Data Level:1} {Size:65536 Type:Instruction Level:1} {Size:524288 Type:Unified Level:2}]}] Caches:[]} {Id:1 Memory:0 Cores:[{Id:0 Threads:[1] Caches:[{Size:65536 Type:Data Level:1} {Size:65536 Type:Instruction Level:1} {Size:524288 Type:Unified Level:2}]}] Caches:[]} {Id:2 Memory:0 Cores:[{Id:0 Threads:[2] Caches:[{Size:65536 Type:Data Level:1} {Size:65536 Type:Instruction Level:1} {Size:524288 Type:Unified Level:2}]}] Caches:[]} {Id:3 Memory:0 Cores:[{Id:0 Threads:[3] Caches:[{Size:65536 Type:Data Level:1} {Size:65536 Type:Instruction Level:1} {Size:524288 Type:Unified Level:2}]}] Caches:[]}]}\nI0108 14:29:56.612760 00001 manager.go:99] Version: {KernelVersion:3.13.0-39-generic ContainerOsVersion:Buildroot 2014.02 DockerVersion:1.3.1 CadvisorVersion:0.7.1}\nI0108 14:29:56.648702 00001 factory.go:231] Registering Docker factory\nI0108 14:29:56.653585 00001 factory.go:56] Registering Raw factory\nI0108 14:29:56.657551 00001 manager.go:395] Added container: \"/\" (aliases: [], namespace: \"\")\nI0108 14:29:56.657583 00001 manager.go:132] Starting recovery of all containers\nI0108 14:29:56.657635 00001 container.go:154] Start housekeeping for container \"/\"\nI0108 14:29:56.690146 00001 manager.go:395] Added container: \"/docker/74f10ac1648032a449b5d5a933ccde64eb514eaa3216d5fbc3012d62dd097ca8\" (aliases: [cache02 74f10ac1648032a449b5d5a933ccde64eb514eaa3216d5fbc3012d62dd097ca8], namespace: \"docker\")\nI0108 14:29:56.690256 00001 container.go:154] Start housekeeping for container \"/docker/74f10ac1648032a449b5d5a933ccde64eb514eaa3216d5fbc3012d62dd097ca8\"\nI0108 14:29:56.699514 00001 manager.go:395] Added container: \"/docker/d0e7cc44b729fe242492af5257362e7d547725b41c3fff879f822301df1ea8d6\" (aliases: [bax-memcached d0e7cc44b729fe242492af5257362e7d547725b41c3fff879f822301df1ea8d6], namespace: \"docker\")\nI0108 14:29:56.699751 00001 container.go:154] Start housekeeping for container \"/docker/d0e7cc44b729fe242492af5257362e7d547725b41c3fff879f822301df1ea8d6\"\nI0108 14:29:56.703694 00001 manager.go:395] Added container: \"/user/0.user/64.session\" (aliases: [], namespace: \"\")\nI0108 14:29:56.703867 00001 container.go:154] Start housekeeping for container \"/user/0.user/64.session\"\nI0108 14:29:56.712495 00001 manager.go:395] Added container: \"/docker/02542e45b912653a587540d47b955947c4fc82e14e33ee9a0227e8bcdfc44315\" (aliases: [web04 02542e45b912653a587540d47b955947c4fc82e14e33ee9a0227e8bcdfc44315], namespace: \"docker\")\nI0108 14:29:56.712595 00001 container.go:154] Start housekeeping for container \"/docker/02542e45b912653a587540d47b955947c4fc82e14e33ee9a0227e8bcdfc44315\"\nI0108 14:29:56.720694 00001 manager.go:395] Added container: \"/docker/2dc6b0db171d1902e9e302318e6711649f41130d2bea4e3217bff32ad21c2e66\" (aliases: [api02 2dc6b0db171d1902e9e302318e6711649f41130d2bea4e3217bff32ad21c2e66], namespace: \"docker\")\nI0108 14:29:56.720829 00001 container.go:154] Start housekeeping for container \"/docker/2dc6b0db171d1902e9e302318e6711649f41130d2bea4e3217bff32ad21c2e66\"\nI0108 14:29:56.728897 00001 manager.go:395] Added container: \"/docker/611d1adb0651f80a3338761ae17f0f6d7af98a3c05895cc21fdf558d679cd10c\" (aliases: [mysql01 611d1adb0651f80a3338761ae17f0f6d7af98a3c05895cc21fdf558d679cd10c], namespace: \"docker\")\nI0108 14:29:56.728987 00001 container.go:154] Start housekeeping for container \"/docker/611d1adb0651f80a3338761ae17f0f6d7af98a3c05895cc21fdf558d679cd10c\"\nI0108 14:29:56.736152 00001 manager.go:395] Added container: \"/docker/d7fdc950659c882f7218401a820d9884b66205d44465fd00a96dfeb2675fe642\" (aliases: [search02 d7fdc950659c882f7218401a820d9884b66205d44465fd00a96dfeb2675fe642], namespace: \"docker\")\nI0108 14:29:56.736231 00001 container.go:154] Start housekeeping for container \"/docker/d7fdc950659c882f7218401a820d9884b66205d44465fd00a96dfeb2675fe642\"\nI0108 14:29:56.741272 00001 manager.go:395] Added container: \"/user\" (aliases: [], namespace: \"\")\nI0108 14:29:56.741376 00001 container.go:154] Start housekeeping for container \"/user\"\nI0108 14:29:56.745628 00001 manager.go:395] Added container: \"/user/0.user/63.session\" (aliases: [], namespace: \"\")\nI0108 14:29:56.745690 00001 container.go:154] Start housekeeping for container \"/user/0.user/63.session\"\nI0108 14:29:56.753684 00001 manager.go:395] Added container: \"/docker/2602e369463b14c324859692011d6fa24373425bb789f6663562c84b0dd68aaf\" (aliases: [web03 2602e369463b14c324859692011d6fa24373425bb789f6663562c84b0dd68aaf], namespace: \"docker\")\nI0108 14:29:56.753784 00001 container.go:154] Start housekeeping for container \"/docker/2602e369463b14c324859692011d6fa24373425bb789f6663562c84b0dd68aaf\"\nI0108 14:29:56.761585 00001 manager.go:395] Added container: \"/docker/65111b78f9c0c9c957d82e3219153df4e726b3f6a709404e494038358c44e090\" (aliases: [search01 65111b78f9c0c9c957d82e3219153df4e726b3f6a709404e494038358c44e090], namespace: \"docker\")\nI0108 14:29:56.761661 00001 container.go:154] Start housekeeping for container \"/docker/65111b78f9c0c9c957d82e3219153df4e726b3f6a709404e494038358c44e090\"\nI0108 14:29:56.769576 00001 manager.go:395] Added container: \"/docker/8d4d69029412ea0e9c77d9501519c1bed524e971eb46c2b91d71b5bec0e280b2\" (aliases: [lb01 8d4d69029412ea0e9c77d9501519c1bed524e971eb46c2b91d71b5bec0e280b2], namespace: \"docker\")\nI0108 14:29:56.769669 00001 container.go:154] Start housekeeping for container \"/docker/8d4d69029412ea0e9c77d9501519c1bed524e971eb46c2b91d71b5bec0e280b2\"\nI0108 14:29:56.773907 00001 manager.go:395] Added container: \"/user/0.user/62.session\" (aliases: [], namespace: \"\")\nI0108 14:29:56.774015 00001 container.go:154] Start housekeeping for container \"/user/0.user/62.session\"\nI0108 14:29:56.778357 00001 manager.go:395] Added container: \"/docker\" (aliases: [], namespace: \"\")\nI0108 14:29:56.778547 00001 container.go:154] Start housekeeping for container \"/docker\"\nI0108 14:29:56.785744 00001 manager.go:395] Added container: \"/docker/4d0445f9afa5d33d9f81d83748f898797b480132e1fde5462f209e4d666dd783\" (aliases: [cache01 4d0445f9afa5d33d9f81d83748f898797b480132e1fde5462f209e4d666dd783], namespace: \"docker\")\nI0108 14:29:56.785872 00001 container.go:154] Start housekeeping for container \"/docker/4d0445f9afa5d33d9f81d83748f898797b480132e1fde5462f209e4d666dd783\"\nI0108 14:29:56.792650 00001 manager.go:395] Added container: \"/docker/cd8e9930f1638b7bf85ddaa1c343b6f4ff1e662a4030edf5ef9476c9cba93ea1\" (aliases: [api01 cd8e9930f1638b7bf85ddaa1c343b6f4ff1e662a4030edf5ef9476c9cba93ea1], namespace: \"docker\")\nI0108 14:29:56.792852 00001 container.go:154] Start housekeeping for container \"/docker/cd8e9930f1638b7bf85ddaa1c343b6f4ff1e662a4030edf5ef9476c9cba93ea1\"\nI0108 14:29:56.800264 00001 manager.go:395] Added container: \"/docker/e8200d2b2a57928c668b8025d60700447b908fe79e4a52a94c1a9b972a13dad8\" (aliases: [bax-mobile-memcached e8200d2b2a57928c668b8025d60700447b908fe79e4a52a94c1a9b972a13dad8], namespace: \"docker\")\nI0108 14:29:56.800350 00001 container.go:154] Start housekeeping for container \"/docker/e8200d2b2a57928c668b8025d60700447b908fe79e4a52a94c1a9b972a13dad8\"\nI0108 14:29:56.807373 00001 manager.go:395] Added container: \"/docker/f98637ff412b6317b699c85574e33e808a8541ab6ac086802a3d9e131542a54e\" (aliases: [web02 f98637ff412b6317b699c85574e33e808a8541ab6ac086802a3d9e131542a54e], namespace: \"docker\")\nI0108 14:29:56.807459 00001 container.go:154] Start housekeeping for container \"/docker/f98637ff412b6317b699c85574e33e808a8541ab6ac086802a3d9e131542a54e\"\nI0108 14:29:56.811405 00001 manager.go:395] Added container: \"/user/0.user\" (aliases: [], namespace: \"\")\nI0108 14:29:56.811572 00001 container.go:154] Start housekeeping for container \"/user/0.user\"\nI0108 14:29:56.815609 00001 manager.go:395] Added container: \"/user/0.user/65.session\" (aliases: [], namespace: \"\")\nI0108 14:29:56.815678 00001 container.go:154] Start housekeeping for container \"/user/0.user/65.session\"\nI0108 14:29:56.822617 00001 manager.go:395] Added container: \"/docker/399e0e6bc19ad226f497e13cf89784c1abfecdf563bf07cb1744aa6645a37e67\" (aliases: [cadvisor 399e0e6bc19ad226f497e13cf89784c1abfecdf563bf07cb1744aa6645a37e67], namespace: \"docker\")\nI0108 14:29:56.822699 00001 container.go:154] Start housekeeping for container \"/docker/399e0e6bc19ad226f497e13cf89784c1abfecdf563bf07cb1744aa6645a37e67\"\nI0108 14:29:56.829867 00001 manager.go:395] Added container: \"/docker/b049808bfa077669191b50fadd489f10a5a76e63378001187c08d65690a1ce7e\" (aliases: [mobile01 b049808bfa077669191b50fadd489f10a5a76e63378001187c08d65690a1ce7e], namespace: \"docker\")\nI0108 14:29:56.829951 00001 container.go:154] Start housekeeping for container \"/docker/b049808bfa077669191b50fadd489f10a5a76e63378001187c08d65690a1ce7e\"\nI0108 14:29:56.838514 00001 manager.go:395] Added container: \"/docker/dbcac66ac6bb64cd60d9e048abd70ccee9e20af4f414303059d5c458a9c5131b\" (aliases: [web01 dbcac66ac6bb64cd60d9e048abd70ccee9e20af4f414303059d5c458a9c5131b], namespace: \"docker\")\nI0108 14:29:56.838553 00001 manager.go:137] Recovery completed\nI0108 14:29:56.839452 00001 container.go:154] Start housekeeping for container \"/docker/dbcac66ac6bb64cd60d9e048abd70ccee9e20af4f414303059d5c458a9c5131b\"\nI0108 14:29:56.884642 00001 cadvisor.go:150] Starting cAdvisor version: \"0.7.1\" on port 8080\n```\nDoes this help?\n. Here is the output.\n```\ncAdvisor version: 0.7.1\nOS version: Buildroot 2014.02\nKernel version: [Supported and recommended]\n    Kernel version is 3.13.0-39-generic. Versions >= 2.6 are supported. 3.0+ are recommended.\nCgroup setup: [Supported and recommended]\n    Available cgroups: map[cpuacct:1 memory:1 devices:1 freezer:1 blkio:1 hugetlb:1 cpuset:1 cpu:1 perf_event:1]\n    Following cgroups are required: [cpu cpuacct]\n    Following other cgroups are recommended: [memory blkio cpuset devices freezer]\nCgroup mount setup: [Supported, but not recommended]\n    Cgroups are mounted at /rootfs/sys/fs/cgroup.\n    Cgroup mount directories: blkio cpu cpuacct cpuset devices freezer hugetlb memory perf_event systemd \n    Any cgroup mount point that is detectible and accessible is supported. /sys/fs/cgroup is recommended as a standard location.\n    Cgroup mounts:\n    cgroup /rootfs/sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /rootfs/sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /rootfs/sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /rootfs/sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /rootfs/sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /rootfs/sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /rootfs/sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/b9ae85d7256636f2adb2b0da477be212c622198a735f111ce2af408c5728fd03/sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/b9ae85d7256636f2adb2b0da477be212c622198a735f111ce2af408c5728fd03/sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /var/lib/docker/aufs/mnt/b9ae85d7256636f2adb2b0da477be212c622198a735f111ce2af408c5728fd03/sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/b9ae85d7256636f2adb2b0da477be212c622198a735f111ce2af408c5728fd03/sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /var/lib/docker/aufs/mnt/b9ae85d7256636f2adb2b0da477be212c622198a735f111ce2af408c5728fd03/sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/b9ae85d7256636f2adb2b0da477be212c622198a735f111ce2af408c5728fd03/sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/b9ae85d7256636f2adb2b0da477be212c622198a735f111ce2af408c5728fd03/sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/b9ae85d7256636f2adb2b0da477be212c622198a735f111ce2af408c5728fd03/sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/b9ae85d7256636f2adb2b0da477be212c622198a735f111ce2af408c5728fd03/sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /var/lib/docker/aufs/mnt/b9ae85d7256636f2adb2b0da477be212c622198a735f111ce2af408c5728fd03/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/b9ae85d7256636f2adb2b0da477be212c622198a735f111ce2af408c5728fd03/rootfs/sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/b9ae85d7256636f2adb2b0da477be212c622198a735f111ce2af408c5728fd03/rootfs/sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /var/lib/docker/aufs/mnt/b9ae85d7256636f2adb2b0da477be212c622198a735f111ce2af408c5728fd03/rootfs/sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/b9ae85d7256636f2adb2b0da477be212c622198a735f111ce2af408c5728fd03/rootfs/sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /var/lib/docker/aufs/mnt/b9ae85d7256636f2adb2b0da477be212c622198a735f111ce2af408c5728fd03/rootfs/sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/b9ae85d7256636f2adb2b0da477be212c622198a735f111ce2af408c5728fd03/rootfs/sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/b9ae85d7256636f2adb2b0da477be212c622198a735f111ce2af408c5728fd03/rootfs/sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/b9ae85d7256636f2adb2b0da477be212c622198a735f111ce2af408c5728fd03/rootfs/sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/b9ae85d7256636f2adb2b0da477be212c622198a735f111ce2af408c5728fd03/rootfs/sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /var/lib/docker/aufs/mnt/b9ae85d7256636f2adb2b0da477be212c622198a735f111ce2af408c5728fd03/rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\nDocker version: [Supported and recommended]\n    Docker version is 1.3.1. Versions >= 1.0 are supported. 1.2+ are recommended.\nDocker driver setup: [Supported and recommended]\n    Docker exec driver is native-0.2. Storage driver is aufs.\n    Cgroups are being created through cgroup filesystem.\n```\n. Tried the newer version, unfortunately still the same problem. \nAnd no error found in the Docker driver setup section\n```\ncAdvisor version: 0.8.0\nOS version: Buildroot 2014.02\nKernel version: [Supported and recommended]\n    Kernel version is 3.13.0-44-generic. Versions >= 2.6 are supported. 3.0+ are recommended.\nCgroup setup: [Supported and recommended]\n    Available cgroups: map[blkio:1 hugetlb:1 cpuacct:1 memory:1 devices:1 freezer:1 cpuset:1 cpu:1 perf_event:1]\n    Following cgroups are required: [cpu cpuacct]\n    Following other cgroups are recommended: [memory blkio cpuset devices freezer]\nCgroup mount setup: [Supported, but not recommended]\n    Cgroups are mounted at /rootfs/sys/fs/cgroup.\n    Cgroup mount directories: blkio cpu cpuacct cpuset devices freezer hugetlb memory perf_event systemd \n    Any cgroup mount point that is detectible and accessible is supported. /sys/fs/cgroup is recommended as a standard location.\n    Cgroup mounts:\n    cgroup /rootfs/sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /rootfs/sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /rootfs/sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /rootfs/sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /rootfs/sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /rootfs/sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /rootfs/sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/rootfs/sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/rootfs/sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/rootfs/sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/rootfs/sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/rootfs/sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/rootfs/sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/rootfs/sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/rootfs/sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/rootfs/sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /var/lib/docker/aufs/mnt/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982/rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\nDocker version: [Supported and recommended]\n    Docker version is 1.3.1. Versions >= 1.0 are supported. 1.2+ are recommended.\nDocker driver setup: [Supported and recommended]\n    Docker exec driver is native-0.2. Storage driver is aufs.\n    Cgroups are being created through cgroup filesystem.\n    Docker container state directory is at \"/var/lib/docker/execdriver/native\" and is accessible.\n```\nLogs below\n/tmp # cat cadvisor.WARNING\nLog file created at: 2015/01/26 12:45:55\nRunning on machine: 062f8bcf282a\nBinary: Built with gc go1.3 for linux/amd64\nLog line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\nW0126 12:45:55.546356 00001 container.go:124] Failed to get RecentStats(\"/docker/8d4d69029412ea0e9c77d9501519c1bed524e971eb46c2b91d71b5bec0e280b2\") while determining the next housekeeping: unable to find data for container /docker/8d4d69029412ea0e9c77d9501519c1bed524e971eb46c2b91d71b5bec0e280b2\n/tmp # cat cadvisor.INFO \nLog file created at: 2015/01/26 12:42:51\nRunning on machine: 062f8bcf282a\nBinary: Built with gc go1.3 for linux/amd64\nLog line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\nI0126 12:42:51.911790 00001 storagedriver.go:89] Caching 60 recent stats in memory; using \"\" storage driver\nI0126 12:42:51.912279 00001 manager.go:78] cAdvisor running in container: \"/docker/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982\"\nI0126 12:42:51.922795 00001 manager.go:92] Machine: {NumCores:4 CpuFrequency:2599998 MemoryCapacity:8373043200 Filesystems:[] DiskMap:map[252:0:{Name:dm-0 Major:252 Minor:0 Size:2046820352} 252:1:{Name:dm-1 Major:252 Minor:1 Size:83324043264} 253:0:{Name:vda Major:253 Minor:0 Size:85899345920}] NetworkDevices:[{Name:eth0 MacAddress:00:1a:4a:66:dd:07 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:8373043200 Cores:[{Id:0 Threads:[0] Caches:[{Size:65536 Type:Data Level:1} {Size:65536 Type:Instruction Level:1} {Size:524288 Type:Unified Level:2}]}] Caches:[]} {Id:1 Memory:0 Cores:[{Id:0 Threads:[1] Caches:[{Size:65536 Type:Data Level:1} {Size:65536 Type:Instruction Level:1} {Size:524288 Type:Unified Level:2}]}] Caches:[]} {Id:2 Memory:0 Cores:[{Id:0 Threads:[2] Caches:[{Size:65536 Type:Data Level:1} {Size:65536 Type:Instruction Level:1} {Size:524288 Type:Unified Level:2}]}] Caches:[]} {Id:3 Memory:0 Cores:[{Id:0 Threads:[3] Caches:[{Size:65536 Type:Data Level:1} {Size:65536 Type:Instruction Level:1} {Size:524288 Type:Unified Level:2}]}] Caches:[]}]}\nI0126 12:42:51.944955 00001 manager.go:99] Version: {KernelVersion:3.13.0-44-generic ContainerOsVersion:Buildroot 2014.02 DockerVersion:1.3.1 CadvisorVersion:0.8.0}\nI0126 12:42:54.485820 00001 factory.go:231] Registering Docker factory\nI0126 12:42:54.489955 00001 factory.go:56] Registering Raw factory\nI0126 12:42:54.494081 00001 manager.go:395] Added container: \"/\" (aliases: [], namespace: \"\")\nI0126 12:42:54.494120 00001 manager.go:132] Starting recovery of all containers\nI0126 12:42:54.494276 00001 container.go:154] Start housekeeping for container \"/\"\nI0126 12:42:54.525841 00001 manager.go:395] Added container: \"/docker/2602e369463b14c324859692011d6fa24373425bb789f6663562c84b0dd68aaf\" (aliases: [web03 2602e369463b14c324859692011d6fa24373425bb789f6663562c84b0dd68aaf], namespace: \"docker\")\nI0126 12:42:54.525979 00001 container.go:154] Start housekeeping for container \"/docker/2602e369463b14c324859692011d6fa24373425bb789f6663562c84b0dd68aaf\"\nI0126 12:42:54.526896 00001 container.go:205] Failed to update stats for container \"/docker/2602e369463b14c324859692011d6fa24373425bb789f6663562c84b0dd68aaf\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:42:54.535251 00001 manager.go:395] Added container: \"/docker/65111b78f9c0c9c957d82e3219153df4e726b3f6a709404e494038358c44e090\" (aliases: [search01 65111b78f9c0c9c957d82e3219153df4e726b3f6a709404e494038358c44e090], namespace: \"docker\")\nI0126 12:42:54.535371 00001 container.go:154] Start housekeeping for container \"/docker/65111b78f9c0c9c957d82e3219153df4e726b3f6a709404e494038358c44e090\"\nI0126 12:42:54.536472 00001 container.go:205] Failed to update stats for container \"/docker/65111b78f9c0c9c957d82e3219153df4e726b3f6a709404e494038358c44e090\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:42:54.545097 00001 manager.go:395] Added container: \"/docker/8d4d69029412ea0e9c77d9501519c1bed524e971eb46c2b91d71b5bec0e280b2\" (aliases: [lb01 8d4d69029412ea0e9c77d9501519c1bed524e971eb46c2b91d71b5bec0e280b2], namespace: \"docker\")\nI0126 12:42:54.545317 00001 container.go:154] Start housekeeping for container \"/docker/8d4d69029412ea0e9c77d9501519c1bed524e971eb46c2b91d71b5bec0e280b2\"\nI0126 12:42:54.546235 00001 container.go:205] Failed to update stats for container \"/docker/8d4d69029412ea0e9c77d9501519c1bed524e971eb46c2b91d71b5bec0e280b2\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:42:54.555421 00001 manager.go:395] Added container: \"/docker/d7fdc950659c882f7218401a820d9884b66205d44465fd00a96dfeb2675fe642\" (aliases: [search02 d7fdc950659c882f7218401a820d9884b66205d44465fd00a96dfeb2675fe642], namespace: \"docker\")\nI0126 12:42:54.555554 00001 container.go:154] Start housekeeping for container \"/docker/d7fdc950659c882f7218401a820d9884b66205d44465fd00a96dfeb2675fe642\"\nI0126 12:42:54.556601 00001 container.go:205] Failed to update stats for container \"/docker/d7fdc950659c882f7218401a820d9884b66205d44465fd00a96dfeb2675fe642\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:42:54.561601 00001 manager.go:395] Added container: \"/docker\" (aliases: [], namespace: \"\")\nI0126 12:42:54.561711 00001 container.go:154] Start housekeeping for container \"/docker\"\nI0126 12:42:54.569915 00001 manager.go:395] Added container: \"/docker/02542e45b912653a587540d47b955947c4fc82e14e33ee9a0227e8bcdfc44315\" (aliases: [web04 02542e45b912653a587540d47b955947c4fc82e14e33ee9a0227e8bcdfc44315], namespace: \"docker\")\nI0126 12:42:54.569968 00001 container.go:154] Start housekeeping for container \"/docker/02542e45b912653a587540d47b955947c4fc82e14e33ee9a0227e8bcdfc44315\"\nI0126 12:42:54.570777 00001 container.go:205] Failed to update stats for container \"/docker/02542e45b912653a587540d47b955947c4fc82e14e33ee9a0227e8bcdfc44315\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:42:54.578528 00001 manager.go:395] Added container: \"/docker/2dc6b0db171d1902e9e302318e6711649f41130d2bea4e3217bff32ad21c2e66\" (aliases: [api02 2dc6b0db171d1902e9e302318e6711649f41130d2bea4e3217bff32ad21c2e66], namespace: \"docker\")\nI0126 12:42:54.578613 00001 container.go:154] Start housekeeping for container \"/docker/2dc6b0db171d1902e9e302318e6711649f41130d2bea4e3217bff32ad21c2e66\"\nI0126 12:42:54.579510 00001 container.go:205] Failed to update stats for container \"/docker/2dc6b0db171d1902e9e302318e6711649f41130d2bea4e3217bff32ad21c2e66\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:42:54.587017 00001 manager.go:395] Added container: \"/docker/611d1adb0651f80a3338761ae17f0f6d7af98a3c05895cc21fdf558d679cd10c\" (aliases: [mysql01 611d1adb0651f80a3338761ae17f0f6d7af98a3c05895cc21fdf558d679cd10c], namespace: \"docker\")\nI0126 12:42:54.587126 00001 container.go:154] Start housekeeping for container \"/docker/611d1adb0651f80a3338761ae17f0f6d7af98a3c05895cc21fdf558d679cd10c\"\nI0126 12:42:54.594063 00001 container.go:205] Failed to update stats for container \"/docker/611d1adb0651f80a3338761ae17f0f6d7af98a3c05895cc21fdf558d679cd10c\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:42:54.595469 00001 manager.go:395] Added container: \"/docker/74f10ac1648032a449b5d5a933ccde64eb514eaa3216d5fbc3012d62dd097ca8\" (aliases: [cache02 74f10ac1648032a449b5d5a933ccde64eb514eaa3216d5fbc3012d62dd097ca8], namespace: \"docker\")\nI0126 12:42:54.595572 00001 container.go:154] Start housekeeping for container \"/docker/74f10ac1648032a449b5d5a933ccde64eb514eaa3216d5fbc3012d62dd097ca8\"\nI0126 12:42:54.596389 00001 container.go:205] Failed to update stats for container \"/docker/74f10ac1648032a449b5d5a933ccde64eb514eaa3216d5fbc3012d62dd097ca8\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:42:54.603955 00001 manager.go:395] Added container: \"/docker/d0e7cc44b729fe242492af5257362e7d547725b41c3fff879f822301df1ea8d6\" (aliases: [bax-memcached d0e7cc44b729fe242492af5257362e7d547725b41c3fff879f822301df1ea8d6], namespace: \"docker\")\nI0126 12:42:54.604207 00001 container.go:154] Start housekeeping for container \"/docker/d0e7cc44b729fe242492af5257362e7d547725b41c3fff879f822301df1ea8d6\"\nI0126 12:42:54.605776 00001 container.go:205] Failed to update stats for container \"/docker/d0e7cc44b729fe242492af5257362e7d547725b41c3fff879f822301df1ea8d6\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:42:54.616647 00001 manager.go:395] Added container: \"/docker/dbcac66ac6bb64cd60d9e048abd70ccee9e20af4f414303059d5c458a9c5131b\" (aliases: [web01 dbcac66ac6bb64cd60d9e048abd70ccee9e20af4f414303059d5c458a9c5131b], namespace: \"docker\")\nI0126 12:42:54.616749 00001 container.go:154] Start housekeeping for container \"/docker/dbcac66ac6bb64cd60d9e048abd70ccee9e20af4f414303059d5c458a9c5131b\"\nI0126 12:42:54.617756 00001 container.go:205] Failed to update stats for container \"/docker/dbcac66ac6bb64cd60d9e048abd70ccee9e20af4f414303059d5c458a9c5131b\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:42:54.625347 00001 manager.go:395] Added container: \"/docker/f98637ff412b6317b699c85574e33e808a8541ab6ac086802a3d9e131542a54e\" (aliases: [web02 f98637ff412b6317b699c85574e33e808a8541ab6ac086802a3d9e131542a54e], namespace: \"docker\")\nI0126 12:42:54.625476 00001 container.go:154] Start housekeeping for container \"/docker/f98637ff412b6317b699c85574e33e808a8541ab6ac086802a3d9e131542a54e\"\nI0126 12:42:54.626714 00001 container.go:205] Failed to update stats for container \"/docker/f98637ff412b6317b699c85574e33e808a8541ab6ac086802a3d9e131542a54e\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:42:54.630604 00001 manager.go:395] Added container: \"/user\" (aliases: [], namespace: \"\")\nI0126 12:42:54.630707 00001 container.go:154] Start housekeeping for container \"/user\"\nI0126 12:42:54.636446 00001 manager.go:395] Added container: \"/user/0.user\" (aliases: [], namespace: \"\")\nI0126 12:42:54.636550 00001 container.go:154] Start housekeeping for container \"/user/0.user\"\nI0126 12:42:54.644883 00001 manager.go:395] Added container: \"/docker/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982\" (aliases: [cadvisor 062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982], namespace: \"docker\")\nI0126 12:42:54.645002 00001 container.go:154] Start housekeeping for container \"/docker/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982\"\nI0126 12:42:54.645857 00001 container.go:205] Failed to update stats for container \"/docker/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:42:54.655684 00001 manager.go:395] Added container: \"/docker/4d0445f9afa5d33d9f81d83748f898797b480132e1fde5462f209e4d666dd783\" (aliases: [cache01 4d0445f9afa5d33d9f81d83748f898797b480132e1fde5462f209e4d666dd783], namespace: \"docker\")\nI0126 12:42:54.657641 00001 container.go:154] Start housekeeping for container \"/docker/4d0445f9afa5d33d9f81d83748f898797b480132e1fde5462f209e4d666dd783\"\nI0126 12:42:54.658735 00001 container.go:205] Failed to update stats for container \"/docker/4d0445f9afa5d33d9f81d83748f898797b480132e1fde5462f209e4d666dd783\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:42:54.663958 00001 manager.go:395] Added container: \"/docker/cd8e9930f1638b7bf85ddaa1c343b6f4ff1e662a4030edf5ef9476c9cba93ea1\" (aliases: [api01 cd8e9930f1638b7bf85ddaa1c343b6f4ff1e662a4030edf5ef9476c9cba93ea1], namespace: \"docker\")\nI0126 12:42:54.666713 00001 container.go:154] Start housekeeping for container \"/docker/cd8e9930f1638b7bf85ddaa1c343b6f4ff1e662a4030edf5ef9476c9cba93ea1\"\nI0126 12:42:54.667569 00001 container.go:205] Failed to update stats for container \"/docker/cd8e9930f1638b7bf85ddaa1c343b6f4ff1e662a4030edf5ef9476c9cba93ea1\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:42:54.668156 00001 manager.go:395] Added container: \"/user/0.user/20.session\" (aliases: [], namespace: \"\")\nI0126 12:42:54.668199 00001 manager.go:137] Recovery completed\nI0126 12:42:54.668215 00001 container.go:154] Start housekeeping for container \"/user/0.user/20.session\"\nI0126 12:42:54.706007 00001 cadvisor.go:150] Starting cAdvisor version: \"0.8.0\" on port 8080\nI0126 12:43:54.529308 00001 container.go:205] Failed to update stats for container \"/docker/2602e369463b14c324859692011d6fa24373425bb789f6663562c84b0dd68aaf\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:43:54.536715 00001 container.go:205] Failed to update stats for container \"/docker/65111b78f9c0c9c957d82e3219153df4e726b3f6a709404e494038358c44e090\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:43:54.546777 00001 container.go:205] Failed to update stats for container \"/docker/8d4d69029412ea0e9c77d9501519c1bed524e971eb46c2b91d71b5bec0e280b2\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:43:54.556909 00001 container.go:205] Failed to update stats for container \"/docker/d7fdc950659c882f7218401a820d9884b66205d44465fd00a96dfeb2675fe642\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:43:54.571779 00001 container.go:205] Failed to update stats for container \"/docker/02542e45b912653a587540d47b955947c4fc82e14e33ee9a0227e8bcdfc44315\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:43:54.580035 00001 container.go:205] Failed to update stats for container \"/docker/2dc6b0db171d1902e9e302318e6711649f41130d2bea4e3217bff32ad21c2e66\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:43:54.597103 00001 container.go:205] Failed to update stats for container \"/docker/74f10ac1648032a449b5d5a933ccde64eb514eaa3216d5fbc3012d62dd097ca8\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:43:54.618042 00001 container.go:205] Failed to update stats for container \"/docker/dbcac66ac6bb64cd60d9e048abd70ccee9e20af4f414303059d5c458a9c5131b\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:43:54.626891 00001 container.go:205] Failed to update stats for container \"/docker/f98637ff412b6317b699c85574e33e808a8541ab6ac086802a3d9e131542a54e\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:43:54.646240 00001 container.go:205] Failed to update stats for container \"/docker/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:43:54.659032 00001 container.go:205] Failed to update stats for container \"/docker/4d0445f9afa5d33d9f81d83748f898797b480132e1fde5462f209e4d666dd783\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:43:54.667893 00001 container.go:205] Failed to update stats for container \"/docker/cd8e9930f1638b7bf85ddaa1c343b6f4ff1e662a4030edf5ef9476c9cba93ea1\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:43:55.588302 00001 container.go:205] Failed to update stats for container \"/docker/611d1adb0651f80a3338761ae17f0f6d7af98a3c05895cc21fdf558d679cd10c\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:43:55.605496 00001 container.go:205] Failed to update stats for container \"/docker/d0e7cc44b729fe242492af5257362e7d547725b41c3fff879f822301df1ea8d6\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:44:54.536872 00001 container.go:205] Failed to update stats for container \"/docker/65111b78f9c0c9c957d82e3219153df4e726b3f6a709404e494038358c44e090\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:44:54.646279 00001 container.go:205] Failed to update stats for container \"/docker/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:44:54.667944 00001 container.go:205] Failed to update stats for container \"/docker/cd8e9930f1638b7bf85ddaa1c343b6f4ff1e662a4030edf5ef9476c9cba93ea1\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:44:55.533410 00001 container.go:205] Failed to update stats for container \"/docker/2602e369463b14c324859692011d6fa24373425bb789f6663562c84b0dd68aaf\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:44:55.546354 00001 container.go:205] Failed to update stats for container \"/docker/8d4d69029412ea0e9c77d9501519c1bed524e971eb46c2b91d71b5bec0e280b2\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:44:55.557034 00001 container.go:205] Failed to update stats for container \"/docker/d7fdc950659c882f7218401a820d9884b66205d44465fd00a96dfeb2675fe642\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:44:55.574941 00001 container.go:205] Failed to update stats for container \"/docker/02542e45b912653a587540d47b955947c4fc82e14e33ee9a0227e8bcdfc44315\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:44:55.583575 00001 container.go:205] Failed to update stats for container \"/docker/2dc6b0db171d1902e9e302318e6711649f41130d2bea4e3217bff32ad21c2e66\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:44:55.596642 00001 container.go:205] Failed to update stats for container \"/docker/74f10ac1648032a449b5d5a933ccde64eb514eaa3216d5fbc3012d62dd097ca8\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:44:55.609433 00001 container.go:205] Failed to update stats for container \"/docker/d0e7cc44b729fe242492af5257362e7d547725b41c3fff879f822301df1ea8d6\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:44:55.618838 00001 container.go:205] Failed to update stats for container \"/docker/dbcac66ac6bb64cd60d9e048abd70ccee9e20af4f414303059d5c458a9c5131b\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:44:55.630819 00001 container.go:205] Failed to update stats for container \"/docker/f98637ff412b6317b699c85574e33e808a8541ab6ac086802a3d9e131542a54e\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:44:55.659597 00001 container.go:205] Failed to update stats for container \"/docker/4d0445f9afa5d33d9f81d83748f898797b480132e1fde5462f209e4d666dd783\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:44:56.588182 00001 container.go:205] Failed to update stats for container \"/docker/611d1adb0651f80a3338761ae17f0f6d7af98a3c05895cc21fdf558d679cd10c\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:45:54.646872 00001 container.go:205] Failed to update stats for container \"/docker/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:45:55.536596 00001 container.go:205] Failed to update stats for container \"/docker/65111b78f9c0c9c957d82e3219153df4e726b3f6a709404e494038358c44e090\": could not find device with major: 252, minor: 1 in cached partitions map\nW0126 12:45:55.546356 00001 container.go:124] Failed to get RecentStats(\"/docker/8d4d69029412ea0e9c77d9501519c1bed524e971eb46c2b91d71b5bec0e280b2\") while determining the next housekeeping: unable to find data for container /docker/8d4d69029412ea0e9c77d9501519c1bed524e971eb46c2b91d71b5bec0e280b2\nI0126 12:45:55.596687 00001 container.go:205] Failed to update stats for container \"/docker/74f10ac1648032a449b5d5a933ccde64eb514eaa3216d5fbc3012d62dd097ca8\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:45:55.667956 00001 container.go:205] Failed to update stats for container \"/docker/cd8e9930f1638b7bf85ddaa1c343b6f4ff1e662a4030edf5ef9476c9cba93ea1\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:45:56.527634 00001 container.go:205] Failed to update stats for container \"/docker/2602e369463b14c324859692011d6fa24373425bb789f6663562c84b0dd68aaf\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:45:56.557081 00001 container.go:205] Failed to update stats for container \"/docker/d7fdc950659c882f7218401a820d9884b66205d44465fd00a96dfeb2675fe642\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:45:56.571634 00001 container.go:205] Failed to update stats for container \"/docker/02542e45b912653a587540d47b955947c4fc82e14e33ee9a0227e8bcdfc44315\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:45:56.579990 00001 container.go:205] Failed to update stats for container \"/docker/2dc6b0db171d1902e9e302318e6711649f41130d2bea4e3217bff32ad21c2e66\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:45:56.588488 00001 container.go:205] Failed to update stats for container \"/docker/611d1adb0651f80a3338761ae17f0f6d7af98a3c05895cc21fdf558d679cd10c\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:45:56.606085 00001 container.go:205] Failed to update stats for container \"/docker/d0e7cc44b729fe242492af5257362e7d547725b41c3fff879f822301df1ea8d6\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:45:56.618316 00001 container.go:205] Failed to update stats for container \"/docker/dbcac66ac6bb64cd60d9e048abd70ccee9e20af4f414303059d5c458a9c5131b\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:45:56.628444 00001 container.go:205] Failed to update stats for container \"/docker/f98637ff412b6317b699c85574e33e808a8541ab6ac086802a3d9e131542a54e\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:45:56.659806 00001 container.go:205] Failed to update stats for container \"/docker/4d0445f9afa5d33d9f81d83748f898797b480132e1fde5462f209e4d666dd783\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:46:55.538351 00001 container.go:205] Failed to update stats for container \"/docker/65111b78f9c0c9c957d82e3219153df4e726b3f6a709404e494038358c44e090\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:46:55.546978 00001 container.go:205] Failed to update stats for container \"/docker/8d4d69029412ea0e9c77d9501519c1bed524e971eb46c2b91d71b5bec0e280b2\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:46:55.596806 00001 container.go:205] Failed to update stats for container \"/docker/74f10ac1648032a449b5d5a933ccde64eb514eaa3216d5fbc3012d62dd097ca8\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:46:55.646213 00001 container.go:205] Failed to update stats for container \"/docker/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:46:55.668073 00001 container.go:205] Failed to update stats for container \"/docker/cd8e9930f1638b7bf85ddaa1c343b6f4ff1e662a4030edf5ef9476c9cba93ea1\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:46:56.557117 00001 container.go:205] Failed to update stats for container \"/docker/d7fdc950659c882f7218401a820d9884b66205d44465fd00a96dfeb2675fe642\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:46:56.580077 00001 container.go:205] Failed to update stats for container \"/docker/2dc6b0db171d1902e9e302318e6711649f41130d2bea4e3217bff32ad21c2e66\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:46:56.588492 00001 container.go:205] Failed to update stats for container \"/docker/611d1adb0651f80a3338761ae17f0f6d7af98a3c05895cc21fdf558d679cd10c\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:46:56.621278 00001 container.go:205] Failed to update stats for container \"/docker/dbcac66ac6bb64cd60d9e048abd70ccee9e20af4f414303059d5c458a9c5131b\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:46:57.527377 00001 container.go:205] Failed to update stats for container \"/docker/2602e369463b14c324859692011d6fa24373425bb789f6663562c84b0dd68aaf\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:46:57.571292 00001 container.go:205] Failed to update stats for container \"/docker/02542e45b912653a587540d47b955947c4fc82e14e33ee9a0227e8bcdfc44315\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:46:57.605160 00001 container.go:205] Failed to update stats for container \"/docker/d0e7cc44b729fe242492af5257362e7d547725b41c3fff879f822301df1ea8d6\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:46:57.626510 00001 container.go:205] Failed to update stats for container \"/docker/f98637ff412b6317b699c85574e33e808a8541ab6ac086802a3d9e131542a54e\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:46:57.658918 00001 container.go:205] Failed to update stats for container \"/docker/4d0445f9afa5d33d9f81d83748f898797b480132e1fde5462f209e4d666dd783\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:47:55.596842 00001 container.go:205] Failed to update stats for container \"/docker/74f10ac1648032a449b5d5a933ccde64eb514eaa3216d5fbc3012d62dd097ca8\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:47:55.646236 00001 container.go:205] Failed to update stats for container \"/docker/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:47:56.537093 00001 container.go:205] Failed to update stats for container \"/docker/65111b78f9c0c9c957d82e3219153df4e726b3f6a709404e494038358c44e090\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:47:56.547059 00001 container.go:205] Failed to update stats for container \"/docker/8d4d69029412ea0e9c77d9501519c1bed524e971eb46c2b91d71b5bec0e280b2\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:47:56.563066 00001 container.go:205] Failed to update stats for container \"/docker/d7fdc950659c882f7218401a820d9884b66205d44465fd00a96dfeb2675fe642\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:47:56.580294 00001 container.go:205] Failed to update stats for container \"/docker/2dc6b0db171d1902e9e302318e6711649f41130d2bea4e3217bff32ad21c2e66\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:47:56.588993 00001 container.go:205] Failed to update stats for container \"/docker/611d1adb0651f80a3338761ae17f0f6d7af98a3c05895cc21fdf558d679cd10c\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:47:56.668304 00001 container.go:205] Failed to update stats for container \"/docker/cd8e9930f1638b7bf85ddaa1c343b6f4ff1e662a4030edf5ef9476c9cba93ea1\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:47:57.527775 00001 container.go:205] Failed to update stats for container \"/docker/2602e369463b14c324859692011d6fa24373425bb789f6663562c84b0dd68aaf\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:47:57.571353 00001 container.go:205] Failed to update stats for container \"/docker/02542e45b912653a587540d47b955947c4fc82e14e33ee9a0227e8bcdfc44315\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:47:57.605712 00001 container.go:205] Failed to update stats for container \"/docker/d0e7cc44b729fe242492af5257362e7d547725b41c3fff879f822301df1ea8d6\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:47:57.618485 00001 container.go:205] Failed to update stats for container \"/docker/dbcac66ac6bb64cd60d9e048abd70ccee9e20af4f414303059d5c458a9c5131b\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:47:57.626932 00001 container.go:205] Failed to update stats for container \"/docker/f98637ff412b6317b699c85574e33e808a8541ab6ac086802a3d9e131542a54e\": could not find device with major: 252, minor: 1 in cached partitions map\nI0126 12:47:57.659371 00001 container.go:205] Failed to update stats for container \"/docker/4d0445f9afa5d33d9f81d83748f898797b480132e1fde5462f209e4d666dd783\": could not find device with major: 252, minor: 1 in cached partitions map\n. I can access some containers now, but not all... \nHere is the log output\n```\n~ # docker run --rm --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro --publish=8080:8080 --name=cadvisor google/cadvisor:canary --logtostderr                                                                                         root@par-staging\nUnable to find image 'google/cadvisor:canary' locally\nPulling repository google/cadvisor\n83c486636955: Download complete \n511136ea3c5a: Download complete \ne2fb46397934: Download complete \n015fb409be0d: Download complete \n21082221cb6e: Download complete \nbbd692fe2ca1: Download complete \n8db8f013bfca: Download complete \n7fff0c6f0b8d: Download complete \n7acf13620725: Download complete \n6f114d3139e3: Download complete \n06ba2b469f02: Download complete \nd15a38b5a572: Download complete \n123194e726f3: Download complete \nStatus: Downloaded newer image for google/cadvisor:canary\nI0130 11:38:53.954794 00001 storagedriver.go:89] Caching 60 recent stats in memory; using \"\" storage driver\nI0130 11:38:53.955821 00001 manager.go:79] cAdvisor running in container: \"/docker/779c561e66c49cac55ab7040a18e53764f7d75e6ebc08812f9285833f0682d06\"\nI0130 11:38:53.960143 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:38:53.963788 00001 manager.go:93] Machine: {NumCores:4 CpuFrequency:2599998 MemoryCapacity:8373043200 Filesystems:[] DiskMap:map[252:0:{Name:dm-0 Major:252 Minor:0 Size:2046820352 Scheduler:none} 252:1:{Name:dm-1 Major:252 Minor:1 Size:83324043264 Scheduler:none} 253:0:{Name:vda Major:253 Minor:0 Size:85899345920 Scheduler:none}] NetworkDevices:[{Name:eth0 MacAddress:00:1a:4a:66:dd:07 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:8373043200 Cores:[{Id:0 Threads:[0] Caches:[{Size:65536 Type:Data Level:1} {Size:65536 Type:Instruction Level:1} {Size:524288 Type:Unified Level:2}]}] Caches:[]} {Id:1 Memory:0 Cores:[{Id:0 Threads:[1] Caches:[{Size:65536 Type:Data Level:1} {Size:65536 Type:Instruction Level:1} {Size:524288 Type:Unified Level:2}]}] Caches:[]} {Id:2 Memory:0 Cores:[{Id:0 Threads:[2] Caches:[{Size:65536 Type:Data Level:1} {Size:65536 Type:Instruction Level:1} {Size:524288 Type:Unified Level:2}]}] Caches:[]} {Id:3 Memory:0 Cores:[{Id:0 Threads:[3] Caches:[{Size:65536 Type:Data Level:1} {Size:65536 Type:Instruction Level:1} {Size:524288 Type:Unified Level:2}]}] Caches:[]}]}\nI0130 11:38:53.965108 00001 manager.go:100] Version: {KernelVersion:3.13.0-44-generic ContainerOsVersion:Buildroot 2014.02 DockerVersion:1.3.1 CadvisorVersion:0.8.0}\nI0130 11:38:54.011049 00001 factory.go:231] Registering Docker factory\nI0130 11:38:54.016015 00001 factory.go:56] Registering Raw factory\nI0130 11:38:54.016276 00001 cpuload.go:43] Using a sched debug based load reader\nI0130 11:38:54.031333 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:38:54.031437 00001 manager.go:414] Added container: \"/\" (aliases: [], namespace: \"\")\nI0130 11:38:54.031480 00001 manager.go:147] Starting recovery of all containers\nI0130 11:38:54.031623 00001 container.go:163] Start housekeeping for container \"/\"\nI0130 11:38:54.053066 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:38:54.053125 00001 manager.go:414] Added container: \"/docker/4e1cb6cbd5ccdf35878d0c8b7a9c39d0ead450e2108a4117cb39c3179c4f2351\" (aliases: [], namespace: \"\")\nI0130 11:38:54.053212 00001 container.go:163] Start housekeeping for container \"/docker/4e1cb6cbd5ccdf35878d0c8b7a9c39d0ead450e2108a4117cb39c3179c4f2351\"\nI0130 11:38:54.057902 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:38:54.057955 00001 manager.go:414] Added container: \"/user\" (aliases: [], namespace: \"\")\nI0130 11:38:54.058020 00001 container.go:163] Start housekeeping for container \"/user\"\nI0130 11:38:54.061361 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:38:54.061407 00001 manager.go:414] Added container: \"/user/0.user\" (aliases: [], namespace: \"\")\nI0130 11:38:54.061548 00001 container.go:163] Start housekeeping for container \"/user/0.user\"\nI0130 11:38:54.065182 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:38:54.065224 00001 manager.go:414] Added container: \"/user/0.user/30.session\" (aliases: [], namespace: \"\")\nI0130 11:38:54.065281 00001 container.go:163] Start housekeeping for container \"/user/0.user/30.session\"\nI0130 11:38:54.068681 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:38:54.068728 00001 manager.go:414] Added container: \"/docker\" (aliases: [], namespace: \"\")\nI0130 11:38:54.068781 00001 container.go:163] Start housekeeping for container \"/docker\"\nI0130 11:38:54.073694 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:38:54.073791 00001 manager.go:414] Added container: \"/docker/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982\" (aliases: [], namespace: \"\")\nI0130 11:38:54.074682 00001 container.go:163] Start housekeeping for container \"/docker/062f8bcf282ae4977c47446c1e4bb252d825a3ec97522d9a65db1ce86339c982\"\nI0130 11:38:54.079116 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:38:54.079172 00001 manager.go:414] Added container: \"/docker/611d1adb0651f80a3338761ae17f0f6d7af98a3c05895cc21fdf558d679cd10c\" (aliases: [], namespace: \"\")\nI0130 11:38:54.079358 00001 container.go:163] Start housekeeping for container \"/docker/611d1adb0651f80a3338761ae17f0f6d7af98a3c05895cc21fdf558d679cd10c\"\nI0130 11:38:54.084608 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:38:54.086419 00001 manager.go:414] Added container: \"/docker/779c561e66c49cac55ab7040a18e53764f7d75e6ebc08812f9285833f0682d06\" (aliases: [cadvisor 779c561e66c49cac55ab7040a18e53764f7d75e6ebc08812f9285833f0682d06], namespace: \"docker\")\nI0130 11:38:54.086521 00001 container.go:163] Start housekeeping for container \"/docker/779c561e66c49cac55ab7040a18e53764f7d75e6ebc08812f9285833f0682d06\"\nI0130 11:38:54.089038 00001 container.go:214] Failed to update stats for container \"/docker/779c561e66c49cac55ab7040a18e53764f7d75e6ebc08812f9285833f0682d06\": could not find device with major: 252, minor: 1 in cached partitions map\nI0130 11:38:54.091701 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:38:54.091801 00001 manager.go:414] Added container: \"/docker/86bd8e417375becdb96456b2827210d1dd1b6827bdb618a76465c6bda44b9630\" (aliases: [], namespace: \"\")\nI0130 11:38:54.091901 00001 manager.go:152] Recovery completed\nI0130 11:38:54.091989 00001 container.go:163] Start housekeeping for container \"/docker/86bd8e417375becdb96456b2827210d1dd1b6827bdb618a76465c6bda44b9630\"\nI0130 11:38:54.123217 00001 cadvisor.go:150] Starting cAdvisor version: \"0.8.0\" on port 8080\nI0130 11:39:31.663669 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:39:31.663743 00001 manager.go:414] Added container: \"/user/0.user/31.session\" (aliases: [], namespace: \"\")\nI0130 11:39:31.663783 00001 container.go:163] Start housekeeping for container \"/user/0.user/31.session\"\nI0130 11:39:37.194500 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:39:37.196695 00001 manager.go:414] Added container: \"/docker/d0e7cc44b729fe242492af5257362e7d547725b41c3fff879f822301df1ea8d6\" (aliases: [bax-memcached d0e7cc44b729fe242492af5257362e7d547725b41c3fff879f822301df1ea8d6], namespace: \"docker\")\nI0130 11:39:37.196820 00001 container.go:163] Start housekeeping for container \"/docker/d0e7cc44b729fe242492af5257362e7d547725b41c3fff879f822301df1ea8d6\"\nI0130 11:39:37.197901 00001 container.go:214] Failed to update stats for container \"/docker/d0e7cc44b729fe242492af5257362e7d547725b41c3fff879f822301df1ea8d6\": could not find device with major: 252, minor: 1 in cached partitions map\nI0130 11:39:37.583932 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:39:37.587998 00001 manager.go:414] Added container: \"/docker/65111b78f9c0c9c957d82e3219153df4e726b3f6a709404e494038358c44e090\" (aliases: [search01 65111b78f9c0c9c957d82e3219153df4e726b3f6a709404e494038358c44e090], namespace: \"docker\")\nI0130 11:39:37.588079 00001 container.go:163] Start housekeeping for container \"/docker/65111b78f9c0c9c957d82e3219153df4e726b3f6a709404e494038358c44e090\"\nI0130 11:39:37.589305 00001 container.go:214] Failed to update stats for container \"/docker/65111b78f9c0c9c957d82e3219153df4e726b3f6a709404e494038358c44e090\": could not find device with major: 252, minor: 1 in cached partitions map\nI0130 11:39:37.873375 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:39:37.875509 00001 manager.go:414] Added container: \"/docker/d7fdc950659c882f7218401a820d9884b66205d44465fd00a96dfeb2675fe642\" (aliases: [search02 d7fdc950659c882f7218401a820d9884b66205d44465fd00a96dfeb2675fe642], namespace: \"docker\")\nI0130 11:39:37.875562 00001 container.go:163] Start housekeeping for container \"/docker/d7fdc950659c882f7218401a820d9884b66205d44465fd00a96dfeb2675fe642\"\nI0130 11:39:37.877089 00001 container.go:214] Failed to update stats for container \"/docker/d7fdc950659c882f7218401a820d9884b66205d44465fd00a96dfeb2675fe642\": could not find device with major: 252, minor: 1 in cached partitions map\nI0130 11:39:38.291183 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:39:38.293239 00001 manager.go:414] Added container: \"/docker/cd8e9930f1638b7bf85ddaa1c343b6f4ff1e662a4030edf5ef9476c9cba93ea1\" (aliases: [api01 cd8e9930f1638b7bf85ddaa1c343b6f4ff1e662a4030edf5ef9476c9cba93ea1], namespace: \"docker\")\nI0130 11:39:38.293287 00001 container.go:163] Start housekeeping for container \"/docker/cd8e9930f1638b7bf85ddaa1c343b6f4ff1e662a4030edf5ef9476c9cba93ea1\"\nI0130 11:39:38.294440 00001 container.go:214] Failed to update stats for container \"/docker/cd8e9930f1638b7bf85ddaa1c343b6f4ff1e662a4030edf5ef9476c9cba93ea1\": could not find device with major: 252, minor: 1 in cached partitions map\nI0130 11:39:38.711339 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:39:38.713155 00001 manager.go:414] Added container: \"/docker/2dc6b0db171d1902e9e302318e6711649f41130d2bea4e3217bff32ad21c2e66\" (aliases: [api02 2dc6b0db171d1902e9e302318e6711649f41130d2bea4e3217bff32ad21c2e66], namespace: \"docker\")\nI0130 11:39:38.713229 00001 container.go:163] Start housekeeping for container \"/docker/2dc6b0db171d1902e9e302318e6711649f41130d2bea4e3217bff32ad21c2e66\"\nI0130 11:39:38.714460 00001 container.go:214] Failed to update stats for container \"/docker/2dc6b0db171d1902e9e302318e6711649f41130d2bea4e3217bff32ad21c2e66\": could not find device with major: 252, minor: 1 in cached partitions map\nI0130 11:39:39.157521 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:39:39.159805 00001 manager.go:414] Added container: \"/docker/dbcac66ac6bb64cd60d9e048abd70ccee9e20af4f414303059d5c458a9c5131b\" (aliases: [web01 dbcac66ac6bb64cd60d9e048abd70ccee9e20af4f414303059d5c458a9c5131b], namespace: \"docker\")\nI0130 11:39:39.159871 00001 container.go:163] Start housekeeping for container \"/docker/dbcac66ac6bb64cd60d9e048abd70ccee9e20af4f414303059d5c458a9c5131b\"\nI0130 11:39:39.160872 00001 container.go:214] Failed to update stats for container \"/docker/dbcac66ac6bb64cd60d9e048abd70ccee9e20af4f414303059d5c458a9c5131b\": could not find device with major: 252, minor: 1 in cached partitions map\nI0130 11:39:39.515726 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:39:39.518367 00001 manager.go:414] Added container: \"/docker/f98637ff412b6317b699c85574e33e808a8541ab6ac086802a3d9e131542a54e\" (aliases: [web02 f98637ff412b6317b699c85574e33e808a8541ab6ac086802a3d9e131542a54e], namespace: \"docker\")\nI0130 11:39:39.518405 00001 container.go:163] Start housekeeping for container \"/docker/f98637ff412b6317b699c85574e33e808a8541ab6ac086802a3d9e131542a54e\"\nI0130 11:39:39.519609 00001 container.go:214] Failed to update stats for container \"/docker/f98637ff412b6317b699c85574e33e808a8541ab6ac086802a3d9e131542a54e\": could not find device with major: 252, minor: 1 in cached partitions map\nI0130 11:39:39.857588 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:39:39.870317 00001 manager.go:414] Added container: \"/docker/2602e369463b14c324859692011d6fa24373425bb789f6663562c84b0dd68aaf\" (aliases: [web03 2602e369463b14c324859692011d6fa24373425bb789f6663562c84b0dd68aaf], namespace: \"docker\")\nI0130 11:39:39.870528 00001 container.go:163] Start housekeeping for container \"/docker/2602e369463b14c324859692011d6fa24373425bb789f6663562c84b0dd68aaf\"\nI0130 11:39:39.871976 00001 container.go:214] Failed to update stats for container \"/docker/2602e369463b14c324859692011d6fa24373425bb789f6663562c84b0dd68aaf\": could not find device with major: 252, minor: 1 in cached partitions map\nI0130 11:39:40.150855 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:39:40.161535 00001 manager.go:414] Added container: \"/docker/02542e45b912653a587540d47b955947c4fc82e14e33ee9a0227e8bcdfc44315\" (aliases: [web04 02542e45b912653a587540d47b955947c4fc82e14e33ee9a0227e8bcdfc44315], namespace: \"docker\")\nI0130 11:39:40.161589 00001 container.go:163] Start housekeeping for container \"/docker/02542e45b912653a587540d47b955947c4fc82e14e33ee9a0227e8bcdfc44315\"\nI0130 11:39:40.162613 00001 container.go:214] Failed to update stats for container \"/docker/02542e45b912653a587540d47b955947c4fc82e14e33ee9a0227e8bcdfc44315\": could not find device with major: 252, minor: 1 in cached partitions map\nI0130 11:39:40.549854 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:39:40.561212 00001 manager.go:414] Added container: \"/docker/4d0445f9afa5d33d9f81d83748f898797b480132e1fde5462f209e4d666dd783\" (aliases: [cache01 4d0445f9afa5d33d9f81d83748f898797b480132e1fde5462f209e4d666dd783], namespace: \"docker\")\nI0130 11:39:40.561282 00001 container.go:163] Start housekeeping for container \"/docker/4d0445f9afa5d33d9f81d83748f898797b480132e1fde5462f209e4d666dd783\"\nI0130 11:39:40.562883 00001 container.go:214] Failed to update stats for container \"/docker/4d0445f9afa5d33d9f81d83748f898797b480132e1fde5462f209e4d666dd783\": could not find device with major: 252, minor: 1 in cached partitions map\nI0130 11:39:40.974623 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:39:40.986278 00001 manager.go:414] Added container: \"/docker/74f10ac1648032a449b5d5a933ccde64eb514eaa3216d5fbc3012d62dd097ca8\" (aliases: [cache02 74f10ac1648032a449b5d5a933ccde64eb514eaa3216d5fbc3012d62dd097ca8], namespace: \"docker\")\nI0130 11:39:40.986398 00001 container.go:163] Start housekeeping for container \"/docker/74f10ac1648032a449b5d5a933ccde64eb514eaa3216d5fbc3012d62dd097ca8\"\nI0130 11:39:40.989223 00001 container.go:214] Failed to update stats for container \"/docker/74f10ac1648032a449b5d5a933ccde64eb514eaa3216d5fbc3012d62dd097ca8\": could not find device with major: 252, minor: 1 in cached partitions map\nI0130 11:39:41.433928 00001 fs.go:70] Filesystem partitions: map[]\nI0130 11:39:41.436430 00001 manager.go:414] Added container: \"/docker/8d4d69029412ea0e9c77d9501519c1bed524e971eb46c2b91d71b5bec0e280b2\" (aliases: [lb01 8d4d69029412ea0e9c77d9501519c1bed524e971eb46c2b91d71b5bec0e280b2], namespace: \"docker\")\nI0130 11:39:41.436707 00001 container.go:163] Start housekeeping for container \"/docker/8d4d69029412ea0e9c77d9501519c1bed524e971eb46c2b91d71b5bec0e280b2\"\nI0130 11:39:41.437462 00001 container.go:214] Failed to update stats for container \"/docker/8d4d69029412ea0e9c77d9501519c1bed524e971eb46c2b91d71b5bec0e280b2\": could not find device with major: 252, minor: 1 in cached partitions map\nI0130 11:39:55.088148 00001 container.go:214] Failed to update stats for container \"/docker/779c561e66c49cac55ab7040a18e53764f7d75e6ebc08812f9285833f0682d06\": could not find device with major: 252, minor: 1 in cached partitions map\nI0130 11:40:26.964697 00001 manager.go:448] Destroyed container: \"/docker/d0e7cc44b729fe242492af5257362e7d547725b41c3fff879f822301df1ea8d6\" (aliases: [bax-memcached d0e7cc44b729fe242492af5257362e7d547725b41c3fff879f822301df1ea8d6], namespace: \"docker\")\nI0130 11:40:37.113196 00001 manager.go:448] Destroyed container: \"/docker/611d1adb0651f80a3338761ae17f0f6d7af98a3c05895cc21fdf558d679cd10c\" (aliases: [], namespace: \"\")\nI0130 11:40:37.405290 00001 manager.go:448] Destroyed container: \"/docker/65111b78f9c0c9c957d82e3219153df4e726b3f6a709404e494038358c44e090\" (aliases: [search01 65111b78f9c0c9c957d82e3219153df4e726b3f6a709404e494038358c44e090], namespace: \"docker\")\nI0130 11:40:37.691676 00001 manager.go:448] Destroyed container: \"/docker/d7fdc950659c882f7218401a820d9884b66205d44465fd00a96dfeb2675fe642\" (aliases: [search02 d7fdc950659c882f7218401a820d9884b66205d44465fd00a96dfeb2675fe642], namespace: \"docker\")\nI0130 11:40:37.884310 00001 manager.go:448] Destroyed container: \"/docker/cd8e9930f1638b7bf85ddaa1c343b6f4ff1e662a4030edf5ef9476c9cba93ea1\" (aliases: [api01 cd8e9930f1638b7bf85ddaa1c343b6f4ff1e662a4030edf5ef9476c9cba93ea1], namespace: \"docker\")\nI0130 11:40:38.169465 00001 manager.go:448] Destroyed container: \"/docker/2dc6b0db171d1902e9e302318e6711649f41130d2bea4e3217bff32ad21c2e66\" (aliases: [api02 2dc6b0db171d1902e9e302318e6711649f41130d2bea4e3217bff32ad21c2e66], namespace: \"docker\")\nI0130 11:40:38.474403 00001 manager.go:448] Destroyed container: \"/docker/dbcac66ac6bb64cd60d9e048abd70ccee9e20af4f414303059d5c458a9c5131b\" (aliases: [web01 dbcac66ac6bb64cd60d9e048abd70ccee9e20af4f414303059d5c458a9c5131b], namespace: \"docker\")\nI0130 11:40:38.712949 00001 manager.go:448] Destroyed container: \"/docker/f98637ff412b6317b699c85574e33e808a8541ab6ac086802a3d9e131542a54e\" (aliases: [web02 f98637ff412b6317b699c85574e33e808a8541ab6ac086802a3d9e131542a54e], namespace: \"docker\")\nI0130 11:40:38.955873 00001 manager.go:448] Destroyed container: \"/docker/2602e369463b14c324859692011d6fa24373425bb789f6663562c84b0dd68aaf\" (aliases: [web03 2602e369463b14c324859692011d6fa24373425bb789f6663562c84b0dd68aaf], namespace: \"docker\")\nI0130 11:40:39.187876 00001 manager.go:448] Destroyed container: \"/docker/02542e45b912653a587540d47b955947c4fc82e14e33ee9a0227e8bcdfc44315\" (aliases: [web04 02542e45b912653a587540d47b955947c4fc82e14e33ee9a0227e8bcdfc44315], namespace: \"docker\")\nI0130 11:40:40.563126 00001 container.go:214] Failed to update stats for container \"/docker/4d0445f9afa5d33d9f81d83748f898797b480132e1fde5462f209e4d666dd783\": could not find device with major: 252, minor: 1 in cached partitions map\nI0130 11:40:41.438384 00001 container.go:214] Failed to update stats for container \"/docker/8d4d69029412ea0e9c77d9501519c1bed524e971eb46c2b91d71b5bec0e280b2\": could not find device with major: 252, minor: 1 in cached partitions map\nI0130 11:40:41.988234 00001 container.go:214] Failed to update stats for container \"/docker/74f10ac1648032a449b5d5a933ccde64eb514eaa3216d5fbc3012d62dd097ca8\": could not find device with major: 252, minor: 1 in cached partitions map\nI0130 11:40:42.696818 00001 manager.go:448] Destroyed container: \"/docker/4d0445f9afa5d33d9f81d83748f898797b480132e1fde5462f209e4d666dd783\" (aliases: [cache01 4d0445f9afa5d33d9f81d83748f898797b480132e1fde5462f209e4d666dd783], namespace: \"docker\")\nI0130 11:40:44.861544 00001 manager.go:448] Destroyed container: \"/docker/74f10ac1648032a449b5d5a933ccde64eb514eaa3216d5fbc3012d62dd097ca8\" (aliases: [cache02 74f10ac1648032a449b5d5a933ccde64eb514eaa3216d5fbc3012d62dd097ca8], namespace: \"docker\")\nI0130 11:40:45.088898 00001 manager.go:448] Destroyed container: \"/docker/8d4d69029412ea0e9c77d9501519c1bed524e971eb46c2b91d71b5bec0e280b2\" (aliases: [lb01 8d4d69029412ea0e9c77d9501519c1bed524e971eb46c2b91d71b5bec0e280b2], namespace: \"docker\")\nI0130 11:40:56.094312 00001 container.go:214] Failed to update stats for container \"/docker/779c561e66c49cac55ab7040a18e53764f7d75e6ebc08812f9285833f0682d06\": could not find device with major: 252, minor: 1 in cached partitions map\n```\n. I am terrible sorry for not getting back to you earlier with this. I just ran some tests with the latest version and all seems to be working just fine now. So somehow the problem got fixed along the way :) \nHere is the output of my /validate\n```\ncAdvisor version: 0.9.0\nOS version: Buildroot 2014.02\nKernel version: [Supported and recommended]\n    Kernel version is 3.13.0-45-generic. Versions >= 2.6 are supported. 3.0+ are recommended.\nCgroup setup: [Supported and recommended]\n    Available cgroups: map[cpu:1 perf_event:1 cpuset:1 memory:1 devices:1 freezer:1 blkio:1 hugetlb:1 cpuacct:1]\n    Following cgroups are required: [cpu cpuacct]\n    Following other cgroups are recommended: [memory blkio cpuset devices freezer]\nCgroup mount setup: [Supported, but not recommended]\n    Cgroups are mounted at /rootfs/sys/fs/cgroup.\n    Cgroup mount directories: blkio cpu cpuacct cpuset devices freezer hugetlb memory perf_event systemd \n    Any cgroup mount point that is detectible and accessible is supported. /sys/fs/cgroup is recommended as a standard location.\n    Cgroup mounts:\n    cgroup /rootfs/sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /rootfs/sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /rootfs/sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /rootfs/sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /rootfs/sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /rootfs/sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /rootfs/sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/c127eb4b247faf5f7c9b806b1fd4d036bc328ae9743b70bb0c0a59296afa69f6/sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/c127eb4b247faf5f7c9b806b1fd4d036bc328ae9743b70bb0c0a59296afa69f6/sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /var/lib/docker/aufs/mnt/c127eb4b247faf5f7c9b806b1fd4d036bc328ae9743b70bb0c0a59296afa69f6/sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/c127eb4b247faf5f7c9b806b1fd4d036bc328ae9743b70bb0c0a59296afa69f6/sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /var/lib/docker/aufs/mnt/c127eb4b247faf5f7c9b806b1fd4d036bc328ae9743b70bb0c0a59296afa69f6/sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/c127eb4b247faf5f7c9b806b1fd4d036bc328ae9743b70bb0c0a59296afa69f6/sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/c127eb4b247faf5f7c9b806b1fd4d036bc328ae9743b70bb0c0a59296afa69f6/sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/c127eb4b247faf5f7c9b806b1fd4d036bc328ae9743b70bb0c0a59296afa69f6/sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/c127eb4b247faf5f7c9b806b1fd4d036bc328ae9743b70bb0c0a59296afa69f6/sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /var/lib/docker/aufs/mnt/c127eb4b247faf5f7c9b806b1fd4d036bc328ae9743b70bb0c0a59296afa69f6/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/c127eb4b247faf5f7c9b806b1fd4d036bc328ae9743b70bb0c0a59296afa69f6/rootfs/sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/c127eb4b247faf5f7c9b806b1fd4d036bc328ae9743b70bb0c0a59296afa69f6/rootfs/sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /var/lib/docker/aufs/mnt/c127eb4b247faf5f7c9b806b1fd4d036bc328ae9743b70bb0c0a59296afa69f6/rootfs/sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/c127eb4b247faf5f7c9b806b1fd4d036bc328ae9743b70bb0c0a59296afa69f6/rootfs/sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /var/lib/docker/aufs/mnt/c127eb4b247faf5f7c9b806b1fd4d036bc328ae9743b70bb0c0a59296afa69f6/rootfs/sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/c127eb4b247faf5f7c9b806b1fd4d036bc328ae9743b70bb0c0a59296afa69f6/rootfs/sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/c127eb4b247faf5f7c9b806b1fd4d036bc328ae9743b70bb0c0a59296afa69f6/rootfs/sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/c127eb4b247faf5f7c9b806b1fd4d036bc328ae9743b70bb0c0a59296afa69f6/rootfs/sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/c127eb4b247faf5f7c9b806b1fd4d036bc328ae9743b70bb0c0a59296afa69f6/rootfs/sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /var/lib/docker/aufs/mnt/c127eb4b247faf5f7c9b806b1fd4d036bc328ae9743b70bb0c0a59296afa69f6/rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\nDocker version: [Supported and recommended]\n    Docker version is 1.5.0. Versions >= 1.0 are supported. 1.2+ are recommended.\nDocker driver setup: [Supported and recommended]\n    Docker exec driver is native-0.2. Storage driver is aufs.\n    Cgroups are being created through cgroup filesystem.\n    Docker container state directory is at \"/var/lib/docker/execdriver/native\" and is accessible.\nBlock device setup: [Supported, but not recommended]\n    None of the devices support 'cfq' I/O scheduler. No disk stats can be reported.\n     Disk \"dm-0\" Scheduler type \"none\".\n     Disk \"dm-1\" Scheduler type \"none\".\n     Disk \"vda\" Scheduler type \"none\".\n```\n. ",
    "jbdalido": "Hello, \nI've got the very same problem here, on deb 8, \n```\ncAdvisor version: 0.7.1\nOS version: Buildroot 2014.02\nKernel version: [Supported and recommended]\n  Kernel version is 3.14-2-amd64. Versions >= 2.6 are supported. 3.0+ are recommended.\nCgroup setup: [Supported, but not recommended]\n  Cgroup memory not enabled. Available cgroups: map[cpuset:1 cpu:1 memory:0 freezer:1 net_cls:1 blkio:1 cpuacct:1 devices:1 perf_event:1]\n  Following cgroups are required: [cpu cpuacct]\n  Following other cgroups are recommended: [memory blkio cpuset devices freezer]\nCgroup mount setup: [Supported and recommended]\n  Cgroups are mounted at /sys/fs/cgroup.\n  Cgroup mount directories: blkio cpu cpu,cpuacct cpuacct cpuset devices freezer net_cls perf_event systemd \n  Any cgroup mount point that is detectible and accessible is supported. /sys/fs/cgroup is recommended as a standard location.\n  Cgroup mounts:\n  cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n  cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n  cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n  cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n  cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n  cgroup /sys/fs/cgroup/net_cls cgroup rw,nosuid,nodev,noexec,relatime,net_cls 0 0\n  cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n  cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n  cgroup /var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n  cgroup /var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n  cgroup /var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n  cgroup /var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n  cgroup /var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n  cgroup /var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/net_cls cgroup rw,nosuid,nodev,noexec,relatime,net_cls 0 0\n  cgroup /var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n  cgroup /var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n  cgroup /rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n  cgroup /rootfs/sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n  cgroup /rootfs/sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n  cgroup /rootfs/sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n  cgroup /rootfs/sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n  cgroup /rootfs/sys/fs/cgroup/net_cls cgroup rw,nosuid,nodev,noexec,relatime,net_cls 0 0\n  cgroup /rootfs/sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n  cgroup /rootfs/sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n  cgroup /rootfs/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n  cgroup /rootfs/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n  cgroup /rootfs/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n  cgroup /rootfs/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n  cgroup /rootfs/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n  cgroup /rootfs/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/net_cls cgroup rw,nosuid,nodev,noexec,relatime,net_cls 0 0\n  cgroup /rootfs/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n  cgroup /rootfs/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n  cgroup /rootfs/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n  cgroup /rootfs/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n  cgroup /rootfs/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n  cgroup /rootfs/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n  cgroup /rootfs/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n  cgroup /rootfs/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/net_cls cgroup rw,nosuid,nodev,noexec,relatime,net_cls 0 0\n  cgroup /rootfs/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n  cgroup /rootfs/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/var/lib/docker/aufs/mnt/6591e8e3220045c143034882bf6f59cc171ed9ef7390797673a6708ff64a543d/sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\nDocker version: [Supported and recommended]\n  Docker version is 1.2.0. Versions >= 1.0 are supported. 1.2+ are recommended.\nDocker driver setup: [Supported and recommended]\n  Docker exec driver is native-0.2. Storage driver is aufs.\n  systemd is being used to create cgroups.\n```\n. [edit]\nSeems to be no error. I'll give it a closer look over the week end to provide you with more informations.\nDocker driver setup: [Supported and recommended]\n    Docker exec driver is native-0.2. Storage driver is aufs.\n    systemd is being used to create cgroups.\n    Docker container state directory is at \"/var/lib/docker/execdriver/native\" and is accessible.\n. ",
    "yesnault": "I have many container with lxc-driver on this machine. But after test and without lxc-driver, it works. So, the problem is the LXC Driver.\n. Other containers world well with volume on this machine (centos). So, I can close this issue and open a new one on docker repo for Lxc Driver with mounting /cgroup. I can try to run cadvisor on Ubuntu with lxc driver too. \n. Thank you ! I'll try to integrate that in https://github.com/docktor/docktor/ (Docktor, having all my cAdvisorURL, can exec this script to send data to Riemann)\n. A docker image is available : \ndocker run \\\n    -e RIEMANN_ADDRESS=192.168.59.104:5555 \\\n    -e CADVISOR_ADDRESS=192.168.59.104:8080 \\\n    -e INTERVAL=5s \\\n    docktor/gorycadvisor:latest\nI close this issue, @dipankar  does the job :+1: \n. With 0.7.0, I have this error on going on url http://:8080/docker/\n\"Failed to get container \"\" with error: unable to find data for container /docker/af6b7c91c3226cd09839f981e3ddab7d8a5502b5b71364fe881416773b2b583f\"\nWith 0.6.2, no error (so, there a bug in this diff  https://github.com/google/cadvisor/compare/0.6.2...0.7.0 :-) )\nDockerVersion (it's a CentOS) : {\"ApiVersion\":\"1.15\",\"Arch\":\"amd64\",\"GitCommit\":\"4e9bbfa\",\"GoVersion\":\"go1.3.3\",\"KernelVersion\":\"3.14.1-1.el6.elrepo.x86_64\",\"Os\":\"linux\",\"Version\":\"1.3.1\"}\nIs it the same issue as yours ? If not, I can open a new one.\n. @vmarmol no, no ligne with that. But it's same bug as #385. I completed this issue.\n. /validate : \n```\ncAdvisor version: 0.7.0\nOS version: Buildroot 2014.02\nKernel version: [Supported and recommended]\n    Kernel version is 3.14.1-1.el6.elrepo.x86_64. Versions >= 2.6 are supported. 3.0+ are recommended.\nCgroup setup: [Supported and recommended]\n    Available cgroups: map[cpu:1 net_cls:1 cpuset:1 memory:1 devices:1 freezer:1 blkio:1 perf_event:1 cpuacct:1]\n    Following cgroups are required: [cpu cpuacct]\n    Following other cgroups are recommended: [memory blkio cpuset devices freezer]\nCgroup mount setup: [Supported, but not recommended]\n    Cgroups are mounted at /rootfs/cgroup.\n    Any cgroup mount point that is detectible and accessible is supported. /sys/fs/cgroup is recommended as a standard location.\nDocker version: [Supported and recommended]\n    Docker version is 1.3.1. Versions >= 1.0 are supported. 1.2+ are recommended.\nDocker driver setup: [Supported and recommended]\n    Docker exec driver is native-0.2. Storage driver is devicemapper.\n    Cgroups are being created through cgroup filesystem.\n```\nls /sys/fs/cgroup/*\nls: cannot access /sys/fs/cgroup/*: No such file or directory\n/tmp # head cadvisor.XXXX-cAdvisor-xxxx-Test.root.log.WARNING.20141223-112917.1\nLog file created at: 2014/12/23 11:29:17\nRunning on machine: XXXX-cAdvisor-xxxx-Test\nBinary: Built with gc go1.3 for linux/amd64\nLog line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\nW1223 11:29:17.020011 00001 container.go:115] Failed to get RecentStats(\"/docker/29343889165fa204537de499dbe53cbc8dc3f79bfc5ddd4c829da958307addec\") while determining the next housekeeping: unable to find data for container /docker/29343889165fa204537de499dbe53cbc8dc3f79bfc5ddd4c829da958307addec\nW1223 11:29:17.043394 00001 container.go:115] Failed to get RecentStats(\"/docker/bd57cf63b927df638433ac2883a5c9d0436c2df43b4f478d47385dab2656562a\") while determining the next housekeeping: unable to find data for container /docker/bd57cf63b927df638433ac2883a5c9d0436c2df43b4f478d47385dab2656562a\nW1223 11:29:17.064509 00001 container.go:115] Failed to get RecentStats(\"/docker/27511201eb539dc2ccdcdce653df7bbf24c2f83667e37e409e0b0928a8738513\") while determining the next housekeeping: unable to find data for container /docker/27511201eb539dc2ccdcdce653df7bbf24c2f83667e37e409e0b0928a8738513\nW1223 11:29:17.082080 00001 container.go:115] Failed to get RecentStats(\"/docker/0fff7313a462f269b144cdeb1b4e321ad536816b8f44acfc441ac4833f71f36a\") while determining the next housekeeping: unable to find data for container /docker/0fff7313a462f269b144cdeb1b4e321ad536816b8f44acfc441ac4833f71f36a\nW1223 11:29:17.105278 00001 container.go:115] Failed to get RecentStats(\"/docker/8a951f64d7ae866593fb308566ef04382b60a749095d18abe73e7b2f9f522263\") while determining the next housekeeping: unable to find data for container /docker/8a951f64d7ae866593fb308566ef04382b60a749095d18abe73e7b2f9f522263\nW1223 11:29:17.125529 00001 container.go:115] Failed to get RecentStats(\"/docker/0f35771d19b45eeca0116c23f800292991dc4ac6ccbbf15ad902bfa9c38ba6f3\") while determining the next housekeeping: unable to find data for container /docker/0f35771d19b45eeca0116c23f800292991dc4ac6ccbbf15ad902bfa9c38ba6f3\n/tmp # head cadvisor.XXXX-cAdvisor-xxxx-Test.root.log.INFO.20141223-112916.1\nLog file created at: 2014/12/23 11:29:16\nRunning on machine: XXXX-cAdvisor-xxxx-Test\nBinary: Built with gc go1.3 for linux/amd64\nLog line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\nI1223 11:29:16.393461 00001 storagedriver.go:89] Caching 60 recent stats in memory; using \"\" storage driver\nI1223 11:29:16.395945 00001 manager.go:78] cAdvisor running in container: \"/docker/bd57cf63b927df638433ac2883a5c9d0436c2df43b4f478d47385dab2656562a\"\n(cadvisor works well on same machine with version 0.6.2)\n. cgroups are mounted on /rootfs and not under /sys.\n/tmp # ls /rootfs/cgroup/cpu\ncgroup.clone_children  cpu.cfs_period_us      cpu.rt_runtime_us      docker                 tasks\ncgroup.procs           cpu.cfs_quota_us       cpu.shares             notify_on_release\ncgroup.sane_behavior   cpu.rt_period_us       cpu.stat               release_agent\n. Thanks ! \nI will try this on Saturday or Sunday, I do not have access to my server at this time.\n. Ok, it's fixed ! Thanks a lot.\n. /api/v1.2/machine :\n{\"num_cores\":2,\"cpu_frequency_khz\":2500000,\"memory_capacity\":16862670848,\"filesystems\":[{\"device\":\"/dev/mapper/docker-253:6-311299-5fd53625310fb022678266f92b2fb1a7cd7ff7eed8bf978157e424725cc4602e\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/docker-253:6-311299-ff58e24fd4408a2b7958b6ae69de6223d556445344ba25c8cb2b278005b62168\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/lvm-data\",\"capacity\":31572619264},{\"device\":\"/dev/mapper/docker-253:6-311299-755ab8b1878e9cda93dd35dc90441b396239ab7dc53b8ef6cea80340c5891d37\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/lvm-var\",\"capacity\":12615598080},{\"device\":\"/dev/mapper/docker-253:6-311299-27511201eb539dc2ccdcdce653df7bbf24c2f83667e37e409e0b0928a8738513\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/docker-253:6-311299-e5c44128aed3e9f7b315d1398d9b1e265bc69f4cdcd47c6414128bb5b49a0d57\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/docker-253:6-311299-29343889165fa204537de499dbe53cbc8dc3f79bfc5ddd4c829da958307addec\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/lvm-registry\",\"capacity\":21003628544},{\"device\":\"/dev/mapper/lvm-tmp\",\"capacity\":2046640128},{\"device\":\"/dev/mapper/docker-253:6-311299-508e1284e5fe9152d6bb4e30a56b1f528f15ea480f192f9ede1f4b370a8aab0f\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/docker-253:6-311299-dec74f3539147d34df44c1f01fdc924fcda9a9142da0c47d0aac446e9e5aa99d\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/docker-253:6-311299-a03d6b97aea7e555ec06f48cc77278c4c4cc144a8b164bd2b0af4b6fbae5bd3f\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/docker-253:6-311299-90976f27e4dc484c33060c6b3bfc13ecb68247992c4217bb58680696b5897d2c\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/docker-253:6-311299-0fff7313a462f269b144cdeb1b4e321ad536816b8f44acfc441ac4833f71f36a\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/docker-253:6-311299-c8f7e9ef3080a31714cf187800b812ff0954433b4071823d861460c7ac5a7d4d\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/docker-253:6-311299-0fe152403f153172e01d8a8ca23994bbb438a278e321014090692d3173ceb91b\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/docker-253:6-311299-384a4a427c56b9f207833d629f1cf0645ebbd5af2d8a93ae5fbdada4e5765df6\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/docker-253:6-311299-b68414996bb7fc0465582eec5853741ccdf4fc0f88254fa63395dbd4b5cdeb1d\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/docker-253:6-311299-af6b7c91c3226cd09839f981e3ddab7d8a5502b5b71364fe881416773b2b583f\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/docker-253:6-311299-4346444fb0f87fa9559340627141e894a8350e87df694bd64c0b4078cb2ff8fd\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/docker-253:6-311299-8a951f64d7ae866593fb308566ef04382b60a749095d18abe73e7b2f9f522263\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/docker-253:6-311299-c4d0ae06ac9b0339d111ba64ba9fede32557ee9abad7b32cc8072764f6c32f5f\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/docker-253:6-311299-0731f22b32dd378a22ea330f4b288ad76ecf954e1a7a49e11b63c9d8106bda0d\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/docker-253:6-311299-9ef28bcbc6df04038e657fb222a38b9b6a45f1fc1ac80f026bc65cd79d8b8179\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/lvm-root\",\"capacity\":1040080896},{\"device\":\"/dev/mapper/lvm-usr\",\"capacity\":4093313024},{\"device\":\"/dev/mapper/docker-253:6-311299-62800f9828076630033dda97c2c1994d16ac07f79aa1cd73f44cdf9657a37c5f\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/docker-253:6-311299-4bb6b9be1cf2ad765d0f185bfc1e7d2160acaaf205200c11c8d781056643c08a\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/docker-253:6-311299-ce512c2ee1a0ab4263daa8c91b12a7b4def6db341ea35e5d81eab98f49b3ee28\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/lvm-opt\",\"capacity\":251575296},{\"device\":\"/dev/mapper/docker-253:6-311299-691dbfa79f806ea8dfa42622fde1e55de197e4a14776da6c191e9547525296b8\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/docker-253:6-311299-f4b7d5631ca1523293fdb2711049d3cbe90ec526565a7bf8824be54e0236c213\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/docker-253:6-311299-89749bdb937c2e0669fabc9d7d21523750cd3c9e17193fdfb267e87190b8ebc2\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/lvm-varlibdocker\",\"capacity\":52710469632},{\"device\":\"/dev/sda1\",\"capacity\":251575296},{\"device\":\"/dev/mapper/lvm-home\",\"capacity\":2046640128},{\"device\":\"/dev/mapper/docker-253:6-311299-27bbdeacb9fc0e7a192f1cb024924c933bc4e30a76e75e4924e21b4e3d16c8ff\",\"capacity\":10434699264},{\"device\":\"/dev/mapper/docker-253:6-311299-0f35771d19b45eeca0116c23f800292991dc4ac6ccbbf15ad902bfa9c38ba6f3\",\"capacity\":10434699264}],\"disk_map\":{\"11:0\":{\"name\":\"sr0\",\"major\":11,\"minor\":0,\"size\":1073741312},\"253:0\":{\"name\":\"dm-0\",\"major\":253,\"minor\":0,\"size\":1073741824},\"253:1\":{\"name\":\"dm-1\",\"major\":253,\"minor\":1,\"size\":12884901888},\"253:10\":{\"name\":\"dm-10\",\"major\":253,\"minor\":10,\"size\":10737418240},\"253:11\":{\"name\":\"dm-11\",\"major\":253,\"minor\":11,\"size\":10737418240},\"253:12\":{\"name\":\"dm-12\",\"major\":253,\"minor\":12,\"size\":10737418240},\"253:13\":{\"name\":\"dm-13\",\"major\":253,\"minor\":13,\"size\":10737418240},\"253:14\":{\"name\":\"dm-14\",\"major\":253,\"minor\":14,\"size\":10737418240},\"253:15\":{\"name\":\"dm-15\",\"major\":253,\"minor\":15,\"size\":10737418240},\"253:16\":{\"name\":\"dm-16\",\"major\":253,\"minor\":16,\"size\":10737418240},\"253:17\":{\"name\":\"dm-17\",\"major\":253,\"minor\":17,\"size\":10737418240},\"253:18\":{\"name\":\"dm-18\",\"major\":253,\"minor\":18,\"size\":10737418240},\"253:19\":{\"name\":\"dm-19\",\"major\":253,\"minor\":19,\"size\":10737418240},\"253:2\":{\"name\":\"dm-2\",\"major\":253,\"minor\":2,\"size\":4294967296},\"253:20\":{\"name\":\"dm-20\",\"major\":253,\"minor\":20,\"size\":10737418240},\"253:21\":{\"name\":\"dm-21\",\"major\":253,\"minor\":21,\"size\":10737418240},\"253:22\":{\"name\":\"dm-22\",\"major\":253,\"minor\":22,\"size\":10737418240},\"253:23\":{\"name\":\"dm-23\",\"major\":253,\"minor\":23,\"size\":10737418240},\"253:24\":{\"name\":\"dm-24\",\"major\":253,\"minor\":24,\"size\":10737418240},\"253:25\":{\"name\":\"dm-25\",\"major\":253,\"minor\":25,\"size\":10737418240},\"253:26\":{\"name\":\"dm-26\",\"major\":253,\"minor\":26,\"size\":10737418240},\"253:27\":{\"name\":\"dm-27\",\"major\":253,\"minor\":27,\"size\":10737418240},\"253:28\":{\"name\":\"dm-28\",\"major\":253,\"minor\":28,\"size\":10737418240},\"253:29\":{\"name\":\"dm-29\",\"major\":253,\"minor\":29,\"size\":10737418240},\"253:3\":{\"name\":\"dm-3\",\"major\":253,\"minor\":3,\"size\":268435456},\"253:30\":{\"name\":\"dm-30\",\"major\":253,\"minor\":30,\"size\":10737418240},\"253:31\":{\"name\":\"dm-31\",\"major\":253,\"minor\":31,\"size\":10737418240},\"253:32\":{\"name\":\"dm-32\",\"major\":253,\"minor\":32,\"size\":10737418240},\"253:33\":{\"name\":\"dm-33\",\"major\":253,\"minor\":33,\"size\":10737418240},\"253:34\":{\"name\":\"dm-34\",\"major\":253,\"minor\":34,\"size\":10737418240},\"253:35\":{\"name\":\"dm-35\",\"major\":253,\"minor\":35,\"size\":10737418240},\"253:36\":{\"name\":\"dm-36\",\"major\":253,\"minor\":36,\"size\":10737418240},\"253:37\":{\"name\":\"dm-37\",\"major\":253,\"minor\":37,\"size\":10737418240},\"253:38\":{\"name\":\"dm-38\",\"major\":253,\"minor\":38,\"size\":10737418240},\"253:39\":{\"name\":\"dm-39\",\"major\":253,\"minor\":39,\"size\":10737418240},\"253:4\":{\"name\":\"dm-4\",\"major\":253,\"minor\":4,\"size\":2147483648},\"253:40\":{\"name\":\"dm-40\",\"major\":253,\"minor\":40,\"size\":10737418240},\"253:41\":{\"name\":\"dm-41\",\"major\":253,\"minor\":41,\"size\":10737418240},\"253:42\":{\"name\":\"dm-42\",\"major\":253,\"minor\":42,\"size\":10737418240},\"253:43\":{\"name\":\"dm-43\",\"major\":253,\"minor\":43,\"size\":10737418240},\"253:44\":{\"name\":\"dm-44\",\"major\":253,\"minor\":44,\"size\":10737418240},\"253:5\":{\"name\":\"dm-5\",\"major\":253,\"minor\":5,\"size\":2147483648},\"253:6\":{\"name\":\"dm-6\",\"major\":253,\"minor\":6,\"size\":53687091200},\"253:7\":{\"name\":\"dm-7\",\"major\":253,\"minor\":7,\"size\":32212254720},\"253:8\":{\"name\":\"dm-8\",\"major\":253,\"minor\":8,\"size\":21474836480},\"253:9\":{\"name\":\"dm-9\",\"major\":253,\"minor\":9,\"size\":107374182400},\"2:0\":{\"name\":\"fd0\",\"major\":2,\"minor\":0,\"size\":4096},\"8:0\":{\"name\":\"sda\",\"major\":8,\"minor\":0,\"size\":21474836480},\"8:16\":{\"name\":\"sdb\",\"major\":8,\"minor\":16,\"size\":21474836480},\"8:32\":{\"name\":\"sdc\",\"major\":8,\"minor\":32,\"size\":21474836480},\"8:48\":{\"name\":\"sdd\",\"major\":8,\"minor\":48,\"size\":107374182400}},\"topology\":[{\"node_id\":0,\"memory\":16862670848,\"cores\":[{\"core_id\":0,\"thread_ids\":[0],\"caches\":[{\"size\":32768,\"type\":\"Data\",\"level\":1},{\"size\":32768,\"type\":\"Instruction\",\"level\":1},{\"size\":262144,\"type\":\"Unified\",\"level\":2}]}],\"caches\":[{\"size\":15728640,\"type\":\"Unified\",\"level\":3}]},{\"node_id\":2,\"memory\":0,\"cores\":[{\"core_id\":0,\"thread_ids\":[1],\"caches\":[{\"size\":32768,\"type\":\"Data\",\"level\":1},{\"size\":32768,\"type\":\"Instruction\",\"level\":1},{\"size\":262144,\"type\":\"Unified\",\"level\":2}]}],\"caches\":[{\"size\":15728640,\"type\":\"Unified\",\"level\":3}]}]}\n. ",
    "jeinwag": "Just a quick addition: Viewing containers via the cadvisor web ui works without any problems.\n. Thanks a lot for the quick reply!\n. v1.2 API is now working as expected.\n. Looking good now, thanks a lot!\n. We're using docker 1.10.2\nsudo docker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=9090:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  google/cadvisor:latest\n. @yujuhong The container did restart because the application running inside the container did crash (the container was created with restart-policy 'on failure'). Unfortunately, the issue causing the application crash has been fixed since, so I can't reproduce the issue any longer.\n. I'm currently experiencing the same problem with docker containers, the cadvisor UI says RSS of one container is 11.68 GB while the prometheus endpoint returns 1.305858048e+09 of container_memory_rss for the same container.\nWe're running cadvisor version 0.24.1.. ",
    "pdwinkel": "Nothing:\nroot@peter: ~  > l -a /var/lib/docker/execdriver/native\ntotal 8\ndrwx------ 2 root root 4096 Mar 21  2014 .\ndrwx------ 3 root root 4096 Mar 21  2014 ..\nI've got the following in the /etc/default/docker file:\nDOCKER_OPTS=\"-H tcp://127.0.0.1:2375 -H unix:///var/run/docker.sock -g /data/docker -api-enable-cors --insecure-registry 192.168.1.0/24\"\nexport TMPDIR=\"/data/docker-tmp\"\n. Yep:\nroot@peter: ~  > l -a /data/docker/execdriver/native\ntotal 112\ndrwx------ 28 root root 4096 Nov 30 11:02 .\ndrwx------  3 root root 4096 Sep 28 19:04 ..\ndrw-r-xr-x  2 root root 4096 Nov 30 10:57 1a7771412789506050421166d9ff4b5989544e34093812b69b10cc9fac42ff41\ndrw-r-xr-x  2 root root 4096 Nov 27 23:36 1ceb78cc72e93cc49166842b26aafcee90c2b8ba3babcc83d391d500305a16fb\ndrw-r-xr-x  2 root root 4096 Nov 27 23:37 21c8e4359f1d7d18f0a6561aec63f5ad9014d6077f6fcd3166d5c4ca1906ed47\ndrw-r-xr-x  2 root root 4096 Nov 30 10:57 22286b631d471847e9443df64965ce740179f8d7d6b19f7d5369b321acb32b18\ndrw-r-xr-x  2 root root 4096 Nov 28 21:26 329b4ab9408a7a94e28b1c48eaed8422a603e0e5919f0a60dd04e01b94658bfc\ndrw-r-xr-x  2 root root 4096 Nov 26 10:45 376fa967d9918e12aaa91045bf6dacf8e8b6288da0acbe386886e5209c3fad92\ndrw-r-xr-x  2 root root 4096 Sep 29 19:04 3b9446b8e3e03d104bebce4dbe6b9cc0e8652a75f48a7d0bbca58e998ddc333f\ndrw-r-xr-x  2 root root 4096 Nov 28 22:19 448f962a62f5b271a66a63a61ba0584f27f073b10e8106710496570339e44827\ndrw-r-xr-x  2 root root 4096 Nov 28 21:13 4bc6fce2c88eb3eb4f5cb49511ce521eb47314ff599ce5832d13420abfb35262\ndrw-r-xr-x  2 root root 4096 Nov 29 19:16 5267e4d7c7992c3d0c8b8b6a9c9cb0215a18b9ec5ad675204e395a93e766ce8b\ndrw-r-xr-x  2 root root 4096 Nov 25 15:03 52f862f3430907c6b796f03f9145ffe44b9c73198dd31f658e2fa4d00d0ea81f\ndrw-r-xr-x  2 root root 4096 Nov 29 10:12 5982d1c5b65be438099c86463dbe75945665cd9c7014187e0d9df53e893ff93b\ndrw-r-xr-x  2 root root 4096 Nov 27 23:37 5d9d3b8b51f12ea05e2c0c18ac31d7a5a5f98489d4384b8f604700d5031fd19b\ndrw-r-xr-x  2 root root 4096 Nov 29 19:16 671f341a793c7cfac907be0e6d9d1dbc05de184720c49d112b0052c02e99841c\ndrw-r-xr-x  2 root root 4096 Oct 17 07:23 6d52c89111e7917921ed476edb29766d00c5f66ce5e576e3a27061669328957b\ndrw-r-xr-x  2 root root 4096 Nov 27 23:41 83df74aa1ae0f0886deb33d7079303f0334fb50585ebe5d24290001349189a84\ndrw-r-xr-x  2 root root 4096 Nov 27 23:41 854db73be8af3dadb44287c58a23926cee237c5b196863933f26674c64a756bf\ndrw-r-xr-x  2 root root 4096 Nov 29 19:16 87d02a9a49ef1e66dd89f4822bed5d870621031eccd1943d1ed9e2a606ca366f\ndrw-r-xr-x  2 root root 4096 Oct 17 07:23 8fe87a743ca4053365c367884ff392b9335c15598e176ee6202e0955fbf6408d\ndrw-r-xr-x  2 root root 4096 Nov 28 21:21 a9d72c93a6b53e8bcd3103f31608146e551de2af599e8c86571f6cbd9245a142\ndrw-r-xr-x  2 root root 4096 Nov 29 10:12 b9d456fcad23ed44a05099dd3360863ff6ec6889bb1e0340a75b0e44bd52946a\ndrw-r-xr-x  2 root root 4096 Nov 30 10:57 bdf8d8f24e35b3200c422a8e1df3479c21af4a0a17efca673e0ff63f1a8c6957\ndrw-r-xr-x  2 root root 4096 Nov 30 16:51 bf27aae5caeadff7771b6dde649665e27ecd2be32c37ca99ec3e9815d9803e58\ndrw-r-xr-x  2 root root 4096 Nov 28 22:22 e73f1eb357de9cd49daa0b656f2c66eb65826b0099369f4162b640d9b24da79f\ndrw-r-xr-x  2 root root 4096 Nov 29 10:12 eea64f7067712ed40e6b0ec5ac5a7af2f1d939b0bac04678bf3c089a0e546f5a\ndrw-r-xr-x  2 root root 4096 Nov 29 19:16 f3e402ba8a737986b88b88cfa26b696875224328c5e133fd8b340b9d09638af4\nAnd yes, I can see my container now in the cadvisor web-ui ;-)\nThx\n. ",
    "josselin-c": "Hm, I have the same kind of message with cadvisor:0.6.2 and cadvisor:latest running on kubernetes over centos7:\nE1211 12:33:46.275419 00001 handler.go:262] Libcontainer state not found for container \"/system.slice/docker-ba4f5e49433eba974ffa87f07f56f74900be148695955defcdf0599f66f84307.scope\"\nHere is the manifest file used to start cadvisor:\nversion: v1beta2\nid: cadvisor-agent\ncontainers:\n  - name: cadvisor\n    image: google/cadvisor:0.6.2\n    ports:\n      - name: http\n        containerPort: 8080\n        hostPort: 4194\n    volumeMounts:\n      - name: varrun\n        mountPath: /var/run\n        readOnly: false\n      - name: varlibdocker\n        mountPath: /var/lib/docker\n        readOnly: true\n      - name: cgroups\n        mountPath: /sys/fs/cgroup\n        readOnly: true\nvolumes:\n  - name: varrun\n    source:\n      hostDir:\n        path: /var/run\n  - name: varlibdocker\n    source:\n      hostDir:\n        path: /var/lib/docker\n  - name: cgroups\n    source:\n      hostDir:\n        path: /sys/fs/cgroup\n. I don't have this file on my server, for now I install docker by directly copying the binary & the systemd file.\nHere is the command line used: \nExecStart=/usr/local/bin/docker -d \\\n                  --bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU} \\\n                  --iptables=false --ip-masq=false \\\n                   -G 'docker' \\\n                    -H fd://\n. Okay, I restarted cAdvisor with more logs (-v=4). \nHere are the first 1000 lines of the logs: http://fpaste.org/160298/51545141/\n. @vmarmol  Switched to cAdvisor 0.7.0 and it seems to work, I can see machine stats in grafana now!\n. @rjnagal the \"/sys/fs/cgroup/cpu,cpuacct/system.slice\" exists both in the base system and in the cadvisor container.\n[root@b8 run]# docker exec -ti  310ba1066ded ls -l /sys/fs/cgroup/cpu,cpuacct/system.slice | head -n 30\ntotal 0\ndrwxr-xr-x    2 root     root             0 Dec 18 15:34 -.mount\ndrwxr-xr-x    2 root     root             0 Dec 18 15:34 brandbot.service\n-rw-r--r--    1 root     root             0 Dec 18 15:34 cgroup.clone_children\n--w--w--w-    1 root     root             0 Dec 18 15:34 cgroup.event_control\n-rw-r--r--    1 root     root             0 Dec 18 15:45 cgroup.procs\n-rw-r--r--    1 root     root             0 Dec 18 15:34 cpu.cfs_period_us\n-rw-r--r--    1 root     root             0 Dec 18 15:34 cpu.cfs_quota_us\n-rw-r--r--    1 root     root             0 Dec 18 15:34 cpu.rt_period_us\n-rw-r--r--    1 root     root             0 Dec 18 15:34 cpu.rt_runtime_us\n-rw-r--r--    1 root     root             0 Dec 23 08:01 cpu.shares\n-r--r--r--    1 root     root             0 Dec 18 15:34 cpu.stat\n-r--r--r--    1 root     root             0 Dec 18 15:34 cpuacct.stat\n-rw-r--r--    1 root     root             0 Dec 18 15:34 cpuacct.usage\n-r--r--r--    1 root     root             0 Dec 18 15:34 cpuacct.usage_percpu\ndrwxr-xr-x    2 root     root             0 Dec 18 15:34 crond.service\ndrwxr-xr-x    2 root     root             0 Dec 18 15:34 data.mount\ndrwxr-xr-x    2 root     root             0 Dec 18 15:34 dbus.service\ndrwxr-xr-x    2 root     root             0 Dec 18 15:34 dbus.socket\ndrwxr-xr-x    2 root     root             0 Dec 18 15:34 dev-disk-by\\x2did-ata\\x2dHGST_HUS724020ALA640_PN2134P6HE0KLP\\x2dpart3.swap\ndrwxr-xr-x    2 root     root             0 Dec 18 15:34 dev-disk-by\\x2did-wwn\\x2d0x5000cca22dd40275\\x2dpart3.swap\ndrwxr-xr-x    2 root     root             0 Dec 18 15:34 dev-disk-by\\x2dlabel-swap\\x2dsda3.swap\ndrwxr-xr-x    2 root     root             0 Dec 18 15:34 dev-disk-by\\x2dpartlabel-primary.swap\ndrwxr-xr-x    2 root     root             0 Dec 18 15:34 dev-disk-by\\x2dpartuuid-853b16f4\\x2d8380\\x2d472b\\x2d9f59\\x2d04d80ff66591.swap\ndrwxr-xr-x    2 root     root             0 Dec 18 15:34 dev-disk-by\\x2duuid-562f7dff\\x2d4be3\\x2d41a2\\x2d8f4f\\x2d46bc4d9c4b64.swap\ndrwxr-xr-x    2 root     root             0 Dec 18 15:34 dev-sda3.swap\ndrwxr-xr-x    2 root     root             0 Dec 18 15:34 docker-021699d8c1e9b09aee38bf90f569621bf65a78f0c955200435cfdf00e0830f86.scope\ndrwxr-xr-x    2 root     root             0 Dec 19 10:33 docker-0729eeb9f49d5c1a2cf0fb66149e04127ed82a5be9a5d68a23bc0edd8325f4f2.scope\ndrwxr-xr-x    2 root     root             0 Dec 22 10:48 docker-0a7996edf0f53465976cfe864d6a1a6b494e234e060d072fbad135493b391d62.scope\ndrwxr-xr-x    2 root     root             0 Dec 18 15:35 docker-0b3708941eb996570cea8409ef991eb53637cc9cc0d2632b1778e6c4c2fcd18a.scope\n```\n[root@b8 run]# curl -L 10.244.74.215:8080/validate\ncAdvisor version: 0.7.0\nOS version: Buildroot 2014.02\nKernel version: [Supported and recommended]\n    Kernel version is 3.10.0-123.9.3.el7.x86_64. Versions >= 2.6 are supported. 3.0+ are recommended.\nCgroup setup: [Supported and recommended]\n    Available cgroups: map[memory:1 freezer:1 blkio:1 perf_event:1 hugetlb:1 cpuset:1 cpu:1 net_cls:1 cpuacct:1 devices:1]\n    Following cgroups are required: [cpu cpuacct]\n    Following other cgroups are recommended: [memory blkio cpuset devices freezer]\nCgroup mount setup: [Supported, but not recommended]\n    Cgroups are mounted at /sys/fs/cgroup/cpu,cpuacct.\n    Any cgroup mount point that is detectible and accessible is supported. /sys/fs/cgroup is recommended as a standard location.\nDocker version: [Supported and recommended]\n    Docker version is 1.4.1. Versions >= 1.0 are supported. 1.2+ are recommended.\nDocker driver setup: [Supported and recommended]\n    Docker exec driver is native-0.2. Storage driver is devicemapper.\n    systemd is being used to create cgroups.\n```\n. @rjnagal  @vishh  With cAdvisor 0.7.1 I don't see errors in the logs anymore. Thanks for the fix!\n. ",
    "liuyunsh": "Hi vmarmol, thanks for your response.\nYes, the space on that directory is enough.\nI tried the way you suggested, it seems that that container can work now, however,\nThe web page of http://ip:port  come out a error as following:\nFailed to get container \"/\" with error: unable to find data for container /\nAny suggestion?\n. hi vishh,\nYes, centos7' cgroups located in /sys/fs/cgroup/ \nso I tried the following command with same error.\ndocker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --volume=/sys/fs/cgroup/:/cgroup \\\n  --publish=9090:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  google/cadvisor:latest --logtostderr\n. {\"log\":\"I1204 05:06:14.456169 00001 container.go:189] Failed to update stats for container \\\"/system.slice\\\": stat /cgroup/cpu/system.slice: permission denied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.456234515Z\"}\n{\"log\":\"W1204 05:06:14.456195 00001 container.go:112] Failed to get RecentStats(\\\"/system.slice\\\") while determining the next housekeeping: unable to find data for container /system.slice\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.456234515Z\"}\n{\"log\":\"I1204 05:06:14.462423 00001 container.go:189] Failed to update stats for container \\\"/system.slice/iscsiuio.socket\\\": stat /cgroup/cpu/system.slice/iscsiuio.socket: permission denied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.462506599Z\"}\n{\"log\":\"W1204 05:06:14.462450 00001 container.go:112] Failed to get RecentStats(\\\"/system.slice/iscsiuio.socket\\\") while determining the next housekeeping: unable to find data for container /system.slice/iscsiuio.socket\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.462506599Z\"}\n{\"log\":\"I1204 05:06:14.468533 00001 container.go:189] Failed to update stats for container \\\"/system.slice/dev-mapper-centos\\x2dswap.swap\\\": stat /cgroup/cpu/system.slice/dev-mapper-centos\\x2dswap.swap: permission denied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.468576181Z\"}\n{\"log\":\"W1204 05:06:14.468563 00001 container.go:112] Failed to get RecentStats(\\\"/system.slice/dev-mapper-centos\\x2dswap.swap\\\") while determining the next housekeeping: unable to find data for container /system.slice/dev-mapper-centos\\x2dswap.swap\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.468613314Z\"}\n{\"log\":\"I1204 05:06:14.474999 00001 container.go:189] Failed to update stats for container \\\"/system.slice/sshd.service\\\": stat /cgroup/cpu/system.slice/sshd.service: permission denied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.475206764Z\"}\n{\"log\":\"W1204 05:06:14.475040 00001 container.go:112] Failed to get RecentStats(\\\"/system.slice/sshd.service\\\") while determining the next housekeeping: unable to find data for container /system.slice/sshd.service\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.475206764Z\"}\n{\"log\":\"I1204 05:06:14.481028 00001 container.go:189] Failed to update stats for container \\\"/system.slice/systemd-logind.service\\\": stat /cgroup/cpu/system.slice/systemd-logind.service: permission denied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.481085612Z\"}\n{\"log\":\"W1204 05:06:14.481068 00001 container.go:112] Failed to get RecentStats(\\\"/system.slice/systemd-logind.service\\\") while determining the next housekeeping: unable to find data for container /system.slice/systemd-logind.service\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.481129612Z\"}\n{\"log\":\"I1204 05:06:14.487069 00001 container.go:189] Failed to update stats for container \\\"/system.slice/NetworkManager.service\\\": stat /cgroup/cpu/system.slice/NetworkManager.service: permission denied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.487102662Z\"}\n{\"log\":\"W1204 05:06:14.487092 00001 container.go:112] Failed to get RecentStats(\\\"/system.slice/NetworkManager.service\\\") while determining the next housekeeping: unable to find data for container /system.slice/NetworkManager.service\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.487137988Z\"}\n{\"log\":\"I1204 05:06:14.493288 00001 container.go:189] Failed to update stats for container \\\"/system.slice/avahi-daemon.socket\\\": stat /cgroup/cpu/system.slice/avahi-daemon.socket: permission denied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.493401236Z\"}\n{\"log\":\"W1204 05:06:14.493310 00001 container.go:112] Failed to get RecentStats(\\\"/system.slice/avahi-daemon.socket\\\") while determining the next housekeeping: unable to find data for container /system.slice/avahi-daemon.socket\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.493401236Z\"}\n{\"log\":\"I1204 05:06:14.499541 00001 container.go:189] Failed to update stats for container \\\"/system.slice/dev-disk-by\\x2duuid-a5d93a86\\x2da084\\x2d4542\\x2dace3\\x2d6fa218904979.swap\\\": stat /cgroup/cpu/system.slice/dev-disk-by\\x2duuid-a5d93a86\\x2da084\\x2d4542\\x2dace3\\x2d6fa218904979.swap: permission denied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.499698731Z\"}\n{\"log\":\"W1204 05:06:14.499563 00001 container.go:112] Failed to get RecentStats(\\\"/system.slice/dev-disk-by\\x2duuid-a5d93a86\\x2da084\\x2d4542\\x2dace3\\x2d6fa218904979.swap\\\") while determining the next housekeeping: unable to find data for container /system.slice/dev-disk-by\\x2duuid-a5d93a86\\x2da084\\x2d4542\\x2dace3\\x2d6fa218904979.swap\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.499698731Z\"}\n. sure,please check the following infor\n[root@docker fs]# pwd\n/sys/fs\n[root@docker fs]# ls -l ./cgroup/cpu\nlrwxrwxrwx. 1 root root 11 Dec  3 04:12 ./cgroup/cpu -> cpu,cpuacct\n[root@docker fs]#\n. hi vmarmol\nhere is my result of  \"grep cgroup /proc/mounts\"\n[root@docker ~]# grep cgroup /proc/mounts\ntmpfs /sys/fs/cgroup tmpfs rw,seclabel,nosuid,nodev,noexec,mode=755 0 0\ncgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\ncgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\ncgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpuacct,cpu 0 0\ncgroup /sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0\ncgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\ncgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\ncgroup /sys/fs/cgroup/net_cls cgroup rw,nosuid,nodev,noexec,relatime,net_cls 0 0\ncgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\ncgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\ncgroup /sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0\n. Hi all, It works normally now after I closed firewall,and set selinux to disable. thanks for all of your help.\n. @scottscreations ,yeah,good luck!\n. @vmarmol , Could you please help to give a detail  setup steps with selinux enabled?\nI think this would help us a lot. thanks a lot.\n. @vmarmol ,thanks for your kindly reply. I just use centos7, I think RHEL7  also will be ok. thanks.\n. ",
    "scotticles": "I am getting the same thing, just wanted to try this out.  -- CentOS7\nsudo docker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/sys/fs/cgroup/:/cgroup \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  google/cadvisor:latest --logtostderr\n. Mine looks similiar I noticed at the start it said permission denied on the\ndocker.sock\nOn Wed, Dec 3, 2014, 10:10 PM liuyunsh notifications@github.com wrote:\n\n{\"log\":\"I1204 05:06:14.456169 00001 container.go:189] Failed to update\nstats for container \\\"/system.slice\\\": stat /cgroup/cpu/system.slice:\npermission\ndenied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.456234515Z\"}\n{\"log\":\"W1204 05:06:14.456195 00001 container.go:112] Failed to get\nRecentStats(\\\"/system.slice\\\") while determining the next housekeeping:\nunable to find data for container\n/system.slice\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.456234515Z\"}\n{\"log\":\"I1204 05:06:14.462423 00001 container.go:189] Failed to update\nstats for container \\\"/system.slice/iscsiuio.socket\\\": stat\n/cgroup/cpu/system.slice/iscsiuio.socket: permission\ndenied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.462506599Z\"}\n{\"log\":\"W1204 05:06:14.462450 00001 container.go:112] Failed to get\nRecentStats(\\\"/system.slice/iscsiuio.socket\\\") while determining the next\nhousekeeping: unable to find data for container\n/system.slice/iscsiuio.socket\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.462506599Z\"}\n{\"log\":\"I1204 05:06:14.468533 00001 container.go:189] Failed to update\nstats for container \\\"/system.slice/dev-mapper-centos\\x2dswap.swap\\\": stat\n/cgroup/cpu/system.slice/dev-mapper-centos\\x2dswap.swap: permission\ndenied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.468576181Z\"}\n{\"log\":\"W1204 05:06:14.468563 00001 container.go:112] Failed to get\nRecentStats(\\\"/system.slice/dev-mapper-centos\\x2dswap.swap\\\") while\ndetermining the next housekeeping: unable to find data for container\n/system.slice/dev-mapper-centos\\x2dswap.swap\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.468613314Z\"}\n{\"log\":\"I1204 05:06:14.474999 00001 container.go:189] Failed to update\nstats for container \\\"/system.slice/sshd.service\\\": stat\n/cgroup/cpu/system.slice/sshd.service: permission\ndenied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.475206764Z\"}\n{\"log\":\"W1204 05:06:14.475040 00001 container.go:112] Failed to get\nRecentStats(\\\"/system.slice/sshd.service\\\") while determining the next\nhousekeeping: unable to find data for container\n/system.slice/sshd.service\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.475206764Z\"}\n{\"log\":\"I1204 05:06:14.481028 00001 container.go:189] Failed to update\nstats for container \\\"/system.slice/systemd-logind.service\\\": stat\n/cgroup/cpu/system.slice/systemd-logind.service: permission\ndenied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.481085612Z\"}\n{\"log\":\"W1204 05:06:14.481068 00001 container.go:112] Failed to get\nRecentStats(\\\"/system.slice/systemd-logind.service\\\") while determining the\nnext housekeeping: unable to find data for container\n/system.slice/systemd-logind.service\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.481129612Z\"}\n{\"log\":\"I1204 05:06:14.487069 00001 container.go:189] Failed to update\nstats for container \\\"/system.slice/NetworkManager.service\\\": stat\n/cgroup/cpu/system.slice/NetworkManager.service: permission\ndenied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.487102662Z\"}\n{\"log\":\"W1204 05:06:14.487092 00001 container.go:112] Failed to get\nRecentStats(\\\"/system.slice/NetworkManager.service\\\") while determining the\nnext housekeeping: unable to find data for container\n/system.slice/NetworkManager.service\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.487137988Z\"}\n{\"log\":\"I1204 05:06:14.493288 00001 container.go:189] Failed to update\nstats for container \\\"/system.slice/avahi-daemon.socket\\\": stat\n/cgroup/cpu/system.slice/avahi-daemon.socket: permission\ndenied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.493401236Z\"}\n{\"log\":\"W1204 05:06:14.493310 00001 container.go:112] Failed to get\nRecentStats(\\\"/system.slice/avahi-daemon.socket\\\") while determining the\nnext housekeeping: unable to find data for container\n/system.slice/avahi-daemon.socket\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.493401236Z\"}\n{\"log\":\"I1204 05:06:14.499541 00001 container.go:189] Failed to update\nstats for container\n\\\"/system.slice/dev-disk-by\\x2duuid-a5d93a86\\x2da084\\x2d4542\\x2dace3\\x2d6fa218904979.swap\\\":\nstat\n/cgroup/cpu/system.slice/dev-disk-by\\x2duuid-a5d93a86\\x2da084\\x2d4542\\x2dace3\\x2d6fa218904979.swap:\npermission\ndenied\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.499698731Z\"}\n{\"log\":\"W1204 05:06:14.499563 00001 container.go:112] Failed to get\nRecentStats(\\\"/system.slice/dev-disk-by\\x2duuid-a5d93a86\\x2da084\\x2d4542\\x2dace3\\x2d6fa218904979.swap\\\")\nwhile determining the next housekeeping: unable to find data for container\n/system.slice/dev-disk-by\\x2duuid-a5d93a86\\x2da084\\x2d4542\\x2dace3\\x2d6fa218904979.swap\\n\",\"stream\":\"stderr\",\"time\":\"2014-12-04T05:06:14.499698731Z\"}\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/333#issuecomment-65538806.\n. Here is my grep on the /proc/mounts\n\ntmpfs /sys/fs/cgroup tmpfs rw,seclabel,nosuid,nodev,noexec,mode=755 0 0\ncgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\ncgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\ncgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpuacct,cpu 0 0\ncgroup /sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0\ncgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\ncgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\ncgroup /sys/fs/cgroup/net_cls cgroup rw,nosuid,nodev,noexec,relatime,net_cls 0 0\ncgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\ncgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\ncgroup /sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0\n. Here are the first few entries right after I start the cadvisor container:\nI1205 03:14:31.761574 00001 storagedriver.go:89] Caching 60 recent stats in memory; using \"\" storage driver\nI1205 03:14:31.761740 00001 manager.go:77] cAdvisor running in container: \"/system.slice/docker-721c8d048f594698ba7749b41cb3d7d6439e421d0604fa840f53204a93735d1c.scope\"\nI1205 03:14:31.775806 00001 manager.go:91] Machine: {NumCores:1 MemoryCapacity:3903705088 Filesystems:[{Device:/dev/mapper/docker-253:1-67641253-721c8d048f594698ba7749b41cb3d7d6439e421d0604fa840f53204a93735d1c Capacity:10434662400}]}\nI1205 03:14:31.779043 00001 manager.go:98] Version: {KernelVersion:3.10.0-123.9.3.el7.x86_64 ContainerOsVersion:Buildroot 2014.02 DockerVersion:Unknown CadvisorVersion:0.6.2}\nE1205 03:14:31.779156 00001 cadvisor.go:62] Docker registration failed: unable to communicate with docker daemon: dial unix /var/run/docker.sock: permission denied.\nI1205 03:14:31.785884 00001 factory.go:78] Registering Raw factory\nI1205 03:14:31.791596 00001 manager.go:394] Added container: \"/\" (aliases: [], namespace: \"\")\nI1205 03:14:31.791632 00001 manager.go:131] Starting recovery of all containers\nI1205 03:14:31.797082 00001 container.go:141] Start housekeeping for container \"/\"\nI1205 03:14:31.797233 00001 container.go:189] Failed to update stats for container \"/\": stat /sys/fs/cgroup/cpuacct: permission denied\nW1205 03:14:31.797251 00001 container.go:112] Failed to get RecentStats(\"/\") while determining the next housekeeping: unable to find data for container /\nI1205 03:14:31.922553 00001 manager.go:394] Added container: \"/system.slice\" (aliases: [], namespace: \"\")\nI1205 03:14:31.927679 00001 manager.go:394] Added container: \"/system.slice/dev-disk-by\\x2did-dm\\x2dname\\x2dcentos\\x2dswap.swap\" (aliases: [], namespace: \"\")\nI1205 03:14:31.928801 00001 container.go:141] Start housekeeping for container \"/system.slice\"\nI1205 03:14:31.929435 00001 container.go:189] Failed to update stats for container \"/system.slice\": stat /sys/fs/cgroup/cpu/system.slice: permission denied\nW1205 03:14:31.929463 00001 container.go:112] Failed to get RecentStats(\"/system.slice\") while determining the next housekeeping: unable to find data for container /system.slice\nI1205 03:14:31.929473 00001 container.go:141] Start housekeeping for container \"/system.slice/dev-disk-by\\x2did-dm\\x2dname\\x2dcentos\\x2dswap.swap\"\nI1205 03:14:31.929543 00001 container.go:189] Failed to update stats for container \"/system.slice/dev-disk-by\\x2did-dm\\x2dname\\x2dcentos\\x2dswap.swap\": stat /sys/fs/cgroup/cpu/system.slice/dev-disk-by\\x2did-dm\\x2dname\\x2dcentos\\x2dswap.swap: permission denied\n. I turned off SELinux and it did load up, i wasn't able to see any containers listed. So I am not sure its 100% working. I've never used cadvisor to compare. I might kick up a ubuntu host and put some containers on to see. \n. ",
    "saifikhan": "In CentOS 7, cgroups are mounted under /sys/fs/cgroup/\npwd\n/sys/fs/cgroup\nls -F\nblkio/  cpu@  cpuacct@  cpu,cpuacct/  cpuset/  devices/  freezer/  hugetlb/  memory/  net_cls/  perf_event/  systemd/\n\n. ",
    "mdshuai": "@vmarmol  More logs: http://fpaste.org/211498/91484841/\n. @vmarmol \n1) kernel version: 3.10.0-229.el7.x86_64\n2) Cadvisor version: 0.11.0; \n3) selinux version\nlibselinux-python-2.2.2-6.el7.x86_64\nselinux-policy-3.13.1-23.el7.noarch\nlibselinux-2.2.2-6.el7.x86_64\nlibselinux-utils-2.2.2-6.el7.x86_64\nselinux-policy-targeted-3.13.1-23.el7.noarch\n4) docker version:\ndocker --version\nDocker version 1.4.1-dev, build d26b358/1.4.1\n5) when enable the selinux, run the cadvisor and run another container ; then from the web to access the container, there is no container list.\nThe /var/log/message is : http://fpaste.org/213633/95841921/\nNot sure this info can help you to debug the issue.\n. @ncdc @sjenning\n. @ncdc thanks for your info, so in cadvisor webconsole for container level the fs usage only can be increased.\n. @ncdc @sjenning Could you help check this issue. thanks\n. This issue should be fixed by https://github.com/google/cadvisor/pull/1489. ",
    "tanmx": "CentOS, Fedora, and RHEL\nYou may need to run the container with --privileged=true and --volume=/cgroup:/cgroup:ro \\ in order for cAdvisor to monitor Docker containers.\nRHEL and CentOS lock down their containers a bit more. cAdvisor needs access to the Docker daemon through its socket. This requires --privileged=true in RHEL and CentOS.\nOn some versions of RHEL and CentOS the cgroup hierarchies are mounted in /cgroup so run cAdvisor with an additional Docker option of --volume=/cgroup:/cgroup:ro .\n. ",
    "franc0is": "From the docker repo, it appears 0.6.6 was the first one exhibiting this behavior. See https://github.com/docker/docker/commit/2205bb43ea1e1709b104896266edeaf58395005b\n. Docker version 1.0.1, build 990021a. I've updated docker since, I can try it again at some point.\n. Works with Docker version 1.3.2, build 39fa2fa. That being said, it fails with the docker version currently in ubuntu LTS repositories. As far as // being a trick, I think that was the advice I got from docker's IRC channel but I may be wrong. In any case, I have a working setup.\n. ",
    "bluecmd": "How is this currently expected to work? I'm running cAdvisor inside a docker container with systemd (https://gist.github.com/bluecmd/e9f00f813aa8891ada2be939d98e45ba)\nThis is how cAdvisor looks like for me: http://goo.gl/TTIr4l\nFurthermore, the metrics are prefixed with the absolute docker path:\n...\ncontainer_cpu_system_seconds_total{id=\"/\"} 21716.47\ncontainer_cpu_system_seconds_total{id=\"/docker\"} 4784.47\ncontainer_cpu_system_seconds_total{id=\"/docker/905e57d8fa7c9d69e5b1eef9ea9b8766f57fb3dd86e9c05a267d34e38d769635\"} 414.4\ncontainer_cpu_system_seconds_total{id=\"/docker/905e57d8fa7c9d69e5b1eef9ea9b8766f57fb3dd86e9c05a267d34e38d769635/system.slice\"} 405.09\ncontainer_cpu_system_seconds_total{id=\"/docker/905e57d8fa7c9d69e5b1eef9ea9b8766f57fb3dd86e9c05a267d34e38d769635/system.slice/aaautils.service\"} 0.04\ncontainer_cpu_system_seconds_total{id=\"/docker/905e57d8fa7c9d69e5b1eef9ea9b8766f57fb3dd86e9c05a267d34e38d769635/system.slice/acpid.service\"} 0\n...\nI'm not seeing how this is useful right now. What I would expect is that only things under the instance's cgroup root would be exported. Also that the container-local systemd is detected.\nEDIT: I should add, this is how I start the container:\ndocker run --privileged -v /tmp:/tmp -v /dev/log:/dev/log -v /sys/fs/cgroup:/sys/fs/cgroup -h ops --name ops my_image /sbin/init &\n. As a continuation of this: I would like to use cAdvisor to simply scrape cgroup information from systemd, not use any Docker/rkt or other container runtime. I would appreciate if there was a \"-cgroup_only\" mode or something, as there is \"-docker_only\".\nThanks!\n. ",
    "kateknister": "I think everything is done with this now.  I changed the stats_buffer_test.go file back so it didn't import its own package.\n. I squashed my commits into one.\n. Updated the handler and tests so that WatchEvents can send events out.\n. Thanks, I think I fixed all the comments for the last commit, if you want to take another look!\n. I got rid of the redundancies in the tests and in the WatchEvents/AddEvent functions that I forgot to clean up earlier.\n. Would this be okay to merge now?\n. @dchen1107 Here is the events handler that will be used to manager oom events\n. Does it still need a rebase?\n. I'm putting in the streaming write function next, but I tried to break it up a bit by not putting it in this commit.\n. I removed the extra test\n. This will allow container events (creation and deletion) to be tracked by the manager's event manager.\n. Oomparser on with 100ms delay for every loop call: (average: 0.010 cores), 9.785 MB of memory\nNo oomparser: (average: 0.008 cores), 9.753 MB of memory\nThe delay minimizes the effect of the oomparser but there is a bit of difference.\n. Here is an updated version!\n. @rjnagal , @vmarmol  could you review this change if you have a chance? Thanks!\n. Hey @rjnagal, does this look alright as a first patch? Once I figure out how to communicate with the dbus as you suggested I can replace it with that solution.\n. Hey @rjnagal,  I moved the events client to the client library and got rid of the hardcoded url. Rather than printing, now the functions return a struct or a channel through which events may be retrieved. I put in an example main package as well but I can remove that, I wasn't sure if that was useful or not.\n. @vmarmol here's the TODO statement\n. This shouldn't be merged yet, I just wanted some feedback on this approach\n. @rjnagal \n. @cadvisorJenkinsBot \n. It is part of the package. However when I remove it, the test does not compile because types defined in the memory package like StatsBuffer are undefined.  Is there a way to fix this so I don't import the local package into the file?\n. Errors.New() won't take %s arguments as fmt will, apparently.  I could construct a string with the variables concatenated or just leave them in the string.\n. https://docs.google.com/a/google.com/document/d/1pSlVGqAqafk2dmMWz8SdUtbOnFBYAJy19J0mN5TqF9U/edit?usp=sharing\n. Should I change the name to something else? \n. I was thinking if the user specified both, it would return up to N events that fell in the time range.  I could change the field name LastNEvents to MaxEventsReturned or GetNEvents. In case there's a ton of events, it could be helpful to the user to limit the number of events returned.\n. Did you mean move the call to StreamOoms outside the Go routine here?  I made a change in oomparser so that that function StreamOoms will return rather than hang by putting its monitoring call in another goroutine.\n. Is there an advantage to converting the string to a url and calling query on it rather than the current implementation?\n. Do you mean there might be a backlog of IN_MODIFY notices to the watcher if there are many changes to the file all at once?\n. The information from cont.handler.GetSpec() pertains more to the container when it was alive, but we could still return it here as event data in case the user wanted to see information about the container before it was stopped.\n. I called it in a goroutine in manager's Start function\n. since CleanupWatcher is called in a goroutine and the channel deadWatchAlert does not terminate, I thought by making a defer call to Unlock() would mean it would never release the watcherLock. But should I instead change this back to a function that is called every time an EventChannel is no longer needed rather than a constantly running thread?\n. In that case should I also get rid of the EventManager's cleanup thread?\nI'm not sure how to alert the EventManager's cleanup thread that this particular watcher is no longer needed if we only tell the EventChannel to stop. But I could change it back to the way it was originally written, so that I tell the EventChannel to stop and then, rather than putting the CleanupWatcher in a goroutine and having it always run, just call it on a case by case basis here to search for and remove any stopped EventChannel objects. Or we could still have the cleanup thread, but rather than alerting it immediately to stopped watches, it searches for them on a timed basis\n. Only if it's run as root.\n. I deleted these tests because I deleted the function.  The tests for StreamOoms are almost identical to the tests I deleted for analyzeLines\n. I am running into issues using the Client().ContainerInfo() and DockerContainer() methods with this image type because I keep getting the error \"failed to get container...\" with error \"unknown container....\" I assume this is because I force the container to timeout and generally the client's attempt to get the container information occurs after the container has stopped. However, the container id is returned when the \"-d\" flag is added to the docker command, so would it be sufficient to check that the event container name contains the returned containerId?\n. This list of subcontainers is a set of all descendants of the container, based on how it was populated in getSubcontainerContainerData in manager.go 708. But this would need to be recursive if that were to change to only immediate children. Why does the below need to only track immediate children rather than all descendant containers?\n. Sure, and I'll change it to be recursive. That seems like it would be more efficient, so that we're not always checking the sub-sub containers of a given container.\n. ",
    "jamshid": "Right, well a graph is shown, but it's always at 0.\nSorry, not sure if you mean run that in a container or in boot2docker ssh, so here's both:\n$ docker run --rm -ti ubuntu bash \nroot@cfd6494e0619:/# cat /proc/cgroup\ncat: /proc/cgroup: No such file or directory\nroot@cfd6494e0619:/# grep cgroup /proc/mounts\n$ boot2docker ssh sudo cat /proc/cgroup\ncat: can't open '/proc/cgroup': No such file or directory\nerror in run: exit status 1\n$ boot2docker ssh grep cgroup /proc/mounts\ncgroup /sys/fs/cgroup tmpfs rw,relatime,mode=755 0 0\ncgroup /sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\ncgroup /sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\ncgroup /sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\ncgroup /sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\ncgroup /sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\ncgroup /sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\ncgroup /sys/fs/cgroup/net_cls cgroup rw,relatime,net_cls 0 0\ncgroup /sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\ncgroup /sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n. I guess upgrading to docker 1.4.1 fixed things, I just restarted cadvisor and now I see memory usage.\n```\n$ boot2docker ssh sudo cat /proc/cgroups\nsubsys_name    hierarchy   num_cgroups enabled\ncpuset  1   16  1\ncpu 2   16  1\ncpuacct 3   16  1\nmemory  4   16  1\ndevices 5   16  1\nfreezer 6   16  1\nnet_cls 7   1   1\nblkio   8   16  1\nperf_event  9   16  1\n``\n. Every time I try cadvisor, after a while I have to disable it because of high cpu usage (eg, see below). Likely the sameduissue, saw du's intop` briefly. I did a fresh docker pull today of google/cadvisor. \n```\n$ docker ps -q | xargs docker stats\nCONTAINER           CPU %               MEM USAGE/LIMIT     MEM %               NET I/O\n68e20776129f        0.00%               1.082 MiB/500 MiB   0.22%               452 KiB/46.16 KiB\n7b31946e2c1e        638.20%             42 MiB/500 MiB      8.40%               21.02 MiB/516 MiB\n8ab17fc74c77        3.47%               3.832 MiB/500 MiB   0.77%               8.042 KiB/648 B\n...\n$ docker ps -a | grep 7b31946e2c1e\n7b31946e2c1e        google/cadvisor:latest                                                    \"/usr/bin/cadvisor t   3 hours ago         Up 3 hours                     0.0.0.0:911->8080/tcp                                                                     buildenv_cadvisor_1\n$ docker-machine ssh dev sudo top\nMem: 3650684K used, 402700K free, 0K shrd, 290K buff, 292K cached\nCPU: 15.0% usr 78.1% sys  0.0% nic  6.3% idle  0.0% io  0.0% irq  0.3% sirq\nLoad average: 13.33 13.02 9.79 14/1423 6992\n  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND\n 2356   718 root     R     885m 22.3   7 70.4 /usr/bin/cadvisor true\n 8833  8816 root     S    2470m 62.2   0  3.0 /usr/bin/java -Xms256m -Xmx1g -Xss256k -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyF\n14698 14688 root     S    2480m 62.5   4  2.6 /usr/bin/java -Xms256m -Xmx1g -Xss256k -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyF\n...\n```\n. Thanks @achille-roussel and looks like it's documented officially below. I'm surprised docker-machine doesn't do this grub change automatically, or at least warn that you might want to.\nhttps://docs.docker.com/engine/installation/linux/ubuntulinux/#adjust-memory-and-swap-accounting\n. ",
    "hinesmr": "Yes, a map would be great. If we use a map, the \"container_hints.json\" could be extended to Docker without hard-coding specific aliases into cAdvisor. (I know container_hints doesn't work with Docker yet, but this would be a start). \nWhat if we do both a map and a list at the same time?\nSomething like this:\naliases : [ \"foo\", \"bar\", \"baz\" ],   # same as before, Cadvisor can still generate /api/v1.2/docker/XXXX access\naliases_map : { \n      \"foo\" : { \n          \"source\" : \"docker|raw|libcontainer|whatever......\",   # Where did this alias come from?\n          \"type\" : \"ID\",\n          \"something\" : \"else\"\n       },\n       \"bar\" : {\n           \"source\" : \"docker|raw|libcontainer|whatever......\",\n           \"type\" : \"hostname\",\n        },\n        \"baz\" : {\n           ...... you get the idea.\n        }\n}\nThis would allow the client-side code to configure which aliases are useful and which aliases are not useful so they can make direct lookups of individual container statistics.\n. Probably this patch will be replaced by new discussion, so I'll squash my commit next time =)\n. Yes, that's correct: I need direct access to a container by hostname (or any other alias that is important). If the host machine has hundreds (maybe even thousands) of containers running, it would be too costly to iterate one-by-one through all the docker containers and manually sort. Client software already knows which containers to query for - so it should be possible to access them with the same alias mechanism that you guys have already provided.\nThe problem is that: In order to avoid iteration, we need to know where a particular alias comes from - without knowing that, we can't be sure whether to use that alias or not in client code........\n. Yes, it has a list of hostnames prior to communicating with cadvisor.\nIn our architecture, the software that communicates with cadvisor is not the same as the software that instructs docker to create a new container. So, no, this client software in particular does not know any list of Docker IDs or Docker names in advance. Only the IDs that we have created.\nSo, we need some way to inject \"custom\" IDs that are respected by cAdvisor.\n. No, that's not possible - it's already used by our management software stack for other purposes. And it's made worse by the fact that Docker does not (yet) support arbitrary key/name value pairs upon creating new containers, so there's no way to store additional metadata inside Docker to be consumed later.\n. Very interesting. How do you install those environment variables on a per-container basis? Not through libcontainer or docker? Do you install them before the container starts or somewhere in the container filesystem? (I would like to avoid reaching into the container filesystem if at all possible).\n. Right, exactly. Thanks for the consideration, guys.\n. OK, I've pull the latest master inside my nested environment and I get the following error:\n(My previous version cAdvisor binary 0.6.2 can still startup OK, though).\n$ ./cadvisor \nF0207 10:50:59.524656 38963 cadvisor.go:76] Failed to create a Container Manager: failed to get cache information for node 0: open /sys/devices/system/cpu/cpu0/cache/index0/shared_cpu_list: no such file or directory\ngoroutine 16 [running]:\nglog.stacks\n        /root/gopath/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:726\ngithub_com_golang_glog.output.pN31_github_com_golang_glog.loggingT\n        /root/gopath/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:677\ngithub_com_golang_glog.printf.pN31_github_com_golang_glog.loggingT\n        /root/gopath/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:635\ngithub_com_golang_glog.Fatalf\n        /root/gopath/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:1033\nmain.main\n        /root/gopath/src/github.com/google/cadvisor/cadvisor.go:76\ncreated by main\n        ../../../src/libgo/runtime/go-main.c:42\nAnd here is the output of that /sys directory in my nested environment:\n$ ls /sys/devices/system/cpu/cpu0/cache/index0/\ncoherency_line_size  level  number_of_sets  shared_cpu_map  size  type  ways_of_associativity\nLooks like I don't have that shared_cpu_list file.\nComments?\n. Success! Here's the output inside my outer LXC container finding the inner Docker container\n./cadvisor -logtostderr=true -alsologtostderr=true -v 5\n..... snip .......\nI0208 10:26:55.126235 51975 factory.go:63] Using factory \"docker\" for container \"/lxc/host_container/docker/3f982e936ec2c568b2f256e2a1306a037eb3ed83386011a6714d196a6342bc9d\"\nI0208 10:26:55.127071 51975 fs.go:70] Filesystem partitions: map[/dev/disk/by-uuid/c786659d-5d04-423b-b794-c540a508763e:{mountpoint:/init.lxc.static major:8 minor:2} /dev/sdd1:{mountpoint:/ major:8 minor:49}]\nI0208 10:26:55.129846 51975 manager.go:414] Added container: \"/lxc/host_container/docker/3f982e936ec2c568b2f256e2a1306a037eb3ed83386011a6714d196a6342bc9d\" (aliases: [nova-dd8fb809-064a-43e8-94db-358746a83734 3f982e936ec2c568b2f256e2a1306a037eb3ed83386011a6714d196a6342bc9d], namespace: \"docker\")\nThanks, guys =)\n. ",
    "cadvisorJenkinsBot": "Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. ok to test\nOn Tue, Jun 9, 2015 at 2:55 PM, Ananya Kumar notifications@github.com\nwrote:\n\nI authored this commit.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/764#issuecomment-110516069.\n\nYou received this message because you are subscribed to the Google Groups\n\"cadvisor jenkins bot\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an\nemail to cadvisor-jenkins-bot+unsubscribe@google.com.\nTo post to this group, send email to cadvisor-jenkins-bot@google.com.\nTo view this discussion on the web visit\nhttps://groups.google.com/a/google.com/d/msgid/cadvisor-jenkins-bot/google/cadvisor/pull/764/c110516069%40github.com\nhttps://groups.google.com/a/google.com/d/msgid/cadvisor-jenkins-bot/google/cadvisor/pull/764/c110516069%40github.com?utm_medium=email&utm_source=footer\n.\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. ok to test\n\nOn Fri, Jun 19, 2015 at 5:37 PM, anushree-n notifications@github.com\nwrote:\n\nI signed it.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/776#issuecomment-113686844.\n\nYou received this message because you are subscribed to the Google Groups\n\"cadvisor jenkins bot\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an\nemail to cadvisor-jenkins-bot+unsubscribe@google.com.\nTo post to this group, send email to cadvisor-jenkins-bot@google.com.\nTo view this discussion on the web visit\nhttps://groups.google.com/a/google.com/d/msgid/cadvisor-jenkins-bot/google/cadvisor/pull/776/c113686844%40github.com\nhttps://groups.google.com/a/google.com/d/msgid/cadvisor-jenkins-bot/google/cadvisor/pull/776/c113686844%40github.com?utm_medium=email&utm_source=footer\n.\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Build finished. No test results found.\n. Build finished. No test results found.\n. Can one of the admins verify this patch?\n. Yes. I do think we will have to support both versions ideally.\n\nOn Tue, Aug 11, 2015 at 8:18 AM, Rohit Jnagal notifications@github.com\nwrote:\n\nIIRC, after merging this, we end up dropping support for older influxdb\nformats. Is that right?\nWe have quite a few users at the older version. @vishh\nhttps://github.com/vishh do you think we need to support both versions\nfor a while, or is it okay to drop the old one?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/800#issuecomment-129926752.\n\nYou received this message because you are subscribed to the Google Groups\n\"cadvisor jenkins bot\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an\nemail to cadvisor-jenkins-bot+unsubscribe@google.com.\nTo post to this group, send email to cadvisor-jenkins-bot@google.com.\nTo view this discussion on the web visit\nhttps://groups.google.com/a/google.com/d/msgid/cadvisor-jenkins-bot/google/cadvisor/pull/800/c129926752%40github.com\nhttps://groups.google.com/a/google.com/d/msgid/cadvisor-jenkins-bot/google/cadvisor/pull/800/c129926752%40github.com?utm_medium=email&utm_source=footer\n.\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. That's a \"good\" breaking change :) go for it!\nOn Dec 8, 2015 11:58 AM, \"Jimmi Dyson\" notifications@github.com wrote:\nDefinitely most recent samples.\nI'm happy with the breaking change. cadvisor is still only a 0.x release\nso API stability not guaranteed [image: :stuck_out_tongue_closed_eyes:]\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1010#issuecomment-162997109.\n\nYou received this message because you are subscribed to the Google Groups\n\"cadvisor jenkins bot\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an\nemail to cadvisor-jenkins-bot+unsubscribe@google.com.\nTo post to this group, send email to cadvisor-jenkins-bot@google.com.\nTo view this discussion on the web visit\nhttps://groups.google.com/a/google.com/d/msgid/cadvisor-jenkins-bot/google/cadvisor/pull/1010/c162997109%40github.com\nhttps://groups.google.com/a/google.com/d/msgid/cadvisor-jenkins-bot/google/cadvisor/pull/1010/c162997109%40github.com?utm_medium=email&utm_source=footer\n.\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. Can one of the admins verify this patch?\n. \n",
    "googlebot": "Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n need_author_consent \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n need_author_consent \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\nGooglers can find more info about SignCLA and this PR by following this link.\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n\u2139\ufe0f Googlers: Go here for more info.\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n\u2139\ufe0f Googlers: Go here for more info.\n need_sender_cla \n. CLAs look good, thanks!\n\u2139\ufe0f Googlers: Go here for more info.\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n\u2139\ufe0f Googlers: Go here for more info.\n need_sender_cla \n. CLAs look good, thanks!\n\u2139\ufe0f Googlers: Go here for more info.\n ok \n. ",
    "k8s-bot": "Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit d4ad9ce0ff2f38d2c6c457962c9ec328476ee2e5.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit d4ad9ce0ff2f38d2c6c457962c9ec328476ee2e5.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit d4ad9ce0ff2f38d2c6c457962c9ec328476ee2e5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 9ec37ec1ed04d9f0a19f6eb383f38f47aa7d4ceb.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit febf302fca2ba9227b132f35f4f6080a8b9cd98d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 6ca87a7739c3a776503de4dbc78ce7e88fb0287f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 592322f52b81b1683821e647109f22180a843f48.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 367ae973f66ed922e01f720b9bb5a86d32c4a5e7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 51c6745cf84e8a516365046b1cbf165bf5454d8e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 80241d8db0d71e3618d50547609bf4671d5fc108.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit b6bdd0c9abf568094e9da0c4d416a8b27dba4ca0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 0ce61e379e164ce35bc7b02ef01e921e1115937b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 0ce61e379e164ce35bc7b02ef01e921e1115937b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 0ba1846f43478c2a9f0d401bd8ddf9584066aab0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 04369cee2c2264fe2045bc4cccf70cddb61daad0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 66e973a62dabc38dea6694bdee6cdca2bfed14ea.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 66e973a62dabc38dea6694bdee6cdca2bfed14ea.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 56b47aa13c95cce6678b90d698fcc436d640aac4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit bee2a329136b09147815b912a326f88a4c9f0b57.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit bee2a329136b09147815b912a326f88a4c9f0b57.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4a3a276f18e81fd3e2e4cb062a206adcb18c02d4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 2583c39e819e3ec2e3c65710e72339ed03a7fa3d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c93d365bfd9e550b92177128757d3b8402ec44ba.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 136467351fa28de1ea221efef95b4548e3ca959d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3096281db66ef983429024681149e940b797b95f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c15f115858c1804cd3f1ae5e69f8e5d6878c43f3.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 4263cc047c6edd9d4caf4f9977727cb14d83f48e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 54c6281aebc0315879e9b54fe14ab42eec0643bd.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit f8f2e6107a3b7073c1460fe168df0aaa43fb13ae.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit f8f2e6107a3b7073c1460fe168df0aaa43fb13ae.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit f8f2e6107a3b7073c1460fe168df0aaa43fb13ae.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 9f52cad036bab3eea35fe296c1a56a2c4c827d9c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 9f52cad036bab3eea35fe296c1a56a2c4c827d9c.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. EXPERIMENTAL - CAN IGNORE\nBuild/test failed for commit e4f920796ef595f3a44fbaf41558e213c5a03841.\n- Build Log\n- Test Artifacts\n. Jenkins GCE e2e\nBuild/test failed for commit e4f920796ef595f3a44fbaf41558e213c5a03841.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit e4f920796ef595f3a44fbaf41558e213c5a03841.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3675a96045fda3d7f05fdc7b39ac90373e303137.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 78592db73231eaf6c055a6f6aa27506838a28ec1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 669bc4abfab349e18bc4efbec0f8d13b13602e29.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. EXPERIMENTAL - CAN IGNORE\nBuild/test passed for commit fd06fcab754b96be11f4e507aa7d836129cff77a.\n- Build Log\n- Test Artifacts\n. Jenkins GCE e2e\nBuild/test failed for commit fd06fcab754b96be11f4e507aa7d836129cff77a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit fd06fcab754b96be11f4e507aa7d836129cff77a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit fd06fcab754b96be11f4e507aa7d836129cff77a.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. EXPERIMENTAL - CAN IGNORE\nBuild/test failed for commit 8cc528621ccf9e7e2e4a72bff3c4e9a6cdcb9c0b.\n- Build Log\n- Test Artifacts\n. Jenkins GCE e2e\nBuild/test failed for commit 8cc528621ccf9e7e2e4a72bff3c4e9a6cdcb9c0b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 8cc528621ccf9e7e2e4a72bff3c4e9a6cdcb9c0b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 8cc528621ccf9e7e2e4a72bff3c4e9a6cdcb9c0b.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 29469c7806aa6c09d97b04adf76553cc05daed5c.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. EXPERIMENTAL - CAN IGNORE\nBuild/test passed for commit 58f31d884a72853ddaf4d5ddabf820e402728a46.\n- Build Log\n- Test Artifacts\n. EXPERIMENTAL - CAN IGNORE\nBuild/test failed for commit 97257ccf61a5a246d122eb02723ecfbe7bc55b2e.\n- Build Log\n- Test Artifacts\n. EXPERIMENTAL - CAN IGNORE\nBuild/test passed for commit 808a917fc0ba312cbe7d6aab6809182b342ab1f0.\n- Build Log\n- Test Artifacts\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. EXPERIMENTAL - CAN IGNORE\nBuild/test failed for commit 0f770ffadd2532dcf81307dc740e728cb0a06070.\n- Build Log\n- Test Artifacts\n. EXPERIMENTAL - CAN IGNORE\nBuild/test passed for commit f6a2def861c61cfaeeb7afa56edb22189bfd8cf9.\n- Build Log\n- Test Artifacts\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit e0fef76668ffa336d6e885af2c2b0000ba40c87c.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit e5b6bfa94f4f8d492c96f4ada64db325eccc9c29.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e5b6bfa94f4f8d492c96f4ada64db325eccc9c29.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 768aba06cfbfedce72bfc56545c1f7cf77c35877.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 768aba06cfbfedce72bfc56545c1f7cf77c35877.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 768aba06cfbfedce72bfc56545c1f7cf77c35877.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 768aba06cfbfedce72bfc56545c1f7cf77c35877.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 768aba06cfbfedce72bfc56545c1f7cf77c35877.\n- Build Log\n. EXPERIMENTAL - CAN IGNORE\nBuild/test passed for commit dee3f07b0afc36bd41014679989e9610a4c1e8a7.\n- Build Log\n- Test Artifacts\n. Jenkins GCE e2e\nBuild/test passed for commit dee3f07b0afc36bd41014679989e9610a4c1e8a7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e2b96bb8779ad6b5e35c12bd638e704e9e75cd74.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit e2b96bb8779ad6b5e35c12bd638e704e9e75cd74.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit be439fb2bf2542dbc1c77d720283ea257ec3dafc.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 081f391bba8450f0118826f8dfc8a1c96a151e46.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 144ccdf65701e992de28ccfe49af67cbb12adae7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 144ccdf65701e992de28ccfe49af67cbb12adae7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 144ccdf65701e992de28ccfe49af67cbb12adae7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 144ccdf65701e992de28ccfe49af67cbb12adae7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 144ccdf65701e992de28ccfe49af67cbb12adae7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ae7f38b16f572af9704d97fbfa7e5c8e4f235eec.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ae7f38b16f572af9704d97fbfa7e5c8e4f235eec.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ae7f38b16f572af9704d97fbfa7e5c8e4f235eec.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 8bb99d773d621046b3f9df1affd3cd698f01e0fe.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit b3159b50e52efc865985ba49a15e93279f712d6d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit cfa614d8abd338d8cf24e296b912d5aebcadef65.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit fbb7c46f05e9ba4402fd25926121a4e1d05e955b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit d9b07311fef127721d2f710e121cf1aaf16a8e6a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 7ef6909567646b92612fc6aec51d075fa578fdb7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit d2b938b1e9982dc63b85870c31f84ab888251e12.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 9626d536d473baafb402e00acf0cd9153f3a93c1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 8addb50ae7b3ab04d129cfd08faa5a0071145ee2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 4cbf5a4796f893d67becc58b963675a36fb48980.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b3f7d812b31f419e3c682590ad9dd3d0a90421d1.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 0b06645e3d5f9527760e5c074b0d9b1de3e139f7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c489dd686cb8fa1cf433a75c6046652c9fe828e6.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d44eba6bb24f4b2d12e01cec783a243fcd74396d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 99cb43bca14a8e43b40a48501a0f205ffa3247fb.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 46eded2cc72747d9df72bc218c73120e57e49649.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b903b751e2850c575aa6896b10745c5e96ae7efe.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 49ee94d6772d242149b0d5845c6b6cc34a9aeeae.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 05b435e1ebc218592cb2f12d1f71938567d8befe.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 706a954e661d31eb76ead3093417546b4244fad0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ee73baeef717c0cffba60f9bcd84060d69ab37da.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5b7a2fbd450beabae8d6a8fb5287b9f66c07535e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 313be38e81f9b0dbf2a427de4b5bc65a95b087b9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit f7fd8c136b80f532384bf88354084a5351c089f7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit e370363bb14cb7b481362eaff2afe9dc376c8483.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit e370363bb14cb7b481362eaff2afe9dc376c8483.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e011781a880c7a8605d8912d61b3c6c86557063b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 56be6ff900ada71185590e90041251968d865a60.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 3c1fd3e4b255def95853e558be9cc2f2f51415c1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 3c1fd3e4b255def95853e558be9cc2f2f51415c1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3c1fd3e4b255def95853e558be9cc2f2f51415c1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3c1fd3e4b255def95853e558be9cc2f2f51415c1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 33386f899b40e128179b85e3e16adc28c89f00ca.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 1e1736ebc924bdc33831bd59405bd7e3cf019caa.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 985e846cbe0082b2f0da6d13f4c9627d484997c8.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 09f948c76ed03d5fa30b75126b3ab0b6d62621c5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4e9d29a408f2f14a8d2ce4793fbbdf793a218499.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 8bfca2b86b5ddc8000446a0bdcb10e50b7dafb62.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit d656ffb0dbd8032f95f8d4b9349ae1d78f0b5e1e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 97e71c9ac02ec5d3550a4f5fb90f9195f0cc25ee.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 325664766811cdac02735f97895fef6c4aa7034f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 18a0031a595160596c69b9bc7e791d7f9947aa8d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 04f3743a09b0ca39337aa2cc2e538d211b2e5e68.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 04f3743a09b0ca39337aa2cc2e538d211b2e5e68.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 127b09f5aec1fd65d68010d5109bd9c1f2f18af9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 9f10f858517d85affc732be8a3e6b2e19b612475.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 7d053578d455b71cc4dc68642ebe00dd99e15262.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 7d053578d455b71cc4dc68642ebe00dd99e15262.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit d0b1c5e87743c8637c72133f92e254410c86114b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f24e322c7d4482c16edf9c73a9b9496409cefcbf.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4c43545dd582ba1ce37ecd16b393ff01b8670be4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit fac1daa1e71e4d63c0cc33d1506f0523a879f112.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit fac1daa1e71e4d63c0cc33d1506f0523a879f112.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 3a79686a0905bfe7acb24fc36b5f22b13be5124c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 3045b08a397133315ead521f77b22f10995678a0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 460e27c0d1d2a2e07b47aeb571511a6d8f862fa6.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 460e27c0d1d2a2e07b47aeb571511a6d8f862fa6.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 460e27c0d1d2a2e07b47aeb571511a6d8f862fa6.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 07d208581c79a4032b3e461a2e463dbfe726e036.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 00c282ee976038ffd036d2a36e8d1d8c66cc3199.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 3edf02e0b9fa9462ac778fad9d90dd2aec888191.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 4f892c4a2605ee6f1f5912f21a54d8602dc78d49.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 4f892c4a2605ee6f1f5912f21a54d8602dc78d49.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 336821d28effa6a66f3c26d697731000393127c0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f9bd97b3372c23baf43a0dfcb97f74ad60313fc8.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 9bc6e7a51bf09bac3d66788dfe95720a20bc6289.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4f7f2ff9d26fe1e8328efd6ba143cd47e8f3eb5c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 0894671c74665a6abaedb90bc7a4339b93b5a052.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 0894671c74665a6abaedb90bc7a4339b93b5a052.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit be243ef164fdd31878ede6bebc2cea85f098f9f2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit be243ef164fdd31878ede6bebc2cea85f098f9f2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 40dacb3269e57e0cb5d8e9b24d57d9c3bf2628e9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 41a1550805d6606daaed2fcedf971a58bdccb8b3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5ab2f45fc645e49265b5f0b1686fe42e8ae8b059.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ae9c2e420b687d152622573eee785ffe70f5e24d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ae9c2e420b687d152622573eee785ffe70f5e24d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 16abcd2cdc16e6e6692abd1778c5c61d34f22edc.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 11b63cffda14efcc948e4bb4590dcd43935c33f2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 8b1c249e87074a74f4d919866196a726aa6342e7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5759952500d75a33d81e08ea96a9681bd040ed28.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit bde0f2e92d26576306811a0f2fe8dfd25136e2af.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 4eeafb2109d8682f30ce4cb3394abcf9af00bb90.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 7001a519cf49f5c7b32a95efe7fcb26626e7fe4c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f9be680db3d165c278d3bec7b1fa541ce24a607f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f0fd525a242aa3f5ef58992761ba02460d455bdf.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 97506415d151ff570003ba606f2f7f8209137aad.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 97506415d151ff570003ba606f2f7f8209137aad.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5917e84a0c6fa53985be173fea607a0a7169a3ec.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 9389fdabe554d11edf693201cd7aa308378f3428.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit fbfdbb6516ec4ed46c98847f836957964118a811.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c6a3206f0c5cf1c717e079c04f550e1149ad633e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d688758ebf840e18dfdfc574b16a93511a1efd53.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 5e8c65095abb87191f724fab3c2d77432b651072.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 9519ac3af017c87f9d0bd7fa25a3698314f46fed.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 31dcb5f3b7155c4206c66ab1e20c65b734f19a08.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 7e80024724d170ea9266e78cb9ca5dca057babe9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 874bd40a97542b1a1ccd72607d4522bbcabc3a00.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 4bc3affe9866fd127bda1e1110616a1cc1ba71db.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 78c85bd97808ba3b62e4cbca3e5cf7145b12ddd1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 66d3a36704fa8c585403aacb2db351430ab64b01.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 02c22af7f2559d9dea284252726f421e0f50dcbc.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 6f0c6d366760a7ea5351d2caeb06c07d58b4ee84.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 0fd7b63873fa9bf040d56f9ec4f2da70dad459b7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 0fd7b63873fa9bf040d56f9ec4f2da70dad459b7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b1c4e6644f9d07d2faf9fb41edbd12deab4be245.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 6a6abc19f4cc5bcedadab1c20d7d5215c513ba9d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 489dbfdc8848eaa038029c56e36e6e1d31f4ace5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c45d68a906adbd1de4e9ea20afdde3260f0a0c28.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit dee717ab9db00570c4f7cd233528f25109d6ca24.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 6e27fb58f1dc03cce57636c1895f4d4e37c87df7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3f8fe2933db34c1f0b63ea0331c3bb816b30fa0c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 5a2c760306fe8e66562094aceaca8b9923ecc08d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 5a2c760306fe8e66562094aceaca8b9923ecc08d.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 3265f155250eb98f24acbca43be09ba4bf139bc7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit fb8efe0cf6a30907ade164c22153546fb2db87e9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit fb8efe0cf6a30907ade164c22153546fb2db87e9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit fb8efe0cf6a30907ade164c22153546fb2db87e9.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 1dc3fce98a17f7c44d5422eca164e6d0d4816768.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit d79f3a00bfefbcbc493bdc73606eeaaacb0e6e15.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 104d266a0299658c34f87b4bee52c5bd85a40a30.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit b3ca84247f53997666a246267f043e53975ef3eb.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 854cb7a9fbe75b69482dbf99b69b974e16cfdd69.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d306c5a918cd2edf2cf25f6360af0162ee1dd675.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ceda10eac2feed76169a533b12a59386439af0b5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ceda10eac2feed76169a533b12a59386439af0b5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c5657fe44d5b47bdaefdf59ccc20af498023c12c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c2f5c07ee7725596f94539f1190c506d927b8ba8.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 510feb2d3edca3dd72da1948b072f6c95daa389a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit d8abb1de4716ab96b2b8f436ea6837f74574970b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ddeb4b60a1392264497ee9db19b1810a42467b2f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ca59716d4055304615aa761780e5e57290e9d1d4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3134788291ec45bbafd22c0c0bc5f30b13e71e06.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit afc94d84d83a9b76f70f50acec4f2c55f8a9d2b8.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 6637a36a401e3e8d1933267a44a749da9dc307fb.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e4686f1233f391c893f09c59ebe67339611dc4cd.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit cf8f46f58d3247dbbcf0e177840c46ca50b3928e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b34dce3bfaae4db83cf05cc55d25d3bb51eca7ec.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 20dca22dedb115dac8395ac3240c380ceefa92f4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a5b0fc298d023788b81b5aa8617c880924de9f28.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 84738b9226c36bbc301898855cf574d13839dfe6.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 1ccd1da44be1969332980b9945a60b4ec9531fac.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit fd2efdd014a4ffb37fdcea19326b97767298a194.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 683fdfa848d21ab4627df01d4063458d1caf5be6.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 8d017e4459f33032a16a79b540ce0d620f306b52.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c35ede743fc16957e641bd02a606cd09f3775fd5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a32a138c9ea7712413b4e1bcdc70fdf4f4c4eff4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 0bc286dc93a80f5430e39f1825befc02f40fcb75.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit e726534f1f36da6c0de7d60359dbf2ce37a46787.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit e726534f1f36da6c0de7d60359dbf2ce37a46787.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e726534f1f36da6c0de7d60359dbf2ce37a46787.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit e726534f1f36da6c0de7d60359dbf2ce37a46787.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 80e0c6405ca9e9a366b38c7347892fdc1b0d0a7d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d36991fb428c2abe8d10211fdb81f9c7a8c212a2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 427853f349dfd3acbc0b0b575b81c83f934af54b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 86f90278c18e0914b01150896889920124115c46.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit dd46a5b58cdb83bfd8b04c18ec0f2961ddcf4583.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit dd46a5b58cdb83bfd8b04c18ec0f2961ddcf4583.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 110540b4fee44da8dde6b5f3815f9e10d6687842.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 110540b4fee44da8dde6b5f3815f9e10d6687842.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e009e64663841665b83a75552d22646f11b701f5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit a5decfeaad26fcf72ce7254e4516ef18b21eacb5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c90da02f886300e9707e25a94a0bc48aa432b3c0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit a2e2b6bb8712567215bc4c47a36701d915410224.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 3da8f7991e9c615df358c6ce2151af66a2cae1ba.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ebf676ab63c8ca346cc195587606d9e4fc6a629f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 48d9eb390d31dd96281c6d61027c2f3ddbb26109.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e5a64078ca7199ff0dfa7bb5422e0421fbd2b87b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 07136d4e903e12de2ebabc09e21edd09d041b7c9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 746906d4f98dbc3262a890bfcfca990302b6fbb9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 5cef6a09279f33daf39d39c529018519ac5091ed.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f9a92dc0570df94b0a2f97c24ea3e783a7104d01.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 765e0f593b2fb4910a58f7e258a5ed715b3bd0b0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit aca00ad8676cfb11e8f106ba7ea9aafdba5c0a0a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5b0fe14af1fa39412f52ce0bc126d403e519c04d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b59bb989f64c8bea5151953eae19d725de3178a7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 1852707a768ed890b73964ea4779d0b99be82a2e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 43d78baf1dbd295c5d89398570dccd3d25fac86f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit a85bb572c1e038484c0d43bf14da4768620fe082.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 45f8e536cd785547dca8c59c6021908ea00b30e6.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 23ee92888cf58d43f411b66fe634eac3184624d1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit cbb6ffc825aad163aae84b6e738355d4288219e8.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 0848737ad1902da67fc6d219b84e966356c80e20.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 03f19d35765db09768d623258e738b643f45f498.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ed0ae908a2a833ab0e2a3deb33f2994b82e3fd73.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 18a2abc8e5ccea1edd6891a72cf8cd0aa96abd8e.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit fc8cc89121485d960d3952dafca609c32b85ca17.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c562c1a8ed8c7a4db8a1183cfc62a6131a75beb4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 2defa0bddb44639701f8d4d50c6af56d4d2d651d.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 6d7106cfabf583b82f8ca821bc4d1dd3900a9bde.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 5154afb171b8849310a7140c91f75a44232c6062.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 39d5f8ea628f41c3799c8861efa9285e4b15f100.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 49906a77fc8a17e3484ca25eb2bf0c2341389f35.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 868f1c0731c4c6c021ae349b886af66c9daf1513.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c99b6420e92c1060eaa1d282415b290f09ca0bbb.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 066bff914e4b88f5c506922b6358ed202e414745.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ceb4c8ed06102bd0ea50244da2b0ae0292b881fb.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5e04a224aea4b7e4fb4b65e3061ed2edac7c7119.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4acde1f94f352a78631e335c4d0fa79f534426cf.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 5b3a4f50700ae86d62e6ea06b27c7a170b2fc5ef.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit fd90ce6e712523f3ec693e42237c9b545fe34a47.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 56451de99151be0d000504df1b0cae1877b23ca7.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 775aca578beb294c8b39d95cd9c7322b1a2aac29.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit decac1ac1502f4a4971d1d4eb708f802a3afa1e2.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 6556ce20a410c7b773b3546a091459aab790e1c3.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test in $JOB_NAME? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nIf this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit b258214cdb2fe861d70d00fec2c4836befa04cc9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 330e04a102b88ecdaafbf0d4befdc3d8f3fd808c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4886fb51b28575d83f57c8afcf9c89fa469ac5d7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f2e6a33a46cf31ed3dfbb9fb96f21414ee18307f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5838ee218325a72943b2ff990e2dba397db0ce67.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit aa781910b183ac405b17d99d210ae0aa4c3b62ca.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 12b5d62f233528d13ba84f20d7d9287a269b44eb.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 12b5d62f233528d13ba84f20d7d9287a269b44eb.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 79e6de89435351c3aee20b8cbf66c272c30f999a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit aad10a9a0278a86e15808caf75353c600fe710af.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit efb3f8361491c7f763c5e6895204bc92a08ed52b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit efb3f8361491c7f763c5e6895204bc92a08ed52b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit efb3f8361491c7f763c5e6895204bc92a08ed52b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 8de17f99ca71337cbad41a9ef761569862fb3b52.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 4d0f8fafbe38aaca5ff05d65e61e9b4708c741df.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ee52fdf4d14fb0b63631449e1bdf091ec4c238aa.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ee52fdf4d14fb0b63631449e1bdf091ec4c238aa.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ee52fdf4d14fb0b63631449e1bdf091ec4c238aa.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ee52fdf4d14fb0b63631449e1bdf091ec4c238aa.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 82a5eb66e355a06dc50fdcd1a51e09c0eb42c469.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit dcb6156a31fee24e38d25bd2de7f0702d38d3620.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 6a1169e91aa632abed290e266425e7a44dd7c7b2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 8bd34a94a1f0c8176b1bfff1d788413322be7a74.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit cd38796374ab65313d38e18db782f77d73be8c9a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 6eacc4c3c016157cb10d9daf9eaaf5d976fe0c75.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 4a8ce101e3f94f6a75a350f152d96838f1bf3749.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 4a8ce101e3f94f6a75a350f152d96838f1bf3749.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 24bf8dc5430af8f153995b07e22191e2a1dbd0a5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 24bf8dc5430af8f153995b07e22191e2a1dbd0a5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit e3466e088a0729089a7f6f0d2979be14c1f08e6e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e2717d8bb7d095702b5f7469a5f340fb1497a7dd.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 1ce480933a44421cd3b8b50832600ebd5d0b2150.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 1ce480933a44421cd3b8b50832600ebd5d0b2150.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 75ef2ec67f9a55521ffbf7140b891c3641fed3b8.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 10166c149138b5dde1529757adcc04a4851498d0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 1ae7db7acce14b8709a0016ccf0623d36ec75874.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 1ae7db7acce14b8709a0016ccf0623d36ec75874.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit dbb3ed8d5ce51bf34005620259c817205eb38e47.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 74c995763c43508e5225d7780859660d253a9485.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit f34e9ffdc9ebd3b0dcf6f909a76c82a930f8d4c6.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 32c0c3034b3ed7a36a2f2d03b26b409309af9a7a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 490b53e8ffe5331ee94100d9ac33b196b85d86c0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit dbb3ed8d5ce51bf34005620259c817205eb38e47.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit dbb3ed8d5ce51bf34005620259c817205eb38e47.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit dbb3ed8d5ce51bf34005620259c817205eb38e47.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit dbb3ed8d5ce51bf34005620259c817205eb38e47.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ad81dcc94662a51f614d51b8bdd2b01f9667097f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 098e0f18fc2a39c22d7e65676c7d33bf8d4537e5.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? (reply \"ok to test\", or if you trust the user, reply \"add to whitelist\")\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit b52cf9f5dbf8abb727bd6953a88ccd04959540b4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit b52cf9f5dbf8abb727bd6953a88ccd04959540b4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b52cf9f5dbf8abb727bd6953a88ccd04959540b4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 9a7b93c1219b56db6894e13ed603fec5a14140c4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4b279bb4bae981da003731c119c26a2bd3e831fa.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 58768716e5384aae226909b5120cc3a09050c28a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 62c4d547bc0359c54822167aea8e2ef04dcc8e39.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 12f3849d4d3171e29f692e7d99eed31af718c02f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 12f3849d4d3171e29f692e7d99eed31af718c02f.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 002d8aae35acb23a69ddca2c1eedc29c75c36e2f.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 002d8aae35acb23a69ddca2c1eedc29c75c36e2f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 3c70a017dbcd67cfe2f052c5215167fce697311f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 2e1f0e2a08c4a1e31583f6375022114220b5b702.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e3d66bff1e9d484b4789d4ec13bf03fea73cc256.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit dc5be4111a29c3358e61c3a5cb86455657dca298.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d9c864324b03ef9d62620990d267df00039d77d3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 71ec26b795d5b04772363d5715a936f0d00b36ca.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 71ec26b795d5b04772363d5715a936f0d00b36ca.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit b90878fcb1a510335af40682344b312b1538eae4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b90878fcb1a510335af40682344b312b1538eae4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 3f9c3e37a324b6ef22cb5a1a5b6c87c950bde20e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 3f9c3e37a324b6ef22cb5a1a5b6c87c950bde20e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit e3e6d9d39e5571c5abeab733de2625fd1df4a284.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 0c8ec62bb63c5b8a4344df5f885bcb97657a501f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 0c8ec62bb63c5b8a4344df5f885bcb97657a501f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 89ca644a42ba1d5e7698e4eb65dd91661b52ba77.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 3afadf953d37a22302646d39809bf71892549c4b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e4473b29c453e16a661eb6501a64907e26400097.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b768a9d1dc4e8339e3591c1041a1a876d880a21a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 1379fc81839722b869e4f715cbe99f7935e74119.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a663e835c43585090981ae82638cd6216c467c2a.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 0d6d6ad6b09ff8e52bb68d87e8a54bef6c989c94.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a2128c6efa580df08ac17af243b2b2cd9978a14e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d434a4e5347f1fb2c5144032e85f5f10a9227e86.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit f984134c77bde6df61048b98d9dce966aa60b513.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 8a5b04c496aff5a6ad394589a5d6cd68207ca8e2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit f181635850ea46d9b5b6de97527bba29208ceb30.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit f09e1337170067fbcedd6ad72fb3ded205564a53.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit e2c4f34b05a58a5239b60f12be484ce0b651f241.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 9ca3db9ad9a30f9cf0fbf0f8c897779fa7aca141.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 4c7d0334d23dd862feb13632b6ce45d0644b48d4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 45e02d725df74bfc6468000f3d00137b79997416.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 76c2f5159a8a44a2c05f969dd8400f5230000ad7.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 76c2f5159a8a44a2c05f969dd8400f5230000ad7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 76c2f5159a8a44a2c05f969dd8400f5230000ad7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 667f9cc7ef32655fce7e75312e8087cd0988ca34.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 6eaa0d046adbd6c067333ceaf84fd04e070ed48c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 647224c95ab4362c9eda5607059060e1acf6d84a.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit ee8701aa974a36de2f392922391d800dc77dc315.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 7b1820b1d446edbb647561d29f4b3091508f96e5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4a8f3e4c9369998c2e95efc47a329fea1dc418f0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit dc6415aef7fc3d46fcdbf5746eee1c868c2853bc.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit dcc52e7ef99ecad4734d58c7db2b0e5bbd7ea429.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c36e967afa5dd3c54001cf13859a6071acca0451.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c25ea2789e948b87da2281403a8494904c7c3260.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 700c3d1d4db2a6c831faabff3e1d15c3a2940afd.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 7f4e9f6ef3dbf8c70bacaf5196b9a9d137ee0cd9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3aae756753a9f896a25b852a0628845e646f487e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 755073cf24e0b3825fa88283e53d05c94ebea01d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ed565061c42e64e30b6e85e3b4a4e218e6d4a6b9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 37a17840c06a2bf741df80e509137b4d325160ac.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 9790a0d3f49a53016092edb601f50205f8733b5f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a89c731930d80918f39bf79f2d27cf2ca4dca900.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 9961e371687b5d44cf885f456a7a176a7ee06fff.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 9961e371687b5d44cf885f456a7a176a7ee06fff.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4d3ef349fb1f64f6c579e29d24138470703d5432.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f52bc3cd72bf14e9775d9640313b7320ee03dd06.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e7399ae692378e59a562f8395320fc478eea621a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 8d0fe308d1a172ad82120592136ca8344dd38f2c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 2a81cfae01cb88f1f3a0e5f0986b49add9f941b1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ebbda7e47f5a5db1763e75623c4f8d87e55c0b91.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ebbda7e47f5a5db1763e75623c4f8d87e55c0b91.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ef83d05aec443a578b09e26d9e200011202cf3fc.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ef83d05aec443a578b09e26d9e200011202cf3fc.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 35f7bc5ee74f45a6e454dc6f68d0ec37c54544ad.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit bb87ca1b709436ac2127aea29a85b310aa5567d1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit bb87ca1b709436ac2127aea29a85b310aa5567d1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 83a6645c7ee76db6bcb55fdab91129e26649a4d6.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 66ad12f307014edf4a8e79bbc17216196fde7d45.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ba12e133dd15bfa6f526cd3586315c87080ec5ab.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 7fc1a906253f727cee5b662c05153929cbfc8b43.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 7fc1a906253f727cee5b662c05153929cbfc8b43.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 513bf7acbe6c7ba712934c490ecf709c73146045.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 513bf7acbe6c7ba712934c490ecf709c73146045.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5474c1e505e228c3b8d8944914fb50263cdd9408.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit d68dd18694874a3e830022d7d874659c64a6a135.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d68dd18694874a3e830022d7d874659c64a6a135.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit d12f1e3608d39bff05edb8a3a95557ac8f687df1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 1e83e063172ae248d7c2b3c18f08701ed2644025.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit fcf54310cd5e282fe101bc03f27e68c1483ecc67.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c16cefb9763f5815e415323e4d1465c033863b4d.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit d01934a3e409b2fde078a62ee317c3b8b3250d25.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 61ee971bb362ebd4a78f6768f577da1daa0d8cd7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit e9a3c466d92f83295fff0864f388d24f9c4d99b3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 73fbb57cd651950830db8b02035fa1ea7b6d808f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c7e2e113a9084142964556bd76b43ffb64b9e0e9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 48c87a61cd75d7105225a2271c0ae94fc154fc4a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 5b2bbd1a582c1c934c5f7ce435862e8df10057ff.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c339a0c7b2dee424f55800c6366cb06af5ba9292.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c339a0c7b2dee424f55800c6366cb06af5ba9292.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit c339a0c7b2dee424f55800c6366cb06af5ba9292.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 20f40eb279fc0d7db6a4ecc6421fa617b35d7567.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 82c690c38034fc4c4ef266e2041b88920c67c8aa.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f2beaa5ae1c82031e494615e0af22bd14790377b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 0973c854d6d8ed68426bd609cf2b6eb12b79b5ef.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 0973c854d6d8ed68426bd609cf2b6eb12b79b5ef.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 0973c854d6d8ed68426bd609cf2b6eb12b79b5ef.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 0d4e44fa3444f9ee86292a8d4c1bb8155626224d.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 3adf0ae8432123130faab8be5e1357b58f220540.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 638e8cff9b44e7536c5e1c9209b86d853f5d8e46.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 187ec58441bc7c1a518d9c9200238b90c9bb5831.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e5840360a6a7d789bf90ea12cea40d0bc9ad2424.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b7faf83410b23bdbfc37c5f2c6071e42dcbc2111.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 350842ec585ae544a4669e3d7184c5667ab88286.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 350842ec585ae544a4669e3d7184c5667ab88286.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 8204fcd8be134320c351472f46c7476e9117200f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 97ac208588777d15872e39ffe615760b853c91c4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c262eb7a245ae17a119e853cdcafaaed1eb7a8b8.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5d1057c79d5f3fd5a1faf5a81756448b03c44025.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3691e41fecf9a125de9cf3462876b04198f400a8.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 1d4c0fc398378c766222f522f2786540f1990eb9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 1d4c0fc398378c766222f522f2786540f1990eb9.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit f031e90bc463d216dcc4b683946b26d48757ccb3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5fa60e50196b0d30cf8a7fd96c7d36bd9307fef2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b63501865521a59f35ff98e471577016292c02e8.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 7fd7f207174f340ece77c24c9ea51aa5e9762a1f.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 7fd7f207174f340ece77c24c9ea51aa5e9762a1f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 6c25ee3df6cd56fa00f425d5c9fcc4d83a524f9c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 6c25ee3df6cd56fa00f425d5c9fcc4d83a524f9c.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in hack/jenkins/job-configs/kubernetes-jenkins-pull/ instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit f695b7cfc8b57a1a52894cb3afddebdb6ae4ea74.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f01900d2ca3321140adb3029816e630535083210.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c8d9cc16dcdf9a4d39bf9ce1fb2b290dee6929d3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5ca11bb1a07589692f0d7222e68471814de511da.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d624bcde0cd5ec5b075b9aac36418dbb8db5c9c4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit e7322cad30f87c252e06301346c9d3f698a820dc.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e1b0275516a7bc6df242ab3acf6c51b5e7033cd8.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit bf746e73f613378359e6ffce97dc115078dc96f2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit fe2b3177da3f513a5daaf305ed09c769a7a3e335.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit be19bcf0fca56d9a18ace24aeab214d48cc6ba7b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5e1622bdc3ecc81617182a1af9412fd6ab42505a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 76a5ae50aa5826a5403997a0bd28da7e8c0d913f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 76a5ae50aa5826a5403997a0bd28da7e8c0d913f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 634421a1171c266731f66b52c333212d425a46a7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 8a446b883c9e8df1f52d9a8a1c10edebac78c12b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 436db3074fc7d6ffd62a0f125c6e9bf799d543e4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 918145ceeac8042d07d13cfce71cc3a5dd4b458b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 1e407cb85f7e9a8ea71edc06711a02e44289ec34.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 060c14de36a2f3bbdebefb73d34c8c745af0bf05.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 49a0b9368b986f7f74bf7d93ca396be317bdc4c9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 49a0b9368b986f7f74bf7d93ca396be317bdc4c9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c61c7118eeca3bfd5d5f67d03ed8f61d75ea523e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 05a9b07df2375d00847efeb4d73d5a57ed1da9a4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit bb10c3805c96b94c68d6dbb8654c3c25a6774889.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 1382470dcec3d9265abd1327f8fbeaa2eedfd18f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 55b640f8e6a44db1e28345f9a0bb04c0905702d8.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 522895c7a1fef003956f815266e52b585238b686.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 9159322c3b24e3673bfaa5c29ac57316369d107d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 677008195b5c5e84f72c17f6172ced8ce62ca79b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ac0e4036086a92dafabe0d1f793af11ee3932a02.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e02632463b5c62da8efaf421cee2c8ce64e1734a.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit e8eb9caf677771a9f5a266cb1a13c689ce20f29f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a05813213d4121e4cb177b4690040907940c0909.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 559dce29d3c601e6a1343903f3de93393e93e3e3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 8d80aaca81434fbf0804c6923f451df1869cb9c3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4790ea203bd919f9f3da0881502b9738264c67f0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a92834c8af2d3dc39c88f3ce896dc2eb54f8f549.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4c3454ad6f68e8dbacb6c1bea041482472ab9a05.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ee159390b7aae6f014e3f0fe320ec56304169a57.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 95f696fe1c6d55f2c3f9979862619eb061f7a07c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit aa46bd4989c22f2f45d729f77d4d95c54048bac0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e14c1b47666968f8b7b7de5a19801ca191a865cb.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 0aae2f5de5bfbe3d6d7d265aad2514a1cddb34aa.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 44795d7ce0ec9474f08ad558bcb6fb5d5832d576.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit d1b7fd990c0193ec14874b87503365b0485d7a91.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 99d83017486b60a5d7ce70d505a94b9230d99ef3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 221b89f605ced23acf7281c2d54439b479c3a5d1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 438a1c9377fc56e58d48fd9202b3c0532fa04be1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f7178d2a315925358189759a4ea446d20fb44461.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 58b26c9d2a6abf313595ac3bf30e1901e1b361fa.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 49290b02540804381cc10ed9069238f8820a98cd.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 6e09a8bb4b3cbd933e8a0276bbefaec7d821c161.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ea28c02d2784c83eea454cb82689d9ef24aba9d6.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit e8cecb36b7d48c05149d68710f42402cc9836429.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5c54f6199f97fb1ad32a46f84c441bf92f06d523.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit cbcf95b0bc1b9e102c94e5216d6acd22ea1cc4bc.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 019bfaecc7f557b6b2fc60c524608dabc45ca864.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ae4e0eb3cfd9b9d474bed8d30f48a51db6df227f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d1868287dbb5eb74c84ef82146ae890dae47ace3.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 438cebc941de3773818dda859c94392b0d38ad7a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e1af50bb63bb9fefcdbc7c2902a59a3b5cc07b50.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 176f8e23e1c405959bf1b2c8ae6d090d78e57206.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a278ad1d45bd321d8a79f8af41ec78b0fb2a58a9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a278ad1d45bd321d8a79f8af41ec78b0fb2a58a9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f882aa450ee66f5336656471ff10f12ab0448884.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 0e0ed319d9656fc70b3c3ba1d18e4a89a7f4bce2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ccd0c72c08bd79481935eda1fe5956d9c985ceb5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 0dbdd7eca8562e07ab09f543721dbeaadf71dd46.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 0a6ad9a750025a3f33de5e95ba600fb30514dae3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 06bb41728ccc0c9d93ac47627bd1d71ff83fc6cf.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 3659364754084e2035b16a23477f3d8353cb9315.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 29e0782cedec140df582c9422f4414e4cb3d611d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 299ccc1cabdc1dd1766b3afb5daee6462995cc86.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 545b23a5d855dd574bea99ae2a0fed6e46bbdeeb.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 545b23a5d855dd574bea99ae2a0fed6e46bbdeeb.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit e2934ac1508ff5a8e51d0fb420b4e4a5343c4531.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit d3ab9020cc485cff7a33f0ba33243cbe4f9b2ed4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit d9e08835186d8ed7e2f2979a86352a35107b0cbb.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit a7726d4e5978a054ee419c5619eb24bbbd6413a2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit a7726d4e5978a054ee419c5619eb24bbbd6413a2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a7726d4e5978a054ee419c5619eb24bbbd6413a2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 98957019190b60297d69544e7dd402f54933ee81.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit bac24618661d06ccae6066c159832c1ea63344c9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit bac24618661d06ccae6066c159832c1ea63344c9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 39fe454f3224a019489ce14d0f7a69839e3b71e7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 39fe454f3224a019489ce14d0f7a69839e3b71e7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 2b40c6f849a99465679f4e0dc09970bfce94a086.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 9534d8d206628636223a01aec4006902f1475231.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 42dda5ef646663e3337ab3f6ec5d6d0d6ae01334.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit ee24f28f737c67f593172a9acc19f29c52e32a70.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 2bafa62f8734dd7565db79f7cb73996115190c42.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 0ff6ed0ddd8fe6050e729fb87cbd07a87f507860.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 2f0af7bb40f3ad9655267e8abf689665f324f57c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 81786ec1d270aaf0142a9c28f798ce9562000803.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 7f6127620a99e766cd6e151a95310e2217b7c19f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 732677b6774035c89d1e3b562b7b6c6d8f7f28c0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e6f2951381548eef13aa44b845d602c834514a12.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b2509bbf14369cdac9668f540c6cf25d85792f57.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f02ec8a967a21103a75969b17cf850127ea072c4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f796871304a6ea81896b3b87001464161b49d562.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit afefa6274402aa8ea6f7e31f4fd16ebb9f8acfd0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e1c182f9045dabcc3af1d5a51e61bd413b1dc7e3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit be2f3c25e54be359c083bcba17f8482acc25610a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d93237b51d573c1560a6e33ecffb996f7b0a1338.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 86463bb0aaa0f56dd1e93b17a43e8505ceb4b631.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 86463bb0aaa0f56dd1e93b17a43e8505ceb4b631.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c57d5a8ac9c3f4c0f3e4a8be8349f689a786ac7a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 8552765b5a9189bb6569069ccf0bb2fa13c302b1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 9b3f6eb9ea03b630d069712ab19e3fee599f9a1c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit f93d2bc8d82303aa4c2246fab378e92460dfc86b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 62d5c1892e5660c4562ee734f0d056d934cb526a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d29a25e43257f72895972166daca864f99d3f2e5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 92fe527f85350b60aa0f80c04c11e39df986f313.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit a416e49d42c4de2805546cf0ddfa2a358f30e0c3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit f39d8906ac3c2e1fbbab6b4863291036d0d2dd23.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3393e852c469bb88dc8f605c335f6591bd797446.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 73bbb6ae0d51b4362a2ab30fbb7df1568486c030.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 8215a803c78e5ee74b1b5e62d1daf4721d77012b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 120f58e0be7f7b968498eb6a4b3b8bf74795828d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c45e6e0006177030358ed153bfedbb09b26d3fc6.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c27f7c054e5489fbf4b5556b6bd69a527bbdcaf4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4cef2147d086dd8189fe66908bd1aadcc56bf87d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 03e812c001653006a2e798f4f8e8616ac7f206a5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 08f89325d7b7d5f80cb16cbd74b2e9b8a9bf5c51.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit efcc9c4015aa60cc4be5507bde784dbbb5d6c380.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit db77150b9e64ded85d96d7606713212f506ff485.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f406dd0782995dbd849c84dba9cff0ef2af5252b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 9ff572d8b366aca01e6ebe477933f8b91dc2bd16.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit be6e25111fa9cf430183491eaf5370e7e9f02582.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3ce98a46c47066eb02e29f93206d29c00945db48.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit eb9f614cf6be23b7a90c349ffe33fdbf6c165dcf.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e8e0b8501fd16fce82e05bc0924e2bbe0df4ee2c.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit fff0b04277c552f8d4b1b40090f543e9d5cf283e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 6da4202b285c5acd003d0082b9df85a506b08f3a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a87e2529ef168685f3b4a1252f9358d966eb2c05.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 960df35f4ebc37f4e545cc95c3756ca93d774cda.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 7a7ae080fd020510228f5606fa1c8144f5fa0485.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 20267a629df928ffdc20397ac633cf578defc660.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 32acff7789f2e6a617271f122df43202898d7e20.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 32acff7789f2e6a617271f122df43202898d7e20.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c37d048e1bc8f9f0195f49c35bcd7d51e28732a1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c37d048e1bc8f9f0195f49c35bcd7d51e28732a1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c37d048e1bc8f9f0195f49c35bcd7d51e28732a1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c37d048e1bc8f9f0195f49c35bcd7d51e28732a1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c37d048e1bc8f9f0195f49c35bcd7d51e28732a1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c37d048e1bc8f9f0195f49c35bcd7d51e28732a1.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 9f2526b46d53a71c510f49da33b10df894e79d91.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 6ea1955396aba7209b86ded8f03b2115b74f17d1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 148bc71924481d859b525a62765ad06944aae73d.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 8c362dc0dc9bcadb4d2caacb31543d698c23df8d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit fa873f0e7bcec7ee30194c841ec9026fb6fdb6e4.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 464b7796b75ad933b28fe42c0c3c5cf5b063b9c1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 336892707db74d4a27f4d63d26337f5672cb1121.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 35d9a34d6120272d92ba5b1ca63826a8021e862a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5a0caa3228c3621c60c7fb09f303fa8473e3c5ed.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 82bdaa086ea923e151e02c90bb9e049cb77b5f98.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 7ba1f7e60fc2088506b8e7703d169ec3128f3e7c.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 9f4fba10665cfa77886ee6d199e650710e0ee7b8.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit eafff745a505c320b37bafa56976dfff965bc6b1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit eafff745a505c320b37bafa56976dfff965bc6b1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit eafff745a505c320b37bafa56976dfff965bc6b1.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit c4173e886f444a55c5f33cf0fbe79d5e1778637d.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 24e872fa733fe4eded6efd0c350cfb8a3140ace9.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 5ce38c15daaf31238e6cc6659c986d0602cde5af.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 46242463893772aaf88aa5530fdd90e3fab71281.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit cf548f1b70cd95321d5cde118e2c5b9f729423a7.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 8faba9efbd4dcd245225157a10cf2b8bcf565c56.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4b581fc3a55d9695f29d3f9ada67114092ba3b01.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f01486033289ee0868f7b35772abbc852d943243.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 95ad7c6628c15fd40aafdc28f97e21f062b267c1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b192cbbeb0016bef5ca0c78fb2f4040ec1450058.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit edfe90fff2e3c45d41a2fa6b2de79b834d91b2b0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 7c62ac39d2a5721df698d8b78d36acc5898a1e55.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 213c254bb7907f2b40129a9b9aaf6241a9254f0a.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 3647ad0da1d4e8b38f91367d7c8c46f04d9fcdbe.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 372b96f6ac0a83429f99a2fd6bb9eec2ac6ed79c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f24db4324c31b102435778de707ea98f9475908c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 29ffb3b6b9d7bd365e48387dc3b39249963a3b4b.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 3b8566c072c31aafdef4029757487c0441a095ad.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 0522a747ccf24f950270278773b7cf2bf2fa2ce5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b7883a84a71ca11a7d16dad1f18f746c75181a5b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 9281aa2fabfddbef650ec04f7922d020506e7d0d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 306854a42ddee4bd9ebf5e24e0cba4f593ec2f53.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 5dbf9558a3f0ab0e7139c1d7595bd479bcf4ac4c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 5dbf9558a3f0ab0e7139c1d7595bd479bcf4ac4c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5dbf9558a3f0ab0e7139c1d7595bd479bcf4ac4c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4b80217dcc1382ae4ecf54cc937865f61c23ae78.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit b80f17ffa4b5e0be91b51926b196451ddaac1aa9.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a8c592e6011cf9a5548de4ea0af226a431c447e8.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 13f1b2f749656d3713ca0b7f2d7216fc059d4730.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 46c41743878b759623c02a508c7c1439aa2b3307.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 1e7b128713983369f1033056b4ed6086cd9caf0b.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d05098b314886bbc25a64ad6587fad54f5fefff8.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 503395e4d83b5b160d66cb5f2aab1721c92da862.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 3a8f35aba5a7cbd6d44d2de0fccec96c6ae12d62.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 3a8f35aba5a7cbd6d44d2de0fccec96c6ae12d62.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 08b66e4a9ad981a15e00d627667ca171cb0544ac.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c3fb95afe56d626e967b8f8b215abcb48776fabb.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 2c69c0b634d9c34bf9eba85bbb9b08fa354549b7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 05d2cbd2343c1cfa1c0b1870f7f7d9d6d87cffaa.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 7708bb5f9ef419aab7dd71b1c940c66eae5d21f1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 0bd660a23e3222b79b5cfa6a698ea5938101e264.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 6c114be580f106d2844277dc0a8eca066903b022.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a73d92ca85338bf3fb3b22c9106bb2a655bb6120.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5825a2e945cf5a42388d83af9c6412237644e838.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 29f712aad501fa0c25649804bde49593e62e4075.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 565007eb62f5ccb1d0548474c725b7830a026ac0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5f2f335a871328ac2fe0826713bb098013f94207.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit cc54a8623f8fa9be6e78dcc241e072f5a4fbdb28.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 743a7022f4898efa0ff051ac2fd08ea566ec1248.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ad6a348acfda73c3f785178262f15a28122a151e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 091b2eb46cf417659a8705cf986a58017751379a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 8d88bf0891650174370ea59f59c8fa55f075873c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ef9c2d0947f48ff4ba54d698afb6276450b87cdb.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit eb179b88be94a6c53eea2adfd36d127b8146b857.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit cccf9d5feca1cc6d6635da6d9ab9913c6bf7e7bf.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit d2dba37eb8cc7f33fbc36effef0b813318f8e2ea.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c1f3340830bb4fc5f65f01ebc6429e1d2f4be40c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 60e569b529175058fc98cbdcdc6b3769c8e7ca36.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 2eecea13536a500b966b290f15b15fd9f908f034.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c39a6455eed5ac09a9672597c4d32a33797479df.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit c39a6455eed5ac09a9672597c4d32a33797479df.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 3c4e0626e9d0896f300662e9f56c013b83ed6cdf.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit aa229653bdae6704254362a0fe199b0b5337f0c1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ee91400bc07bd6e228ed041f92b939aed86a6966.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 096842ccb1a33aaaa57db4ebc06f21d2902cfac4.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit b8c19739dcac014c4566a7729363dd40ccbdba44.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 6ef612f21e042dcbec189ab253741f94e47804d7.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit ff55a85df3449109f91c2864f5e94f646308e7a4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 3628cc95e4be85c5266e6e642161391eafa09070.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 28a49634d8f00629da54a7bff17d87aabb6a9856.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit aaa2d51e207f7a379cb1d760a30daaa2dfe55d79.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ad6d1b8070214373ae68838c238d7f71d2b3a4d0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b8b541d86a9da37b2c0a37660ca984c29f8be437.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 307d1b1cb320fef66fab02db749f07a459245451.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 9e6289a41d61923e34158bb064f166fdccf6884c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 41e74494b394d70e646a5e3c34b4589beb0fa413.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit bc757bc9df08b751080d9a54be9b309624b5d067.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit bc757bc9df08b751080d9a54be9b309624b5d067.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit bc757bc9df08b751080d9a54be9b309624b5d067.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 65d0763619a881a4ee6532b77b2fba2fa596174c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 1ac09cb75b6e6f0223ec40441be29a94417dc902.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a2a863fb74bf43730bef10c51567d47077c58889.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c9170005f5dcebec8cb52c1c40163b4ff98611be.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 947f713eb0506cae992a4fcbb71a065332211b31.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 32e4393953b396799bb70fb767c42fb245e21bc7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 1653733ea7f6357280aa7d34723c6095f2c9eb55.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 2b845f561d63f900ced1fec742000d78c02b2bc2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 1bdd7e9d615cd2a367c16f1d69642f619a753df3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4c0d60d9de78f4ebec8cb29034053ca65516d244.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit ee58aa7ab2bf2656fbb513db56e21974960ca000.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d4ce3dccdd7b0da822910a4030da4861e39c5678.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit cc75e02ee9df09270c7b10f02f65721f0f23de13.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b3514115101a1746badd6230a07039336121a7a2.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit c869c318f96f6fde5b4e8e2e5b444aee51c9efdb.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f8f474e7915ff14c767bc1deb37a6f805c8c3469.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e3625ca1cdfe4556b97e13e4fda1ab5db1ca6ac1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5d38822b8618e2e71291f0b1196e0cb59e8f3ef9.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit ebafc88532c6741b4848f076ec559c1ec7656442.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e459b6b4292c740f416ca509ac9e8e4df70f6688.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 8551b0ebf752328ad1b9abcf02e588b7886ed523.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit b080bde6944066e7b9b70e3334bd0c27f72bb835.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d8b0b802edbf171b3b24bd400f70097f91532590.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit fff7526aa6b90b6c3e634c7081979a473d6098f0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 74cbe58f91fdf2861ac4e28cc8e38ed456e217e7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 7eac6dc0c33c108999b37843a3a74fab5dad8f91.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f1d8b2da7017372dc3e011748a2a2d33843744a6.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 64882357a7695e1c2b9cd8fc9314e3ab8f74f6ec.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 7ba480c9ee81a5dc14bd80c67e10b8a8e3448272.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4530a1c7c612b383a29389f409436aa0c6137051.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d1e20410961d80cf2bde3a8a68c6249cf7c06c95.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 5bb3c312c4a226ef659c71db0c107faca9233d08.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 1bd001025dc9da3058f29e0b4ca4b631ea06ca33.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 2b525ff87ef562cc8b5ce12b3c81ee3d3fae38d0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 7cd592869eec4a2e2f58cc01c2859485a76749f2.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 27adc3335899416368e3934c865ab50fe5b8f720.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 27adc3335899416368e3934c865ab50fe5b8f720.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 27adc3335899416368e3934c865ab50fe5b8f720.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 27adc3335899416368e3934c865ab50fe5b8f720.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 2128bb139b7fc3694d18642e663e2ec4861c07e2.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit 844c98e073f2c5cfbd0c47c7a6690e282149ec8d.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test passed for commit d7b456556f2be014e70092511d6ee421e7ef251e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f316d1b01fd2d18e8a81ad5f9f6a662daa894ba5.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Jenkins GCE e2e\nBuild/test failed for commit 303e4ac039cbf0a4aaf1e1773e77cd753d4c3e18.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 303e4ac039cbf0a4aaf1e1773e77cd753d4c3e18.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 303e4ac039cbf0a4aaf1e1773e77cd753d4c3e18.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e76096d4f6387397a76cc34416ff75d8479053ef.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\nOtherwise, if this message is too spammy, please complain to ixdy.\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message will repeat several times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\n. Jenkins GCE e2e\nBuild/test passed for commit d84e0758ab16ee68598702793119c9a7370c1522.\n- Build Log\n. Can one of the admins verify that this patch is reasonable to test? If so, please reply \"ok to test\".\n(Note: \"add to whitelist\" is no longer supported. Please update configurations in kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull instead.)\nThis message will repeat several times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry.\n. Jenkins GCE e2e\nBuild/test passed for commit 7acd82ff27d5c5675972f845ba6aebeeaa980fbc.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 7ae23841d61198dab127b3a65b1b1f98014533cd.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit dd6cc41e003c10c1498bafb3c2ac57d5032fe609.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a9bb292b5291bc02e0f08e6554165f125ddf679d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 27b9eed7388c7627f53143e9be16112488dffa28.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 24338af66b265aa7eeb0f8039fef1223a2c51dd4.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 96d7e923ee82d84caee5ddba05b8a122bd236059.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 7585d75a51257f2b229e81e41913cfcab80ddc68.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit a5f107da06bfded6b76d1404edfd5cf578f69c3f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e7576e5790f78c70b996e437991c8462d01a44f0.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d30400f261052e1f674901604e87386708a44605.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 2720a4139dee90e8182d666948a58e3c4507df82.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c38032ca6771c819748817585f8686a5976790d1.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 96b51f3b00dca014212418192ec8cbf4b9a00a1f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit df3c88836935ce93a4dedc83591136452fcd1391.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b9ad949c4663ab713a916cd9f8e3f95ccb4948f8.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4810be603944d325449f7320cbc28eaf11fcfa6e.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 0b4de74d381f273a9f4bdb98a33631936055a834.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ee8d995d70c4d37d9f7aaf63d85882d6d15fd576.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit f66f9b127c13f53a42a7f6953d0d0a011b163459.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 4e993cf5efbf8c52abaad91e2e80b9847d7b4c3d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit bd7ea539bc883bc35e404196da4cd17ead330bf6.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ee53bd3124c7c2c9e5348aa7ceeb1bf2a5d01113.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 8abfbd266bed1ebafbb033f74c00b8d59d5d4228.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit e3f72e9fc20033b351bd16f054da8172c43dc96d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 80e3f7be7f2cdf6e37f0806ce586a8c198bcdeaa.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 80e3f7be7f2cdf6e37f0806ce586a8c198bcdeaa.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d5c95e33d932dc456a78020fa40845f963be78a8.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 078eed599ff5cb8da41bd3d91b7d20e5b38c332c.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 923dbc58c14b6b9438ae88525ec559b87e7d21f3.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 9fb061ccc8c9699da1565a77e9fc938761a0f3da.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit e3ee9a09cf16bb115b3f3dd0b5e6ee1786c7a54d.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit c7acc62989d2b854a6da80e10559e70c21bb22af.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 9d71675c5ea63a91b72528f3b3477a5de05da1a5.\n- Build Log\n. Jenkins GCE e2e\nBuild/test failed for commit 63fffe90c8e037b63160c5aceb118c624d4dc2b7.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 7e255a5516b779648cc0dd5f6b6d74915314667a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit d7a936fcd665453a10275f82f7160da036ba478a.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit b84046f12c826f13388f300d4ff610537349e047.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit ac9388cf76723ef432e655a7f3ac6d67a926f64f.\n- Build Log\n. Jenkins GCE e2e\nBuild/test passed for commit 159f002f9a9bfd3af56070f435a7f4287ff6b24b.\n- Build Log\n. ",
    "timstclair": "This PR looks abandoned. Please reopen it if that's not the case.\n. @a86c6f7964 We want to make cAdvisor more modular and configurable so that most stats can be disabled if desired. I'm planning to send out a design proposal in the next couple weeks.\n. container_cpu_user_seconds_total is total across all cores (based on this field in the API: https://github.com/google/cadvisor/blob/master/info/v1/container.go#L276)\n. Not that I know of, but patches are welcome!\n. I was able to reproduce this locally with docker v1.9.1 and cAdvisor 0.22.0, but only right after starting cAdvisor and only once (removing a second container works). I could not reproduce with docker v1.11.\nIs this consistent with everyone else's experience?\n. Yeah, that was fixed in https://github.com/google/cadvisor/pull/1259, but it's not integrated into any release.\n. Could you add more details about your repro steps? How many containers are you running, with what options? It would help if we could reproduce from a clean VM centos image.\n. No, the issue was https://github.com/google/cadvisor/issues/1461. I'll update the release notes.\n. Sorry no one has taken a look at this. To make reviewing easier, could you please rebase the commits such that one commit contains the godeps changes, and everything else is in the other commit?\n. Completed review pass with comments on https://github.com/google/cadvisor/pull/774/commits/51c6745cf84e8a516365046b1cbf165bf5454d8e\nAlso could you regenerate your godeps with a recent version of godep? It looks like you have a lot of files (i.e. tests) that aren't included anymore.\n. Mostly looks good, a few more nits.\n. @k8s-bot test this\n. This is currently blocked by https://github.com/google/cadvisor/issues/1458. Unfortunately no one on our team has time to work on it right now.. This PR has been labeled inactive due to lack of recent activity. If it remains in this state for more than 2 weeks, it will be closed. If it is still being developed, please provide a status update.\n. Closing as inactive. Please reopen if development resumes.\n. This PR has been labeled inactive due to lack of recent activity. If it remains in this state for more than 2 weeks, it will be closed. If it is still being developed, please provide a status update.\n. Closing as inactive. Please reopen if development resumes.\n. I assume you're asking about the SubcontainersInfo in the client? You can think of it as the recursive version of ContainerInfo. For example calling SubcontainersInfo on the root container, will return info on all containers.\n. Is it possible to exclude directories or files in Godeps? I don't like that we're pulling in that doge.jpg (why is there a dependency on the examples directory anyway?).\n. This was superseded by https://github.com/google/cadvisor/pull/1217 and https://github.com/google/cadvisor/pull/1245. If you think we should be using go-rice instead, let's open a new discussion.\n. Friendly ping? Is anything blocking this? Are we just waiting for the k8s API to get finalized?\n(@vishh I can't assign cadvisor PRs)\n. ok to test\n. I'm closing this for now, in favor of the simplified https://github.com/google/cadvisor/pull/1045\n. I believe so. @pmorie is there anything left?\n. > @timstclair Thanks for a thorough code review - this is making it a much more worthwhile endeavour than it would have otherwise been :)\nNo problem! Thanks for the cleanup!\n. LGTM (please squash)\n\nIntegration tests pass, so it must be good right?\n\nOf course! I always like to have some simple unit tests for regexes, but I won't force you to add them if you aren't up for it...\n\nAs an observation there is a lot we could do to lower resource utilisation. Regexps are hungry & need to be used judiciously. They're overused here where we could probably do some cheaper string parsing.\n\nAgreed. Although I don't think it's worth spending too much time on this without first doing some profiling to identify the places it really matters.\n. LGTM\n. I believe this was fixed by https://github.com/google/cadvisor/pull/1033 (thanks!). Please reopen if that is not correct.\n. This PR has been labeled inactive due to lack of recent activity. If it remains in this state for more than 2 weeks, it will be closed. If it is still being developed, please provide a status update.\n. Sure. I can work on this after k8s 1.2 is cut.\n. Fixed the main issues. I agree that there is a large issue with the documentation and examples as a whole, but I'm going to close this for now.\n. Ping. What's the status of this?\n. Thanks!\n. LGTM. Do you have merge permissions?\n. > Shouldn't we just respect max results if it's specified?\n+1 - I think we should just remove the logic that ignores NumStats when a time range is specified.\n. Why should it be changed? Changing the default is a breaking change, and if anyone needs another value they can just provide it (especially with #1010 )\n. I wasn't able to reproduce your result: https://gist.github.com/timstclair/5a1abd7fdee2ba93e337\nThe original regexp yielded the expected \"/test.slice\". What version of go were you using?\n. I wasn't able to reproduce your result: https://gist.github.com/timstclair/5a1abd7fdee2ba93e337\nThe original regexp yielded the expected \"/test.slice\". What version of go were you using?\n. @k8s-bot ok to test\nLGTM\n. @k8s-bot ok to test\nLGTM\n. @k8s-bot add to whitelist\n. @k8s-bot add to whitelist\n. Did you run godep save? I think you're missing the sarama deps.\n. Did you run godep save? I think you're missing the sarama deps.\n. @k8s-bot ok to test\n. @k8s-bot ok to test\n. Oh yeah, that needs to be fixed.\nFYI: Fix in https://github.com/google/cadvisor/pull/1047\n. Oh yeah, that needs to be fixed.\nFYI: Fix in https://github.com/google/cadvisor/pull/1047\n. It looks like you somehow picked up a bunch of extraneous commits. Can you try rebasing?\n. It looks like you somehow picked up a bunch of extraneous commits. Can you try rebasing?\n. It looks like your commits for this PR are interspersed with merged commits, in which case you might need to manually repair it. I'd suggest trying git rebase -i HEAD~35 and moving all your commits so they are grouped together at the bottom. If that doesn't fix it, try rebasing again using the instructions here: https://github.com/kubernetes/kubernetes/blob/master/docs/devel/development.md#keeping-your-development-fork-in-sync\nEDIT: or just squash your commits into 1, that's ok too.\n. It looks like your commits for this PR are interspersed with merged commits, in which case you might need to manually repair it. I'd suggest trying git rebase -i HEAD~35 and moving all your commits so they are grouped together at the bottom. If that doesn't fix it, try rebasing again using the instructions here: https://github.com/kubernetes/kubernetes/blob/master/docs/devel/development.md#keeping-your-development-fork-in-sync\nEDIT: or just squash your commits into 1, that's ok too.\n. Great, but unfortunately github won't show the diff now because of all the godeps changes. Could you please move the godeps changes to a separate commit, so that I can review the other files separately? Thanks!\n. Great, but unfortunately github won't show the diff now because of all the godeps changes. Could you please move the godeps changes to a separate commit, so that I can review the other files separately? Thanks!\n. Sorry, I was out of office last week. I'll take a final look at this later today.\n. Sorry, I was out of office last week. I'll take a final look at this later today.\n. Just a few more comments. Thanks for your patience.\n. Just a few more comments. Thanks for your patience.\n. LGTM, just 1 small comment\n. LGTM, just 1 small comment\n. LGTM. Thanks!\n. LGTM. Thanks!\n. @k8s-bot ok to test\n. @k8s-bot ok to test\n. LGTM\n. LGTM\n. Please re-post your question to stackoverflow.\nWe are trying to consolidate the channels to which questions for help/support are posted so that we can improve our efficiency in responding to your requests, and to make it easier for you to find answers to frequently asked questions and how to address common use cases.\nWe regularly see messages posted in multiple forums, with the full response thread only in one place or, worse, spread across multiple forums. Also, the large volume of support issues on github is making it difficult for us to use issues to identify real bugs.\nThe Kubernetes (and cAdvisor) team scans stackoverflow on a regular basis, and will try to ensure your questions don't go unanswered.\n. Please re-post your question to stackoverflow.\nWe are trying to consolidate the channels to which questions for help/support are posted so that we can improve our efficiency in responding to your requests, and to make it easier for you to find answers to frequently asked questions and how to address common use cases.\nWe regularly see messages posted in multiple forums, with the full response thread only in one place or, worse, spread across multiple forums. Also, the large volume of support issues on github is making it difficult for us to use issues to identify real bugs.\nThe Kubernetes (and cAdvisor) team scans stackoverflow on a regular basis, and will try to ensure your questions don't go unanswered.\n. @vishh - This will conflict with your changes in https://github.com/google/cadvisor/pull/1035, how would you like to handle it?\n. @vishh - This will conflict with your changes in https://github.com/google/cadvisor/pull/1035, how would you like to handle it?\n. The tests don't like 2016: https://github.com/google/cadvisor/pull/1047 needs to be submitted.\n. The tests don't like 2016: https://github.com/google/cadvisor/pull/1047 needs to be submitted.\n. Squashed.\n. Squashed.\n. @k8s-bot test this\n. @k8s-bot test this\n. I forgot to update the imports after moving the conversion files to v2. Fixed.\n. I forgot to update the imports after moving the conversion files to v2. Fixed.\n. I'm not sure what org you need to be in to be assigned cAdvisor issues, so I marked it community-assigned for now.\n. I'm not sure what org you need to be in to be assigned cAdvisor issues, so I marked it community-assigned for now.\n. @ncdc - definitely. I will raise some discussions internally.\n. @ncdc - definitely. I will raise some discussions internally.\n. /cc @mwielgus\n. /cc @mwielgus\n. /cc @jimmidyson \nThansk Vish!\n. /cc @jimmidyson \nThansk Vish!\n. Nope :( I'm adding a test.\n. Nope :( I'm adding a test.\n. @k8s-bot test this\n. @k8s-bot test this\n. Thanks. Is it possible to cherrypick into a tagged release, or do we need to do another minor release?\n. Thanks. Is it possible to cherrypick into a tagged release, or do we need to do another minor release?\n. Ok. I realized this is not actually blocking me at the moment, so I'll just wait for the next cAdvisor update to grab all the fixes.\n. Ok. I realized this is not actually blocking me at the moment, so I'll just wait for the next cAdvisor update to grab all the fixes.\n. Ping. What's the status of this?\n. Ping. What's the status of this?\n. Is anything else needed here?\n. Got it, thanks.\n. I believe this was fixed by https://github.com/google/cadvisor/pull/1097. Please reopen if that is not correct.\n. Obsolete.\n. @k8s-bot test this\n. Anyone know how to refresh the jenkins auth credentials?\n. And any objections to ignore the tests for this? (It's just docs)\n. Thanks. I added the new labels, and made a quick pass (a lot more is needed) to add them to some issues. In doing so, I got a few ideas for other labels that might be useful. I'll follow up to add them later, but I'm listing them now so I remember :)\n- area/runtime (e.g. docker, rkt, hyper, etc.)\n- metrics/{cpu,memory,network,fs,etc.} for issues related to specific metrics\n. > @timstclair is what I just pushed along the lines of what you were thinking?\nCloser, but rather than a bunch of switch statements I think it would be better to encapsulate those branch points in the ContainerHandler, and simply delegate to the container handler at that point. The manager might need a list of handlers (e.g. for Raw containers), but I'm not sure exactly how that would work. Maybe it would be good to schedule a VC meeting to discuss this in more detail?\n. Yeah, that's the gist of it. Or, the ContainerHandler could be instantiated in the manager, but I'm trying to reduce the number of places switch containerRuntime { ... } is needed. So, having one switch to instantiate the runtime (or delegate to a factory) would be acceptable.\n. LGTM\n. I'm sorry, I could have sworn this worked the first time around... For the build tags to work, we would always need to specify -tags test when running unit tests, which is annoying. It looks like the go standard library just appends _test to test-only files (even if they don't contain any tests), so I'll go with that method unless there's any objections...\n. Actually, I was wrong again - *_test.go files aren't exported, which is problematic for container/mock.go which is required by the manager package. So, I think the only option without a more substantial refactoring (to move test utilities to their own package) is to use the test tag. WDYT?\n. PTAL\n/cc @jimmidyson @vishh \n. > Can we move the mock/stubs to a separate package?\nIt's not that simple unfortunately... just moving the file to a separate package creates a circular dependency. The circular dependency could be fixed by moving tests to a _test package, but then they lose access to the unexported types. The types could be exported in another _test.go file, but in the end I think the build tags are cleaner... WDYT?\n. > Build is still failing. I suspect this is because the jenkins has not been updated to use make yet. I will make that change now.\nWhere do you see that? I just see the auth failure... \n. Failure is actually a good sign that the new presubmit check is working :) This PR depends on https://github.com/google/cadvisor/pull/1110\n. @jdef were you asking for more of an explanation here, or to document it somewhere in cAdvisor?\nTo elaborate here, the flags are added by each package in the init() function, which is called the first time the package is imported. This means that if a caller is going to use flag.Add(All)Flags in an Init() function, the package that calls it must import all the cAdvisor packages that add desired flags. This can all be avoided by just calling Add(All)Flags outside the Init function, so it should not be much of an issue.\n. > Should be documented for sanity.\nDone.\n. Closing in favor of solution proposed in https://github.com/google/cadvisor/issues/1175#issuecomment-199563817\n. LGTM, but I'm concerned that if du times out, it may indicate a recurring problem (i.e. rerunning du may also timeout). If this seems to be the case, we should consider marking it as a permanent failure (maybe give it a few tries).\n. Maybe an exponential backoff on the retries would be better.\n. Commented on https://github.com/vishh/cadvisor/commit/a5decfeaad26fcf72ce7254e4516ef18b21eacb5\n. LGTM\n. LGTM, ok to self merge once e2e passes.\n. Does the --port flag provide what you're looking for?\n. How are you running the docker image? Does the docker run -p <host port>:8080 option work for you? (https://docs.docker.com/engine/reference/run/#expose-incoming-ports)\n. Completed review pass.\n. LGTM with 1 small nit. Self merge when ready :)\n. LGTM\n. LGTM. Thanks!\n. @vishh can you rebuild the docker image?\n. LGTM, just 2 style nits. Feel free to self merge after fixing.\n. > As cadvisor is embedded in kubelet binary, how can the cadvisor be upgraded in the kubernetes setup ?\nThe only way to upgrade the cAdvisor version used by the Kubelet is to build a new kubelet with the latest release (included at head, but not the latest pre-release). cAdvisor can be run in \"standalone\" mode (see https://github.com/timstclair/kube-contrib/blob/master/devel/manifests/cadvisor-pod.yaml), but you are likely to run into other issues running Kubernetes with docker v1.11 without running Kubelet binaries compiled in the last week.\n. Looks good, most comments are requests to clean up the code that you're touching.\n. LGTM\n. Please squash the commits and then I'll merge\n. I think @vishh was asking whether you are going to resolve https://github.com/google/cadvisor/pull/1128/files#r54950999 now or later.\n. LGTM\n. @k8s-bot ok to test\nLGTM, thanks!\n. > I usually copy the JS contents into a\n\nseparate file and then move it to its original location.\n\nWe should consider doing this programmatically in the build process. I wonder if we could use -ldflags, or if the length of the data would be a problem...\n. Thanks!\n. Did you remove the code to add network to the ignore metrics?\n. Yeah, this is fine. I just wanted to make sure that was a deliberate decision.\n. Thanks. BTW - I'm @timstclair not @timothysc - there are 2 of us :)\n. LGTM\n. Can we wrap this up and get it merged?\n. cc @sjenning @ncdc @derekwaynecarr\n. https://storage.googleapis.com/kubernetes-jenkins/logs/cadvisor-gce-e2e-ci/41/build-log.txt\nI'm not sure that this caused the issue, but we're having trouble debugging, so decided to try the rollback.\n. Oops, updated to external link.\n. @k8s-bot test this\n. Indeed, it would make the transition easier.\nxref: https://github.com/google/cadvisor/issues/606\n. Proposed solution:\n- Create a configuration package which holds a cAdvisor configuration struct.\n- Library integration points (e.g. manager) should accept a configuration object instead of relying on global variables.\n- Provide a DefaultConfiguration global with the same defaults used by the standalone cAdvisor binary.\n- Provide a mechanism for adding flags for (parts) of the configuration object.\n. @Thiht - Awesome! I filed a separate tracking issue for you to use: https://github.com/google/cadvisor/issues/1180\n. @bluecmd see my suggestion on https://github.com/google/cadvisor/issues/1201\n. Filed https://github.com/google/cadvisor/issues/1302 to track flag issue separately.\n. @thiht - Please add your suggestions for improvements to this tracking issue, and I'll be happy to review your PRs.\n. Great suggestions! A few notes:\n\nWhen creating a client, the version shouldn't be hardcoded but parameterizable\nMaybe this would result in a single client able to handle both v1 and v2? I don't know if it's doable or a good idea\n\nThe V1 and V2 clients may expose different methods, and those methods are going to have different input and output parameter types. I don't think there is a good way of accomplishing this point without using interface{} types to circumvent the type system, which is a bad idea. I think it's better to require users to explicitly import the client type they want, even if the constructors for both look the same.\n\nAt least httpGetJsonData should be publicly exposed to allow users to access non-exposed endpoints (in case the development of the API moves faster than the client)\n\nI think this would be better solved with policy / pre-submit checks: require additional endpoints to be handled in the clients.\n\nAs for moving client to client/v1, won't it result in breaking the existing builds?\n\nA lot of the big cleanup changes we're proposing will break builds that switch to the new code. As @vishh mentioned, users should be vendoring their dependencies to avoid this. However, I've created an api-cleanup branch for staging breaking changes for now. This way we can merge all the breaking changes into the master branch at the same time, and clients will only need to be updated once (it will be a big update - there are ways we can help with it).\n. > Is there the same issue between the v2 minor versions?\nI believe the minor version differences are mostly in the endpoints / request API, which should be hidden by the client wrapper, so I think we can just provide a single v1 and v2 client based on the most recent minor release.\n. >  I noticed the httpGet* methods take a postData argument, which doesn't really make sense semantically in a REST context.\nI agree this needs to be cleaned up, both on the client side and the server side, and would be happy to accept a patch. On the server side, I'd like to see the v1.ContainerInfoRequest and other requests encoded / decoded as query params to provide propre GET endpoints.\n. As I mentioned above, I think we should hide the details of the minor versions in the client. In other words, let's provide a single V2 client interface which provides the latest v2 API. The client should handle backwards compatability, and upconvert the deprecated version to the latest version. Does that make sense?\n. PR: https://github.com/google/cadvisor/pull/1073\n. Looks like the docker inspect API changed in https://github.com/docker/docker/commit/68fb7f4b744bf71206898d32fe203556a6261e5d, we might just need to update Godeps to the latest docker client (to include https://github.com/fsouza/go-dockerclient/pull/107)\n. Fixed via https://github.com/google/cadvisor/issues/1190\n. @k8s-bot ok to test\n. I'd like to see some performance analysis before we accept this. In particular, a comparison of CPU & memory usage with & without this patch, on a machine running a variable number of containers (up to at least 200). @krallin Will you be able to provide that data?\n. Sorry for the delay (I fell behind on PRs). This LGTM, and I can merge it once it's rebased.\n. FYI, we're hoping to cut a v0.23.2 release tomorrow (2016-05-18) for kubernetes 1.3. I think this just needs a rebase, will you be able to get it done?\n. Thanks!\n. @k8s-bot test this\n. @k8s-bot test this\n. Oops, this is printing as a byte array. I'll send a fix shortly.\n. @k8s-bot test this\n. @k8s-bot test this\n. @k8s-bot test this\n. Finally got tests passing - the issue is that ServerVersion doesn't get set in the info for docker < 1.9. I fixed it by falling back to the version API, and filling it in from that.\n@vishh PTAL\n. Dropped https://github.com/google/cadvisor/pull/1192/commits/f9377267a833048649e62d5933aa24ac8627697a which had snuck in here.\n. LGTM\n. More samples:\n- https://console.cloud.google.com/storage/browser/kubernetes-jenkins/logs/cadvisor-gce-e2e-ci/712\n- https://console.cloud.google.com/storage/browser/kubernetes-jenkins/logs/cadvisor-gce-e2e-ci/638\n- https://console.cloud.google.com/storage/browser/kubernetes-jenkins/logs/cadvisor-gce-e2e-ci/586\n- https://console.cloud.google.com/storage/browser/kubernetes-jenkins/logs/cadvisor-gce-e2e-ci/562\n. More complete error message:\n--- FAIL: TestDockerContainerSpec (3.00s)\n        framework.go:338: Failed to run \"sudo\" [docker run -d --cpu-shares 2048 --cpuset 0 --memory 1073741824 kubernetes/pause] in \"e2e-cadvisor-coreos-beta\" with error: \"exit status 1\". Stdout: \"c866ea6de6b7615385d4c9f4167f912e564cd44ea5a3a25b45593994b1e71328\\n\", Stderr: Warning: Permanently added 'e2e-cadvisor-coreos-beta' (ECDSA) to the list of known hosts.\n                Warning: '--cpuset' is deprecated, it will be replaced by '--cpuset-cpus' soon. See usage.\n                Error response from daemon: Cannot start container c866ea6de6b7615385d4c9f4167f912e564cd44ea5a3a25b45593994b1e71328: [8] System error: open /sys/fs/cgroup/memory/system.slice/docker-c866ea6de6b7615385d4c9f4167f912e564cd44ea5a3a25b45593994b1e71328.scope/memory.memsw.limit_in_bytes: no such file or directory\nIt appears to be the same file that's missing in all failures: /sys/fs/cgroup/memory/system.slice/docker-c866ea6de6b7615385d4c9f4167f912e564cd44ea5a3a25b45593994b1e71328.scope/memory.memsw.limit_in_bytes\n. @pwittrock where can we find our node setup for e2e-cadvisor-coreos-beta?\nI'm not sure how to reproduce this other than run the tests a bunch of times. It is flaking frequently enough that it shouldn't take too many tries.\n. > Would it ok if we just suppress those errors? From a usability stand point, it is great if cadvisor just works on a node that is running rkt.\nI'm less concerned about the error messages, and more concerned with the startup latency. It blocks cAdvisor startup for about 4 seconds while grpc retries (hence the need for https://github.com/google/cadvisor/pull/1200). If we could register the handlers in separate go routines it might not be a problem, but I haven't worked through the implications of doing it concurrently.\n. Happier, yes, but it would be nice to have some data around connection times rather than just picking an arbitrary numbers. Since we're dialing localhost, I imagine the success case is very fast. Is there an option to mark \"connection refused\" as a permanent error, or any other way to determine if rkt is running?\n. ok to test\n. LGTM, just one small comment.\n. LGTM\n. Ah, sorry for the nitty-comments, I missed the WIP tag.\n. @k8s-bot test this\n. Just some nits around error handling, then LGTM.\n. Sorry, a couple more nits around errors.\n. Thanks for the cleanup. LGTM.\n. @pwittrock did you ever get a documentation update?\n. ok to test\n. LGTM. Thanks for the patch! I'll merge it once the tests pass.\n. Thanks!\n. Looks like the format of state.json changed again, and the config is stored separately.\n. Fixed by https://github.com/google/cadvisor/pull/1213\n. It looks like cAdvisor is just picking up the aufs mount for the docker container\n```\n$ curl -G localhost:8080/api/v1.3/machine\n  ...\n  \"filesystems\": [\n    {\n      \"device\": \"none\",\n      \"capacity\": 10534313984,\n      \"type\": \"vfs\",\n      \"inodes\": 655360\n    },\n    {\n      \"device\": \"\\/dev\\/sda1\",\n      \"capacity\": 10534313984,\n      \"type\": \"vfs\",\n      \"inodes\": 655360\n    }\n  ],\n  ...\n$ cat /proc/self/mountinfo | grep docker\n43 22 8:1 /var/lib/docker/aufs /var/lib/docker/aufs rw,relatime - ext4 /dev/sda1 rw,data=ordered\n44 43 0:36 / /var/lib/docker/aufs/mnt/c2fcfb310c5f4d452a05e88ded99a64a6f6328abd7201d2bac069ca3b4881174 rw,relatime - aufs none rw,si=c8e0dd9b22c6182d,dio,dirperm1\n45 22 0:37 / /var/lib/docker/containers/4c9e3cc46f4043774b493a3bb65f7018acb89c40f4624d262291eae0519fc545/shm rw,nosuid,nodev,noexec,relatime - tmpfs shm rw,size=65536k\n260 21 0:3 / /run/docker/netns/18230b8e5227 rw - nsfs nsfs rw\n$ sudo df -T\nFilesystem     Type     1K-blocks    Used Available Use% Mounted on\nudev           devtmpfs   1884868       8   1884860   1% /dev\ntmpfs          tmpfs       378956     412    378544   1% /run\n/dev/sda1      ext4      10287416 3746372   6000436  39% /\nnone           tmpfs            4       0         4   0% /sys/fs/cgroup\nnone           tmpfs         5120       0      5120   0% /run/lock\nnone           tmpfs      1894768      32   1894736   1% /run/shm\nnone           tmpfs       102400       0    102400   0% /run/user\nnone           aufs      10287416 3746372   6000436  39% /var/lib/docker/aufs/mnt/c2fcfb310c5f4d452a05e88ded99a64a6f6328abd7201d2bac069ca3b4881174\nshm            tmpfs        65536       0     65536   0% /var/lib/docker/containers/4c9e3cc46f4043774b493a3bb65f7018acb89c40f4624d262291eae0519fc545/shm\n. @yujuhong - could you take a look at this since Vish is out today?\n. Thanks for the heads up. There's an open PR to switch to the official docker client: https://github.com/google/cadvisor/pull/1203\n. Thanks for the nice cleanup! Can we nest the asset directory under `/pages/static` or just `/pages`? I'd like to keep all the UI code together.\n. Thanks for the fixes. I think you should be able to squash to 2 commits: The first separates the js from the go, but doesn't modify the code, the second commit cleans up the js & lint errors. If this turns out to be too complicated, don't waste too much time on it.\n. LGTM. I assume you've manually verified that the pages still work?\n. @vishh please add ok to test\n. ok to test\n.\n+ ./build/check_gofmt.sh .\nThe following files are not properly formatted:\npages/static/assets.go pages/static/assets.go\n```\nI'm surprised this is necessary, but it looks like you need to add gofmt to your assets.sh script.\n. Oh yeah, the boilerplate should be prefixed to the generated file. As an example, see https://github.com/kubernetes/kubernetes/blob/master/hack/build-ui.sh. Feel free to copy a lot of that script into yours.\n. Thanks for fixing this!\n. Looks like you need to run gofmt, then LGTM.\n. LGTM\n. Rebased.\n. @k8s-bot test this\n. Addressed comments. Thanks!\n. I like the idea, but I'm nervous about the potential race condition. Can you elaborate more on how that can happen? Is it possible to fix it within the rkt API?\n. /cc @Random-Liu \n. Thanks @grobie . I agree about the global state, and don't consider this work finished, but rather it's a step in the right direction.\n. +cc @mtaufen since this is similar to the kubelet config work you just did\n. Thanks for the comments!\n\nIt looks like there's some initialization of components going on in main, are these things that will have to move to a separate e.g. Init function to make this a library?\n\nWe already use cAdvisor in \"library form\" in the kubelet, mostly accessed through the manager package. Generally, the main package is doing similar things to what the kubelet sets up with the manager, plus a few extras that aren't relevant to us. I think this is working ok.\n\ne.g. Init function to make this a library? If you have an Init code path, you'll be able to pass config into that and from there to sub-components and can probably get rid of the global state.\n\nI think you mean component constructors, as opposed to an actual golang init function? The golang init function adds more global state, and should be avoided... To reduce the global state, I would pass the config through to the various component constructors. \n\nIt might be worth adding an explicit validation stage for the config before any sub-components start running.\n\nI like this idea, but from the library perspective, it's not clear where this validation would go. It could be a separate function that is the responsibility of the library-user to call before starting any cAdvisor components. I think a better option is to just make each sub-component validate the relevant part of the config (I think it already works this way to some extent).\n\nLooking at this PR again, one change I'd like to add a way to control which flags are added, rather than the current all-or-nothing approach.\nI'll clean this up and target it for the Kubernetes 1.5 (cAdvisor v0.25) timeframe.\n. I dropped the changes to the storage backends, since I have a separate proposal addressing them (https://github.com/google/cadvisor/issues/1458).\n. Another:\nhttps://console.cloud.google.com/storage/browser/kubernetes-jenkins/logs/cadvisor-gce-e2e-ci/1045\nE0418 17:07:24.267037   11652 framework.go:335] About to run - [ssh -i /home/jenkins/.ssh/google_compute_engine -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o CheckHostIP=no -o StrictHostKeyChecking=no e2e-cadvisor-rhel-7 -- sh -c \" sudo docker rm -f 9749032b01832053d81b14607e2efc527b653869cb3af8038eb47e51481a3a7a \"]\n--- FAIL: TestGetAllDockerContainers (8.42s)\n        framework.go:338: Failed to run \"sudo\" [docker run -d kubernetes/pause] in \"e2e-cadvisor-rhel-7\" with error: \"exit status 255\". Stdout: \"\", Stderr: Warning: Permanently added 'e2e-cadvisor-rhel-7' (ECDSA) to the list of known hosts.\n                Permission denied (publickey,gssapi-keyex,gssapi-with-mic).\n. https://console.cloud.google.com/storage/browser/kubernetes-jenkins/logs/cadvisor-gce-e2e-ci/1831\n. This recently started happening on container-vm as well:\nhttps://console.cloud.google.com/storage/browser/kubernetes-jenkins/logs/cadvisor-gce-e2e-ci/6694\nframework.go:338: Failed to run \"sudo\" [docker rm -f e651803f0c740863434cbb4b6cc48048959f07dcc14c3fc469e0b91b1b78cc10] in \"e2e-cadvisor-container-vm-v20160127\" with error: \"exit status 255\". Stdout: \"\", Stderr: Warning: Permanently added 'e2e-cadvisor-container-vm-v20160127' (ECDSA) to the list of known hosts.\n                Permission denied (publickey).\nhttps://console.cloud.google.com/storage/browser/kubernetes-jenkins/logs/cadvisor-gce-e2e-ci/6768\nframework.go:338: Failed to run \"sudo\" [docker run -d kubernetes/pause] in \"e2e-cadvisor-container-vm-v20151215\" with error: \"exit status 255\". Stdout: \"\", Stderr: Warning: Permanently added 'e2e-cadvisor-container-vm-v20151215' (ECDSA) to the list of known hosts.\n                Permission denied (publickey).\n. Looks good, please resolve the CLA issues.\n. ok to test\n. LGTM\n. @sjpotter - FYI\n. @k8s-bot test this\n. No, sorry, our PR builder is broken right now. Fix is on the way.\n. Which version of cAdvisor? Dockers 1.11 support was recently added in cAdvisor 0.23\n. I believe latest currently points to v0.23.1. I'll bump that to v0.23.2 soon.\n. Closing as resolved. Please reopen if there are remaining issues.\n. The -docker_only flag should be what you're looking for.\n-docker_only\n        Only report docker containers in addition to root stats\n. Filed https://github.com/google/cadvisor/issues/1244\n. Thanks! I've been meaning to separate the storage documentation from the main readme, but this is ok for now.\n. ok to test\n. One last comment, then LGTM.\n. Thanks! LGTM\n. Reassigning to Vish - I'd like to get this in to k8s v1.3.\n. @k8s-bot test this\n. Thanks! I had heard a rumor you weren't working on cAdvisor anymore.\n. Friendly ping, this is blocking https://github.com/kubernetes/kubernetes/issues/24698\n. ok to test\n. Nice! 2 nits and LGTM\n. LGTM\n. The sort is expensive enough that it shows up in cAdvisor profiles, but that can be fixed by the much simpler changes in https://github.com/google/cadvisor/pull/1242. This PR deals mostly with optimizing the adding & removing elements. Or were you suggesting we just eat the cost of a full sort instead of insert when an append won't do?\n. +1 for on demand stats. I'd also like to avoid adding more flags if we can.\n. Can't we address that with on-demand scraping as well?\nWRT configuring internal-cAdvisor, I opened a proposal in https://github.com/google/cadvisor/pull/1224. It's an intermediate step, but would mean we could stop leaking cAdvisor flags into kube-binaries, and avoid the flag-editing needed for things like https://github.com/kubernetes/kubernetes/pull/24771\n. I don't know that I have more experience in this area, but my main concern is that as we have more and more components which want various stats at various intervals the complexity will get out of hand, and stats will be collected unnecessarily often. If we can make it a happen, I think a good on demand model could clean this up and lead to greater efficiency.\nI think this is probably complex enough to warrant at least an informal design document. I'd be happy to help out with it, but here are a few issues I can think of off the top of my head:\n- Blocking callers on a potentially slow operation: we made need to provide an async interface, or at least a timeout\n- Concurrent stats requests: we should return the stats to both callers in this case, but that could be a problem if the requests are slightly different\n- Cacheing the data appropriately\n- Handling slow operations (stating large directories): we should continue to rely on asynchronous scrapers for this \n. +1 - This will also let us simplify the container handler interface by removing some root-container specific functions. If I get some time later this week I'll take this on.\n/cc @sjpotter - FYI\n. Can we do something like the solution in http://stackoverflow.com/questions/26118303/docker-with-make-build-image-on-dockerfile-change so the docker image doesn't need to be rebuilt on every execution?\n. Partly I was concerned about the output spam, but maybe if we clean that up the build time is a non-issue.\n. I'm ok with what you have, but here's an alternative solution: create a script which: 1) builds the docker image if necessary (easier to just do it in a script than make...) 2) execute the make command in docker (like you have). Then, you could just control the use of docker with a USE_DOCKER variable, something like:\n```\nUSE_DOCKER =\nifdef USE_DOCKER\n  DOCKERMAKE = ...\nendif\nbuild:\n  $(DOCKERMAKE) ./build/build.sh\netc.\n```\nDoes that make sense? I'll leave it up to you.\n. Thanks, LGTM\n. ok to test\n. LGTM\n. > Relying on cgroups will not work for kvm stage1 anyway. And containers don't just mean cgroup + namespace. \n\ncAdvisor today does seem to have some abstraction (i.e. the ContainerHandler) I am not sure if that's enough though. If we implement that interface without directly talking with the cgroup fs, will the rkt + cAdvisor work?\n\ncAdvisor still makes some assumptions around containers using cgroups, though we could refactor more of that logic into the ContainerHandler interface. Another problem is that if rkt is still generating cgroups, the raw handler will pick it up (it's a catch-all for cgroups-based containers).\n. @k8s-bot test this\n. Looks like a flake, but that's the second time I've seen it: https://github.com/google/cadvisor/issues/1257\n. This is happening very frequently, and if I ssh into the host and issue that command, I can reproduce it pretty reliably (roughly 1 of every 2 or 3 tries fails):\nstclair@e2e-cadvisor-coreos-beta ~ $ sudo docker run -d --cpu-shares 2048 --cpuset-cpus 0 --memory 1073741824 --env TEST_VAR=FOO --label bar=baz kubernetes/pause\nbaef8231b979c04e03bb44d21bb85a5a85369771de97eb57034239f88f3a5524\ndocker: Error response from daemon: Container command not found or does not exist..\nI observed that removing the --memory 1073741824 flag seems to prevent the problem (I couldn't reproduce without that flag).\nSetting the memory limit to the minimum reliably gives this error:\nstclair@e2e-cadvisor-coreos-beta ~ $ sudo docker run -d --cpu-shares 2048 --cpuset-cpus 0 --memory 4194304 --env TEST_VAR=FOO --label bar=baz kubernetes/pause\n6d65e0c7d078990e78440b93944a7c668ac2f2fca649e043624ae19da50fd61a\ndocker: Error response from daemon: Cannot start container 6d65e0c7d078990e78440b93944a7c668ac2f2fca649e043624ae19da50fd61a: [9] System error: write parent: broken pipe.\nAnd adding 1 reproduces the Container command not found:\nstclair@e2e-cadvisor-coreos-beta ~ $ sudo docker run -d --cpu-shares 2048 --cpuset-cpus 0 --memory 4194305 --env TEST_VAR=FOO --label bar=baz kubernetes/pause\nae7897e3f5ef100582abf512f08c2b7992ab66f7da5c4c399b2c89899f34055e\ndocker: Error response from daemon: Container command not found or does not exist..\nThis looks like a bug in the docker version running.\nstclair@e2e-cadvisor-coreos-beta ~ $ sudo docker version\nClient:\n Version:      1.10.3\n API version:  1.22\n Go version:   go1.5.3\n Git commit:   8acee1b\n Built:        \n OS/Arch:      linux/amd64\nServer:\n Version:      1.10.3\n API version:  1.22\n Go version:   go1.5.3\n Git commit:   8acee1b\n Built:        \n OS/Arch:      linux/amd64\n@Random-Liu did you see anything like this when you were testing Docker v1.10?\n/cc @yifan-gu @sjpotter \n. More details from the docker logs:\n$ sudo journalctl -u docker --no-pager\n...\nMay 23 18:56:15 e2e-cadvisor-coreos-beta.c.kubernetes-jenkins.internal dockerd[1335]: time=\"2016-05-23T18:56:15.323147623Z\" level=warning msg=\"signal: killed\"\nMay 23 18:56:15 e2e-cadvisor-coreos-beta.c.kubernetes-jenkins.internal dockerd[1335]: time=\"2016-05-23T18:56:15.459091218Z\" level=error msg=\"error locating sandbox id f8ce9a4850614d5bdec61b7d9ba824d52a90df94ea3178d4e7ed72f51f38c23f: sandbox f8ce9a4850614d5bdec61b7d9ba824d52a90df94ea3178d4e7ed72f51f38c23f not found\"\nMay 23 18:56:15 e2e-cadvisor-coreos-beta.c.kubernetes-jenkins.internal dockerd[1335]: time=\"2016-05-23T18:56:15.459677653Z\" level=warning msg=\"failed to cleanup ipc mounts:\\nfailed to umount /var/lib/docker/containers/9c2855c6d168e5662682d9d2eb858944d1d366010d1ac9667c86ceb8a7209b7b/shm: invalid argument\"\nMay 23 18:56:15 e2e-cadvisor-coreos-beta.c.kubernetes-jenkins.internal dockerd[1335]: time=\"2016-05-23T18:56:15.459971658Z\" level=error msg=\"Error unmounting container 9c2855c6d168e5662682d9d2eb858944d1d366010d1ac9667c86ceb8a7209b7b: not mounted\"\nMay 23 18:56:15 e2e-cadvisor-coreos-beta.c.kubernetes-jenkins.internal dockerd[1335]: time=\"2016-05-23T18:56:15.460397447Z\" level=error msg=\"Handler for POST /v1.22/containers/9c2855c6d168e5662682d9d2eb858944d1d366010d1ac9667c86ceb8a7209b7b/start returned error: Container command not found or does not exist.\"\n. ok to test\n. LGTM\n. Please reopen with more details if the existing API is insufficient.\n. I don't like passing around the detection type. I think it would be better if factories are registered per detection mechanism instead. Rather than registering the factories against a global (https://github.com/google/cadvisor/blob/master/container/factory.go#L76), we could create a \"ContainerWatcher\" interface, something like:\ngo\ntype ContainerWatcher interface {\n  Start()\n  Stop()\n  RegisterFactory(factory ContainerHandlerFactory)\n}\n. Ok, I still don't like passing around the WatchSource but I think fixing it would be a more substantial change, so I'm willing to unblock this with the WatchSource. If you clean up the remaining issues, we can get this merged. Also, please update the PR title.\n. Yes. Please make sure that any remaining issues / PRs are marked with the kubernetes v1.3 milestone\n. Can we just use inotify?\n. What's the status on this? It would be good to get this fix into k8s 1.3.\n. BTW - why did you decide not to use https://github.com/hpcloud/tail?\n. ok to test\n. Which parts are you having trouble with? The race condition can be avoided by protecting the reader with a mutex, and you can punt on the unit test for now.\n. FYI, we're hoping to cut a v0.23.2 release tomorrow (2016-05-18) for kubernetes 1.3, and I'd like to get this change in. Please ping me if there's anything I can help with to move this along.\n. Great, thanks!\n. Thanks, this looks much better. Just a couple small things, then LGTM.\n. Taking this as-is, will follow up with a couple fixes. Thanks!\n. Addressed comments.\n. This should be fixed in the latest cAdvisor release. Please respond if that's not the case.\n. LGTM\n. ok to test\n. Thanks for the documentation, could you please sync it with the structure that was just merged in https://github.com/google/cadvisor/pull/1265 (move to docs/storage/kafka.md, and add a link to docs/storage/README.md). Everything else LGTM.\n. LGTM\n. Review pass complete.\n. > Haha... seems the disk on the machine Jenkins is running on is full?\n@pwittrock - how do we fix this? Is there something we can do to prevent this from happening again?\n. LGTM, just 2 nits\n. Awesome! I tested this locally, and ran into a few issues. Let me know if you'd like to address them in this PR or another.\n1. The graph would occasionally sporatically jump, which I suspect might be caused by a different container becoming a \"top N\" user (and bumping another from the graph).\n2. The graph tail disappearing, like this:\n   \n   I think this is due to an offset in container collection frequency (the blue line lags), but I'm not sure what to do about it.\n3. The legend is paginated with more containers (see screenshot below), but the pagination is reset each time the graph is updated.\n   \n. SGTM, let me know once you've made the change to sort by name, and I'll merge it.\n. LGTM\n. > a better change would be to implement it \nIt's not just a matter of implementing it though. To be useful, it also needs to be wired through to all the endpoints: REST API, clients, and UI. If you think it's valuable we should open an issue for it, but as is I think it was just a confusing bit for new runtimes (rkt) to deal with.\n. Failure was from a merge conflict.\n. Duplicate of https://github.com/google/cadvisor/issues/1177\n. This is because you've exposed the influxdb:8086 port to the docker host, but not to the cAdvisor container. The easiest (though not recommended) way to fix this is to run cAdvisor in the host network with --net=host. The docker recommended approach is to create a shared network, and is detailed in their networking documentation. Another option is to use --link=influxdb on the cAdvisor container along with -storage_driver_host=influxdb:8086.\n. Can you describe the errors after applying one of the above suggestions?\n. See my comment on https://github.com/google/cadvisor/issues/1201#issue-147065503\nI think -runtime_only is a bit weird, and we should just give the option to control which container handlers are enabled (default should be all). As for deprecating, I'd just update the usage message to be DEPRECATED: Use -container_handlers instead (or whatever the new flag is).\n. FYI, cAdvisor PR builder is hosed right now. Ignore the failures.\nOn Mon, May 16, 2016 at 3:10 PM, Kubernetes Bot notifications@github.com\nwrote:\n\nJenkins GCE e2e\nBuild/test failed for commit e2934ac\nhttps://github.com/google/cadvisor/commit/e2934ac1508ff5a8e51d0fb420b4e4a5343c4531\n.\n- Build Log\n  https://storage.cloud.google.com/kubernetes-jenkins/pr-logs/pull/1284/cadvisor-pull-build-test-e2e/577/build-log.txt\n\u2014\nYou are receiving this because you were assigned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1284#issuecomment-219563891\n. @k8s-bot test this\n. Still poking the infrastructure.. I think these just timed out since this was the first run on the instance and the images weren't cached.\n. @k8s-bot test this\n. Thanks, done.\n. @k8s-bot test this\n. /cc @pwittrock \n. Rebased & reverted Makefile changes. Will merge once tests pass.\n. Thanks!\n. Updated to mention Device Mapper module. PTAL\n. Please elaborate. Are you asking about the API, UI, or a command-line option to specify which containers cAdvisor should monitor?\n. See https://github.com/google/cadvisor/issues/1302 for filtering to a specific runtime. What is your use case for filtering to a specific container?\n. No, it's not currently possible. Why do you want to do that? Is it a performance issue?\n. TBR: @vishh \n. What are the concerns around building this way? Is it just that it increases our shipping binary size?\n. LGTM\n. This won't affect kubelet though.\n. This should be fixed in the prometheus handler (here), not the docker handler. If you change it in the docker handler, it will break other clients expecting to get the labels un-altered. Also, fixing it in prometheus will fix it for all container runtimes.\n. Actually, the actual flake looks like https://github.com/google/cadvisor/issues/1228:\n\nF0523 15:08:38.158517   10157 runner.go:290] Error 0: error on host e2e-cadvisor-rhel-7: command \"godep\" [\"go\" \"test\" \"--timeout\" \"15m0s\" \"github.com/google/cadvisor/integration/tests/...\" \"--host\" \"e2e-cadvisor-rhel-7\" \"--port\" \"8080\" \"--ssh-options\" \"-i /home/jenkins/.ssh/google_compute_engine -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o CheckHostIP=no -o StrictHostKeyChecking=no\"] failed with error: exit status 1 and output: godep: WARNING: Go version (go1.6) & $GO15VENDOREXPERIMENT= wants to enable the vendor experiment, but disabling because a Godep workspace (Godeps/_workspace) exists\n--- FAIL: TestDockerContainerCpuStats (1.87s)\n        framework.go:338: Failed to run \"sudo\" [docker rm -f 3348d8eca4c0b22dd53f626ec3387662581f865f1ace484b99792f55811e741c] in \"e2e-cadvisor-rhel-7\" with error: \"exit status 255\". Stdout: \"\", Stderr: Warning: Permanently added 'e2e-cadvisor-rhel-7' (ECDSA) to the list of known hosts.\n                Permission denied (publickey,gssapi-keyex,gssapi-with-mic).\nFAIL\nFAIL    github.com/google/cadvisor/integration/tests/api        90.315s\nok      github.com/google/cadvisor/integration/tests/healthz    0.011s\ngodep: go exit status 1\nI think the errors reported in the logs should still be investigated, even if it just means turning down the log spam.\n. If that's the case the codepath that's generating these errors should not\nbe enabled...\nOn Mon, May 23, 2016 at 7:16 PM, Vish Kannan notifications@github.com\nwrote:\n\nWe might have to install the relevant kernel modules and packages to have\nthin_ls work on our test nodes.\nOn Mon, May 23, 2016 at 4:51 PM, Paul Morie notifications@github.com\nwrote:\n\nI'm on PTO until Thurs. Will look at it then.\nOn Mon, May 23, 2016 at 6:23 PM, Tim St. Clair <notifications@github.com\nwrote:\n\nActually, the actual flake looks like #1228\nhttps://github.com/google/cadvisor/issues/1228:\nF0523 15:08:38.158517 10157 runner.go:290] Error 0: error on host\ne2e-cadvisor-rhel-7: command \"godep\" [\"go\" \"test\" \"--timeout\" \"15m0s\" \"\ngithub.com/google/cadvisor/integration/tests/...\" \"--host\"\n\"e2e-cadvisor-rhel-7\" \"--port\" \"8080\" \"--ssh-options\" \"-i\n/home/jenkins/.ssh/google_compute_engine -o UserKnownHostsFile=/dev/null\n-o\nIdentitiesOnly=yes -o CheckHostIP=no -o StrictHostKeyChecking=no\"] failed\nwith error: exit status 1 and output: godep: WARNING: Go version (go1.6)\n&\n$GO15VENDOREXPERIMENT= wants to enable the vendor experiment, but\ndisabling\nbecause a Godep workspace (Godeps/_workspace) exists\n--- FAIL: TestDockerContainerCpuStats (1.87s)\nframework.go:338: Failed to run \"sudo\" [docker rm -f\n3348d8eca4c0b22dd53f626ec3387662581f865f1ace484b99792f55811e741c] in\n\"e2e-cadvisor-rhel-7\" with error: \"exit status 255\". Stdout: \"\", Stderr:\nWarning: Permanently added 'e2e-cadvisor-rhel-7' (ECDSA) to the list of\nknown hosts.\nPermission denied (publickey,gssapi-keyex,gssapi-with-mic).\nFAIL\nFAIL github.com/google/cadvisor/integration/tests/api 90.315s\nok github.com/google/cadvisor/integration/tests/healthz 0.011s\ngodep: go exit status 1\nI think the errors reported in the logs should still be investigated,\neven\nif it just means turning down the log spam.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\n<https://github.com/google/cadvisor/issues/1306#issuecomment-221113903\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1306#issuecomment-221129046\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1306#issuecomment-221148679\n. I'm marking this as a cherrypick-candidate, since I'm worried about it flooding logs.\n. Closing this in favor of https://github.com/google/cadvisor/issues/1308.\n. From offline discussions, here's how we'd like to handle rkt testing:\n- Eventually we'd like to move to a distributed testing model where CoreOS maintains a CI project which we add to our build monitoring\n- For the near-term, if you create a GCE VM image (with rkt setup) and share it with us, we'll add it to our e2e test nodes (this way it will hopefully be easier to debug failures in a consistent manner)\n\nWith regards to this PR, I think whether rkt tests are run should be explicitly controlled with a test flag (either a build tag or a per-host parameter to the runner).\n/cc @vishh \n. I already cut the cAdvisor cherry-pick release, but since it's still in pre-release I think it would be OK to pull this fix in to it. I'm going to be OOO starting tomorrow though, so someone else will need to take responsibility for getting this in.\n. LGTM. Needs a rebase though.\n. cAdvisor and lots of it's dependencies use cgo:\nhttps://github.com/google/cadvisor/blob/master/utils/procfs/jiffy.go\nhttps://github.com/google/cadvisor/blob/master/utils/cpuload/netlink/defs.go\nhttps://github.com/google/cadvisor/blob/master/Godeps/_workspace/src/github.com/seccomp/libseccomp-golang/seccomp.go#L26\netc.\n. Yes, the labels should all be treated as opaque by cAdvisor.\n. @vishh - This is a breaking change for the cAdvisor-prometheus API but it sounds like a necessary fix. Do you have any additional concerns about this change, or should we just clearly document it in the next release?\n. LGTM. I'll merge once the tests pass.\n. Sorry, the tests were never run against this. Please rebase, then I will kick off the tests.\n. ok to test\n. Thanks!\n. LGTM. Please rebase & squash commits.\n. Blocked by https://github.com/google/cadvisor/issues/1458. Sorry for not following up with this. Please rebase then I'll merge.\n. ok to test\n. Thanks! Hmm, looks like there's no way to ignore the tests now that they're marked as required...\n. @k8s-bot ok to test\n. @vishh How do I handle the CLA with cherrypicks?\n. The issue is your prometheus scrape interval of 1m. The prometheus endpoint doesn't include historical data, only the current data. So, when prometheus scrapes cAdvisor, it will only see the currently running containers.\n. FYI, there is a scrape interval for cAdvisor as well, but it defaults to 1s, so I don't think that should be a problem for you.\n. -storage_duration duration\n        How long to keep data stored (Default: 2min). (default 2m0s)\nOther relevant flags:\n-event_storage_age_limit string\n        Max length of time for which to store events (per type). Value is a comma separated list of key values, where the keys are event types (e.g.: creation, oom) or \"default\" and the value is a duration. Default is applied to all non-specified event types (default \"default=24h\")\n  -event_storage_event_limit string\n        Max number of events to store (per type). Value is a comma separated list of key values, where the keys are event types (e.g.: creation, oom) or \"default\" and the value is an integer. Default is applied to all non-specified event types (default \"default=100000\")\n  -global_housekeeping_interval duration\n        Interval between global housekeepings (default 1m0s)\n  -housekeeping_interval duration\n        Interval between container housekeepings (default 1s)\n  -max_housekeeping_interval duration\n        Largest interval to allow between container housekeepings (default 1m0s)\n. Sorry, this needs another rebase. I will merge it as soon as you do.\n. It looks like you're doing 2 different things here:\n1. Move Godeps/_workspace to vendor\n2. Replace godep with trash for dependency management\nThe first sounds reasonable to me, as godep now supports the vendor directory and Kubernetes has already made the move. As for the second, I would like to keep the cAdvisor tooling as close to the Kubernetes tooling as possible. Some people have been investigating replacing godep with glide, but I'm not sure what the status is. If you think trash is a supperior option, I recommend opening a discussion on the kubernetes repo.\n. Superseded by https://github.com/google/cadvisor/pull/1369\n. ok to test\n. CoreOS issue: https://github.com/google/cadvisor/issues/1344 - looks like this has started affecting the build job as well.\n. Disabled coreos-beta.\n@k8s-bot test this\n. @pwittrock do you know when you restarted the jenkins CI VMs? At first glance this doesn't look like the PR it started failing at would have caused it.\n. /cc @dchen1107 \n. Failing since kubekins build #4096 (sorry, Google internal only)\nActually, ignore my comment about https://github.com/google/cadvisor/pull/1333 - many builds succeeded on that revision prior to the failures starting.\nI'm guessing the core-os VM auto-updated and is no longer setting the CPU mask. I'll ping the pr-builder jenkins to see if it's having the same issue.\n. No, I haven't tracked it down yet. It looks like this has started affecting the build jobs as well (it didn't yesterday). I think this lends credence to it being caused by a core OS update.\n. Let me see if I can reproduce on a new GCE coreos instance.\n. I'll disable it for the builder.\n. Ok, I was able to reproduce this on a fresh coreOS beta image (VERSION_ID=1068.2.0). I just did:\n1. Install go to home directory\n2. go get godep & cadvisor\n3. I had trouble building cadvisor, so I copied the binary built on another machine\n4. run build/integration.sh, which surfaced the same error:\n```\n$ build/integration.sh \n\n\nstarting cAdvisor locally\nWaiting for cAdvisor to start ...\nrunning integration tests against local cAdvisor\ngodep: WARNING: Godep workspaces (./Godeps/_workspace) are deprecated and support for them will be removed when go1.8 is released.\ngodep: WARNING: Go version (go1.6) & $GO15VENDOREXPERIMENT= wants to enable the vendor experiment, but disabling because a Godep workspace (Godeps/_workspace) exists\n--- FAIL: TestDockerContainerSpec (0.51s)\n        Location:       docker_test.go:225\n    Error:      Not equal: \"0\" (expected)\n                    != \"\" (actual)\n    Messages:   Cpu mask should be \"0\", but is \"\"\n\n\nFAIL\nFAIL    github.com/google/cadvisor/integration/tests/api    29.321s\nok      github.com/google/cadvisor/integration/tests/healthz    0.004s\ngodep: go exit status 1\nIntegration tests failed\n\n\nstopping cAdvisor\nbuild/integration.sh: line 45:  5341 Killed                  sudo ./cadvisor --docker_env_metadata_whitelist=TEST_VAR\n```\n\n\n@euank can you take it from here?\n. (3) sounds reasonable to me, and I think we should do it anyway (filed https://github.com/google/cadvisor/issues/1361). Once the jenkins jobs are updated to use the cAdvisor jenkins script (https://github.com/kubernetes/test-infra/pull/248) I'll add a coreos-stable VM.\n. I'm not sure what (if anything) needs to be changed from the unmodified coreos image, so it might just work. If you're up for trying it and figuring out what (if anything) needs to be added, I'd certainly welcome the help :)\nYou can see the command used to run the tests here.\n. @krousey we're going to do another cAdvisor update, and it would be good to include this. Could you send a cherrypick against the v0.23 branch?\n. Cherrypick your commit into a local copy of the v0.23 branch (via git cherry-pick), then submit a PR from your branch to the v0.23 branch. Ping me if you run into trouble or want a more detailed explanation.\n. ok to test\n. LGTM, but needs a rebase.\n. ok to test\n. ok to test\n. Sorry, needs another rebase.\n. LGTM\n. From the e2e logs:\n+ ./build/check_gofmt.sh .\nThe following files are not properly formatted:\ncontainer/docker/handler.go container/docker/handler.go\nBuild step 'Execute shell' marked build as failure\nLooks like you need to run gofmt on container/docker/handler.go and container/docker/handler.go.\n. Can you move the collector changes to a separate PR? We try to limit PRs to a single conceptual change.\n. LGTM, but needs a rebase. Sorry for the slow turn around. Feel free to ping me directly on slack or hangouts if I take more than a day to respond on a PR.\n. My apologies. I missed this comment before. \n\nHow receptive is cAdvisor to fixes and updates?\n\nVery. We're happy to have community contributions. Unfortunately we're a bit short-staffed on reviewers right now. I'd like to empower more community (non-Googler) maintainers, but this is slightly complicated by the current lack of a well defined scope & goal for cAdvisor. We're working on addressing this.\n\nI am wondering how difficult its going to be to get custom metrics functioning properly.\n\nHopefully it will be easier going from here. Optionally, you could have a coworker take a first pass so I can accelerate my review (especially if they're a Kubernetes maintainer). I'll look into adding more k8s maintainers to cAdvisor as well.\n\nIts been a bit frustrating to have pr succumb to bit rot.\n\n:( Staying on top of all the github PRs is a challenge. Again, please feel free to ping me directly if you don't get a response in a timely manner.\nThanks for your contributions!\n. ok to test\n. Thanks! LGTM\n. LGTM, but needs a rebase\n. LGTM.\n. ok to test\n. Thanks! LGTM.\n. How is this \"out of date\"? Github thinks you need a rebase...\n. Rebased.\n. Better, but it looks like there's still an issue:\n/tmp/hudson7584363431913630153.sh: line 3: cd: go/src/github.com/google/cadvisor: No such file or directory\n. Needs a rebase.\n. LGTM, but needs a rebase.\nDid GitHub change their This branch is out-of-date with the base branch? I feel like I've seen it way more often in the past 2 weeks or so.\n. @k8s-bot test this\n. We don't support running on a mac. If you want to build on a mac, you'll need to cross compile for linux.\n. InfluxDB? Did you follow the instructions here? If you set the storage driver to stdout, do you see the expected output?\n. @ronnielai Can you take this?\n. xref: https://github.com/google/cadvisor/issues/1174\n. LGTM\n. Thanks for fixing this!\n. LGTM. Go ahead and self-merge after squashing the comment commits.\n. Sorry, I'm losing track of which comments I left where:\n\nAh, that makes sense, sorry I missed that before. Do you think it's better to build cAdvisor in the docker container, like it was before? Or does it not make a difference? I'm OK with whichever you think is best :)\n\nThis LGTM, if you decide this approach is the way to go (if you do, go ahead with a self-merge)\n. Actually, I think to do this without breaking CI we need to split it into 2 PRs that are merged in this order:\n1. Add the make rule\n2. Update the CI to use the make rule\n3. Delete the dockerfile\n. Fair. Do you have merge permissions on test-infra? If we merge both at roughly the same time we shouldn't have any failures anyway...\n. SGTM.\n. We're currently heads-down for our next release. I can take a look in a couple weeks if no one gets to it before then.\n. This is currently blocked by https://github.com/google/cadvisor/issues/1458. LGTM with one nit\n. Thanks, LGTM.\n. This means you're missing the required libraries: libpthread and libc. Those libraries are pretty standard, what kind of environment are you building in?\n. ok to test\n. It looks like our system still thinks you haven't signed the CLA. Did you commit with your company email address?\n. Hmm, can you try responding with \"I signed it\" again? Is your corp email associated with your github account (in https://github.com/settings/emails)?\n. Hmm, I'm not sure exactly. I'll try to get to this review in the next ~2 weeks, and I'll look into it more then.\n. We're reconsidering how we want to handle storage drivers in cAdvisor right now (see https://github.com/google/cadvisor/issues/1458). For the moment, storage driver additions are on-hold.\n/cc @dchen1107 @vishh \n. I commented on that issue. I think the resolution is more complicated than I initially thought, but unfortunately I don't have the time to think through it right now.\n. Approach looks good, but I'm concerned about the InsecureSkipVerify. Can you explain the need for that in more detail?\n. Thanks for the explanation. I think this looks good as is, we can always add in the option later if there is demand for it. Needs another rebase though.\n. LGTM, if @yujuhong is OK with ignoring the error type.\n. I think we should cherry-pick https://github.com/google/cadvisor/pull/1357 instead, since it has much fewer changes and won't affect the binaries at all.\n. The underlying issue is that we changed the jenkins jobs to use the build/jenkins_e2e.sh script, but the release-v0.23 branch doesn't have a functional version of that script.\n. Sure. @mtaufen can you do the cherry-pick?\n. LGTM. Thanks!\n. The CLA bot isn't picking up your signature... Did you sign it with the same email address you used with your commit?\n. Do you have your soundcloud email address connected to your github account? (i.e. is it listed here? https://github.com/settings/emails)\n. Is this different from https://github.com/google/cadvisor/pull/1370?\n. My same comment applies here: should we update to an official tagged release rather than an untagged commit?\n. Ok, LGTM.\n. cAdvisor includes docker labels with the containers API. You can get all container stats, and aggregate based on a label. Another option is to setup each tenant in a separate parent cgroup, and then cAdvisor can get stats for the subcontainers of a cgroup.\n. ok to test\n. LGTM\n. needs a rebase\n. Yours :-)\nOn Jul 21, 2016 4:08 PM, \"Alexander Staubo\" notifications@github.com\nwrote:\n\n@timstclair https://github.com/timstclair: Is that merger's job or mine?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1391#issuecomment-234411175, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ADHtaKgEwoFMvjpajcQZA_1ie1Fyczouks5qX_v4gaJpZM4JRboM\n.\n. @vishh - you have more context on application metrics than me. Could you take a look at this?\n. Sorry, I've been hung up on 1.4 release stuff. I'll get to this later this week.\n. @mwringe @vishh - I'm wary of collectors ending up in a similar place to our \"storage backends\", with every metrics product adding their own until maintenance becomes too much of a burden.\n\nI'm not familiar enough with the metrics space to know whether this should be in \"core\", so can you provide some justification? Or would it be better to build a similar plugin mechanism to that being discussed in https://github.com/google/cadvisor/issues/1458?\n. ok to test\n. LGTM, with one question.\n. I do not think we should make cAdvisor run on windows. The model is too different. Instead, we should wait for the container runtime interface to define a stats interface, and implement that interface through another model (not cAdvisor).\nIt would be good to keep windows support in mind when we're defining the stats interface, so we can choose the right level of abstraction (i.e. not cgroups).\n. @jimmidyson The problem is that Go reuses the memory for the object in a range clause, and the for loop is taking the address of some fields on that struct:\n...\n            Usage:      &stat.Usage,\n            Available:  &stat.Available,\n            InodesFree: &stat.InodesFree,\n...\nThis means that every machine stats's Usage, Available, InodesFree will all point to the same memory address, which will contain the last item that was iterated over. The solution forces a new struct to be allocated for each iteration.\n. Nope. Kubernetes v1.3.5 shipped with cAdvisor v0.23.7. This change was recently cherrypicked into the v0.23 branch, but hasn't been cherrypicked into Kubernetes yet.\n. I'll bump latest (@dashpole FYI). LGTM\n. I can handle the release.\n. I spun up a RHEL7 instance on GCE, and I'm trying to run the integration tests against this image, but hitting this error:\nF0804 18:48:48.415384       1 cadvisor.go:151] Failed to start container manager: inotify_add_watch /sys/fs/cgroup/cpuacct,cpu: no such file or directory\nThe directory name is slightly different:\n[stclair@cadvisor-rhel cadvisor]$ ls /sys/fs/cgroup\nblkio  cpu  cpuacct  cpu,cpuacct  cpuset  devices  freezer  hugetlb  memory  net_cls  perf_event  systemd\nI'm running with:\nsudo docker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  --privileged=true \\\n  google/cadvisor:test\n@derekwaynecarr @jimmidyson any suggestions?\n. Mount /proc/mounts is correct, as are the symlinks. Interestingly, if I omit --volume=/sys:/sys:ro that error goes away, but the tests fail with a different error:\n--- FAIL: TestDockerContainerCpuStats (5.82s)\n    Location:   docker_test.go:63\n    Error:      No error is expected but got request \"http://localhost:8080/api/v1.3/docker/578645cef7dad17177ada1bb7041320e00b4895bec4de22a618d27177ad70249\" failed with error: \"failed to get Docker container \\\"578645cef7dad17177ada1bb7041320e00b4895bec4de22a618d27177ad70249\\\" with error: unable to find Docker container \\\"578645cef7dad17177ada1bb7041320e00b4895bec4de22a618d27177ad70249\\\"\"\n    Messages:   Timed out waiting for container \"578645cef7dad17177ada1bb7041320e00b4895bec4de22a618d27177ad70249\" to be available in cAdvisor: request \"http://localhost:8080/api/v1.3/docker/578645cef7dad17177ada1bb7041320e00b4895bec4de22a618d27177ad70249\" failed with error: \"failed to get Docker container \\\"578645cef7dad17177ada1bb7041320e00b4895bec4de22a618d27177ad70249\\\" with error: unable to find Docker container \\\"578645cef7dad17177ada1bb7041320e00b4895bec4de22a618d27177ad70249\\\"\"\n. 16 days ago: https://github.com/google/cadvisor/pull/1387\n. Thanks for looking into that. Should we move ahead with this PR then?\n. Just one nit. All the regex parsing is unfortunate, but I won't block you on it. Fix the nit, squash, and I'll merge.\n. You're fast! Waiting for tests to complete.\n. lgtm\n. Please send a PR updating https://github.com/google/cadvisor/blob/master/CHANGELOG.md and https://github.com/google/cadvisor/blob/release-v0.23/version/VERSION\n. @k8s-bot test this\n. LGTM\n. @k8s-bot test this\n. Duplicate of https://github.com/google/cadvisor/issues/1175\nxRef https://github.com/kubernetes/kubernetes/issues/19432\n. I was thinking this was blocked by https://github.com/google/cadvisor/issues/1458, but after skimming I like that it is a generic push-model, so I think it fits into the current design more cleanly. I'll try to get to a review in the next week or so.. Superseded by https://github.com/google/cadvisor/pull/1575. There are several discussions and proposals (e.g. https://github.com/kubernetes/heapster/blob/master/docs/proposals/push-metrics.md) for a separate application metrics pipeline in Kubernetes. I think the more relevant discussion here is how much value this feature adds for non-Kubernetes (a.k.a. standalone) users.\n. LGTM\n. ok to test\n. LGTM. Thanks!\n. ok to test\n. Can you provide some justification for why this should be cherrypicked?\n. I guess I'm just wondering if this is fixing a bug that's high enough priority to warrant a cherrypick. FWIW, we'll be cutting a release-v0.24 branch (with a v0.24.0-alpha1 tag) this Friday.\n. ok to test\n. Thus far we've avoided leaking Kubernetes concepts into cAdvisor, so I'm concerned about the precedent this is setting. I think I still don't quite understand what the problem here is. It sounds to me like the bug is that Kubernetes is abusing container labels, and this shouldn't be cAdvisor's problem.\nIf we really do need the logic in cAdvisor, I think a better approach would be to accept a list of labels to ignore, and have the Kubelet pass the list in rather than hardcoding it in cAdvisor. I also think the pod name, namespace, and container name are important to the identity and should remain on the container.\n. Yes, that was my suggestion. Specifically, it should be passed to this function.\nI would suggest renaming ContainerNameToLabelsFunc to ContainerLablesFunc, and have it take a few more parameters:\ntype ContainerNameToLabelsFunc func(containerName string, containerLabels map[string]string, containerEnv map[string]string) map[string]string\n(also, make the sanitizeLabelName func exported, so it can be reused by implementers of the above function).\nFinally, I would move all the existing label logic into an else condition. I.e., if that function is set, just use the output of that. Otherwise, do what we do today.\nWDYT?\n. ok to test. ok to test\n. @jimmidyson Yes. I just retagged v0.24.0 to v0.24.0-alpha1 (I know it's bad to change tags, but that was the original intent). We can cherrypick this into the release-v0.24 branch so it will be included in the actual v0.24 release in a couple weeks.\n. Oh yeah, our cAdvisor e2e tests are hosed right now. I'll retest this once they're back up and running.\n. No, it should be merged into master too.\n. Just a couple small comments. Mostly looks good.\n. @k8s-bot test this\n. Yeah, we could consider adopting that practice, but for now I'm just mirroring the practices used by Kubernetes (which is to cherry-pick from master into release).\n. @k8s-bot test this\n. LGTM. Waiting for tests to merge.\n. ok to test\n. @k8s-bot test this\n. Please send a PR with this change against the release-v0.24 branch, then you can update the k8s godeps from there.\n. ok to test\n. Ref: https://github.com/google/cadvisor/issues/1367\n. Unfortunately our API is a mess right now. I'm actually not sure this is worth tackling in isolation. Most of cAdvisor uses the v1 API right now, so I think the best path forward would be to clone the v1 API to an \"internal\" API that get's used within cAdvisor, and only provide the v1/v2 APIs at the process boundaries (i.e. json API). Once we have the internal API, we're free to clean that up however we see fit. All this is to say, if we start moving individual components to the v2 API piece-meal, it's going to be more difficult to transition to that internal API state.\n. ok to test\n. Could you set a non-zero-valued inodes free/total in the test container info?\n. I think you just need to manually add the values here: https://github.com/google/cadvisor/blob/master/metrics/prometheus_test.go#L130\n. LGTM\n. Yes, you can just run sudo cadvisor (needs root privileges). The binary can be found on our releases page, or you can build your own with make build.\n. LGTM, thanks!\n. LGTM\n. See https://github.com/google/cadvisor/issues/1621 - I recommend against cAdvisor basic auth. Try an nginx proxy instead (see https://github.com/beevelop/docker-nginx-basic-auth). The canary image builds cAdvisor from HEAD (in the container), so it has a lot more tool requirements. I'm sure it could be optimized more, but I'm not sure it's worth it (we'd still welcome a PR though). If the need is just to have a recent image built from head, it might be easier to just automate building with the same dockerfile used for the production images.\n. I didn't realize you were also working on this. You're missing a few files, but I have a PR ready to go.\n. Yeah, a general purpose library sounds good. We could implement it so that it uses the exact same StorageDriver interface we already have, and all that would need to be done is build a binary out of it.\n. I'll put together a PoC. Any objections to using go's standard net/rpc package over a unix socket?\n. I think using the existing rest API would require polling. I was envisioning a push model where cAdvisor connects to a \"server\" and sends an \"AddStats\" RPC every time AddStats would normally be called on the driver.\n. Sorry, I sent that too soon :)\nYes, I share your concerns about the maintenance problem, specifically around version skew. However, I'm not sure that using the REST endpoints makes it that much easier, since it would still be a problem to deprecate v1. What if instead we include a Version rpc call that returns the API version supported? That way the client can handle different versions, and we can eventually deprecate the older ones.\nI think the advantages of implementing it this way are:\n- Stats are pushed immediately, no lag from polling\n- Scraping the REST endpoint would require significantly more CPU & memory, since it needs to marshal & unmarshal the JSON for all containers at each polling interval, and check which stats are new.\n- The RPC implementation is actually much simpler, for the above reasons.\nThe initial server RPCs would be:\ntype Server interface {\n  AddStats(info *v1.ContainerInfo, _ *struct{}) error\n  Version(_ struct{}, version *string) error\n}\nWDYT?\n. > Do we need an RPC interface though?\nI don't have much experience with IPC programming. What are the alternatives?\n\nIf we go down the rpc + protobuf route\n\nDo we need protobuf? I was thinking of just using go's built in rpc library\n. I put together the simplest implementation I could in https://github.com/google/cadvisor/pull/1462\nThis is low priority, but I'd appreciate feedback if you get a chance. Id like to get this done for cAdvisor v0.25.\n. I was just reading up on go plugins. There's a proposal for 1st class dynamically linked plugins here which might (?) go out with go1.8.  Another option that looks interesting is github.com/natefinch/pie, which uses stdin/out + rpc stdlib + gob encoding for IPC.\n. I think there are enough nuances of this issue that it deserves a formal design proposal. There are still some open questions about the trade-offs of a local (IPC) vs remote (push stats over HTTP), and both of those have their own set of challenges.\n. +1\nIs anything other than the vars here (prometheus.NewDesc(...)) leaking?\n. Got it. I think that's actually not too bad, since everything that method does is through exported methods and it's not done by init, but a fix would still be welcome!\n. Got it. Thanks!\n. Is this the same as https://github.com/google/cadvisor/issues/1444?\n. Do we need this in k8s v1.4 / cAdvisor v0.24? The cAdvisor release was supposed to be cut today, but I held it back. If so, please get approval for cherrypicking into k8s v1.4 first.\n. @pwittrock in case we need this for v1.4\n. @eparis \n. Addressed comments, but I'm going to reopen this against the master branch, so I can grab all the release notes at once.\n. Can you add the cAdvisor output for one of those containers?\n. <cAdvisor_addr>:8080/api/v2.1/stats/<full_container_id>\nSpecifically, I'm interested in the aliases field.\n. Hmm, which version of docker & which version of cAdvisor are you running?\n. Actually, that looks like it's working as intended. The name of that container is mesos-f3b1b03c-c900-43ac-b07c-697c555dc65e-S12.fc74c1b1-23c1-4c3c-a26f-4b74e2da6ded. cAdvisor uses the first alias as the container name. In the UI it will use that name if you click the \"docker containers\" link. Where is the confusion?\n. @jimmidyson I'm aware of the issue. I'll try to have it sorted out by EOD\n. @k8s-bot test this\n. LGTM, feel free to merge.\n. xref: http://stackoverflow.com/q/39890410/1837431 with more detail\n. We will publish a v0.24.1 with the fix, probably in the next 1-2 weeks.\n/cc @derekwaynecarr \n. The log reports that this is a fatal error, but it looks like cAdvisor still starts up. Are you actually seeing any issues from this, other than the scary log message?\nIt looks like a segfault from deep in the network package (probably something wrong with the glibc version we patch into alpine), but if the panic is recovered somewhere, it should only affect rkt, which it doesn't look like you're running with.\nAlso, what OS / distro are you running on? I couldn't reproduce on my local workstation.\n. I'd be surprised if that error was contributing more than a couple seconds (though I could be wrong). Do you have a lot of containers running? If there are a lot of containers running on the system, cAdvisor can take a little while to load them before it starts.\n. Looks like the same issue as https://github.com/prometheus/alertmanager/issues/267, which includes a possible fix.\n. opt-in SGTM. Add a list of handlers to install to the manager interface, and maybe replace the --docker-only flag with a flag that takes a list of handlers (raw, docker, rkt for now).\n. ok to test\n. @k8s-bot test this\n. LGTM once you fix the typo and squash the commits.\n. We have done manual profiling of cAdvisor, along with some tuning as part of scalaing Kubernetes. However, for Kubernetes we were only interested in the container manager performance, since we don't run the full standalone version. We decided we could meat our performance goals by lowering the resolution of collected stats. Since most of CPU time is spent scraping metrics, if you decrease the scraping interval from 1s to 10s, it will roughly cut the CPU usage by 90%.. For some reason I can no longer reproduce this issue, and I'm curious whether you see it in a clean environment, e.g.\ndocker run --rm -w \"/go/src/github.com/google/cadvisor\" -v \"${GOPATH}/src/github.com/google/cadvisor:/go/src/github.com/google/cadvisor\" golang:1.6.3 make\nI think the underlying issue is real, whether or not go vet detects it. So lets proceed with this change either way...\n. Oops, I guess 1.7.2 isn't out yet. Let's go with 1.7.1.\n. Now that Kubernetes is requiring go1.7, can we merge this as is? We can update the local builds in a follow up.. From your fixes it looks like the test was pretty broken. How come it was passing though?\n. This is fixed in cAdvisor 0.24.1 (docker run google/cadvisor:v0.24.1). I'll update latest to point to that release soon.\n. Docs only - can skip tests.\n. 1460K + 237904K + 1735816K = 1975180K = 2022584320\n2282536960 - 2022584320 = 259952640\nSo it's off by about 250Mb. Could you post the output of the summary API on that node? (localhost:10255/stats/summary)\n. And the output of free at the same time.\n. LGTM\n. See also https://github.com/google/cadvisor/issues/879\n. I think the best solution is to move away from gcharts to an opensource alternative. chartjs looks nice, but I haven't spent much time looking at the options.\n. For a node or container? If a container, a percentage of what? You can find available CPU numbers in the MachineInfo and CpuSpec (under the ContainerSpec). I suspect this is an issue with docker 1.3. We don't test with anything older the 1.9.\n. Nice! This LGTM, but I'd like to get input from a few more people who understand the OOM pathways better than I do.. @vishh - Could you take a look at this?. Looks like an oversight. We'd welcome a PR :). @k8s-bot ok to test. Please squash your commits, and make sure the email address matches your github account.. > I can access the URL without any authentication.\nWhich URL? Can you access the UI?. I assume you've verified the browser isn't caching the credentials? (what does curl localhost:8080/containers/ give you?)\nSee https://github.com/google/cadvisor/issues/1621 - I don't recommend you use the cAdvisor auth.. I think that's a reasonable fix until we get https://github.com/google/cadvisor/pull/1576 in. The rest API includes CpuInstStats for cpu usage of containers & node: https://github.com/google/cadvisor/blob/master/info/v2/container.go#L280. Try http://16.103.209.61:8081/api/v2.1/stats/<container_name>, or http://16.103.209.61:8081/api/v2.1/stats/?recursive=true to get all containers.. @k8s-bot test this. Superseded by https://github.com/google/cadvisor/issues/1566. Thanks!. @k8s-bot ok to test. @dashpole is investigating the test failure. @k8s-bot test this. Hmm, for some reason this pulled in a bunch of other commits when I tried to rebase on master. I think you'll need to rebase manually.. Yes, our test infrastructure is running with go1.7.1 now.. Still failing the check. When I rebuild locally it only changes the year, so I'm not sure why _pagesAssetsHtmlContainersHtml changed for you.. @dashpole do you know the answer?. Is https://github.com/kubernetes/heapster/issues/1438 the same issue?. The CLA bot is going to be unhappy with the multiple committers, but since we'll need to squash before committing you can ignore it for now.. @k8s-bot ok to test. @joeyasperger needs to chime in that he signed the CLA. Actually, I see he already signed off on the other PR. I'll manually approve it.. Please do a rebase, then I can merge.. Accuracy increase is great, we should definitely push forward with this, but I'm a little concerned about the slowdown. If I'm reading the cAdvisor code correctly, it looks like we might be running a separate FSHandler per-container, all scanning the same rootfs. If we only run a single handler scanning the rootfs, this would easily overshadow any performance downsides of this change (although it's still worth benchmarking). Would you be willing to pursue this optimization?. > My reading of the code is that the fshandlers are created per-container and apply to the read/write upper layer, so the directory is unique per container and can't be so easily deduped.\nYou're right, I misread. Thanks for clearing it up!\nHow about only using the map if Nlink > 1?. Take a look at https://github.com/golang/go/issues/16399, in particular the solution for goimports: https://go-review.googlesource.com/c/25001/11/imports/fastwalk.go.  Unfortunately that's not exported, but we could just clone it if it's sufficiently better. Another possible improvement here: https://gist.github.com/YuriyNasretdinov/a68b6a997216103b13ea0baa4204230f. I think at least volumes will show up as a separate device. @k8s-bot test this. Is it possible the keys just expired? I feel like we've run into this before, but I don't know what the resolution was.. This error (ENOSPC) comes from the inotify_add_watch syscall, and actually has multiple meanings (the message comes from golang). Most likely the problem is from exceeding the maximum number of watches, not filling the disk. This can be increased with the fs.inotify.max_user_watches sysctl, but I would investigate what else is creating so many watches first. How many containers are you running?. 737 containers is a lot. Take a look at this SO answer: http://unix.stackexchange.com/a/13757/68061\nI'm curious what the result of the suggested command is:\nsudo find /proc/*/fd -lname anon_inode:inotify |\n   cut -d/ -f3 |\n   xargs -I '{}' -- ps --no-headers -o '%p %U %c' -p '{}' |\n   uniq -c |\n   sort -nr. @k8s-bot ok to test. Thanks!. Duplicate of https://github.com/google/cadvisor/issues/1401. The suggested fix is in the version you're running (v24.1), but based on your log, I don't think it worked :disappointed: \nThe symptoms look identical, so I suggest continuing the discussion on the linked issue.. It depends on whether it would be an update to the existing elasticsearch\nplugin, or a new one. I was blocking new plugins, but not changes to\nexisting ones.\nOn Thu, Apr 20, 2017 at 9:19 AM, David Ashpole notifications@github.com\nwrote:\n\n@timstclair https://github.com/timstclair should this wait on #1458\nhttps://github.com/google/cadvisor/issues/1458?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1597#issuecomment-295798666, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ADHtaA4RwCZW9zPK1N6nfMuTUuj0-Rcdks5rx4WKgaJpZM4MGxZn\n.\n. That's the convention, but it's up to the image maintainers to push the updated image to both tags. I checked the golang image on dockerhub, and it looks like 1.7 follows the latest patch version: https://hub.docker.com/_/golang/. What were you trying to do with this PR?. This is a breaking change, not something to casually change. I don't like that those flags are overloaded for different storage plugins, but fixing it would require a larger overhaul.. This error is expected when running on a non-GCE host. If you would like to add a comment to that effect I would accept that change.. > The error can still be recorded.\n\nIt is being recorded, but at the info level. Since this is not an actual error from cAdvisor's perspective, I want to keep it as an Info log.\n\nCould you please make it more clear\n\nI meant that you could add a comment about why it's expected. Not required.. LGTM. Please add a link to the original PR being cherrypicked into this PR description. As long as there aren't any other changes that need to be picked into v24 now, this LGTM.. Duplicate of https://github.com/google/cadvisor/issues/1481. This isn't currently supported, but we would welcome a change to add support. Maybe there's an API in the docker client code that we can leverage?. The handler may no longer be necessary in the current implementation, but the advantage to the interface approach is it leaves the code open to future implementations of non-standard FS types. This may be over designing for the future, but one option is to make the utility methods, but keep the interface, and just have the implementation delegate to the utility functions (a lot of the ContainerHandler implementations work this way). I agree with David. I think this metric is more runtime specific, which is something we're trying to move away from. This metric makes more sense if cAdvisor is a general container monitoring solution. It makes less sense if we think of it as a resource monitor. I'm not sure we've concretely defined the scope of cAdvisor anywhere.. @k8s-bot ok to test. /test pull-cadvisor-e2e. Is there any kernel documentation or paths you can link to here to make this easier to verify, and easier to understand/modify in the future?\nSame below.\n. Why move this out of the function? Personally I find it more readable if I can see the regexp right where it's used (and I don't think this should be used outside that function).\n. Maybe change this to a raw string so you don't need to escape the backslash?\n. (nit) space after //\n. Also this comment reads a little weird. Maybe rephrase to:\n// TCP state is the 4th field.\n// Format: sl local_address rem_address st tx_queue rx_queue tr tm->when retrnsmt  uid timeout inode\n. If this is not OK it means you encountered an unexpected state. I think it's worth logging an error or warning if that happens (although that could generate a LOT of log spam if the file is corrupted, so it might be good to rate limit to a single log entry per call)\n. Does this regex optionally start with ::, or is that supposed to be a non-capturing group? If the latter, it's backwards:\n(?:re)\nAlso would prefer to move this closer to where it's used.\n. Ah, makes sense. Maybe move the declaration to just above the function in that case? Also, consider making the string const.\n. Does this compile correctly? I would expect that { needs to be escaped.\n. (nit) raw strings\n. I wonder if + should be \\s+?\n. Actually, same with \\t (use \\s)? Might be more robust.\nSame below for all these.\n. escape decimal? \\.\n. here too: \\.\n. Unused? Did you mean to remove the lines where this was used below? Also, consider differentiating the variable name more from CpuClockSpeedMHz\n. Why are these pointers?\n. {1} unnecessary\n(optional) [0-9A-Za-z_] can be replaced with \\w, although I'm not sure that's more readable.\n. This regex should use a raw string too.\n. Yeah, makes sense.\n. Actually, maybe just combine this with CpuClockSpeedMHz? Something like:\n`(?:cpu MHz|clock)\\t*: +([0-9]+\\.[0-9]+)(?:MHz)?`\n. This whole loop could use some cleanup. I think it would be much more efficient and readable to switch on strings.TrimSpace(line[:strings.Index(line, \":\")-1])\nThat's sort of a separate change, so up to you where / whether you want to address it.\n. I think using a negated character set is prefered to lazy matching. Also, the .* on either end isn't doing anything except slowing this down.\n. Can you remove the .* at the beginning and end too?\n. I'd prefer not to duplicate these messages. Move these constants to validate.go, and use in both files.\n. s/desction/description/\n. Ditto re: duplicating messages\n. I like to put the test case number in all error messages, e.g. \"[%d] ....\", i, where i is the first value returned by range. Same applies to messages below. This makes debugging a failing test case much easier.\n. Can we separate request handling from conversion? (At least separate files). E.g. something like https://github.com/google/cadvisor/pull/921/files#diff-aac76d5d60dea772214f3f48e0b21705 (might make more sense in a separate PR).\nLonger term, I think we should switch to a model closer to the k8s API, with an unversioned internal model & versioned client APIs.\n. Aren't most of the DiskIo stats duplicated in DiskStats? Can this field be removed?\n. Consider calling this ReadMillis, to make the unit explicit. Same below.\n. Do we still need per-device statistics per-container?\n. (nit) make the 2 log messages more consistent\n. (nit) s/0/len(conts)/\n. time.Time is for a timestamp, not a duration. This could be time.Duration though. I agree that overloading Time in the field name is a little confusing.\n. Right, but isn't this also at the node level?\n. Why is this required for a reference, but optional for the spec? I think these should be consistent, or at least the Reference should have weaker requirements than the spec.\n. I don't think these are common flags. They should be in the kafka package (e.g. like bigquery)\n. 2016 :)\n. (nit) move this down to where it's used (just before the if)\n. This method name confusing to me. Consider changing it to something like createDetailSpec, getDetailSpec, or infoToDetailSpec.\n. This is basically the exported API, right? Would it be useful to put this in its own package & export it, so that go kafka consumers can import the type?\n. Document the format of the timestamp. Can you use time.Time instead?\n. Microseconds is a bit of an unusual unit - is that a Kafka thing?\nAlso, for clarity, prefer:\ntimestamp := stats.Timestamp.UnixNano() / time.Microsecond\n. Can you move this block to a shared utility function? Something like GetPreferredName. It's non-obvious to me that in practice Name is usually the ID, and Aliases[0] is a more accurate representation of the container \"name\".\n. Is there a reason to ignore the error here? I think at the very least you should log it.\n. This if is unnecessary, as you return nil below.\n. AddStats(..) should not block. Can you just use the kafka.AsyncProducer instead?\n. I'm surprised that go allows this, since new is a built-in. Since I'm confused as to what function new actually refers to in this file, I'd prefer to call this something like newKafka or newDriver\n. or newStorage :) You can just inline newStorage in this method.\n. This should be a log statement (probably info level 4).\n. Just log brokers, rather than joining on the same character you split on.\n. I see, that should probably be fixed. But, consistency trumps, so I'm ok with what you have for now. Let me know when you're ready for me to take another look.\n. Good call, done.\n. What do you mean by \"essentially breaking the existing API\"? I'm trying to change the minimum in this PR since it's blocking the stats API implementation, and also since I think you've already done the necessary work for this in https://github.com/google/cadvisor/pull/1035? Is there a reason not to wait for https://github.com/google/cadvisor/pull/1035 to remove the pointer fields?\n. time.Duration is just a alias of int64, so these should still be pointer types.\n. While I like this change, I wonder if it would be better to split it into a separate PR? Both because it seems a little unrelated to the disk stats, and we could make sure the disk changes are not blocked while dealing with merge conflicts etc. from the API change. If you think it's easier to keep it in this PR, I won't stop you...\n. No, I wasn't planning on it. This just adds a new method (GetContainerInfoV2) which will be called from kubelet for serving the new API at /stats/summary. I do not intend to make any changes to the other /stats/... endpoints.\n. > I really wish we separate the internal API from the wire format soon.\nAgreed. I think this issue is asking for that? I'll try to prioritize it after the 1.2 freeze.\n. (nit) 2016\n. As mentioned offline, go with whatever you think will be easiest in terms of unblocking the critical parts. If you think you can get the critical part merged faster by splitting this up, go with that. Otherwise, I'm fine with leaving it as is.\n. (nit) can you rename this file to conversion.go so that git picks up the diff correctly?\n. I feel like this should be in a separate package (docker). That will make it easier to match the plugin style used elsewhere (storage), and help to keep runtimes dependencies isolated.\n. These flags don't seem docker specific\n. should this be set in Start() rather than here?\n. (nit) prefer: } else if httpDigestFile != \"\" {\n. I think it would be better to use FindBox here and return the error.\n. No hostnotes for this one?\n. is this the same as e2e-cadvisor-ubuntu-trusty-docker1-10? Which is correct?\n. I think it would be more readable if you make this list a variable\n. I think ! -z is equivalent to -n?\n. I think you need to call exit after this\n. Done: added community-assigned label.\n. (nit) flag adds the default value into the usage string, so you don't need it in the description (it's only needed when the default is filled in later).\n. (nit) flags should be lowercase. Unfortunately there doesn't seem to be a convention for separating words with _ - or no separator.\n. Should this be RktRoot for consistency, or does this represent something different from DockerRoot?\n. What if instead of hardcoding this, we created an FsHandler abstraction, and create a docker & rkt implemention in the respective Container directory, and have each implementation register itself? Then here you just loop over all the registered FsHandlers and call handler.AddImagesLabel?\n. I don't think Manager is the right level of abstraction for splitting container runtimes (there's a lot of duplicated code here). I know it currently integrates some docker concepts, but I'd rather see those concepts abstracted out. I think we should try to stick with ContainerHandler as the runtime abstraction, and move the problematic points of the manager to the ContainerHandlers.\n. Ok. I started my review at too fine a grain (sorry). We can ignore this fore now.\n. Ok. I think it would be better to try and tackle this from the ContainerHandler interface before creating a second manager though. BTW, if you're still developing this PR, can you add [WIP] to the title?\n. I'm not too familiar with awk, but I'm sure there's a native go way to do this?\n. Alternatively, go has an ssh client. The advantage to using the go client is you wouldn't need to open and close multiple connections, so it seems like it would be less flaky? Although if you weren't using multiple connections, the connection would be open for longer, so I'm not sure which would be better.\n. I think scp takes different options from ssh - maybe this should just be RunCommand?\n. If you do that, you could drop the cmd argument in RunSshCommand, and just always use the 'ssh' command. \n. I see, I guess we can just rely on the error checking if invalid options are passed in :)\n. I think we should log these errors at Error level (here: https://github.com/vishh/cadvisor/blob/timeout-du/container/docker/fsHandler.go#L99 - I can't figure out how to comment there on GH)\n. (nit) prefer require.Equal(t, 1, len(containerInfo))\n. If you make ignoreMetrics a map[MetricKind]bool, then these checks become cleaner. E.g.\ngo\nif !ignoreMetrics[container.DiskUsageMetrics] {\n. Consider declaring a MetricSet type here to clean up method signatures.\n. Delete this comment\n. (optional nit) I think this would all read a little more naturally if you inverted the logic (e.g. includedMetrics or enabledMetrics). \n. Nevermind, I realize that this isn't an inclusive list. In that case, I have a slight preference for disabledMetrics but leave it up to you.\n. Use a switch statement. Or better yet, check the value against a whitelist (disk, network, tcp), and just cast the value:\ngo\nif !whitelist[ip] { return error }\n(*ml)[container.MetricKind(ip)] = true\n. I don't think this should be an error. For instance, what if someone wanted to override the default value?\n. what does ip mean in this context?\n. unused?\n. Wouldn't it make more sense to return an error here?\n. That's a lot of returns! Either name them, or just return an Fs type\n. Use the constant types here\n. I'm fine with the complex type, but just wanted to clarify my suggestion. Using a bool value that's always true works because the zero value is a bool (false). I recognize it can be a little ambiguous, but I think it's ok for contained usages. Since the set is used across files, I think your approach is good.\n. (nit) use a MetricSet?\n. would you mind documenting what the returns are here?\n. (nit) extra blank line\n. A more go-style name would be ContainerHandlerInterface, or just ContainerHandler\n. (nit) avoid \"self\" or \"this\" names - prefer something like \"handler\" (https://github.com/golang/go/wiki/CodeReviewComments#receiver-names)\n. (nit) space after //\n. I don't think this is rkt specific?\n. (nit) change this to: spec.HasNetwork = spec.HasNetwork || len(nd) != 0\n. (nit)spec.HasFilesystem = name == \"/\" || externalMounts != nil || self.HasFilesystem(). Actually, just delete these comments - they don't add anything.\n. What if we keep this private, and renameGetContainerHintsFromFiletoGetContainerHintswith no arguments (use the flag value)?\n. The test could set the flag value instead. This is pretty nit-picky though.\n. Add TODO comments for cases like this (makes it easier for reviewers to understand)\n. (nit) const\n. (nit) most of these comments are unnecessary\n. What isSJP?\n. This should probably be a warning or an error\n. delete this?\n. If there is an error closing the watcher, this will never return.\n. Should this be error level?\n. (nit) preferif _, err := parseName(name); err != nil {`\n. or alternatively:\n_, err := parseName(name)\nreturn err == nil, err\n. Can you add a comment with a few example paths here?\n. (nit) Go style prefers // style comments (https://golang.org/doc/effective_go.html#commentary) in most places.\n. (nit) Be consistent with your exports. It looks like all these helpers are not used outside the rkt package, so should not be exported.\n. I think this should be debug level (V(4))\n. I think this should also be debug level (V(4))\n. error level?\n. I'd prefer that this was explicitly passed in from the main module rather than implicitly set here.\n. Yeah, sounds good.\n. Rather than calling flag.Set, just add TCP to the default metrics on line 58\n. Prefer:\nreturn self.name == \"/\"\n. This seems like a bit of a hack, what if instead we add the network metrics to the set of ignoreMetrics we pass for non-root containers?\n. I see. I guess my comment about adding network metrics to the ignoreMetrics is optional in that case.\n. (nit) remove this\n. (nit) use .Equal( here\n. I don't understand... why are the expectations different here? What changed?\n. Ah, nevermind - I see you switched extraDir & rootfs. Can you add a comment so that's more clear?\n. (nit) prefer baseUsage, totalUsage, since I had to go lookup what the returns were.\n. (nit) move the comment too.\n. This test isn't really doing anything anymore, since the ignoreMetrics flag is never actually added. I think the best way to solve this is by moving the flag declaration to an init() function, or an addFlags() function that you can call here.\nAlso, consider adding a flag.Parse() and verifying that the assertion still holds.\n. I think you need to make a deep copy of ignoreMetrics, or it will affect all containers.\n. Your Copy is really a merge, so this isn't actually creating a new copy. I'd just update the copy function to initialize a new map, and return it.\n. I don't think the main README is the right place to describe every storage option. I think we should add a new directory in the docs folder called storage, with a README that includes snippets like this one, as well as more detailed documents (such as atsd_install.md).\n. I would drop this paragraph. It's not really specific to ATSD\n. Can you add the image to this repo?\n. It looks like this document is the same as this one? If so, drop the link.\n. (nit) drop sudo here and below. If users need it they can add it, but we shouldn't encourage unnecessary sudo use.\n. I would say something like, \"If ATSD is not already running, you can start it using docker: ...\"\n. Out of curiosity, what are all these ports used for?\n. I would make these values bash variables to make it more clear that the user is expected to configure them (e.g. -e ATSD_USER_PASSWORD=${ATSD_USER_PASSWORD})  (and reuse the same variables below)\n. Why do you need to adjust the housekeeping interval?\n. Once this PR goes in you can use google/cadvisor:latest here.\n. I don't think this style comment is valid in toml?\n. drop this\n. I would add a separate header for this section, and say\n\nAlternatively, cAdvisor can read the ATSD configuration from a configuration file:\n. Either these images need to be added to this repo, or you can drop them from this document and link to an external page (for more info...)\n. s/key/flag/\n. (nit) prefer italics to quotation for notes like this.\n. Can you rephrase this to:\nThe built-in Overview, Disk Detail, Host and Multi-Host visualization portals can be used to identify bottlenecks in a microservices infrastructure.\n. (nit) 2016\n. (nit) move stdlib libraries to first group\n. (nit) space after // in comments, same everywhere\n. What does this do?\n. (nit) use flag.Int for all int values\n. Does this really need to be 64-bit int? That seems awfully high...\n. Why is this needed?\n. Drop unused parameters\n. (nit) Add a line break to separate from flags.\n. (nit) prefer: make(deduplicationParamsList)\n. This won't work correctly since the flags haven't been parsed at this point. You should move this to new\n. (nit) declare a type for this\n. Is this necessary?\n. Check that groupValues is the expected length\n. return the error instead (same below)\n. Can you use a type alias instead of a struct?\n. (nit) unnecessary blank line\n. (nit) s/argDbBufferDuration/bufferDuration/\n. return the error instead of panicking, same below.\n. Is there a reason for using uint64 time rather than time.Time?\n. What do you think about collapsing these three maps into a single map[string]sentStatus{....}, where sentStatus (can you think of a better name) is a struct containing these 3 fields per-container?\n. How about just adding the cadivsor config to the struct instead of each field?\n. Call this variable something different so you don't shadow the storage package.\n. unused?\n. I think this would be more readable if you iterated over a map instead. I.e.\n\nfor metric, dataType := range map[string]atsdHttp.DataType{\n  containerCpuHostUsageSystemPct: atsdHttp.FLOAT,\n  containerCpuHostUsageTotalPct: atsdHttp.FLOAT,\n  ...\n} {\n  self.innerStorage.RegisterMetric(\n    atsdHttp.NewMetric(metric).SetDataType(dataType))\n}\n. (nit) extra blank line\n. (nit) group with cpuSeriesCommands\n. Are these requests batched? Would it send fewer requests if you append all the commands together and send with a single method call?\n. I think you should call self.innerStorage.StopPeriodicSending() here\n. Invert the firstTime logic (e.g. hasSentEntityTags), then you don't need to check ok, just: return !hasSentEntityTags[containerRefName]\n. (nit) inline this\n. (nit) inline this and the one below.\n. stdlib should be in the first import group\n. (nit) 2016, same for the rest of the files\n. (nit) use time.Millisecond.Nanoseconds() instead of 1e6\n. I don't think this these need to be floats. Inline the float conversion if you really must.\n. Aren't these the same as above? Just do metricsMap[containerCpuHostUsageSystemPct] = metricsMap[containerCpuUsageSystemPct]\n. (nit) drop the else make dedup the return\n. ditto re: imports (same everywhere)\n. unused?\n. (nit) extract a helper function for this\n. (nit) no need to set cap here\n. (nit) count := 0\n. (nit) move out of if\n. just do tags[containerAliasEntityTag] = a\n. Unused?\n. Do you want to tag the network stats with the interface?\n. By using the same value for every field you're not really testing much other than that each field is set.\n. (nit) const\n. Consider: I like to avoid methods with more than 2 return parameters. What if we change the semantics to return if the factory returns an error? Then we can create an \"IgnoreError\", and simplify the logic here to just three cases: Return if err != nil || handler != nil, and continue if both are nil. You'd also need to update the factory methods to not return errors in the \"expected\" cases (e.g. in docker don't return an error when inspect fails) - which I think makes more sense anyway...\nWDYT?\n. I'm worried that the ignore handler  will unnecessary complicate things, especially if the results of some of those functions are assumed to be non-nill if the error is nil.\n\nbut doesn't this prevent NewContainerHandler from returning other errors that might occur?\n\nIt means we'd change the behaviour of an error, and say that errors are returned rather than logged & ignored (in NewContainerHandler). If the error is bubbled up to manager the behaviour is still consistent (manager.go#L774). Tracing up through the code, I think this is actually preferable. For example, consider the case of creating the root container (manager.go#L261): ignoring the container in that case should be an actual error, and not silently ignored. Everywhere else it will simply be logged (e.g. \"E: Failed to create existing container: foo: ignoring container foo\").\n. It's not necessary, but I think it makes it easier to parse the code (and figure out what docker refers to). We do it in some other places, but I can revert it if you prefer.\n. ACK. I'm planning on sending a follow up for this.\n. I didn't mean to include it in https://github.com/google/cadvisor/pull/1190 so I removed it here.\n. Yeah, I'll try to clean this up.\n. It's used by the Exists() function to determine whether the container still exists. We might be able to switch to the same method that raw & rkt use:\ngo\nfunc (self *rawContainerHandler) Exists() bool {\n    // If any cgroup exists, the container is still alive.\n    for _, cgroupPath := range self.cgroupPaths {\n        if utils.FileExists(cgroupPath) {\n            return true\n        }\n    }\n    return false\n}\nBut I'm not sure what the full implications of that are...\n. How long does this take? If it's slow, we might want to make it a separate rule, since UI changes are relatively infrequent.\n. Alternatively, you could update the script to only run if the modified time on source files is more recent than the output files.\n. nit: delete trailing comma\n. delete trailing comma, and below\n. nit: missing semicolon, and below\n. nit: missing semicolon\n. semicolon\n. semicolon\n. ;\n. ;\n. ;\n. ;\n. Could you make all the changes to this file in a separate commit so that git sees this as a move and retains the file history. I added some comments for js problems, feel free to ignore them and I can do a follow up fix if you'd like.\n. Thought about this some more, decided to go with it.\n. Use defaultRktAPIServiceAddr here\n. I think you should use DialTimeout with a reasonable value\n. The connection should be closed\n. Just do return nil, fmt.Errorf... instead\n. Use a different error message here so the failures can be distinguished. Maybe \"Unable to dial rkt api at localhost:15441\"\n. Any reason you chose a different timeout from the one below? If not, can you create a timeout const to use in both places instead.\n. nit: our bash style is to put do / then on the previous line with a semicolon (e.g. for f in $FOO; do).\nSame for if below.\n. (nit) exit $?\n. It's not clear from the description that this overrides listening on TCP. Maybe we should deprecate listen_ip and port, and switch to a similar approach to docker, a list of paths / URIs to listen to (e.g. --listen=tcp://localhost:8080,unix:///var/run/cadvisor.sock)\n. Drop the true == everywhere. Same below.\n. stray i\n. Just do return strings.Contains(arch, \"arm\")\n. I think you need to add && go install github.com/jteeuwen/go-bindata for this to be used.\n. (optional) This is already implemented by \"github.com/blang/semver\".Version.Compare\n. unnecessary?\n. It's a debateable topic, but there are some pretty good arguments here: https://www.reddit.com/r/golang/comments/2q5vdu/int_vs_uint/\nIn your case the value must be > 0, so you already need to do some validation (since uint can be 0), so you might as well use an int.\n. We should just read this from rootfs + 'etc/hosts'. \n. IIUC the only reason for wrapping the cadvisorParams here is for toml.DocdeFile? In that case, just use cadvisorParams here, and create a temporary wrapper in the block where you call DecodeFile:\nwrapper := struct{ Cadvisor *cadvisorParams }{ &cadvisor }\n...\n. I meant is the cast necessary? Does the Sprint function format it any differently without the cast?\n. Ah, makes sense.\n. Removing flags is a lot harder than adding them, so if we want to use a --listen flag, I'd prefer not to add a --listen_path first.\nIn other words, yes, would you mind adding it in this patch?\n. Yeah, we should maintain backwards compatibility. Thanks! \n. Capitalize the \"search\" in ElasticSearch, same below.\n. nit: Capitalize the \"search\" in ElasticSearch\n. Can you add a comment about the optional flags too:\nargTypeName      = flag.String(\"storage_driver_es_type\", \"stats\", \"ElasticSearch type name\")\n    argEnableSniffer = flag.Bool(\"storage_driver_es_enable_sniffer\", false, \"ElasticSearch uses a sniffing process to find all nodes of your cluster by default, automatically\")\n(The comment on the enable_sniiffer flag is confusing, since it's disabled by default)\n. nit: make this a full sentence, e.g.\n\nFor a detailed tutorial, see docker-elk-cadvisor-dashboards.\n. Leave this the docker NetworkMode type, then you can use networkMode.IsContainer() in needNetwork\n. Can you reword this to something like, ElasticSearch can use a sniffing process to find all nodes of your cluster automatically. False by default.\n. Creation events for containers that were running before cAdvisor started.\n. Yeah, I was playing around with that. It adds a bunch of complexity for relatively little gain, but I'll think about cleaning it up and sending a patch.\n. This needs to be rebased to use the docker-engine api.\n. Can we put this in the container/docker package instead? I was trying to consolidate some of the docker logic in https://github.com/google/cadvisor/pull/1221\n. (nit) by convention error messages start with lowercase (check_errorf.sh)\n\nSame applies everywhere.\n. You should probably check the length of Fields to be safe\n. Reuse the first call to Fields\n. Missing the log statement.\n. I'd rather not. I'm not positive there aren't any other times that events could arrive out of order, and I think the \"common case\" check makes the cost of this very low.\n. Make this a function with 2 parameters: $INPUT_DIR and $OUTPUT\n. nit: TMP=$(mktemp) and use that instead of ${TMP_ASSETS_OUTPUT}\n. Quote parameters, I think you'll lose your styles input otherwise. Same below.\n. mktemp ?\n. (nit) return nil instead err for clarity\n. Done. I moved it to info/v1 for now, since that is our \"internal\" version at the moment. Once I get time to cleanup the versions properly, I'll add it to v2 as well.\n. Because it already existed :)  I moved the whole utils/machine package to a top-level machine package (we should reconsider where this package should live when https://github.com/google/cadvisor/issues/1250 is implemented)\n. How does this play with https://github.com/google/cadvisor/pull/1253? Is it still able to reuse the intermediate build results?\n. Would it make sense to move some of this (e.g. godep install) into a Dockerfile? Maybe add another make rule to build the docker image first?\n. Indeed! I'll send a fix, thanks for catching this.\n. Fixed in https://github.com/google/cadvisor/pull/1259\n. I don't think it is. I think you need to mount the $GOPATH/pkg directory for it to work.\nWith your changes (irrelevant output omitted):\n```\n[(c262eb7...)] cadvisor: time make build\n\n\nbuilding binaries using docker\n  cadvisor\n\n\nreal    0m14.201s\nuser    0m0.066s\nsys 0m0.032s\n[(c262eb7...)] cadvisor: time make build\n\n\nbuilding binaries using docker\n  cadvisor\n\n\nreal    0m14.279s\nuser    0m0.073s\nsys 0m0.027s\n```\nWithout your changes:\n```\n[install] cadvisor: time make build\n\n\nbuilding binaries\n  cadvisor\n\n\nreal    0m6.658s\nuser    0m13.909s\nsys 0m1.622s\n[install] cadvisor: time make build\n\n\nbuilding binaries\n  cadvisor\n\n\nreal    0m0.781s\nuser    0m0.343s\nsys 0m0.188s\n```\nNotice how the second build is super fast once everything is cached.\n. Hmm, what if we create an _output/pkg directory in cAdvisor which we map in instead?\n. I don't understand this comment.\n. Use a for atempt := 0; attempt < maxReopenAttempts; attempt++ style loop (I don't think you're incrementing attempt)\n. delete this?\n. Stop should work here.\n. This is a change in behavior, as we won't get old events.\n. I think this check is redundant.\n. Things from days ago are probably not relevant, but events from seconds ago might be. Either way, I think it's out of scope for this PR (feel free to leave a TODO)\n. Duplicated\n. (nit) Capitalize Kafka everywhere\n. optional: consider simplifying the flag name to storage_driver_kafka_ssl\n. This should return the error instead.\n. nit: I prefer to only log where there error is actually captured, other wise you end up with the same error being logged multiple times. Instead, consider adding some context to the error (e.g. fmt.Errorf(\"open failed on %s: %v\", t.filename, err))\n. Why don't you move this whole block (here to the end) into a function, that way you can just use defer oldfile.close() here.\n. nit: Let's make this V(4)\n. t.file could be nil here. Probably best to just let the watchFile routine handle closing.\n. I'd prefer to close the file here, and fix the hot-looping behavior. I think you should modify Read to return EOF in this case.\n. I think there's a lot that needs to be closed here, and similarly for returns below. I think this can all be refactored into several functions and rely on defers to close everything.\n. Ok, update the comment with that explanation? :)\nI think the comment is not a proper sentence.\n. I still don't like having \"detection type\". It just feels messy and not the right abstraction. Why is it necessary? I don't think container factories should care how the container was detected, their job is to just create the handler for it. It should be up to the manager (or a child module) to ensure that the correct factories are hooked up to the right detection sources.\n. I think the raw sub-package is probably unnecessary (can just be in watcher?). Also, can we make watcher a sub-package of manager?\n. nit: just use if binVersion.LT(minVersion) {\n. switch on eventType\n. just use MustParse, since it's hard coded (add a unit test)\n. Because I wanted to dynamically create the usage message. Options for making it more consistent:\n1. Keep storageDriver a *string, and use the flag.String method\n2. Move storageDuration initiallization into init()\n3. Make a function for getting the sorted options, inline the usage creation\nWDYT?\n. Doesn't matter (I don't like go's flag library), but 1-dash seems more consistent with the other storage docs.\n. Return the error here too.\n. Went with 3.\n. This is a bit different, since it doesn't check the start line. Is the desired information ever on that first line? If not this is fine, otherwise you can keep the original loop and just check finished before receiving the line.\n. There's a separate endpoint for this: http://cadvisor:8080/api/v1.1/subcontainers (note that it's not available in 1.0).\n. What is \"Ns\" here?\n. I don't understand what this is doing.\n. Ah, nanoseconds. Drop the \"In\"? Just intervalNs\n. How does this handle 200+ containers? Perhaps it should only include the top N.\n. nit: Dedup this with the above method, i.e. function getSubcontainerChartData(subcontainerInfos, dataFn), where dataFn is a function that generates the data points.\n. I'm confused by this comment - I think that's what this PR does? I left the flag in (and marked it deprecated) in case anyone was using it (to not break the CLI), but I think we should just remove it in a couple releases.\n. I see. That makes sense, but do we need to support this case? It's already the case that if cAdvisor can't connect to docker at start-up the docker factory isn't registered (https://github.com/google/cadvisor/blob/master/container/docker/factory.go#L176)\n. I see. Please add a comment explaining this. Also, does it matter if the data points are out of order?\n. (nit) drop intervalNs from the function API, and just compute it here.\n. nit: avoid naked returns\n. EOF should be used to signal there is no more data, but in this case it is used to signal that readLinesFromFile should retry. I think it would be better to just return 0 bytes read and nil error in this case (unless err is an actual error and not EOF). The logic in readLinesFromFile  will need to be updated too.\n. nit: these named returns don't give any additional information\n. nit: return event.Mask&flag == flag\n. nit: named result again (don't stutter)\n. This will close whatever t.file was when WatchFile started, not when it finished. See defer.\nI suggest flipping the inner loop segments (open the file then watch then close -- rather than watch, close, open), and encapsulating the loop body in a function, and doing defer t.file.Close() in that function.\n. nit: count from 0\n. You have a race here on t.reader with the watchFile goroutine. Please add a unit test to catch this.\n. I have some concerns around how override is handled, but it might make more sense once the rkt watcher is added. Can we just remove the override event & method from this PR, and I'll review it when it's actually used?\n. I'm pretty sure this will deadlock. Please add a test for this case.\n. Can we s/Subcontainer/Container/ everywhere? I don't think subcontainer is really accurate everywhere, and I think it just adds confusion.\n. nit: I think you can just call these methods Start and Stop\n. It's not the same, because you're doing it in a loop. The calling code does this:\ngo\nquit <- nil\nerr := <- quit\nWhich will catch the first result, but if there are multiple watchers will get stuck.\n@yifan-gu I totally agree, in particular I think the Stop() needs to be fixed. @sjpotter it's up to you whether you want to tackle that, but either way this code should not deadlock.\n. Why is this check needed?\n. nit: make errors a []string, then just do strings.Join here.\n. I thought I said this elsewhere, but can't find the comment.... Can we remove overrrideContainer for now (since it's unused), so that I can review it when I can see how it's actually used?\n. just do err.Error()\n. Can we remove this?\n. Let's add it when it's needed.\n. Or maybe Type\n. It looks like this is only used in one place, why the helper?\n. (nit) name the return values to distinguish the bools\n. Check that len(splits) > 0\n. (nit) space after // everywhere\n. nit: add error context (failed to list pods: %v)\n. nit: how about \"returned %d (expected 1) pods for cgroup %v\"\n. nit: This error is a bit confusing. How about \"couldn't find Apps annotation in Pod matching %v\"\n. Should this be an error? Are there any circumstances where this would be expected?\n. (nit) add error context\n. Why is this check necessary?\n. nit: add some context\n. nit: 2016\n. Wrong package comment\n. nit: this should be part of the previous import group (non-core/non-cadvisor)\n. don't need _ =\n. Remove this, or move it to an extra-verbose level\n. Log the error?\n. Consider moving this case to a function (e.g. syncRunningPods) to decrease indentation levels\n. nit: space after // everywhere\n. Is this a TODO, or is it describing the next block of code? Reword the comment either way.\n. Can you use the event stream rather than polling?\n. This should be at least debug level, or remove it. If you leave it, add more context (e.g. \"Updating rkt pod %v cgroup %v\")\n. It's confusing that there's a separate event type for this that means \"add a new container and if there is already a handler replace it\". Can we just make this an \"add\" event type, and let the manager decide what action to take, given the source?\n. ditto\n. len(pods)\n. nit: move this to the constructor\n. nit: add error context\n. (nit) prefer cgroups[0] = baseCgroup\n. similarly, prefer cgroups[i+1] = ...\n. What's the cost of a list running pods call? If the cost is low, it would be nice to poll more frequently.\n. Why is it relatively common that the rkt server is unreachable? I believe this watcher is only registered if the service is initially reachable.\n. Yeah, that's way better. No idea what I was thinking...\n. (nit) Fix comment.\n. How long does this call take? This is called synchronously on every new cgroup, so at the very least we should have a reasonable timeout set. Can we cache the results or run it asynchronously?\n. nit: include name in the error\n. I think you can remove \"shouldn't have reached here\". It's an error, so it's clear that it shouldn't have happened...\n. Can you add a comment describing what this case means? IIUC this is the case of handling the pod container?\n. +1. I don't think the \"override\" logic should be part of the watcher. The watcher sees a new container, and reports an \"add\" event, and then I think the manager should have the logic to say either, \"This already exists, ignore it\" or \"This exists in raw form, replace it with the runtime-specific handler\".\n. Yes, I think 1) makes sense.\n. What do you mean?\n. What do you mean add flags to the event struct? I'd prefer for the logic to be in the manager since it actually knows which watcher sources are running. The watchers should be independent. Why would the raw handler try to overwrite itself?\n. Yeah, I guess this makes sense. Can you add a comment here explaining why this is necessary in the Rkt case?\n. s/detectRktContainers/syncRunningPods/\n. Please address this. Is there any reason Stop() needs to be synchronous?\n. nit: actually log the error :)\n. nit: strings.EqualFold\n. nit: add error context (fmt.Errorf(....))\n. actually log it :)\n. nit: add error context\n. nit: avoid ignoring error returns. I'd suggest either removing the error from the return of Refresh, or move the logging to here.\n. thinPoolUsage is 0 if there's an error. Do you still want to set baseUsage to it in that case?\n. nit: Is it necessary to Refresh immediately?\n. Since you're logging the error returned by this function, I'd prefer to just wrap the errror with this context (fmt.Errorf(\"error determining whether snapshot is reserved: %v\", err))\n. Same everywhere else in this function.\n. Just change this line to return t, err or return nil, err. Also change the return type to (*Tail, error) since the variable names don't add anything in that case.\nIMHO there are only 2 reasons to give return types names:\n1. If the return type is something like (int, int, int, int), names distinguish the different ints (though I would usually prefer a struct for this)\n2. If you need to modify a return value in a defer statement (again, this should be avoided if possible)\n. Nevermind. You may want to consider propagating the same error though (i.e. return 0, err), or was there a reason you always return EOF here?\n. Call this readerLock (if it's embedded it's less clear what it's protecting)\n. Why are you locking here?\n. Move this to attemptOpen. Just lock the reader for the whole method (I think that's safer anyway since we don't want to read from the old reader), and unlock with defer.\n. nit: shorten this to\nif t.watcher.Watch(t.filename) != nil { ...\nSame below.\n. Did you mean to set reader to nil here as well? I don't think setting the state does anything otherwise.\n. I'd prefer to call this readerErr, and have nil represent the working (opening) state. That way if there is an error, you can provide more context around it.\n. Yeah, do you think that's undesireable? I thought printing the commands that were executing was better for understanding what was happening, but I don't feel too strongly about it.\n. Done.\n. It's tied to the TODO. I actually don't think we want to do that...\n. Manually verified this works by forcing a logrotate.\n. You don't need to add a new version for this, since it's not changing exsiting behavior. Also, can we expose this as part of th 2.x API instead of 1.x?\n. I know I said to add a new endpoint, but what do you think about modifying the containersApi instead? It looks like it's missing a v2 version, so you could add the v2 version and use the RequestOptions to specify the namespace (the IdType field).\n. What are InnerArgs?\n. I think it would be good to split this file up, and put the rkt-specific code in rkt.go\n. TODO?\n. This looks unrelated to this PR? Same below.\n. This should be added to v2 as well.\n. (nit) capitalize container\n. We should also check that i+2 < len(lines)\n. Should be >=\n. Is the freenode IRC line still accurate?\n. Why +1 here? Doesn't look like the last field is used? (thinPoolDmsetupStatusMinTokens is already incremented from Root)\n. Would the fields ever be split by tabs or multiple spaces? If so strings.Fields would be a better choice.\n. nit: these don't need to be exported\n. nit: call this gceProductName\n. Let's log the error at info (V(2))\n. (nit) end in a period for consistency\n. It was a comment, but I think disabled tests should be TODOs. I'm tempted to delete this since we don't run the node tests against centos. Do you know how to run the cAdvisor tests against a remote host? I tried running against the existing host but when it tries to talk to the API it can't (firewall not open). I also tried bringing up my own centos instance to test on, but couldn't figure out how to get docker setup correctly. Thoughts?\n. The mock should be configurable. Just follow the pattern of the other methods.\n. nit: V(4) for debug info. Add more context (e.g. \"Attempting to read file at %s\")\n. This is not necessary a not found error.\n. Prefer strings.TrimPrefix\n. s/networkNode/networkMode/ ?\n. This looks unused?\n. Done (reverted Makefile changes).\n. SGTM\n. Do you mean the FsInfo struct? I think we should try to maintain backwards compatibility between releases, but I think we don't need to worry about backwards compatibility with HEAD between releases.\n. I think this should go in a FilesystemSpec struct nested in ContainerSpec. We should prioritize removing the v1 API...\n. Oops, that was meant for the container FsStats, but the same applies here too.\n. I was suggesting to move this to ContainerSpec, or better yet, create a FsSpec struct in that file with this field, and link to that struct from the ContainerSpec.\n. Sorry, you're right: there's no precedent for doing this with Machine. I added a comment above re: Container FsInfo.\n. Why did Shopify/sarama files change? I don't see any corresponding changes in Godeps.json\n. ditto RE: misc changes\n. There's conflicting information above: \"Ensure that your version of Go is at least 1.3.\" and \"Note: cAdvisor requires Go 1.5 to build.\". Could you fix it? Does the vendor directory work by default on go 1.5 or do you need to set a flag?\n. how about instead of putting this here, we add a presubmit make rule that depends on the necessary other rules? (then change the presubmit call in the jenkins script to call make instead)\n. I'd prefer make build here. I don't think fmt vet and test need to be part of this script.\n. ditto: use make build here. Are you sure the Makefile should be the source of truth for all our build commands? I'm inclined to prefer calling build/build.sh directly, but don't have any concrete reasons for it.\n. Can we just delete this file?  make docker seems sufficient for local development and uses deploy/Dockerfile instead.\n. make build includes ./build/assets.sh, so the above line is redundant. I've been meaning to split out a separate make assets rule that build depends on...\n. Again, I'd prefer to be explicit about which rule to execute (do we really need all here?)\n. As long as you promise to do it :) I'm worried that leaving all these naked make calls around is fragile.\n. Ah, that makes sense, sorry I missed that before. Do you think it's better to build cAdvisor in the docker container, like it was before? Or does it not make a difference? I'm OK with whichever you think is best :)\n. I was able to reproduce this in the old tree with:\ngodep restore\nrm -rf Godeps\nexport  GO15VENDOREXPERIMENT=0\ngodep save ./...\nI filed https://github.com/google/cadvisor/issues/1373 to prevent this in the future, but I think you can leave this as is.\n. [ignore]\n. (nit) space after //\n. @yifan-gu How should this be handled?\n. Should this default to the host IP, or are saying that \"\" implies host IP?\n. Do we want to update to a patched release instead of head?\n. Consider moving the configFile parsing to a shared method so you don't need this duplicate code.\n. Yes, I agree with @Random-Liu. It doesn't ignore the error on the base directory, but if there is an error on a sub-directory, I think continuing at a best-effort is reasonable.\n. I'm not a TLS expert, but if a certificate is provided why do we want to skip verification?\n. I think you should call this collectorHTTPClient, since the flags are specifically specifying collector certs.\n. The problem with consuming the error here is we don't know whether the value returned is valid. For instance, is it a problem to set the baseUsage to 0 in handler.go when there isn't a cached value?\n. I think there should be a common MetricConfig that has the fields shared by the prometheus MetricConfig and JolokiaMetricConfig, then create a PrometheusMetricConfig, and embed the common MetricConfig in both.\n. Typo? to be used Eg used\n. What about other runtimes (rkt)? You could use the namespace from the ContainerReference.\n. Would it be better to call this \"cpu_load_average\" to group it?\n. This function already exists here: https://github.com/google/cadvisor/blob/master/machine/info.go#L142\n. nit: name these something more semantic, e.g. minKernelVersion and minRhelKernelVersion\n. Is this just to check that kernelVersion has sane formatting? I think blang/semver already does that with semver.Make\n. Is this because 4.0.0 <= version < 4.4.0 kernels won't work?\n. I think you're looking for semver.Version.Pre\n. 2.x kernels will get picked up by the following checks.\n. nit: since you're calling Atoi on it, make the match group digits (([0-9]+)).\n. Ick. Oh well...\n. I'd actually prefer to keep it relative. You just need to make it relative to the docs directory (i.e. ../container/common/container_hints.go)\n. Should we export these intermediate steps for reuse? Or at least make the keys (image, name, id) exported?\n. > I could split up the id+name+image part and the labels+env parts into two functions we expose each? I'm also fine with exposing constants for the labels though. \nThinking about this some more, I think just exposing the constants is the way to go.\n\nLastly, actually we're doing it all wrong here by applying so many labels to all metrics. It should be only one identifier (probably id) and export another metric containing the mapping. See http://www.robustperception.io/target-labels-are-for-life-not-just-for-christmas/ and http://www.robustperception.io/exposing-the-software-version-to-prometheus/.\n\nAck. I think this is too big a change for this late in the release though. Could you open an issue with a proposal to fix it, and we can try to address it in the next (v0.25) release. Thanks!\n. I left this one since it's not configuration. If this flag is set, cAdvisor just prints the version to stdout and exits immediately.\n. This is in the main package, AFAIK it can't be imported into another program.\n. GitHub renders lists without the whitespace, but it's not \"proper markdown\", and some engines do not, so I think we should use whitespace. For this PR, I'd prefer not to touch the original commits, but I'll send a follow-up to clean up the file.\n. what does dto stand for?\n. How'd you pick 50? What happens if this overflows?\n. Yeah, I agree this is not ideal. Unfortunately this requires an API change to fix properly. Can you file an issue? Sorting + concatenating sounds good for now. Check the allowed symbols on prometheus labels to decide how to concatenate.\n. You eventually want to set this to true, right? It's fine to just do that in this PR (we frequently update dependencies while making changes that rely on them).\n. Yup. Done.\n. Yes, please. We might also consider the factory model used by the storage backends, but that doesn't need to be done in this PR. Lets keep it in a separate package though.\n. nit: errors start with lowercase\n. nit: Can we create a struct to return with this info instead? IMHO there are very few situations where a method should return more than 2 values...\n. How about extracting something like a \"BackoffPoller\" that abstracts all the shared backoff & polling logic between trackInodesUsage & trackDiskUsage?\n. nit: make constants for the default periods\n. Under what conditions would the rootfs not be set? It should probably be an error, or exit the tracking loop...\n. Actually, on further thought I'm wondering if this should be in the same routine as the du call. The advantages are that containers with very large filesystems wouldn't hog as many exec calls, and there might be less disk thrashing.\n. When I tried this, I found find $dir -xdev -printf '.' | wc -c to be about twice as fast. Might be a premature optimization though...\n. don't ignore the error\n. A timeout should be an error. Does killing the findCmd cause Wait() to return an error?\n. nit: move the error handling to just below the function that returns the error.\n. delete this\n. Is the fields call necessary?\n. Does it make sense to track inode usage for devicemapper? If not we should skip it.\n. I think this will double count inodes on base layers shared across containers. That's probably what we want, but it means killing a countainer will not reclaim all it's reported inodes.\n. shouldn't numFiles == inodes? Under what circumstances would it be different?\n. If we skip inode collection for some storage drivers, then we should probably leave InodeUsage nil here when it's not collected.\n. If that's the case, shouldn't it be zero for a new container? That's not what I'm seeing.\n. I don't think so. If you're taking the address of a variable, it will always be non-nil. The value might be zero, but if possible we should distinguish between 0 and not set (nil).\n. nit: no need to specify this (values default to 0)\n. nit: space after //, start comments with a Capital.\n. Do we need a separate pool for each of these? On the one hand, if one operation is significantly slower, it prevents it from hogging the tokens. On the other hand, the operations happen in sequence, so the one operation will be blocking anyway. I'm leaning towards a single \"exec\" pool.\n. I think it would be better to assign a bytes.Buffer to stdout and stderr. I'm not positive, but I tihnk the process could hang if the pipe fills up, waiting for it to be drained.\n. nit: spaces & capitals, same below.\n. s/tihe/the/\nDid you mean 1,000 KB or 10,000 KB?\n. I think you should use a require here, otherwise this could be a lot of error spam.\n. ditto\n. looks like you missed the typo\n. Mock.On(...) just appends a call to Mock.ExpectedCalls (https://github.com/stretchr/testify/blob/master/mock/mock.go#L202). It might be simpler to just clear out ExpectedCalls. Alternatively, you could just modify the call directly, something like:\ngo\nfailingHandler.ExpectedCalls[0].Return(info.ContainerSpec{}, mockErr)\n...\n. nit: error messages should start with a lowercase character (log messages start with Capital)\nSame below.\n. Yeah, I was suggesting reusing the existing handler rather than replacing it. I.e.\ngo\nhandlerMap[failing].ExpectedCalls[0].Return(info.ContainerSpec{}, mockErr)\netc.\n. Rather than having this method call GetFsInfo (which iterates over all filesystems), move the inner logic from that method to a helper.\n. Spoke offline, decided to stick with this.\n. I'm not sure how to do that without cloning every make rule. Is that worth it? Locally I can control my go version, but with jenkins this is the recommended way.\n. nit: change to io.ReadCloser\n. Good idea. Done.\n. Done.\n. Done.\n. nit: s/blocks/block/. nit: If this really should not happen it should be error level. Add a test case with multiple OOM events (note: I think writeAll needs to run in a goroutine to prevent deadlock). nit: I like the logger pattern, consider setting the logger to use glog for consistency?. I think reporting recent OOM events is desireable, and it's also consistent with the non-journald behavior.. nit: remove these extra lines. Please remove \"update by fzu-huang\" everywhere. It is recorded in the git history.. If there is another manager running, this will break it. I think it's better to just return an error in this case, something like \"cannot start manager: container factories already registered\", and leave it up to the caller to call container.ClearContainerHandlerFactories() if this was really intentional.\nIdeally we would just move away from the global registry.. Since this is in a retry loop, it should be logged at info, maybe V(4).\nI suggested saving the error to a variable outside the loop, and then including it in the returned error at line 119. e.g.\nerr := fmt.Errorf(\"can't open log file %s: %v\", t.filename, lastErr). nit: for consistency, reformat to:\n\"open log file %s error: %v\". Maybe check for the root container before logging the error too?. What happens if there's overlap in env & label & common tag names? We work around this in the prometheus plugin by prefiixing the keys: metrics/prometheus.go. Won't this traverse devices?. I understand that this increases accuracy, but what's the performance impact on a relatively full FS? Would be good to do a comparison to the find method too.. nit: 2017. Can we make the json field names more consistent with the go field names? e.g. container_stats. nit: Field is redundant. Maybe Description is a better name for this? or Annotation?. Delete this file?. I know this is a pretty simple storage driver, but a basic unit test would be nice.. RE: testing - if you change this type to a io.WriteCloser then you can test AddStats with a simple mock (e.g. bytes buffer) rather than messing with a test connection.. We should check that len(splits) == 2 too.. What if there is no equal sign?. What is the right behavior when len(splits) == 1?\n\nFail silently (current approach)\nSucceed (handler.envs[strings.ToLower(exposedEnv)] = \"\")\nLog a warning\n\nI think 2 or 3 would probably be a better approach, but I'm not sure when this could happne, so present your case. If this \"should never happen\", we should just log a warning.. Sounds good. Thanks for the thorough response!. you can drop this. nit: consistent capitalization. start each line with uppercase. I'd reword this to \"Update to go 1.7 for releases\". Actually, if errors are returned they should not be logged. Combine this with the next line instead:\ngo\nreturn fmt.Errorf(\"failed to send data %q: %v\", formatted, err). ",
    "sudhakso": "Thanks for the hint on cpu usage value interpretation, \nThe formula would look something like this, for the machine as well as for containers.\nUsage % = (Used CPU Time (in nanoseconds) for the interval) /(interval (in nano secs) * num cores)\nWhile we plan for capacity, max Cpu Clock Speed is what is considered the most. And dynamic clock speed is anyway a power saving feature with modern cpu(s) ,and not really considered while analyzing performance issues in containers.\nThat being said, CPU Clock Speed, would be really a valuable info to have in static machine info that is exposed.\n. ",
    "dashpole": "closing as outdated. closing this as outdated.  My feeling is that we should leave log management to third party log managers.. closing due to inactivity. closing due to inactivity. closing due to inactivity. Closing due to inactivity.  I would also prefer to stay away from docker-specific features.. I dont think anyone has any plans to address this.  Anyone who has interest in this can feel free to propose and implement a solution.. Storage driver discussion: #1458 . closing since issue was solved. Closing due to inactivity. Storage discussions: #1458. closing due to inactivity. closing as outdated.  Feel free to reopen if anyone wants to take this up.. closing via #739.  For questions about future storage, see #1458.. this issue is blocked on #1458 for now. havent seen this recently.  Feel free to reopen if anyone encounters this.\n. Instance type is included in the v2 api.. closing since this is outdated.. closing as outdated. We have fixed a number of systemd related issues recently.  If anyone is still having issues running with systemd on the latest release, please open a new issue.. closing since this issue is resolved. Closing since it is inactive. Closing as outdated.  Please open a new issue with repros on a new release if this problem persists.. If anyone can reproduce this on the recent release, and post a PR to fix it, that would be appreciated.. closing since we have an elasticsearch storage driver.  #1575 adds \"logstash\" equivalent capabilities.. agreed.  Logrotate, or other third party rotation mechanisms are recommended.. closing as inactive. Container start time is a field in ContainerSpec.  You can use container start time, and 0 to find long run average cpu usage, or use a previous measurement for average usage over a smaller time period.. closed via #835 . This seems like an issue involving the handling of \".slice\" systemd groups.  We have fixed a number of issues recently related to this.  Please open a new issue with a repro on the most recent release if the problem persists.. If anyone has an idea for how to make the UI clearer, feel free to put up a PR for that.. closing as outdated (and possibly solved?). Closing as outdated.  Please open a new issue with a repro the latest cadvisor release, and we can try and debug it.  We have had some issues with disappearing metrics recently that may have fixed this.. Is this specific to influxDB?. This appears to be resolved.  cAdvisor doesn't currently support running with swap enabled.. Does this problem still exist in the latest release?  I am not at all familiar with how docker handles restarting with the --restart flag.. closing as not reproducible.  We have fixed some issues with disappearing metrics in v0.25.0, if anyone else has this issue.. closing as outdated.  Please open a new issue with a repro on a recent release if the problem persists.. closing via #960 . closing as outdated. closing via comment. This appears to be resolved.  Please reopen if there is any more work to be done for this.. Post is different with any language.  From the command line you could do:\ncurl -H \"Content-Type: application/json\" -X POST -d '{\"key\":\"value\"}' IP:PORT/ENDPOINT. closing because it is outdated.. closing via #1020. implementation: #1251 . Sorry for the lack of a timely response.  If you are still experiencing issues, please reopen this issue and provide the cadvisor log.. closing since this issue is resolved.. is this fixed?. closed via #1095 . Closed, since the issue is resolved.. Closing since this is resolved.. Closing this, as it deals with a previous version, and appears to be resolved.. closed via #1143 . @justinsb is this still relevant?  Or do we detect aws in a different way?. Closing, since we no longer read docker config files.. closing since the question was answered. Did #1188 address this issue?. closed via #1203 . closing via #1299 . I think this was \"resolved\" by #1327.  Either way, this is almost a year old.... closing, since this is outdated.  AFAIK, if cAdvisor is unable to collect metrics for a container, it will simply not report those, and still report metrics from other containers it successfully monitors.. Apologies for the slow response.  Can we get more information?  Which os/version?  What cadvisor release?\nSince this is an old issue, are you able to reproduce this on v0.25.0?. IIUC, you would like to add functionality to the kafka storage driver?  Feel free to open a PR for that.. If docker.Status() returns an error, is dockerStatus nil, or is it info.Dockerstatus{}?. /retest. this needs a rebase @ronnielai . closed via #1346. closing, as the question is answered. closing due to inactivity. Closing due to inactivity, and since the question was addressed.. Closing due to inactivity, and since the question was addressed.. closing, since the updating to the latest release solved the issue. This is an old issue.  We have fixed many bugs related to systemd slices recently.\nFeel free to reopen this issue if you can reproduce on the latest release (v0.25.0).. @sjenning, do you plan to pick this up again?. What os version, docker version, and cadvisor version are you using?. I just realized, this error occurs when a container is deleted while cAdvisor is traversing its filesystem with du.  If you have a lot of short lived containers, it isn't surprising to see this.  Is anyone seeing any other adverse effects, or just this error?  Is anyone seeing seeing it multiple times for the same container?  How \"short lived\" are these containers?  \ndu on containers with a moderately large number of files can take ~seconds, so if containers last less than a couple seconds, it may not be possible to get disk statistics on them.  However, theoretically, this shouldnt cause any problems, other than possibly no disk stats on short lived containers.. cAdvisor's latest tag is now v0.32.0.\nIt looks like the original issue was fixed a long time ago.\nThe Couldn't collect info from any of the files in \"/rootfs/etc/machine-id,/var/lib/dbus/machine-id\" is a different issue.  It is likely because the mounts for cAdvisor aren't specified correctly.. Is it just diskIO stats, or are other stats missing as well?. this may be the same as #1450 . @smarterclayton cAdvisor does not export any volume metrics, as volumes are a kubernetes construct.  This issue refers to container disk io metrics.  It would appear based on this code, that we do attempt to export container disk io metrics.  Based on the above, it looks like this bug is not confined to one storage driver, but rather is a problem converting to prometheus metrics.  I wont be able to debug this for a couple days, but anyone else is welcome to try and figure out what is broken.. duplicate of #1403. closed via #1455 . @k8s-bot ok to test. Yes, this issue is fixed by #1489.  We currently use this for inode-based eviction.  We also report inode usage for volumes.. @Bart-Z \nThe author hasn't responded in a while.  I'll close this for now.\nFeel free to open a similar PR if you need this functionality, and I can review it.. @timstclair, do you know the answer to this?. If I understand you correctly, I believe this is a duplicate of #977.  Feel free to post a PR that fixes this, but we do not currently support container renaming.. What is the correct behavior here?  It seems like this race condition is unavoidable, and the result is benign.. I am afraid I dont know what all users of cadvisor use.  There may be people out there who rely on that field.\nEither change would be welcome!  I would rather not remove the error message completely, but you could change it to a warning and add a clarifying statement, like \"Failed to get network devices, the network device may have been removed:...\". #1572 is included in v0.25.0.  Can someone who is experiencing this confirm that this bug is fixed (or that it isnt fixed)?. actually, based on docker/docker#29056 this looks like a different bug.. ref: #1545 . Feel free to open a PR for this, and Ill take a look. @SamSaffron That is quite a change.  I agree that we should change the default.  Can you open a PR to do this?. closing via #1642.. Sorry, I am not familiar with docker compose.  Can you explain how your question relates to cAdvisor?. @timstclair security-related question. W0416 16:16:43.668] vendor/github.com/prometheus/client_golang/prometheus/promhttp/http.go:75: undefined: prometheus.DefaultGatherer\nW0416 16:16:43.668] vendor/github.com/prometheus/client_golang/prometheus/promhttp/http.go:80: undefined: prometheus.Gatherer\nLooks like it doesnt compile?. sorry, this fell off of my radar.\nplease rebase and squash. usually, we just squash to make the git tree more readable, but I see your point.  We can merge this as is once tests pass.. Ill see if I can get the test infra stuff fixed today.  Then we can merge this.. thanks for the help @cyrossignol.  Closing, since the question was answered. Feel free to put up a PR to change this!. closing this, since v0.24.1 has been released. @k8s-bot ok to test. Yes, something is up with our test infrastructure.  I have been looking into it. ignore the test failure for now.  It is an infrastructure issue. @k8s-bot test this. Issue: https://github.com/kubernetes/kubernetes/issues/33382\n. pushed a commit addressing all comments except device-mapper related ones.\n. Code changes look good to me!  @rikatz please add this to docs/storage/elasticsearch.md, and then rebase and squash.. @k8s-bot ok to test. sorry for the delay, I want to figure out what is happening with #1597 first.. The fix for #1572, which is #1573 is in v0.25.0.  Can you confirm that your issue is solved with the new version?. closing until we see if this is reproducible on the newest version. Leaving this open in-case anyone has any interest in adding performance-related testing, or documenting ways to lower cavisor's resource consumption.. Apologies for the slow response.  What OS/runtime are you using?. What are the columns from your output?  We recently fixed bugs related to cadvisor metrics dissapearing with on systemd systems.\nSorry to hear that.  If you arent interested in debugging this, feel free to close the issue.. I would be happy to review if you want to put a PR that adds support for tmpfs.. @k8s-bot test this\n. @k8s-bot test this\n. lgtm\n. @vishh This is the cadvisor change we spoke about.  It allows cadvisor to be queried for filesystem information for the filesystem that includes a given directory.\n. @timstclair comment addressed!\n. @ntquyen Your logs dont have anything suspicious.  People were being spammed with:\nlevel=error msg=\"Handler for GET /containers/CONTAINER_UID/json returned error: No such container: CONTAINER_UID\"\nIt is expected that .mount cgroups are ignored.. https://github.com/google/cadvisor/issues/1513#issuecomment-355541010 seems reasonable to me.  We already collect libcontainer's CpuStats.CpuUsage.TotalUsage, and store it as containerStats.Cpu.Usage.Total in our api.\nWe could add the ability to disable per-cpu metrics by adding a constant here, and then not setting PerCPU usage here if the per-cpu usage is ignored.. ES5 support #1597 is currently blocked by #1458.. LGTM\n. LGTM\n. what cadvisor version are you running?. To me, this seems out of the scope of cAdvisor, which is primarily concerned with collecting and publishing stats for containers.  How does this feature relate to container monitoring?. The following is taken from kubernetes/kubernetes.github.io#2892, and seems relevant here.\nThe value for memory.available is derived from the cgroupfs instead of tools like free -m.  This is important because free -m does not work in a container, and if users use the node allocatable feature, out of resource decisions are made local to the end user pod part of the cgroup hierarchy as well as the root node.  The following script simulates the same set of steps that the kubelet performs to calculate memory.available.  The kubelet excludes inactive_file (i.e. # of bytes of file-backed memory on inactive LRU list) from its calculation as it assumes that memory is reclaimable under pressure.\n```sh\n!/bin/bash\n!/usr/bin/env bash\nThis script reproduces what the kubelet does\nto calculate memory.available relative to root cgroup.\ncurrent memory usage\nmemory_capacity_in_kb=$(cat /proc/meminfo | grep MemTotal | awk '{print $2}')\nmemory_capacity_in_bytes=$((memory_capacity_in_kb * 1024))\nmemory_usage_in_bytes=$(cat /sys/fs/cgroup/memory/memory.usage_in_bytes)\nmemory_total_inactive_file=$(cat /sys/fs/cgroup/memory/memory.stat | grep total_inactive_file | awk '{print $2}')\nmemory_working_set=$memory_usage_in_bytes\nif [ \"$memory_working_set\" -lt \"$memory_total_inactive_file\" ];\nthen\n    memory_working_set=0\nelse\n    memory_working_set=$((memory_usage_in_bytes - memory_total_inactive_file))\nfi\nmemory_available_in_bytes=$((memory_capacity_in_bytes - memory_working_set))\nmemory_available_in_kb=$((memory_available_in_bytes / 1024))\nmemory_available_in_mb=$((memory_available_in_kb / 1024))\necho \"memory.capacity_in_bytes $memory_capacity_in_bytes\"\necho \"memory.usage_in_bytes $memory_usage_in_bytes\"\necho \"memory.total_inactive_file $memory_total_inactive_file\"\necho \"memory.working_set $memory_working_set\"\necho \"memory.available_in_bytes $memory_available_in_bytes\"\necho \"memory.available_in_kb $memory_available_in_kb\"\necho \"memory.available_in_mb $memory_available_in_mb\". @k8s-bot ok to test. Apologies for the delay, this looks good!  Please rebase and squash. similar issues: #1403, #1450.  I will look to resolve these soon.... closing via #1642. Can you provide a little more context?  What command did you call to produce MemAvailable?  Where is the code snippet from?  Where are you proposing we add the MemAvailable field to?. assigning to @derekwaynecarr since this is DM related. closing since the question was answered.. @k8s-bot ok to tes. Does this make sense across different runtimes?  I would think that such information would make more sense to acquire directly from the runtime.. @k8s-bot ok to test. Apologies for being slow.  The code changes look good, but dont have the context to review this.  @derekwaynecarr, if you could give this a quick look, that would be appreciated.. @bakins, this will eventually need to be squashed and rebased.. I've read up on Devicemapper and ZFS, and I think I mostly understand this PR.\nHow does this differ from the current behavior for ZFS?  Do we currently not support it at all?  Or does it just not work?. Alright, I am satisfied.  Thanks for the work and patience.  If you could squash down to one commit, then we should be good to merge.. Closed via #1558.  Anyone experiencing this problem can update to v0.25.0. My bad, it looks like this issue was separate from the one solved in #1558.  . @timstclair should we add GNUfind utils as a dependency?. This was fixed in https://github.com/google/cadvisor/pull/1698. @k8s-bot ok to test. @k8s-bot test this. @k8s-bot test this. it passed the e2e_node InodeEviction test, so /LGTM. ill have to figure out why the pull is failing.... #1567 has merged. Try rebasing and see if the problem goes away.. Was this resolved by vegasbrianc/prometheus#29. @VariantMonkey this appears to be resolved.  Do you have any remaining issues?. #1567 has merged.  Try rebasing and see if the problem goes away.. closing via #1609. Now ignores root container, and includes container name in the log message, which will be useful for debugging.\nDoes not change original verbosity level.. I dont believe cAdvisor has any concept of volumes at the moment.  Kubernetes has separate libraries (not cAdvisor) for monitoring volumes, as they are largely tied to pods in k8s.  Ill put this on my todo list for the upcoming cAdvisor work.. There are a number of files in kubernetes/kubernetes/pkg/volume that deal with metrics collection for volumes.  My current question is this:  are kubernetes volumes the same thing as docker volumes?  If yes, then it would make sense to unify those pieces of code, and publish volume metrics.  If not, then I will likely only have the bandwidth to work on the kubernetes volume-related code.. Sorry for the slow response.  It appears that kubernetes doesnt use docker volumes, so I dont think it would make sense to move kubernetes volume information into cAdvisor.  If you are looking for the library cadvisor uses to collect disk IO stats, take a look at how we use libcontianer.  Im closing this for now.. This issue is about whether or not cAdvisor exports metrics for docker volumes.  From your earlier comment, it sounded like you were asking for usage metrics for remote block storage?  I am not familiar with EBS... If you want to propose adding any metrics, feel free to open an issue with your proposal.  If you need kubernetes volume metrics, those are available via the node summary API: /stats/summary. This is in 1.5.6. @k8s-bot ok to test. Just linking this here, because it was helpful for me to understand the problem and solution:\nhttps://www.spinics.net/lists/linux-btrfs/msg59039.html. @k8s-bot test this. Just as an update, this is an issue with our test infrastructure relating to ssh key management.  Ill let you know when it is fixed.. @k8s-bot test this. @grokbot Code changes lgtm.  Can you squash?\n@timstclair what is the correct thing to do for cla check?. @alinoeabrassart I think we are just waiting on the unit test, and then it should be all good to go. > My reading of the code is that the fshandlers are created per-container and apply to the read/write upper layer, so the directory is unique per container and can't be so easily deduped.\nI believe @euank is correct in that we have one FsHandler per container, and we are only counting inodes using find on the container's writeable layer.\nGiven that we only currently use this on the container's r/w upper layer, will we actually see much of an accuracy increase?  It looks like the find command has issues counting inodes used by an overlay filesystem, but does a decent job with \"regular\" files and folders.\nThe latency increase definitely concerns me, as we believe we have some issues related to evictions using data that isnt recent enough.  I am also curious how the latency would change if we were running more than one command at a time (which is the case with multiple containers).\nIf a couple extra days of delay on this is alright, I would like to collect some data on latency in use myself.. I mostly just want to get numbers in the context of our e2e eviction tests.. If there aren't any downsides to removing the map, and the change isn't too hard to make, then ill do my testing after that.. Oh, I see.  Ill just run it on this then. I ran the inode eviction test with and without this change.\nWith this change, the test flaked once, out of 4 runs, and each run had a call to GetDirInodeUsage that took > 10s.\nWithout this change, the test did not flake in 9 runs, and had NO calls to GetDirInodeUsage that took > 1s.  The highest recorded was ~700ms.\nIll run some more testing on this change, to make sure that these stats are representative, but this version may pose too high of latency increases.\nI spoke with @vishh, and he suggested that we could possibly optimize this in two ways: \nparallelization of walking the tree with limits on the max number of threads\ncombination of this and the GetDirDiskUsage: if we are making the effort of walking the tree, we might as well collect both disk space and inode stats at the same time.\n. I have done a couple more runs with the change, and I havent seen any that were as slow as the first.  The highest seems to be around 2.5 seconds.\nI tried it with the change timstclair suggested, and the results were about the same.\nI realized something while running my tests: Some of the rarely seen extremely long times (> 10s) may be due to the limits on the number of concurrent exec calls, and they could be waiting to be able to run.\nCombining the GetDirInodeUsage and GetDirDiskUsage functions may alleviate this, since we will have half the total number of calls.\nThis can be done with info.Size()\nI think the latency increase is still too large ATM, and I would love to try some optimizations before merging anything.. @euank Ill try and get some numbers on \"du\" latency.  Sounds like that may be slower than find, since it needs fstat.  If we can collect both inodes and disk usage in comparable time to du, then this obviously would be a great thing to add.. Just curious, are users actually creating containers with multiple filesystem partitions?  When I debugged this months ago, all of the spammy error messages were coming from the \"/\" partition, which #1568 addresses.. @k8s-bot test this. I will cut the cadvisor release tomorrow morning if there is still interest in getting this in 1.7. /retest\nSeems like this is still something we want. Thanks for your PR!  The code changes look good.  Please rebase, and reply to this PR to let me know!. This appears to be resolved.  Feel free to reopen if there are anymore issues.. ZFS support has been added in recent versions, so it should just work now.. based on the --device docs also linked above, it looks like that includes both mounting, and granting privileges to the container.\n--device=[]: Allows you to run devices inside the container without the --privileged flag.\nSo you should be able to configure the cAdvisor daemonset to have those privileges as well.  Check out the security context docs.. @k8s-bot test this. @k8s-bot test this. I saw an e2e test pass in a different PR, so this should work...\n@k8s-bot test this. magically fixed. @k8s-bot test this. @k8s-bot test this. @k8s-bot ok to test. @jjqq2013 would you mind moving this to running.md?  This is more specific information than I would like to include in the README.  Thanks! . @jjqq2013 git doesnt seem to think you authored your commits.  The icon next to the commit is not yours: https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user. @k8s-bot test this. yes, please rebase and squash.  Code changes lgtm as well.  Have you manually tested this at all?  I dont think adding a full test suite should block this, but I would at least like to know that someone has successfully used it.. @k8s-bot test this. k8s-bot ok to test. @k8s-bot ok to test. I dont think we have done any validation on docker 1.13.  If anyone wants to help validate and debug docker 1.13, that would be appreciated.. @k8s-bot ok to test\n. @k8s-bot test this. I would prefer if you used the same description as in storage/common_flags.go, just to be consistent.  This will have to rebase before it can go in.. please squash, and we can get this merged!. @k8s-bot ok to test. please rebase.  Changes lgtm. please squash, and ill merge this!. @k8s-bot ok to test. @timstclair this looks like an entirely new storage driver.  Is it not possible just to update the old ES driver?  If it is a new one, this is probably blocked on #1458.... @k8s-bot test this. @oopschen apologies for the delay.  Is it possible to update the existing ES storage driver rather than adding a new one while maintaining compatibility with older versions?  I don't think we should have a driver for each version of each driver if we can avoid it.. @timstclair should this wait on #1458?. It is a new plugin, since elasticsearch 2.x and 5.x are not compatible. /ok-to-test. closing because of inactivity.  feel free to reopen if you are still working on this.. @derekwaynecarr is this solved by #1588?. /lgtm. I believe we currently use the latest 1.7.x release. see #1603.  I am not 100% sure what our policy for switching go versions is, but ill probably wait a couple patch releases before bumping to 1.8. /lgtm. Just for my knowledge for future reference: Does not specifying the minor release (e.g. 1.7 vs 1.7.1) cause it to default to the latest release (if, for example, 1.7.2 is out)?. I am not familiar with LXD containers or canonical kubernetes, so let me make sure I understand the issue. You are running the kubelet inside an LXD container, but cant give the kubelet access to the underlying filesystem?  What alternative are you proposing?. @cruwe I am afraid I dont understand ZFS well enough to understand your issue.  Why can docker only use VFS?. @cruwe thanks for the explanation, glad to hear it is working.. We generally try not to rely on docker's APIs.  What event happened 19 hours ago?  Did cAdvisor restart?. I dont have bandwidth to look at this right now.  If someone has time to dig into this, that would be greatly appreciated.. It turns out cgroup files are not actual files.  If the dentry caches are cleared, the next time the file is touched it has a new modification time.  I believe this can occur when running low on memory.\nI havent found a good way to reliably get the start time of a cgroup.  We use the start time for docker containers that docker gives us, so that shouldn't be a problem.  but for other cgroups and runtimes it may be an issue.. I am afraid that flag is simply to change the name of the endpoint.  According to #1621, the best setup for authentication is using a proxy.. @k8s-bot test this. duplicate of #1572.  The fix, #1573 is in v0.25.0. @derekwaynecarr Ok to cherrypick this to 1.4?. The json APIs at /api/vX.Y are different from the prometheus endpoint.  The prometheus takes information in that format and exports a subset of them.\nIt is just a matter of adding those metrics to the prometheus endpoint.  I am happy to review that work.\n. I would be happy to review such a PR.. @k8s-bot ok to test. fix the nit, rebase, and squash, and I think we will be set to merge this.. @k8s-bot test this. @k8s-bot ok to test. @k8s-bot test this. ignore the failure. looks like something is up with the testing infrastructure.. @k8s-bot test this. @k8s-bot ok to test. ignore the failure.  looks like something is up with the testing infrastructure.. ill review it later today. Once you make the change, squash it and ill merge it.. Thanks a bunch @JinsYin . only interesting thing I see is: cadvisor/container/rkt/client.go:44, which is a call to net.DialTimeout(\"tcp\", \"localhost:15441\", 2*time.Second).\nI am not sure why this would panic...\nhttps://github.com/golang/go/issues/6232 suggests this could be related to dns failures, which would make sense, since the top of that trace has lookupIPDeadline. @k8s-bot ok to test. cc @derekwaynecarr devicemapper. I am not going to bother testing it right now, as our test infrastructure is having problems.  Ill test when that is fixed.. @k8s-bot ok to test. That is really strange... I have googled a little bit, and it may be possible that etc/mtag hasnt been symlinked to /proc/mounts, which is where cAdvisor is reading mounts from in the error message.  See https://github.com/zfsonlinux/zfs/issues/4680 and https://bugs.launchpad.net/ubuntu/+source/sysvinit/+bug/1607920.  If you restart cAdvisor, do you get the same error?. It is probably an issue of cAdvisor not being able to \"see\" a file it depends on.  Try running the command inside a container with the same directories mounted.  You could use:\nsudo docker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro -i -t alpine:3.4 /bin/sh\nand run zfs get -Hp all mydata/mirrors/github/shidenggui from inside the container.  cAdvisor is built on alpine 3.4\n. According to #1621, a number of our auth functions are broken right now... @timstclair will probably know more about your specific use case.. Yeah, lets move this to #1436.  @tianshapjq, I am happy to answer questions about cAdvisor, but I am not familiar with any tools for collecting metrics on GPUs.  I think that is being discussed in the other issue.  Closing as duplicate.v. cc @timstclair security-related. @k8s-bot ok to test. closing as this is outdated.  This shouldn't be an issue in newer versions. ok, sounds good. I wasnt sure how the tokens would interact if this utility function is used by the volume stats collector as well, so I moved claiming and releasing tokens outside of the utility method.. @k8s-bot test this. I have run it with --modify.\n@k8s-bot test this. Doesnt look like I am an admin.  @timstclair can you change the required tests?. closing via https://github.com/kubernetes/kubernetes/issues/44328#issuecomment-293733002. we export version info here, and you could probably do something similar for docker info.  The equivalent of getVersionInfo() is DockerInfo(), and the DockerStatus struct is defined here. The code changes look good, but I am still trying to wrap my head around: Since blkio cgroup tracks devices, we create a synthetic device /dev/NAME for the metric.\nNAME in this case is the name of the disk, right?  In your output, this device is dev/sda?  Would it be better to name it \"disk/sda\" so users dont expect an actual device?  Does \"dev/sda actually exist, or are we making up a device?\nAs you point out, we dont have any prometheus tests for this.  Your manual test above should be good enough.. Ok, so the name comes from reading the /sys/block/ directory.  Would it make more sense to name these /sys/block/NAME in the interest of accurately representing what the metrics are based off of?  I am also alright with leaving it as dev/NAME, since the filesystem devices seem to be named similarly.  E.g. block device is named sda, filesystem device is dev/sda1.  That might make it easier for users to correlate the two.   WDYT?. yes, so that is /sys/block/sda/dev. yep, ill cut a cAdvisor release next week sometime before the kubernetes 1.7. these are diskIO stats. cAdvisor should support devicemapper. We have never tested with more than 3xx or so containers per node AFAIK.  We would welcome help fixing scalability bugs.. just looking at the Memory GetStats function, looks like each call reads up to 17 files.  I would guess that is a big part of it.. The unable to connect to rkt error isnt important.  It is expected if you are not running rkt.. From the cAdvisor logs, the only thing out of the ordinary is cadvisor.go:191] Exiting given signal: terminated 4/22/2017 11:28:29 PMI0423 06:28:29.871653\n\"Recovery\" just refers to the process of discovering what containers already exist on the host, and is part of the startup process.\nI suspect cAdvisor isnt correctly configured?  This: Error: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\" makes me think you didnt mount /var/run inside the cadvisor container.... That looks correct... Based on the logs, it looks like cAdvisor is also randomly terminated.  Are you getting system OOMs or something?. If you are using systemd, then use journalctl -k.  If not, then look in var/log/kern.log or var/log/messages.log, or var/log/dmesg.log, or use dmesg. I am going to close this for now.  Feel free to reopen if you have follow-up questions.. this looks like the same as #1481 . How does docker expose this value?  We currently only monitor by using cgroups, so this may be a large change if we need to use docker's api.. The total usage is under '/'.  @k8s-bot ok to test. I am on board with exposing this as a prometheus metric.  I agree this is useful for users monitoring pure docker deployments.  Not exporting if it is 0 would also be nice.\nMy main concern is that this adds a field to the ContainerStats struct, which is exposed via the api/vX.X/containers endpoint.  For users of other runtimes, or using kubernetes, this additional metric would not only not be useful, but would be misleading, since they would expect this number to equal the kubernetes container restart count.  I would prefer at least renaming this to DockerRestartCount to differentiate it from the kubernetes one.  Generally, we don't explicitly add runtime-specific fields like this to the API.\nThe ContainerStats struct is currently only being used for general resource usage metrics: CPU, memory, disk, network, and diskio.  I don't think restart count fits into this model.  We don't really have a good way to express a runtime-specific orchestration metric like this.\n@timstclair any thoughts on the best way to expose docker restart count?. @k8s-bot ok to test. @timstclair this looks acceptable to me.  It would never show up in kubernetes, since we don't use docker's restart feature.\nWDYT?. Yeah, sorry.  I plan to merge this once we get our test-infra stuff figured out...  Just hang tight.. would you mind squashing?. Assuming you are looking at docker containers, they are both part of the handler struct:\ndockerContainerHandler. I would recommend against looping through.  The dockerContainerHandler has a number of methods, including GetStats(), GetSpec(), Start(), etc.  Most metric collection can be implemented by modifying these methods.. @k8s-bot ok to test. @WIZARD-CXY, this change looks fine, but I would prefer that we coordinate on the solution we add for monitoring GPUs #1436.\nI would like GPU-related PRs to wait until we have come to agreement on how we plan to move forward.. @WIZARD-CXY what is the impact for end users?  Where does this information surface?  Sorry, I dont have a ton of bandwidth for looking into this.... > After all, a seemingly good function doesn't work like it claim to be is very confusing for the user IMHO.\nCan you shed some more light on how this impacts users?  Thats all I have been trying to figure out.\nWhich external APIs does this affect?  If this doesnt make any changes that are externally visible, then you should add it to whatever PR makes those changes.\nIIUC, if this change isn't user-visible, then we are monitoring devices, but not using this information in any way and wasting resources.. Ok, I have had time to dig into this a little bit more, and I would prefer leaving it the way it is, unless we have a more substantial reason to change it.\nGetCgroupSubsystems is used by the docker, rkt, and raw handlers to get cgroup paths, as well as the raw container watcher.\nWe currently detect cgroups that belong to containers using a regex, and we have run into issues in the past with some cgroups being mistaken for containers.\nI am surprised that ListProcesses exists, if it isnt used.  It looks like it wasnt used even when it was first added...\n. What is your end goal?  What functionality are you trying to add to cadvisor?. do you have a PR open somewhere?. I think I prefer the approach taken by https://github.com/google/cadvisor/pull/1891, since it works for containers.  That will give the total number of threads for each cgroup instead of just the total on the machine. #1597 is currently pending.  It isnt clear when we will be able to add 5.x support.. yeah, that would be nice. @k8s-bot ok to test. what command are you running?  make test doesn't run unit tests from my vendor directory... I am closing this for now. @k8s-bot ok to test. @k8s-bot ok to test. Those could be diskio metrics, and not disk metrics.  Are you seeing container_fs_inodes_xxx or container_fs_usage_xxxx?  AFAIK, we dont have the ability to disable diskio metrics.. Ill look into this.. do those disk metrics have non-zero values?  Something I fixed recently is that we still omit metrics (with values of 0) for disabled metrics.. I would be happy to review a PR that fixes this.  Although I must be missing something, because it seems that you say it works with -race still included in the command?. Issue appears resolved.. I think the -printf option is in GNU findutils.. @k8s-bot ok to test. Thanks for this.  Ill try and get to this in the next week, but I am pretty busy atm.. Have you vendored (or just copied) these changes into kubernetes to verify that they work?\nCode changes look great!. can you rebase?  Then I will merge this.. I think this is fixed in the most recent release (v0.26.0) by #1642.  I havent had a chance to build and upload the latest version, but you can build from source to see if that fixes your issues.. tcp metrics are disabled by default.  You can configure this using the --disable-metrics flag documented here.  We also have already support for custom application metrics. tcp connections are already supported, cpu load is already supported.  You can see the api that we expose for containers.   The current design of cAdvisor is to monitor containers, not processes.. fixed by #1679, and in the 0.26.1 release.. Make sure you are setting the flag --enable_load_reader=true, otherwise you wont see any metrics.. I remember looking into this before... cAdvisor needs to be on the host network, but i'm not sure how to accomplish that with a docker compose.  See https://github.com/google/cadvisor/issues/2051 for more details, and a kustomize patch to do this in kubernetes.. @k8s-bot ok to test. > Currently, tcp/udp usage will be disabled via disable_metrics flag by default, thus /metrics would show zero values. Due to ContainerSpec has no ignoreMetrics flag, I don't know how to handle this...\nSorry, why would you want the container spec to have ignoreMetrics?. looks good.  please squash your commits. @k8s-bot ok to test. Thanks for this!  Ill try and fix this in the near future.. this should be fixed by #1711. /ok-to-test. please rebase and squash. This was fixed by https://github.com/google/cadvisor/pull/1831. I am not familiar at all with zfs.  cc @bakins who added the zfs watcher. in #1555. @jianzi123 can you let me know what the purpose of this PR is?  These functions are not called anywhere.. closing due to inactivity. We dont support filtering metrics in cadvisor.  It should be fairly simple to filter the output of any api call.. /ok-to-test. /test pull-cadvisor-e2e. it looks like your account isnt associated with the commits.  You will need to make sure those are consistent, and then squash.. I am not sure I understand your issue.  What do you mean by a stream of metrics?. /retest. /test pull-cadvisor-e2e. oh sorry, you need to rebase. looks like the question is answered. The current scope of cAdvisor is restricted to monitoring containers.  It provides the kubelet with container metrics only, and has no concept of volumes, pods, or any other higher-level Kubernetes API objects.  The kubelet has a built-in cadvisor, which monitors containers in kubernetes.  The kubelet currently takes the container metrics provided by cAdvisor, and combines them with volume metrics, and kubernetes-specific metadata (e.g. container->pod mappings) to produce the summary API, which is exposed by the kubelet at 10255/stats/summary.\nThe underlying issue you are raising here is that the kubelet does not expose this via prometheus.. No, cAdvisor does not provide volume metrics, and has no concept of volumes or pods, which is why there are no volume metrics exposed via prometheus.\nThe kubelet itself collects and exposes volume metrics via the summary API, but doesnt expose it in any other way (e.g. prometheus). Would just adding volume metrics to the kubelet's prometheus endpoint be sufficient?. I think it is reasonable.  We should also have a discussion on the relationship between kubernetes and prometheus.  Seems odd to have some metrics on the cadvisor port, and others on the kubelet port.  I think ideally, the prometheus endpoint should mirror the information provided by the kubelet's http endpoints (e.g summary API).. I guess my biggest concern/question is that I don't know how prometheus deals with metrics changing.  What about a metric (name, format, labels) can change across a release without causing disruption?  Prometheus doesnt look like it has versioning.. yes, these metrics were added in https://github.com/kubernetes/kubernetes/pull/51553, which first became available in 1.8.0.. @juliohm1978  You need to query the kubelet's /metrics endpoint, rather than cadvisor's /metrics endpoint.. Thanks @bboreham!  Can you submit a PR with your fix?  I will try and get this in the 1.8 release.. @matthiasr I can cherrypick this to 1.7.\nWe had some errors that were introduced when we updated to prometheus v0.8.0 (#1680), but were not sure what the root cause was until now.  Because the checks were introduced recently, we couldn't point to a change in cAdvisor that caused the inconsistency, and haven't had a chance to look into this myself yet.. Yes, I think that just made it report an incomplete set of metrics instead of none at all.. It looks like #51473 fixes this in kubernetes, and ill cherrypick it into the 1.7 branch.  However, this wont fix it for stand-alone cAdvisor, as that uses the DefaultContainerLabels function.  It would appear that exposing container labels as prometheus labels is considered an anti-patern.  I am not quite sure what the best way forward is in that respect.. seems like InfluxDB has similar issues: https://github.com/google/cadvisor/issues/1730\n. once we have the label whitelist #1730, we can ensure all whitelisted labels are present as metric labels, or set them as \"\". Does the set of labels need to be consistent across time, or just at a single point in time?  That would only work if we don't need consistency across time.. The fix is released in version v0.28.3. /retest. /test pull-cadvisor-e2e. rebase, and this should pass . thanks @derekwaynecarr.  I had misread the code: I thought it was checking the contents of the file, not the number of lines.\nLGTM. /test pull-cadvisor-e2e. rebase, and this should pass. /test pull-cadvisor-e2e. @sjenning please rebase.  The infra issue should be fixed. reopening via #1807.  Issue: https://github.com/kubernetes/kubernetes/issues/55909. It looks like the switch to fsnotify actually worked... The summary test passes if we use a non-restarting pod, but fails with the current version of the test.  The test checks the summary api for pods that restart.  But when they restart, we loose metrics for it.  I am not sure how it passes when we are using inotify.... We are still using inotify.  Reopening. @euank thanks for this.  I haven't had time to get around to this yet.  Issue: #1676\nThe only remaining item is to test to make sure this works...  I am not sure running on AWS is a possibility for me, but ill ask around and see if I can get my hands on a 4.7 kernel to test.. /ok-to-test. done via #1698 . Looks like you messed up here.  Feel free to open an issue if you have questions, or would like to help us find a bug. It seems reasonable that minute/hour/day usage doesnt show up until that quantity of time has passed.  I am not sure I want to change the behavior at this point, since I would rather not break anyone who may rely on the current behavior.  . If anyone wants to introduce this capability, I can review it.  But I don't have the bandwidth to make UI changes right now.. Yeah, I am not quite sure why the name was chosen initially.  It also appears that MaxLimit isn't ever set either.  I would recommend using Quota as the container limit.. If we make a V3 of the API, then we can change it.  But in the interest of having a stable API, we should leave it as is for now.. Ill try and take a look at this soon. I think the original issue was solved here: https://github.com/google/cadvisor/pull/1642#issuecomment-321238965\nCan you see if your cgroup mounts are correct?\n@ZhiqinYang can you tell us what you had to change with your docker run command?. @Brandonage I am not sure.  I was hoping @ZhiqinYang would chime in. /ok-to-test. can you rebase and squash your commits?. duplicate of #1704 \nPrometheus changed the way they treat missing metrics, so we havent known about it until recently.. ill take a look today later today.. IIRC from our earlier conversation, this will only work on newer kernels.  What is the behavior for older kernels?  What is the plan for maintaining the current behavior with older kernel versions?. can you rebase and squash?  Ill merge after that. xref: https://github.com/kubernetes/kubernetes/issues/34965. closed via #1729 . That seems like a reasonable use case.  Feel free to post a PR.  My main request is that it defaults to the current behavior.. fixed by #1984. tim is OOO, so @vishh will have to approve this.. @inbalzoran \nyou probably want container_cpu_usage_seconds_total.  That should be roughly equivalent to container_cpu_user_seconds_total + container_cpu_system_seconds_total.\nI'm not sure about how docker stats is implemented.  \n@RoyceShen1 \nI am not familiar with grafana, but at least in prometheus you can take a rate over the cumulative cpu metric, and then multiply/divide to get whatever units you want (e.g. % of cpu cores)\n@getvivekv \nThat is probably a better question for Influxdb folks.  This Influxdb documentation is probably relevant, but i'm no expert:\nhttps://docs.influxdata.com/influxdb/v0.8/api/aggregate_functions/#derivative\n. /ok-to-test. /ok-to-test. It seems that prometheus has a similar problem: https://github.com/google/cadvisor/issues/1704\nI dont think it would be very hard to generalize this to other runtimes (RKT), although you dont need to implement it here.\nWDYT about naming them enforce_label_whitelist (default=false) and label_whitelist (default=[]).. If we only had one flag, how would we differentiate between \"send all labels\"  and \"send no labels\"?  I think having two is fine.. In docker/handler.go:GetSpec(), we should check to see if we are enforcing the whitelist and if \"restartcount\" is in the whitelist before adding \"restartcount\" as a label\". I am not sure.  I would not worry about the rkt integration (dont make any changes there).  Once this is merged, ill open an issue to track that these flags do not have rkt or cri-o support.  . closing in favor of #1984. Yes, findutils is currently required for collecting disk metrics.  This is a short-term solution until we are able to transition to project quota.. Installing findutils into your base image (debian jesse) should fix this issue.. I believe we currently only support a single network interface per container.. Sure, the real question here is what API changes you are suggesting.  How do we plan to represent this in the API in a way that is backwards compatible?. cc @yguo0905 \ncan I get a second set of eyes on this before I merge it?. #1742 builds without error.  I havent seen this before, so it is likely something with this PR.. Thanks for pointing this out.  . Can you make sure #1759 is what you were looking for?. docker distribution commit taken from here: https://github.com/moby/moby/blob/master/vendor.conf. actually from here: https://github.com/moby/moby/blob/8af4db6f002ac907b6ef8610b237879dfcaa5b7a/vendor.conf. Looks like you found the related issue already: #1517. /ok-to-test. which orchestration system are you using?. Have you tested this?  I dont think the memory reservation is actually what that metric is, although I havent tested it.. /ok-to-test. thanks for this!  Would you mind squashing your commits?. I would rather just stick with docker/docker.  Who knows what will happen to godeps if we change it. /test pull-cadvisor-e2e. I dont see any relevant errors.\nunable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp 127.0.0.1:15441: getsockopt: connection refused\nThis is an expected error, as you are not running rkt.  cAdvisor tries to connect to each runtime it can find.  You must be running into a different issue. @mindprince Ill take another look at the implementation once it is close to being finished.  Let me know when it is ready. cAdvisor is not designed to be a metric aggregator, but rather a source of metrics.  You can use the prometheus storage, or other storage options:\nhttps://prometheus.io/docs/prometheus/latest/storage/. cc @euank . FYI, this change should not add any additional overhead because we were already reading this cgroup file.\nWe previously threw away the value, as we did not expose it in any API.. please rebase, and then Ill merge it!. we should cherrypick this onto the v0.27 and v0.26 branch. cc @tallclair . actually, lets wait on this until I have done some more digging.. I tested the directory setup by doing the following:\nOn a node running ubuntu xenial, docker 1.12.6.\nChange storage driver to aufs, restart docker, run docker run -it busybox sh, then touch myfile.\nRepeat for overlay and overlay2.\n/var/lib/docker# find . | grep myfile\n./overlay2/25bee5c56e3f26d2aaaf228a71ffa947970329fc9692e85ae68a5418dcc6387b/diff/myfile\n./aufs/diff/6d1dbb63ff495abe6389cdadc9bc49b7621f03c5d98f136e30316f3d0bf8984f/myfile\n./overlay/b037efcadbc8129c861ec5e613803697fd09c0c368c02bd48db0e576d8400690/upper/myfile\nSo we may also have a bug with overlay as well.... Based on the docker docs, and my own experiments, I think this fixes the overlay2 incorrect root directory issue.  @derekwaynecarr would you mind providing a review?. I plan to cut v0.27.2 to cherrypick #1770 into kubernetes 1.8.2.\nShould this PR be included as well? @derekwaynecarr . looks like it is just waiting on the CLA?  Edit: Oh, it wont change from that state.  Not sure why the other tests are not running. The reason for this is that cAdvisor attempts to collect metrics on all filesystems on the host when it starts up.  We should lower the verbosity of the log message or ignore devicemapper filesystems.. @derekwaynecarr should this be cherrypicked to the kubernetes 1.8.2 release along with #1770?. FYI, I plan to cut cadvisor v0.28.1, which will correspond to k8s v1.9.0 this Friday, 11/17.. A number of performance tweaks have been made since then.  From a quick scan, it looks like a large amount of that time is spent handling external requests.  Might be worth measuring the number of requests cadvisor is serving, and seeing what its resource usage is when it is not handling any requests.. @bmerry the graph you show looks like about what I would expect.  Assuming your containers don't have anything in their r/w layer, most cpu usage generally comes from reading from cgroup files, of which there are many for the memory cgroup.\nNot sure about the memory leak.  I'm not super familiar with the cgroup implementation, but I know they aren't real files, and are \"stored\" at least partially in the dentry cache.  Accessing cgroup files repeatedly should make the dentry cache grow, but it should be reclaimed when free memory gets low.. We don't have the option to disable collection of the root cgroup.  We have an option --docker_only, but that keeps the root cgroup around.. I would rather not add a flag for that, but you can just remove the registration of the raw factory here: https://github.com/google/cadvisor/blob/master/manager/manager.go#L335, and that would turn off collection for all cgroups that are not containers.  I am planning to introduce a command line flag to control which factories are used in the future, which should allow this behavior without rebuilding cAdvisor. Looks like we enable \"Allow merge commits\".  Maybe we just arent using it correctly.  Let me test it out.. I see.  We did have the \"Require branches to be up to date before merging\".  I think we can disable this.  We just need to be careful, but we can always revert if builds go red.. I have made the change, but would like to get a second opinion\ncc @ixdy @krzyzacy\ncAdvisor doesnt have the \"rebase required\" label infrastructure.  But we only get ~5 commits per week at most.\nWould you recommend we keep the \"Require branches to be up to date before merging\" enabled?  Or can we disable it to make rebasing less frustrating for contributors?. cc @derekwaynecarr . FYI, I plan to cut cadvisor v0.28.1, which will correspond to k8s v1.9.0 this Friday, 11/17.. @tallclair unit test added. Unit testing is complete.  I had to plum through a clock.  It should be fine to use apimachinery, right?. I tested this in kubernetes/kubernetes and verified that it passes the summary API test as well. Ok, it should be ready. /ok-to-test. Great find @majst01!. Try using a more recent version of cadvisor.  Maybe v0.27.3 or v0.28.2. This is likely #1704. . No problem.. The fix is released in version v0.28.3. /ok-to-test. /ok-to-test. /test pull-cadvisor-e2e. /test pull-cadvisor-e2e. /ok-to-test. Looks like the unit test uses a separate filesystem to test. /test pull-cadvisor-e2e. /test pull-cadvisor-e2e. FAIL: TestDirDiskUsage (0.06s)\n    Error Trace:    fs_test.go:106\n    Error:          Should be true\n    Messages:       expected dir size to be at-least 102400; got size: 4096\nLooks like your unit tests are failing.  Try running make locally. Would it make sense to log something (at low verbosity) when we skip a subsystem? . GPU support for nvidia gpus was just added yesterday #1762!  Try out version v0.28.0. /ok-to-test. FYI, I plan to cut cadvisor v0.28.1, which will correspond to k8s v1.9.0 this Friday, 11/17.. It doesnt need to happen now, but eventually it would be nice to have disk stats for containerd through cAdvisor.  k8s would disable disk stats, but it would mean that standalone cAdvisor could still be used to monitor docker after docker rebases on top of containerd.\nThis PR lgtm other than the one nit.. We had to limit the labels that cadvisor adds to metrics because more recent versions of the prometheus client require all time series to have the same set of labels: https://github.com/google/cadvisor/issues/1704\nI'm afraid I have no idea where the label originated from; I cant find a reference to it anywhere in the code base.  Do you know what the kubernetes_io_hostname label was for?. I cant seem to replicate this...\n$ docker info\nContainers: 24\n Running: 22\n Paused: 0\n Stopped: 2\nImages: 16\nServer Version: 17.03.2-ce\nStorage Driver: overlay2\n Backing Filesystem: extfs\n Supports d_type: true\n Native Overlay Diff: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins: \n Volume: local\n Network: bridge host macvlan null overlay\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nInit Binary: docker-init\ncontainerd version: 595e75c212d19a81d2b808a518fe1afc1391dad5 (expected: 4ab9917febca54791c5f071a9d1f404867857fcc)\nrunc version: 54296cf (expected: 54296cf40ad8143b62dbcaa1d90e520a2136ddfe)\ninit version: v0.13.0 (expected: 949e6facb77383876aeff8a6944dde66b3089574)\nSecurity Options:\n apparmor\n seccomp\n  Profile: default\nKernel Version: 4.4.70+\nOperating System: Container-Optimized OS from Google\nOSType: linux\nArchitecture: x86_64\nCPUs: 1\nTotal Memory: 3.619 GiB\nID: JUS7:YXP6:MT7Y:YP5J:5EZD:MJBI:333A:TTYM:MKEW:NXCX:MWMV:AXLL\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): false\nRegistry: https://index.docker.io/v1/\nExperimental: false\nInsecure Registries:\n 10.0.0.0/8\n 127.0.0.0/8\nRegistry Mirrors:\n https://mirror.gcr.io/\nLive Restore Enabled: true\nI have also tried on Ubuntu 16.04.3 LTS\nIt looks like the issue is happening during container recovery, since it starts but does not complete recovery of containers.\nhttps://github.com/google/cadvisor/blob/master/manager/manager.go#L323\nCan you run it with --v=10 to see if we get any more logs?. It looks like we are picking up some cgroups ending in \".swap\".  I dont think this is related to disable metrics but probably isnt intended to be picked up...\nYou can try out #1800 to see if it helps to monitor fewer cgroups.\nEarlier you said it only used ~350Mb when running without --disable-metrics=disk, but it looks like it is using ~1.9Gb / 2 Gb.  Or am I reading that wrong?\nIt wouldnt be surprising if it OOMed if it is running so close to the limit. . closing due to inactivity. /ok-to-test. Can you explain the change in the rkt handler a bit more?  I would prefer just removing the code instead of commenting it out.. Would you like me to merge this, or do you want another set of eyes for the nvml changes?. The error message in the title occurs because you are not running rkt.  cAdvisor attempts to connect to all container runtimes it knows about.  Since you are running docker, but not rkt, it is expected that it will fail to connect to rkt.. Are you experiencing any other issues?. No, there is no flag.  The way cAdvisor works is by monitoring all containers (and cgroups) on the machine it lands on, including docker, rkt, cri-o, and containerd soon.  If you are not running rkt, for example, then it will not monitor rkt containers and will produce that log message.  However, docker container monitoring should work regardless of whether or not you are also running other container runtimes.  So this error message should be benign.\nAre you experiencing any other issues?. I dont think that is related to this issue @Asher-Shoshan.  Based on the documentation that may be the only option for some OS distributions.. You can probably still use cAdvisor in that case.  You just wont get docker metadata.. Is this the cadvisor that is embedded in the kubelet?  Or are you running a seperate cadvisor pod?\nThat error is generally a permissions issue.  Or it could also be that you have docker set up in a non-standard way, and need to configure cadvisor.\nI was under the impression that kubelet->heapster or kubelet->metrics api server could provide metrics for autoscaling, but my knowledge may be outdated. Have you checked out the autoscaling docs?\nThe kubelet's cadvisor port is 4194, although we plan to deprecate this in the future. Try curl localhost:4194/metrics from a node.\nThe kubelet also exposes the summary API, which is exposed on port 10255.  Try curl localhost:10255/stats/summary from a node.\nHeapster is an open-source aggregator, which collects metrics from the summary API, and can be used for autoscaling (see the docs above).. consent based on https://github.com/google/cadvisor/pull/1585#issue-204371578\n. heapster collects disk-io stats?  Does it use cadvisor directly?. right, but does heapster consume metrics from cAdvisor's APIs?  I was under the impression that it only consumed metrics from the summary API. IIUC, this wont affect the summary API, as we dont marshal/unmarshal when the kubelet consumes this. \nSo this change is purely to allow heapster to consume this, which is fine.. /ok-to-test. Not actually sure.  Presumably we can fix it, but I'm not sure I have time to fix it in time for the release. @khenidak, I would love to move back to fsnotify, but I havent found time to do so.  If you know what the issue was, and can fix it, I would be happy to review a PR.. Yes, the goal is to move to fsnotify.. I tested this by running cadvisor locally, and verified that the page loads without any console errors.  It looks normal too.. sure.  Ill work on that. The best reference is probably taking a look at the APIs: https://github.com/google/cadvisor/blob/master/info/v1/container.go. They are organized within the info directory.. cc @mindprince . > My guess is that it's unlikely that there are containers with GPUs attached when the kubelet starts up but I maybe wrong and we can change that in the future.\nwill this work across kubelet restarts then?. This may be because the thin pool is disabled? https://github.com/google/cadvisor/pull/1588. I am not very familiar with dm.. going to rebase this on top of https://github.com/kubernetes/utils/pull/25. rebase finished. /test pull-cadvisor-e2e. That is an interesting idea.  Although for kubernetes, I would think that volumes are generally a better abstraction for storage devices.  Are there ways to use volumes for your use-case?. cAdvisor is a container monitoring solution, and isnt designed to monitor arbitrary filesystems.  In kubernetes, volumes are designed to handle network attached storage, and we have already implemented monitoring, attaching, mounting, and debugging for volumes.. You would need to add custom metrics to the prometheus exporter here: https://github.com/google/cadvisor/blob/master/metrics/prometheus.go. Closing as duplicate of https://github.com/google/cadvisor/issues/1616.  Feel free to discuss this there.. Where is that graph from?  cAdvisor monitors all cgroups, including the root cgroup \"/\", which will have much higher usage.  I will need more information to be able to figure out what is up.. Are you running with swap enabled?  We dont generally recommend running with swap enabled.  If you are, then the extra memory you see are pages that have been paged out.\nEven if you are not running with swap, RSS is somewhat lower than Usage, since Usage counts pages that have not been accessed in a while.  So if your program is using and freeing pages, those are not reclaimed until the machine gets close to running out of memory, and starts to reclaim inactive pages.. lgtm. great work!. It looks like this is available in the v2 API: https://github.com/google/cadvisor/blob/master/info/v2/container.go#L99, as well as exported by prometheus:\nhttps://github.com/google/cadvisor/blob/master/metrics/prometheus.go#L793.\nWhat version of cadvisor are you using?. This may be related to https://github.com/google/cadvisor/issues/1704.  I still need to release a version with the fix.. The fix should be in v0.27.3 and v0.28.3. Looking at the two versions, I cant see anything that should have an effect on node disk metrics collection:\nhttps://github.com/google/cadvisor/compare/release-v0.25...release-v0.26\nAre any other metrics missing, or is it just that metric?. /ok-to-test. Just to make sure I understand correctly, cadivisor would return \"Unknown\" for anything that it times out, right?\nCan we make \"Unknown\" an exported constant so that we can handle it in the kubelet?  It may make sense for the kubelet to use the previous value for something if it recieves an \"Unknown\" response.. lgtm.  Squash, and Ill get this merged.. Ill cut another release. done. /ok-to-test. What is your use case?  We currently assume that the machine info does not change.  Are you using an orchestrator?  Or just a runtime?. We could definitely add an architecture doc.\nAt its core, cAdvisor watches for changes in the cgroup tree using a filesystem notifier (currently inotify).\nOnce a file addition is discovered, the path to the new cgroup is passed to each container handler that is registerd.\nEach container handler knows how to identify a container based on the cgroup path.  Docker, for example, extracts the docker id from the cgroup path, and queries the docker client for that docker id to see if that cgroup is a docker container.\nMany metrics are extracted from cgroups, including memory, cpu, diskio, network, and more.  We use the opencontainers/runc/libcontainer library to extract these metrics from cgroups.\nOther metrics, such as disk, inodes, and GPU metrics are collected separately from cgroups by using runtime-specific knowledge, such as where the container's writable layer exists on disk.\nEach container's metrics are collected at a roughly 10s interval.  Historical data for each container is contained in memory, and supplies these metrics for all APIs and storage sinks.\ncAdvisor has two json APIs (v1 at api/v1.3, and v2 at api/v2,0, and a prometheus endpoint at metrics.  cAdvisor also has a number of storage sinks. I dont think updating latest is part of our release process right now.  That would explain why it is so far behind.. I would recommend 0.27.3, unless you use prometheus, in which case you would want to use v0.25.0 or v0.28.3.. I updated latest to v0.28.3. You would have to add it to the prometheus endpoint.. No one is working on it as far as I am aware. cAdvisor doesnt try and maintain container state.  It can tell when a container exists by examining the cgroup tree, but cant tell what is running in it, or anything else.  Monitoring docker directly for this information is probably the best solution. You could write a lightweight exporter to query docker, and export metrics in prometheus format, or use one of the prometheus exporters that scrape the docker api.. You could also look for when there are memory or CPU metrics from cadvisor.  Stopped containers do not have any processes, and thus should have 0 for both of these metrics.. looks like it doesnt have permission to access /var/lib/docker/image/aufs/layerdb/mounts/REDACTED_HASH/mount-id.  It needs this to collect disk metrics.  Try creating a shell-based containers with the same permissions, and seeing if you can open the file.. /ok-to-test. /test pull-cadvisor-e2e. you will need to rebase.  Verifying assets fails because of the new year. oh, sorry.  Didn't realize this was on another branch.  Run ./build/jenkins_e2e.sh.. /test pull-cadvisor-e2e. /test pull-cadvisor-e2e. Can you cherrypick this to the v0.28 branch as well?. #1830 is into master.  I dont want to ff the v0.28 branch anymore, since it is after kubernetes 1.9.0 was cut.. Sorry, my mistake.  This made it into the 1.9.0 release...  So we only need to cut a release for  the release v0.27 branch. @jsravn v0.27.4 has been cut. That is an ancient issue, and I dont think we can actually retrieve container's ip address in newer versions of docker.  cc @Random-Liu . Can we add a glog.V(4).Infof message specifying which mountpoint was not found if utils.FileExists returns false?  This will allow us to find issues during testing.. looks like it has been failing for a few days: https://k8s-testgrid.appspot.com/sig-node-cadvisor#cadvisor-kubelet.  Not sure why.. Can you regenerate the assets?  I think the change from 2017->2018 made the tests fail (happened in k/k as well :)\nRunning ./build/jenkins_e2e.sh seems to do the trick. Classic new year's bugs. The writable layer disk usage is already reported for containers.  Image disk usage is a seperate concept from container usage, as images can be shared between many containers.  It looks like we have some image functionality added, as I can see there is an API type for docker images  But it looks like it is only used for the UI.  The only reasons I can think of that this isnt exported in the versioned APIs or prometheus is that it is docker specific, and adds additional API calls to the docker daemon, which can easily be overwhelmed.  Any implementation would need to be disabled by default.\nHow are you using cAdvisor?  Are you planning to use the Json API directly, or do you plan to use prometheus, or another storage plugin?. /ok-to-test. I can take a look.  From my first glance, I would rather not add a dependency to do the detection of mount changes.  For something that simple, we should just add it to cAdvisor.  Also it looks like you didnt use Godep to add the dependency.\n. /ok-to-test. I think we should collect the total usage the same way regardless of whether we are ignoring per-cpu stats.  If we are ignoring them, just skip that portion.\nI would be interested to see how the CpuStats.Total compares with summing the per-cpu stats.. looking at the libcontainer implementation, it reads from cpuacct.usage to populate stats.CpuStats.CpuUsage.TotalUsage.  It reads from cpuacct.usage_percpu to populate stats.CpuStats.CpuUsage.PercpuUsage.  I think using CpuStats.Total to populate our total cpu usage makes more sense.  We should use that regardless of whether or not we are collecting per-cpu usage.. Can you do a sanity check to see if it gives approximately correct values?  You can use the stress container, by using image k8s.gcr.io/stress:v1 to test it, and run it with args: --cpus=X, and see if it uses the correct quantity of CPU.. Oh, and please also squash your commits.. @bboreham have you done your sanity check?  I would like to get this in the next release.. Thats strange... usually it is easier to run things outside of a container than inside.  Are you getting any errors related to GPU initialization in cadvisor?\ncc @mindprince . It gets some container metrics, such as cpu and memory, through cgroups.  It gets disk metrics by using statfs on the filesystems, or du to get container writable layer usage.  Runtime-specific metrics or info are retrieved directly from the runtime.  . I am not quite sure what you mean by intrusive.  I would say it isnt intrusive at all--containers monitored by cAdvisor are entirely unaware of its existence.  For most metrics, cAdvisor simply observes the cgroup tree, and reports the metrics it exposes.\nI think it depends on how you deploy your software.  Running cAdvisor as a container could allow you, for example, to set container CPU or memory limits if you want to ensure it doesn't consume too many resources.  You can use systemd configurations or containers to automatically start cAdvisor, and restart it if it were to crash for some reason.  I dont think either is better or worse.. no problem. Anything interesting in the cadvisor logs?. This is probably because of dynamic housekeeping, which adds jitter to the time between stats collection to try and prevent spikes of usage.\nSee https://github.com/google/cadvisor/blob/master/docs/runtime_options.md#housekeeping for more info.. @tallclair do you think this is a reasonable change?  It is technically backwards-incompatible, but just requires users to use a different field.\n@andyxning I think labels make more sense as part of the reference, rather than the spec.  In kubernetes, labels are part of metadata, rather than spec.. @andyxning that makes sense to me.  We can keep it in the spec for now.  If we ever create a v3 api, which is unlikely to be soon, we can include labels in the reference.\nI am still hoping to get a second opinion from @tallclair on whether the backwards-incompatible change is acceptable.. I talked with @tallclair, and we think the best path is to put a deprecation notice in for this coming release (which isnt too far off).  Then, after that, we can merge this and the release after that will not have the field anymore.. added a milestone so we do not forget.. it could be because you need to run godep restore ./... and godep save ./.... and occasionally, I need to run godep save ./... more than once.. Just a heads up: If you would like to cherrypick this commit to kubernetes 1.9, 1.8, etc, you first need to cherrypick it to the corresponding cadvisor branch: v0.28, v0.27, etc, and then cut releases in each branch, and update the version of cAdvisor that each branch vendors.\nIf this is just for kubernetes 1.10, then no action is needed, and Ill vendor all cadvisor changes in before 1.10 is cut.. /retest. The failure doesnt look related to RHEL.. superceeded by #1868 . closing as duplicate of https://github.com/google/cadvisor/issues/771.  There are some workarounds posted there.  See if any of those work for you.. Yes.  I would be happy to review a pull request. Can you change it to target master instead of v0.28. Yeah, it looks like it was exposed once upon a time: https://github.com/google/cadvisor/pull/142, but then removed as https://github.com/google/cadvisor/issues/1865#issuecomment-357943893 points out.  According to https://github.com/google/cadvisor/pull/431, it looks like some of the metrics were not accurate.. I would be willing to review a PR which adds those metrics.  I request that they be disabled by default (we have an ignoreMetrics mechanism you can use).  They should also be metrics associated with containers, rather than processes in the API.. Does this issue happen for short periods of time, or is it permanent?  Can you check the cAdvisor logs to see if it is logging any error messages related to connecting to docker?. I believe cadvisor didn't start up correctly.  What version of cAdvisor are you on?. It seems like a bug if we allow docker to timeout during factory registration.. cc @jsravn . @jsravn can we add an indefinite retry for startup?  The 5s timeout is probably fine.... /ok-to-test. /test pull-cadvisor-e2e. can you run ./build/jenkins_e2e.sh to update assets?. This PR must be the first to the v0.28 branch this year.... You will have to rebase before tests will run. this was updated in a different PR. @BenTheElder what version of go will the image have?. sgtm. /test pull-cadvisor-e2e. We had it set previously... maybe I should try it without first though. Required 'compute.instances.get' permission for 'projects/cadvisor-e2e/zones/us-central1-f/instances/e2e-cadvisor-ubuntu-trusty'. /test pull-cadvisor-e2e. /retest\ngce enforcer save us. /retest. (just so you know, its only green because I removed the integration tests from prow_e2e.sh... it only builds cadvisor, and builds the integration tests, but doesn't run them). /retest. /retest. /retest. /retest\n/woof. /retest. /retest. /retest. Whoops, I had added kubekins instead of pr-kubekins to the project . /retest. /retest. you will have to rebase before tests will work\n/ok-to-test. cadvisor tests are not working right now.  We are in the process of migrating them off of jenkins.. @jsravn I agree with @tn-osimis that just using a longer timeout would be preferable.  What I meant in the issue by \"infinite retry\", is actually using a call with a long timeout (e.g. 30 sec), but retrying the call if the error is a timeout.  This way, as soon as docker recovers, cAdvisor starts correctly.  If docker is permanently hung, we have bigger problems than missing metrics.  WDYT?. This will have to wait for #1868 before tests will pass.. you will have to rebase before tests will work\n/ok-to-test. /retest. I am going to wait for https://github.com/google/cadvisor/pull/1868 to be resolved before testing this.  We are currently in the process of migrating our testing infrastructure. you will have to rebase before tests will work\n/ok-to-test. Can you add this to the v2 api and implement conversion between them?  It should be pretty straightforward.. otherwise, it lgtm. oh, ok.  Didn't look closely enough.. I can't seem to reproduce the issue on head.\n$ docker run -d --name=abc123 busybox /bin/sh -c 'sleep 600'\n69b7c6640be01a52df751e5d6e45341fbfc7297bd86d3561414bde3201dbe048\n$ docker ps\nCONTAINER ID        IMAGE                                                                                                                                         COMMAND                  CREATED              STATUS              PORTS               NAMES\n69b7c6640be0        busybox                                                                                                                                       \"/bin/sh -c 'sleep...\"   3 seconds ago        Up 2 seconds                            abc123\n$ curl localhost:8080/metrics | grep abc123 | grep container_cpu_usage_seconds_total\ncontainer_cpu_usage_seconds_total{container_label_annotation_io_kubernetes_container_hash=\"\",container_label_annotation_io_kubernetes_container_ports=\"\",container_label_annotation_io_kubernetes_container_restartCount=\"\",container_label_annotation_io_kubernetes_container_terminationMessagePath=\"\",container_label_annotation_io_kubernetes_container_terminationMessagePolicy=\"\",container_label_annotation_io_kubernetes_pod_terminationGracePeriod=\"\",container_label_annotation_kubernetes_io_config_hash=\"\",container_label_annotation_kubernetes_io_config_seen=\"\",container_label_annotation_kubernetes_io_config_source=\"\",container_label_annotation_kubernetes_io_created_by=\"\",container_label_annotation_scheduler_alpha_kubernetes_io_critical_pod=\"\",container_label_component=\"\",container_label_controller_revision_hash=\"\",container_label_io_kubernetes_container_logpath=\"\",container_label_io_kubernetes_container_name=\"\",container_label_io_kubernetes_docker_type=\"\",container_label_io_kubernetes_pod_name=\"\",container_label_io_kubernetes_pod_namespace=\"\",container_label_io_kubernetes_pod_uid=\"\",container_label_io_kubernetes_sandbox_id=\"\",container_label_k8s_app=\"\",container_label_kubernetes_io_cluster_service=\"\",container_label_maintainer=\"\",container_label_name=\"\",container_label_pod_template_generation=\"\",container_label_pod_template_hash=\"\",container_label_tier=\"\",container_label_version=\"\",cpu=\"cpu00\",id=\"/docker/69b7c6640be01a52df751e5d6e45341fbfc7297bd86d3561414bde3201dbe048\",image=\"busybox\",name=\"abc123\"} 0.032998774\n. The initial issue looks like it is only recognizing the root cgroup.  Maybe there is a different issue?\n@marcbachmann, can you run the experiment I ran?. \"/\" is the ID of the \"root\" cgroup, which gives machine stats, and doesn't have a container name.  You should expect this to always be present.  The issue appears to be that metrics for other cgroups are missing, which could have a variety of different causes.. I think the log_dir flag comes from glog, which we dont control.  Are you adding a flag to your application that has the same name?\nAs far as being able to vendor only the client, the client needs the apis (stored in info/) to parse output.. This doesnt seem related to #1875.  Can you explain how this solves that issue?. you will have to rebase before tests will work\n/ok-to-test. you will have to rebase before tests will work\n/ok-to-test. Just curious, what errors does it fail with?. sorry, tests are not working right now.  We are migrating to prow.  It should be done in the next few days. @dims you will have to rebase before tests will pass. https://github.com/google/cadvisor/pull/1806 was merged, and is in the most recent release.. cc @BenTheElder . look at those gorgeous logs.... /ok-to-test. >> checking go formatting\nThe following files are not properly formatted:\nfs/fs.go manager/manager.go. `==================\nWARNING: DATA RACE\nWrite at 0x00c42044c360 by goroutine 83:\n  github.com/google/cadvisor/fs.setMountAndAddLabel()\n      /go/src/github.com/google/cadvisor/fs/fs.go:138 +0x1bf\n  github.com/google/cadvisor/fs.CheckFsInfoUpdateForever()\n      /go/src/github.com/google/cadvisor/fs/fs.go:183 +0x576\nPrevious read at 0x00c42044c360 by goroutine 123:\n  github.com/google/cadvisor/fs.(RealFsInfo).GetDirFsDevice()\n      /go/src/github.com/google/cadvisor/fs/fs.go:571 +0xf5\n  github.com/google/cadvisor/container/docker.(dockerContainerHandler).getFsStats()\n      /go/src/github.com/google/cadvisor/container/docker/handler.go:419 +0x21f\n  github.com/google/cadvisor/container/docker.(dockerContainerHandler).GetStats()\n      /go/src/github.com/google/cadvisor/container/docker/handler.go:470 +0x11a\n  github.com/google/cadvisor/manager.(containerData).updateStats()\n      /go/src/github.com/google/cadvisor/manager/container.go:556 +0x6f\n  github.com/google/cadvisor/manager.(containerData).housekeepingTick()\n      /go/src/github.com/google/cadvisor/manager/container.go:504 +0x235\n  github.com/google/cadvisor/manager.(containerData).housekeeping()\n      /go/src/github.com/google/cadvisor/manager/container.go:452 +0x306\nGoroutine 83 (running) created at:\n  github.com/google/cadvisor/manager.(*manager).Start()\n      /go/src/github.com/google/cadvisor/manager/manager.go:350 +0xa1d\n  main.main()\n      /go/src/github.com/google/cadvisor/cadvisor.go:155 +0x685\nGoroutine 123 (running) created at:\n  github.com/google/cadvisor/manager.(containerData).Start()\n      /go/src/github.com/google/cadvisor/manager/container.go:106 +0x4c\n  github.com/google/cadvisor/manager.(manager).createContainerLocked()\n      /go/src/github.com/google/cadvisor/manager/manager.go:1031 +0xcbc\n  github.com/google/cadvisor/manager.(manager).createContainer()\n      /go/src/github.com/google/cadvisor/manager/manager.go:949 +0xab\n  github.com/google/cadvisor/manager.(manager).watchForNewContainers.func1()\n      /go/src/github.com/google/cadvisor/manager/manager.go:1183 +0x43c\n==================. I think you have to use-racewhen you build.  The test is run by doing./build/prow_e2e.sh, and then./build/integration.sh.prow_e2e.shexportsGO_GLAGS=\"-race\". I'm actually not sure.  If you want to run with the same path the test is using, clone k8s.io/kubernetes, and from there runmake test-e2e-node TEST_SUITE=cadvisor. nvm.  When I cloned your PR locally, and ran./build/prow_e2e.sh && ./build/integration.sh, it failed with the same errors as the test suite.  I would either just use the failure from the test framework we have here, or try and get prow_e2e.sh to work locally for you. Use the failure linked here to debug.  Click ondetails -> artifacts -> artifacts ->  tmp-node-e2e-7b7badd2-cos-stable-60-9592-84-0 -> cadvisor.log` to see the race.. Based on the build log, it failed on ubuntu.  Looking at the cadvisor log from ubuntu, it looks like ubuntu hit a data race this time, although it may not be deterministic.  Check out https://storage.googleapis.com/kubernetes-jenkins/pr-logs/pull/google_cadvisor/1881/pull-cadvisor-e2e/686/artifacts/tmp-node-e2e-03d06ea9-ubuntu-gke-1604-xenial-v20170420-1/cadvisor.log\n. /retest. /retest. /test pull-cadvisor-e2e. https://github.com/google/cadvisor/pull/1555#issuecomment-286864135 is all the context I have. cc @bakins in-case he has any insight. @nightah, you are correct.  We have a similar isssue for devicemapper: https://github.com/google/cadvisor/issues/1772.  My current thinking is that watching the mounts, and updating them would solve this, but it is a fairly complicated problem. Just curious, why keep the feature?  Just in-case we need it later?. > @dashpole For backward compatibility. There might be users using it, right? It is part of cadvisor api.\nIt is part of the manager API, which doesnt have any stability guarantees.  A quick search of github doesnt show anyone using it outside of k8s forks.. ah, ok.  We can leave it in. /lint. @nielsole does this look good to you?. It isnt hard-coded.  You can either map port 8080 to a different port, or use the --port flag. feel free to reopen if you have follow-up questions. Please look through #1843, #1444, and #1461, and see if the solutions described there fix your issues. @lindhor thanks for giving me versions that work/dont.  I think this issue is because of https://github.com/google/cadvisor/pull/1807.  We were experiencing some issues with fsnotify that I didn't have time to debug, and had to move back to inotify.  It is depreciated, so it seems there are some problems with it.\nThis issue is likely linked to https://github.com/google/cadvisor/issues/1708.. Yeah, I'll prioritize getting back over to FsNotify soon.  Inotify has been deprecated for a while and we should move off it.\nI am closing this as a duplicate of https://github.com/google/cadvisor/issues/1708.  Anyone who experiences issues can try one of the workarounds listed here, or track progress on the underlying issue there.. /ok-to-test. /restest. It looks like the tests are a little flaky right now: https://k8s-testgrid.appspot.com/sig-node-cadvisor#cadvisor-e2e, but since it is failing consistently, it is probably something in your PR. Looks like all tests are failing with this on each distro: \nI0228 08:56:02.077] --- FAIL: TestMachineStatsIsReturned (0.01s)\nI0228 08:56:02.077]    machinestats_test.go:32: unable to post \"machine stats\" to \"http://localhost:8080/api/v2.1/machinestats\": Get http://localhost:8080/api/v2.1/machinestats: EOF\nThere is a panic in one of the cAdvisor logs: https://storage.googleapis.com/kubernetes-jenkins/pr-logs/pull/google_cadvisor/1891/pull-cadvisor-e2e/689/artifacts/tmp-node-e2e-db502bef-cos-stable-60-9592-84-0/cadvisor.log. Can you explain the issue a bit more?  Is it that certain OS distros don't have the file, or that the root \"/\" cgroup doesn't have this file?  Or something different?. I think this could be related to #1866.  We recently introduced a timeout for the connection to the docker daemon, but if it times out initially, it will never reconnect.. The error message you list shouldn't cause cAdvisor to restart.  Can you provide more of your log?. I would also recommend using tagged releases.  Perhaps try out v0.29.0?. What version of docker?  It looks like it is trying and failing to connect to containerd.. cc @Random-Liu it looks like there are some issues with docker 18.02.0.  Does that version have the new containerd built in?. The containerd error is probably a red herring. See my comment here: https://github.com/google/cadvisor/issues/1910#issuecomment-375394532\n@hshahar @delskev are there any other error messages in the log.  Would you be able to upload more of your log for me to look at?. Just to make sure, you passed --disable-metrics='disk,tcp,udp', right? I would recommend increasing housekeeping even higher to 10s as well.. yes.  I have actually been digging into exactly this on GKE for the past week, and haven't come up with anything.  I am in the process of audit logging on the file to see if I can figure out what is touching it. sounds good to me! Feel free to post a PR.. That is quite odd.  Do you know if they are actually using docker to run the \"rest of [the] containers? Or are they running them with something else?. /ok-to-test. Can you update the unit tests to test serMemoryRss?\n. checking go formatting\nI0313 16:05:18.659] The following files are not properly formatted:\nI0313 16:05:18.659] storage/influxdb/influxdb.go storage/influxdb/influxdb.go. cAdvisor does not get diskIO from docker stat.  It looks at the cgroup.  Which prometheus metrics are you missing?  Can you share your kubelet log?. Ill put this in my backlog of things to do.  Of course, I would be happy to review a PR which does this as well.. cc @Random-Liu . /ok-to-test. Is this the cAdvisor embedded in the kubelet, or a stand-alone cAdvisor?. There isn't a way to do this right now.  There was someone who was trying to add a whitelist for labels a little while back: https://github.com/google/cadvisor/pull/1735, but it wasn't finished.\nI would be happy to review a PR that does something similar.. @Random-Liu if this is intended, we should make the error message less crazy-looking.  Maybe something like: Unable to connect to containerd.  This is expected if you are not using containerd directly.  Then we can separately log the error with lower verbosity afterwards.\n@xgt001 We added containerd support recently (running directly against containerd, rather than docker).  If you are using docker, you should expect to get this error message.. cAdvisor gets its information directly from cgroups, so it is giving you the value stored in memory.limit_in_bytes in the container's cgroup.  If we used the machine's limit if none is set, that would be incorrect when using nested cgroups, where the real limit wouldn't be the machine's memory usage, but one of the parent's cgroup limits.\nCan you do this conditional in prometheus?  I am not super familiar with it, but I would think something like (container_spec_memory_limit_bytes==0)*machine_memory_bytes + container_spec_memory_limit_bytes would work.. cc @mindprince . I'm not sure how much use they have gotten since they were added.  I don't see an issue with adding them to heapster if there is demand for them.  I guess we will find out if it works well or not once people start using them.  Do you have users who are asking for these metrics?. /retest. /retest. /retest. Ok, the reason the e2e tests are failing is because of https://github.com/google/cadvisor/pull/1917.  For some reason, this race condition is more likely with fsnotify.  This will need to be rebased on https://github.com/google/cadvisor/pull/1917 to pass. This has been rebased on #1917, and is ready for review.. The only thing I changed relative to #1807 was renaming variables and comments with inotify -> fsnotify.  It is the same as the other PR otherwise.. you may have to run godep save ./... more than once :/. Thanks for this @Random-Liu!. @answer3y No one has added gpu metrics to the UI.  If you want to add this feature, feel free to open an issue describing what you want to add, and a PR with the implementation.. This metric is measuring virtual memory size.  \"It includes all memory that the process can access, including memory that is swapped out, memory that is allocated, but not used, and memory that is from shared libraries.\"\nfrom https://stackoverflow.com/questions/7880784/what-is-rss-and-vsz-in-linux-memory-management. Looks like you are in the process page.  You should be able to see disk usage in the container page.. Disk utilization should be one of the metrics collected in that version IIRC, but that was ages ago.  You could try running a newer version of cAdvisor as a daemonset to see if it was added since that version.. If I read this correctly, we will be collecting metrics on each GPU twice?\nI would like to hear more about how you plan to use this... It isn't going to be exposed in the summary API, and the kubelet cadvisor port is deprecated and will be removed in an upcoming version.  How will you access this information?\n/ok-to-test. /ok-to-test. This is a great point.  Ill add this to my list of things to do.... lgtm other than naming nit. Actually, how would you feel about making this an argument to manager.New()?  The desire to whitelist particular cgroup trees seems fairly kubernetes specific, and I would rather not add another flag if we can avoid it.  I think this would be a win for everyone in the community, as everyone would see better performance.  It would also give us flexibility to change this interface in the future if we want a different model without having to deprecate flags and such.\nThe only downside is that if people are using cadvisor metrics for particular cgroups that are outside of what we plan to monitor, those would no longer be accessible.. This looks good to me, although I would really like to hear @tallclair 's opinion on this.\n@andyxning have you brought this up at sig-instrumentation at all?  It may be valuable to hear more opinions on limiting the cgroups that are scraped.. /ok-to-test. What problem are you experiencing?  I don't see any unusual errors in the logs.... Not sure what you mean by \"stock\"ing metrics...  How are you running cAdvisor?  Just with docker, or in kubernetes as a daemonset, or using the one built into the kubelet?. can you try running this curl localhost:8080/metrics from the host and see if there is output?\nThat would help determine if it is a problem with cAdvisor, or a problem reaching cAdvisor from your browser.. It likely isn't an issue with docker @Eric-LiuGang, please share your cadvisor log, and try curling the /metrics endpoint as I describe in my previous comment.. can you open a pull request for this?. /ok-to-test. Did cAdvisor log anything before exiting?  I believe IBM was doing some testing on different architectures, but it broke recently.  . What version of docker are you using?  What docker storage driver are you using?  Does it happen consistently, or only occasionally?\nCan you verify that the r/w lager id on both good/bad nodes is stored in /var/lib/docker/image/overlay2/layerdb/mounts/<CONTAINER_UID>/mount-id?\ncAdvisor operates independently of kubelet.  The kubelet only queries cAdvisor, and doesn't provide it any information.  So it is purely a cAdvisor issue.  It seems like the correct behavior here is to continue on, even if we do not have a r/w layer id for the docker container (only drop the \"image\" field, but not the others).  There is also likely an underlying bug with the cAdvisor monitoring for a docker storage driver, or an incompatibility with a more recent docker version. sorry, typo.  Should be r/w layer id, not r/w lager id.\nFrom your log, it looks like cAdvisor isn't respecting the nonstandard docker root dir, since it is failing to open the file /var/lib/docker/image/overlay2/layerdb/mounts/<CONTAINER_ID/mount-id, when it should be looking in /mnt/containers/docker/image/overlay2/layerdb/mounts/<CONTAINER_ID>/mount-id.  But if that is the case, I am surprised it is only happening occasionally.  Can you check your \"good\" nodes, and see if you still see that error message?  I am wondering if it is a red herring. tcp metrics are disabled by default.  Try passing --ignore-metrics=\"\" to cAdvisor to turn everything on, and see if you get metrics in that case.. No, they are very expensive to collect when running lots of containers.  We recommend only enabling them if you are either running very few containers, or if you know you need the metrics.. Opened https://github.com/google/cadvisor/issues/1935 for verifying godeps.. That is fine with me. did you do godep save ./... (with ./... at the end)?  I sometimes have to run save more than once as well.... Looks like freeze is may 29th, so ill set the next milestone for may 20th.  Who knows what other bugs we will find in the meantime :). Of course, if you want extra testing time on something, feel free to bump the godeps sooner.. How are you measuring the CPU usage of zfs list?  Can you tell me a bit more about your setup: How large of a machine/vm, how many containers, and how many zfs pools?\nWe have assumed up to this point that measuring disk space on a partition/pool is cheap, and running du is expensive.  Thus, disabling disk just results in the latter being disabled.  I guess this is a case where the former is expensive as well.\nWe could make disabling disk also disable zfs accounting.  I don't know enough about ZFS, but there may be optimizations we could make for ZFS accounting to fox your problem as well.\nI would be happy to review a PR for either one. /ok-to-test. ping this PR when you have the CLA resolved. Should we do the same for find?  Does this require any additional packages in our Dockerfile?. ok, great.. They are usually lumped together.  I did a performance analysis at one point and found that find used about 1/2 the CPU of du, but i'm not sure about the disk io usage of find.. Is there a downside to using ionice?  Inode metrics are generally less important than disk metrics for containers.  It would be odd to prioritize the find command over the du command in that respect.  I am in favor of adding ionice here as well.. Also thanks for opening this. I 'm not sure why a go program would perform better than a c program.  We have tried this in the past, and it ended up being slightly slower when written in go.. looks like you already have a different version of docker installed?  I had no idea cAdvisor could be installed with yum.... The yum cadvisor RPM is not provided by or managed by this project.  It is possible someone else has added something, but the cAdvisor maintainers do not support it.. The volumes look correct... I bet it is related to this limitation.  You are trying to mount in /dev/..., which clearcontainers doesn't support.. If it doesn't work, then it probably hasn't been added.  I believe this is the code responsible for generating blkio metrics for containerd (docker): https://github.com/containerd/cgroups/blob/master/blkio.go.\ncAdvisor uses the runc stats library: https://github.com/opencontainers/runc/blob/master/libcontainer/cgroups/fs/blkio.go, so adding the functionality you are looking for probably means adding something there, and then bumping our runc godep in cAdvisor.. /ok-to-test. /ok-to-test. /ok-to-test. prometheus does not allow sending a metric with the same name, but different labels.  So we had to \"fake\" it by adding empty label values based on the set of all labels for a given metric.. By \"in the metric\" I presume you mean in the metric name?  I think this goes against the prometheus model for metrics.\nLabels are a big reason why prometheus became popular.   Older metric systems used to make the naming of the metric contain information about where it came from: e.g. you would have a metric named: cluster_a.namespace_b.pod_c.container_d.memory_usage.  You end up with a tree of metrics.  But this confines you to a single hierarchy.  Someone who comes along and wants an infrastructure based tree would want cluster_a.node_b.pod_c.container_d.memory_usage instead, so they can aggregate by node, rather than namespace.  But you can only get one of the two with any tree.  Labels allow arbitrary groupings, and can meet many different needs without conforming to any hierarchy.. Sorry for the slow response, but this is definitely something we should tackle.. I'm happy to review a PR that allows either label whitelisting, or a boolean for not including docker labels as prometheus labels.. Sorry, I should have updated here.  I went ahead and implemented label whitelisting yesterday: https://github.com/google/cadvisor/pull/1981.  Maybe label whitelisting is overkill...  Ill take a look later today.. Let me know what your opinion is too @jaloren . Dealing with lots of endpoints seems like a harder task for users than dealing with empty labels IMO.\nMy thinking was that if users wanted a particular docker label that is relevant for their metrics (e.g. \"app:frontend\" vs \"app:backend\"), and exists across all containers, it may be useful to be able to include that as well so they can use labels to do more useful aggregation.  But as no one has asked for that capability yet, I am inclined to omit it for now.. not needed. cAdvisor needs access to those filesystems in order to monitor other containers on the node.  There are many users who run cAdvisor in production.  We do our best to apply security patches to our software.  You should always perform a security review of open-source software you use.  Using this safely depends on your security boundaries, and your threat models, and is far more complicated than can be covered in this issue.. @anomaly256 \nYou are correct that we don't need write access to the docker socket (turns out you only need read access to write to a socket).  Ill update the documentation to reflect that.  However, having read access to the docker socket is the equivalent to root on the host (as you can create a container with arbitrary privileges), so there isn't a security boundary to be enforced by carefully scoped mounts.. You can't edit vendor files directly.  This is a change you will have to suggest to the runc folks, and then vendor into cAdvisor.  \nWhile I agree it could be useful in some cases, it has somewhat confusing behavior and would interfere with other monitoring agents on the node.  I might suggest running a separate container which periodically resets these values instead.. its very possible that cAdvisor doesn't include those metrics in the prometheus endpoint.  Feel free to add them here: https://github.com/google/cadvisor/blob/master/metrics/prometheus.go.  It looks like process metrics are in the v2 API, so we will need to write something to expose metrics from the v2 api to prometheus format.  https://github.com/google/cadvisor/blob/master/info/v2/container.go#L244. cAdvisor logs are found in the kubelet logs.\nI don't see any null labels, it looks like they are all filled in with empty strings if they are not found.  #1704 results in a different set of metrics each time.  Sometimes it shows only containers, sometimes only cgroups.  Your issue sounds different.. cAdvisor produces metrics for each cgroup.  It just happens to attack extra information to cgroups that it identifies as containers.. Just curious, what behavior did you see that lead you to this fix?. I tested this by adding a log line right before we add an inotify watch.  I verified that before this change, we see .mount cgroups being watched.  After the change, we no longer see .mount cgroups being watched.. /retest. /ok-to-test. /lint. Just a few minor nits, and this is good to go.  I checked the godeps and licenses, and they look good (2 apache, 1 MIT license).. We limit the number of concurrent du and find operations to 20 per node to avoid overwhelming disks.  Those metrics may also take more than 1 second to collect if the writable layer of the container is large.  How many containers are you running during your experiments?. do you see anything in the logs indicating that du or find took longer than expected?  Look for du and find on following dirs took... messages. Ah, it looks like the period for disk stats collection is hard-coded at 1 minute: https://github.com/google/cadvisor/blob/master/container/common/fsHandler.go#L58.  This is almost definitely because of performance concerns.. Disk io metrics (# writes, bytes read, io, etc) come from cgroups, and don't require running du or find to get metrics.. So they have a much lower performance cost. Is this needed for 1.11?  Trying to determine urgency :). You can update the cAdvisor godeps in master without a release once the 1.11 release has been cut.  We just need to make sure k/k releases have a versioned cAdvisor release, but master can be updated to a specific commit.. Yes, cadvisor expects docker to be running when it starts up.. 1. You can configure cAdvisor to restart when it exits with systemd.\n2. Same as above.\n3. You can do either.  cAdvisor relies on docker regardless of whether it is containerized or not.  It depends on how you package, configure, and deploy your software.. What is the entry-point of your upper components?  Do they call AllDockerContainers directly, or are they making an API call?\n. Sounds like it would be better to log a warning, and return a best-effort response.  Feel free to send me a PR, and ill review it.. /ok-to-test. This looks reasonable to me, although it makes the brtfs-specific code even more complex.\ncc @flavio @dmrub who have made improvements to this in the past in-case (and likely are more familiar with brtfs than I am), in-case they have any comments. cc @mindprince @MaximilianMeister @runcom . I will do all of the godep updates in k/k. this branch doesn't have the e2e testing changes, so tests wont pass.... whoops, just need to update docker in k/k. /assign @tallclair . Should we put cAdvisor in the kube-system namespace?\n. It is also worth noting that the stackdriver sidecar doesn't work in current form.  cAdvisor, because it adds labels for each docker container label, and produces more labels than the stackdriver sidecar is willing to accept.\nI plan to add a way to limit the set of labels attached to prometheus, and to exclude metrics specified in --disable_metrics from metrics in the prometheus endpoint.. (but I tested it with those changes, and it works out of the box). It can connect to the docker client even with /var/run readonly.. Talked with Filipe, and it turns out that to \"write\" to a socket, you actually just need to open the socket file.  After that, interactions with the socket are not governed by r/w permissions on the socket file.  So we can make /var/run read-only. I think that would be a better model in general since it means when we add a new metric that is disabled by default (a common scenario), previous values of --disable_metrics wouldn't add the new metric.\nBut it would be backwards-incompatible to change it, right?. We can change the flag to a whitelist from a blacklist in a separate PR if we want.   This PR should make prometheus more use-able for those who are ignoring metrics, and I would like to add this soon.. We should probably do that for both NewPrometheusCollector and manager.New.  Ill can change this PR to do that.. Thanks for reminding me.  I just cut the release, and I am working on the godep update for k/k. also related #1951. closing in favor of https://github.com/google/cadvisor/pull/1984. /ok-to-test. /ok-to-test. Yeah, check the beginning of the logs for a failure to connect to docker.  If that fails, you won't get labels or anything from docker.. I found this in the failure for https://github.com/google/cadvisor/pull/1979. The failure was on RHEL here: https://148.100.110.186/jenkins/job/cAdvisor_RHEL7_s390x_GHPRB/255/console . ```\n+ godep go build .\ngithub.com/google/cadvisor/container/mesos\ncontainer/mesos/mesos_agent.go:33: syntax error: unexpected = in type declaration\ngodep: go exit status 2\n```. Apparently type aliases don't exist in go versions before go 1.9.  We need to update the jenkins builds to use a more recent go version.. closing in favor of https://github.com/google/cadvisor/issues/1988. Thanks!. Since cAdvisor serves endpoints other than prometheus, and we don't want to make them all update metrics on each query, we can't make the change you are suggesting.\nThe correct change to make for you is to increase your housekeeping interval, and also increase the window you are taking your rate over.  Try making the window 4x the scrape interval, and making the scrape interval 2x the collection interval.  The ratio of these, rather than the absolute durations is what matters for obtaining an accurate graph.  Tweak them until it is close enough.. > add a plug into prometheus endpoint to start ondemand housekeeping when collecting metrics. ( perhaps with optional maxAge so that if we happen to land closely on a housekeeping tick, dont perform additional work )\nIf I understand the suggestion here, it is still to make prometheus scrapes trigger on-demand collection of metrics.  That still means users using more than one endpoint or users which have multiple agents scraping the /metrics endpoint will trigger lots of extra collection.  This isn't a performance issue, it is a precision issue that stems from a pull-based metrics model with prometheus.  For the Json API, we added on-demand metrics because we can accept arguments to the metric call, but it does not take effect unless explicitly specified.  Prometheus, AFAIK doesn't support passing arguments when it scrapes metrics.. In that case, I think it is acceptable to add a MaxAge parameter to the prometheus endpoint to match the json API.  I'm willing to review a PR that accomplishes this.. Your go version is out of date.  Try running with go version 1.9 or higher. See https://github.com/google/cadvisor/issues/1988, which tracked the bug that we found in one of out automated testing runners which was also out of date.. I don't think this is needed.  See my comment on your issue.. At first glance, this seems intentional, as stopped containers can be assumed to have no resource usage.  Can you tell me more about your use-case, and why this is problematic?\nMy understanding is that many orchestrators (e.g. kubernetes) leave containers in a stopped state after they are run to allow for introspection of stopped containers and their logs.  Emitting metrics for all of these would greatly increase the number of metric streams prometheus has to handle, with little apparent benefit.. That seems like a metric that should be provided by the container runtime, or container orchestrator.  I think the storage_duration flag mostly exists to support longer housekeeping intervals, and to change what data is included in the metrics which show the \"summary\" of things over time.  I'm not sure cAdvisor is the best way to get at the metric you are looking for.. What version of cAdvisor?  Which overlay driver is docker using? We had this issue on an older version of cAdvisor, so see if you can reproduce it on the latest version. I think this would be quite useful, as many users need to diagnose what the offending process is when they run out of open file descriptors.  We can figure out where in the API it belongs during the review of the implementation.. I think number of processes is a reasonable metric to expose.  IIRC, we expose this in the v2 Json API, so we just need to figure out how to expose this in prometheus format.. Labels are available in the container spec in the Json API: https://github.com/google/cadvisor/blob/master/info/v1/container.go#L49\nLabels are attached to prometheus metrics as prometheus labels.. Ah, I see.  Prometheus performs poorly with very large numbers of labels, which we often get with adding container labels.  I would recommend running cAdvisor as a daemonset if you need this functionality.  The cAdvisor port is disabled in 1.11 anyways and will be removed entirely in 1.12.\nI believe its also possible to get this information using kube-state-metrics from here.. cAdvisor provides the container creation time as part of the container spec: https://github.com/google/cadvisor/blob/master/info/v1/container.go#L46.  However, the method by which we collect this information has a known issue when the node is under memory pressure, and the metric may reset.  Querying the container orchestration tool you are using is usually the best way to obtain this information.. what do you mean by storage?  Can you tell me more about your set-up?. I am not very familiar with elasticsearch.  Could we change the elasticsearch plugin to also add container ID to metrics?  Does elasticsearch have a concept of labels, or attached metadata?. /ok-to-test. /ok-to-test. that would be most welcome.  Im happy to review.. This sounds like a duplicate of https://github.com/google/cadvisor/issues/1970.  Can you try with the canary version of cAdvisor (google/cadvisor:canary)?. I can take a look. /ok-to-test. make sure you follow this: https://help.github.com/articles/setting-your-commit-email-address-on-github/. From the fact that I dont see your icon on the commits above, this is probably what you are missing. /ok-to-test. Can you try querying the prometheus endpoint (/metrics) and see what is missing?  I am not sure what endpoint the UI is hitting there...  Also, uploading more of your log will help.. try turning your logs up to --v=5.  I can see from your logs, you are missing the \"Registering Docker factory\" line.  We should get a message at level 5 saying:\nRegistration of the Docker container factory failed:... which will let you know what error it is getting connecting to docker. Interesting.  It looks like when memory.use_hierarchy is set, cadvisor returns total_rss, but regardless of hierarchy, runc returns cache.\nWe made this change here: https://github.com/google/cadvisor/pull/1728, but runc still has the old behavior.  Can you open an issue with opencontainers/runc, and see if they are willing to change the behavior to match?. You are right.  Feel free to open the PR, and Ill review it. cAdvisor effectively has root on your node.  It has access to the docker socket, which would allow an attacker to run pretty much anything on the node (even if the socket is mounted as read-only), as it can create arbitrary containers with even higher permissions.  We do our best to keep our dependencies up-to-date, and fix security vulnerabilities, but that obviously can't guarantee that there are no potential exploits.. Using a reverse-proxy is one way to limit who has access to cAdvisor's APIs.  It all depends on your security boundaries and trust model.. /ok-to-test. #1965. let me know when you would like this merged (or if you are waiting for review from anyone else).. /retest. /ok-to-test. /ok-to-test. I can check to see if the corporate CLA has been processed.  Can you ping me the name of the corporation on slack (username is dashpole), or email it to me?. /ok-to-test. how does a queue of du and find requests affect the kubelet's health?  cAdvisor doesn't do housekeeping in parallel, so you can have at most one queued du or find call for each container.. /ok-to-test. The following files are not properly formatted:\nmanager/manager.go. /ok-to-test. looks good other than my comment. The units are in number of tasks.  This article does a decent job of explaining it: https://serverfault.com/questions/667078/high-cpu-utilization-but-low-load-average. It depends on the what you are using to run containers.  In kubernetes, container_spec_cpu_quota maps to container limits, and container_spec_cpu_shares is based on container requests.\nIf you wanted to know the percentage of cpu a container was using, you should be able to do something like container_cpu_usage_seconds_total/container_spec_cpu_quota* some constant.  I don't remember what quota is measured in off the top of my head.. My previous formula was actually incomplete, as the \"some constant\" is a configurable parameter, cpu.cfs_period_us.  Containers use the cgroup attributes cpu.cfs_period_us and cpu.cfs_quota_us to limit a container's CPU usage.  See the redhat's definitions.  Both attributes are measured in microseconds, although technically cpu.cfs_quota_us is microseconds of CPU time (not just microseconds).  container_cpu_usage_seconds is measured in seconds.  You can get the container's limit in CPUs/second by cpu.cfs_quota_us / cpu.cfs_period.  Then to get the percentage of this limit used over a period of time, take the rate of CPU usage rate(container_cpu_usage_seconds[10m])/(container_spec_cpu_quota / container_spec_cpu_period).. @szediktam \ndocker stats is the % of the host's cpu and memory: https://docs.docker.com/engine/reference/commandline/stats/#examples.\nyour query will give you usage in cores.\nI also suspect that docker stats is an average over a much smaller time window than 10 minutes. /ok-to-test. That seems like a bug, rather than something we should work around.  Any idea why it chooses a data point further back in time, rather than just the most recent data point?. > The metrics api should not return the out of date metrics.\n@ringtail I agree.  The question is: Why is cAdvisor is returning out-of-date metrics?  The current query should return the most recent metric for each container.  That should never go backwards in time for any reason.  While your change makes the problem go away, it isn't clear (at least to me) why it fixes the issue. It means it is likely to happen again in the future.\nBefore we move forward with this change, I would like to figure out why metrics returned by the prometheus endpoint go backwards in time.. This is fixed by https://github.com/google/cadvisor/pull/2124. > The way I understand it (and I may very well be wrong) the io.kubernetes.pod.name label is a generic docker label.\nIt is a generic docker label.  But it is added by the kubelet for the purpose of generating pod status after a pod restart (so it can identify the pod associated with a container).  That is why we consider it an internal implementation detail.  We could add the mechanism you suggest, although that would just be encouraging users to rely on the internal implementation detail.\nI am going to close this for now while I try and come up with a better way to do this.. I'm assuming you are taking a rate of the metric.  What is your rate window?  Using a very short window can result in spikes, as cAdvisor returns cached metrics.  But the long-term average should stay beneath the limit.. usageNanoCores is reporting 1.002 cores used.  The metric is calculated using the current and previous usageCoreNanoSeconds metric, which are ~10 seconds apart.  The reason it exceeds 1 core is likely because of small differences between when we read the cgroup file, and when we record the collection time.  I would recommend relying on the cumulative metric usageCoreNanoSeconds, and taking the rate over a larger window if you need to decrease the margin of error.. There is not.  Most storage backends support cumulative metrics, and are able to take rates over them.. /ok-to-test. Looks good.  Please add this to the prometheus table: https://github.com/google/cadvisor/blob/master/docs/storage/prometheus.md. /ok-to-test. cc @mindprince . % of what?  If it is of limits, that should be based on cpu cfs quota (and cpu cfs period).  If it is of CPUs, (i.e. 200% is 2 CPUs being used), then you already have the correct query, I think.. I think this is a duplicate of https://github.com/google/cadvisor/issues/2026.  Feel free to reopen if you that does not answer your question. hmmm... maybe we need to ignore .scope cgroups as well?  I am no expert, but I thought mount cgroups ended in .mount.... cc @derekwaynecarr @sjenning \nThis seems devicemapper specific. Are you sure that line is related to cAdvisor?  I would expect to see an OOM if cAdvisor ran out of memory.  Do you have the cAdvisor log to share?. Yep, that looks like the problem.  Closing, assuming your issue is fixed.. I don't believe the v2 and v6 storage drivers are compatible with each other.  This would be a backwards incompatible change.. /ok-to-test. @SamSaffron thanks a bunch for this.  It is quite helpful.  I agree that we should reduce the set of metrics provided by the kubelet.  You can see an overview of the roadmap here: https://github.com/kubernetes/kubernetes/issues/68522.\n\nDoes it make sense to ship a \"minimal\" build of cadvisor with \"docker only\" so people only using it to monitor containers don't need to load up mesos/rkt/systmed/crio/aws and so on?\n\nWe should make use of https://github.com/google/cadvisor/pull/1926 in kubernetes to ignore mesos/systemd/aws, etc containers.  We should also introduce an option to only collect raw cgroups (no docker, CRI-O, etc) for runtimes that provide container metrics via CRI.\n\nDo we want a flag for -disable_metrics diskio we have one for disk now?\n\nI would be happy to review a PR that adds diskIO as a metric that can be ignored.\nThe rough plan is for the kubelet to disable all metrics other than CPU/Memory/Disk starting in 1.15 to allow for a deprecation window.. Where is privilage enabled in the podSpec?. ah, I see.  I'm not sure there is much we can do, as we can't discover which container is using which devices any other way.\nIn the future, I am working on supporting out-of-tree monitoring of GPUs for kubernetes to avoid this: https://github.com/kubernetes/community/pull/2454/files?short_path=118849b#diff-118849b7d846b51868e42c9ce1d7299e.  But that likely wont be production-ready for a few quarters. We don't currently monitor for that.  Is it common for network interfaces to \"go down\"?. I am not very familiar with that area.  How would one determine if a network interface was down?  Is there somewhere else cAdvisor should be looking?. We recently added a \"prefix whitelist\": https://github.com/google/cadvisor/pull/1926, but don't expose it through flags, and just use /.  We could add a flag to set this in cAdvisor.\nAlternatively, we could allow setting the handlers that are enabled. I.E. collecting only from certain runtimes.. Try using --ignore-metrics to disable metrics you are not using.  Particularly expensive metrics are disk, diskIO, UDP, and tcp metrics.. The --enable-load-reader option is not set on the cAdvisor embedded in the kubelet on GKE for performance reasons.  You will need to add this to the daemonset you are using, and use hostNetwork:true (because of this https://github.com/google/cadvisor/issues/1287) in order to get those metrics.  If you are using the provided cAdvisor kustomize daemonset, I created an example patch that accomplishes that here:\nhttps://github.com/google/cadvisor/compare/master...dashpole:load_example\nYou could run cAdvisor with that patch with kustomize build deploy/kubernetes/overlays/loadReader/ | kubectl create -f -. /ok-to-test. /retest. /retest. @dims thanks for catching this.\nIll let @sashankreddya review before I merge.  I don't think this is worth a cherrypick, as it is mostly to reduce testing log volume.... /assign @tallclair . heh, our e2e tests and canary use different go versions.... I tested this using \ndocker build -t google/cadvisor:canary --no-cache=true --pull=true --file=deploy/canary/Dockerfile .\nto ensure the canary builds will succeed now.. There isn't one that cAdvisor publishes.  You can check out the kustomize daemonset we have, or make your own helm chart based on it.. /ok-to-test. I thought we agreed in the issue to find the root cause?  This seems like another workaround.  I prefer the original workaround to this one, as this one adds new metrics.. I am assuming this was opened by mistake. for reference, what version of kubernetes are you using?. /ok-to-test. You can see all of the memory metrics we produce in prometheus format here: https://github.com/google/cadvisor/blob/master/docs/storage/prometheus.md#monitoring-cadvisor-with-prometheus.  I don't think the cgroup tree supports explicit heap memory metrics, but it does have RSS.. cAdvisor needs access to the host's filesystem (at minimum /sys/fs/cgroup/) in order to provide container metrics.  I don't know of a way to do that remotely.. There isn't a plan to do so.  The general plan within the kubernetes community is to encourage container runtimes themselves to produce container metrics to make the metrics pipeline simpler and more direct.. Yeah, is probably just hasn't been added yet.  As long as it is not output when the metric is disabled, I think it is appropriate to add.. If you are using kubernetes, I would highly recommend running the Node Problem Detector, which is purpose-built for collecting kernel events: https://github.com/kubernetes/node-problem-detector#node-problem-detector. what version of cAdvisor is this?. Can you run a quick experiment for me?  Run a container (busybox or some basic image) with the command, and show me the same stuff as the original post:\ni=0; while [ $i -lt 100 ]; do dd if=/dev/urandom of=file${i} bs=1048576 count=1 2>/dev/null; i=$(($i+1)); done; while true; do sleep 5; done\nThat should write almost exactly 100 MiB to the container's writable layer.  The biggest problem for me is that I'm not really sure what docker's definition of disk space used includes...  But I can at least check and make sure cAdvisor is returning the correct value.. The reason we didn't decide to include that in our metric is because images can be shared.  I.E. you wouldn't be able to sum over container's disk space to get aggregate usage.  We do have basic image information from this API: https://github.com/google/cadvisor/blob/master/info/v1/docker.go#L32.  I havent been able to figure out where we serve it outside of the UI yet though.. /ok-to-test. can you rebase on-top of https://github.com/google/cadvisor/pull/2081 to fix the test failures?. One question, but this is good to go. assigning @tallclair . cc @Random-Liu . /ok-to-test. ignore the test failure.  This needs to rebase on-top-of https://github.com/google/cadvisor/pull/2081 once it is merged. git reset HEAD~8; git add -A; git commit -m \"your message here\";. You will need to rebase on top of https://github.com/google/cadvisor/pull/2081. you will need to rebase on top of https://github.com/google/cadvisor/pull/2081 because our test-infra recently updated to go 1.11. /ok-to-test. The following files are not properly formatted: container/common/endian_little.go container/common/helpers_unix.go container/common/endian_little.go container/common/helpers_unix.go. Is there a reason why you added a new repo and vendored it in here?  That might make it harder for us to make changes in the future.  Are there any other more commonly used implementations of this?\nI'm also not sure if we can use BSD licenses or not.  Let me check.... Is there a golang function we can use that isn't private?  Yeah, we keep all of the code in the repo that we don't own.  If it is code from golang, then the proper thing to do is keep it under their license and vendor it in.  If you can actually add a commit which is the exact copy of the original golang code, and then your seperate changes on top of that, that would be easier for me to review.  Also link to the original golang code if possible.. sorry for all the hassle :/. So the proper way to do this is:\n\nmake your own fork of the original go repo (the entire thing)\nmake you changes to that fork\nvendor that fork into cAdvisor\n\nYou leave their license and boilerplate (header of the files), just as it is.\nI'm not sure how licenses work if you are just copying files, so please use the above approach.. cAdvisor uses the cgroup hierarchy as its source for metrics, rather than /proc files, so they won't be the same.  CAdvisor's memory usage metrics, container_memory_usage_bytes / container_spec_memory_limit_bytes, maps roughly to memory usage %age on linux, and rate(container_cpu_usage_seconds_total) is about maps to cpu usage.. It probably just isn't implemented.  Custom metrics are not very widely used, and the combination of custom + graphite may not have been tried yet.  Feel free to add it here: https://github.com/google/cadvisor/blob/master/storage/statsd/statsd.go. I'm not sure I understand... What do you mean by \"external machines\" or \"metrics running on kubernetes\"?  You could run cAdvisor as a daemonset, if that is what you are trying to do: https://github.com/google/cadvisor/tree/master/deploy/kubernetes#cadvisor-kubernetes-daemonset. what version of elasticsearch are you using?  IIRC, we only support v2. sadly i'm not very familiar with elasticsearch.  It looks like it is failing to connect to the host you provided.  cAdvisor is just a very simple wrapper around the elastic client library.. /ok-to-test. Note that this should wait until after 1.32.0 is cut. can you post the duplicate metrics?. Ah, you have found metrics for the pod's cgroup as well.  The top one is container metrics, and the bottom is pod metrics.. Since you are using the cAdvisor embedded in kubernetes, no.  But you can always filter for a metric that has a non-empty container name.. cAdvisor exposes metrics for all cgroups on the node, so the pod cgroup metrics should exist in all versions of cAdvisor.. /ok-to-test. /ok-to-test. I got a container running using NVML a couple weeks ago: https://github.com/dashpole/example-gpu-monitor/blob/master/deploy/kubernetes/daemonset.yaml.  It probably isn't a great example in terms of security, as it just makes the pod privileged, but it is somewhere to start.. @Cherishty just exec into the pod, and wget or curl localhost:8080/metrics, and look for the container_accelerator... metrics.  It is also worth noting that cAdvisor only monitors GPUs being used by containers, so make sure a container is running using the GPU when you are looking for metrics. Please do not use the example GPU monitor.  I just shared that as an example of how to get NVML to work from inside a container.  I am adding this patch for the cAdvisor daemonset to show how to get cAdvisor working.. It shouldn't matter how your container is consuming GPUs.  cAdvisor interacts directly with the cgroup tree.. hmmm.  I confirmed that the container_accelerator_... metrics are in the 1.9.5 release.\n@Cherishty it looks like NVML loaded successfully: nvidia.go:100] NVML initialized. Number of nvidia devices: 1.  What OS are you using?  It looks like you are running into https://github.com/google/cadvisor/issues/1444. Glad you got cAdvisor working.  I don't see any unusual errors in the kubelet log you provided.  It doesn't look like it is actually the full log from kubelet startup (as it usually starts with the flags provided by to the kubelet).  Try sudo journalctl -u kubelet --no-pager | grep NVML. Can you provide the full kubelet log from a run after you restart the kubelet?  sudo systemctl restart kubelet. Ah, it looks like the log line in question is emitted at V(4).  Can you increase the verbosity of the kubelet to --v=4 in KUBELET_EXTRA_ARGS, and look again?. container_accelerator_duty_cycle is the name of the utilization metric.. @reachmeselva the first glance looks like everything you need should be present.  Can you check the prometheus API at /metrics?  I think the problem might just be that the influxdb storage plugin doesn't push accelerator metrics.... Yeah, you can see all of the series pushed to influxDB.  If the prometheus endpoint returns the correct metrics, then all that needs to be done is add accelerator metrics there.. In storage/influxdb/influxdb.go#L224, you add the influxDB \"points\" from the v1 container stats.  You just need to take the stats in the Accelerators portion of container stats (info/v1/container.go#L583), and change them to the influxdb format.. It shouldn't require configuration; it is just obtained from here: https://github.com/google/cadvisor/blob/master/utils/cloudinfo/aws.go.. > Couldn't collect info from any of the files in \"/rootfs/etc/machine-id,/var/lib/dbus/machine-id\"\nInteresting that it is looking in /rootfs/... instead of /etc/....  It looks like cAdvisor is detecting inHostNamespace incorrectly, and adding /rootfs to the beginning of the path. It looks like we assume we are are in the host namespace if we can't find the directory /rootfs/proc: https://github.com/google/cadvisor/blob/master/manager/manager.go#L196.  Does that path exist for you?. > Yes, the directory exists in the cAdvisor container:\nOh, I thought you were running this as a daemon, not a container.  So cAdvisor it is correctly detecting the path: /rootfs/etc/machine-id.  So the reason why cAdvisor doesn't have the machine id is because the file /etc/machine-id doesn't exist on the host.\nWe have two options: either create this file on the host, or add to cAdvisor to detect a different file for this  OS.. thats strange.  Can you exec into the cAdvisor container and make sure the file is there?  It only tries to get that info on startup, so you need to restart cAdvisor after creating the file.. merging based on https://github.com/google/cadvisor/pull/2097#issuecomment-437439429. I wonder if it is the fact that you are now running 2 cAdvisors at the same time?  When you are running the daemonset, can you still query the kubelet's cAdvisor (:10255/metrics/cadvisor). Can you post the output of docker info?. Hmm, we haven't seen any issues recently with overlay2. can you try running with the cadvisor-args patch, and see if that helps?  I'm wondering if there is an expensive metric we collect by default that that disables, or if the housekeeping interval is different.... my best guess is that you need to install gcc.. /ok-to-test. rawPrefixWhitelist looks like what you are looking for...  I don't want to add another nearly identical cgroup prefix.. I don't believe it works with unified cgroup.. cAdvisor detects which GPUs are being used by a container using the devices cgroup.  I'm not actually sure how the nvidia device plugin gets access to GPUs, as the daemonset doesn't mount /dev.  It must use some other method to get access to GPUs so it can manage them which uses the devices cgroup.. I'm not sure of any way to avoid it with your current setup.  I know for GKE we use a different device plugin for nvidia GPUs that I don't believe has this issue, and doesn't require using nvidia-docker.  See https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/#deploying-nvidia-gpu-device-plugin for more details.. are you seeing any strange behavior?  Missing metrics or crash-looping?. Feel free to open a PR with these improvements.  If we use multistage build, we should add documentation about the docker version required to build.. Can you fix the CLA?  It looks like you are pushing a commit by someone else?. /ok-to-test. That is definitely a good principle, but infeasible for us at the current time.  We are hoping to move to using project quota to collect disk metrics in the future, but for now have to rely on du and find.. /ok-to-test. /ok-to-test. /ok-to-test. let me know about the os.IsNotExist(err); otherwise lgtm. You could, with the current approach, place the kubelet and runtime in different cgroups (not within system.slice) using the runtime-cgroups and kubelet-cgroups flags.. By the \"current approach\" I mean using rawContainerCgroupPathPrefixWhiteList, rather than adding a new whitelist.  That one collects all cgroups within the provided paths.  So if you had the structure /kubelet, /runtime, /kubepods/..., etc., we can pass those in when we start cAdvisor and get only the metrics you care about.. /ok-to-test. You are welcome to run the canary build.  We will have a release within the next month that corresponds with the kubernetes 1.14 release. /ok-to-test. With a few exceptions (image name and labels, for example), cAdvisor is scoped to collecting metrics that can be observed through cgroups and linux proc files.  It does not generally provide information about the specification of docker containers, such as volumes, ports, env variables, IP, etc.. Oh, it does have env variables.  My mistake.  We interact with a number of different container runtimes, not just docker.  We would need to find the equivalent information for each container runtime if we want to consider adding this.. /ok-to-test. container/mesos/mesos_agent.go:43:43: undefined: mesos. great work @Betula-L . /ok-to-test. looks like you will need to update the test data to include timestamps as well: https://github.com/google/cadvisor/tree/master/metrics/testdata. I'm not sure I want to publish cAdvisor using snaps.  As far as I know, cAdvisor is run almost exclusively as a container, or as part of the kubelet in kubernetes.. That's interesting, since that is the exact metric used for kubectl top...\nDoes kubectl describe no correctly report the number of cores the machine has?\nCan you take two samples from the summary, and compare usageCoreNanoSeconds rate of change over time so we can see the average CPU usage over a longer period of time?  usageNanoCores is an average over 10s, so it can be prone to spikes.. Yeah, that's interesting\n(1225852938179140 - 1225467515929555) / (22*60 + 16) = 288489707.773 = 0.29 cores/second = 15% cpu.  So it looks like kubectl top matches up with the long-run average.  Now the question is why the 10s average differs from the long-run average. Yes. The summary api is here: https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/stats/v1alpha1/types.go\nYou can read through the comments to find that info. /ok-to-test\nWhat is the effect of reporting coreId = threadId/nodeId=0?  Can you expand on that a little bit?. > cadvisor.go:94] Starting cAdvisor version: \"0.18.0\" on port 8080\nIt looks like you were using a really old version of cAdvisor before.\nI don't see anything unusual in the logs...  Is the docker daemon on the node responsive?. Based on the log, cAdvisor is hung at the beginning of createContainer, as the last log was from the previous createContainer call starting container housekeeping in a seperate goroutine.  My best guess is that you are either hanging on connecting to docker, or have deadlock.. docker_only makes cAdvisor only collect docker container metrics, and machine level metrics.  We don't actually expose rawPrefisWhiteList as a flag.  We may want to consider consuming this in the kubelet to get that behavior.. with docker_only, you only get cgroup metrics for cgroups that belong to docker containers, e.g. /kubepods/pod<pod_uid>/<container_id> will be collected, but /kubepods/pod<pod_uid> will not.\nMachine metrics are metrics from the / cgroup, as well as, for example, disk metrics from filesystems on the node.  We collect these metrics when --docker_only is specified even though it isn't a docker container cgroup.. no, but you should be able to use the --docker_only flag if you just want metrics for docker containers.. @viberan that sounds like a bug.  The --docker_only flag should mean you only get docker containers + the / (root) container.. can you paste your configuration (flags + cadvisor version) and the prometheus metrics you are getting here?. Oh yeah, you are correct.  That is a bug. Ill open something to fix it.... I am not sure about how to do it in rancher, but in kubernetes, prometheus server adds meta labels that we can use to identify where the metric came from: https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml#L126.  There is also a __address_ label that seems like it might work to at least make sure cpu usage isn't stacked.. > cadvisor currently does not provide free bytes and inodes for container volumes; \nBy container volumes, do you mean PVs?  Or are you including \"ephemeral local storage\" ones as well (empty-dir backed volumes)?\n\nthis information is useful for e. g. eviction decisions based on inode pressure in Kubernetes/OpenShift.\n\nCan you expand on this?  We currently only take usage into account for inode pressure.  We wouldn't want to take PV usage into account, as those are on their own filesystems and have a separate pool of inodes.  We already take empty-dir usage into account, but those metrics are provided by the kubelet.. /ok-to-test. > Ephemeral also (including writable layers).\nSo this is really just for container writable layers, right?  Because cAdvisor doesn't monitor empty-dir backed volumes.\n\nThe issue is that the container_fs_inodes_free metric is returning 0 for all volumes, so we can't alert on pods that are close to running out of inodes on one or more of their filesystems.\n\ncontainer_fs_inodes_free for a container should be the same as that metric for the node, right?  Can we just alert on that metric?  Does that work for devicemapper, or does that have a separate filesystem for each writable layer?. > I don't have a devicemapper cluster set up to test that (and the underlying issue referred to use of an overlay2 filesystem); with overlay2, I've verified that the free inodes is reported correctly with\n\ncontainer_fs_inodes_free{container_name=\"\"}\n\nI was just trying to see if this was to fix something for dm specifically.  I think my point still stands that you can use the node-level inodes free to monitor this.  In fact, you would end up with an identical alert per-container if you monitored this per-container.\nIIRC, the initial intent of leaving capacity-related (I consider free to be capacity related) out of container metrics is that we expect usage to be compared to requests or limits, not the actual capacity of the node.  For inode metrics, I can see your point, as we don't expose the capacity anywhere, or set requests/limits for inodes.  But I still think the principle of emitting only usage metrics for containers from cAdvisor is a good one.\nOn the other hand, if this is required to monitor dm-based containers, then this is a change worth making.. > If we decide not to do this, then perhaps we shouldn't be reporting the -free information for containers at all, to eliminate a source of confusion.\nCompletely agree. This makes sense to me, although we are planning to deprecate and remove some of the direct cAdvisor endpoints in the kubelet (kubernetes/kubernetes#68522). /ok-to-test. I think this is the right thing to do, even if it looks a little funky. its probably a test-infra issue.  Ill take a look today. wrong button. Happy 2019!  Please rebase on top of https://github.com/google/cadvisor/pull/2137, and that should fix it.. /ok-to-test. Happy 2019! Please rebase on top of #2137, and that should fix it.\nIll try and take a look today or tomorrow. I'm surprised this doesn't reintroduce https://github.com/google/cadvisor/issues/1704.  I thought all metrics in prometheus had to have the same set of labels?. /retest. What is your setup?  E.g. are you using docker swarm, kubernetes, or something else?. That looks like a problem with the docker registry, not a bug with cAdvisor.. It looks like we refresh stats every second: https://github.com/google/cadvisor/blob/master/pages/assets/js/containers.js#L1101.. > I understand fargate does not allow privileged access could this be the reason why?\nThat is most likely the reason why.  Access to the docker daemon is effectively root on the VM running the workload, which would give you access to anything else AWS is running there.\n\nHas anyone tried cadvisor with aws fargate?\n\nYou are the first that has opened an issue here.  Does fargate not already have monitoring built-in to the product?  It would surprise me if it didnt.. I pushed an updated image to dockerhub.  gcr.io/google_containers/cadvisor:latest is in the process of being updated.. > Disabling metrics doesn't exclude them from the /metrics endpoint or causing them to be zero every time, by the way\nThat is really odd.  Which metrics does this happen for?\nYour problem seems similar to https://github.com/google/cadvisor/issues/1774, as it manifests as inexplicably high CPU usage, but only on some machines.\n\nnow I must drop unnecessary metrics in the Prometheus and it was the best bad idea I could come up with. What is the best way to completely disable everything I don't need on the cAdvisor's side (including the /metrics endpoint)?\n\nI added https://github.com/google/cadvisor/pull/1980 just after the version you are using was cut.  Try bumping the version to v0.31.0 or higher.\n\nPS: why --disable_metrics option has network and tcp,udp at the same time? \n\nI agree the naming is confusing.  They are meant to be non-overlapping sets of metrics, as tcp/udp create an enormous number of additional metric streams compared with the basic metrics.\n\nI suggested network is a tcp+udp, but metric container_network_tcp_usage_total is always zero without enabled network\n\nThat sounds like a bug.. yeah, I need to update that.  I would try the new latest (as of yesterday), v0.32.0. Someone is working on the container_fs metrics:\nhttps://github.com/google/cadvisor/pull/2103\nNo one has ever requested disabling cpu/memory before, but we could add it.\n\nSeems like disabling network really causing tcp metrics be zero even if I use the v.0.32.0 tag.\n\nYes, this is a bug.  I'll look into it sometime.... Yeah, cAdvisor doesn't exec into anything.  You have to expose the endpoint outside of the container. Yeah, the feature never left alpha after being introduced a few years ago.. I generally recommend having the prometheus server scrape your application.  It removes the dependency on cAdvisor, and is generally how prometheus is designed to be used. Maybe try using push instead of pull?  That way only the prometheus server needs to be discoverable...\nhttps://prometheus.io/docs/instrumenting/pushing/. /ok-to-test. /ok-to-test. I0115 19:49:44.116] >> checking go formatting\nI0115 19:49:44.669] The following files are not properly formatted:\nI0115 19:49:44.669] manager/manager.go. What are you running in your container?  How is it writing to disk?. docker is probably reporting usage, whereas cadvisor reports working_set, which is usage - inactive_file.  working_set is the value used by kubernetes to make eviction decisions, which is why cadvisor produces it.. maybe you need to mount /dev/kmsg into the container?. use --disable_metrics to achieve this: https://github.com/google/cadvisor/blob/master/docs/runtime_options.md#metrics\nFeel free to reopen if you have further questions.. cAdvisor is currently built-in to the kubelet on each node.  The prometheus metrics are exposed at /metrics/cadvisor from each kubelet.  You need to install that config in the cluster to get metrics collected by cAdvisor.. What happens if you remove the container after stopping it?  I.E.\n```\ndocker stop  upbeat_brahmagupta\ndocker rm upbeat_brahmagupta\ndocker run -d --name upbeat_brahmagupta google/ipython-predictions\nca3c244985639d261c0fafdd8473e50f75e094013889c006b427277bcfab6ad6\n```. hmmm... We really need to move to fsnotify, but I haven't had any bandwidth.  Ill see if I can prioritize it, but can't make any guarantees.. See https://github.com/google/cadvisor/issues/1708. This looks reasonable.  Do we want to handle cases where one of the files isn't found?. /ok-to-test\nIll take another look after you resolve the CLA. nope, just fell off my radar.  Thanks for the contribution and the ping!. Can you be more specific when you say \"nothing works\"?  Does the kubelet simply log that and then exit, or does it hang somewhere, or does it just never report any container metrics?. I have looked through the changes between those versions, but can't see anything immediately that would affect this. This must be hanging in the machine.Info( function because we log the result immediately after that is called during initialization.\nThe only thing that happens between then is reading the machine id (/etc/machine-id) and the boot id (/proc/sys/kernel/random/boot_id), and getting all of the cloud info from the cloud-provider.. I am amazed you got it working at all and get any metrics.  Yeah, there is weak/non-existent support of windows 10.. Are you asking about changing the blog post?  Or changing the cAdvisor docs?. /ok-to-test. /lgtm. /ok-to-test. Does docker respond to other commands?  e.g. docker ps?. /ok-to-test. looks good, please squash. /ok-to-test. Does an equivalent exist for all container runtimes cAdvisor supports (mesos, containerd, rkt, docker)?\nWe usually try and stay away from spec-based metrics, as they tend to be runtime-specific, and generate large numbers of metric streams for each container.. /ok-to-test. > Because cgroups file may not exist at the timing of the read\nCan you describe such a case?. Which API are you using?. Can you try the machine info API: https://github.com/google/cadvisor/blob/master/docs/api.md#machine-information?\nIt looks like it should have a number of identifiers: https://github.com/google/cadvisor/blob/master/info/v1/machine.go#L201. I think we are just using whatever docker passes back to us.  Can you query docker and see what docker thinks the start time of the container is?. Are you using kubernetes, swarm, or something else?\nI think there might be a difference in the mechanism they use to \"restart\" containers on docker.  I.E. I think kubernetes might actually create a new container each time, whereas swarm makes use of docker's restart policy.\nThe container_start_time_seconds metric was intended to work for uses like this.. I agree with @ravjanga that we should probably separate the two out into two metrics.  The creation time is still very important, even for swarm users, as that has the start time of cumulative container metrics.. @ravjanga that makes sense, I forgot we only get the spec during container creation.  I do worry about the impact on docker of calling its api once every ~15 seconds for each container.. How much data is in the directory it is getting disk and inodes on?. Can you try it on a large directory?  Say 10 GB?\nThanks a bunch for running these experiments.. I am going to run our eviction tests with the PR as another data point.\nYou mentioned at the beginning: \"both du and find overwhelm our audit log\".  Have you done any experiments to show that this helps your issue?\n. eviction tests are all good.  It looks like the latency on du and find is about the same as this implementation in the inode eviction test (lots of small files).. Ah, I get it now.  It was the calls themselves, not the du and find took ... log messages that were overwhelming your audit logging.. /retest. /ok-to-test. You need to run build/assets.sh. Can you say more about your use-case?  Is serving metrics once per second problematic for some reason?. huh, still thinks the assets aren't correct... \nI0219 17:47:49.135] Found changes to UI assets:\nI0219 17:47:49.135] pages/static/assets.go. You have confirmed docker is running, right?  I.E. docker ps works.  I have run into some trouble in the past with docker being run as sudo, but cAdvisor not or vice-versa.  Does sudo docker ps also work?. See if any of the docker auth options we have can fix your problem: https://github.com/google/cadvisor/blob/master/docs/runtime_options.md#docker.  Otherwise, you would need to add a new option to cAdvisor to be passed through.. The tls cert option is local to cAdvisor.  What orchestrator are you using to run the cAdvisor container?  I'm only familiar with kubernetes, which has the concept of a Secret to pass that kind of information along with the container. . I think it should be relative to the cadvisor binary.  Normally, the orchestration layer handles secrets: https://docs.docker.com/engine/swarm/secrets/.  I'm not sure what the best practices are for doing this with just docker.. What do you mean by not run?  Do you still get the tls error?. do you have logs?. The network ones are all errors or throttling related.  Are you sending enough traffic to trigger throttling?\nIll look into container_fs ones.... The metrics in question come from this cgroup file:\n/sys/fs/cgroup/blkio/<cgroup_name>/blkio.io_service_time_recursive\nBut it looks like the metric only exist on CFQ enabled kernels.  I looked at an older cAdvisor I have running, and the those metrics are reported for some containers, but are 0 for most.. cc @sashankreddya \nwho has dome most of the mesos integration.\nCan you share the cadvisor log from when it starts up and doesn't get GPU metrics and when you restart it and it does?\nThere are a couple of potential reasons this could be happening that I can think of:\n\nYou are attaching a GPU to the machine/VM after starting cAdvisor.  cAdvisor doesn't initialize NVML unless there are GPUs present at startup.\nSomething strange with the mesos integration.\n\nWe should try and rule out (1) first.  For (2), I wonder if mesos does some late initialization of GPU devices or something?  The only reason cAdvisor wouldn't monitor the GPU is if it wasn't present when the container is initially discovered by cAdvisor.. Thanks for the log! So it looks like NVML initializes all 8 devices each time.  This is probably a quirk with how mesos uses cgroups. No.  Just that cAdvisor makes lots of assumptions about what a \"container\" is, and how it behaves.  If the mesos containerizer doesn't follow those assumptions, then we can't monitor it.\nCan you try running cAdvisor with --v 4 to see if we are getting this log: https://github.com/google/cadvisor/blob/master/manager/manager.go#L1024\nIf that doesn't show anything, you would probably have to add logging in https://github.com/google/cadvisor/blob/master/accelerators/nvidia.go#L163 to confirm that the nvidia device in question isn't in the devices cgroup when the container is started.. right.  I'm sure the device is there eventually, but i'm wondering if there is a race condition happening.  I.E. Mesos creates the cgroup, then cAdvisor reads from the devices cgroup, then the GPU is added.  If you can build cAdvisor with an extra log line which prints out the contents of the cgroup, we could confirm that.. alerting is done upstream from cAdvisor.  cAdvisor just provides the metrics.  For example, if you are using cAdvisor's prometheus endpoint, you can configure the prometheus alertmanager to alert on metrics provided by cAdvisor. It doesn't look like you can get the name of the container with that query, as it only returns a single \"name\" identifier for the container using that API: https://github.com/google/cadvisor/blob/master/manager/manager.go#L486.  Other APIs which return containerInfo will have multiple identifiers, including the container name.. Try /api/v1.3/subcontainers, which has pretty much everything.. Just to confirm, this doesn't happen on v0.32.0?. I think he is running cAdvisor as a pod in kubernetes, not using the cAdvisor built-in to the kubelet.. I would happily review a PR to fix it.  Yeah, this is something we should add to the kubernetes yaml.... In the influxdb storage plugin, it sends \"cpu_usage_total\" as the value of stats.CPU.Usage.Total, which is in nanoseconds according to the v1 api. yeah, it is a cumulative metric, so it is the total number of nanosecond-cores a container has utilized since its creation.. Yeah, it does seem like you hit #1704.  It looks like that was released in v0.28.3, which corresponds to kubernetes 1.9. . I'm afraid they stopped accepting patches to 1.8 a little while ago, as kubernetes currently patches the last 3 minor releases (currently 1.10 - 1.13 but 1.14 is about to be released).. can we just re-use the whitelisted_container_labels flag?. /ok-to-test. The kubelet re-registers the --enable_load_reader flag as part if it's flagset. So to answer your question, it depends on how you or your provider has set up your cluster.  For GKE, for example, the options are set at /etc/default/kubelet, but that might change if you used something like KOPS or something else. You are welcome to open a PR to add more metrics to the elasticsearch storage backend.  The code for that is in the directory: https://github.com/google/cadvisor/tree/master/storage/elasticsearch. What is querying cAdvisor?  Usually the aggregator (e.g. prometheus server) adds information about the thing being queried, such as host ip.  cAdvisor does have a machine api /api/v1.3/machine that provides some information, such as cloud provider and instance id.. You will need to rebase on https://github.com/google/cadvisor/pull/2191, since the update to go 1.12 made our go-lint fail. Huh, I don't see any errors that should cause it to exit.  Those errors will just cause it not to have OOM events or the machine id.  It looks like cadvisor was given an external exit signal: Exiting given signal: terminated . can you try turning up the log verbosity? --v=4 should provide more info.  It is also really strange that you get each message twice.  Mind posting your swarm config for cAdvisor?. Its just a command line argument, like logtostderr, except it is -v. yes, that looks correct.. container/common/container_hints_test.go:30:3: Errorf format %s has arg cHints of wrong type github.com/google/cadvisor/container/common.containerHints\ncontainer/common/container_hints_test.go:47:4: Errorf format %s has arg cHints of wrong type github.com/google/cadvisor/container/common.containerHints\nI think maybe we updated the go version or something recently.  Can you fix the go lint errors?\nYour change looks correct, and should work for CRI runtimes.. I'm not very familiar with influxdb at all.  I'll mark this as help wanted, but I would recommend looking for influxdb expertise elsewhere.  I'm happy to answer questions about cAdvisor if any come up.. /ok-to-test. The following files are not properly formatted:\nmetrics/prometheus.go. The cAdvisor UI uses the json endpoint (/api/v1.3/subcontainers, for example) to supply data for the dashboards.  cAdvisor itself supports many ways of exporting metrics, including json, prometheus, influxdb, elasticsearch, etc.  Check out the exporting stats section of the README.  If you are using the /metrics prometheus endpoint, you can use prometheus server + grafana to collect, store, and graph data from cAdvisor.. It is definitely possible.  I recommend looking at what /api/v1.3/subcontainers returns for the full set of information.  You would have to parse it yourself if you don't use a metrics backend.. cAdvisor is only written in golang, i'm afraid.  The kubelet uses cadvisor's v2 container api.\nThe original question has been answered, so I'll close this for now.  Feel free to open new bugs with new questions.. /ok-to-test. are we using this flag for anything in cAdvisor itself?  Or just as a way to pass this value to the UI?. How have you determined that cAdvisor isn't listening on port 8080?  What happens if you run curl localhost:8080/metrics?. try api/v1.3/machine.  It has a numCores field that you can use to calculate the % usage.  You will have to take a rate of the total, and convert to nanocores first though.. I believe it is actually, numFiles + 1 == inodes, since the directory I created counts as well.  I could change it to that, but I wasn't sure if it would consistently be the case.\n. Just manually tested this since I couldn't find it on the web.  Killing the command causes Wait to return an error.\nAlso, while testing, I found that the timeouts were not working properly for this command.\nioutil.ReadAll blocks until the end of the stream, so timeouts were waiting until the command finished to start...\nIll add a test to ensure that timeouts function properly.\n. stdout = int64 + \\n, so the Fields call was just to remove the newline character.\n. I think that is what actually happens.  &val.Filesystem[0].Inodes may be nil if we skipped collection.  In that case, it should also set InodeUsage to nil as well?\n. I am very likely wrong, but I think the \"rootfs\" directory that it is pointed at is the container's modifiable directory, and shouldn't contain the base layers.  At least in most of my tests, the containers start out reporting 0 inodes used, which would indicate to me that they are not counting any shared resources in base layers.\n. Ill go ahead and try it out.  It shouldnt hurt to make it faster, and it should still work.\n. Confirmed, I changed it to numFiles +1 == inodes\n. Wouldn't it actually be more accurate to set InodesUsed to 0 in that case?  We only skip collection for some storage drivers if they do not use inodes, which means they use 0 of the inodes shared by the rest of the pods.\n. See the note I made in the Issue.  I was able to test this, and the find command works correctly (and takes almost no time, since it always finds 0 inodes) with devicemapper.\n. I tested this.  It only includes the writeable layer.\n. Just to document, since these storage drivers do not use inodes, I am leaving this the way it is, since using 0 inodes is equivalent to using no inodes.  Also, making a change here would require chaning the v1 api, or a large refactor of the code.\n. done\n. done\n. done\n. done\n. done\n. Ill try that.  It would be much simpler than what I did...\n. The reason go vet was throwing a fit was that we cant do the last line \"handlerMap[failing] = failingHandler\".\n. done\n. nit: no need to add storage to the front. nit: remove \"udp\", or add \"udp/tcp\" since protocol is configurable. nit: same as above. You should close connections:  \"return driver.connection.Close()\".  Correct me if I am missing something.. done. done.. done.. nit: remove this line.. could you expand this comment to explain what this is fixing?. The apply fix to btrfs mount line.  Doesnt seem like it conveys any important information.. remove \"(Established, Listen...)\". same as above. Add comment describing specifically what this stat is.. same as above. I realize we duplicate structs for TcpStat, but I would prefer if you removed this struct, and used v1.UdpStat in this file. nit: use the retentionPolicy variable here. To reduce the duplication here, could you create a method:\nparseVersion(version_string string, regex re.Regexp, length int) ([]int, error) {\nthat could be used by both parseDockerAPIVersion, and ParseDockerVerison?. If I read this correctly, you are only updating handler.restartCount when a new docker container handler.  Have you verified that this increases when the container restarts?. do you convert restartcount from v1 to v2 anywhere?. remove. just meant the extra comment. This is where we convert.  Should be simple to implement.. Yes, I was more making sure that we create a new dockerContainerHandler each time the container is restarted by docker.  Kubernetes doesn't use docker's restart policies, so I am not familiar with how we handle them.. nit: we usually place all of the golang imports (e.g. fmt, time, strconv) together at the top.. Please remove this.  It isnt used anywhere . I would prefer not to add functions here.  Do you think you could just create the list within the prometheus function like container_tasks_state?  Same goes for the TcpStatMap function. can you add a comment explaining why we want to return error if n != 1?. nit: use a constant for the directory. should we return an error here, instead of nil?. I dont think this is neccessary.  If we are just going to check to see if device == \"\" in GetDeviceInfoByFsUUID() anyways, we might as well just do self.fsUUIDToDeviceName[uuid] there.. Can we return an error here instead of nil?  This eliminates the need check if DeviceInfo is nil after calling in manager.GetFsInfoByFsUUID. sorry if I am missing something, but where is this used?. I would prefer having an error type:\nErrorNoSuchDevice = errors.New(\"No Such Device\")\nThen we can check for that error in the kubelet if we want to.. ahh, ok. nit: condense this\nif labelValue, present := ctnr.Config.Labels[exposedLabel]; present {\nhandler.labels[exposedLabel] = labelValue\n}. dont specify the runtime here: ... \"collected for containers\". I would omit runtimes here as well: \" cadvisor can restrict the set of labels it collects from containers.  This can be useful in ensuring metrics have a consistent set of labels.\". Do we want this to be nvidia-specific?  Would it be better to have a GpuStats that we could identify as an nvidia GPU?  I guess it comes down to whether all GPUs generally have the same metrics, or if Nvidia's metrics are different than AMD's GPU metrics, for example. How do we plan to identify a GPU in the device plugin model?  It would be nice if the minimum set of identifying information was the same across cAdivsor and kubernetes (e.g. maker, model, minor_number)?. Is this cumulative?  if so, we should specify.  If not, what is the period of time?. This should be a constant, right?  If so, does it belong in the monitoring pipeline?  Would it be better to surface it as a \"spec\" somewhere?. I am not familiar at all with nvml's capabilities, but I think it is generally preferable to use cumulative usage (in seconds), or another cumulative metric.  This allows metric aggregators to get average usage over short (for \"instantaneous\") or long (for average) intervals.  If we cant get that, then I would think it would be difficult to measure average usage over longer periods of time.  Do you know if the \"sample period\" is a constant?  The added c file below says \"Each sample period may be between 1 second and 1/6 second, depending on the product being queried\".. SGTM. Ok, I think that is reasonable.. done.. Is this common jargon for GPU utilization?  It doesn't seem intuitive to me, but I also am also not very familiar with GPUs.... To clarify: Is \"DutyCycle\" something most people will know is the utilization of their GPU?. why remove this error checking?. so the default period is 1 minute, and we are trying to do 3 operations within that time, whereas the old timeout was 2 minutes for each operation.  Seems like a huge change.... I like that it is clearer.  UpdateStats seemed vague, but I couldn't think of anything clearer.  I have no plans to do anything other than trigger an update, mainly for responding when memcg thresholds are crossed.  Do you think it is still worth doing MaxAge if we wont use it in k8s?. done, although I dont think we want a for loop around it.. done. done. Ok, I changed it to MaxAgeMS *int32.  Although I was wondering if just using time.Duration directly would be better?. I believe we still support it...  IIRC from my last conversation with the warsaw folks, we probably wont support this model forever, but I believe we still support it ATM. do you collect container disk stats?  If not, you should just set this to false.  I believe this is meant to indicate whether or not this container has disk stats.. Duration is just a wrapper around int64, so this should be safe for the api\nhttps://golang.org/src/time/time.go?s=21095:21114#L610\n  . done. I can switch to that.. done. yeah, but the double arrow looks really cool.  Done. do we want to defer unlock here?  What if we do not enter into this if statement?. This actually seems concerning, as we only call Register once.  If it times out the first time, then you get no disk stats on DM.. can we move these to the end?  Some callers may wish for partial results, in which case we wouldn't want to skip the assignments below.. Can you change this to ignore the error?  It should never fail to populate status.DriverStatus if you reorder the operations above as I requested.. IIUC, users either get total OR per-cpu?  Why not just add a new metric for total?. Ok, I agree with this.. Why remove this?. Any tips on how to do that? . I would rather avoid adding a flag right now.  I think the retry mechanism is enough to fix the issue, and I don't think there is a meaningful decision for users to make here.. nit: add \"during initialization\" so we know this is delaying startup. Is this different from CPU usage?. do you think container_cpu_schedstat_run_periods_total would be better?. Since CPU and RunTime are the same, we should omit it from the API.. How feasible would it be to maintain a cache of PID->schedstat metrics, update the PIDs that are active, and then sum over all PIDs ever seen?  This way we would get a cumulative metric, and we get the invariant that our reported schedstat metrics never decrease.. The two graphs here are not comparable, but he says here https://github.com/google/cadvisor/pull/1872#issuecomment-361949155 that he figured it out and they are the same.. What in particular don't you like about the code changes?  I guess my main concern is that it is difficult to make sense of a number that decreases when a process dies.  I would rather have a slightly more complex implementation than a metric that isn't useful.  Most monitoring systems can't make sense of a total value that can decrease.. done. so after 5 retries we continue on?  Will we not get any metrics in this case?  Would it be better just to return an error so it doesnt fail silently?. If retry == 0 case is hit, we will return err = nil.  This will probably result in a nil pointer somewhere down the road.  Can we return a new error for this case?. ah, right. this is no longer the case, right?. What is the resolution here @nielsole?  Is this the same as cpu usage, or are they different metrics?. you are welcome to do this in your PR.  IIRC ignoreMetrics was still used by the other handlers, but I may be mistaken.... I would prefer using inotify over polling.. I would prefer just reconstructing machineInfo each time instead of watching for disk mount changes.  We can assume that inHostNamespace doesnt change, so we can avoid repeatedly calling os.Stat(\"/rootfs/proc\").  But this watching for events is more complexity than it is worth.. Try using fsnotify. https://github.com/fsnotify/fsnotify. I looked a little closer and this is a more expensive operation than I thought.  Can we only update the Filesystems field in machineInfo, and keep the rest?. indenting is off everywhere. Yes, the recursive call to watchDirectory will run through that directory's subdirectories and end up adding the container.. can we call this rawPrefixWhitelist?  I am hoping we can move away from dockerOnly, and instead use a whitelist of handlers in the future.. instead of having this only in effect when --docker_only is specified, can we make it always take effect, but have the default be [ \"/\" ], so all raw cgroups are collected?  We can make setting --docker_only set this to [ ] during creation of the raw factory.. I didn't realize this was something we were not already collecting.  Can you add this to ignoremetrics, and ignore it by default?. looks like this is removed from the godeps, but the packages in vendor/ are still there.  I think some of the other packages are in the same boat. do we want to log the error V(4)?  Just in-case we get support questions about unknown OS.. ok, sgtm.  Thanks!. why is this file added?. you dont need the else here.  Just unlock outside the if statement.. nit: We use \"mock\" to refer to a testing struct for which function calls and return values are explicitly specified in the test.  See https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cadvisor/testing/cadvisor_mock.go#L23, as an example.  Most use the github.com/stretchr/testify/mock library.\nFor testing structs like this one, which is a \"fake\" version of the real client, we refer to them as fake.. If the case c.cntr.Status.ExecutorPID <= 0 occurrs, we won't log a useful error message, since err is nil.  Can you have separate if-statements?. We don't ever initialize an fsHandler, can we remove it from the mesosContainerHandler?. Are there golang client libraries that encapsulate this functionality?  Timeouts, backoffs, retries, etc. just so we don't reinvent the wheel unless we have to.. Are these defined in any libraries anywhere?  Are there multiple versions of this API that we need to handle?  This sort of thing seems like it belongs in a client library (as I said above), but those may not exist.... Yeah, just add it later.. not sure.  I tried running it with readOnly: true, and it seemed to work fine. still works. readOnlyRootFilesystem: true works. done. updated. yeah, you should know better than me, but I thought privileged only mattered for making syscalls.  cAdvisor just reads lots of stuff in sys/fs/cgroup. done. done. done. removed command entirely, as those are the default args specified by the Dockerfile for cadvisor. done. done. I was following https://github.com/GoogleCloudPlatform/k8s-stackdriver/blob/master/prometheus-to-sd/kubernetes/prometheus-to-sd-kube-state-metrics.yaml#L30, but maybe they got it wrong. yeah, they got it wrong. Ideally, we want it to track whatever we consider the \"latest version\" version to be, right?  So maybe add it to the \"updating latest\" part of the release?. lets make these label names package constants, and then use them in tests as well.  This includes other label names defined in this file (framework, source, scheduler_sla).. lets add a test case which expects an error here to ensure we handle them correctly. nit: name this TestContainerReference, since that is the function it is testing. nit: comment should include exported function name.. nit: prefer iterating over a list of predefined structs (like the above function does) so we understand what the test-case's inputs are, and so that we can easily add inputs in the future if we change the function.. i'm not sure I understand why we return an error here.  Are we trying to distinguish between when docker is entirely down vs when we miss a container because of a race?  Is there a way to do that without relying on the number of containers?  If we only had 1 container on the node, then we still get 500s when we shouldn't.. Define this error as a new type of error, and then use type assertion here, rather than string parsing.. This isn't limited to docker labels (as it works for other container runtimes that expose labels), lets rename this to store_container_labels or something similar.. Test failed because I forgot to remove the ! here.. Does this recreate the PrometheusCollector each time a request comes in?  Can we avoid this?. do we still need to make this call?. Typo is intentional.  Kustomize is a kubernetes yaml customization tool.. I would prefer a solution that does something like that to the current solution. nit: rather than locking for all but the ListContainers call, can we instead lock around just the critical sections?  I.E. lock and unlock around the := m.containers call above, and then Lock, and defer Unlock before the Determine which were added... section. do we still need to assign cont.handler to a new variable here?. ok. I am not convinced there is a data race in reading the handler attribute of cont.  It is not modified once it is initialized.  The containersLock is meant to prevent concurrent access to the containers map, which the  current design does.  I don't think you need to add a new contHandler variable here.. I think we will need to reassess that when the time comes.  Other functions, including getContainer and getSubcontainers, getAllDockerContainers, etc. return references to containerData structs, and could allow concurrent modification of containerData objects, if used in the way you are suggesting it could be.  We would need much more than a change here to fix it.. This isn't needed.  I just mean a note in the PR description. what changed in this line?. can you add a comment explaining why we use memory?  I'm not actually sure why we use devices, but it would be nice to know why we chose that.. nit: change the comment as well to reference memory. can the error message include that we are listing the directory to get fd count?\nE.g.: error while listing directory %s to measure fd count: %v. nit: can we not rename the container variable?  it makes it harder to see code changes. container_process_total -> container_processes, as it isn't a COUNTER type metric.  See https://prometheus.io/docs/practices/naming/ for naming conventions.. container_fd_total -> container_file_descriptors for the same reasons as above, and use the full name to be more explicit.. can we group this in with the previous NetworkTcpUsageMetrics block?. also group this with the block below. ah, ok.  It is fine then. done. it looks like we could just use the golang implementation directly, since it is public? https://github.com/golang/go/blob/master/src/syscall/dirent.go#L64. I don't think you need to repaste the license into each file.  Having it as the license is good enough. just for my knowledge, why did you need to switch from devices to memory?. can you add a comment explaining why we subtract 1 here?. nit: remove \"inside a container\", as the struct isn't container-specific.. we use uint64 for all metrics. remove \"in a container\". Use the plural Processes instead of singular Process.  Same for the json.. so does this mean one of the pids in our list above isn't actually a pid, but an empty string (or something else)?  Can we just remove this from the list of pids above instead of this?. this looks strange.  I am surprised it passes go lint?  If you put a new line between these, does it go away?. done. done. s/true == isAArch64()/isAArch64(). can we use a full regexp instead of just matching the prefix?  That would be more precise.  E.g. ^node([0-9]+)$. s/true == isAArch64()/isAArch64(). I don't think we need seperate WhiteListedContainerLabels and BaseContainerLabels.  BaseContainerLabels is just the case where WhitelistedContainerLabels is passed an empty list.. See my comment about not separating WhitelistedContainerLabels and BaseContainerLables.  That simplifies this section as well, as you can just do a single assignment here.. I think we want to explicitly say that you need to specify store_container_labels=false to use the whitelistedContainerLables.. whitelisted is one word, not two (even though my browser disagrees).  The flag should be whitelisted_container_labels, and variables should be whitelistedFoo.. can we make this less generic-sounding?  Maybe url_base_prefix?. does this failure always indicate there is no NUMA?  Maybe only return threadId if the error is os.IsNotExist(err).. As in, can we use the regexp to do the matching? I.E. regexp.MatchString. I was suggesting \nif os.IsNotExist(err) {\n    // report threadId if no NUMA\n    return threadId, nil\n} else if err != nil {\n    return 0, err\n}\nI don't have enough context on the change to know if that is actually the right thing to do here, but whatever you think the correct behavior is works for me.. Why did these metrics disappear?. What is this fixing?  It isn't related to diskIO AFAICT. rather than filtering out afterwards, I think it would be cleaner to do this when we first get the list of cgroup subsystems: https://github.com/google/cadvisor/blob/master/container/libcontainer/helpers.go#L37.. we don't currently do this for other ignored metrics.  What is the benefit of removing the blkio cgroup from the list of cgroups?. Now that we are passing in two of these, I would rather just pass ignoreMetrics to the function.. nit: remove extra parenthesis. rHEL looks kinda strange.... i'm not a particular fan of the naming for the docker_only flag, since it is runtime-specific.  Can we call it raw_cgroup_roots or something?  We can make it clear in the comment that setting it causes those cgroups to be collected even when --docker-only is specified since comments don't have backwards compatibility guarantees. that would be fine as well. nit: lets make this a constant.. I am pretty concerned about removing the limit on the number of concurrent calls...  Especially now that the filesystem walk is no longer being run with nice and ionice, this has the potential to starve everything else on the node of cpu or blkio.. With CRI-based runtimes, the disk metrics come from the runtime, rather than cAdvisor, so I don't think (1) is really worth it. . I think we had this function declaration so other binaries can use this function without needing to create a RealFsInfo.  Can you add this back in?. Sorry, you don't need this exact function.  Just GetDirUsage is fine.. maybe store a list or map of labels so we don't re-parse it each time?. can we re-use the whitelisted_container_labels flag?. I'm not a fan of defining a new fsType.  Maybe put this inside of case ZFS.String(), and use goto default to revert to the default behavior when this is the case?. I think this will stay permanently true, and we will skip all subsequent labels. s/duplicate != true/!duplicate. maybe just declare duplicate inside the for l := range rawLabels block?  Then you don't need to reset it to false each loop.. can we end up with fewer labels than values?  Should we move the values = append(values, containerLabels[l]) statement inside here as well?. I would be surprised if prometheus didn't yell at us if we tried to, for example, use a description with 3 labels, but then provide 4 label values when creating the metric.  What you are implicitly doing here is just using the first occurrence of a given sanitized label, and ignoring subsequent ones.. wasn't that exactly what your bug was?. I think just picking the first is a fine behavior.. Ok, I get the difference now.  I think we would still always get more values than labels, since if the label isn't present, we still add it with the value \"\".. I don't think this does anything?. same with container.go. I would rather explicitly plumb this through, and do all of the flag parsing when cAdvisor starts if possible.. Yeah... Prometheus requires that all metric streams in a given scrape have the same set of labels.  So our workaround is just to add empty values for all labels we don't have.. oh, if you can't goto default, then I don't know if that will work...  Let me take another look. ",
    "furlongm": "Running cadvisor on Centos 7 in a docker container, and no other docker containers show up. There are at least 7 others running. I can see that stats for the cadvisor subcontainer though.\nMaybe a red herring, but I noticed Centos has no /var/run/docker directory. Would that make a difference?\n. Docker output on the host machine: http://pastebin.com/raw.php?i=xfABJTfK\nNone of the other containers seem to figure at all.\n. SELinux is disabled for me, so I don't think that is the issue.\n--privilege was not accepted as a command line argument:\nflag provided but not defined: --privilege\nSee 'docker run --help'.\nSee #700 \n. Also getting this issue on Centos 7 (with systemd on the main system, and journalctl available on the main system).\nThe error message is from within the cadvisor docker container and it is the only message returned by docker logs cadvisor.\nShould journalctl be installed in the docker container?\n. I signed it!\n. I signed it!\n. ",
    "xinzhige": "@rjnagal, it does work. Thanks a lot! Should we add some \"Note\" in README.md for Mac OS X users?\n. ",
    "ahjdzx": "@rjnagal  Really? I just want looking for the information about machine network stats. This is my container_hints.json\n{\n  \"name\": \"Container Hints\",\n  \"description\": \"Container hints file\",\n  \"all_hosts\": [\n    {\n      \"network_interface\": {\n        \"veth_child\": \"eth0\",\n        \"veth_host\": \"veth28a464e\"\n      }\n    }\n  ]\n}\nbut i can't found the network infomation from the cadvisor web page. Is that i am config wrong? \n. @rjnagal @vmarmol Thank you very much!\n. ",
    "kelonye": "Thanks .. seems I can't get one per container .. select derivative(cpu_cumulative_usage) from stats where container_name = '/docker/project_monitoring_1'. My use case is, I need metrics to generate such a graph:\n\n. https://github.com/tutumcloud/container-metrics/issues/1#issuecomment-68619739\n. ",
    "richp10": "Worked perfectly, thanks..\nR\n\nRichard Phillips\nOn 4 January 2015 at 02:47, Rohit Jnagal notifications@github.com wrote:\n\n@richp10 https://github.com/richp10 Can you please try\njnagal/cadvisor:canary image to verify the patch? Thanks.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/414#issuecomment-68618576.\n. \n",
    "sathieu": "Note that the memory controller is enabled by default in Debian 9 (stretch):\n```\nlinux (4.7.2-1) unstable; urgency=medium\n  [...]\n  * cgroups: Enable memory controller by default\n  [...]\n-- Ben Hutchings ben@decadent.org.uk  Sun, 28 Aug 2016 15:56:10 +0100\n```. ",
    "arkadijs": "Great! My Google Individual CLA is already signed, just checked - its still there.\n. Strictly speaking you're right - usage goes under /container but /machine describes provisioned capacity. I wanted to display basic machine info without going to two places.\n. I understand.\n/containers also provide capacity.\nThank you for looking into this.\n. Sorry for that, fixed.\nI can force-push as a single commit if necessary.\n. Also added device-mapper to regex. Works for BTRFS volumes on LVM as they appears as /dev/dm-* in /proc/mounts even if mounted as /dev/vg/lv.\nNot a complete solution because ext4 on LVM appears as intended, ie. /dev/vg/lv and thus does not match /proc/diskstats. No idea why such difference.\n. I checked it on Docker host running with devicemapper storage driver. It doesn't help to collect the stats, but not making this worse either:\n{\n  \"device\": \"/dev/mapper/docker-202:1-922914-e6128e016bdac12e73c36d70659e4984159878c3c33e1e3b962891185aaeb525\",\n  \"capacity\": 10434662400,\n  \"usage\": 651108352,\n  \"reads_completed\": 0,\n  \"reads_merged\": 0,\n  \"sectors_read\": 0,\n  \"read_time\": 0,\n  \"writes_completed\": 0,\n  \"writes_merged\": 0,\n  \"sectors_written\": 0,\n  \"write_time\": 0,\n  \"io_in_progress\": 0,\n  \"io_time\": 0,\n  \"weighted_io_time\": 0\n},\n. ",
    "jzy1688": "@rjnagal Hello. Could you tell me how could I enable load_stats? I always get \"has_load =flase\"\n. Thanks\n. Thanks\n. ",
    "krallin": "Hey there,\n(let me know if I should be opening a new issue).\nJust got this error running cAdvisor 0.22.2. The trace is as follows:\n```\npanic: runtime error: slice bounds out of range\ngoroutine 125 [running]:\nbufio.(Reader).Read(0xc8201d0e40, 0xc820adc2d0, 0x10, 0x10, 0xa4a2a0, 0x0, 0x0)\n        /usr/local/go/src/bufio/bufio.go:214 +0x3c8\nio.ReadAtLeast(0x7f12a57c6100, 0xc8201d0e40, 0xc820adc2d0, 0x10, 0x10, 0x10, 0x0, 0x0, 0x0)\n        /usr/local/go/src/io/io.go:298 +0xe6\nio.ReadFull(0x7f12a57c6100, 0xc8201d0e40, 0xc820adc2d0, 0x10, 0x10, 0x10, 0x0, 0x0)\n        /usr/local/go/src/io/io.go:316 +0x62\nencoding/binary.Read(0x7f12a57c6100, 0xc8201d0e40, 0x7f12a57c18e8, 0x14272d8, 0xa21220, 0xc820f77bc0, 0x0, 0x0)\n        /usr/local/go/src/encoding/binary/binary.go:216 +0x1336\ngithub.com/google/cadvisor/utils/cpuload/netlink.(Connection).ReadMessage(0xc8204141e0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)\n        /home/build/gopath/src/github.com/google/cadvisor/utils/cpuload/netlink/conn.go:88 +0x146\ngithub.com/google/cadvisor/utils/cpuload/netlink.getLoadStats(0xc820390018, 0x39, 0xc8204141e0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)\n        /home/build/gopath/src/github.com/google/cadvisor/utils/cpuload/netlink/netlink.go:231 +0x1b6\ngithub.com/google/cadvisor/utils/cpuload/netlink.(NetlinkReader).GetCpuLoad(0xc820402e70, 0xc820544550, 0x48, 0xc820399740, 0x5a, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\n        /home/build/gopath/src/github.com/google/cadvisor/utils/cpuload/netlink/reader.go:73 +0x33f\ngithub.com/google/cadvisor/manager.(containerData).updateStats(0xc820674960, 0x0, 0x0)\n        /home/build/gopath/src/github.com/google/cadvisor/manager/container.go:508 +0x330\ngithub.com/google/cadvisor/manager.(containerData).housekeepingTick(0xc820674960)\n        /home/build/gopath/src/github.com/google/cadvisor/manager/container.go:447 +0x2b\ngithub.com/google/cadvisor/manager.(containerData).housekeeping(0xc820674960)\n        /home/build/gopath/src/github.com/google/cadvisor/manager/container.go:398 +0x29f\ncreated by github.com/google/cadvisor/manager.(*containerData).Start\n        /home/build/gopath/src/github.com/google/cadvisor/manager/container.go:94 +0x41\n```\nIs there anything I can do to help troubleshoot this?\nCheers,\n. @dashpole yep. CLA should be sorted out now!\n. That makes sense, of course. \nI was actually going to do some performance testing for ourselves, so I'll see how far I can get, and report here the results I have to share (and whether I need help collecting them). \n. @vishh:\n\n\nThis ensures each goroutine is given its own Netlink connection, and presumably avoids having a message destined for one goroutine read by another goroutine.\n\nCan you explain this statement a bit more? How is central cpu load collection an issue?\n\nOf course. \nCurrently, it seems that multiple goroutines are accessing a single CpuLoadReader at the same time. In turn, they are accessing the same underlying netlink connection. I don't see any locking code in netlink code, so unless I'm mistaken (which is entirely possible), it looks the following race-condition scenario is possible:\n- Goroutine A writes a load query message to the Netlink socket(at conn.WriteMessage in getLoadStats), then control switches to Goroutine B.\n- Goroutine B writes a load query message to the Netlink socket (same place), then control switches to Goroutine A.\n- Goroutine A reads a Netlink header from the Netlink socket (at binary.Read in ReadMessage), then control switches to Goroutine B\n- Goroutine B reads what it thinks is a Netlink header from the Netlink socket (same place), but it's actually the body of the message Goroutine A started reading.\nNow, to be clear:\n- I haven't had been able to able (yet, hopefully) to confirm that I'm indeed hitting this race condition in my testing (although it could definitely explain the issue I ran into in https://github.com/google/cadvisor/issues/1182). \n- It's entirely possible that the Go scheduler would not actually perform the scheduling scenario I described, but without any locking in, it might be risky to rely on that.\nHope this clarifies, but I'm of course happy to expand in more detail (I'm UTC+1 though, so probably not today), and I plan to do more testing around this.\nThanks!\n. Hi @vishh, @timstclair,\nI started running some performance tests as requested by Time. Here's the script I used for testing (I'm running cAdvisor outside of a container, but using cgroups for memory and CPU accounting).\nThis basically starts cAdvisor, runs a bunch (400) of containers, then waits for 10 minutes and logs the memory and CPU usage into a CSV file every second. I ran it against current master and my version.\n```\n!/bin/bash\nset -o errexit nounset\nCG_ROOT=\"/sys/fs/cgroup\"\nOUT_FILE=\"perf.out\"\nTEST_CG_NAME=\"test-cadvisor\"\nCPU_USAGE_FILE=\"${CG_ROOT}/cpuacct/${TEST_CG_NAME}/cpuacct.usage\"\nMEM_USAGE_FILE=\"${CG_ROOT}/memory/${TEST_CG_NAME}/memory.usage_in_bytes\"\nif [[ \"$(docker ps -q)\" != \"\" ]]; then\n  echo \"Some Docker containers are already running - aborting.\"\n  exit 1\nfi\nfor cgroup in cpuacct memory; do\n  tasks_file=\"${CG_ROOT}/${cgroup}/${TEST_CG_NAME}/tasks\"\n  if [[ ! -f \"$tasks_file\" ]]; then\n    echo \"Cgroup does not exist: ${cgroup}:${TEST_CG_NAME} - aborting.\"\n    exit 1\n  fi\n  if [[ \"$(< \"$tasks_file\")\" != \"\" ]]; then\n    echo \"Some tasks found in ${cgroup}:${TEST_CG_NAME} - aborting.\"\n    exit 1\n  fi\ndone\nPull image beforehand\necho \"Pulling test image\"\ndocker pull alpine\nReset CPU counters\necho \"Resetting CPU counters\"\necho 0 > \"$CPU_USAGE_FILE\"\nif [[ \"$(< \"$CPU_USAGE_FILE\")\" != \"0\" ]]; then\n  echo \"Failed to clear CPU counters-  aborting.\"\n  exit 1\nfi\necho \"Clearing Memory counters\"\necho 0 > \"${CG_ROOT}/memory/${TEST_CG_NAME}/memory.force_empty\"\nif [[ \"$(< \"$MEM_USAGE_FILE\")\" != \"0\" ]]; then\n  echo \"Failed to clear memory counters - aborting.\"\n  exit 1\nfi\necho \"Starting test\"\nCADVISOR_VERSION=\"$(cadvisor --version)\"\nsudo cgexec -g \"cpuacct,memory:${TEST_CG_NAME}\" \"$(which cadvisor)\" --enable_load_reader --docker_only --logtostderr &\nCADVISOR_PID=\"$?\"  # Technially sudo's, but that's fine\n(\n  # Start some containers that will start and die throughout the test\n  for i in $(seq 0 199); do\n    docker run -d alpine sleep \"$((200 - i))\"\n  done\n# Now, start some more containers that'll persist for a while longer\n  for i in $(seq 0 199); do\n    docker run -d alpine sleep 200\n  done\necho \"Done dispatching containers\"\n) &\necho \"Time,Version,Memory in bytes,CPU in ns\" > \"$OUT_FILE\"\nfor _ in $(seq 0 599); do\n  echo \"\\\"$(date -Ins -u)\\\",\\\"$CADVISOR_VERSION\\\",$(< \"$MEM_USAGE_FILE\"),$(< \"$CPU_USAGE_FILE\")\" >> \"$OUT_FILE\"\n  sleep 1\ndone\nsudo kill -TERM \"$CADVISOR_PID\"\necho \"Done running test!\"\n```\nThe bottom line is that the current master didn't even finish the test. \nAbout 4 minutes into the tests, it was terminated by the Kernel OOM killer after having exhausted the system memory and swap (it also caused kswapd to start consuming 100% of CPU pretty early into the test, which I think might be reflected in the results considering the amount of CPU the old version ended up using). So, as it turns out, the above script is a repro for https://github.com/google/cadvisor/issues/1182\nNote that cAdvisor started malfunctioning much earlier since the system was running out of memory. Here a few lines from the output:\nE0405 11:59:42.832035    7225 fsHandler.go:106] failed to collect filesystem stats - failed to exec du - fork/exec /usr/bin/nice: cannot allocate memory\nE0405 11:59:42.832730    7225 fsHandler.go:106] failed to collect filesystem stats - failed to exec du - fork/exec /usr/bin/nice: cannot allocate memory\nE0405 11:59:42.833828    7225 fsHandler.go:106] failed to collect filesystem stats - failed to exec du - fork/exec /usr/bin/nice: cannot allocate memory\nE0405 11:59:42.834633    7225 fsHandler.go:106] failed to collect filesystem stats - failed to exec du - fork/exec /usr/bin/nice: cannot allocate memory\nE0405 11:59:42.836327    7225 fsHandler.go:106] failed to collect filesystem stats - failed to exec du - fork/exec /usr/bin/nice: cannot allocate memory\nI planned on conducting multiple runs to have a better idea of performance, but considering current master doesn't even finish the tests, I did not do so (I'm happy to do it if you feel that's necessary though).\nHowever, I do understand the patch I'm proposing may have an impact on performance, so I will try to come up with a test where the current master doesn't crash so we can compare them (I suspect launching containers slower might reduce the probability of hitting the race condition this PR seeks to solve).  \n\nIn the meantime, here are a few graphs:\nThis PR:\n\n\nCurrent master:\n\n\n\nHere are the raw results:\nThis PR:\nTime,Version,Memory in bytes,CPU in ns\n\"2016-04-05T09:09:47,560740501+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",0,0\n\"2016-04-05T09:09:48,563987993+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",4915200,41156413\n\"2016-04-05T09:09:49,567173639+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",4915200,42004294\n\"2016-04-05T09:09:50,570529745+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",5361664,46060766\n\"2016-04-05T09:09:51,574346115+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",5361664,46060766\n\"2016-04-05T09:09:52,581116436+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",10874880,213855720\n\"2016-04-05T09:09:53,588637793+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",8404992,570164547\n\"2016-04-05T09:09:54,596453603+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",8421376,657604521\n\"2016-04-05T09:09:55,601382966+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",8425472,758402842\n\"2016-04-05T09:09:56,605046761+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",8466432,867407586\n\"2016-04-05T09:09:57,608735688+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",8626176,968231725\n\"2016-04-05T09:09:58,611502156+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",8962048,1080240876\n\"2016-04-05T09:09:59,617042511+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",9375744,1176115118\n\"2016-04-05T09:10:00,620633281+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",9482240,1291384731\n\"2016-04-05T09:10:01,624201587+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",9793536,1423140809\n\"2016-04-05T09:10:02,627071957+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",10096640,1541068694\n\"2016-04-05T09:10:03,630031220+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",10559488,1661016464\n\"2016-04-05T09:10:04,637717703+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",10752000,1783186285\n\"2016-04-05T09:10:05,642700777+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",11141120,1926039948\n\"2016-04-05T09:10:06,647387968+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",11235328,2060268038\n\"2016-04-05T09:10:07,656023352+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",11616256,2208681005\n\"2016-04-05T09:10:08,659558138+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",11886592,2333831578\n\"2016-04-05T09:10:09,663790760+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",12161024,2476763403\n\"2016-04-05T09:10:10,670970447+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",12726272,2633001433\n\"2016-04-05T09:10:11,674843928+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",15667200,2772326262\n\"2016-04-05T09:10:12,679893017+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",13336576,2904914765\n\"2016-04-05T09:10:13,683011660+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",13737984,3039258579\n\"2016-04-05T09:10:14,690196217+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",13869056,3183340044\n\"2016-04-05T09:10:15,696032510+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",14614528,3335911612\n\"2016-04-05T09:10:16,699701676+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",14753792,3460528122\n\"2016-04-05T09:10:17,705708560+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",15228928,3606608308\n\"2016-04-05T09:10:18,709969905+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",15405056,3755659020\n\"2016-04-05T09:10:19,714295769+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",15859712,3907072993\n\"2016-04-05T09:10:20,724344008+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",16314368,4056439136\n\"2016-04-05T09:10:21,728674115+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",16900096,4224704143\n\"2016-04-05T09:10:22,731814042+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",17158144,4380015407\n\"2016-04-05T09:10:23,734859996+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",17518592,4524710494\n\"2016-04-05T09:10:24,739195092+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",18108416,4692530760\n\"2016-04-05T09:10:25,743146190+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",18550784,4866223464\n\"2016-04-05T09:10:26,746582385+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",19116032,5051109164\n\"2016-04-05T09:10:27,750734245+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",19509248,5224469555\n\"2016-04-05T09:10:28,756442225+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",19767296,5413054269\n\"2016-04-05T09:10:29,769283527+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",20471808,5623928617\n\"2016-04-05T09:10:30,772568551+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",20873216,5782256735\n\"2016-04-05T09:10:31,776629164+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",21041152,5980577176\n\"2016-04-05T09:10:32,779846856+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",21917696,6158110624\n\"2016-04-05T09:10:33,785856742+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",22269952,6315026876\n\"2016-04-05T09:10:34,789262318+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",22482944,6477721921\n\"2016-04-05T09:10:35,795078452+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",22974464,6682165693\n\"2016-04-05T09:10:36,798967591+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",23777280,6867524897\n\"2016-04-05T09:10:37,802097169+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",23900160,7082797041\n\"2016-04-05T09:10:38,807245052+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",24367104,7240363303\n\"2016-04-05T09:10:39,810228940+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",25202688,7422320383\n\"2016-04-05T09:10:40,813275491+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",25522176,7583746986\n\"2016-04-05T09:10:41,817370598+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",26107904,7760158871\n\"2016-04-05T09:10:42,821931539+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",26374144,7956889764\n\"2016-04-05T09:10:43,824818627+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",26955776,8123517258\n\"2016-04-05T09:10:44,827785362+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",27250688,8304688518\n\"2016-04-05T09:10:45,831127305+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",27815936,8484210791\n\"2016-04-05T09:10:46,838300639+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",28114944,8647501085\n\"2016-04-05T09:10:47,841790592+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",28364800,8866299197\n\"2016-04-05T09:10:48,844333064+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",28872704,9044551855\n\"2016-04-05T09:10:49,848043133+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",29126656,9271642690\n\"2016-04-05T09:10:50,852083480+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",29609984,9436277309\n\"2016-04-05T09:10:51,855288493+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",30130176,9620100880\n\"2016-04-05T09:10:52,858664332+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",30445568,10145928039\n\"2016-04-05T09:10:53,862277735+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",31137792,10500213868\n\"2016-04-05T09:10:54,865581272+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",31330304,10728624199\n\"2016-04-05T09:10:55,868912212+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",31494144,11005353591\n\"2016-04-05T09:10:56,873651348+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",31989760,11255978640\n\"2016-04-05T09:10:57,877140348+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",32563200,11519436516\n\"2016-04-05T09:10:58,880162912+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",32710656,11814609172\n\"2016-04-05T09:10:59,884202053+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",33009664,12049227914\n\"2016-04-05T09:11:00,887513291+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",33464320,12282011363\n\"2016-04-05T09:11:01,890860401+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",34050048,12565445217\n\"2016-04-05T09:11:02,895169506+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",34607104,12809381349\n\"2016-04-05T09:11:03,904942684+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",34598912,13071959408\n\"2016-04-05T09:11:04,908058062+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",37031936,13311837345\n\"2016-04-05T09:11:05,913651527+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",35184640,13604901803\n\"2016-04-05T09:11:06,916984398+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",35872768,13930017857\n\"2016-04-05T09:11:07,926805281+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",36306944,14237637668\n\"2016-04-05T09:11:08,930019676+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",36777984,14473805999\n\"2016-04-05T09:11:09,935886853+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",36831232,14723589309\n\"2016-04-05T09:11:10,939324323+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",37015552,14965255829\n\"2016-04-05T09:11:11,942401062+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",37548032,15224302945\n\"2016-04-05T09:11:12,946142390+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",37863424,15494365394\n\"2016-04-05T09:11:13,949256271+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",38260736,15752135379\n\"2016-04-05T09:11:14,957723389+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",38166528,15968960933\n\"2016-04-05T09:11:15,962238474+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",38596608,16260632371\n\"2016-04-05T09:11:16,965630411+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",38912000,16521686468\n\"2016-04-05T09:11:17,968955234+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",39526400,16799393468\n\"2016-04-05T09:11:18,971832455+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",39526400,17019334057\n\"2016-04-05T09:11:19,975113800+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",39833600,17337255352\n\"2016-04-05T09:11:20,980081460+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",40132608,17629526746\n\"2016-04-05T09:11:21,983172756+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",40361984,17836715639\n\"2016-04-05T09:11:22,985752440+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",40689664,18005865375\n\"2016-04-05T09:11:23,989142371+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",40923136,18197496000\n\"2016-04-05T09:11:24,993139589+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",41775104,18398226498\n\"2016-04-05T09:11:25,996884672+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",42192896,18598476278\n\"2016-04-05T09:11:27,001466026+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",42115072,18784323841\n\"2016-04-05T09:11:28,004849692+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",42696704,18978103460\n\"2016-04-05T09:11:29,008710011+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",43192320,19184218452\n\"2016-04-05T09:11:30,013415896+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",43200512,19355027037\n\"2016-04-05T09:11:31,017403938+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",43610112,19560365760\n\"2016-04-05T09:11:32,023536760+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",43610112,19764344387\n\"2016-04-05T09:11:33,027503735+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",44261376,19958306039\n\"2016-04-05T09:11:34,033256159+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",44220416,20150269832\n\"2016-04-05T09:11:35,037440865+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",44429312,20351706823\n\"2016-04-05T09:11:36,041085301+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",45101056,20560510971\n\"2016-04-05T09:11:37,048296936+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",45092864,20747353300\n\"2016-04-05T09:11:38,051086744+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",51302400,20995872702\n\"2016-04-05T09:11:39,056908024+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",46526464,21234070526\n\"2016-04-05T09:11:40,060315262+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",46534656,21440972574\n\"2016-04-05T09:11:41,063507821+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",46780416,21683582640\n\"2016-04-05T09:11:42,074067982+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",47603712,21938825772\n\"2016-04-05T09:11:43,078378807+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",47546368,22129775018\n\"2016-04-05T09:11:44,082435346+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",47771648,22363463975\n\"2016-04-05T09:11:45,086729904+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",48001024,22603738004\n\"2016-04-05T09:11:46,090805973+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",48021504,22819005152\n\"2016-04-05T09:11:47,093602235+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",48644096,23068835966\n\"2016-04-05T09:11:48,097726150+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",49049600,23334846656\n\"2016-04-05T09:11:49,102102580+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",49012736,23565214476\n\"2016-04-05T09:11:50,107296893+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",49672192,23877567357\n\"2016-04-05T09:11:51,110844052+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",50081792,24135408319\n\"2016-04-05T09:11:52,116117646+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",50098176,24388193839\n\"2016-04-05T09:11:53,119322306+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",50425856,25152461941\n\"2016-04-05T09:11:54,122248882+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",50569216,25516510358\n\"2016-04-05T09:11:55,125615712+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56356864,25868815347\n\"2016-04-05T09:11:56,128616697+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",51965952,26187834697\n\"2016-04-05T09:11:57,131679107+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",52420608,26534378089\n\"2016-04-05T09:11:58,135855803+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",52445184,26836305621\n\"2016-04-05T09:11:59,138908252+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",53071872,27177620226\n\"2016-04-05T09:12:00,141826343+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",53227520,27506183869\n\"2016-04-05T09:12:01,145274136+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",53080064,27854252750\n\"2016-04-05T09:12:02,148687875+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",53227520,28222996330\n\"2016-04-05T09:12:03,152996998+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",53092352,28498172808\n\"2016-04-05T09:12:04,156992021+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",53444608,28825601972\n\"2016-04-05T09:12:05,160821990+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",53420032,29164404330\n\"2016-04-05T09:12:06,164270142+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",54042624,29453758863\n\"2016-04-05T09:12:07,168431628+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",54329344,29746200286\n\"2016-04-05T09:12:08,173634977+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",54337536,29952878895\n\"2016-04-05T09:12:09,177144312+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",54444032,30224249253\n\"2016-04-05T09:12:10,180266246+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",54382592,30452412546\n\"2016-04-05T09:12:11,183847353+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",54714368,30728379683\n\"2016-04-05T09:12:12,188137660+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",55238656,30993028285\n\"2016-04-05T09:12:13,191403697+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",55263232,31235406433\n\"2016-04-05T09:12:14,198423531+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",55758848,31505419036\n\"2016-04-05T09:12:15,201773609+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",60215296,31705245412\n\"2016-04-05T09:12:16,205340184+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",55988224,31949579503\n\"2016-04-05T09:12:17,209186889+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",55877632,32190122500\n\"2016-04-05T09:12:18,213267872+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",55881728,32443503509\n\"2016-04-05T09:12:19,217414267+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56098816,32694766081\n\"2016-04-05T09:12:20,221060153+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56098816,32871287009\n\"2016-04-05T09:12:21,230342412+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56176640,33139465055\n\"2016-04-05T09:12:22,234622868+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56184832,33342901213\n\"2016-04-05T09:12:23,238354075+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56954880,33618943755\n\"2016-04-05T09:12:24,242136933+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56958976,33826480124\n\"2016-04-05T09:12:25,245653714+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56958976,34129106778\n\"2016-04-05T09:12:26,253330630+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56963072,34299340298\n\"2016-04-05T09:12:27,261179400+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56963072,34395528087\n\"2016-04-05T09:12:28,265611930+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56967168,34574504843\n\"2016-04-05T09:12:29,272504120+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56967168,34726188478\n\"2016-04-05T09:12:30,277115791+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56971264,34827116334\n\"2016-04-05T09:12:31,280455094+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56975360,34878925782\n\"2016-04-05T09:12:32,285752347+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56975360,34915855864\n\"2016-04-05T09:12:33,293313800+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56975360,34996833820\n\"2016-04-05T09:12:34,299585436+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56975360,35148567697\n\"2016-04-05T09:12:35,304049358+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56975360,35236375992\n\"2016-04-05T09:12:36,313131814+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56975360,35320365632\n\"2016-04-05T09:12:37,317420353+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56979456,35402486411\n\"2016-04-05T09:12:38,320873969+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56979456,35461739975\n\"2016-04-05T09:12:39,327169808+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56979456,35567869676\n\"2016-04-05T09:12:40,333448280+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56995840,35658598439\n\"2016-04-05T09:12:41,338299798+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56979456,35760105339\n\"2016-04-05T09:12:42,342255273+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56979456,35854316339\n\"2016-04-05T09:12:43,346628914+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,35929997273\n\"2016-04-05T09:12:44,350585571+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56983552,36023099332\n\"2016-04-05T09:12:45,353942612+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",62226432,36122147176\n\"2016-04-05T09:12:46,361182485+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57004032,36268329884\n\"2016-04-05T09:12:47,368869696+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57077760,36428298823\n\"2016-04-05T09:12:48,374335175+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,36541941517\n\"2016-04-05T09:12:49,384759783+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,36647756164\n\"2016-04-05T09:12:50,388480220+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,36762768426\n\"2016-04-05T09:12:51,393296820+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,36830886918\n\"2016-04-05T09:12:52,397768035+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,36908851783\n\"2016-04-05T09:12:53,401972821+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,37498251157\n\"2016-04-05T09:12:54,406700659+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,37585739907\n\"2016-04-05T09:12:55,410623440+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,37648916247\n\"2016-04-05T09:12:56,415422382+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,37747676471\n\"2016-04-05T09:12:57,424241004+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,37811879180\n\"2016-04-05T09:12:58,428799548+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,37893240457\n\"2016-04-05T09:12:59,433703736+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,37994821385\n\"2016-04-05T09:13:00,437092168+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,38076634536\n\"2016-04-05T09:13:01,442970969+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,38151972755\n\"2016-04-05T09:13:02,451826166+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,38239747530\n\"2016-04-05T09:13:03,456048473+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,38319506362\n\"2016-04-05T09:13:04,460734343+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,38390444847\n\"2016-04-05T09:13:05,465302668+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,38485193904\n\"2016-04-05T09:13:06,469110780+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,38583921808\n\"2016-04-05T09:13:07,473834331+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,38687402365\n\"2016-04-05T09:13:08,477609415+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,38766357111\n\"2016-04-05T09:13:09,481081319+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57085952,38840266974\n\"2016-04-05T09:13:10,484873202+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57110528,38957352961\n\"2016-04-05T09:13:11,490491862+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,39028622003\n\"2016-04-05T09:13:12,496573370+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,39096776241\n\"2016-04-05T09:13:13,503587628+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,39217138507\n\"2016-04-05T09:13:14,507893768+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,39267781516\n\"2016-04-05T09:13:15,512115416+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,39316814589\n\"2016-04-05T09:13:16,515995310+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,39383908986\n\"2016-04-05T09:13:17,519707186+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,39513694327\n\"2016-04-05T09:13:18,523255184+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,39554581099\n\"2016-04-05T09:13:19,528073275+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,39599015353\n\"2016-04-05T09:13:20,533829365+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,39650566825\n\"2016-04-05T09:13:21,538328784+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,39684956460\n\"2016-04-05T09:13:22,547015752+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,39737181402\n\"2016-04-05T09:13:23,550995553+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,39765791915\n\"2016-04-05T09:13:24,556215557+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,39832850377\n\"2016-04-05T09:13:25,560911300+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,39865101586\n\"2016-04-05T09:13:26,565360055+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,39922432889\n\"2016-04-05T09:13:27,571580615+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,39964686556\n\"2016-04-05T09:13:28,576298840+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,40009213330\n\"2016-04-05T09:13:29,581135876+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,40042216671\n\"2016-04-05T09:13:30,585701240+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57262080,40088432001\n\"2016-04-05T09:13:31,591885036+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,40124386083\n\"2016-04-05T09:13:32,595914323+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,40159494479\n\"2016-04-05T09:13:33,604755168+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,40196627095\n\"2016-04-05T09:13:34,611835384+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,40218549793\n\"2016-04-05T09:13:35,616686755+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,40265406148\n\"2016-04-05T09:13:36,621353166+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,40290228863\n\"2016-04-05T09:13:37,625312222+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,40336350600\n\"2016-04-05T09:13:38,635389724+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,40420985354\n\"2016-04-05T09:13:39,640669138+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,40491234630\n\"2016-04-05T09:13:40,645624250+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,40563937892\n\"2016-04-05T09:13:41,651826564+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,40648071622\n\"2016-04-05T09:13:42,656081609+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,40780625395\n\"2016-04-05T09:13:43,660734897+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,40845914544\n\"2016-04-05T09:13:44,665864030+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,40884763964\n\"2016-04-05T09:13:45,669231008+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,40974674232\n\"2016-04-05T09:13:46,676892134+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,41032875404\n\"2016-04-05T09:13:47,684624408+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,41134200268\n\"2016-04-05T09:13:48,688755148+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,41203452152\n\"2016-04-05T09:13:49,692986510+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57012224,41267762323\n\"2016-04-05T09:13:50,697613421+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57040896,41331397433\n\"2016-04-05T09:13:51,705194798+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,41423162554\n\"2016-04-05T09:13:52,709044347+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,41544266067\n\"2016-04-05T09:13:53,715418375+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,41718537981\n\"2016-04-05T09:13:54,719421036+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,41802827493\n\"2016-04-05T09:13:55,723425242+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56987648,41865409928\n\"2016-04-05T09:13:56,728408397+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57180160,41946486749\n\"2016-04-05T09:13:57,735238222+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56991744,42027740004\n\"2016-04-05T09:13:58,739499636+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56991744,42103741170\n\"2016-04-05T09:13:59,744150075+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56995840,42192273821\n\"2016-04-05T09:14:00,748396755+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57106432,42278559910\n\"2016-04-05T09:14:01,752912276+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57012224,42347676583\n\"2016-04-05T09:14:02,758144811+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56995840,42429979730\n\"2016-04-05T09:14:03,762075419+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56995840,42517355486\n\"2016-04-05T09:14:04,766757774+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56995840,42580481516\n\"2016-04-05T09:14:05,770778159+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56995840,42654508562\n\"2016-04-05T09:14:06,774944418+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56995840,42754851837\n\"2016-04-05T09:14:07,778774387+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56995840,42828662875\n\"2016-04-05T09:14:08,783961731+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56995840,42941178194\n\"2016-04-05T09:14:09,788091418+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56995840,43014577603\n\"2016-04-05T09:14:10,793186512+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56995840,43079929966\n\"2016-04-05T09:14:11,800057396+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56995840,43136390205\n\"2016-04-05T09:14:12,804664756+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56995840,43225051734\n\"2016-04-05T09:14:13,808469558+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56995840,43304609571\n\"2016-04-05T09:14:14,812134391+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56995840,43328185903\n\"2016-04-05T09:14:15,816008847+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56995840,43368780210\n\"2016-04-05T09:14:16,820385041+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,43409891812\n\"2016-04-05T09:14:17,826013571+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,43450468168\n\"2016-04-05T09:14:18,831668083+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,43496921386\n\"2016-04-05T09:14:19,835032125+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,43527013511\n\"2016-04-05T09:14:20,838559840+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,43569815854\n\"2016-04-05T09:14:21,843360177+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,43611463115\n\"2016-04-05T09:14:22,847612630+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,43646420105\n\"2016-04-05T09:14:23,854910237+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,43688970861\n\"2016-04-05T09:14:24,858687871+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,43738870752\n\"2016-04-05T09:14:25,863755108+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,43771990387\n\"2016-04-05T09:14:26,867771890+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57081856,43821939191\n\"2016-04-05T09:14:27,871954191+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57008128,43846191432\n\"2016-04-05T09:14:28,877111700+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,43894255756\n\"2016-04-05T09:14:29,881816531+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,43931584191\n\"2016-04-05T09:14:30,886214358+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,43966096993\n\"2016-04-05T09:14:31,889877207+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,44001898314\n\"2016-04-05T09:14:32,895094236+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,44024217815\n\"2016-04-05T09:14:33,898869523+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,44060488063\n\"2016-04-05T09:14:34,904253405+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,44090995821\n\"2016-04-05T09:14:35,908025392+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,44109845154\n\"2016-04-05T09:14:36,912223415+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,44144494986\n\"2016-04-05T09:14:37,917020050+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,44208089710\n\"2016-04-05T09:14:38,923517578+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57008128,44227441737\n\"2016-04-05T09:14:39,927854483+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,44316937352\n\"2016-04-05T09:14:40,934682197+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,44347276706\n\"2016-04-05T09:14:41,938880951+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,44396935806\n\"2016-04-05T09:14:42,942450018+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,44441724779\n\"2016-04-05T09:14:43,946251761+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,44477931685\n\"2016-04-05T09:14:44,949647114+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,44510122430\n\"2016-04-05T09:14:45,955966714+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,44572015102\n\"2016-04-05T09:14:46,959977135+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,44605939429\n\"2016-04-05T09:14:47,964720119+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,44644918698\n\"2016-04-05T09:14:48,971990952+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,44716177958\n\"2016-04-05T09:14:49,976230957+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,44750245115\n\"2016-04-05T09:14:50,980026492+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57053184,44801366017\n\"2016-04-05T09:14:51,984086787+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56999936,44893327046\n\"2016-04-05T09:14:52,988624552+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57004032,44979209806\n\"2016-04-05T09:14:53,991939181+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57126912,45057000283\n\"2016-04-05T09:14:55,000196710+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57004032,45126014155\n\"2016-04-05T09:14:56,004734928+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,45190089926\n\"2016-04-05T09:14:57,009274076+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57004032,45243299409\n\"2016-04-05T09:14:58,013977572+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57081856,45304460355\n\"2016-04-05T09:14:59,019023328+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57004032,45361376336\n\"2016-04-05T09:15:00,023814373+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,45420133210\n\"2016-04-05T09:15:01,029604217+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57081856,45460556069\n\"2016-04-05T09:15:02,033985903+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57040896,45515369539\n\"2016-04-05T09:15:03,041256155+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57004032,45584273149\n\"2016-04-05T09:15:04,044977394+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57004032,45638896678\n\"2016-04-05T09:15:05,048884165+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57004032,45672592684\n\"2016-04-05T09:15:06,052331008+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,45731558690\n\"2016-04-05T09:15:07,056670696+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,45838536470\n\"2016-04-05T09:15:08,060341344+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57049088,45872565940\n\"2016-04-05T09:15:09,064529685+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57008128,45923110526\n\"2016-04-05T09:15:10,068695497+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57008128,45969363328\n\"2016-04-05T09:15:11,075118816+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,46014030555\n\"2016-04-05T09:15:12,080667665+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57008128,46067564266\n\"2016-04-05T09:15:13,084651719+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57008128,46121323095\n\"2016-04-05T09:15:14,089933333+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57012224,46163464036\n\"2016-04-05T09:15:15,094656007+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57065472,46195302296\n\"2016-04-05T09:15:16,098549669+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57012224,46221736144\n\"2016-04-05T09:15:17,103421255+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57012224,46243639899\n\"2016-04-05T09:15:18,108393272+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57012224,46282076633\n\"2016-04-05T09:15:19,112249311+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57012224,46305913093\n\"2016-04-05T09:15:20,118994266+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57012224,46339721394\n\"2016-04-05T09:15:21,125324148+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57012224,46351136343\n\"2016-04-05T09:15:22,131087103+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57012224,46363007625\n\"2016-04-05T09:15:23,136977043+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57012224,46400490658\n\"2016-04-05T09:15:24,141885513+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,46438787975\n\"2016-04-05T09:15:25,146560178+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57012224,46474414434\n\"2016-04-05T09:15:26,151224490+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57012224,46488154258\n\"2016-04-05T09:15:27,155900121+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57081856,46527095910\n\"2016-04-05T09:15:28,162427148+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57012224,46549273641\n\"2016-04-05T09:15:29,166137442+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57012224,46553935874\n\"2016-04-05T09:15:30,174721051+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57049088,46572055896\n\"2016-04-05T09:15:31,178391130+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57012224,46598387823\n\"2016-04-05T09:15:32,183259770+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57012224,46620922222\n\"2016-04-05T09:15:33,189292085+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57012224,46643185573\n\"2016-04-05T09:15:34,196501260+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57094144,46665565732\n\"2016-04-05T09:15:35,204504873+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57077760,46681642326\n\"2016-04-05T09:15:36,208631176+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57085952,46701590106\n\"2016-04-05T09:15:37,213376928+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57135104,46724314636\n\"2016-04-05T09:15:38,223823399+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57032704,46744828341\n\"2016-04-05T09:15:39,227583943+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57012224,46778122099\n\"2016-04-05T09:15:40,231106074+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57040896,46789590095\n\"2016-04-05T09:15:41,235703686+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57016320,46799240685\n\"2016-04-05T09:15:42,241454867+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57016320,46820220891\n\"2016-04-05T09:15:43,246291303+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57016320,46832997219\n\"2016-04-05T09:15:44,252282862+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57016320,46841616587\n\"2016-04-05T09:15:45,257217425+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57016320,46851968715\n\"2016-04-05T09:15:46,261124210+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57016320,46860297577\n\"2016-04-05T09:15:47,265200824+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57016320,46863951502\n\"2016-04-05T09:15:48,268877350+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57016320,46873458023\n\"2016-04-05T09:15:49,273266945+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57016320,46877069123\n\"2016-04-05T09:15:50,278077166+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57016320,46887472871\n\"2016-04-05T09:15:51,281944071+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,46907140455\n\"2016-04-05T09:15:52,286923964+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,46910476659\n\"2016-04-05T09:15:53,290698145+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,46924697405\n\"2016-04-05T09:15:54,295065950+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57102336,46943695621\n\"2016-04-05T09:15:55,299740255+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,46946190497\n\"2016-04-05T09:15:56,304095586+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,46954444732\n\"2016-04-05T09:15:57,308445738+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,46965093488\n\"2016-04-05T09:15:58,312593284+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,46979581124\n\"2016-04-05T09:15:59,316646174+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,46988696063\n\"2016-04-05T09:16:00,321457658+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,46998429150\n\"2016-04-05T09:16:01,325162369+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57155584,47012250705\n\"2016-04-05T09:16:02,328449642+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47018802096\n\"2016-04-05T09:16:03,333395391+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47029662995\n\"2016-04-05T09:16:04,340185187+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47034598954\n\"2016-04-05T09:16:05,346193347+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47051796976\n\"2016-04-05T09:16:06,350901922+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47061017968\n\"2016-04-05T09:16:07,355691415+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47069392548\n\"2016-04-05T09:16:08,359204972+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47072335644\n\"2016-04-05T09:16:09,364111414+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47081069568\n\"2016-04-05T09:16:10,368329769+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47085028862\n\"2016-04-05T09:16:11,372491789+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47102174279\n\"2016-04-05T09:16:12,377210630+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47130997779\n\"2016-04-05T09:16:13,381438065+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47134155874\n\"2016-04-05T09:16:14,385856184+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47143093607\n\"2016-04-05T09:16:15,394871009+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47151972702\n\"2016-04-05T09:16:16,400056919+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47154500774\n\"2016-04-05T09:16:17,404748436+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47164698435\n\"2016-04-05T09:16:18,414210901+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47174870654\n\"2016-04-05T09:16:19,418556446+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47183678291\n\"2016-04-05T09:16:20,422240006+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47186707440\n\"2016-04-05T09:16:21,426419611+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47196503399\n\"2016-04-05T09:16:22,430672605+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47205533404\n\"2016-04-05T09:16:23,434865996+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47208508726\n\"2016-04-05T09:16:24,438437957+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47217117337\n\"2016-04-05T09:16:25,443294749+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47219258989\n\"2016-04-05T09:16:26,447777398+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47226938580\n\"2016-04-05T09:16:27,451770354+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47229544086\n\"2016-04-05T09:16:28,455769360+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47237320712\n\"2016-04-05T09:16:29,463279144+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47245616986\n\"2016-04-05T09:16:30,471991598+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47252918636\n\"2016-04-05T09:16:31,476024074+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47255429848\n\"2016-04-05T09:16:32,482599498+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47265593582\n\"2016-04-05T09:16:33,486068981+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47273026096\n\"2016-04-05T09:16:34,490252663+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47281234370\n\"2016-04-05T09:16:35,495074402+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47283470486\n\"2016-04-05T09:16:36,499810344+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47293044074\n\"2016-04-05T09:16:37,506492489+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47301122447\n\"2016-04-05T09:16:38,513560008+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47303583844\n\"2016-04-05T09:16:39,517730196+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47311087830\n\"2016-04-05T09:16:40,523995013+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47319289334\n\"2016-04-05T09:16:41,528936762+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47321353427\n\"2016-04-05T09:16:42,532820421+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47328516269\n\"2016-04-05T09:16:43,536652294+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47335450595\n\"2016-04-05T09:16:44,544040836+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47342695751\n\"2016-04-05T09:16:45,548231568+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47344583732\n\"2016-04-05T09:16:46,551802938+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47351210007\n\"2016-04-05T09:16:47,555701517+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47352926746\n\"2016-04-05T09:16:48,561909455+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47361860437\n\"2016-04-05T09:16:49,565982759+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47371384721\n\"2016-04-05T09:16:50,569794353+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47373698802\n\"2016-04-05T09:16:51,573640809+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47397636818\n\"2016-04-05T09:16:52,577624736+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57020416,47406180087\n\"2016-04-05T09:16:53,581093863+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47412908641\n\"2016-04-05T09:16:54,584602886+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47421152244\n\"2016-04-05T09:16:55,588333027+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47423510517\n\"2016-04-05T09:16:56,592130702+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47432600091\n\"2016-04-05T09:16:57,596481478+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47440453044\n\"2016-04-05T09:16:58,600698710+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47442308551\n\"2016-04-05T09:16:59,605168575+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47449668140\n\"2016-04-05T09:17:00,612798516+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47457937504\n\"2016-04-05T09:17:01,616863074+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47460076366\n\"2016-04-05T09:17:02,620589774+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47469068197\n\"2016-04-05T09:17:03,623840061+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47478533750\n\"2016-04-05T09:17:04,627976759+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47484678054\n\"2016-04-05T09:17:05,632124983+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47486756976\n\"2016-04-05T09:17:06,636921050+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47493611079\n\"2016-04-05T09:17:07,642542110+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47500001968\n\"2016-04-05T09:17:08,646583165+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47501940700\n\"2016-04-05T09:17:09,654088717+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47509836572\n\"2016-04-05T09:17:10,658685068+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47516789517\n\"2016-04-05T09:17:11,663720956+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47518684956\n\"2016-04-05T09:17:12,669002704+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47526177520\n\"2016-04-05T09:17:13,673011240+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47528461805\n\"2016-04-05T09:17:14,677054284+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47537460676\n\"2016-04-05T09:17:15,680869035+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47545295501\n\"2016-04-05T09:17:16,684839538+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47554244219\n\"2016-04-05T09:17:17,688801966+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47556704789\n\"2016-04-05T09:17:18,693988498+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47566135554\n\"2016-04-05T09:17:19,698880071+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47573196422\n\"2016-04-05T09:17:20,703991194+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57024512,47575773970\n\"2016-04-05T09:17:21,709263585+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,47584443491\n\"2016-04-05T09:17:22,713579579+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,47593963829\n\"2016-04-05T09:17:23,718391483+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,47595763611\n\"2016-04-05T09:17:24,723197646+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,47604217094\n\"2016-04-05T09:17:25,727474647+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,47612937590\n\"2016-04-05T09:17:26,732603373+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,47615626092\n\"2016-04-05T09:17:27,736486510+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,47624642469\n\"2016-04-05T09:17:28,741636017+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,47627066035\n\"2016-04-05T09:17:29,746464439+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,47635039349\n\"2016-04-05T09:17:30,751435775+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,47656420091\n\"2016-04-05T09:17:31,755744600+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,47659031753\n\"2016-04-05T09:17:32,762952741+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,47666800713\n\"2016-04-05T09:17:33,766786833+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,47674672997\n\"2016-04-05T09:17:34,771152782+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,47677220411\n\"2016-04-05T09:17:35,776052183+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,47684778249\n\"2016-04-05T09:17:36,779492340+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,47692376428\n\"2016-04-05T09:17:37,783271141+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,47694293071\n\"2016-04-05T09:17:38,787803424+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,47703581087\n\"2016-04-05T09:17:39,791901121+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,47706155590\n\"2016-04-05T09:17:40,796168965+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,47714217442\n\"2016-04-05T09:17:41,800759190+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57028608,47721938128\n\"2016-04-05T09:17:42,804864990+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57032704,47729495422\n\"2016-04-05T09:17:43,814055108+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57032704,47731932919\n\"2016-04-05T09:17:44,822759424+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57032704,47738371871\n\"2016-04-05T09:17:45,827695495+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57032704,47745722686\n\"2016-04-05T09:17:46,832246778+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57032704,47753042644\n\"2016-04-05T09:17:47,837079385+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57032704,47755271492\n\"2016-04-05T09:17:48,840505808+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57032704,47761354108\n\"2016-04-05T09:17:49,843862786+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57032704,47765998519\n\"2016-04-05T09:17:50,848027411+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57032704,47774134245\n\"2016-04-05T09:17:51,851689407+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57032704,47781465568\n\"2016-04-05T09:17:52,856733146+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47785733548\n\"2016-04-05T09:17:53,860178638+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47792845556\n\"2016-04-05T09:17:54,866758393+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47811373026\n\"2016-04-05T09:17:55,870092114+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47813158791\n\"2016-04-05T09:17:56,875128211+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47819867291\n\"2016-04-05T09:17:57,879717479+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47827023901\n\"2016-04-05T09:17:58,884327059+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47829203421\n\"2016-04-05T09:17:59,888347622+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47837234575\n\"2016-04-05T09:18:00,892581625+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47854400604\n\"2016-04-05T09:18:01,897469185+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47856407767\n\"2016-04-05T09:18:02,902466586+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47864616720\n\"2016-04-05T09:18:03,906640538+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47872389227\n\"2016-04-05T09:18:04,910229176+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47880315047\n\"2016-04-05T09:18:05,914553314+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47888262547\n\"2016-04-05T09:18:06,918414341+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47896470999\n\"2016-04-05T09:18:07,926117820+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47898553459\n\"2016-04-05T09:18:08,932593083+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47905254563\n\"2016-04-05T09:18:09,936519504+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47918946418\n\"2016-04-05T09:18:10,939991813+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47921165176\n\"2016-04-05T09:18:11,943647765+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47928585432\n\"2016-04-05T09:18:12,947506229+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47930647946\n\"2016-04-05T09:18:13,950837215+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47937933345\n\"2016-04-05T09:18:14,954863176+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47944841918\n\"2016-04-05T09:18:15,958405119+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47947038234\n\"2016-04-05T09:18:16,961814694+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47956739903\n\"2016-04-05T09:18:17,967658811+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47961209986\n\"2016-04-05T09:18:18,972869222+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47969573727\n\"2016-04-05T09:18:19,976733940+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47978099629\n\"2016-04-05T09:18:20,980390400+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47980359032\n\"2016-04-05T09:18:21,986987020+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47986632590\n\"2016-04-05T09:18:22,995610777+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47993661623\n\"2016-04-05T09:18:23,999640404+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,47995916124\n\"2016-04-05T09:18:25,007530127+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48004096311\n\"2016-04-05T09:18:26,011861612+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48010057006\n\"2016-04-05T09:18:27,016489700+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48011747162\n\"2016-04-05T09:18:28,021065140+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48018894865\n\"2016-04-05T09:18:29,024895856+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48026669462\n\"2016-04-05T09:18:30,028749993+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48032677353\n\"2016-04-05T09:18:31,034010831+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48034521917\n\"2016-04-05T09:18:32,038355847+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48042135849\n\"2016-04-05T09:18:33,041912830+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48050324697\n\"2016-04-05T09:18:34,045054648+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48052421555\n\"2016-04-05T09:18:35,048379560+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48059327729\n\"2016-04-05T09:18:36,051436396+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48067719063\n\"2016-04-05T09:18:37,055604510+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48075038950\n\"2016-04-05T09:18:38,063980748+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48077007911\n\"2016-04-05T09:18:39,068218513+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48083899830\n\"2016-04-05T09:18:40,072966244+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48086011272\n\"2016-04-05T09:18:41,079650049+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48104888608\n\"2016-04-05T09:18:42,083851926+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48106557515\n\"2016-04-05T09:18:43,088869651+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48113893649\n\"2016-04-05T09:18:44,092624262+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48121445226\n\"2016-04-05T09:18:45,097362941+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48129011392\n\"2016-04-05T09:18:46,105159788+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48136101445\n\"2016-04-05T09:18:47,111127191+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48144650932\n\"2016-04-05T09:18:48,115832911+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48147041549\n\"2016-04-05T09:18:49,124980910+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48154562176\n\"2016-04-05T09:18:50,133018524+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48161323299\n\"2016-04-05T09:18:51,137273014+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48163938146\n\"2016-04-05T09:18:52,143474713+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48171221528\n\"2016-04-05T09:18:53,146771771+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48182474275\n\"2016-04-05T09:18:54,150714161+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48187582556\n\"2016-04-05T09:18:55,156897115+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48195087817\n\"2016-04-05T09:18:56,165063292+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48203500968\n\"2016-04-05T09:18:57,169308621+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48206538117\n\"2016-04-05T09:18:58,174214164+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48214775576\n\"2016-04-05T09:18:59,178258629+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48223360758\n\"2016-04-05T09:19:00,182488219+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48225947140\n\"2016-04-05T09:19:01,186542141+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48233780390\n\"2016-04-05T09:19:02,190439105+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48242148273\n\"2016-04-05T09:19:03,195575735+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48244328223\n\"2016-04-05T09:19:04,200975702+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48253470904\n\"2016-04-05T09:19:05,205967284+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48255863238\n\"2016-04-05T09:19:06,210172153+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48262511960\n\"2016-04-05T09:19:07,213616430+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48269890116\n\"2016-04-05T09:19:08,217377676+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57036800,48272173633\n\"2016-04-05T09:19:09,224324581+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57040896,48279981998\n\"2016-04-05T09:19:10,229350346+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57040896,48288989941\n\"2016-04-05T09:19:11,234021028+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57040896,48296181095\n\"2016-04-05T09:19:12,239780246+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57040896,48298378496\n\"2016-04-05T09:19:13,244276818+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57040896,48306870736\n\"2016-04-05T09:19:14,249225100+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57040896,48313662430\n\"2016-04-05T09:19:15,254018163+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57040896,48315984298\n\"2016-04-05T09:19:16,257543890+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57040896,48337515348\n\"2016-04-05T09:19:17,260938979+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57040896,48339131757\n\"2016-04-05T09:19:18,264958447+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57040896,48346040048\n\"2016-04-05T09:19:19,269199375+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57040896,48354748986\n\"2016-04-05T09:19:20,275536363+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57040896,48364458605\n\"2016-04-05T09:19:21,280106488+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57040896,48366840559\n\"2016-04-05T09:19:22,284884553+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48375434305\n\"2016-04-05T09:19:23,288843766+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48383952465\n\"2016-04-05T09:19:24,293324826+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48392980367\n\"2016-04-05T09:19:25,297887822+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48395624832\n\"2016-04-05T09:19:26,302498221+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48403226704\n\"2016-04-05T09:19:27,307917390+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48405999351\n\"2016-04-05T09:19:28,312499794+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48414864449\n\"2016-04-05T09:19:29,316992028+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48417457921\n\"2016-04-05T09:19:30,320996710+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48426894588\n\"2016-04-05T09:19:31,326503944+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48435769456\n\"2016-04-05T09:19:32,331217113+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48445480000\n\"2016-04-05T09:19:33,335535920+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48447741589\n\"2016-04-05T09:19:34,339131327+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48453968428\n\"2016-04-05T09:19:35,344247533+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48460703772\n\"2016-04-05T09:19:36,353244757+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48469260700\n\"2016-04-05T09:19:37,358056412+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48471795214\n\"2016-04-05T09:19:38,365404574+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48480356999\n\"2016-04-05T09:19:39,369149000+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48489594775\n\"2016-04-05T09:19:40,374150341+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48491608660\n\"2016-04-05T09:19:41,378315159+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48498802441\n\"2016-04-05T09:19:42,384059817+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48507097100\n\"2016-04-05T09:19:43,388463711+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48509487297\n\"2016-04-05T09:19:44,395052693+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48518347636\n\"2016-04-05T09:19:45,399071049+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48528400395\n\"2016-04-05T09:19:46,404046410+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48534846578\n\"2016-04-05T09:19:47,408611828+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",57044992,48542216425\n\"2016-04-05T09:19:48,416331060+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56307712,48544538930\n\"2016-04-05T09:19:49,421093900+0000\",\"cAdvisor version 0.22.0 (002d8aa)\",56307712,48552648409\nCurrent master:\nTime,Version,Memory in bytes,CPU in ns\n\"2016-04-05T09:56:21,060994613+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",0,0\n\"2016-04-05T09:56:22,064740789+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",2514944,33240044\n\"2016-04-05T09:56:23,068689053+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",2514944,33736971\n\"2016-04-05T09:56:24,073604399+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",2961408,38312976\n\"2016-04-05T09:56:25,077203635+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",2961408,38312976\n\"2016-04-05T09:56:26,080224282+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",3485696,43306903\n\"2016-04-05T09:56:27,083077129+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",10559488,493365819\n\"2016-04-05T09:56:28,086610700+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",10526720,589460183\n\"2016-04-05T09:56:29,092716486+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",10534912,709370479\n\"2016-04-05T09:56:30,095904926+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",10543104,821242358\n\"2016-04-05T09:56:31,099133394+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",10625024,926569869\n\"2016-04-05T09:56:32,102932090+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",10731520,1022967208\n\"2016-04-05T09:56:33,107781058+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",10637312,1158362683\n\"2016-04-05T09:56:34,113011785+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",10883072,1292098822\n\"2016-04-05T09:56:35,116531821+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",11083776,1428667268\n\"2016-04-05T09:56:36,120283465+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",11390976,1538064076\n\"2016-04-05T09:56:37,126066217+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",11558912,1657855504\n\"2016-04-05T09:56:38,129244748+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",11771904,1815109396\n\"2016-04-05T09:56:39,133977801+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",12148736,1916561353\n\"2016-04-05T09:56:40,137975946+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",12574720,2061077837\n\"2016-04-05T09:56:41,142451904+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",12685312,2187372620\n\"2016-04-05T09:56:42,145860367+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",13123584,2330019918\n\"2016-04-05T09:56:43,148854821+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",13262848,2472600764\n\"2016-04-05T09:56:44,153809293+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",13320192,2581224560\n\"2016-04-05T09:56:45,157984988+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",13733888,2747942350\n\"2016-04-05T09:56:46,161132290+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",14069760,2878770248\n\"2016-04-05T09:56:47,164967381+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",14135296,3018481587\n\"2016-04-05T09:56:48,167851833+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",14594048,3192610430\n\"2016-04-05T09:56:49,170355352+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",14946304,3341979389\n\"2016-04-05T09:56:50,174718229+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",15212544,3513869102\n\"2016-04-05T09:56:51,179733892+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",15527936,3635838720\n\"2016-04-05T09:56:52,185627977+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",154054656,3850884472\n\"2016-04-05T09:56:53,197955307+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",3244572672,4670073136\n\"2016-04-05T09:56:54,225809733+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",4135862272,5860024289\n\"2016-04-05T09:56:55,267051482+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",4598874112,7001826649\n\"2016-04-05T09:56:56,270096044+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",4603203584,7133933423\n\"2016-04-05T09:56:57,291365479+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",4882763776,7802746912\n\"2016-04-05T09:56:58,320441050+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",5147406336,8963198271\n\"2016-04-05T09:56:59,362654586+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6080638976,10155373354\n\"2016-04-05T09:57:00,370169359+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6221344768,11242111342\n\"2016-04-05T09:57:01,373533324+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6248169472,12264306358\n\"2016-04-05T09:57:02,377916768+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6280810496,13338823468\n\"2016-04-05T09:57:03,384118248+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6283268096,14487508946\n\"2016-04-05T09:57:04,387778144+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6302380032,15503025627\n\"2016-04-05T09:57:05,399668447+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6324346880,16572335047\n\"2016-04-05T09:57:06,405885922+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6357188608,17618070103\n\"2016-04-05T09:57:07,410208836+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6365724672,18674155864\n\"2016-04-05T09:57:08,417109287+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6424035328,19719943716\n\"2016-04-05T09:57:09,420422416+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6530236416,20755744209\n\"2016-04-05T09:57:10,424549512+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6565523456,21801893018\n\"2016-04-05T09:57:11,430476609+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6638903296,22845833596\n\"2016-04-05T09:57:12,434331270+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6670237696,23882285384\n\"2016-04-05T09:57:13,437134162+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6704689152,24930797368\n\"2016-04-05T09:57:14,440571253+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6744625152,25964386341\n\"2016-04-05T09:57:15,446157624+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6823477248,27036647845\n\"2016-04-05T09:57:16,449876861+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6915514368,28071263594\n\"2016-04-05T09:57:17,458790425+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6933438464,29107874562\n\"2016-04-05T09:57:18,469630509+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6948601856,30137298846\n\"2016-04-05T09:57:19,473963634+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6968127488,31169632806\n\"2016-04-05T09:57:20,478431122+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6985613312,32138481208\n\"2016-04-05T09:57:21,483105922+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",6998134784,33128732440\n\"2016-04-05T09:57:22,485818608+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7018029056,34105314898\n\"2016-04-05T09:57:23,492695696+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7026286592,35120573847\n\"2016-04-05T09:57:24,496026037+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7020691456,36106490776\n\"2016-04-05T09:57:25,499333441+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7021678592,37186070873\n\"2016-04-05T09:57:26,509824334+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7020802048,38219099041\n\"2016-04-05T09:57:27,516152794+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7031156736,39239784787\n\"2016-04-05T09:57:28,519151009+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7032328192,40294335937\n\"2016-04-05T09:57:29,522592835+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7033495552,41298411781\n\"2016-04-05T09:57:30,527042443+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7042424832,42294300651\n\"2016-04-05T09:57:31,530188744+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7042678784,43311547741\n\"2016-04-05T09:57:32,535338067+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7043182592,44274690746\n\"2016-04-05T09:57:33,538568286+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7042797568,45259546500\n\"2016-04-05T09:57:34,541986402+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7045664768,46203322634\n\"2016-04-05T09:57:35,545386101+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7047397376,47196594388\n\"2016-04-05T09:57:36,548747866+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7132393472,48323745580\n\"2016-04-05T09:57:37,551892262+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7126261760,49419545391\n\"2016-04-05T09:57:38,555385886+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7104434176,50443307371\n\"2016-04-05T09:57:39,558922365+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7083835392,51480294771\n\"2016-04-05T09:57:40,564451477+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7144103936,52538739169\n\"2016-04-05T09:57:41,568599563+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7115599872,53525774650\n\"2016-04-05T09:57:42,572838989+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7155290112,54521645213\n\"2016-04-05T09:57:43,577700867+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7165063168,55552918427\n\"2016-04-05T09:57:44,582067050+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7174328320,56602098520\n\"2016-04-05T09:57:45,585255407+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7182147584,57647305686\n\"2016-04-05T09:57:46,589028503+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7188709376,58692590020\n\"2016-04-05T09:57:47,595944722+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7191584768,59705664751\n\"2016-04-05T09:57:48,598996827+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7191216128,60678533173\n\"2016-04-05T09:57:49,603808034+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7190831104,61657164071\n\"2016-04-05T09:57:50,608324400+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7192403968,62643864169\n\"2016-04-05T09:57:51,613285485+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7192608768,63641852921\n\"2016-04-05T09:57:52,617273676+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7191900160,64609993185\n\"2016-04-05T09:57:53,621326725+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7194329088,65602499834\n\"2016-04-05T09:57:54,632364412+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7197868032,66605638044\n\"2016-04-05T09:57:55,635670765+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7193784320,67617804724\n\"2016-04-05T09:57:56,638854041+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7193128960,68607265442\n\"2016-04-05T09:57:57,643384153+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7198044160,69680324684\n\"2016-04-05T09:57:58,646667668+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7202271232,70710718863\n\"2016-04-05T09:57:59,650116242+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7203889152,71739669296\n\"2016-04-05T09:58:00,653183995+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7204904960,72733702327\n\"2016-04-05T09:58:01,656756481+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7207276544,73719777218\n\"2016-04-05T09:58:02,661082404+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7200264192,74728772041\n\"2016-04-05T09:58:03,665152272+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7205388288,75700464764\n\"2016-04-05T09:58:04,676142035+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7207309312,76680463525\n\"2016-04-05T09:58:05,684823882+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7202418688,77729771373\n\"2016-04-05T09:58:06,690356209+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7208390656,78757394937\n\"2016-04-05T09:58:07,695202294+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7210262528,79756171219\n\"2016-04-05T09:58:08,700484298+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7210217472,80730296895\n\"2016-04-05T09:58:09,705099043+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7207297024,81722347112\n\"2016-04-05T09:58:10,713265336+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7209623552,82732348803\n\"2016-04-05T09:58:11,723235190+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7211909120,83782743208\n\"2016-04-05T09:58:12,731676395+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7209746432,84833164643\n\"2016-04-05T09:58:13,741553801+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7208177664,85881226607\n\"2016-04-05T09:58:14,756172406+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7205715968,86916903898\n\"2016-04-05T09:58:15,760429536+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7198117888,87870011694\n\"2016-04-05T09:58:16,764653334+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7178149888,88864785572\n\"2016-04-05T09:58:17,770241676+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7153782784,89834237880\n\"2016-04-05T09:58:18,776668355+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7157858304,90862446513\n\"2016-04-05T09:58:19,782630312+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7160832000,91871916678\n\"2016-04-05T09:58:20,786593900+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7170265088,92909829184\n\"2016-04-05T09:58:21,793184064+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7183605760,93885539635\n\"2016-04-05T09:58:22,796671401+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7179247616,94903431640\n\"2016-04-05T09:58:23,800157668+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7184539648,95908070409\n\"2016-04-05T09:58:24,805656193+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7188340736,96920799307\n\"2016-04-05T09:58:25,810390629+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7186022400,97910129060\n\"2016-04-05T09:58:26,814729217+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7189798912,98925872093\n\"2016-04-05T09:58:27,820869380+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7184314368,99919711003\n\"2016-04-05T09:58:28,825978180+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7187775488,100952295106\n\"2016-04-05T09:58:29,830359848+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7185395712,101965390160\n\"2016-04-05T09:58:30,835647115+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7188598784,102999312196\n\"2016-04-05T09:58:31,840312145+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7185956864,104014414337\n\"2016-04-05T09:58:32,844941643+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7175356416,105030099114\n\"2016-04-05T09:58:33,848359991+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7183204352,106068309839\n\"2016-04-05T09:58:34,851988398+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7183785984,107076593279\n\"2016-04-05T09:58:35,855827776+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7186161664,108112488909\n\"2016-04-05T09:58:36,859795306+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7188205568,109126943681\n\"2016-04-05T09:58:37,865072817+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7187804160,110140105474\n\"2016-04-05T09:58:38,882111233+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7185608704,111161119740\n\"2016-04-05T09:58:39,887184316+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7178903552,112194247501\n\"2016-04-05T09:58:40,890244562+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7167205376,113201924578\n\"2016-04-05T09:58:41,894108494+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7163518976,114241349055\n\"2016-04-05T09:58:42,898526986+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7180193792,115235263960\n\"2016-04-05T09:58:43,902381837+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7167614976,116238200726\n\"2016-04-05T09:58:44,906214627+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7159951360,117272712106\n\"2016-04-05T09:58:45,909460915+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7156162560,118291946313\n\"2016-04-05T09:58:46,912943398+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7154384896,119309883781\n\"2016-04-05T09:58:47,918335425+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7206064128,119681092270\n\"2016-04-05T09:58:48,927361341+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7230566400,120348491677\n\"2016-04-05T09:58:49,940238131+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7228080128,120797639213\n\"2016-04-05T09:58:50,943894277+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7255891968,121188508458\n\"2016-04-05T09:58:51,949574707+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7244324864,121539553120\n\"2016-04-05T09:58:52,954567507+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7278022656,121821190331\n\"2016-04-05T09:58:53,958857002+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7263404032,122220332909\n\"2016-04-05T09:58:54,969058679+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7288709120,122560707172\n\"2016-04-05T09:58:55,974846304+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7278587904,122884304885\n\"2016-04-05T09:58:56,979973976+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7310045184,123186620534\n\"2016-04-05T09:58:57,986201786+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7322505216,123543414638\n\"2016-04-05T09:58:58,997889508+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7306141696,123928781370\n\"2016-04-05T09:59:00,002699191+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7315324928,124297913718\n\"2016-04-05T09:59:01,010388930+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7306416128,124699162031\n\"2016-04-05T09:59:02,014171846+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7327985664,125060490477\n\"2016-04-05T09:59:03,018089031+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7316557824,125377894418\n\"2016-04-05T09:59:04,026004317+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7349305344,125737658133\n\"2016-04-05T09:59:05,031859339+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7371943936,126052872268\n\"2016-04-05T09:59:06,036271626+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7391109120,126394845333\n\"2016-04-05T09:59:07,039849709+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7436517376,126723883126\n\"2016-04-05T09:59:08,044466484+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7444271104,127051553419\n\"2016-04-05T09:59:09,050735512+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7431868416,127420976722\n\"2016-04-05T09:59:10,077066114+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7441866752,127772194372\n\"2016-04-05T09:59:11,082807750+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7443542016,128100194907\n\"2016-04-05T09:59:12,093843974+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7472058368,128422907357\n\"2016-04-05T09:59:13,098302469+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7417348096,128811749184\n\"2016-04-05T09:59:14,110665172+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7466749952,129151554172\n\"2016-04-05T09:59:15,116735763+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7453483008,129475667494\n\"2016-04-05T09:59:16,120273571+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7474053120,129805053522\n\"2016-04-05T09:59:17,124727427+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7488974848,130133186573\n\"2016-04-05T09:59:18,128631700+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7488770048,130472109440\n\"2016-04-05T09:59:19,133071789+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7485251584,130816768089\n\"2016-04-05T09:59:20,138083706+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7465676800,131120132348\n\"2016-04-05T09:59:21,141695141+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7468728320,131431216237\n\"2016-04-05T09:59:22,145359856+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7506673664,131714574527\n\"2016-04-05T09:59:23,149443274+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7509385216,132047911322\n\"2016-04-05T09:59:24,153517339+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7495368704,132402805269\n\"2016-04-05T09:59:25,157097706+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7506452480,132724471823\n\"2016-04-05T09:59:26,164841487+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7504355328,133095884906\n\"2016-04-05T09:59:27,171991846+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7502462976,133428735530\n\"2016-04-05T09:59:28,175347368+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7505346560,133774326873\n\"2016-04-05T09:59:29,178923078+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7519326208,134100657617\n\"2016-04-05T09:59:30,188077589+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7523991552,134423403903\n\"2016-04-05T09:59:31,192885138+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7531036672,134768632371\n\"2016-04-05T09:59:32,202592282+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7478079488,135138620060\n\"2016-04-05T09:59:33,215606687+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7483162624,135458072764\n\"2016-04-05T09:59:34,221359152+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7504887808,135800923089\n\"2016-04-05T09:59:35,224585471+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7511195648,136111516919\n\"2016-04-05T09:59:36,228507577+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7526780928,136413159369\n\"2016-04-05T09:59:37,241854116+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7540555776,136732526951\n\"2016-04-05T09:59:38,257579489+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7552958464,137035701342\n\"2016-04-05T09:59:39,264914337+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7536197632,137434610468\n\"2016-04-05T09:59:40,280547790+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7546626048,137807147315\n\"2016-04-05T09:59:41,284343487+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7546294272,138109475166\n\"2016-04-05T09:59:42,288775833+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7552020480,138471386896\n\"2016-04-05T09:59:43,303636204+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7571472384,139538378311\n\"2016-04-05T09:59:44,319246321+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7565639680,140578642394\n\"2016-04-05T09:59:45,388638698+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7627010048,141772555843\n\"2016-04-05T09:59:46,645857641+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7661502464,143127705420\n\"2016-04-05T09:59:47,774906932+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7691460608,144548594668\n\"2016-04-05T09:59:49,178357050+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7704555520,145748415771\n\"2016-04-05T09:59:50,188945226+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7708348416,146957969519\n\"2016-04-05T09:59:51,245455967+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7709958144,148245830447\n\"2016-04-05T09:59:52,282380590+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7703990272,149431184824\n\"2016-04-05T09:59:53,422818542+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7706771456,150592159790\n\"2016-04-05T09:59:54,469489836+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7712169984,151751345970\n\"2016-04-05T09:59:55,488010310+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7713415168,152807893264\n\"2016-04-05T09:59:56,545717277+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7719006208,153981531527\n\"2016-04-05T09:59:57,821179286+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7732572160,155396000889\n\"2016-04-05T09:59:59,191897423+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7729520640,156708114959\n\"2016-04-05T10:00:00,217504807+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7729586176,157832893369\n\"2016-04-05T10:00:01,338360997+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7716675584,159207111444\n\"2016-04-05T10:00:02,356824110+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7720955904,160379896152\n\"2016-04-05T10:00:03,384073027+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7712710656,161515094139\n\"2016-04-05T10:00:04,403897318+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7714787328,162750685351\n\"2016-04-05T10:00:05,419620577+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7707082752,163979422216\n\"2016-04-05T10:00:06,441570141+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7702568960,165191787856\n\"2016-04-05T10:00:07,532089696+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7700951040,166686415238\n\"2016-04-05T10:00:09,126356731+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7699546112,168367022342\n\"2016-04-05T10:00:10,268429661+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7694647296,169651600619\n\"2016-04-05T10:00:11,888578509+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7706415104,171279690253\n\"2016-04-05T10:00:13,151051601+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7719829504,172761930478\n\"2016-04-05T10:00:17,089744117+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7720951808,176850902642\n\"2016-04-05T10:00:18,258627727+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7719948288,177986806555\n\"2016-04-05T10:00:19,729439122+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7719841792,179537976123\n\"2016-04-05T10:00:21,000446000+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7720468480,180801104823\n\"2016-04-05T10:00:22,136293716+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",7718846464,181810275537\n\"2016-04-05T10:00:24,024339633+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:25,055046301+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:26,058957445+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:27,063233039+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:28,066113599+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:29,070225723+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:30,075677680+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:31,080435877+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:32,084706806+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:33,090109666+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:34,094613030+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:35,099254350+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:36,106389306+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:37,114445407+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:38,119552166+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:39,124376834+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:40,129443735+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:41,133957084+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:42,138325125+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:43,144981607+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:44,152053222+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:45,156701317+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:46,162164821+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:47,166623562+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:48,171832803+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:49,177482671+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:50,182773472+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:51,187040057+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:52,191354480+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:53,194776099+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:54,199458969+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:55,204369001+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:56,209271433+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:57,215066833+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:58,219129618+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:00:59,223787524+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:00,228455510+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:01,233400309+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:02,237497138+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:03,241951991+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:04,246322900+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:05,250955989+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:06,259423550+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:07,264543667+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:08,269627670+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:09,278899747+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:10,283267754+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:11,287836936+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:12,292995902+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:13,299148925+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:14,303575201+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:15,309608577+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:16,314352398+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:17,319330308+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:18,324213885+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:19,328105550+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:20,333174935+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:21,341158776+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:22,345697711+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:23,350806897+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:24,356865678+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:25,361404108+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:26,366801398+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:27,371283463+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:28,375849756+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:29,381353142+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:30,385653242+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:31,391045627+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:32,395022376+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:33,400773517+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:34,408517297+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:35,413005023+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:36,422513897+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:37,432365558+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:38,437599019+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:39,441963130+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:40,445612894+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:41,453126843+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:42,458061246+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:43,462407135+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:44,466705981+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:45,471157945+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:46,475606185+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:47,481196826+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:48,485800963+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:49,492198337+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:50,496025085+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:51,505183289+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:52,509577136+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:53,513896411+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:54,517941362+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:55,522664661+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:56,526342845+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:57,530180620+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:58,534790542+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:01:59,539925006+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:00,544238434+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:01,548800378+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:02,553305815+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:03,557615132+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:04,562015175+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:05,566794890+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:06,570413449+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:07,574266050+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:08,581874645+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:09,586166540+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:10,592944811+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:11,596621903+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:12,602688948+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:13,607306698+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:14,613067579+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:15,617121967+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:16,621205503+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:17,624874100+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:18,628311321+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:19,633021646+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:20,637150201+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:21,640595088+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:22,643905844+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:23,647869057+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:24,651841998+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:25,655700618+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:26,659199643+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:27,663015807+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:28,669986565+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:29,673768851+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:30,678222556+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:31,682985331+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:32,691314154+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:33,695219764+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:34,699527802+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:35,703873968+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:36,707727533+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:37,712758728+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:38,717526942+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:39,723074499+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:40,727360731+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:41,732245815+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:42,736588584+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:43,741403398+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:44,745557941+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:45,749348344+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:46,753817107+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:47,757429534+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:48,760839962+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:49,764872587+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:50,769658653+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:51,774157736+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:52,777788183+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:53,782875527+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:54,787271482+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:55,793527704+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:56,797224106+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:57,801354994+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:58,805310147+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:02:59,810117999+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:00,813938082+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:01,819979005+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:02,823972567+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:03,828437261+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:04,832419665+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:05,835941514+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:06,839821872+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:07,843702020+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:08,847866595+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:09,851088596+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:10,854762972+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:11,860060107+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:12,863406963+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:13,866632419+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:14,871353479+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:15,874827436+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:16,879374515+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:17,883822112+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:18,888630043+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:19,892932801+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:20,896811168+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:21,902381437+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:22,907362431+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:23,912501401+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:24,917212354+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:25,921213476+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:26,924791362+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:27,931182686+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:28,935241725+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:29,940316015+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:30,945622140+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:31,950226391+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:32,955058787+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:33,960279013+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:34,965445795+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:35,970314536+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:36,977257330+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:37,982209039+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:38,986693737+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:39,992462278+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:40,997817278+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:42,003285612+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:43,007967731+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:44,012894197+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:45,016994495+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:46,020361274+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:47,024418018+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:48,028183845+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:49,031985010+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:50,035923167+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:51,042604161+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:52,047148508+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:53,051734318+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:54,056476989+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:55,063047676+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:56,067756124+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:57,071788192+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:58,075505329+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:03:59,080350548+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:00,085140030+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:01,089776972+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:02,096755439+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:03,101983629+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:04,110777423+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:05,116143699+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:06,121050707+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:07,126131772+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:08,130776989+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:09,135140250+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:10,140443395+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:11,147467800+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:12,151859682+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:13,156069355+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:14,163520169+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:15,168336201+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:16,173940225+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:17,179786558+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:18,184810279+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:19,190022495+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:20,195779516+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:21,201412800+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:22,206716008+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:23,211866424+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:24,216625341+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:25,221476545+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:26,225704507+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:27,231551984+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:28,236578034+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:29,241621558+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:30,247286483+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:31,253095922+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:32,257309662+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:33,261618928+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:34,266765009+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:35,271793877+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:36,275559344+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:37,282036926+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:38,286060966+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:39,289289169+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:40,295962632+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:41,301518771+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:42,306323222+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:43,311100646+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:44,315561653+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:45,319915159+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:46,323666579+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:47,328451397+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:48,332746757+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:49,336833074+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:50,343428528+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:51,348212482+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:52,354519856+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:53,359160299+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:54,363917331+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:55,372868615+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:56,376987822+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:57,381287407+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:58,386185220+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:04:59,393324598+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:00,398413038+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:01,403968818+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:02,408607040+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:03,412554177+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:04,420922780+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:05,425502730+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:06,429449851+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:07,433995855+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:08,437572045+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:09,441270689+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:10,445213557+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:11,449938760+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:12,454619973+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:13,458700102+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:14,463473015+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:15,469029598+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:16,472380892+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:17,475968116+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:18,480388456+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:19,484735944+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:20,488730686+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:21,492930480+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:22,496795151+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:23,500963137+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:24,505519341+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:25,513043114+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:26,517954731+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:27,521956889+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:28,525930215+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:29,530652868+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:30,534738232+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:31,540183127+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:32,544261901+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:33,548699487+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:34,552988468+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:35,557287996+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:36,561420799+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:37,565302219+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:38,572518327+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:39,576224065+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:40,582255234+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:41,586591276+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:42,592650920+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:43,597960641+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:44,602978115+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:45,607287823+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:46,615463323+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:47,622682539+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:48,631499564+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:49,641328210+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:50,645726899+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:51,652342976+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:52,660762193+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:53,665241767+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:54,669708712+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:55,680639918+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:56,685346807+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:57,689658292+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:58,693657435+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:05:59,698926866+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:00,704101178+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:01,709301670+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:02,714336544+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:03,718440869+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:04,722606845+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:05,728993465+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:06,732516875+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:07,736697027+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:08,741068140+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:09,746218245+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:10,750327900+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:11,754752350+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:12,758932478+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:13,762215077+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:14,766195289+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:15,770813457+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:16,774794162+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:17,779704123+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:18,784905626+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:19,788973453+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:20,792936062+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:21,797233816+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:22,801374286+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:23,805782394+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:24,810668309+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:25,815455846+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:26,820007993+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:27,824134479+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:28,829065430+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:29,833439463+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:30,838543514+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n\"2016-04-05T10:06:31,842942206+0000\",\"cAdvisor version 0.22.0 (5d7c71a)\",49152,182904886950\n(note: the residual 49152 bytes of memory appear to be a cache, by looking at memory.stat)\n. Quick update: I just tried to start test containers slower (sleep one second after launching each container), the outcome was essentially the same (cAdvisor uses tons of CPU and memory, system runs out of memory, kernel kills cAdvisor):\nFor perspective, here's what top reported a few seconds before cAdvisor was killed.\n\n. At this point, I'm a little unsure the load reader currently works with a non-trivial number of containers and the default 1s housekeeping interval (I'm currently running cAdvisor with the load reader in production here at Aptible, but we're using a 30s housekeeping interval, which greatly reduces the probability of a race condition, but makes the data somewhat unreliable, hence why I'd like to get it to work at a lower interval), so I'll stop testing now.\nNonetheless, I'm happy to continue testing if you have suggestions for a test case that might not crash. Just let me know!\nThanks!\n. @vishh The following is a little OT for this PR, but perhaps relevant for you to know: since the load reader probes for the number of tasks at a given instant, I think you do need a reporting interval that's short enough in order to get accurate reporting. 5 seconds is more or less OK in my testing, but above that your LA starts jumping up and down without reflecting any changes in the actual workload. Practically speaking, this means that if your housekeeping interval is e.g. 30 seconds, then the load reader will be very inaccurate (it'll report variations in workload that don't actually exist).\nI actually have a patch set to use a separate interval for the load reader and the rest of the housekeeping loop (https://github.com/aptible/cadvisor-factory/pull/9/files). I'm happy to try and clean it up and submit it as a PR if you think that would make sense.\n. ... continuing on that comment: a dynamic probing interval works great with the load reader (it'll avoid wasting resources on containers that are sleeping, all the while providing accurate data for others), but I think the code that calculates the load average needs to be adjusted a little bit in order to account for the fact that the decay should depend on how old the last point was (I actually have a patch for that in my patch set as well).\n. > @krallin The polling frequency is always an issue when deriving value from instantaneous metrics. Thanks for sharing your experience. It might be helpful to run pprof on cadvisor while doing performance testing, to evaluate the resource consumption of cpu load calculations. If it is trivial, we could even run it every second.\n:+1:\n. Hi folks,\nAny movement here?\nThanks!\n. Hi folks,\nIt looks like there are conflicts with master now. I can work on resolving them, but before I do so, I'd like to be confident that this will actually get merged once I do. Let me know?\nThanks!\n. @timstclair: just updated the PR \u2014 sorry about the delay (I was on vacation when you originally replied to my comment and missed it). \nI have tested this locally and it continues to complete my stress test without crashing. Looks like the CI tests are passing as well. \nCheers,\n. Awesome; thanks @timstclair !\n. No worries; thanks for the heads up @vishh!\n. Looks like there was a test that failed here. I'll look into what happened shortly \ud83d\ude04 \n. @vishh \nI think the failing test is due to a change in behavior I made that causes the fsHandler to stop reporting stats if none are available. Indeed, the fsHandler runs on its own schedule in a separate goroutine, so if the container is asked for metrics before the fsHandler is done running, then the fsHandler has two options:\n- Return incomplete / invalid data. This is what cAdvisor used to do. It would report usage as 0, although it did correctly report disk size. Unfortunately, that's not really a viable option once you stop pulling disk size from a static cache (which we used to). In practice, 0 is not nil, so that would explain why the test used to pass.\n- Return nothing. The fsHandler will eventually update, and it'll eventually return data, but it'll won't return any until it has updated. This is what it does with this patch. For clarity, that's what I was referring to in my \"Second, as of this PR, the FsHandler complains when it's asked for metrics it doesn't have\" comment.\n\nI'm not sure how I can reproduce the test failure to confirm. Perhaps we should update the test code to provide more relevant information, e.g. use:\nrequire.NotNil(t, info.Stats[0].Filesystem, \"got info: %+v\", info.Stats[0])\nLet me know what you think the next steps are here?\nThanks!\n. @lkzcgfvf \nThat's the accumulated CPU usage in nanoseconds. You should be able to convert it to core utilization with a query like:\nSELECT DERIVATIVE(value, 1s) / 1000000000 FROM cpu_usage_total WHERE ... AND time > now() - 1h\nNote that this won't be normalized by the number of cores, so if you have 4 cores utilized at 100%, then you'll get 4 as a result, and if you have 1 core utilized at 50%, then you'll get 0.5.\n. @arichardet the Task Stats netlink family (which cAdvisor uses to collect load stats) doesn't work within a network namespace. You'll need to either run your cAdvisor container in the host network (--net=host), or run outside of a container entirely.\n. @arichardet Do you have anything running in those containers? Do note that the LA reported by cAdvisor isn't fully equivalent to the LA e.g. top would report. In particular, cAdvisor does not include tasks waiting on IO in its LA (it only includes running / runabble tasks). This is something you could patch if you're willing to rebuild cAdvisor \u2014 e.g. https://github.com/aptible/cadvisor-factory/blob/master/patches/0006-Include-uninterruptible-and-IO-wait-tasks-in-LA.patch (I'm not sure how the cAdvisor team feels about updating this in mainline \u2014 I haven't brought it up).\nThere's also an issue with the load reader, where it might crash if you have a load of containers being monitored on a similar schedule (I need to update my PR in  https://github.com/google/cadvisor/pull/1188 to fix this), though IIRC this should cause the load reader to hang, not report 0.\nHope this helps! In my experience, the load reader does work reliably with the few tweaks I mentioned above :)\n. Just providing a little data point: we're using cAdvisor in standalone mode across about 1000 instances here. On average, it's using 0.2% of CPU and 20MB of RAM. There are a few outliers of course, but we've never really had problems with cAdvisor performance.\nAs @timstclair mentioned, tuning down the collection interval is helpful here. In our case, we use the following settings:\n\"--housekeeping_interval=30s\" \\\n    \"--global_housekeeping_interval=2m\" \\\n    \"--disable_metrics=disk,tcp\" \\\n    \"--enable_load_reader\" \\\n    \"--load_reader_interval=5s\"\nIn our experience, the tcp and disk metrics can be very expensive (though that largely depends on what your containers are doing), but the rest (CPU, LA, Memory) is very cheap.. I actually just moved that flag over from manager.go (i.e. it already exists in cAdvisor). \nWe could definitely use disable_metrics to disable CPU load monitoring, but that'd break CLI compatibility with past versions.\nLet me know what you think,\n. @timstclair Just to be sure I'm understanding you correctly, is this (the --listen flag) something you'd expect me to post as part of this patch, or something you'd like to look at later?\nCheers,\n. Sure. I'm out next week, but I can take care of this when I come back.\nDo we want backwards compatibility with the existing port and IP flags?\nOn Friday, 22 April 2016, Tim St. Clair notifications@github.com wrote:\n\nIn cadvisor.go\nhttps://github.com/google/cadvisor/pull/1225#discussion_r60780849:\n\n@@ -35,6 +36,7 @@ import (\n    \"github.com/golang/glog\"\n )\n+var argPath = flag.String(\"listen_path\", \"\", \"Path to listen on (UNIX socket), defaults to empty (use TCP instead)\")\n\nRemoving flags is a lot harder than adding them, so if we want to use a\n--listen flag, I'd prefer not to add a --listen_path first.\nIn other words, yes, would you mind adding it in this patch?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1225/files/7fc1a906253f727cee5b662c05153929cbfc8b43#r60780849\n. \n",
    "cjschroed": "More info: I gave eth0 a static IP, however it did not have a cable attached, thus no link state and no speed. :(\n. ",
    "joelchen": "Hi Victor,\nI did not the second time after this issue happened, as it is very cumbersome to recover if it happens again.\n/validate:\n```\nKernel version: [Supported and recommended]\n    Kernel version is 3.18.2. Versions >= 2.6 are supported. 3.0+ are recommended.\nCgroup setup: [Supported and recommended]\n    Available cgroups: map[cpuacct:1 devices:1 freezer:1 blkio:1 perf_event:1 net_prio:1 cpuset:1 cpu:1 memory:1 net_cls:1]\n    Following cgroups are required: [cpu cpuacct]\n    Following other cgroups are recommended: [memory blkio cpuset devices freezer]\nCgroup mount setup: [Supported, but not recommended]\n    Cgroups are mounted at /rootfs/sys/fs/cgroup.\n    Cgroup mount directories: blkio cpu cpu,cpuacct cpuacct cpuset devices freezer memory net_cls net_cls,net_prio net_prio perf_event systemd \n    Any cgroup mount point that is detectible and accessible is supported. /sys/fs/cgroup is recommended as a standard location.\n    Cgroup mounts:\n    cgroup /rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /rootfs/sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0\n    cgroup /rootfs/sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /rootfs/sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /rootfs/sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /rootfs/sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /rootfs/sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0\n    cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /var/lib/docker/btrfs/subvolumes/8992af1f3ab84f57ca556d8d360379fd098f379edbe1c25a3b80df659e4f0474/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /var/lib/docker/btrfs/subvolumes/8992af1f3ab84f57ca556d8d360379fd098f379edbe1c25a3b80df659e4f0474/sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0\n    cgroup /var/lib/docker/btrfs/subvolumes/8992af1f3ab84f57ca556d8d360379fd098f379edbe1c25a3b80df659e4f0474/sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /var/lib/docker/btrfs/subvolumes/8992af1f3ab84f57ca556d8d360379fd098f379edbe1c25a3b80df659e4f0474/sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /var/lib/docker/btrfs/subvolumes/8992af1f3ab84f57ca556d8d360379fd098f379edbe1c25a3b80df659e4f0474/sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /var/lib/docker/btrfs/subvolumes/8992af1f3ab84f57ca556d8d360379fd098f379edbe1c25a3b80df659e4f0474/sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /var/lib/docker/btrfs/subvolumes/8992af1f3ab84f57ca556d8d360379fd098f379edbe1c25a3b80df659e4f0474/sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /var/lib/docker/btrfs/subvolumes/8992af1f3ab84f57ca556d8d360379fd098f379edbe1c25a3b80df659e4f0474/sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /var/lib/docker/btrfs/subvolumes/8992af1f3ab84f57ca556d8d360379fd098f379edbe1c25a3b80df659e4f0474/sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /var/lib/docker/btrfs/subvolumes/8992af1f3ab84f57ca556d8d360379fd098f379edbe1c25a3b80df659e4f0474/rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /var/lib/docker/btrfs/subvolumes/8992af1f3ab84f57ca556d8d360379fd098f379edbe1c25a3b80df659e4f0474/rootfs/sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0\n    cgroup /var/lib/docker/btrfs/subvolumes/8992af1f3ab84f57ca556d8d360379fd098f379edbe1c25a3b80df659e4f0474/rootfs/sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /var/lib/docker/btrfs/subvolumes/8992af1f3ab84f57ca556d8d360379fd098f379edbe1c25a3b80df659e4f0474/rootfs/sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /var/lib/docker/btrfs/subvolumes/8992af1f3ab84f57ca556d8d360379fd098f379edbe1c25a3b80df659e4f0474/rootfs/sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /var/lib/docker/btrfs/subvolumes/8992af1f3ab84f57ca556d8d360379fd098f379edbe1c25a3b80df659e4f0474/rootfs/sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /var/lib/docker/btrfs/subvolumes/8992af1f3ab84f57ca556d8d360379fd098f379edbe1c25a3b80df659e4f0474/rootfs/sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /var/lib/docker/btrfs/subvolumes/8992af1f3ab84f57ca556d8d360379fd098f379edbe1c25a3b80df659e4f0474/rootfs/sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /var/lib/docker/btrfs/subvolumes/8992af1f3ab84f57ca556d8d360379fd098f379edbe1c25a3b80df659e4f0474/rootfs/sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\nDocker version: [Supported and recommended]\n    Docker version is 1.4.1. Versions >= 1.0 are supported. 1.2+ are recommended.\nDocker driver setup: [Supported and recommended]\n    Docker exec driver is native-0.2. Storage driver is btrfs.\n    systemd is being used to create cgroups.\n    Docker container state directory is at \"/var/lib/docker/execdriver/native\" and is accessible.\n```\ndocker info:\nContainers: 8\nImages: 118\nStorage Driver: btrfs\n Build Version: Btrfs v3.17.1\n Library Version: 101\nExecution Driver: native-0.2\nKernel Version: 3.18.2\nOperating System: CoreOS 561.0.0\nCPUs: 1\nTotal Memory: 996.5 MiB\nName: ip-172-31-6-181.ap-southeast-1.compute.internal\nID: GKAX:GJCQ:JGNA:SSXV:QR2X:MDED:AXHF:VJFE:THIL:TUDW:PRRK:EJDE\nUsername: joelchen\nRegistry: [https://index.docker.io/v1/]\nWarmest Regards,\nJoel\n. ",
    "sstarcher": "+1 let me know if you need any help testing it\n. ",
    "ankushagarwal": "I have some code ready to go in, but I want to understand the long term goal for storage driver config options from CLI.\nCurrently for influxdb, the options are \n-storage_driver_host\n-storage_driver_db\n-storage_driver_user\n-storage_driver_password\n-storage_driver_secure\nAs more and more storage drivers are added, these options will become harder to serve as common options for all the drivers. For example graphite needs a \"graphite_prefix\" config option.\nI can try to fit graphite options using these options, or we can try to have separate options for each driver like what docker/machine does : https://github.com/docker/machine/blob/master/README.md\nWhat are your thoughts on this?\n. Cool. Then I will first refactor influxdb and bigquery config options in a PR and then open up another PR sometime next week to add a stable graphite driver.\n. I like this idea much better. It decouples the core cAdvisor from storage driver issues. Versioning will be one major problem when storage drivers are integrated within cAdvisor.\n. Yup, just run another container which is linked to cAdvisor, uses its API and exports stats to a storage driver. With this approach, data can also be exported to multiple storage drivers with ease.\n. ",
    "lesaux": "@ankushagarwal I tried your graphite driver implementation and it seems to be working well!\nI compiled from https://github.com/ankushagarwal/cadvisor/tree/graphite-driver\nran cadvisor with the following params:\n-logtostderr -storage_driver=\"graphite\" -storage_driver_host=\"192.168.0.61:2003\" -storage_driver_db=\"graphite\"  -allow_dynamic_housekeeping=false -housekeeping_interval=10s\nand got the following output:\nNum bytes written  590 590\nSend shit to graphite  1426196507\nAliases :  [hopeful_mestorf 6001525f0e0e0430659f3831c66ce8d768274dddac71a5584fb3d5e3ec767db8]\ngraphite.1b78e27ac033.hopeful_mestorf.cpu_cumulative_usage 966125072 1426196507\ngraphite.1b78e27ac033.hopeful_mestorf.memory_usage 63438848 1426196507\ngraphite.1b78e27ac033.hopeful_mestorf.memory_working_set 31633408 1426196507\ngraphite.1b78e27ac033.6001525f0e0e0430659f3831c66ce8d768274dddac71a5584fb3d5e3ec767db8.cpu_cumulative_usage 966125072 1426196507\ngraphite.1b78e27ac033.6001525f0e0e0430659f3831c66ce8d768274dddac71a5584fb3d5e3ec767db8.memory_usage 63438848 1426196507\ngraphite.1b78e27ac033.6001525f0e0e0430659f3831c66ce8d768274dddac71a5584fb3d5e3ec767db8.memory_working_set 31633408 1426196507\nConfirmed I was able to browse the graphs by using graphite-api + grafana.\nIt's a very good start for the graphite driver!\n. In the meantime, I ship docker container metrics to graphite this way: https://github.com/lesaux/diamond-DockerContainerCollector\n. ",
    "esseti": "Hi all,\nis there any update/tutorial on how to send the data to grphite? it could be enough to send it to local collectd daemon running on the same machine and then use the write_graphite option. but is it possible?. ",
    "itn3rd77": "Here comes the output:\n```\ncAdvisor version: 0.8.0\nOS version: Buildroot 2014.02\nKernel version: [Supported and recommended]\n    Kernel version is 3.16.0-4-amd64. Versions >= 2.6 are supported. 3.0+ are recommended.\nCgroup setup: [Supported and recommended]\n    Available cgroups: map[memory:1 freezer:1 blkio:1 net_prio:1 cpu:1 cpuacct:1 net_cls:1 perf_event:1 cpuset:1 devices:1]\n    Following cgroups are required: [cpu cpuacct]\n    Following other cgroups are recommended: [memory blkio cpuset devices freezer]\nCgroup mount setup: [Supported, but not recommended]\n    Cgroups are mounted at /rootfs/sys/fs/cgroup.\n    Cgroup mount directories: blkio cpu cpu,cpuacct cpuacct cpuset devices freezer memory net_cls net_cls,net_prio net_prio perf_event systemd \n    Any cgroup mount point that is detectible and accessible is supported. /sys/fs/cgroup is recommended as a standard location.\n    Cgroup mounts:\n    cgroup /rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /rootfs/sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0\n    cgroup /rootfs/sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /rootfs/sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /rootfs/sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /rootfs/sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /rootfs/sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0\n    cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/693045cab59376f10fde9ae6b194c051cd0deb619e656aa180df300be2a987d4/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/693045cab59376f10fde9ae6b194c051cd0deb619e656aa180df300be2a987d4/sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/693045cab59376f10fde9ae6b194c051cd0deb619e656aa180df300be2a987d4/sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/693045cab59376f10fde9ae6b194c051cd0deb619e656aa180df300be2a987d4/sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0\n    cgroup /var/lib/docker/aufs/mnt/693045cab59376f10fde9ae6b194c051cd0deb619e656aa180df300be2a987d4/sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/693045cab59376f10fde9ae6b194c051cd0deb619e656aa180df300be2a987d4/sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/693045cab59376f10fde9ae6b194c051cd0deb619e656aa180df300be2a987d4/sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /var/lib/docker/aufs/mnt/693045cab59376f10fde9ae6b194c051cd0deb619e656aa180df300be2a987d4/sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/693045cab59376f10fde9ae6b194c051cd0deb619e656aa180df300be2a987d4/sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/693045cab59376f10fde9ae6b194c051cd0deb619e656aa180df300be2a987d4/rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/693045cab59376f10fde9ae6b194c051cd0deb619e656aa180df300be2a987d4/rootfs/sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/693045cab59376f10fde9ae6b194c051cd0deb619e656aa180df300be2a987d4/rootfs/sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/693045cab59376f10fde9ae6b194c051cd0deb619e656aa180df300be2a987d4/rootfs/sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0\n    cgroup /var/lib/docker/aufs/mnt/693045cab59376f10fde9ae6b194c051cd0deb619e656aa180df300be2a987d4/rootfs/sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/693045cab59376f10fde9ae6b194c051cd0deb619e656aa180df300be2a987d4/rootfs/sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/693045cab59376f10fde9ae6b194c051cd0deb619e656aa180df300be2a987d4/rootfs/sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /var/lib/docker/aufs/mnt/693045cab59376f10fde9ae6b194c051cd0deb619e656aa180df300be2a987d4/rootfs/sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/693045cab59376f10fde9ae6b194c051cd0deb619e656aa180df300be2a987d4/rootfs/sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\nDocker version: [Supported and recommended]\n    Docker version is 1.3.3. Versions >= 1.0 are supported. 1.2+ are recommended.\nDocker driver setup: [Supported and recommended]\n    Docker exec driver is native-0.2. Storage driver is aufs.\n    systemd is being used to create cgroups.\n    Docker container state directory is at \"/var/lib/docker/execdriver/native\" and is accessible.\n```\n. @vmarmol  Could you find something usefull in the output? Anything else I can do to track this down?\n. ",
    "MattMattV": "This feature seems to be nice but one time again, no documentation to know how that works... :cry: \n. bump\n. CLA is signed\n. The doc I edited needs to be more complete IMO\n. ",
    "fabiand": "@rjnagal @vmarmol Could someone elaborate on the reasons which were speaking against exporting additional metrics?\n. ",
    "gdurr": "I recently met the same issue with these missing values in the stats series. That makes CPU derivatives crazy in grafana.\nDid somone found a particular setting in grafana to skip this empty fields ?\nAre there any plan to make an udpdate on influxDB storage ?\n. @vmarmol. Good news . Thanks for this.\n. ",
    "imrangit": "What is the best way to plot cpu_cumulative_usage in grafana?\nI am using this query:\nselect  derivative(cpu_cumulative_usage) from \"stats\" where $timeFilter and container_name='$Containers' group by time($interval)\nAnd which format should I keep in Left Y axis?\nPlease advise.\n. ",
    "huikang": "@vmarmol \nThis is the CPU plot of a container from influxDB. I found the graph looks different than cAdvisor's UI. I understand the graph is accumulative CPU time. Do you know how can get the data as in cadvisor's UI in influxDB? Thanks.\n\n. @vmarmol Thanks for your comment. The results look better and closer to the output of cadvisor UI.\n\nHowever, the Y-axis is not in percentage. Instead, is it in CPU cycles? Do you have any idea about how to get the output graph with the Y axis as in % unit as in cadvisor UI? Thanks.\n. @vmarmol \nHere is some raw output from \"select derivative(cpu_cumulative_usage) from stats \nwhere time > 1430846420s and time < 1430846488s and container_name='mysql' group by time(5s)\"\n1430846485000   1007085805.3288821\n1430846480000   1379218024.677719\n1430846475000   1437682783.6420352\n1430846470000   1497197523.5402424\nMy machine has 32 cores.\nFor the first point \"1430846485000  1007085805.3288821\", the cpu utilization is \n1437682783 / 1,000,000,000 / 32 = 0.043, which is 4.3% of all the cores. Is my calculation correct? Thanks.\n. @vmarmol \nYes, you are right; the second value is derivative. Thanks for your clarification.\n. ",
    "selvik": "@cbogss @vishh @vmarmol   Looks like this null values issue could be resolved by a new option in InfluxDB 0.9 (https://influxdb.com/docs/v0.9/query_language/data_exploration.html) - \"fill (none)\". This will allow us to skip a row with null values. Since cAdvisor doesn't yet work with 0.9, I can't test. Do you think it'll help ? Another reason why this would be pretty useful for us: https://github.com/google/cadvisor/pull/800. \n. Looking forward to the merge of this in to master. :) Given that the golang client now works with only InfluxDB 0.9, this merge would be very helpful for us! Thank you!\n. @konukhov Thanks for working on this. Looking forward to trying this out when it's in.\n. Looking forward to this feature getting in. Thanks!\n. @vmarmol Would you have any inputs on this? Thanks! \n. @nickvanw @timstclair I forked cadvisor a couple of months ago and have some local changes. I'll rebase with the latest version and try again. Great to know that support is already in! Thank you very much for the responses. \n. ",
    "ztl8702": "Same issue, waiting for answer.\n. I thought reducing the frequency of checking should reduce CPU usage,\nhowever after I set --housekeeping_interval=10s, cAdvisor still seemed to check every second.\n. ",
    "camerondavison": "i think that it would be nice to have a flag that will disable the disk checks for containers if anything just for debugging \n. FYI. I just ran strace on the 'du' that is running, and realized that the du for the cadvisor container disk usage itself is running over every single docker container for the entire system.\nlstat(\"/var/lib/docker/overlay/e14e4f15e1673095fa6562703280ac11a85d8c0e58cbf469199ed6d29249b130/merged/rootfs/var/lib/docker/overlay/e1b4d58b9cae8953890ffdd493d1f84e751dc0b2b2649d2772b0354d5ced6a10/\nthis seems undesirable to me.\nedit: with some looking into it. looks like the combination of \n--volume=/:/rootfs:ro\n--volume=/var/lib/docker/:/var/lib/docker:ro\nis causing problems\n$ id=$(docker run -d --volume=/:/rootfs:ro --volume=/var/lib/docker/:/var/lib/docker:ro alpine sleep 1000)\n$ docker exec $id sh -c \"ls /rootfs/var/lib/docker/overlay/$id/merged/var/lib/docker\" | wc -l\n0\n$ docker exec $id sh -c \"ls /var/lib/docker/overlay/$id/merged/rootfs/var/lib/docker/overlay\" | wc -l\n143\n. I am running this on coreos so there are a ton of services. Not seeing 100% CPU but high CPU. \n\n. I noticed that at least 1 of my machines has been running a 'du -s' on the overlay fs for cAdvisor that seems to be taking a long time. \nps aux output\nroot      9875 29.2  1.9  81824 80908 ?        DN   16:27  41:27 du -s /var/lib/docker/overlay/8c4ffb38e76761b9fa5c29bafa272ea3e8d7509716fc5b0c8c3ace98f6ec998e\n. It is not a concern, I guess I am just trying to help to identify the problem. I just thought that it was interesting that it had a TIME of 41:27\n. Any idea when this will be released, and/or if there is a good work around for the mean time?\n. Would be nice to have this added to https://github.com/google/cadvisor/blob/master/docs/runtime_options.md\n. ",
    "jeremyeder": "@timothysc something to watch for...\n. In fact we're going to be bind-mounting in lots of stuff to enable super privileged containers running software-defined-storage daemon pods scheduled by kube. I bet we are going to hit this in the next few weeks :/\n/cc @ekuric \n. ",
    "beorn7": "@vmarmol As a minimal example for a program that scrapes a Prometheus client and does something with it, look at https://github.com/prometheus/prom2json (which reads protobuf and/or text format and spits out a similarly structured JSON - meant for people who like to use JSON query tool to process Prometheus metrics). Interestingly, it doesn't use the extraction library at all. It stays on the protobuf level, or (for the text format) it uses the text format parser, which spits out the Go protobuf objects, too. The extraction library goes one step further and converts the protobuf objects into the objects used internally by Prometheus (namely model.Samples), which might or might not be what you want.\n@vishh Push vs. pull is one of those loaded discussions. See our co-developer Brian Brazil's nice take on it. We'd really like to not mix up the two concepts (as that might easily end up in a 'worst of both worlds' situation). We might have some kind of bulk sample upload at some point, but that would be for backfilling purposes and not for (near) real-time monitoring. The bridge between the push and pull world is the Pushgateway, but it's really meant as a last resort if you really can't expose metrics with a pull semantics at all.\n. \\o/ And there was much rejoice...\n. @timpoultney CPU seconds are counted by container_cpu_user_seconds_total. if you take the rate of that over the desired time window, let's say 5min, you get relative CPU usage. That times 100 is percentage:\nrate(container_cpu_user_seconds_total[5m]) * 100\n. General remark about usage of client_golang: If you have metric vec's that you are resetting and filling on each scrape, it should also work to just keep the Desc's around (and return them in Describe()) and then use NewConstMetric to create a throw-away metric to send to the channel in Collect().\nPerhaps it's also a good strategy to leave it conceptionally as it is now, and I'll try the approach described above in a separate PR (once we know that the metrics collected are sane).\n. NewConstMetric usage looks good to me.\n. Just as an afterthought: The invalid label names could only be exposed because the code \"cheats\" with the metric descriptors. If EnableCollectChecks would have been set, the scrape would have resulted in an error even without the stricter checks the newer Prometheus server performs.\nThe upcoming revamp of client_golang will always perform those checks. It will also make it more obvious where the \"cheating\" is happening and the programmer is in their own to not create invalid metrics. (The primary goal is to detect all invalid metrics at start-up time, not only at scrape time. But that's not possible if label names are set dynamically.)\n. So yeah, this is fundamentally the same problem as the kube-state-metric issue referenced above.\nI see three possible paths to a solution:\n\nThe \u201ctechnically correct\u201d solution (the best kind of correct ;o): Munge all the container labels into a single Prometheus label with some syntax convention, like @matthiasr suggested above, e.g. `container_labels = \"foo:bar,dings:bums\". This has the problem that you would need quite involved relabeling rules on the Prometheus side to extract the labels you need. And it would need changes of already established procedures around usage of container labels (or the Kubernetes labels from kube-state-metrics).\nThe pragmatic solution: Similar to what I proposed for https://github.com/kubernetes/kube-state-metrics/pull/194 , client_golang could provide a LabelFixingRegistry that auto-adds missing labels with empty strings, i.e. if some metrics have foo=\"bar\" but others don't, it would attach foo=\"\" to those other metrics. A variant on this would be to not provide this tooling in client_golang and ask the user of the package to implement it themself, as already suggested above. I believe the latter would be in line with what @brian-brazil said in https://github.com/kubernetes/kube-state-metrics/pull/194, namely that we should not make it easy for users to create labels with essentially inconsistent label dimensions.\nThe \u201cembrace badness of the world\u201d solution: We could just give up and assume that there will always be label inconsistencies somewhere. Prometheus already deals with that by assuming all missing labels actually do exist but with an empty string as their value. What I suggested in the previous item would, ironically, have exactly the same effect on the metrics stored in Prometheus as just leaving out the labels with empty label values. However, we do require label consistency within a single scrape right now (at least that's what the current registry in client_golang implements). So we needed to give up on that officially. I would still make it an explicit opt-in, i.e. you needed to use a LenientRegistry that would allow label inconsistencies. The downside over the previous bullet point is that we needed to change our contract about exposition. Also, the empty-valued labels created by the solution in the previous bullet points would be a nice marker of where the inconsistencies happen.\n\nLooking forward to feedback. If we go for a solution that required either a LenientRegistry or a LabelFixingRegistry, this tooling needed to be provided in client_golang, which most likely boils down to myself coding it.. OK, nobody likes approach 1. Fair enough. @matthiasr also told me, that was not what he meant. My bad for not reading carefully enough.\n\n\nHowever, when it receives the metrics, Prometheus checks that all metrics in the same family have the same label set, and rejects those that do not.\n\nThis sounds like a bug on the Prometheus side. This should cause the whole scrape to fail, not silently drop metrics. Partial data is to be avoided.\n\nI guess, \u201cPrometheus\u201d above means \u201cthe Prometheus client library\u201d. The default behavior is indeed to fail the whole scrape, but you can explicitly set a continue on error behavior to still serve as many metrics as possible, see https://godoc.org/github.com/prometheus/client_golang/prometheus/promhttp#HandlerErrorHandling .\nI like the approach of a \u2026_info metric with all those container labels instead of assigning them everywhere. That's also how Kubernetes labels are handled. However, this approach is orthogonal to solving the consistency problem (it just reduces it to only one metric family).\nLet's figure out if we want support for this in client_golang at all. If not, you know what to do in cAdvisor, and the same has to happen in kube-state-metrics. I'll document that approach in client_golang then.\nIf we feel we should have support in client_golang, the question would be between LenientRegistry (easy to implement, five lines of code or something) or the LabelFixingRegistry (slightly more complicated). In any case, it would be opt-in with a lot of warning signs attached to it.. cAdvisor devs, how do you feel about implementing the \"fill up with empty-valued labels to reach label consistency\" as done in https://github.com/vladimirvivien/kubernetes/commit/8935d66160f5a53306c914c57f718aad58a8b508 ?\n@brancz as the main https://github.com/kubernetes/kube-state-metrics/ dev, how do you feel about implementing it in parallel?\nJust trying to test the waters if we want/need support in the Prometheus client_golang for that.. After several discussions I had with various people, I came to the conclusion we want to support \"label filling\" within the Prometheus Go client. You can track progress here: https://github.com/prometheus/client_golang/issues/355. You are using this in only one place now. So you could get rid of it. (Or include it in the Desc generation...)\n. The namespace is only one of the options in ...Opts, which are used to construct fully fledged metric objects for Prometheus. Those are meant to be used for \"native\" Prometheus metrics. What we are doing here is copying existing metrics into Prometheus metrics. For that, we create some of the Prometheus metrics on the fly during scraping, which does not follow the 'high level' path with the Opts object, but assembles the necessary parts one level further down (in this case a metric descriptor is created, which doesn't feature setting a namespace via Opts - so the \"container\" part has to be included in the name).\n. ",
    "juliusv": "@vmarmol AFAIU the container name wouldn't actually add extra dimensionality, just an extra label to every time series? Because there is always only one container name per container ID?\n. And to give an impression of dimensionality thresholds in practice: once a single metric has more than tens of thousands of time series, it becomes unwieldly to work with :) But otherwise, we have Prometheus servers with millions of time series that are working fine. The limit always depends on how many instances you monitor, how many time series per instance, and how big your server is.\n. :+1: It's potentially possible to simplify the metric instantiation/mapping code in the future, but looks good for now.\n. @discordianfish @vmarmol \n. Oh wait, I squashed some followup fixes into the wrong commit - will fix in a second.\n. Ok, should be all good now.\n. @vmarmol @discordianfish \n. @discordianfish @vmarmol \n. This fixes #592 for me and looks good, thanks! :+1:\n. Does type indicate the state? Should it be named state then?\n. Should this and some of the counters above also start with \"Cumulative count of...\", like the counters below? That makes it clear to the user that they are indeed cumulative and not over some arbitrary period of time.\n. I extracted this anonymous struct into a non-anonymous one so that it's possible to initialize it inline.\n. There was a bug here where all metrics of this section where outputted as counters, even though some were gauges.\n. ",
    "timgp": "This is a cool feature, and I will admit to being n00b, but how do I view cpu in prometheus as a percentage based on metrics sent by cAdvisor?\n. Extremely helpful, thank you. \n. +1 here.\n. ",
    "michaeljs1990": "Super sorry to hijack this thread but it's the only place I could find any information on this. @beorn7 Would the CPU usage you wrote out be for just one core? In other words on a 10 core machine would i need to divide my 10 in order to get the actual CPU usage.\n. ",
    "mmcelroy1982": "Does container_cpu_user_seconds_total monitor the CPU usage at the VM level, or is this monitoring the CPU use within the containers?  Based on the output when I setup this metric, it appears to be monitoring the CPU use at the container level.  \nIs there a metric that monitors the CPU use of the diego cells?  . ",
    "rbanks74": "Ok, thanks.  Do you recommend any logging tools I could use in conjunction with cadvisor?  Or are there any ways to extract functionality of cadvisor to my own personal ui?\n. Ok thanks.\n. I see the issue, I'm running 0.9 :) .  But the filtering would be a nice touch too.  Thanks!\n. ",
    "knodir": "Thanks for the suggestion! Followed the steps, but still get the same error.\nI think StartTransientUnit() of dbus type is being called incorrectly. The current implementation of the dbus.Conn.StartTransientUnit() requires 4 arguments \nfunc (c *Conn) StartTransientUnit(name string, mode string, properties []Property, ch chan<- string) (int, error)\nBut libcontainer's apply_systemd.go is passing 2 arguments on line #64, 3 arguments on line #76, and varagrs on line #145\nI am pretty sure cAdvisor is calling the old version of StartTransientUnit(), which was as following:\nfunc (c *Conn) StartTransientUnit(name string, mode string, properties ...Property) (string, error)\nChange history shows it was rewritten as the former one at \"dbus: block on jobs only if provided non-nil channel\" commit by bcwaldon on Sep 25, 2014. \nIs there a quick way to fix this? I don't want to do a workaround and be incompatible with cAdvisor master. \nBtw: I suspected go1.4 version I am using and Docker using go1.4.1 might have something to do with these. But still get the same error after updating both to go1.4.1 Just in case, here is the output:\n$ go version\ngo version go1.4.1 linux/amd64\n$ docker version\nClient version: 1.5.0\nClient API version: 1.17\nGo version (client): go1.4.1\nGit commit (client): a8a31ef\nOS/Arch (client): linux/amd64\nServer version: 1.5.0\nServer API version: 1.17\nGo version (server): go1.4.1\nGit commit (server): a8a31ef\nThanks,\nNodir.\n. I get godep: No Godeps found (or in any parent directory) when I do godep go build github.com/google/cadvisor from $HOME.\nThen I manually got into the folder by cd $GOPATH/src/github.com/google/cadvisor and run godep go build ./ It run for 5-6 seconds and did not have any output. I thought \"no output\" means build is successful. Isn't it the case?\n. Yes, I do get cAdvisor binary on CWD. But, I am not looking for a Docker image. I do have cAdvisor  running with other Dockers, and I am trying to retrieve the status information via Go client. \nI'm adopting code from cadvisor/pages/containers.go to retrieve current CPU, RAM, and other information. Code is definitely not complete, but I think I should be able to import github.com/google/cadvisor/manager to do something meaningful. Here is my code:\n``` go\npackage main\nimport (\"fmt\"\n    \"os\"\n    \"github.com/google/cadvisor/pages\"\n    \"github.com/google/cadvisor/manager\"\n)\nfunc checkFail(msg string, err error, terminate bool) {\n    if err != nil {\n        fmt.Printf(\"%s: %s\\n\", msg, err)\n        if terminate {\n            os.Exit(0)\n        }\n    }\n} \nfunc main() {\n    containerName := \"LONG_IMAGE_ID\"\n    request := info.ContainerInfoRequest{2}\ncont, err := manager.Manager.GetContainerInfo(containerName, &request)\ncheckFail(\"Failed to get container\", err, false)\n\ndisplayName := getContainerDisplayName(cont.ContainerReference)\nfmt.Println(\"\\ndisplayName = \", displayName)\n\n}\n```\nWhich gives me following error message on $ go run monitor.go\n``` error\ngithub.com/docker/libcontainer/cgroups/systemd\nsrc/github.com/docker/libcontainer/cgroups/systemd/apply_systemd.go:64: not enough arguments in call to theConn.StartTransientUnit\nsrc/github.com/docker/libcontainer/cgroups/systemd/apply_systemd.go:76: not enough arguments in call to theConn.StartTransientUnit\nsrc/github.com/docker/libcontainer/cgroups/systemd/apply_systemd.go:145: not enough arguments in call to theConn.StartTransientUnit\n``\n. Okay, it seems I quickly got distracted with nice looking web UI, and did not look at the data structures available atinfo/container.go. I thinkinfo/container.gosuits my needs better thanpages/container.go. I played withContainerInfo,ContainerStats,ContainerSpecininfo/container.go` and they seem to provide everything I need. Here is the direction I'm headed now.\n``` go\nfunc main() {\n    dockClient, err := client.NewClient(\"http://localhost:8080/\")\n    checkFail(\"Could not create NewClient\", err, false) \n    containerName := \"LONG_IMAGE_ID\"  \n    request := info.ContainerInfoRequest{2}\n    sInfo, err := dockClient.ContainerInfo(\"/docker/\"+containerName, &request)\n    checkFail(\"Could not get container info\", err, false)\nfmt.Println(\"Name = \", sInfo.Name)\nfmt.Println(\"Aliases = \", sInfo.Aliases)\nspec := sInfo.Spec\nfmt.Println(\"HasCpu = \", spec.HasCpu)\nfmt.Println(\"nCpu = \", spec.Cpu)\n\nstats := sInfo.Stats\nfmt.Println(\"Timestamp = \", stats[0].Timestamp)\nfmt.Println(\"CpuStats = \", stats[0].Cpu)\n\n}\n```\nWhich outputs following:\ntext\nName =  /docker/LONG_IMAGE_ID\nAliases =  [trusting_meitner LONG_IMAGE_ID]\nHasCpu =  true\nnCpu =  {1024 0 0-3}\nTimestamp =  2015-02-16 01:41:58.741376674 +0000 UTC\nCpuStats =  {{15846157 [6243279 1400997 3806319 4395562] 0 0} 0}\nThis is great, and cAdvisor is awesome. Thanks for the help, and good work on cAdvisor! I'm closing this issue.\nNodir.\n. This worked, thanks @rjnagal!\n. ",
    "pireslaert": "When is that going to be released  and be part of the docker image?\nCheers,\n. great!\nCheers\nSent from my BlackBerry 10 smartphone.\nFrom: Victor Marmol\nSent: Tuesday, 24 February 2015 6:09 PM\nTo: google/cadvisor\nReply To: google/cadvisor\nCc: pireslaert\nSubject: Re: [cadvisor] v0.9.0 fails to start on Centos6 (#505)\nWe've been waiting to release it as part of 0.10.0 but that has taken longer to get out than expected. We'll build a 0.9.1 with this fix tomorrow.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/google/cadvisor/issues/505#issuecomment-75709519\n. is that part the latest docker image?\n. awesome, working now!\n. ",
    "difro": "@vmarmol I've submitted the CLA with my github id.\n. @vmarmol thanks. pushed another commit. Ping me if you prefer new forced commit update with both commits merged in.\n. merged & force-pushed.\n. same here with CentOS + Docker 1.8.1(devicemapper)\nHad to remove --volume=/:/rootfs:ro && --volume=/var/lib/docker:/var/lib/docker:ro\n. hmm. os.Stat() will follow eth0 symlink and modify f with the symlinked original entry(../../devices/pci0000:00/0000:00:04.0/virtio1/net/eth0) which IS a directory.. no?\n. ",
    "Rob-Johnson": "Thanks for your suggestions, I'll look at whether we can adapt Heapster for what we need.\n. ",
    "tbleiker": "+1 for this feature :smile:\n. ",
    "schmichri": "+1\n. guys I'm trying to achive this and I'm one step from surrendering. I've tried this the zuul proxy \nzuul config: \nzuul:\n  routes:\n      cadvisor:\n      path: /cadvisor1/**\n      url: http://10.0.0.11:9002/ \n      stripPrefix: false\n\nmy cadvisor container is portmapped 10.0.0.11:9002:8080\n\nIt might be also a L8 Problem so I appreciate any hints. \nis there maybe and option to the set for the \"rootdir variable\" in your code?  \n. +1 \nI don't understand it. I can see it in your codings but it's not document and I have the feeling it's not working at all. \nIMHO must have for production environment. . +1. ",
    "cburroughs": "That was fast, thanks!\n. ",
    "samek": "Something is not right and I don't know what is it :( \nOk with the --port flag it starts and it runs but i don't have any access to it.  (no port mapping)\nCONTAINER ID        IMAGE                    COMMAND               CREATED             STATUS              PORTS               NAMES\n46d3ed240800        google/cadvisor:0.10.0   \"/usr/bin/cadvisor\"   37 seconds ago      Up 36 seconds                           mesos-f6565aef-cd98-4839-b2e0-9cf98e08c125   \ncurrent json\n{\n  \"container\": {\n    \"type\": \"DOCKER\",\n    \"docker\": {\n      \"image\": \"google/cadvisor:0.10.0\"\n    },\n    \"volumes\": [\n      {\n        \"containerPath\": \"/rootfs\",\n        \"hostPath\": \"/\",\n        \"mode\": \"RO\"\n      },\n      {\n        \"containerPath\": \"/var/run\",\n        \"hostPath\": \"/var/run\",\n        \"mode\": \"RW\"\n      },\n      {\n        \"containerPath\": \"/sys\",\n        \"hostPath\": \"/sys\",\n        \"mode\": \"RO\"\n      },\n      {\n        \"containerPath\": \"/var/lib/docker\",\n        \"hostPath\": \"/var/lib/docker\",\n        \"mode\": \"RO\"\n      },\n      {\n        \"containerPath\": \"/cgroup\",\n        \"hostPath\": \"/cgroup\",\n        \"mode\": \"RO\"\n      }\n    ],\n    \"network\": \"BRIDGE\",\n    \"portMappings\": [\n        { \"containerPort\": 8833, \"hostPort\": 0, \"protocol\": \"tcp\" }\n    ]\n  },\n  \"id\": \"cadvisor-beta3\",\n  \"instances\": 1,\n  \"arg\": [\"--port=8833\"],\n  \"cpus\": 0.5,\n  \"mem\": 512,\n  \"constraints\": [\n    [\n      \"hostname\",\n      \"UNIQUE\"\n    ]\n  ]\n}\nbellow is an simple nginx json for marathon. \n(if you define  hostport 0 then marathon will pick one), and will work on hosts with 80 already binded. \n\n{\n \"container\": {\n   \"type\": \"DOCKER\",\n   \"docker\": {\n      \"image\": \"nginx\",\n     \"network\": \"BRIDGE\",\n     \"portMappings\": [\n       { \"containerPort\": 80, \"hostPort\": 0, \"servicePort\": 80, \"protocol\": \"tcp\" }\n     ]\n   }\n },\n \"id\": \"nginx\",\n \"instances\": 1,\n \"cpus\": 0.25,\n \"mem\": 256,\n \"uris\": []\n}\nand the output: \nCONTAINER ID        IMAGE                    COMMAND                CREATED             STATUS              PORTS                            NAMES\nf51d2957b812        nginx:latest             \"nginx -g 'daemon of   6 seconds ago       Up 5 seconds        443/tcp, 0.0.0.0:31690->80/tcp   mesos-b87176d4-4b07-4e0a-bf83-e65c4f05630f\nd37272ab373b        nginx:latest             \"nginx -g 'daemon of   12 seconds ago      Up 11 seconds       443/tcp, 0.0.0.0:31000->80/tcp   mesos-10cbfabd-1a43-4f91-a387-aa8eb8bf023c \nWell it's 1am - If i manage do set it up tomorrow I'll let you know the solution. \n. Works like a charm. \nmy json was wrong, network and portmappings was outside of docker .....\n\n{\n  \"container\": {\n    \"type\": \"DOCKER\",\n    \"docker\": {\n      \"image\": \"google/cadvisor:0.10.0\",\n      \"network\": \"BRIDGE\",\n      \"portMappings\": [\n        {\n          \"containerPort\": 8080,\n          \"hostPort\": 0,\n\"protocol\": \"tcp\"\n    }\n  ]\n},\n\"volumes\": [\n  {\n    \"containerPath\": \"/rootfs\",\n    \"hostPath\": \"/\",\n    \"mode\": \"RO\"\n  },\n  {\n    \"containerPath\": \"/var/run\",\n    \"hostPath\": \"/var/run\",\n    \"mode\": \"RW\"\n  },\n  {\n    \"containerPath\": \"/sys\",\n    \"hostPath\": \"/sys\",\n    \"mode\": \"RO\"\n  },\n  {\n    \"containerPath\": \"/var/lib/docker\",\n    \"hostPath\": \"/var/lib/docker\",\n    \"mode\": \"RO\"\n  },\n  {\n    \"containerPath\": \"/cgroup\",\n    \"hostPath\": \"/cgroup\",\n    \"mode\": \"RO\"\n  }\n]\n},\n  \"id\": \"cadvisor-beta\",\n  \"instances\": 1,\n\"cpus\": 0.5,\n  \"mem\": 512,\n  \"constraints\": [\n    [\n      \"hostname\",\n      \"UNIQUE\"\n    ]\n  ]\n}\n. Which version of marathon?\nSent from my iPhone\n\nOn 24 Nov 2015, at 08:21, fenglvming notifications@github.com wrote:\nI wanted to start the lastest version of cAdvisor from marathon but it keeps restarting.\n{\n\"type\": \"DOCKER\",\n\"volumes\": [\n{\n\"containerPath\": \"/rootfs\",\n\"hostPath\": \"/\",\n\"mode\": \"RO\"\n},\n{\n\"containerPath\": \"/var/run\",\n\"hostPath\": \"/var/run\",\n\"mode\": \"RW\"\n},\n{\n\"containerPath\": \"/sys\",\n\"hostPath\": \"/sys\",\n\"mode\": \"RO\"\n},\n{\n\"containerPath\": \"/var/lib/docker\",\n\"hostPath\": \"/var/lib/docker/\",\n\"mode\": \"RO\"\n},\n{\n\"containerPath\": \"/cgroup\",\n\"hostPath\": \"/cgroup\",\n\"mode\": \"RO\"\n}\n],\n\"docker\": {\n\"image\": \"google/cadvisor\",\n\"network\": \"BRIDGE\",\n\"portMappings\": [\n{\n\"containerPort\": 8080,\n\"hostPort\": 0,\n\"servicePort\": 10000,\n\"protocol\": \"tcp\"\n}\n],\n\"privileged\": true,\n\"parameters\": [],\n\"forcePullImage\": false\n}\n}\n\u2014\nReply to this email directly or view it on GitHub.\n. Add /usr/bin/cadvisor to the cmd field and try again.\n\nSent from my iPhone\n\nOn 24 Nov 2015, at 08:26, fenglvming notifications@github.com wrote:\nmarathon : 0.11.1\n\u2014\nReply to this email directly or view it on GitHub.\n. try like that. I just give it a go and it works.  * I did not expose any ports .. \n\nAnyway I told you the wrong cmd .. use /usr/bin/cadvisor -logtostderr\n\nOn Nov 24, 2015, at 8:35 AM, fenglvming notifications@github.com wrote:\nhere is my configure page:\n https://cloud.githubusercontent.com/assets/2157195/11360592/f0310378-92c0-11e5-8aae-7fb591a5ed66.png\n https://cloud.githubusercontent.com/assets/2157195/11360594/f1dcaa2e-92c0-11e5-8a95-12b6971eeb3b.png\n\u2014\nReply to this email directly or view it on GitHub https://github.com/google/cadvisor/issues/537#issuecomment-159183489.\n. Does it work ? \nOn Nov 24, 2015, at 8:44 AM, samo gabrovec samek@me.com wrote:\ntry like that. I just give it a go and it works.  * I did not expose any ports .. \nAnyway I told you the wrong cmd .. use /usr/bin/cadvisor -logtostderr\n\n\nOn Nov 24, 2015, at 8:35 AM, fenglvming notifications@github.com> wrote:\nhere is my configure page:\n https://cloud.githubusercontent.com/assets/2157195/11360592/f0310378-92c0-11e5-8aae-7fb591a5ed66.png\n https://cloud.githubusercontent.com/assets/2157195/11360594/f1dcaa2e-92c0-11e5-8a95-12b6971eeb3b.png\n\u2014\nReply to this email directly or view it on GitHub https://github.com/google/cadvisor/issues/537#issuecomment-159183489.\n. What does the stderr say in mesos ui \ncompleted tasks -> cadvisor Sandbox ? \n\nOn Nov 24, 2015, at 9:27 AM, fenglvming notifications@github.com wrote:\nit didn't work. \n537 (comment) https://github.com/google/cadvisor/issues/537#issuecomment-159191134\n\u2014\nReply to this email directly or view it on GitHub https://github.com/google/cadvisor/issues/537#issuecomment-159193468.\n. @vmarmol  yes I'm not 100% sure but aliases are exported.\nProblem is that  there's no link to the marathon task id/app which is different. \nAnd If you want to autoscale depending on the app which is run by marathon you would also need at least MARATHON_APP_ID to go with it.\n\nAre you guys at all parsing json from the /var/lib/docker/containers/XXXXX/config.json ? \n. @mhausenblas We're not using cadvisor anymore :(\nFor mesos monitoring and task monitoring from marathon we use\nhttps://github.com/bobrik/collectd-docker\nhttps://github.com/bobrik/docker-collectd-mesos\nIt solved all our problems. \nIf you need those I can post them for sure. \n. @scalp42 I've sent it directly to @mhausenblas  since it's not related to cadvisor at all. \nBut sure, I'll just resend the mail to you. \n. @salimane It feals wrong that I'm posting solution in cadvisor page since It doesn't use it. \nanyway. \nSo in order to use it: \nOn each mesos-slave run   docker run -d  -e GRAPHITE_HOST=IP_OF_GRAPHITE_HOST -e COLLECTD_HOST=IP_OF_MESOS_SLAVE_WITH_UNDERSCORES bobrik/collectd-docker\nFor example If my graphite host is 10.0.0.251 and the mesos slave ip is 10.0.0.193 you would run: \ndocker run -d  -v /var/run/docker.sock:/var/run/docker.sock     -e GRAPHITE_HOST=10.0.0.251 -e COLLECTD_HOST=10_0_0_193   bobrik/collectd-docker\n(I suggest that you run the docker with restart=always) \nThen when defining app in marathon you have to add couple of env vars which are picked by that docker. \nYou have to add COLLECTD_DOCKER_APP, COLLECT_DOCKER_TASK_ENV and COLLECTD_DOCKER_TASK_ENV_TRIM_PREFIX.\nfor example one of our api project looks like this: \n{\n \"container\": {\n   \"type\": \"DOCKER\",\n   \"docker\": {\n      \"image\": \"10.0.0.48:5000/spored-api:v6\",\n     \"network\": \"BRIDGE\",\n     \"portMappings\": [\n       { \"containerPort\": 80, \"hostPort\": 0, \"servicePort\": 8885, \"protocol\": \"tcp\" }\n     ]\n   },\n    \"volumes\": [\n      {\n        \"containerPath\": \"/var/log/nginx\",\n        \"hostPath\": \"/var/log/dockerlogs/nginx\",\n        \"mode\": \"RW\"\n      }\n    ]\n },\n \"id\": \"spored-api\",\n \"cpus\": 0.5,\n \"mem\": 500,\n \"env\":  {\"COLLECTD_DOCKER_APP\":\"spored-api\", \"COLLECTD_DOCKER_TASK_ENV\":\"MESOS_TASK_ID\", \"COLLECTD_DOCKER_TASK_ENV_TRIM_PREFIX\":\"spored-api\"},\n \"constraints\": [\n    [\"env\", \"CLUSTER\", \"live\"]\n  ],\n\"upgradeStrategy\": {\n        \"minimumHealthCapacity\": 0.5,\n        \"maximumOverCapacity\": 0.8\n    },\n \"healthChecks\": [\n    {\n      \"protocol\": \"HTTP\",\n      \"portIndex\": 0,\n      \"path\": \"/\",\n      \"gracePeriodSeconds\": 60,\n      \"intervalSeconds\": 20,\n      \"maxConsecutiveFailures\": 6\n    }\n  ]\n}\nNow When you go to grafana (load the template which is available on githhub) and you should be able to pick stats by app. \n\n. ",
    "fenglvming": "I wanted to start the lastest version of cAdvisor from marathon but it keeps restarting.\n{\n  \"type\": \"DOCKER\",\n  \"volumes\": [\n    {\n      \"containerPath\": \"/rootfs\",\n      \"hostPath\": \"/\",\n      \"mode\": \"RO\"\n    },\n    {\n      \"containerPath\": \"/var/run\",\n      \"hostPath\": \"/var/run\",\n      \"mode\": \"RW\"\n    },\n    {\n      \"containerPath\": \"/sys\",\n      \"hostPath\": \"/sys\",\n      \"mode\": \"RO\"\n    },\n    {\n      \"containerPath\": \"/var/lib/docker\",\n      \"hostPath\": \"/var/lib/docker/\",\n      \"mode\": \"RO\"\n    },\n    {\n      \"containerPath\": \"/cgroup\",\n      \"hostPath\": \"/cgroup\",\n      \"mode\": \"RO\"\n    }\n  ],\n  \"docker\": {\n    \"image\": \"google/cadvisor\",\n    \"network\": \"BRIDGE\",\n    \"portMappings\": [\n      {\n        \"containerPort\": 8080,\n        \"hostPort\": 0,\n        \"servicePort\": 10000,\n        \"protocol\": \"tcp\"\n      }\n    ],\n    \"privileged\": true,\n    \"parameters\": [],\n    \"forcePullImage\": false\n  }\n}\n. marathon : 0.11.1\nThe output of mesos's stdout  file is : No such file or directory\n. still   : No such file or directory\n. here is my configure page:\n\n\n. it didn't work. \nhttps://github.com/google/cadvisor/issues/537#issuecomment-159191134\n. I think it's my mesos-slave configuration problem. I just use --containerizers=docker,mesos ,It works.\nBefore that I use --containerizers=mesos,docker . It's werid. \n. ",
    "DreadPirateShawn": "Thanks -- this is in progress. I actually am engaging with cAdvisor as part of a company project, I just saw the typo before getting into any \"real\" work. So rather than accepting the CLA personally, I've sent this to my boss, and we're going through some minor internal hoops prior to accepting on behalf of the company. I'll eventually have approval to remove the \"r\" from \"oftern\"... :-P\n. ...and done! CLA has been signed and filed, corporation name is \"Skytap Inc.\"\n. Fair enough. If it helps, it was signed / submitted / confirmed this past Friday (March 20, 2015) -- we got an email confirmation on the same day from \"Google echosign@echosign.com\" with the CLA attached, subject \"Google Corporate CLA between Google and Brad Schick is Signed and Filed!\", cc'ed to \"Google CLA Submissions cla-submissions@google.com\". Not sure if there are any additional internal steps pending in Google's processing pipeline.\n. ",
    "brian-brazil": "\nname is aliases joined by ',' and definitely not what we want: Ideas to solve that? We can:\n\nWhat I'd do is have an additional metric called 'names', with a label called 'name' and a timeseries for each name with value 1.\n\nUsage is in seconds, not percent. Not sure what's better\n\nSeconds is what you want as you should leave the math to prometheus\n. > I don't think we want such 'map metric' in this case. The docker container name in aliases is what you are usually use when refering to a container. Always through through that map sounds painful.\nIt sounds like the names/aliases aren't static, so it'd not wise to use them as label values on everything as every change to them will break your graphs. Having all the aliases may be usable for particular use cases such as Docker but not work for the general case.\nThis incidentally is where the more advanced label features and console templates come into play.\n. > today we only ever have two so we can split by \"id\", \"name\", and \"alias\" unless there is a better solution.\nIs that the case for every potential user of this code?\nPrometheus as of today has the features to work with a single timeseries that has all the potentially useful labels.\n. > I'm not sure I understand your second sentence?\nI'd propose to have metrics such as:\ncontainer_names{id=\"myid\",name=\"name1\"} 1\ncontainer_names{id=\"myid\",name=\"name2\"} 1\ncontainer_names{id=\"myid\",name=\"name3\"} 1\nother_metrics{id=\"myid\"} 7\nUsing the group_left/group_right modifiers you can join the names on to the metrics when you need the names, without the disadvantages of dealing with names changing over time.\n. > I don't think the names really change over time. Theoretically the alias can, although I haven't seen it in practice.\nOkay, sounds like we can get away with the name as a label on everything and an alias as I suggest.\n. > But I assume that's okay, isn't it?:\nIt's not okay. You want your timeseries to have the same labels over their lifetime, you'll run into all sorts of problems if this is not the case.\n. > The timeseries, for example 'memory usage of container X' will have the same labels for the lifetime of 'container X'. But if you, for example, delete and recreate the container with the same name etc the id and therfor the label changes. I assume this is okay because it's in fact a new container.\nThat's fine, and what you want.\nThe issue arises if the labels change during the lifetime of container X, such as if the name or aliases change.\n. > However, when it receives the metrics, Prometheus checks that all metrics in the same family have the same label set, and rejects those that do not.\nThis sounds like a bug on the Prometheus side. This should cause the whole scrape to fail, not silently drop metrics. Partial data is to be avoided.\n\nMunge all the container labels into a single Prometheus label with some syntax convention, like @matthiasr suggested above, e.g. `container_labels = \"foo:bar,dings:bums\"\n\nI don't think that's a great idea, labels should be represented as labels and we generally try to dissuade users from building up structure inside label values. Having non-trivial relabelling rules doesn't really help anyone.\n\nA variant on this would be to not provide this tooling in client_golang and ask the user of the package to implement it themself, as already suggested above. \n\nYes, this is what I'd go for. It shouldn't be too many lines of code. Likely most of these labels should also be moved to an per-container _info metric rather than being on all time series.\n\nI believe the latter would be in line with what @brian-brazil said in kubernetes/kube-state-metrics#194, namely that we should not make it easy for users to create labels with essentially inconsistent label dimensions.\n\nYes, the client library guidelines are very clear about not allowing this for direct instrumentation, so it'd be in the spirit of the guidelines not to allow this given that Go already does label consistency checks. The Go client is the only client currently checking for this sort of inconsistency, though with the 2.0 scrape parser being laxer I can see other clients starting to have some checks to make up for that.. > I guess, \u201cPrometheus\u201d above means \u201cthe Prometheus client library\u201d. The default behavior is indeed to fail the whole scrape, but you can explicitly set a continue on error behavior to still serve as many metrics as possible, see \nYes, that's what I meant. In that case it's a Cadvisor bug that it sets ContinueOnError rather than using the default HTTPErrorOnError as that was hiding this problem. This was introduced in #1679.\n\nHowever, this approach is orthogonal to solving the consistency problem (it just reduces it to only one metric family).\n\nAgreed.\n\nLet's figure out if we want support for this in client_golang at all.\n\nI would say no, there are only two use cases so far which I don't think is enough. Even with warning signs users will use it where it doesn't apply, just like ContinueOnError was used here to paper a over problem rather than fixing it.\nI've implemented related code in the past by hand, it's not particularly complicated to write. It's standard data munging.\n\nIf we feel we should have support in client_golang, the question would be between LenientRegistry (easy to implement, five lines of code or something\n\nIf we go for it I'd go for this, but it'd feel weird that there'd now be the default registry settings, the lenient registry and the pedantic registry.. I've looked into this, and there looks to be a simpler solution.\nI believe that using the approach at https://github.com/kubernetes/kubernetes/pull/51473 in Cadvisor would be sufficient to resolve the issue here. That is in DefaultContainerLabels produce an empty string for the missing labels.\nIs there something I'm missing?. Ah, I see. It's the container.Spec.Labels and container.Spec.Envs which need extra handling.. I've put together https://github.com/google/cadvisor/pull/1831 which I believe will fix this.. I signed it!. This should all be one metric I think, as it sounds like a partition on the tasks\n. This sounds like a gauge\n. I'd only have one cpu metric, and split it out by cpu.\nYou shouldn't export multiple versions of a metric with different labels unless you're running into performance problems.\n. s/hits/hit/\n. transmitted, and below\n. I think you'd be better off creating the metrics afresh each time. It avoids locking too.\n. Usually this is done as a gauge that's 1/0 depending on whether this particular scrape worked.\n. Yes.\nI'm used to the other clients where there is no Describe() :)\n. Why do you expect them to be less accurate when broken down? If is this is the same sort of cpu stat as in /proc/stat I'd expect it to all add up.\n. > These are also in nanoseconds\nThey're converted down in line 385\n. Removes all samples from that metric.\n. tasks_state sounds like a good name.\nhttp://prometheus.io/docs/practices/naming/#metric-names has guidelines on when to use labels.\n. Sounds like two different metrics then.\nNaming will be fun on those as you want to avoid \"by\" in metric names as a label will not always be present after aggregation.\n. http://prometheus.io/docs/practices/instrumentation/#collectors :)\n. We call this scrape_error elsewhere\n. .Set(1) would be clearer\n. You're missing the type label here, and the names of metrics need to be unique.\nI don't suppose we can just ignore the user vs. system metric?\n. Okay, I never came across a good way to name things like this.\ncontainer_cpu_usageclass_seconds_total maybe?\n. > cpu_usage_type_seconds there are 3 labels: total, user, system no?\nThe label would be 'type' the value would be 'user' or 'system'\nYou shouldn't mix total in with breakdowns, as that breaks aggregation.\n. This needs to be named differently to the above metric, otherwise prometheus won't be able to parse the metrics\n. That'll be confusing when aggregated up, i'd suggest container_cpu_usagetype_seconds_total for the other\n. ",
    "martensson": "Going to join the thread, having the same issue when trying to use cadvisor in combination with Marathon and Mesos.\n. Going to join the thread, having the same issue when trying to use cadvisor in combination with Marathon and Mesos.\n. ",
    "BrickXu": "+1 \n. I think you should send the metrics to the buffer, like MQ or DB, then use ELK consume it later. Or maybe lost some metrics when the backend(ELK) crash!\n. join this thread. \nhi @derekwaynecarr , could you please give the original issue refs ?\n. I think this PR could fix it #1476 , Is this one  @derekwaynecarr  ?\n. ",
    "mhausenblas": "@samek can you pls share your Marathon app spec (JSON) file for cAdvisor?\n. Awesome, thanks @samek \u2014 yes, the Marathon app spec would be appreciated!\n. ",
    "scalp42": "@samek any chance?\n. @jimmidyson  or @vmarmol  Any chance to see this merged now that we have to whitelist the env vars?\n. cc @Marmelatze :sake: \n. @Marmelatze any chance ? :rotating_light: \n. @jmaitrehenry do you know if there's a way to use a Docker env variable as the docker container name ? Kinda like what was done with Prometheus backend: https://github.com/google/cadvisor/issues/546\nWhen running in a Mesos environment, containers are just IDs:\n4f7c0aa8d03a        java:8                   \"/bin/sh -c 'JAVA_OPT\"   4 hours ago         Up 4 hours          0.0.0.0:31926->8080/tcp   mesos-29e183be-f611-41b4-824c-2d05b052231b-S6.3dbb1004-5bb8-432f-8fd8-b863bd29341d\n66f2fc8f8056        java:8                   \"/bin/sh -c 'JAVA_OPT\"   4 hours ago         Up 4 hours          0.0.0.0:31939->8080/tcp   mesos-29e183be-f611-41b4-824c-2d05b052231b-S6.60972150-b2b1-45d8-8a55-d63e81b8372a\nf7382f241fce        java:8                   \"/bin/sh -c 'JAVA_OPT\"   4 hours ago         Up 4 hours          0.0.0.0:31656->8080/tcp   mesos-29e183be-f611-41b4-824c-2d05b052231b-S6.39731a2f-d29e-48d1-9927-34ab8c5f557d\n880934c0049e        java:8                   \"/bin/sh -c 'JAVA_OPT\"   23 hours ago        Up 23 hours         0.0.0.0:31371->8080/tcp   mesos-29e183be-f611-41b4-824c-2d05b052231b-S6.23dfe408-ab8f-40be-bf6f-ce27fe885ee0\n5eab1f8dac4a        java:8                   \"/bin/sh -c 'JAVA_OPT\"   44 hours ago        Up 44 hours         0.0.0.0:31500->8080/tcp   mesos-29e183be-f611-41b4-824c-2d05b052231b-S6.5ac75198-283f-4349-a220-9e9645b313e7\nb63740fe56e7        java:8                   \"/bin/sh -c 'JAVA_OPT\"   44 hours ago        Up 44 hours         0.0.0.0:31382->8080/tcp   mesos-29e183be-f611-41b4-824c-2d05b052231b-S6.5d417f16-df24-49d5-a5b0-38a7966460fe\n5c7a9ea77b0e        java:8                   \"/bin/sh -c 'JAVA_OPT\"   2 days ago          Up 2 days           0.0.0.0:31186->8080/tcp   mesos-29e183be-f611-41b4-824c-2d05b052231b-S6.b05043c5-44fc-40bf-aea2-10354e8f5ab4\n53065e7a31ad        java:8                   \"/bin/sh -c 'JAVA_OPT\"   2 days ago          Up 2 days           0.0.0.0:31839->8080/tcp   mesos-29e183be-f611-41b4-824c-2d05b052231b-S6.f0a3f4c5-ecdb-4f97-bede-d744feda670c\nYou can pass Docker environment variables though, so you could technically inspect the container to grab a certain variable to use as name. Thoughts?\n. I did, but I can't find how you can use it in the statsd backend unless I'm mistaken the storage backend need to support it no?\n. ",
    "salimane": "@samek the marathon app spec could be put in a gist on github somewhere and it would be appreciated :)\n. @samek thanks :+1: \n. hmm yeah but I want to be able to set it while running the docker container. Here is the cadvisor Dockerfile https://github.com/google/cadvisor/blob/master/deploy/Dockerfile . The exposed port 8080 is hardcoded.\nThanks\n. I want to run the docker image with the following options docker run --net=host, which basically means bind your ports as if you were the host. But let's say you already have an app running on port 8080, then you can't really do that.\n. hmm... ok i wil see what i can do\n. ",
    "sjkyspa": "Any update? Is the feature support now?\n. ",
    "rikwasmus": "We lazily fixed it in a fork by abusing the 'exposedenv' system, as I was not going to fix the difference between outputs getting either ContainerReference or ContainerInfo objects. This is most surely not the way to do it properly, hence no merge request, but it might help some folks.\n```\ndiff --git a/container/docker/handler.go b/container/docker/handler.go\nindex dd0a2cd..11bcf91 100644\n--- a/container/docker/handler.go\n+++ b/container/docker/handler.go\n@@ -257,6 +257,14 @@ func newDockerContainerHandler(\n                                if len(splits) == 2 && splits[0] == exposedEnv {\n                                        handler.envs[strings.ToLower(exposedEnv)] = splits[1]\n                                }\n+                               // Add exposed environments as labels to enable them in all outputs\n+                               // Ideally, the outputs would handle it themselves, however, the \n+                               // difference between a propagated ContainerReference or ContainerInfo\n+                               // is harder to fix, and this is easier for now. This means the outputs\n+                               // do not differentiate between labels and exposed environmental vars,\n+                               // and environmental vars with the same name can possibly overwrite \n+                               // labels: this can be seen as a feature.\n+                               handler.labels[strings.ToLower(exposedEnv)] = splits[1]\n                        }\n                }\n        }\n```. Running into it as well. Pull request is already there: https://github.com/google/cadvisor/pull/1063\n. ",
    "nstott": "Thanks, I was confused by the docker message, I'll close this.\nI think this is a problem with the cgroup mounts within the container.\nE0228 02:18:32.494433 00099 cadvisor.go:81] Docker registration failed: unable to communicate with docker daemon: dial unix /var/run/docker.sock: no such file or directory.\nF0228 02:18:32.495640 00099 cadvisor.go:86] Raw registration failed: failed to get cgroup subsystems: failed to find cgroup mounts.\ngoroutine 1 [running]:\ngithub.com/golang/glog.stacks(0x4c20800b800, 0x0, 0x0, 0x0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:726 +0xcd\ngithub.com/golang/glog.(*loggingT).output(0xf1c4c0, 0x4c200000003, 0x4c2080ee240)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:677 +0x24c\ngithub.com/golang/glog.(*loggingT).printf(0xf1c4c0, 0x3, 0x9d7230, 0x1c, 0x4c208213f38, 0x1, 0x1)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:635 +0x19c\ngithub.com/golang/glog.Fatalf(0x9d7230, 0x1c, 0x4c208213f38, 0x1, 0x1)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:1033 +0x64\nmain.main()\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/cadvisor.go:86 +0x833\n. ",
    "jhunthrop": "Did you ever resolve the issue of the cgroup mounts within the lxc container? I'm working through running cadvisor inside an lxc-container to provide metrics for all containers running on the host machine, and I am hitting the same issue.\n. ",
    "simon3z": "@vmarmol I am covered by the Red Hat CLA as far as I know, I commit to kubernetes already (still it seems that the bot has a problem recognizing me)\n. @vmarmol I thought that cAdvisor was pulling the data from the backends to expose it through its api, but from your description it seems the other way around: cAdvisor providing metrics for collectd/pcp.\nIf that's the case (cAdvisor providing metrics for collectd/pcp) does it mean that cAdvisor will be focused/specialized only on container metrics?\n. @vmarmol I think it would be beneficial is someone can explain what this meant:\n\n\nUse collectd\ncAdvisor plugin for collectd is still a WIP\n\n\nFrom: https://github.com/GoogleCloudPlatform/kubernetes/issues/990\n/cc @vishh @lavalamp @thockin\nWhat was the idea there?\nWhat was the cAdvisor plugin for collectd going to do? Get metrics from collectd?\n. Actually I looked into this as well and I couldn't spot anything better in the amazon documentation and api. (E.g. I heard someone is checking the mac address of the network interface, which seems even more fragile).\nSo for me at the moment is :+1: \ncc @vishh \n. > One alternative: have a fast-path, and then slower but more complete fallback logic. So dmi.product_version=amazon => AWS, dmi.product_name=Google => GCE etc, but if we don't identify any of them then we go through the official procedure.\n@justinsb let's assume the fast-path fails and you fallback to the official procedure... you would still hit this bug (identifying an OpenStack deployment as AWS).\nThe fail-safe addition of the slow-path that you're suggesting would just produce the same wrong results we have today.\n. @vishh I have no news since my last https://github.com/google/cadvisor/pull/1171#issuecomment-199247868\n. Can this take a long time? (e.g. is it polling ip addresses that may not be always available?)\nWould that be a problem? Should this be async?\n. @enoodle ok :+1: \n. @enoodle is this a stable check? Is it documented?\n. ",
    "fche": "Is the medium-term goal to have data collectors that are peers to cadvisor send data to the same storage backend?  Or to have cadvisor participate in that pipeline as an intermediary?\n. Could you by any chance point this k8s/cadvisor neophyte at the appropriate integration interfaces as to how to be a peer/sibling to cadvisor?   heapster polling cadvisor -and- other providers, pushing data to influxdb?\n. @vmarmol , no, that's a separate thing.  We want to supplement the data from cadvisor, not consume data from it.\n. @rjangal, can you confirm this advice?  i.e., to bypass cadvisor for collectd/pcp-type data supplementation?\n. ",
    "mkjaer": "I signed the CLA now\n. ",
    "yeasy": "Any progress on this?\nSeems the latest cadvisor docker image still requires lots of /static resources.\n. ",
    "smarterclayton": "I have a CLA under Red Hat.\n. Thank you - cAdvisor gives me such pretty numbers to show everyone.  :)\n----- Original Message -----\n\nLGTM, will wait for green. Thanks @smarterclayton\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/google/cadvisor/pull/603#issuecomment-83771382\n. Nm, I wasn't mounting in the right things.\n. Nm, still happening :)\n. Ah, sorry.  Jumped the gun.  It's not in Kube but we're enabling it in OpenShift and testing it.  Not a high priority fix.\n. Will move this to Kube\n. Moved to Kube\n. If that's truly required that's going to be a problem for containerizing\nthe Kubelet - do we want the kubelet in the parent PID namespace?  Can\nyou provide more info about why that is necessary (or tell me to read\nupthread if I just missed it)?\n\nOn Fri, Aug 28, 2015 at 11:08 AM, Jimmi Dyson notifications@github.com\nwrote:\n\n@lukaf https://github.com/lukaf Need to update documentation. When\nrunning inside a container it has to run inside the pid namespace of the\nhost (--pid=host). So try:\ndocker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro  --volume=/var/lib/docker/:/var/lib/docker:ro --publish=8080:8080 --detach=true --pid=host --name=cadvisor test/cadvisor\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/822#issuecomment-135783362.\n\n\nClayton Coleman | Lead Engineer, OpenShift\n. If we start kube-proxy in a container it'll have to be in host netns - in\npractice we run containerized kubelet in host netns for simplicity of\nconnection.\nOn Fri, Aug 28, 2015 at 12:27 PM, Rohit Jnagal notifications@github.com\nwrote:\n\nWe already have root filesystem mapped in as a volume on /rootfs (so we use\n/rootfs/proc//root/sys/class/net). its used in many places within\ncadvisor.\nThe netlink comment isn't relevant for networking stats. There's netlink\ncommand to get taskstats which gives us cpuload per container. It only\nworks when cadvisor is running under root netns, so we have it disabled for\nnow.\nOn Fri, Aug 28, 2015 at 8:19 AM, Jimmi Dyson notifications@github.com\nwrote:\n\n/proc//root/sys/class/net/ won't be available unless we're running\nhost pid namespace anyway so it's no better/safer when running in a\ncontainer. Mounting proc will screw the cadvisor container up, if it's\neven possible. Could mount host /proc somewhere else in the container I\nguess but that doesn't feel nice to do.\nWhat do you mean \"via netlink\"? Perhaps that's an option but don't know\nenough about it.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/822#issuecomment-135803277.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/822#issuecomment-135806485.\n\n\nClayton Coleman | Lead Engineer, OpenShift\n. Pid NS guarantees we cleaned child processes - running in host NS will not\nguarantee that.\nOn Fri, Aug 28, 2015 at 12:31 PM, Jimmi Dyson notifications@github.com\nwrote:\n\n@rjnagal https://github.com/rjnagal If we're mounting the whole rootfs\nanyway what does it matter if we're running in pid ns? Privileged &\nmounting rootfs - pid ns isn't exactly reducing security & makes things\nmore consistent between running natively & running in container by using\nsetns to net ns.\n@smarterclayton https://github.com/smarterclayton Right but that still\ndoesn't allow us to switch to other docker containers' net ns. Only way to\ndo that that I know of is to run in host pid ns.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/822#issuecomment-135807467.\n\n\nClayton Coleman | Lead Engineer, OpenShift\n. When you're in a PID namespace you become PID 1 (whatever is docker\ncmd/entrypoint).  If that process dies, the kernel nukes all the children\nspawned by PID 1.  So if the kubelet launches child processes (like exec\ncalls to a network plugin) and the kubelet suddenly dies, when you're in a\nPID namespace the children die, but outside a PID namespace they just\nbecome orphans.  It depends on what the kubelet needs to do w.r.t.\nchildren, but that's the gist.\nOn Fri, Aug 28, 2015 at 12:46 PM, Jimmi Dyson notifications@github.com\nwrote:\n\n@smarterclayton https://github.com/smarterclayton Beyond the limits of\nmy knowledge there :) Could you explain or point me somewhere to read up?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/822#issuecomment-135811137.\n\n\nClayton Coleman | Lead Engineer, OpenShift\n. I1107 21:16:03.727985   14111 machine.go:48] Couldn't collect info from any of the files in \"/etc/machine-id,/var/lib/dbus/machine-id\"\n. The cumulative percentage is low because this is a combined Kube node / master process.  Maybe 20% of the total allocations belong to the node, but 12% of the total belong to libcontainer related code being invoked from cadvisor, so something on the order of 50-70% of steady state allocations in the node side were from libcontainer.\n. Looking at the prometheus exporter, we don't actually ever export DiskIo - so we don't have any of the block data.  We just report the Filesystem data.  DiskIo contains IO related to empty dirs, so it's definitely useful.. @sjenning @derekwaynecarr . I'll take this and put up a provisional PR - blkio stats don't have the device name (just major and minor uids) so stats wouldn't have the same labels.  Going to:\n\nin the handlers set the device name onto PerDiskStats (but not mark it as serializable so the JSON return doesn't change)\nin the prometheus handler merge the appropriate PerDiskStats with the filesystem values using the consistent Device name. container_fs_reads_total{container_name=\"deployment\",device=\"\",id=\"/system.slice/docker-491cec82b501782a8258fb1bb870fa5133f8bc538573693e7c4ef34db13c2378.scope\",image=\"openshift/origin-deployer:v3.6.0-alpha.1\",name=\"k8s_deployment.3512bdfd_docker-registry-1-deploy_default_f596261f-248d-11e7-9470-080027893417_8f752f68\",namespace=\"default\",pod_name=\"docker-registry-1-deploy\"} 1311\ncontainer_fs_reads_total{container_name=\"registry\",device=\"\",id=\"/system.slice/docker-da6260076d47a04536b3a8339ccb40143a6c55c731c98c7a99fb3e32ea5b1f3f.scope\",image=\"openshift/origin-docker-registry:v3.6.0-alpha.1\",name=\"k8s_registry.c5fb07ea_docker-registry-1-g89b9_default_f702f49d-248d-11e7-9470-080027893417_868be8c3\",namespace=\"default\",pod_name=\"docker-registry-1-g89b9\"} 100. Opened a PR in #1642. @derekwaynecarr @ncdc @mrunalp correct me if I got any of that wrong.\n. ```\noc get --raw /metrics | grep fs\n\nHELP container_fs_inodes_free Number of available Inodes\nTYPE container_fs_inodes_free gauge\ncontainer_fs_inodes_free{device=\"/dev/mapper/centos-openshift--xfs--vol--dir\",id=\"/\"} 1.046118e+07\ncontainer_fs_inodes_free{device=\"/dev/mapper/centos-root\",id=\"/\"} 3.0020491e+07\ncontainer_fs_inodes_free{device=\"/dev/sda1\",id=\"/\"} 511656\nHELP container_fs_inodes_total Number of Inodes\nTYPE container_fs_inodes_total gauge\ncontainer_fs_inodes_total{device=\"/dev/mapper/centos-openshift--xfs--vol--dir\",id=\"/\"} 1.0461184e+07\ncontainer_fs_inodes_total{device=\"/dev/mapper/centos-root\",id=\"/\"} 3.0721888e+07\ncontainer_fs_inodes_total{device=\"/dev/sda1\",id=\"/\"} 512000\nHELP container_fs_io_current Number of I/Os currently in progress\nTYPE container_fs_io_current gauge\ncontainer_fs_io_current{device=\"/dev/mapper/centos-openshift--xfs--vol--dir\",id=\"/\"} 0\ncontainer_fs_io_current{device=\"/dev/mapper/centos-root\",id=\"/\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice/NetworkManager.service\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice/atd.service\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice/auditd.service\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice/crond.service\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice/dbus.service\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice/docker-1e8fa84249225e6cd7e4873b0339215d346e924309954f2d3cb3475d3cf93405.scope\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice/docker-6f68e87c197dcc2990731b15a434b237ba784fee3b65f8708196a88f95cce032.scope\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice/docker-bf7e5fce1172be7dd2f4ff7a97847b478920157fae61f86cf44599e431c9f558.scope\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice/docker-containerd.service\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice/docker.service\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice/gssproxy.service\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice/irqbalance.service\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice/lvm2-lvmetad.service\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice/polkit.service\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice/postfix.service\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice/rhel-push-plugin.service\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice/rhsmcertd.service\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice/rsyslog.service\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice/sshd.service\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice/systemd-journald.service\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice/systemd-logind.service\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/system.slice/systemd-udevd.service\"} 0\ncontainer_fs_io_current{device=\"/dev/sda\",id=\"/user.slice\"} 0\ncontainer_fs_io_current{device=\"/dev/sda1\",id=\"/\"} 0\ncontainer_fs_io_current{container_name=\"registry\",device=\"/dev/sda\",id=\"/system.slice/docker-da6260076d47a04536b3a8339ccb40143a6c55c731c98c7a99fb3e32ea5b1f3f.scope\",image=\"openshift/origin-docker-registry:v3.6.0-alpha.1\",name=\"k8s_registry.c5fb07ea_docker-registry-1-g89b9_default_f702f49d-248d-11e7-9470-080027893417_868be8c3\",namespace=\"default\",pod_name=\"docker-registry-1-g89b9\"} 0\nHELP container_fs_io_time_seconds_total Cumulative count of seconds spent doing I/Os\nTYPE container_fs_io_time_seconds_total counter\ncontainer_fs_io_time_seconds_total{device=\"/dev/mapper/centos-openshift--xfs--vol--dir\",id=\"/\"} 0\ncontainer_fs_io_time_seconds_total{device=\"/dev/mapper/centos-root\",id=\"/\"} 0\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/\"} 17468.89820598\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice\"} 3053.64959099\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice/NetworkManager.service\"} 29.678010162\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice/atd.service\"} 0.010909787\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice/auditd.service\"} 1.509095758\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice/crond.service\"} 2.407612592\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice/dbus.service\"} 1.882829536\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice/docker-1e8fa84249225e6cd7e4873b0339215d346e924309954f2d3cb3475d3cf93405.scope\"} 0.025023154\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice/docker-6f68e87c197dcc2990731b15a434b237ba784fee3b65f8708196a88f95cce032.scope\"} 0.066847817\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice/docker-bf7e5fce1172be7dd2f4ff7a97847b478920157fae61f86cf44599e431c9f558.scope\"} 0.096245165\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice/docker-containerd.service\"} 57.734992013\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice/docker.service\"} 16.969229857\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice/gssproxy.service\"} 0.126133858\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice/irqbalance.service\"} 0.46796402\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice/lvm2-lvmetad.service\"} 0.043273694\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice/polkit.service\"} 0.952086428\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice/postfix.service\"} 6.320610364\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice/rhel-push-plugin.service\"} 16.685652057\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice/rhsmcertd.service\"} 4.180338969\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice/rsyslog.service\"} 2.483772873\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice/sshd.service\"} 2.341998992\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice/systemd-journald.service\"} 3.888256042\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice/systemd-logind.service\"} 0.702763816\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/system.slice/systemd-udevd.service\"} 2.053080013\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda\",id=\"/user.slice\"} 324.000092222\ncontainer_fs_io_time_seconds_total{device=\"/dev/sda1\",id=\"/\"} 7.23e-07\ncontainer_fs_io_time_seconds_total{container_name=\"registry\",device=\"/dev/sda\",id=\"/system.slice/docker-da6260076d47a04536b3a8339ccb40143a6c55c731c98c7a99fb3e32ea5b1f3f.scope\",image=\"openshift/origin-docker-registry:v3.6.0-alpha.1\",name=\"k8s_registry.c5fb07ea_docker-registry-1-g89b9_default_f702f49d-248d-11e7-9470-080027893417_868be8c3\",namespace=\"default\",pod_name=\"docker-registry-1-g89b9\"} 3.969092706\nHELP container_fs_io_time_weighted_seconds_total Cumulative weighted I/O time in seconds\nTYPE container_fs_io_time_weighted_seconds_total counter\ncontainer_fs_io_time_weighted_seconds_total{device=\"/dev/mapper/centos-openshift--xfs--vol--dir\",id=\"/\"} 0\ncontainer_fs_io_time_weighted_seconds_total{device=\"/dev/mapper/centos-root\",id=\"/\"} 0\ncontainer_fs_io_time_weighted_seconds_total{device=\"/dev/sda1\",id=\"/\"} 8.91e-07\nHELP container_fs_limit_bytes Number of bytes that can be consumed by the container on this filesystem.\nTYPE container_fs_limit_bytes gauge\ncontainer_fs_limit_bytes{device=\"/dev/mapper/centos-openshift--xfs--vol--dir\",id=\"/\"} 1.0701766656e+10\ncontainer_fs_limit_bytes{device=\"/dev/mapper/centos-root\",id=\"/\"} 4.128114688e+10\ncontainer_fs_limit_bytes{device=\"/dev/sda1\",id=\"/\"} 5.20794112e+08\nHELP container_fs_read_seconds_total Cumulative count of seconds spent reading\nTYPE container_fs_read_seconds_total counter\ncontainer_fs_read_seconds_total{device=\"/dev/mapper/centos-openshift--xfs--vol--dir\",id=\"/\"} 0\ncontainer_fs_read_seconds_total{device=\"/dev/mapper/centos-root\",id=\"/\"} 0\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/\"} 2180.011001925\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice\"} 1778.822306236\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice/NetworkManager.service\"} 29.674368294\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice/atd.service\"} 0.010909787\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice/auditd.service\"} 0.218436287\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice/crond.service\"} 2.407612592\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice/dbus.service\"} 1.882829536\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice/docker-1e8fa84249225e6cd7e4873b0339215d346e924309954f2d3cb3475d3cf93405.scope\"} 0.025023154\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice/docker-6f68e87c197dcc2990731b15a434b237ba784fee3b65f8708196a88f95cce032.scope\"} 0.066847817\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice/docker-bf7e5fce1172be7dd2f4ff7a97847b478920157fae61f86cf44599e431c9f558.scope\"} 0.096245165\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice/docker-containerd.service\"} 57.734992013\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice/docker.service\"} 15.272533128\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice/gssproxy.service\"} 0.126133858\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice/irqbalance.service\"} 0.46796402\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice/lvm2-lvmetad.service\"} 0.043273694\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice/polkit.service\"} 0.952086428\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice/postfix.service\"} 6.320610364\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice/rhel-push-plugin.service\"} 16.685652057\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice/rhsmcertd.service\"} 4.180338969\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice/rsyslog.service\"} 2.483772873\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice/sshd.service\"} 2.341998992\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice/systemd-journald.service\"} 3.888256042\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice/systemd-logind.service\"} 0.702763816\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/system.slice/systemd-udevd.service\"} 2.053080013\ncontainer_fs_read_seconds_total{device=\"/dev/sda\",id=\"/user.slice\"} 308.112142881\ncontainer_fs_read_seconds_total{device=\"/dev/sda1\",id=\"/\"} 7.39e-07\ncontainer_fs_read_seconds_total{container_name=\"registry\",device=\"/dev/sda\",id=\"/system.slice/docker-da6260076d47a04536b3a8339ccb40143a6c55c731c98c7a99fb3e32ea5b1f3f.scope\",image=\"openshift/origin-docker-registry:v3.6.0-alpha.1\",name=\"k8s_registry.c5fb07ea_docker-registry-1-g89b9_default_f702f49d-248d-11e7-9470-080027893417_868be8c3\",namespace=\"default\",pod_name=\"docker-registry-1-g89b9\"} 3.969092706\nHELP container_fs_reads_bytes_total Cumulative count of bytes read\nTYPE container_fs_reads_bytes_total counter\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/\"} 5.2674830336e+10\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice\"} 4.2169052672e+10\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice/NetworkManager.service\"} 1.899642368e+09\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice/atd.service\"} 602112\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice/auditd.service\"} 4.501504e+06\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice/crond.service\"} 2.9442048e+07\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice/dbus.service\"} 8.1825792e+07\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice/docker-1e8fa84249225e6cd7e4873b0339215d346e924309954f2d3cb3475d3cf93405.scope\"} 2.260992e+06\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice/docker-6f68e87c197dcc2990731b15a434b237ba784fee3b65f8708196a88f95cce032.scope\"} 4.554752e+06\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice/docker-bf7e5fce1172be7dd2f4ff7a97847b478920157fae61f86cf44599e431c9f558.scope\"} 3.387392e+06\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice/docker-containerd.service\"} 1.529950208e+09\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice/docker.service\"} 1.25435904e+08\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice/gssproxy.service\"} 3.063808e+06\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice/irqbalance.service\"} 1.8526208e+07\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice/lvm2-lvmetad.service\"} 1.486848e+06\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice/polkit.service\"} 5.1204096e+07\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice/postfix.service\"} 1.81219328e+08\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice/rhel-push-plugin.service\"} 2.37060096e+08\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice/rhsmcertd.service\"} 1.1794432e+08\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice/rsyslog.service\"} 1.21530368e+08\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice/sshd.service\"} 4.2262528e+07\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice/systemd-journald.service\"} 1.35585792e+08\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice/systemd-logind.service\"} 3.2964608e+07\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/system.slice/systemd-udevd.service\"} 4.2074112e+07\ncontainer_fs_reads_bytes_total{device=\"/dev/sda\",id=\"/user.slice\"} 7.86886656e+09\ncontainer_fs_reads_bytes_total{container_name=\"registry\",device=\"/dev/sda\",id=\"/system.slice/docker-da6260076d47a04536b3a8339ccb40143a6c55c731c98c7a99fb3e32ea5b1f3f.scope\",image=\"openshift/origin-docker-registry:v3.6.0-alpha.1\",name=\"k8s_registry.c5fb07ea_docker-registry-1-g89b9_default_f702f49d-248d-11e7-9470-080027893417_868be8c3\",namespace=\"default\",pod_name=\"docker-registry-1-g89b9\"} 5.1924992e+07\nHELP container_fs_reads_merged_total Cumulative count of reads merged\nTYPE container_fs_reads_merged_total counter\ncontainer_fs_reads_merged_total{device=\"/dev/mapper/centos-openshift--xfs--vol--dir\",id=\"/\"} 0\ncontainer_fs_reads_merged_total{device=\"/dev/mapper/centos-root\",id=\"/\"} 0\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/\"} 2858\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice\"} 2731\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice/NetworkManager.service\"} 165\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice/atd.service\"} 0\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice/auditd.service\"} 0\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice/crond.service\"} 0\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice/dbus.service\"} 1\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice/docker-1e8fa84249225e6cd7e4873b0339215d346e924309954f2d3cb3475d3cf93405.scope\"} 0\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice/docker-6f68e87c197dcc2990731b15a434b237ba784fee3b65f8708196a88f95cce032.scope\"} 0\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice/docker-bf7e5fce1172be7dd2f4ff7a97847b478920157fae61f86cf44599e431c9f558.scope\"} 0\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice/docker-containerd.service\"} 943\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice/docker.service\"} 1\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice/gssproxy.service\"} 0\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice/irqbalance.service\"} 0\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice/lvm2-lvmetad.service\"} 0\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice/polkit.service\"} 6\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice/postfix.service\"} 9\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice/rhel-push-plugin.service\"} 21\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice/rhsmcertd.service\"} 0\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice/rsyslog.service\"} 11\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice/sshd.service\"} 0\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice/systemd-journald.service\"} 1\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice/systemd-logind.service\"} 13\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/system.slice/systemd-udevd.service\"} 114\ncontainer_fs_reads_merged_total{device=\"/dev/sda\",id=\"/user.slice\"} 92\ncontainer_fs_reads_merged_total{device=\"/dev/sda1\",id=\"/\"} 0\ncontainer_fs_reads_merged_total{container_name=\"registry\",device=\"/dev/sda\",id=\"/system.slice/docker-da6260076d47a04536b3a8339ccb40143a6c55c731c98c7a99fb3e32ea5b1f3f.scope\",image=\"openshift/origin-docker-registry:v3.6.0-alpha.1\",name=\"k8s_registry.c5fb07ea_docker-registry-1-g89b9_default_f702f49d-248d-11e7-9470-080027893417_868be8c3\",namespace=\"default\",pod_name=\"docker-registry-1-g89b9\"} 0\nHELP container_fs_reads_total Cumulative count of reads completed\nTYPE container_fs_reads_total counter\ncontainer_fs_reads_total{device=\"/dev/mapper/centos-openshift--xfs--vol--dir\",id=\"/\"} 0\ncontainer_fs_reads_total{device=\"/dev/mapper/centos-root\",id=\"/\"} 0\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/\"} 1.29444e+06\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice\"} 824585\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice/NetworkManager.service\"} 16841\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice/atd.service\"} 11\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice/auditd.service\"} 184\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice/crond.service\"} 1674\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice/dbus.service\"} 2715\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice/docker-1e8fa84249225e6cd7e4873b0339215d346e924309954f2d3cb3475d3cf93405.scope\"} 11\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice/docker-6f68e87c197dcc2990731b15a434b237ba784fee3b65f8708196a88f95cce032.scope\"} 14\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice/docker-bf7e5fce1172be7dd2f4ff7a97847b478920157fae61f86cf44599e431c9f558.scope\"} 16\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice/docker-containerd.service\"} 16628\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice/docker.service\"} 9999\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice/gssproxy.service\"} 80\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice/irqbalance.service\"} 522\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice/lvm2-lvmetad.service\"} 61\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice/polkit.service\"} 1079\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice/postfix.service\"} 2425\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice/rhel-push-plugin.service\"} 3890\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice/rhsmcertd.service\"} 6283\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice/rsyslog.service\"} 3239\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice/sshd.service\"} 1736\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice/systemd-journald.service\"} 4532\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice/systemd-logind.service\"} 806\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/system.slice/systemd-udevd.service\"} 1929\ncontainer_fs_reads_total{device=\"/dev/sda\",id=\"/user.slice\"} 298803\ncontainer_fs_reads_total{device=\"/dev/sda1\",id=\"/\"} 2309\ncontainer_fs_reads_total{container_name=\"registry\",device=\"/dev/sda\",id=\"/system.slice/docker-da6260076d47a04536b3a8339ccb40143a6c55c731c98c7a99fb3e32ea5b1f3f.scope\",image=\"openshift/origin-docker-registry:v3.6.0-alpha.1\",name=\"k8s_registry.c5fb07ea_docker-registry-1-g89b9_default_f702f49d-248d-11e7-9470-080027893417_868be8c3\",namespace=\"default\",pod_name=\"docker-registry-1-g89b9\"} 463\nHELP container_fs_sector_reads_total Cumulative count of sector reads completed\nTYPE container_fs_sector_reads_total counter\ncontainer_fs_sector_reads_total{device=\"/dev/mapper/centos-openshift--xfs--vol--dir\",id=\"/\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/mapper/centos-root\",id=\"/\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice/NetworkManager.service\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice/atd.service\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice/auditd.service\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice/crond.service\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice/dbus.service\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice/docker-1e8fa84249225e6cd7e4873b0339215d346e924309954f2d3cb3475d3cf93405.scope\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice/docker-6f68e87c197dcc2990731b15a434b237ba784fee3b65f8708196a88f95cce032.scope\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice/docker-bf7e5fce1172be7dd2f4ff7a97847b478920157fae61f86cf44599e431c9f558.scope\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice/docker-containerd.service\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice/docker.service\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice/gssproxy.service\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice/irqbalance.service\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice/lvm2-lvmetad.service\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice/polkit.service\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice/postfix.service\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice/rhel-push-plugin.service\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice/rhsmcertd.service\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice/rsyslog.service\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice/sshd.service\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice/systemd-journald.service\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice/systemd-logind.service\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/system.slice/systemd-udevd.service\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda\",id=\"/user.slice\"} 0\ncontainer_fs_sector_reads_total{device=\"/dev/sda1\",id=\"/\"} 56802\ncontainer_fs_sector_reads_total{container_name=\"registry\",device=\"/dev/sda\",id=\"/system.slice/docker-da6260076d47a04536b3a8339ccb40143a6c55c731c98c7a99fb3e32ea5b1f3f.scope\",image=\"openshift/origin-docker-registry:v3.6.0-alpha.1\",name=\"k8s_registry.c5fb07ea_docker-registry-1-g89b9_default_f702f49d-248d-11e7-9470-080027893417_868be8c3\",namespace=\"default\",pod_name=\"docker-registry-1-g89b9\"} 0\nHELP container_fs_sector_writes_total Cumulative count of sector writes completed\nTYPE container_fs_sector_writes_total counter\ncontainer_fs_sector_writes_total{device=\"/dev/mapper/centos-openshift--xfs--vol--dir\",id=\"/\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/mapper/centos-root\",id=\"/\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice/NetworkManager.service\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice/atd.service\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice/auditd.service\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice/crond.service\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice/dbus.service\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice/docker-1e8fa84249225e6cd7e4873b0339215d346e924309954f2d3cb3475d3cf93405.scope\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice/docker-6f68e87c197dcc2990731b15a434b237ba784fee3b65f8708196a88f95cce032.scope\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice/docker-bf7e5fce1172be7dd2f4ff7a97847b478920157fae61f86cf44599e431c9f558.scope\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice/docker-containerd.service\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice/docker.service\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice/gssproxy.service\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice/irqbalance.service\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice/lvm2-lvmetad.service\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice/polkit.service\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice/postfix.service\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice/rhel-push-plugin.service\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice/rhsmcertd.service\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice/rsyslog.service\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice/sshd.service\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice/systemd-journald.service\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice/systemd-logind.service\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/system.slice/systemd-udevd.service\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda\",id=\"/user.slice\"} 0\ncontainer_fs_sector_writes_total{device=\"/dev/sda1\",id=\"/\"} 4168\ncontainer_fs_sector_writes_total{container_name=\"registry\",device=\"/dev/sda\",id=\"/system.slice/docker-da6260076d47a04536b3a8339ccb40143a6c55c731c98c7a99fb3e32ea5b1f3f.scope\",image=\"openshift/origin-docker-registry:v3.6.0-alpha.1\",name=\"k8s_registry.c5fb07ea_docker-registry-1-g89b9_default_f702f49d-248d-11e7-9470-080027893417_868be8c3\",namespace=\"default\",pod_name=\"docker-registry-1-g89b9\"} 0\nHELP container_fs_usage_bytes Number of bytes that are consumed by the container on this filesystem.\nTYPE container_fs_usage_bytes gauge\ncontainer_fs_usage_bytes{device=\"/dev/mapper/centos-openshift--xfs--vol--dir\",id=\"/\"} 3.3726464e+07\ncontainer_fs_usage_bytes{device=\"/dev/mapper/centos-root\",id=\"/\"} 3.3596928e+10\ncontainer_fs_usage_bytes{device=\"/dev/sda1\",id=\"/\"} 2.2013952e+08\nHELP container_fs_write_seconds_total Cumulative count of seconds spent writing\nTYPE container_fs_write_seconds_total counter\ncontainer_fs_write_seconds_total{device=\"/dev/mapper/centos-openshift--xfs--vol--dir\",id=\"/\"} 0\ncontainer_fs_write_seconds_total{device=\"/dev/mapper/centos-root\",id=\"/\"} 0\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/\"} 15288.887204055\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice\"} 1274.827284754\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice/NetworkManager.service\"} 0.003641868\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice/atd.service\"} 0\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice/auditd.service\"} 1.290659471\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice/crond.service\"} 0\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice/dbus.service\"} 0\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice/docker-1e8fa84249225e6cd7e4873b0339215d346e924309954f2d3cb3475d3cf93405.scope\"} 0\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice/docker-6f68e87c197dcc2990731b15a434b237ba784fee3b65f8708196a88f95cce032.scope\"} 0\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice/docker-bf7e5fce1172be7dd2f4ff7a97847b478920157fae61f86cf44599e431c9f558.scope\"} 0\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice/docker-containerd.service\"} 0\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice/docker.service\"} 1.696696729\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice/gssproxy.service\"} 0\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice/irqbalance.service\"} 0\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice/lvm2-lvmetad.service\"} 0\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice/polkit.service\"} 0\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice/postfix.service\"} 0\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice/rhel-push-plugin.service\"} 0\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice/rhsmcertd.service\"} 0\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice/rsyslog.service\"} 0\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice/sshd.service\"} 0\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice/systemd-journald.service\"} 0\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice/systemd-logind.service\"} 0\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/system.slice/systemd-udevd.service\"} 0\ncontainer_fs_write_seconds_total{device=\"/dev/sda\",id=\"/user.slice\"} 15.887949341\ncontainer_fs_write_seconds_total{device=\"/dev/sda1\",id=\"/\"} 1.71e-07\ncontainer_fs_write_seconds_total{container_name=\"registry\",device=\"/dev/sda\",id=\"/system.slice/docker-da6260076d47a04536b3a8339ccb40143a6c55c731c98c7a99fb3e32ea5b1f3f.scope\",image=\"openshift/origin-docker-registry:v3.6.0-alpha.1\",name=\"k8s_registry.c5fb07ea_docker-registry-1-g89b9_default_f702f49d-248d-11e7-9470-080027893417_868be8c3\",namespace=\"default\",pod_name=\"docker-registry-1-g89b9\"} 0\nHELP container_fs_writes_bytes_total Cumulative count of bytes written\nTYPE container_fs_writes_bytes_total counter\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/\"} 1.87071492096e+11\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice\"} 1.6475451392e+10\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice/NetworkManager.service\"} 57344\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice/atd.service\"} 0\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice/auditd.service\"} 3.1215616e+07\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice/crond.service\"} 0\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice/dbus.service\"} 0\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice/docker-1e8fa84249225e6cd7e4873b0339215d346e924309954f2d3cb3475d3cf93405.scope\"} 0\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice/docker-6f68e87c197dcc2990731b15a434b237ba784fee3b65f8708196a88f95cce032.scope\"} 0\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice/docker-bf7e5fce1172be7dd2f4ff7a97847b478920157fae61f86cf44599e431c9f558.scope\"} 0\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice/docker-containerd.service\"} 0\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice/docker.service\"} 2.3957504e+07\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice/gssproxy.service\"} 0\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice/irqbalance.service\"} 0\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice/lvm2-lvmetad.service\"} 0\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice/polkit.service\"} 0\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice/postfix.service\"} 0\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice/rhel-push-plugin.service\"} 0\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice/rhsmcertd.service\"} 0\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice/rsyslog.service\"} 0\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice/sshd.service\"} 0\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice/systemd-journald.service\"} 0\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice/systemd-logind.service\"} 0\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/system.slice/systemd-udevd.service\"} 0\ncontainer_fs_writes_bytes_total{device=\"/dev/sda\",id=\"/user.slice\"} 2.45915648e+08\ncontainer_fs_writes_bytes_total{container_name=\"registry\",device=\"/dev/sda\",id=\"/system.slice/docker-da6260076d47a04536b3a8339ccb40143a6c55c731c98c7a99fb3e32ea5b1f3f.scope\",image=\"openshift/origin-docker-registry:v3.6.0-alpha.1\",name=\"k8s_registry.c5fb07ea_docker-registry-1-g89b9_default_f702f49d-248d-11e7-9470-080027893417_868be8c3\",namespace=\"default\",pod_name=\"docker-registry-1-g89b9\"} 0\nHELP container_fs_writes_merged_total Cumulative count of writes merged\nTYPE container_fs_writes_merged_total counter\ncontainer_fs_writes_merged_total{device=\"/dev/mapper/centos-openshift--xfs--vol--dir\",id=\"/\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/mapper/centos-root\",id=\"/\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/\"} 455792\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice/NetworkManager.service\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice/atd.service\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice/auditd.service\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice/crond.service\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice/dbus.service\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice/docker-1e8fa84249225e6cd7e4873b0339215d346e924309954f2d3cb3475d3cf93405.scope\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice/docker-6f68e87c197dcc2990731b15a434b237ba784fee3b65f8708196a88f95cce032.scope\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice/docker-bf7e5fce1172be7dd2f4ff7a97847b478920157fae61f86cf44599e431c9f558.scope\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice/docker-containerd.service\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice/docker.service\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice/gssproxy.service\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice/irqbalance.service\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice/lvm2-lvmetad.service\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice/polkit.service\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice/postfix.service\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice/rhel-push-plugin.service\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice/rhsmcertd.service\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice/rsyslog.service\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice/sshd.service\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice/systemd-journald.service\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice/systemd-logind.service\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/system.slice/systemd-udevd.service\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda\",id=\"/user.slice\"} 0\ncontainer_fs_writes_merged_total{device=\"/dev/sda1\",id=\"/\"} 0\ncontainer_fs_writes_merged_total{container_name=\"registry\",device=\"/dev/sda\",id=\"/system.slice/docker-da6260076d47a04536b3a8339ccb40143a6c55c731c98c7a99fb3e32ea5b1f3f.scope\",image=\"openshift/origin-docker-registry:v3.6.0-alpha.1\",name=\"k8s_registry.c5fb07ea_docker-registry-1-g89b9_default_f702f49d-248d-11e7-9470-080027893417_868be8c3\",namespace=\"default\",pod_name=\"docker-registry-1-g89b9\"} 0\nHELP container_fs_writes_total Cumulative count of writes completed\nTYPE container_fs_writes_total counter\ncontainer_fs_writes_total{device=\"/dev/mapper/centos-openshift--xfs--vol--dir\",id=\"/\"} 0\ncontainer_fs_writes_total{device=\"/dev/mapper/centos-root\",id=\"/\"} 0\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/\"} 5.263333e+06\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice\"} 1.635728e+06\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice/NetworkManager.service\"} 14\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice/atd.service\"} 0\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice/auditd.service\"} 4604\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice/crond.service\"} 0\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice/dbus.service\"} 0\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice/docker-1e8fa84249225e6cd7e4873b0339215d346e924309954f2d3cb3475d3cf93405.scope\"} 0\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice/docker-6f68e87c197dcc2990731b15a434b237ba784fee3b65f8708196a88f95cce032.scope\"} 0\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice/docker-bf7e5fce1172be7dd2f4ff7a97847b478920157fae61f86cf44599e431c9f558.scope\"} 0\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice/docker-containerd.service\"} 0\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice/docker.service\"} 4594\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice/gssproxy.service\"} 0\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice/irqbalance.service\"} 0\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice/lvm2-lvmetad.service\"} 0\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice/polkit.service\"} 0\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice/postfix.service\"} 0\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice/rhel-push-plugin.service\"} 0\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice/rhsmcertd.service\"} 0\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice/rsyslog.service\"} 0\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice/sshd.service\"} 0\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice/systemd-journald.service\"} 0\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice/systemd-logind.service\"} 0\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/system.slice/systemd-udevd.service\"} 0\ncontainer_fs_writes_total{device=\"/dev/sda\",id=\"/user.slice\"} 21654\ncontainer_fs_writes_total{device=\"/dev/sda1\",id=\"/\"} 2067\ncontainer_fs_writes_total{container_name=\"registry\",device=\"/dev/sda\",id=\"/system.slice/docker-da6260076d47a04536b3a8339ccb40143a6c55c731c98c7a99fb3e32ea5b1f3f.scope\",image=\"openshift/origin-docker-registry:v3.6.0-alpha.1\",name=\"k8s_registry.c5fb07ea_docker-registry-1-g89b9_default_f702f49d-248d-11e7-9470-080027893417_868be8c3\",namespace=\"default\",pod_name=\"docker-registry-1-g89b9\"} 0\n```. @DirectXMan12. @dashpole as discussed last week - we don't have a prometheus e2e test, and I wasn't sure what expectations are around new tests on something like this.. All the filesystem data is in form /dev/sda1.  What we get from the machine info is \"sda\".  To make the two roughly consistent, I turn \"sda\" into \"/dev/sda\". And at least on the few systems I looked at (RHEL / Mac / ubuntu) there is always a /dev/NAME for what mount returns. Interestingly enough - we depend on /dev/sda existing because we do this:\nhttps://github.com/google/cadvisor/blob/master/utils/sysfs/sysfs.go#L82\nSo a client that wanted to lookup the device by name can rely on this path (to then get major/minor).. Interestingly enough - we depend on /dev/sda existing because we do this:\nhttps://github.com/google/cadvisor/blob/master/utils/sysfs/sysfs.go#L82\nSo a client that wanted to lookup the device by name can rely on this path (to then get major/minor).. Nice dig. Nice dig. Very happy - short running jobs now can be effectively estimated by prometheus based on peak memory usage in bursty scenarios.. Very happy - short running jobs now can be effectively estimated by prometheus based on peak memory usage in bursty scenarios.. ",
    "kargakis": "@vmarmol unfortunately no it's not.\n. > Looks like we don't ever get the token.\nI changed the generation of the token to be handled from jwt as it was already handled before. Can you retest this?\n. Missing deps added and Travis is happy\n. ",
    "roldancer": "Hi All, if I run the container in previleded mode, it works OK .... any  idea ?\ndocker run -d --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro --publish=8080:8080  --name=cadvisor --privileged=true google/cadvisor:latest\n. Hi @vmarmol, your theory is correct, cadvisor work OK with parameter \"--logtostderr\".\nMany thanks for your help\n. Hi @vmarmol, I found a new error when using the parameter \"--logtostderr\", containers are not discovered, this is the cadvisor's log, I think SELinux is restricting some operations ...\nW0327 20:26:14.555638 00001 handler.go:534] Error while processing event (\"/var/lib/docker/devicemapper/mnt/623abb2aa2cfd323fadd29f02f9f84d266c91a7a7a778999c7fec3441753427d/rootfs/rootfs/sys/fs/cgroup/cpu,cpuacct/system.slice/NetworkManager-dispatcher.service\": 0x40000200 == IN_DELETE|IN_ISDIR): inotify_rm_watch: invalid argument\nW0327 20:26:14.556464 00001 handler.go:534] Error while processing event (\"/var/lib/docker/devicemapper/mnt/623abb2aa2cfd323fadd29f02f9f84d266c91a7a7a778999c7fec3441753427d/rootfs/rootfs/sys/fs/cgroup/blkio/system.slice/NetworkManager-dispatcher.service\": 0x40000200 == IN_DELETE|IN_ISDIR): inotify_rm_watch: invalid argument\nW0327 20:26:33.556400 00001 handler.go:534] Error while processing event (\"/cpuacct.usage\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/cpuacct.usage\"\nW0327 20:26:33.556505 00001 handler.go:534] Error while processing event (\"/cpuacct.usage_percpu\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/cpuacct.usage_percpu\"\nW0327 20:26:33.556546 00001 handler.go:534] Error while processing event (\"/cpuacct.stat\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/cpuacct.stat\"\nW0327 20:26:33.556577 00001 handler.go:534] Error while processing event (\"/cpu.shares\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/cpu.shares\"\nW0327 20:26:33.556626 00001 handler.go:534] Error while processing event (\"/cpu.cfs_quota_us\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/cpu.cfs_quota_us\"\nW0327 20:26:33.556692 00001 handler.go:534] Error while processing event (\"/cpu.cfs_period_us\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/cpu.cfs_period_us\"\nW0327 20:26:33.556728 00001 handler.go:534] Error while processing event (\"/cpu.stat\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/cpu.stat\"\nW0327 20:26:33.556758 00001 handler.go:534] Error while processing event (\"/cpu.rt_runtime_us\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/cpu.rt_runtime_us\"\nW0327 20:26:33.556789 00001 handler.go:534] Error while processing event (\"/cpu.rt_period_us\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/cpu.rt_period_us\"\nW0327 20:26:33.556824 00001 handler.go:534] Error while processing event (\"/tasks\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/tasks\"\nW0327 20:26:33.556881 00001 handler.go:534] Error while processing event (\"/cgroup.procs\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/cgroup.procs\"\nW0327 20:26:33.556914 00001 handler.go:534] Error while processing event (\"/notify_on_release\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/notify_on_release\"\nW0327 20:26:33.556947 00001 handler.go:534] Error while processing event (\"/cgroup.event_control\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/cgroup.event_control\"\nW0327 20:26:33.556976 00001 handler.go:534] Error while processing event (\"/cgroup.clone_children\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/cgroup.clone_children\"\nW0327 20:26:33.557011 00001 handler.go:534] Error while processing event (\"/var/lib/docker/devicemapper/mnt/623abb2aa2cfd323fadd29f02f9f84d266c91a7a7a778999c7fec3441753427d/rootfs/rootfs/sys/fs/cgroup/cpu,cpuacct/system.slice/NetworkManager-dispatcher.service\": 0x40000200 == IN_DELETE|IN_ISDIR): inotify_rm_watch: invalid argument\nW0327 20:26:33.557048 00001 handler.go:534] Error while processing event (\"/blkio.reset_stats\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.reset_stats\"\nW0327 20:26:33.557082 00001 handler.go:534] Error while processing event (\"/blkio.throttle.read_bps_device\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.throttle.read_bps_device\"\nW0327 20:26:33.557111 00001 handler.go:534] Error while processing event (\"/blkio.throttle.write_bps_device\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.throttle.write_bps_device\"\nW0327 20:26:33.557169 00001 handler.go:534] Error while processing event (\"/blkio.throttle.read_iops_device\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.throttle.read_iops_device\"\nW0327 20:26:33.557203 00001 handler.go:534] Error while processing event (\"/blkio.throttle.write_iops_device\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.throttle.write_iops_device\"\nW0327 20:26:33.557237 00001 handler.go:534] Error while processing event (\"/blkio.throttle.io_service_bytes\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.throttle.io_service_bytes\"\nW0327 20:26:33.557267 00001 handler.go:534] Error while processing event (\"/blkio.throttle.io_serviced\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.throttle.io_serviced\"\nW0327 20:26:33.557300 00001 handler.go:534] Error while processing event (\"/blkio.weight_device\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.weight_device\"\nW0327 20:26:33.557328 00001 handler.go:534] Error while processing event (\"/blkio.weight\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.weight\"\nW0327 20:26:33.557360 00001 handler.go:534] Error while processing event (\"/blkio.leaf_weight_device\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.leaf_weight_device\"\nW0327 20:26:33.557389 00001 handler.go:534] Error while processing event (\"/blkio.leaf_weight\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.leaf_weight\"\nW0327 20:26:33.557421 00001 handler.go:534] Error while processing event (\"/blkio.time\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.time\"\nW0327 20:26:33.557456 00001 handler.go:534] Error while processing event (\"/blkio.sectors\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.sectors\"\nW0327 20:26:33.557514 00001 handler.go:534] Error while processing event (\"/blkio.io_service_bytes\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.io_service_bytes\"\nW0327 20:26:33.557951 00001 handler.go:534] Error while processing event (\"/blkio.io_serviced\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.io_serviced\"\nW0327 20:26:33.558015 00001 handler.go:534] Error while processing event (\"/blkio.io_service_time\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.io_service_time\"\nW0327 20:26:33.558066 00001 handler.go:534] Error while processing event (\"/blkio.io_wait_time\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.io_wait_time\"\nW0327 20:26:33.558094 00001 handler.go:534] Error while processing event (\"/blkio.io_merged\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.io_merged\"\nW0327 20:26:33.558170 00001 handler.go:534] Error while processing event (\"/blkio.io_queued\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.io_queued\"\nW0327 20:26:33.558237 00001 handler.go:534] Error while processing event (\"/blkio.time_recursive\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.time_recursive\"\nW0327 20:26:33.558287 00001 handler.go:534] Error while processing event (\"/blkio.sectors_recursive\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.sectors_recursive\"\nW0327 20:26:33.558319 00001 handler.go:534] Error while processing event (\"/blkio.io_service_bytes_recursive\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.io_service_bytes_recursive\"\nW0327 20:26:33.558353 00001 handler.go:534] Error while processing event (\"/blkio.io_serviced_recursive\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.io_serviced_recursive\"\nW0327 20:26:33.558384 00001 handler.go:534] Error while processing event (\"/blkio.io_service_time_recursive\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.io_service_time_recursive\"\nW0327 20:26:33.558417 00001 handler.go:534] Error while processing event (\"/blkio.io_wait_time_recursive\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.io_wait_time_recursive\"\nW0327 20:26:33.558460 00001 handler.go:534] Error while processing event (\"/blkio.io_merged_recursive\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.io_merged_recursive\"\nW0327 20:26:33.558495 00001 handler.go:534] Error while processing event (\"/blkio.io_queued_recursive\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/blkio.io_queued_recursive\"\nW0327 20:26:33.558532 00001 handler.go:534] Error while processing event (\"/tasks\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/tasks\"\nW0327 20:26:33.558564 00001 handler.go:534] Error while processing event (\"/cgroup.procs\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/cgroup.procs\"\nW0327 20:26:33.558591 00001 handler.go:534] Error while processing event (\"/notify_on_release\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/notify_on_release\"\nW0327 20:26:33.558621 00001 handler.go:534] Error while processing event (\"/cgroup.event_control\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/cgroup.event_control\"\nW0327 20:26:33.558648 00001 handler.go:534] Error while processing event (\"/cgroup.clone_children\": 0x200 == IN_DELETE): unable to detect container from watch event on directory \"/cgroup.clone_children\"\nW0327 20:26:33.558686 00001 handler.go:534] Error while processing event (\"/var/lib/docker/devicemapper/mnt/623abb2aa2cfd323fadd29f02f9f84d266c91a7a7a778999c7fec3441753427d/rootfs/rootfs/sys/fs/cgroup/blkio/system.slice/NetworkManager-dispatcher.service\": 0x40000200 == IN_DELETE|IN_ISDIR): inotify_rm_watch: invalid argument\nI0327 20:27:05.634734 00001 manager.go:511] Destroyed container: \"/system.slice/NetworkManager-dispatcher.service\" (aliases: [], namespace: \"\")\n. Many thanks for your help, \n. ",
    "perezd": "I see 4 large numbers when executing cat /sys/fs/cgroup/cpuacct\n. according to: https://golang.org/src/runtime/slice.go\n17   // TODO: take uintptrs instead of int64s?\n    18  func makeslice(t *slicetype, len64 int64, cap64 int64) sliceStruct {\n    19      // NOTE: The len > MaxMem/elemsize check here is not strictly necessary,\n    20      // but it produces a 'len out of range' error instead of a 'cap out of range' error\n    21      // when someone does make([]T, bignumber). 'cap out of range' is true too,\n    22      // but since the cap is only being supplied implicitly, saying len is clearer.\n    23      // See issue 4085.\n    24      len := int(len64)\n    25      if len64 < 0 || int64(len) != len64 || t.elem.size > 0 && uintptr(len) > maxmem/uintptr(t.elem.size) {\n    26          panic(errorString(\"makeslice: len out of range\"))\n    27      }\nSo perhaps we're sticking a huge number in as the len?\n. ",
    "fabxc": "Yes, this is way beyond the scope of cAdvisor. The main purpose is exposing container metrics for consumption by a monitoring system further up the stack.\nIs there disagreement on this or can this feature request be closed? I just want to gather some clarity :)\n. Our prefixing is described in the comment section of the honor_labels option in scrap configuration: https://prometheus.io/docs/operating/configuration/#scrape_config\n1315 seems to prefix all labels with label_ even without collisions? That seems shaky usability-wise.\n. My bad, can be closed then.\nOn Fri, Aug 5, 2016, 8:30 PM Tobias Schmidt notifications@github.com\nwrote:\n\nDuplicate of #1404 https://github.com/google/cadvisor/issues/1404.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1412#issuecomment-237927385,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AEuA8k4SCVg7O3IjUOpOIH8wfkYdzEWnks5qc4E6gaJpZM4JdqDb\n.\n. Thanks for all the quick feedback everyone! Greatly appreciated.\nRemember, when we talk application metrics here, we're not just talking about metrics used solely for monitoring purposes. The horizontal pod autoscaler in Kubernetes (and possibly other components in the future) can use custom metrics to scale on. \n\nI understood that Heapster was serving that purpose.\n\nWhatever the collector is only has to make one HTTP call per node, greatly reducing the number of calls that the central collection agent needs to make. Additionally, having the collection agent one the node opens up more collection possibilities (e.g. I don't want to expose my metrics endpoint publicly to the cluster, just to the collection agent).\n\nThat is one collection scenario. Regardless of my agreement with it (SPOF, number of requests vs response size, one fixed sampling rate for everything), cAdvisor is a generally useful tool for exposing container metrics. We tie an opinionated approach to an orthogonal problem to it? That said, I realize that some people want one collection agent per node.\n\nWe need a path for both concerns. It doesn't have to be the same path, but in the end, we need to make sure that we can still collect a few metrics for use by autoscaling, while not requiring all the metrics to go through that path.\n\nAgreed. \n\nThis makes it an attractive solution for handling application level metrics as well.\n\nThis is based on the fact that cAdvisor is a local agent? Because a separate component would have the same benefits.\n\nThe real answer to this issue will come from users. If users find cAdvisor collecting application metrics to be useful, then why not support it as part of cAdvisor?\n\nBasically for the reasons above. Exposition and collection are orthogonal problems. Why not? Way larger scope of code base, which comes with ownership problems (c.f. many issues, mostly uncommented, unlabelled, and unassigned), unnecessary coupling for releases and the iteration slowdown it comes with.\nIt's two concerns in one process, which comes with all the problems that has. The same reasons apply for why we don't run several processes per container. The operational overhead is mostly negligible, and reduced for everyone who just wants one of the two.\n\nAs for what Kubernetes should do by default, it is beyond cAdvisor. Metrics in kubernetes is a larger, more complex problem. So let's restrict this discussion to just cAdvisor as a standalone entity.\n\nAgreed. And for Kubernetes providing clearly scoped tools serving one purpose well is most beneficial.\n. LGTM\n. For reference, the parser is located in https://github.com/prometheus/common/tree/master/expfmt\n. @zeisss @micahhausler are you both running Prometheus 2.0? In 1.x versions the flapping metrics are not caught by the new staleness handling and thus it should have no immediately visible effect.\nIn general it's definitely wrong behavior by cAdvisor though that violates the /metrics contract.\nThis seems to be a recent regression. @derekwaynecarr @timothysc any idea what could have caused this?. Fix this link to the commit hash?\n. Not sure I entirely understand this. We seem to ignore the labels of the samples entire and just append timestamp/value pairs for the metric name?\n. It's just the initial capacity. The slice is appended to, so it just allocates more if we go beyond.\n50 SGTM. It's a number of metrics we can expect from virtually any endpoint, grow allocations beyond that are fine. But we basically save 6 re-allocations for the general case.\nPlus, the slice is just reset at the end and re-used. \n. Data transfer object. Kinda nasty, but better than go and kinda what we use everywhere else in Prometheus land.\n. In label values all unicode runes are allowed. In Prometheus itself we hence us \\0xff as an invalid rune as a separator for unambiguous concatenation.\n. Agreed\n. ",
    "jdef": "xref https://github.com/mesosphere/kubernetes-mesos/issues/197\n. Pretty sure it's because memory.StatsBuffer is returning addresses that point to locations of it's internal circular buffer that are later overwritten. The writes are guarded by a lock, but since the pointers to the internal buffer locations are exposed externally they are not guarded appropriately.\nI have a patch that (I think) resolves this. Will file a PR.\n. I signed it! james dot defelice at ishisystems dot com\n. xref https://github.com/mesosphere/kubernetes-mesos/issues/200\n. Can you be more explicit about the implicit dependencies on import order?\n. Thanks for the explanation. Should be documented for sanity.\nOn Feb 17, 2016 2:44 PM, \"Tim St. Clair\" notifications@github.com wrote:\n\n@jdef https://github.com/jdef were you asking for more of an\nexplanation here, or to document it somewhere in cAdvisor?\nTo elaborate here, the flags are added by each package in the init()\nfunction, which is called the first time the package is imported. This\nmeans that if a caller is going to use flag.Add(All)Flags in an Init()\nfunction, the package that calls it must import all the cAdvisor packages\nthat add desired flags. This can all be avoided by just calling\nAdd(All)Flags outside the Init function, so it should not be much of an\nissue.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1113#issuecomment-185371853.\n. \n",
    "catrixs": "@vishh Yah. We use ELK to collect container metrics data in production environment.\nELK represents ElasticSearch + LogStash + Kibana. We think cAdvisor could be able to forward metrics data to LogStash, and then we could use ElasticSearch and Kibana to create dashboard, and like this one:\n\n. @vishh I'm working on it.\n. https://github.com/google/cadvisor/pull/693\nWe just submit a pull request which is very simple. Just implements a redis driver which use lpush to push metrics json to redis, then logstash could forward them to ElasticSearch.\nNext step, we want implement a native ElasticSearch driver.\n. Hi @jason2055141 We use cAdvisor to collect stats data of container, use scribe to collect logs of container, use --volume to store the logs of container on local host, use ELK and Graphite to visualize the data.\n. @rjnagal yeah, this is the PR, thank you @hacpai \nhttps://github.com/google/cadvisor/pull/875\n. ",
    "wangjingbomail": "It is a good idea\n. It is a good idea\n. ",
    "ternel": "Would be great :)\n. ",
    "bastnic": ":+1: \n. ",
    "kangman": ":+1: \n. ",
    "jason2055141": "Hi @feiyang21687 How did you get that? What plugin are you using to collect data's or logs in containers? Can you please advice? Thanks. By the way this is a nice and interesting post. Thanks men. \n. @feiyang21687 \nThanks men. I will try that scribe to collects logs from container. Thanks again.\n. ",
    "kerwin": "+1\n. ",
    "trompx": "An elasticsearch storage driver would definitly be a great add ! instead of having to store data to redis first then forward to logstash to save it on elasticsearch.\n. ",
    "cteyton": "+1 for a native elastic search driver\n. thanks @hacpai\n. To overcome the missing \".raw\" issue, i think you should get into the es driver of cadvisor and figure out what's missing. If the container name is a String, thus it's strange that no \"raw\" value is available.\n. sorry, thanks for correcting me !\n. I think you should continue the discussion here :  #634\n. ",
    "philipz": "+1\n. ",
    "hacpai": "You are welcome. @feiyang21687 @cteyton \nI hope this could help you. :)\n. Thanks @Trane9991 \nOh. The Ping service is special in that it doesn't use the URLs of the client but the one specified via the URL member of the PingService (which is http://127.0.0.1:9200 by default).\nSo use this code to issue the ping to your URL instead:\ninfo, code, err := client.Ping().URL(elasticHost).Do()\nCan you help me fix this bug?\njust modify storage/elasticsearch/elasticsearch.go 108 line:\ninfo, code, err := client.Ping().Do()\nto\ninfo, code, err := client.Ping().URL(elasticHost).Do()\n. @Trane9991 It is strange, I see this document\nhttps://github.com/olivere/elastic/blob/master/doc.go#L18\nline 100 and 101 is OK.\n. Thanks @Trane9991 \nIt's the \"docker issue\".\nA ElasticSearch client uses a sniffing process to find all nodes of your cluster by default, automatically. \nWhen sniffing the cluster fails as you create the new client, an error is returned: Elastic might have a problem to find any suitable node in your cluster. \nNow I disable it when creating a client.\nYou can try my new commit and let me know if you succeed.\n. I'm sorry hear about that.\nI try connect remote ElasticSearch with cAdvisor.\nLook this picture, 45.55.62.208 is my remote host. \n\nYou can try connect it.\nAre you using ElasticSearch from container?\n. Hi @stonevil \nWhat is the output of curl http://:9200/_nodes/http?pretty when you run it from the server where Elastic is running?\n. @stonevil \nOkay, my new code fix this problem.\nJust follow me\n1. cd $GOPATH/src/github.com/google/ \n2. rm -rf cadvisor and  git clone https://github.com/hacpai/cadvisor.git -b fixbug-issue-881\n3.  godep go build .\n4. ./cadvisor -storage_driver=\"elasticsearch\"  -storage_driver_es_host=\"http://localhost:9200\" -storage_driver_es_enable_sniffe=true\nIf you're using ElasticSearch from container, you have two options:\n1. Configure the list of HTTP endpoint URLs manually and disable sniffing.\n   ./cadvisor -storage_driver=\"elasticsearch\"  -storage_driver_es_host=\"http://localhost:9200\"\n2. Make sure Elasticsearch nodes bind to an IP address that is accessible from the client side.\nFor 2. you should read the Docker wiki page.\n. @Trane9991 Thanks, a good suggestion. \nThis ElasticSearch client is used  in production for more than 2 years now. It works fine. \nAnyway. :-) HTH.\n. Yes. In ElasticSearch, document belongs to Type and kinds of Type belongs to Index.\n. ",
    "gregbkr": "Hello, thank you for adding the ELK support to cadvisor, that is a great news! ^^\nThis command work, and I can see the data in kibana (in index: cadvisor):\n  docker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  google/cadvisor:latest \\\n  -storage_driver=\"elasticsearch\" -alsologtostderr=true -storage_driver_es_host=\"http://elasticsearch_ip:9200\"\nBy any chance, anyone tried to make a dashboard of these data? I am stuck with the field container_Name, which container some \"/\" and is analysed (and cut). Usually I use the field container_Name.raw to escape but it is not available. Anyone could help, or share some dashboards?\nThanks a lot! and good weekend!\nGreg.\n\n. Good evening everyone!\nThank you for your posts. @pmoust : I followed your path and the issue seems coming from the fact that the field container_Name is analyzed (so split in sub-field) , so I try to edit via setting/indices as showed, but there isn't an option to do that. So I tried via command line.\n- get field mapping :\ncurl -XGET 'http://localhost:9200/cadvisor/_mapping/stats/field/container_Name'\n  {\"cadvisor\":{\"mappings\":{\"stats\":{\"container_Name\":{\"full_name\":\"container_Name\",\"mapping\":{\"container_Name\":{\"type\":\"string\"}}}}}}}\n- force container_Name to not_analysed\ncurl -XPUT 'http://localhost:9200/cadvisor/_mapping/stats' -d '\n  {\n  \"properties\" : {\n  \"container_Name\" : {\"type\" : \"string\", \"index\" : \"not_analyzed\" }\n  }\n  }'\n- Error, cannot change index of a analysed field already in base\n{\"error\":\"MergeMappingException[Merge failed with failures {[mapper [container_Name] has different index values, mapper [container_Name] has different tokenize values, mapper [container_Name] has different index_analyzer]}]\",\"status\":400\n- So I delete cadvisor index, and will try to create a template for mapping when cadvisor fresh start and create the index. The template say, for index=cadvisor, set container_Name = not_analysed \ncurl -XPUT http://localhost:9200/_template/cadvisor -d '\n  {\n  \"template\" : \"cadvisor*\",\n  \"mappings\" : {\n    \"stats\" : {\n    \"container_Name\":{\"type\":\"string\", \"index\":\"not_analyzed\"}\n    }\n  }\n  }'      \n- Run cadvisor but error. Seems like cadvisor don't like when the field is preconfigured via template.\npanic: failed to write stats to ElasticSearch- elastic: Error 400 (Bad Request): MapperParsingException[mapping [stats]]; nested: MapperParsingException[Root type mapping not empty after parsing! Remaining fields:   [container_Name : {index=not_analyzed, type=string}]];\nI had another idea before to post these comments, I copied the template from Logstash-* and create a same one for cadvisor. In string_fields\" section, there is a automatic operation which get the raw value of analysed field. \n- Get your actual setting through:        \ncurl -XGET 'http://localhost:9200/_template/'?pretty        \n- Copy and change \"logstash..\" for cadvisor, and send that template to elastic:\ncurl -XPUT http://localhost:9200/_template/cadvisor -d '\n  {\n  \"template\" : \"cadvisor\",\n  \"mappings\" : {\n    \"default\" : {\n      \"dynamic_templates\" : [ {\n        \"message_field\" : {\n          \"mapping\" : {\n            \"index\" : \"analyzed\",\n            \"omit_norms\" : true,\n            \"type\" : \"string\"\n          },\n          \"match_mapping_type\" : \"string\",\n          \"match\" : \"message\"\n        }\n      }, {\n        \"string_fields\" : {\n          \"mapping\" : {\n            \"index\" : \"analyzed\",\n            \"omit_norms\" : true,\n            \"type\" : \"string\",\n            \"fields\" : {\n              \"raw\" : {\n                \"ignore_above\" : 256,\n                \"index\" : \"not_analyzed\",\n                \"type\" : \"string\"\n              }\n            }\n          },\n          \"match_mapping_type\" : \"string\",\n          \"match\" : \"\"\n        }\n      } ],\n      \"_all\" : {\n        \"omit_norms\" : true,\n        \"enabled\" : true\n      },\n      \"properties\" : {\n        \"geoip\" : {\n          \"dynamic\" : true,\n          \"type\" : \"object\",\n          \"properties\" : {\n            \"location\" : {\n              \"type\" : \"geo_point\"\n            }\n          }\n        },\n        \"@version\" : {\n          \"index\" : \"not_analyzed\",\n          \"type\" : \"string\"\n        }\n      }\n    }\n  },\n  \"aliases\" : { }\n  }\n  }'\n- delete index cadvisor, delete cadvisor container and recreate it. I found out that everything works, the field container_Name got now a raw value, which can be used in visualization.\nThank you all, this will make great dashboard!\nAnd good week!\nGreg.         \n\n. @fiunchinho : sorry, it was working for me for sometimes, but had few performance problems on ELK. I move on to prometheus container for monitoring stats.\n. I am still not fixed to one solution ofr monitoring docker infra, but I will get back to elk when I got more time. I did that doc if that could help anyone. https://github.com/gregbkr/docker-elk-cadvisor-dashboards\n. Hello, thank you for adding the ELK support to cadvisor, that is a great news! ^^\nThis command work, and I can see the data in kibana (in index: cadvisor):\n  docker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  google/cadvisor:latest \\\n  -storage_driver=\"elasticsearch\" -alsologtostderr=true -storage_driver_es_host=\"http://elasticsearch_ip:9200\"\nBy any chance, anyone tried to make a dashboard of these data? I am stuck with the field container_Name, which container some \"/\" and is analysed (and cut). Usually I use the field container_Name.raw to escape but it is not available. Anyone could help, or share some dashboards?\nThanks a lot! and good weekend!\nGreg.\n\n. ",
    "jimmidyson": ".raw suffixes are added via Elasticsearch configuration AFAIK. When you're using Logstash it uses an Elasticsearch template to create these multifields (I think they're called) in Elasticsearch. There isn't really anything for cadvisor to do.\n. Working set = RSS + cache (no swap as @vishh said above). We have #975 open for reporting RSS by itself.\nWe delegate to libcontainer to retrieve stats. We could implement this ourselves but it's kind of nice to have consistency.\n. Working set & total are sent - see https://github.com/google/cadvisor/blob/master/storage/influxdb/influxdb.go#L119-L124\n. So let me try again. Details in https://www.kernel.org/doc/Documentation/cgroups/memory.txt.\nTotal (memory.usage_in_bytes) = rss + cache\nWorking set = Total - inactive (not recently accessed memory = inactive_anon + inactive_file)\nHow's that?\n. Think docker container name & labels are now in the prometheus exposed metrics. Can this be closed?\n. There is a PR (#800) in for this but needs updating now that 0.9.5 of influxdb has been released with fewer client deps. Waiting on the contributor to update. If we don't get an update soon-ish, then we will be looking for someone else to pick up this contribution so volunteers welcome.\n. @carmark Would you be able to ask the contributor on #800 if they'd like some help?\n. We should be able to get all info from a docker inspect rather than parsing the container config file. Seems like mounting /var/lib/docker is causing more trouble than it's worth.\n. :+1: to node exporter plus cadvisor. That's what we use & very happy with that combo.\n. Would really like to see this merged, although only want the image name personally.\n. Image name added in #869.\n. @scalp42 If you can rebase I'll try to find time to review tomorrow. I have no security concerns now that you have added the whitelist for env vars.\n. Fixed by #780.\n. Personally I'd rather wait until dependencies are reduced with 0.9.5. cadvisor is embedded inside a number of other systems, including kubernetes, & the amount of dependencies could very well be a limiting factor in upgrading there.\nDoes anyone know of a timescale for influxdb 0.9.5?\n. @onlyjob Any chance of updating this with latest influxdb client changes?\n. @onlyjob Sorry - copy/pasted wrong person. Ignore me!\n@SlashmanX It should.\n@mnuessler Now that influxdb 0.9.5 is released could you update godeps please?\n. Thanks!\n. Thanks for the update.\nTravis showing failing gofmt check:\nThe following files are not properly formatted:\nstorage/influxdb/influxdb.go storage/influxdb/influxdb.go\n. Let me pull this down locally & submit a separate PR tomorrow with these changes & any necessary fixes.\n@marcellodesales @carmark Feel free to do code review if you have time.\n. This has now been superseded by #1040, including the commits from this PR & cleaning up a few things (including missing godeps). Closing this but please review/comment on #1040 so we can get that merged ASAP.\n. How should the non-docker cgroup parent be handled in FullContainerName?\n. @basvdlei Are you still working on this PR? If not do you mind if I pick this up & swap over the regex? Would really like to get this in - nice optimisation.\n. @vishh LGTM but need to retest - not sure if i'm in the admin list in Jenkins to request that with a comment here?\n. LGTM - thanks!\n. @vmarmol The containers are shown under Docker containers so they're definitely being detected as Docker containers. FYI these containers are managed by Kubernetes, well OpenShift 3, so cadvisor is embedded in the Kubernetes/OpenShift binary. I've tried with latest cadvisor release (0.16) separately too & have the same problem so I believe it is related to cadvisor rather than Kubernetes/OpenShift's embedded version,\nHere's the output of /validate:\n```\ncAdvisor version: 0.16.0\nOS version: Fedora 22 (Twenty Two)\nKernel version: [Supported and recommended]\n    Kernel version is 4.0.8-300.fc22.x86_64. Versions >= 2.6 are supported. 3.0+ are recommended.\nCgroup setup: [Supported and recommended]\n    Available cgroups: map[devices:1 net_cls:1 perf_event:1 net_prio:1 cpuset:1 cpuacct:1 blkio:1 hugetlb:1 cpu:1 memory:1 freezer:1]\n    Following cgroups are required: [cpu cpuacct]\n    Following other cgroups are recommended: [memory blkio cpuset devices freezer]\n    Hierarchical memory accounting enabled. Reported memory usage includes memory used by child containers.\nCgroup mount setup: [Supported and recommended]\n    Cgroups are mounted at /sys/fs/cgroup.\n    Cgroup mount directories: blkio cpu cpu,cpuacct cpuacct cpuset devices freezer hugetlb memory net_cls net_cls,net_prio net_prio perf_event systemd \n    Any cgroup mount point that is detectible and accessible is supported. /sys/fs/cgroup is recommended as a standard location.\n    Cgroup mounts:\n    cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0\n    cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0\nDocker version: [Supported and recommended]\n    Docker version is 1.7.0.fc22. Versions >= 1.0 are supported. 1.2+ are recommended.\nDocker driver setup: [Supported and recommended]\n    Docker exec driver is native-0.2. Storage driver is devicemapper.\n    systemd is being used to create cgroups.\n    Docker container state directory is at \"/var/lib/docker/containers\" and is accessible.\nBlock device setup: [Supported and recommended]\n    At least one device supports 'cfq' I/O scheduler. Some disk stats can be reported.\n     Disk \"dm-6\" Scheduler type \"none\".\n     Disk \"dm-8\" Scheduler type \"none\".\n     Disk \"sda\" Scheduler type \"cfq\".\n     Disk \"dm-10\" Scheduler type \"none\".\n     Disk \"dm-2\" Scheduler type \"none\".\n     Disk \"dm-3\" Scheduler type \"none\".\n     Disk \"dm-5\" Scheduler type \"none\".\n     Disk \"dm-7\" Scheduler type \"none\".\n     Disk \"dm-9\" Scheduler type \"none\".\n     Disk \"dm-0\" Scheduler type \"none\".\n     Disk \"dm-1\" Scheduler type \"none\".\n     Disk \"dm-4\" Scheduler type \"none\".\nInotify watches: \n    /system.slice/dev-dm\\x2d3.swap:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/dev-dm\\x2d3.swap\n        /sys/fs/cgroup/memory/system.slice/dev-dm\\x2d3.swap\n        /sys/fs/cgroup/blkio/system.slice/dev-dm\\x2d3.swap\n    /system.slice/lvm2-lvmetad.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/lvm2-lvmetad.service\n        /sys/fs/cgroup/memory/system.slice/lvm2-lvmetad.service\n        /sys/fs/cgroup/blkio/system.slice/lvm2-lvmetad.service\n    /system.slice/system-systemd\\x2dfsck.slice:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/system-systemd\\x2dfsck.slice\n        /sys/fs/cgroup/memory/system.slice/system-systemd\\x2dfsck.slice\n        /sys/fs/cgroup/blkio/system.slice/system-systemd\\x2dfsck.slice\n    /system.slice/system-systemd\\x2drfkill.slice:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/system-systemd\\x2drfkill.slice\n        /sys/fs/cgroup/memory/system.slice/system-systemd\\x2drfkill.slice\n        /sys/fs/cgroup/blkio/system.slice/system-systemd\\x2drfkill.slice\n    /system.slice/netcf-transaction.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/netcf-transaction.service\n        /sys/fs/cgroup/memory/system.slice/netcf-transaction.service\n        /sys/fs/cgroup/blkio/system.slice/netcf-transaction.service\n    /system.slice/nfs-mountd.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/nfs-mountd.service\n        /sys/fs/cgroup/memory/system.slice/nfs-mountd.service\n        /sys/fs/cgroup/blkio/system.slice/nfs-mountd.service\n    /system.slice/run-user-995.mount:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/run-user-995.mount\n        /sys/fs/cgroup/memory/system.slice/run-user-995.mount\n        /sys/fs/cgroup/blkio/system.slice/run-user-995.mount\n    /:\n        /sys/fs/cgroup/cpu,cpuacct\n        /sys/fs/cgroup/cpuset\n        /sys/fs/cgroup/memory\n        /sys/fs/cgroup/blkio\n    /system.slice/abrt-xorg.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/abrt-xorg.service\n        /sys/fs/cgroup/memory/system.slice/abrt-xorg.service\n        /sys/fs/cgroup/blkio/system.slice/abrt-xorg.service\n    /system.slice/var-lib-nfs-rpc_pipefs.mount:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/var-lib-nfs-rpc_pipefs.mount\n        /sys/fs/cgroup/memory/system.slice/var-lib-nfs-rpc_pipefs.mount\n        /sys/fs/cgroup/blkio/system.slice/var-lib-nfs-rpc_pipefs.mount\n    /system.slice/avahi-daemon.service:\n        /sys/fs/cgroup/blkio/system.slice/avahi-daemon.service\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/avahi-daemon.service\n        /sys/fs/cgroup/memory/system.slice/avahi-daemon.service\n    /system.slice/systemd-remount-fs.service:\n        /sys/fs/cgroup/memory/system.slice/systemd-remount-fs.service\n        /sys/fs/cgroup/blkio/system.slice/systemd-remount-fs.service\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/systemd-remount-fs.service\n    /system.slice/systemd-udevd.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/systemd-udevd.service\n        /sys/fs/cgroup/memory/system.slice/systemd-udevd.service\n        /sys/fs/cgroup/blkio/system.slice/systemd-udevd.service\n    /system.slice/colord.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/colord.service\n        /sys/fs/cgroup/memory/system.slice/colord.service\n        /sys/fs/cgroup/blkio/system.slice/colord.service\n    /system.slice/kmod-static-nodes.service:\n        /sys/fs/cgroup/blkio/system.slice/kmod-static-nodes.service\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/kmod-static-nodes.service\n        /sys/fs/cgroup/memory/system.slice/kmod-static-nodes.service\n    /system.slice/docker.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/docker.service\n        /sys/fs/cgroup/memory/system.slice/docker.service\n        /sys/fs/cgroup/blkio/system.slice/docker.service\n    /system.slice/-.mount:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/-.mount\n        /sys/fs/cgroup/memory/system.slice/-.mount\n        /sys/fs/cgroup/blkio/system.slice/-.mount\n    /system.slice/systemd-journal-flush.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/systemd-journal-flush.service\n        /sys/fs/cgroup/memory/system.slice/systemd-journal-flush.service\n        /sys/fs/cgroup/blkio/system.slice/systemd-journal-flush.service\n    /system.slice/systemd-user-sessions.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/systemd-user-sessions.service\n        /sys/fs/cgroup/memory/system.slice/systemd-user-sessions.service\n        /sys/fs/cgroup/blkio/system.slice/systemd-user-sessions.service\n    /system.slice/systemd-update-utmp.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/systemd-update-utmp.service\n        /sys/fs/cgroup/memory/system.slice/systemd-update-utmp.service\n        /sys/fs/cgroup/blkio/system.slice/systemd-update-utmp.service\n    /system.slice/home.mount:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/home.mount\n        /sys/fs/cgroup/memory/system.slice/home.mount\n        /sys/fs/cgroup/blkio/system.slice/home.mount\n    /system.slice/dracut-shutdown.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/dracut-shutdown.service\n        /sys/fs/cgroup/memory/system.slice/dracut-shutdown.service\n        /sys/fs/cgroup/blkio/system.slice/dracut-shutdown.service\n    /system.slice/nfs-server.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/nfs-server.service\n        /sys/fs/cgroup/memory/system.slice/nfs-server.service\n        /sys/fs/cgroup/blkio/system.slice/nfs-server.service\n    /system.slice/dev-disk-by\\x2duuid-657dff8a\\x2de602\\x2d41fb\\x2dac66\\x2d845c2e038d2f.swap:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/dev-disk-by\\x2duuid-657dff8a\\x2de602\\x2d41fb\\x2dac66\\x2d845c2e038d2f.swap\n        /sys/fs/cgroup/memory/system.slice/dev-disk-by\\x2duuid-657dff8a\\x2de602\\x2d41fb\\x2dac66\\x2d845c2e038d2f.swap\n        /sys/fs/cgroup/blkio/system.slice/dev-disk-by\\x2duuid-657dff8a\\x2de602\\x2d41fb\\x2dac66\\x2d845c2e038d2f.swap\n    /system.slice/nfs-config.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/nfs-config.service\n        /sys/fs/cgroup/memory/system.slice/nfs-config.service\n        /sys/fs/cgroup/blkio/system.slice/nfs-config.service\n    /user.slice:\n        /sys/fs/cgroup/blkio/user.slice\n        /sys/fs/cgroup/cpu,cpuacct/user.slice\n        /sys/fs/cgroup/memory/user.slice\n    /system.slice/dev-disk-by\\x2did-dm\\x2duuid\\x2dCRYPT\\x2dLUKS1\\x2da0c2c92c6cb04c4a90dd6f8198a9454d\\x2dluks\\x2da0c2c92c\\x2d6cb0\\x2d4c4a\\x2d90dd\\x2d6f8198a9454d.swap:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/dev-disk-by\\x2did-dm\\x2duuid\\x2dCRYPT\\x2dLUKS1\\x2da0c2c92c6cb04c4a90dd6f8198a9454d\\x2dluks\\x2da0c2c92c\\x2d6cb0\\x2d4c4a\\x2d90dd\\x2d6f8198a9454d.swap\n        /sys/fs/cgroup/memory/system.slice/dev-disk-by\\x2did-dm\\x2duuid\\x2dCRYPT\\x2dLUKS1\\x2da0c2c92c6cb04c4a90dd6f8198a9454d\\x2dluks\\x2da0c2c92c\\x2d6cb0\\x2d4c4a\\x2d90dd\\x2d6f8198a9454d.swap\n        /sys/fs/cgroup/blkio/system.slice/dev-disk-by\\x2did-dm\\x2duuid\\x2dCRYPT\\x2dLUKS1\\x2da0c2c92c6cb04c4a90dd6f8198a9454d\\x2dluks\\x2da0c2c92c\\x2d6cb0\\x2d4c4a\\x2d90dd\\x2d6f8198a9454d.swap\n    /system.slice/systemd-udev-settle.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/systemd-udev-settle.service\n        /sys/fs/cgroup/memory/system.slice/systemd-udev-settle.service\n        /sys/fs/cgroup/blkio/system.slice/systemd-udev-settle.service\n    /system.slice/rtkit-daemon.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/rtkit-daemon.service\n        /sys/fs/cgroup/memory/system.slice/rtkit-daemon.service\n        /sys/fs/cgroup/blkio/system.slice/rtkit-daemon.service\n    /system.slice/ModemManager.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/ModemManager.service\n        /sys/fs/cgroup/memory/system.slice/ModemManager.service\n        /sys/fs/cgroup/blkio/system.slice/ModemManager.service\n    /system.slice/gssproxy.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/gssproxy.service\n        /sys/fs/cgroup/memory/system.slice/gssproxy.service\n        /sys/fs/cgroup/blkio/system.slice/gssproxy.service\n    /system.slice/dev-hugepages.mount:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/dev-hugepages.mount\n        /sys/fs/cgroup/memory/system.slice/dev-hugepages.mount\n        /sys/fs/cgroup/blkio/system.slice/dev-hugepages.mount\n    /system.slice/abrt-ccpp.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/abrt-ccpp.service\n        /sys/fs/cgroup/memory/system.slice/abrt-ccpp.service\n        /sys/fs/cgroup/blkio/system.slice/abrt-ccpp.service\n    /system.slice/var-lib-openshift-openshift.local.volumes-pods-bcb00f50\\x2d2b05\\x2d11e5\\x2da2d4\\x2d54ee7527188d-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2da2sfj.mount:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/var-lib-openshift-openshift.local.volumes-pods-bcb00f50\\x2d2b05\\x2d11e5\\x2da2d4\\x2d54ee7527188d-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2da2sfj.mount\n        /sys/fs/cgroup/memory/system.slice/var-lib-openshift-openshift.local.volumes-pods-bcb00f50\\x2d2b05\\x2d11e5\\x2da2d4\\x2d54ee7527188d-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2da2sfj.mount\n        /sys/fs/cgroup/blkio/system.slice/var-lib-openshift-openshift.local.volumes-pods-bcb00f50\\x2d2b05\\x2d11e5\\x2da2d4\\x2d54ee7527188d-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2da2sfj.mount\n    /system.slice/proc-fs-nfsd.mount:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/proc-fs-nfsd.mount\n        /sys/fs/cgroup/memory/system.slice/proc-fs-nfsd.mount\n        /sys/fs/cgroup/blkio/system.slice/proc-fs-nfsd.mount\n    /system.slice/plexmediaserver.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/plexmediaserver.service\n        /sys/fs/cgroup/memory/system.slice/plexmediaserver.service\n        /sys/fs/cgroup/blkio/system.slice/plexmediaserver.service\n    /system.slice/rpc-statd.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/rpc-statd.service\n        /sys/fs/cgroup/memory/system.slice/rpc-statd.service\n        /sys/fs/cgroup/blkio/system.slice/rpc-statd.service\n    /system.slice/irqbalance.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/irqbalance.service\n        /sys/fs/cgroup/memory/system.slice/irqbalance.service\n        /sys/fs/cgroup/blkio/system.slice/irqbalance.service\n    /system.slice/abrt-oops.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/abrt-oops.service\n        /sys/fs/cgroup/memory/system.slice/abrt-oops.service\n        /sys/fs/cgroup/blkio/system.slice/abrt-oops.service\n    /system.slice/alsa-state.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/alsa-state.service\n        /sys/fs/cgroup/memory/system.slice/alsa-state.service\n        /sys/fs/cgroup/blkio/system.slice/alsa-state.service\n    /system.slice/iscsi-shutdown.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/iscsi-shutdown.service\n        /sys/fs/cgroup/memory/system.slice/iscsi-shutdown.service\n        /sys/fs/cgroup/blkio/system.slice/iscsi-shutdown.service\n    /system.slice/boot.mount:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/boot.mount\n        /sys/fs/cgroup/memory/system.slice/boot.mount\n        /sys/fs/cgroup/blkio/system.slice/boot.mount\n    /system.slice/sysstat.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/sysstat.service\n        /sys/fs/cgroup/memory/system.slice/sysstat.service\n        /sys/fs/cgroup/blkio/system.slice/sysstat.service\n    /system.slice/upower.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/upower.service\n        /sys/fs/cgroup/memory/system.slice/upower.service\n        /sys/fs/cgroup/blkio/system.slice/upower.service\n    /system.slice/system-lvm2\\x2dpvscan.slice:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/system-lvm2\\x2dpvscan.slice\n        /sys/fs/cgroup/memory/system.slice/system-lvm2\\x2dpvscan.slice\n        /sys/fs/cgroup/blkio/system.slice/system-lvm2\\x2dpvscan.slice\n    /system.slice/dev-disk-by\\x2did-dm\\x2dname\\x2dluks\\x2da0c2c92c\\x2d6cb0\\x2d4c4a\\x2d90dd\\x2d6f8198a9454d.swap:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/dev-disk-by\\x2did-dm\\x2dname\\x2dluks\\x2da0c2c92c\\x2d6cb0\\x2d4c4a\\x2d90dd\\x2d6f8198a9454d.swap\n        /sys/fs/cgroup/memory/system.slice/dev-disk-by\\x2did-dm\\x2dname\\x2dluks\\x2da0c2c92c\\x2d6cb0\\x2d4c4a\\x2d90dd\\x2d6f8198a9454d.swap\n        /sys/fs/cgroup/blkio/system.slice/dev-disk-by\\x2did-dm\\x2dname\\x2dluks\\x2da0c2c92c\\x2d6cb0\\x2d4c4a\\x2d90dd\\x2d6f8198a9454d.swap\n    /system.slice/akmods-shutdown.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/akmods-shutdown.service\n        /sys/fs/cgroup/memory/system.slice/akmods-shutdown.service\n        /sys/fs/cgroup/blkio/system.slice/akmods-shutdown.service\n    /system.slice/wpa_supplicant.service:\n        /sys/fs/cgroup/memory/system.slice/wpa_supplicant.service\n        /sys/fs/cgroup/blkio/system.slice/wpa_supplicant.service\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/wpa_supplicant.service\n    /system.slice/sys-kernel-debug.mount:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/sys-kernel-debug.mount\n        /sys/fs/cgroup/memory/system.slice/sys-kernel-debug.mount\n        /sys/fs/cgroup/blkio/system.slice/sys-kernel-debug.mount\n    /system.slice/dev-mqueue.mount:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/dev-mqueue.mount\n        /sys/fs/cgroup/memory/system.slice/dev-mqueue.mount\n        /sys/fs/cgroup/blkio/system.slice/dev-mqueue.mount\n    /kubelet:\n        /sys/fs/cgroup/cpuset/kubelet\n        /sys/fs/cgroup/memory/kubelet\n        /sys/fs/cgroup/blkio/kubelet\n        /sys/fs/cgroup/cpu,cpuacct/kubelet\n    /system.slice/tmp.mount:\n        /sys/fs/cgroup/blkio/system.slice/tmp.mount\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/tmp.mount\n        /sys/fs/cgroup/memory/system.slice/tmp.mount\n    /system.slice/dev-mapper-luks\\x2da0c2c92c\\x2d6cb0\\x2d4c4a\\x2d90dd\\x2d6f8198a9454d.swap:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/dev-mapper-luks\\x2da0c2c92c\\x2d6cb0\\x2d4c4a\\x2d90dd\\x2d6f8198a9454d.swap\n        /sys/fs/cgroup/memory/system.slice/dev-mapper-luks\\x2da0c2c92c\\x2d6cb0\\x2d4c4a\\x2d90dd\\x2d6f8198a9454d.swap\n        /sys/fs/cgroup/blkio/system.slice/dev-mapper-luks\\x2da0c2c92c\\x2d6cb0\\x2d4c4a\\x2d90dd\\x2d6f8198a9454d.swap\n    /machine.slice:\n        /sys/fs/cgroup/cpu,cpuacct/machine.slice\n        /sys/fs/cgroup/memory/machine.slice\n        /sys/fs/cgroup/blkio/machine.slice\n    /system.slice/system-getty.slice:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/system-getty.slice\n        /sys/fs/cgroup/memory/system.slice/system-getty.slice\n        /sys/fs/cgroup/blkio/system.slice/system-getty.slice\n    /system.slice/fedora-readonly.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/fedora-readonly.service\n        /sys/fs/cgroup/memory/system.slice/fedora-readonly.service\n        /sys/fs/cgroup/blkio/system.slice/fedora-readonly.service\n    /system.slice/jexec.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/jexec.service\n        /sys/fs/cgroup/memory/system.slice/jexec.service\n        /sys/fs/cgroup/blkio/system.slice/jexec.service\n    /system.slice/lvm2-monitor.service:\n        /sys/fs/cgroup/memory/system.slice/lvm2-monitor.service\n        /sys/fs/cgroup/blkio/system.slice/lvm2-monitor.service\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/lvm2-monitor.service\n    /system.slice/systemd-fsck-root.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/systemd-fsck-root.service\n        /sys/fs/cgroup/memory/system.slice/systemd-fsck-root.service\n        /sys/fs/cgroup/blkio/system.slice/systemd-fsck-root.service\n    /system.slice/docker-5b881011f3e7efdae20491f386f50d4eac7355296ffcb5146a2b4d3c936a9430.scope:\n        /sys/fs/cgroup/cpuset/system.slice/docker-5b881011f3e7efdae20491f386f50d4eac7355296ffcb5146a2b4d3c936a9430.scope\n        /sys/fs/cgroup/memory/system.slice/docker-5b881011f3e7efdae20491f386f50d4eac7355296ffcb5146a2b4d3c936a9430.scope\n        /sys/fs/cgroup/blkio/system.slice/docker-5b881011f3e7efdae20491f386f50d4eac7355296ffcb5146a2b4d3c936a9430.scope\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/docker-5b881011f3e7efdae20491f386f50d4eac7355296ffcb5146a2b4d3c936a9430.scope\n    /system.slice/run-user-1000.mount:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/run-user-1000.mount\n        /sys/fs/cgroup/memory/system.slice/run-user-1000.mount\n        /sys/fs/cgroup/blkio/system.slice/run-user-1000.mount\n    /system.slice/NetworkManager.service:\n        /sys/fs/cgroup/memory/system.slice/NetworkManager.service\n        /sys/fs/cgroup/blkio/system.slice/NetworkManager.service\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/NetworkManager.service\n    /system.slice/nfs-idmapd.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/nfs-idmapd.service\n        /sys/fs/cgroup/memory/system.slice/nfs-idmapd.service\n        /sys/fs/cgroup/blkio/system.slice/nfs-idmapd.service\n    /system.slice/mcelog.service:\n        /sys/fs/cgroup/blkio/system.slice/mcelog.service\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/mcelog.service\n        /sys/fs/cgroup/memory/system.slice/mcelog.service\n    /system.slice/system-systemd\\x2dcryptsetup.slice:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/system-systemd\\x2dcryptsetup.slice\n        /sys/fs/cgroup/memory/system.slice/system-systemd\\x2dcryptsetup.slice\n        /sys/fs/cgroup/blkio/system.slice/system-systemd\\x2dcryptsetup.slice\n    /system.slice:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice\n        /sys/fs/cgroup/cpuset/system.slice\n        /sys/fs/cgroup/memory/system.slice\n        /sys/fs/cgroup/blkio/system.slice\n    /system.slice/abrtd.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/abrtd.service\n        /sys/fs/cgroup/memory/system.slice/abrtd.service\n        /sys/fs/cgroup/blkio/system.slice/abrtd.service\n    /system.slice/rngd.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/rngd.service\n        /sys/fs/cgroup/memory/system.slice/rngd.service\n        /sys/fs/cgroup/blkio/system.slice/rngd.service\n    /system.slice/livesys-late.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/livesys-late.service\n        /sys/fs/cgroup/memory/system.slice/livesys-late.service\n        /sys/fs/cgroup/blkio/system.slice/livesys-late.service\n    /system.slice/systemd-journald.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/systemd-journald.service\n        /sys/fs/cgroup/memory/system.slice/systemd-journald.service\n        /sys/fs/cgroup/blkio/system.slice/systemd-journald.service\n    /system.slice/docker-b2753c8b050cf66cfe3c1a21b28c15da4df85046c5500444447b1f72f86ab477.scope:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/docker-b2753c8b050cf66cfe3c1a21b28c15da4df85046c5500444447b1f72f86ab477.scope\n        /sys/fs/cgroup/cpuset/system.slice/docker-b2753c8b050cf66cfe3c1a21b28c15da4df85046c5500444447b1f72f86ab477.scope\n        /sys/fs/cgroup/memory/system.slice/docker-b2753c8b050cf66cfe3c1a21b28c15da4df85046c5500444447b1f72f86ab477.scope\n        /sys/fs/cgroup/blkio/system.slice/docker-b2753c8b050cf66cfe3c1a21b28c15da4df85046c5500444447b1f72f86ab477.scope\n    /system.slice/libvirtd.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/libvirtd.service\n        /sys/fs/cgroup/memory/system.slice/libvirtd.service\n        /sys/fs/cgroup/blkio/system.slice/libvirtd.service\n    /system.slice/systemd-modules-load.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/systemd-modules-load.service\n        /sys/fs/cgroup/memory/system.slice/systemd-modules-load.service\n        /sys/fs/cgroup/blkio/system.slice/systemd-modules-load.service\n    /docker-daemon:\n        /sys/fs/cgroup/memory/docker-daemon\n        /sys/fs/cgroup/blkio/docker-daemon\n        /sys/fs/cgroup/cpu,cpuacct/docker-daemon\n        /sys/fs/cgroup/cpuset/docker-daemon\n    /system.slice/systemd-udev-trigger.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/systemd-udev-trigger.service\n        /sys/fs/cgroup/memory/system.slice/systemd-udev-trigger.service\n        /sys/fs/cgroup/blkio/system.slice/systemd-udev-trigger.service\n    /system.slice/systemd-logind.service:\n        /sys/fs/cgroup/memory/system.slice/systemd-logind.service\n        /sys/fs/cgroup/blkio/system.slice/systemd-logind.service\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/systemd-logind.service\n    /system.slice/systemd-sysctl.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/systemd-sysctl.service\n        /sys/fs/cgroup/memory/system.slice/systemd-sysctl.service\n        /sys/fs/cgroup/blkio/system.slice/systemd-sysctl.service\n    /system.slice/fedora-import-state.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/fedora-import-state.service\n        /sys/fs/cgroup/memory/system.slice/fedora-import-state.service\n        /sys/fs/cgroup/blkio/system.slice/fedora-import-state.service\n    /system.slice/proc-sys-fs-binfmt_misc.mount:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/proc-sys-fs-binfmt_misc.mount\n        /sys/fs/cgroup/memory/system.slice/proc-sys-fs-binfmt_misc.mount\n        /sys/fs/cgroup/blkio/system.slice/proc-sys-fs-binfmt_misc.mount\n    /system.slice/livesys.service:\n        /sys/fs/cgroup/blkio/system.slice/livesys.service\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/livesys.service\n        /sys/fs/cgroup/memory/system.slice/livesys.service\n    /system.slice/acpid.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/acpid.service\n        /sys/fs/cgroup/memory/system.slice/acpid.service\n        /sys/fs/cgroup/blkio/system.slice/acpid.service\n    /system.slice/atd.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/atd.service\n        /sys/fs/cgroup/memory/system.slice/atd.service\n        /sys/fs/cgroup/blkio/system.slice/atd.service\n    /system.slice/rsyslog.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/rsyslog.service\n        /sys/fs/cgroup/memory/system.slice/rsyslog.service\n        /sys/fs/cgroup/blkio/system.slice/rsyslog.service\n    /system.slice/dbus.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/dbus.service\n        /sys/fs/cgroup/memory/system.slice/dbus.service\n        /sys/fs/cgroup/blkio/system.slice/dbus.service\n    /system.slice/bluetooth.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/bluetooth.service\n        /sys/fs/cgroup/memory/system.slice/bluetooth.service\n        /sys/fs/cgroup/blkio/system.slice/bluetooth.service\n    /system.slice/accounts-daemon.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/accounts-daemon.service\n        /sys/fs/cgroup/memory/system.slice/accounts-daemon.service\n        /sys/fs/cgroup/blkio/system.slice/accounts-daemon.service\n    /system.slice/systemd-tmpfiles-setup.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/systemd-tmpfiles-setup.service\n        /sys/fs/cgroup/memory/system.slice/systemd-tmpfiles-setup.service\n        /sys/fs/cgroup/blkio/system.slice/systemd-tmpfiles-setup.service\n    /system.slice/akmods.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/akmods.service\n        /sys/fs/cgroup/memory/system.slice/akmods.service\n        /sys/fs/cgroup/blkio/system.slice/akmods.service\n    /system.slice/crond.service:\n        /sys/fs/cgroup/memory/system.slice/crond.service\n        /sys/fs/cgroup/blkio/system.slice/crond.service\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/crond.service\n    /system.slice/fprintd.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/fprintd.service\n        /sys/fs/cgroup/memory/system.slice/fprintd.service\n        /sys/fs/cgroup/blkio/system.slice/fprintd.service\n    /system.slice/squid.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/squid.service\n        /sys/fs/cgroup/memory/system.slice/squid.service\n        /sys/fs/cgroup/blkio/system.slice/squid.service\n    /system.slice/lightdm.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/lightdm.service\n        /sys/fs/cgroup/memory/system.slice/lightdm.service\n        /sys/fs/cgroup/blkio/system.slice/lightdm.service\n    /system.slice/polkit.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/polkit.service\n        /sys/fs/cgroup/memory/system.slice/polkit.service\n        /sys/fs/cgroup/blkio/system.slice/polkit.service\n    /system.slice/smartd.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/smartd.service\n        /sys/fs/cgroup/memory/system.slice/smartd.service\n        /sys/fs/cgroup/blkio/system.slice/smartd.service\n    /system.slice/firewalld.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/firewalld.service\n        /sys/fs/cgroup/memory/system.slice/firewalld.service\n        /sys/fs/cgroup/blkio/system.slice/firewalld.service\n    /system.slice/auditd.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/auditd.service\n        /sys/fs/cgroup/memory/system.slice/auditd.service\n        /sys/fs/cgroup/blkio/system.slice/auditd.service\n    /system.slice/chronyd.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/chronyd.service\n        /sys/fs/cgroup/memory/system.slice/chronyd.service\n        /sys/fs/cgroup/blkio/system.slice/chronyd.service\n    /system.slice/systemd-random-seed.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/systemd-random-seed.service\n        /sys/fs/cgroup/memory/system.slice/systemd-random-seed.service\n        /sys/fs/cgroup/blkio/system.slice/systemd-random-seed.service\n    /system.slice/systemd-vconsole-setup.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/systemd-vconsole-setup.service\n        /sys/fs/cgroup/memory/system.slice/systemd-vconsole-setup.service\n        /sys/fs/cgroup/blkio/system.slice/systemd-vconsole-setup.service\n    /system.slice/rpcbind.service:\n        /sys/fs/cgroup/blkio/system.slice/rpcbind.service\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/rpcbind.service\n        /sys/fs/cgroup/memory/system.slice/rpcbind.service\n    /system.slice/cups.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/cups.service\n        /sys/fs/cgroup/memory/system.slice/cups.service\n        /sys/fs/cgroup/blkio/system.slice/cups.service\n    /system.slice/sys-kernel-config.mount:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/sys-kernel-config.mount\n        /sys/fs/cgroup/memory/system.slice/sys-kernel-config.mount\n        /sys/fs/cgroup/blkio/system.slice/sys-kernel-config.mount\n    /system.slice/systemd-tmpfiles-setup-dev.service:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/systemd-tmpfiles-setup-dev.service\n        /sys/fs/cgroup/memory/system.slice/systemd-tmpfiles-setup-dev.service\n        /sys/fs/cgroup/blkio/system.slice/systemd-tmpfiles-setup-dev.service\n    /system.slice/system-systemd\\x2dbacklight.slice:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice/system-systemd\\x2dbacklight.slice\n        /sys/fs/cgroup/memory/system.slice/system-systemd\\x2dbacklight.slice\n        /sys/fs/cgroup/blkio/system.slice/system-systemd\\x2dbacklight.slice\nManaged containers: \n    /system.slice/upower.service\n    /system.slice/proc-fs-nfsd.mount\n    /system.slice/system-systemd\\x2dbacklight.slice\n    /kubelet\n    /system.slice/boot.mount\n    /system.slice/gssproxy.service\n    /system.slice/docker-b2753c8b050cf66cfe3c1a21b28c15da4df85046c5500444447b1f72f86ab477.scope\n        Namespace: docker\n        Aliases:\n            k8s_grafana2.52da1e9_grafana2-opuv3_default_bcb00f50-2b05-11e5-a2d4-54ee7527188d_8454404f\n            b2753c8b050cf66cfe3c1a21b28c15da4df85046c5500444447b1f72f86ab477\n    /system.slice/systemd-udevd.service\n    /system.slice/systemd-update-utmp.service\n    /system.slice/sysstat.service\n    /system.slice/ModemManager.service\n    /system.slice/system-systemd\\x2drfkill.slice\n    /system.slice/system-lvm2\\x2dpvscan.slice\n    /system.slice/dracut-shutdown.service\n    /system.slice/run-user-995.mount\n    /system.slice/systemd-logind.service\n    /system.slice/livesys.service\n    /system.slice/run-user-1000.mount\n    /system.slice/kmod-static-nodes.service\n    /system.slice/jexec.service\n    /system.slice/acpid.service\n    /docker-daemon\n    /system.slice/avahi-daemon.service\n    /system.slice/fedora-readonly.service\n    /system.slice/wpa_supplicant.service\n    /system.slice/nfs-idmapd.service\n    /system.slice/rsyslog.service\n    /system.slice/atd.service\n    /system.slice/systemd-sysctl.service\n    /system.slice/rpc-statd.service\n    /system.slice/dev-disk-by\\x2did-dm\\x2dname\\x2dluks\\x2da0c2c92c\\x2d6cb0\\x2d4c4a\\x2d90dd\\x2d6f8198a9454d.swap\n    /system.slice/systemd-modules-load.service\n    /system.slice/abrtd.service\n    /system.slice/sys-kernel-config.mount\n    /system.slice/colord.service\n    /system.slice/rtkit-daemon.service\n    /system.slice/proc-sys-fs-binfmt_misc.mount\n    /system.slice/abrt-oops.service\n    /system.slice/squid.service\n    /system.slice/dev-mapper-luks\\x2da0c2c92c\\x2d6cb0\\x2d4c4a\\x2d90dd\\x2d6f8198a9454d.swap\n    /system.slice/systemd-tmpfiles-setup-dev.service\n    /system.slice/system-getty.slice\n    /system.slice/dev-hugepages.mount\n    /system.slice/systemd-user-sessions.service\n    /system.slice/systemd-fsck-root.service\n    /system.slice\n    /system.slice/dev-disk-by\\x2duuid-657dff8a\\x2de602\\x2d41fb\\x2dac66\\x2d845c2e038d2f.swap\n    /system.slice/dev-dm\\x2d3.swap\n    /system.slice/lvm2-monitor.service\n    /system.slice/rpcbind.service\n    /system.slice/tmp.mount\n    /system.slice/crond.service\n    /system.slice/systemd-vconsole-setup.service\n    /system.slice/systemd-tmpfiles-setup.service\n    /system.slice/nfs-mountd.service\n    /system.slice/firewalld.service\n    /system.slice/var-lib-openshift-openshift.local.volumes-pods-bcb00f50\\x2d2b05\\x2d11e5\\x2da2d4\\x2d54ee7527188d-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2da2sfj.mount\n    /system.slice/abrt-xorg.service\n    /system.slice/fprintd.service\n    /system.slice/polkit.service\n    /system.slice/alsa-state.service\n    /system.slice/systemd-remount-fs.service\n    /system.slice/var-lib-nfs-rpc_pipefs.mount\n    /user.slice\n    /system.slice/docker-5b881011f3e7efdae20491f386f50d4eac7355296ffcb5146a2b4d3c936a9430.scope\n        Namespace: docker\n        Aliases:\n            k8s_POD.cad1e704_grafana2-opuv3_default_bcb00f50-2b05-11e5-a2d4-54ee7527188d_1649edfd\n            5b881011f3e7efdae20491f386f50d4eac7355296ffcb5146a2b4d3c936a9430\n    /system.slice/livesys-late.service\n    /system.slice/NetworkManager.service\n    /system.slice/home.mount\n    /system.slice/system-systemd\\x2dfsck.slice\n    /system.slice/netcf-transaction.service\n    /system.slice/auditd.service\n    /system.slice/akmods.service\n    /machine.slice\n    /system.slice/systemd-udev-settle.service\n    /system.slice/rngd.service\n    /system.slice/systemd-udev-trigger.service\n    /system.slice/sys-kernel-debug.mount\n    /system.slice/dev-disk-by\\x2did-dm\\x2duuid\\x2dCRYPT\\x2dLUKS1\\x2da0c2c92c6cb04c4a90dd6f8198a9454d\\x2dluks\\x2da0c2c92c\\x2d6cb0\\x2d4c4a\\x2d90dd\\x2d6f8198a9454d.swap\n    /system.slice/chronyd.service\n    /system.slice/dbus.service\n    /system.slice/systemd-journal-flush.service\n    /system.slice/akmods-shutdown.service\n    /system.slice/nfs-server.service\n    /system.slice/systemd-random-seed.service\n    /system.slice/plexmediaserver.service\n    /system.slice/lvm2-lvmetad.service\n    /system.slice/systemd-journald.service\n    /system.slice/libvirtd.service\n    /system.slice/nfs-config.service\n    /system.slice/irqbalance.service\n    /\n    /system.slice/dev-mqueue.mount\n    /system.slice/system-systemd\\x2dcryptsetup.slice\n    /system.slice/lightdm.service\n    /system.slice/cups.service\n    /system.slice/bluetooth.service\n    /system.slice/abrt-ccpp.service\n    /system.slice/docker.service\n    /system.slice/fedora-import-state.service\n    /system.slice/smartd.service\n    /system.slice/-.mount\n    /system.slice/accounts-daemon.service\n    /system.slice/iscsi-shutdown.service\n    /system.slice/mcelog.service\n``\n. Usingdocker stats` I can see disk I/O, I believe via libcontainer. Not sure if this is different to how cadvisor gathers its metrics?\nEven the infrastructure container in the pod shows no network traffic either - not sure why that is either.\n. This appears to only be a problem on Fedora 22 - I've tested it with Kubernetes on GKE & OpenShift 3 on RHEL & network stats are correctly reported on the infrastructure containers in pods.\nI wonder if there's some change introduced in fedora 22 that means network stats aren't reported in the same way? Could you point me to where I can look on the filesystem to see where cdvisor looks?\n. @vmarmol As this doesn't seem to be an isolated incident, is there anything more I can provide to help you identify the issue with this? \n. @lukaf What distro is that running on?\n@vmarmol Still seeing this problem now with docker 1.8.1 on fedora 22. Anything else I can do to help identify the issue?\n. Looking through the code, network stats require the state.json file for each container to be readable. In my install I can't find that file for each container, but I assume that is because I'm using devicemapper with thin LVM for docker rather than loopback devicemapper or aufs? I have no idea how to be able to use this to get the state.json file from the docker-meta logical volume where I assume it is stored? Any idea?\n. So more investigation... looks like this network stats are broken with Docker >= 1.7 since the introduction of libnetwork. I've tested this with vanilla binaries on Ubuntu to make sure it's not a Fedora specific thing.\n@vmarmol What is the oldest version of Docker that cadvisor has to support? Does cadvisor currently change behaviour between versions? If not, what do you think about adding this in to use libnetwork similar to https://github.com/docker/docker/blob/master/daemon/stats.go#L77-L118 from Docker?\n. @rjnagal I've had a brief look at that today... not as easy as it looks as you need to get the same libnetwork config as the docker daemon which I can't get a handle on. Will look again tomorrow.\n. @rjnagal One problem of using libnetwork is it has a dependency to later version of docker when docker network plugins were introduced. I'm not quite sure how to proceed without updating the docker dependency which I'm sure you don't want to do. Any thoughts?\n. @lukaf Need to update documentation. When running inside a container it has to run inside the pid namespace of the host (--pid=host). So try:\ndocker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro  --volume=/var/lib/docker/:/var/lib/docker:ro --publish=8080:8080 --detach=true --pid=host --name=cadvisor test/cadvisor\n. In Docker 1.7, libnetwork was introduced & changed how networking was done. This meant that all network stats from cadvisor for >= 1.7 were empty. The Docker recommended way to gather high resolution metrics is to setns into the container net ns to gather stats. This requires access to the host's pid ns.\nI'm not sure it's too much of a bad thing - discussed with @liggitt & @pmorie, albeit briefly. The only other option that I know of is to mount /proc into the container & that is likely to screw up all kinds of stuff in the container (if it's even possible - doubt it is tbh).\nIf there are any other options, happy to hear them & implement differently if better.\n. @lukaf Cool! We might revisit this for a 0.16.1 release if an alternative approach comes along, but it will keep working no matter what now :)\n. /proc/<pid>/root/sys/class/net/ won't be available unless we're running host pid namespace anyway so it's no better/safer when running in a container. Mounting proc will screw the cadvisor container up, if it's even possible. Could mount host /proc somewhere else in the container I guess but that doesn't feel nice to do.\nWhat do you mean \"via netlink\"? Perhaps that's an option but don't know enough about it.\n. @rjnagal If we're mounting the whole rootfs anyway what does it matter if we're running in pid ns? Privileged & mounting rootfs - pid ns isn't exactly reducing security & makes things more consistent between running natively & running in container by using setns to net ns.\n@smarterclayton Right but that still doesn't allow us to switch to other docker containers' net ns. Only way to do that that I know of is to run in host pid ns.\n. @smarterclayton Beyond the limits of my knowledge there :) Could you explain or point me somewhere to read up?\n. That's a good reason to change this then :)\n@rjnagal Let me update to use your approach of /proc/... files - will get it done now.\n. @rjnagal @smarterclayton Switched to /proc approach, works great. Nice idea @rjnagal! PR in - #870.\n. Could you please try a later version? 0.19.3 has been recently released.\n. @vishh Looking back looks like the e2e tests have failed on most PRs recently... although I would have expected this PR to fix those failures :)\nI have just tested it manually against coreos beta (the reported failure) & it works fine. I can't work out how to run the integration tests manually against a local VM - seems like the integration tests are coupled to GCE... not very friendly IMO. Any way to run these locally against a VM?\n. Right:\n\"Today We only support remote execution in Google Compute Engine since that is where we run our continuous builds.\"\nWorking on making it a bit more flexible - would you be open to that? I don't have a GCE account to run integration tests myself.\n. @vishh I've just run integration tests against coreos beta on GCE & it worked fine. Any chance you could retest?\n. Thanks @vishh.\nDon't suppose there's any chance we could get a release cut with this fix to update deps in kubernetes & openshift?\n. Ah OK so no need to tag cadvisor first? Just use a non tagged commit in kubernetes godeps? As long as it's in next release whenever that is I'm happy.\n. Great thanks!\n. @vishh One thing I have noticed in further (post-merge ;)) testing is when running in a Docker container cadvisor needs to be in the hosts pid namespace (see https://docs.docker.com/reference/run/#pid-settings-pid) to be able to switch network namespaces based on pid of the docker containers. I don't see this as a problem but something that needs to be documented. Shall I do that?\n. No we don't - this is how docker works, switching namespaces. Switching namespaces is cheap, no?\n. This is the approach of libnetwork & how docker itself describes how to do high performance metrics collection - see https://docs.docker.com/articles/runmetrics/#tips-for-high-performance-metric-collection.\n. I think we can use this technique for all versions of docker & get rid of all the veth handling code.\nI'm not sure what you mean by network setup agnostic? Aren't we? Shouldn't we be?\n. @vishh As discussed in #865, this now switches namespace for any processes, including the cadvisor process itself to get network metrics.\n. @rjnagal I was following the recommendation from Docker itself - https://docs.docker.com/articles/runmetrics/#tips-for-high-performance-metric-collection. This is also the approach of libnetwork which is where most of the code is borrowed from.\nI have put together a benchmark for this vs straight proc fs without setns - results show that the overhead of switching ns is negligible:\n```\ngo test -bench=. -benchmem -benchtime=30s\ntesting: warning: no tests to run\nPASS\nBenchmarkSetNs    100000            464509 ns/op           14876 B/op        163 allocs/op\nBenchmarkSysFs    100000            459523 ns/op           14735 B/op        157 allocs/op\nok      github.com/jimmidyson/benchmark-setns   122.377s\n```\nGisted the benchmark code & results at https://gist.github.com/jimmidyson/81448529975bc5466bb4.\n. Closing in favour of @rjnagal's approach in #870.\n. Thanks!\nAgree on env vars - dangerous considering they're used for generated passwords, secret data, etc.\n. Looks like there's something going wrong with the e2e test as the error is unrelated to this PR:\nThe following files are not properly formatted:\nmetrics/prometheus_test.go\nIs related to my other PR #871. I'd suggest you add a Clean after checkout additional step in  your source code management section of the Jenkins job config so that new files from other PRs don't accidentally get included in unrelated PR builds.\n. Totally agree it's weird to embed the js/CSS in the go files manually but embedding the static content in the final binary is something we would want to keep to make distribution easy.  go bindata or rice would be much better ways to do this. Do you fancy submitting a PR to fix this?\n. Embedding the files in the binary means you don't have to worry about packaging debs/rpms/etc. You can just ship the binary & that is portable including static resources. This is a pretty common thing to do in the go world.\nTotally agree that the way cadvisor currently does this needs improvement. I'll see if I get time to take a look.\n. While taking a quick look at this (go rice looks promising btw) I noticed that locally hosting the JS for the charting library breaks Google's ToS: see both https://developers.google.com/chart/interactive/faq#offline & https://developers.google.com/chart/interactive/faq#localdownload.\n@vishh Thoughts? Do we need to remove the local copies of gcharts & jsapi so as not to break Google's (your employer's) ToS in one of their own OSS projects?\n. LGTM\n. Thanks @Snorch!\n. This is trying to achieve the same as #821. #821 also handles the case where a container is started with a different cgroup parent other than docker so prefer that PR over this.\n. @whosthatknocking Happy for me to close this & get #821 merged once it's fixed up the cgroup parent stuff?\n. Out of interest, what tcp statistics from /proc/net/tcp are you interested in?\n. @f0 Sounds good!\n. @vishh :+1: to making v2 independent of v1 objects but that is out of scope for this PR.\n. LGTM. Merging... Thanks @f0!\n. When embedded inside a kubelet I'd rather see cadvisor exposed under something like /cadvisor on the kubelet port & totally disabling the separate cadvisor port, thereby reusing the kubelet cert & auth.\nAlternatively we could add in the same TLS & auth stuff to cadvisor that I added to heapster yesterday, but that would then also require those config options adding to the kubelet to enable it, not too keen on that personally.\nThoughts @vishh?\n. I've submitted a PR for that - kubernetes/kubernetes#14556\n. That's exactly what I've done in the referenced PR :)\n. Self-merging after @vishh's LGTM\n. @vishh: --cgroup-parent does apply to non-systemd docker, correct, but I can't think of a simple way to identify that from the cgroup container name. This optimisation will only help with systemd, which is acceptable to me given the wider adoption of systemd now.\n. Closing - #821 sorted this all out :)\n. Please fix up the Prometheus test & then LGTM.\n. LGTM\n. From the referenced issue it seems running du is the culprit. Why does cadvisor run du at all? Don't think we care much about the size of any individual dirs do we? Shouldn't df of equivalent give enough info?\n. AFAICT this du cheek only runs on aufs backed docker, basically docker running on ubuntu. Firstly I don't really like that inconsistency, but secondly I'm not sure I see the value of this check anyway. What do you think about removing it? In the meantime I'll have a think about how we might be able to implement in a better fashion.\nAlternatively we could reduce the polling interval, but that just seems like delaying for when the number of containers rises.\n. Yes only aufs. See https://github.com/google/cadvisor/blob/b22a08525784f32fa5f188d2974705a68c68d480/container/docker/handler.go#L232.\nIf this check is required, the way to perform it is dependent on the backing storage used for docker. In its current form this check would probably also work for devicemapper loopback but isn't going to work for a more production like deployment like using direct lvm.\nHowever I can't think of a better way to do it tbh. I can't see this performing well enough at scale. Still vote to drop the check, open an issue to think if we can do this better.\n. You probably already know this, but thought it worth just noting why du is causing CPU spikes. The only way for du to know dir size is to traverse the file tree & sum file size from file metadata. Unless the files are cached this is a relatively expensive operation. It also affects the disk cache which potentially affects performance of other running processes as files they access may have been evicted & need to be recached.\n. See #771 for another reason not to do du inside the containers' fileystem - blocks container deletes.\n. @dchen1107 du isn't giving you out of disk notifications, that would be handled via df or similar I'd suggest. du is giving you information on where your disk is being used up though, which I agree is useful, but is currently having, to my mind, unacceptable impacts on both performance (high IO & CPU) & stability (blocking container GC). This is also only implemented for aufs which isn't great.\nIf we could somehow swap to df or equivalent for storage backends that may be a better approach, but I have no idea if that is possible.\n. Public again\n. Can you share the docker run command you used? What distro is this on? I assume Ubuntu by the aufs references but what release?\n. Could you also please share the output of docker inspect -f '{{.HostConfig.NetworkMode}}' <CADVISOR_CONTAINER_ID>?\n. OK let me try to recreate - can't see why it wouldn't detect network mode.\n. So running cadvisor straight on the host works fine. Looking to figure out why it fails when running cadvisor in a container.\n. So looks to me like the image google/cadvisor:latest & google/cadvisor:0.18.0 aren't actually version 0.18.0 at all... This works great when i've built the images from the 0.18.0 tag in this repo & it works fine.\nI also noticed that some of the commits prior to the 0.18.0 release are not included in the docker image, most obvious is the cadvisor version info in the prometheus metrics at http://<cadvisor>:8080/metrics. Could you check your deployment to confirm please? You should see a line exactly like:\ncadvisor_version_info{cadvisorVersion=\"0.18.0\",dockerVersion=\"1.8.2\",kernelVersion=\"3.19.0-25-generic\",osVersion=\"Buildroot 2014.02\"} 1\nThis is from the b8fc8cd2ae8ea99b3fcd556c6987cdcf963a8ece commit, nearly 2 weeks before the 0.18.0 release.\nI'm going to add extra git info into the cadvisor version reported so we can have an easy check for this in future.\n. No it's not. Missing the same functionality.\n@vishh @rjnagal Looks like something is pretty wrong with the release process - the 0.18.0 git tag is correct, but the binary on the releases page & the tagged docker image are wrong. Can you update that ASAP?\n. I've just pushed a locally built binary from the 0.18.0 tag to overwrite the dodgy version to https://github.com/google/cadvisor/releases/tag/0.18.0 - can you try it out please?\n. Thanks for the feedback!\n. @vishh Tested & looks much better for both latest & 0.18.0 docker tags - thanks.\n. Can you try adding -logtostderr to the docker cmd however you do that with that Puppet module. That should spit out some logs that might help.\n. There's nothing unexpected in there. So the container just exits? If you do the docker run manually (without using the puppet module) does the same thing happen?\n. ping @vishh \n. @vishh @rjnagal Anyone give a :+1: on this? Don't want to merge without that...\n. Thanks @vishh \n. LGTM. Thanks @mqliang!\n. LGTM. Thanks again.\n. Thanks again!\n. LGTM\n. 1m frequency sounds good to me, but might be a problem for containers that have written a lot of data - see https://github.com/kubernetes/kubernetes/issues/10451#issuecomment-143327536.\nShould there be a check to see if du is already running so it doesn't overlap with a previous run?\nBTW, this idea has got me thinking about a new way to collect metrics in cadvisor, each check individually scheduleable & feeding data back over a channel to central collector.\n. So does that mean that the length of time for du to run will impact on cpu/memory stats collection? Is that a problem?\n. Are you building with godep go build? There's a Makefile now in project root so you should be able to do a simple make now BTW.\n. You don't need to do a godep restore to build cadvisor. godep vendors all required packages locally. So unless you're looking to add/update/remove any dependencies you don't need to run godep restore.\nmake or godep go build should suffice.\n. I'll check this later but even when building in a new workspace, you shouldn't need to do a restore. godep configures your GOPATH to use the vendored dependencies. One downside is that godep means that cadvisor is not go gettable, or at least not buildable directly from go get. godep go build or make after cloning should work fine though.\n. So this builds fine on a clean env with godep go build or make but godep restore doesn't work as you mentioned. Looks like some manual editing to the godeps config.\n. Oh great!\nThe problem with godep restore is that it can't handle packages being deleted from source repos. In this case, the docker repo has deleted that package & that's messing up godep.\n@vishh What do you think about upgrading docker dependencies?\n. Jenkins seems to have got confused with the PR commits vs previous ones... the flake is due to that for some reason. @vishh ?\n. @mvdan @vishh Bit of a cleanup of godeps, including moving to new location for google API packages & upgrading prometheus packages.\n. @mvdan Yeah this is done exactly the same as I did for Heapster, with the exception that I had to migrate packages for bigquery API.\nMerging...\n. See https://github.com/google/cadvisor/blob/master/info/v1/container.go#L304-L319 & https://github.com/google/cadvisor/blob/master/info/v1/container.go#L258-L275\n. @mvdan @vishh @onlyjob Thoughts? Much prefer this way of working, easier to work with the web assets. Does require an internet connection though to be able to use Google jsapi & chart API - Google's ToS say so...\n. @vishh Any comments?\n. Is this reproducible?\n. We also have to support back to docker 1.7 (I think - maybe further) so upgrading docker package imports in cadvisor is probably not possible, unless backwards compatibility is guaranteed?\n. As long as that level of accuracy is enough. I think it probably is for most cases?\n. @rhvgoyal Sadly the human size regex doesn't take into account decimals (https://github.com/docker/docker/blob/master/pkg/units/size.go#L34) even though the inverse does format with decimals (https://github.com/docker/docker/blob/master/pkg/units/size.go#L54). But parsing is pretty easy tbh so we could just do that in cadvisor (although that bug in docker should probably be fixed too).\n. Got the docker info way working but it's incredibly hacky.\nSo one option would be to carry @vbatts 1.9 patch in cadvisor & kubernetes/openshift so we can use the devicemapper packages directly?\nI'd also propose removing the docker-images label from the fs code in cadvisor & moving the get docker size to the docker handler package. This way we can hopefully use docker packages directly to get storage backend info rather than trying to guess the correct fs as it is doing now. This is really fragile & hard to extend. This would be a breaking change but one I think is necessary. We'd have to make the necessary changes in kubernetes image GC too.\n. Yeah that is doing the right thing, but that's then different if we're using thin-pool LVM. And then there's overlay to think of, etc.\nI'm feeling that we're going to be duplicating a reasonable amount of the docker storage backend stuff if we go down this route. I'd rather reuse the docker code if possible for maintenance & compatibility if at all possible.\n. My output is different to yours - what's the difference between our setups I wonder? I'm direct LVM thin-pool (definitely, maybe).\n$ dmsetup status docker-253:4-2646052-pool\n0 409534464 thin-pool 49091 8247/4161600 251685/3199488 - rw no_discard_passdown queue_if_no_space\n. @vishh That is a different question - one we need to answer, just not now I don't think.\n. @ncdc OK cool - now I understand! So there we have the answer to @vishh's question about how to get container filesystem sizes as well - nice.\n. @rhvgoyal I don't think you can get the level of info we need from the docker API: container filesystem size, used, available, etc.\n. @ncdc :white_check_mark: That's my understanding too. The immediate use case is definitely the info on the file system where Docker stores images to trigger image GC by the kubelet. Individual container fs info is a nice to have - use case to help identify containers that are taking up a lot of space but this is a future enhancement in my eyes.\n. @rhvgoyal While I like the idea of docker exec or equivalent via setns but we have to be careful not to interfere with cleaning up containers. We did the old setns trick to get network metrics & found this sometimes messed up cleaning up docker containers as we're switching into container namespaces quite regularly to gather this data. We switched to raw host /proc tricks in the end & works great without any of these problems.\nAlso can we rely on DeviceSize (or alternative for different backends) being consistent between storage backends for retrieval? DeviceSize seems to be different to what dmsetup status shows me so I'm a little wary about that too tbh (although could be me not understanding something).\n. @rhvgoyal Now that makes sense - thanks!\n. So what's the consensus on a way forward for this? Easiest for now is probably: Refactor to retrieve storage backend from docker info. Support devicemapper via dmsetup status parsing for Docker root & container FS (as future work). Should get us where we need?\n. @timstclair Yes we're waiting on the kubernetes API PR so we make sure we're compatible. I know that's a bit backwards but once we start the v2 API here we'll find it harder to break compatibility.\n. Needs rebasing...\n. Just to tidy up the C code as it's provided by the std lib directly.\n. Does it affect cadvisor functionality or is it just an error message?\n. Can you share the full log? Unless you change the log level with e.g. --v=8 then logs are pretty quiet usually.\nI think the problem is we don't check the relevant files from the mounted host rootfs, but rather the container root fs.\n. Just append it to the docker run command:\ndocker run .... google/cadvisor:latest --v=8\n. @vishh Should be a simple fix - don't mind picking it up if you don't have time.\n. As far as I can see, it's not the caller's responsibility to close the docker client. The client returns unmarshalled responses so it should be the docker client that closes the response body?\nThis looks like a bug in the docker client to me. Creating the docker client multiple times is wasteful, but shouldn't leak anything. We should tidy that up but I don't see it fixing the problem unless I'm missing something.\nThe unixHTTPClient shouldn't need to guarded - http.Client and & http.Transport are safe for concurrent use by multiple goroutines as documented.\n. I seem to remember this being an issue somewhere else, perhaps in Prometheus. I seem to remember that you should close the response body even if Do returns an error. Stuff like this (https://github.com/fsouza/go-dockerclient/blob/af9789bbd78acf3e279274caa54682185eb7ed33/misc.go#L14-L17):\nresp, err := c.do(\"GET\", \"/version\", doOptions{})\nif err != nil {\n    return nil, err\n}\ndefer resp.Body.Close()\nShould actually be:\nresp, err := c.do(\"GET\", \"/version\", doOptions{})\ndefer resp.Body.Close()\nif err != nil {\n    return nil, err\n}\n. Forget what I said before, totally wrong about the closing body thing I mentioned above.\n. Ah OK I understand what you're saying about the guards around the creation of the unixHTTPClient now. Yes that would be required.\n. @yujuhong Just pushed #930 which should fix it.\n. /cc @vishh @ncdc @yujuhong\n. @vishh Noticed that there's not a CentOS vm in integration tests to test this on. Any chance of adding one in? All the current integration test platforms use aufs. Well, unsure about coreos tbh.\n. @vishh If you could get a fedora test env up as well that would be awesome.\n. I love alpine so happy to make the switch. Gives us more options in future as well I think.\n. @vishh Yes I'm targeting this for 1.1 & OpenShift 3.1\n. @vishh I've switched to alpine & added devicemapper tools to both release & canary Dockerfiles\n. Tested the Docker image, works great.\n. @ncdc @smarterclayton Any comments before I merge?\n. @ncdc Any chance of a review? Would like to merge ASAP & get a PR in to k8s to update cadvisor dep.\n. @ncdc Of course it works - for me at least ;)\n. Merging...\n. Are you sure this isn't a random flake? 2013 failed in the same way on containervm I see\n. I've just run these locally against centos 7.1 & all works perfectly...\nWhat version of docker are you running on there?\n. I've checked against docker 1.7.1 from CentOS repos & docker 1.8.2 from docker repos - both work fine for me.\n. @pmorie Do you know what version of Docker the type of AdditionalGroups was changed in? Wondering if we can just update cadvisor's config to string & forget about handling old format.\n. Fixed in #930 by bumping libcontainer again. I wouldn't worry about the old format as it didn't look like it worked properly anyway.\n. Also fixes #929 \n. /cc @vishh @yujuhong\n. Switched to sync.Once as suggested by @vishh. This will not try to recreate client if it ever fails but I think that's OK.\n. @dchen1107 I want the devicemapper enhancement too if you're OK to wait?\n. Hey @kansoA - I've tested with master & this seems to be working properly. If you could try with master & re-open if this issue still happens for you.\n. What distros use overlayfs as default?\n. This is on purpose. In Kubernetes it is the network of the infra container (POD) in the pod that handles all networking so it should only be reported by that container, otherwise metrics are duplicated as every container in the pod will have the same network metrics.\n. Can you share the docker inspect output? An empty NetworkMode seems odd.\n. @jekyang Can I close this? Are you happy with the discussion & explanation above?\n. Closing - feel free to reopen if you think this is still a problem.\n. :+1: but needs a rebase.\n. @vishh What's the process for a release? I don't have access to docker hub for cadvisor but can tag & add github release if necessary.\n. @dchen1107 This will be a 0.19.1 release of cadvisor as I've previously tagged 0.19.0 (already in k8s). I'll cut this release today when I find out if there are any nuances I'm not aware of & will get a PR in for updating the cadvisor godeps in k8s.\n. Found high CPU as explained in #938 - holding off on release until this has been looked at.\n. Closing.\n. Flake...\n. No code changes so self-merging.\n. @f0 Nor am I! That's what it looks like to me as well. It's this function (https://github.com/google/cadvisor/blob/master/container/libcontainer/helpers.go#L201-L260) that needs refactoring.\nI'd suggest dropping the regex & going for a fmt.Scanf instead of strings.Fields & seeing how that works out?\n. So what I did to generate the profile graph was to start up cadvisor & run:\ngo tool pprof -png http://localhost:8080/debug/pprof/profile >! cadvisor-profile.png\n. So I tried the scanf approach & it was a small improvement, but not enough.\nI propose disabling this check for now until we sort out the performance so I can get a release out of cadvisor that is needed for kubernetes.\n@vishh @f0 OK with that?\n. @f0 That looks much better. Could you compare locally to the previous version in your environment to confirm?\n. I'm not happy with the volatility of it. On my system (pretty active network connections) it's eating up CPU. I'm going to disable it for now so I can get this release out & revisit it later.\n. Yes & that's a problem - we're using cadvisor in some highly utilised systems such as kubernetes nodes & this is too resource intensive atm. I can't think of a better way to do it though... so disabling is the only option i see at the moment.\n. Thanks.\n. @vishh Yes I am. I was actually trying to tune it to be less CPU intensive but have failed so far. Disabling now - PR coming.\n. @vishh Quick review & merge please.\n. @mvdan You're welcome to review too if you have a chance :) Very simple change going to self merge if someone doesn't review soon so I can get a release out.\n. Can't wait... merging.\n. retest this please\n. Self-merging - minor fix.\n. Switched to using the error rather than the number of fields read.\n. retest this please\n. retest this please\n. retest this please\n. Tested myself & works fine against coreos beta...\n. Merging after LGTM.\n. Closing as dupe of #743 \n. I assume you're talking about devicemapper loopback? Vfs stat doesn't work for devicemapper thinpool lvm (the preferred production use for performance reasons). So I would not revert the change but rather extend the check for loopback devicemapper to use the underlying partition info.\nAny thoughts @ncdc?\n. It certainly should, yes. At least on my machine :)\n. Disclaimer: so is mine.\nIt should report on the whole pool.\n. Thanks @ncdc. Patch looks OK. You want to submit a PR or I can do it?\n. :+1: This is likely to highlight not just regressions but a number of performance improvements too.\n. Fyi docker stats is a docker command, unrelated to cadvisor. If you have a problem with docker stats then you'll need to raise that in the docker repo.\n. Ah sorry @vishh! Thanks for fixing.\n. This works fine for me on Fedora 23. Is this still happening for you? What distro are you using?\n. Can you run with --privileged please?\nAnd are you using the rpm from docker repo or from centos official repo?\n. @feiskyer FYI - https://bugzilla.redhat.com/show_bug.cgi?id=1275554\n. After recent discussion, happy with this approach.\nOne thing: could you enable the storage auditing regardless of the storage driver & only add the aufs dir if aufs is being used? Right now this only works for aufs storage driver but we could still track log usage for any storage drivers.\n. LGTM\n. No worries at all!\n. You can already add a regex to extract labels from container names (https://github.com/google/cadvisor/blob/master/http/handlers.go#L91-L95). We do this in the kubelet. Currently this can only be done by embedding cadvisor & registering the cadvisor & prometheus http handlers there.\nThe other option is to use Prometheus relabelling to extract container name to labels.\n. Another option is to do this with dmsetup as we did with the docker pool storage.\n. This was discussed in https://github.com/google/cadvisor/issues/920#issuecomment-148815756.\nSomething like this may work:\nDM_DEV=docker inspect -f '{{.GraphDriver.Data.DeviceName}}' <containerID>\nDM_STATUS=`dmsetup status $DM_DEV`\nDM_USED_SECTORS=`echo $DM_STATUS| cut -d' ' -f 4`\nDM_USED_BYTES=`expr $DM_USED_SECTORS \\* 512`\nDM_TOTAL_SECTORS=`echo $DM_STATUS| cut -d' ' -f 5`\nDM_TOTAL_BYTES=`expr $DM_TOTAL_SECTORS \\* 512`\necho $DM_USED_BYTES / $DM_TOTAL_BYTES\nI'm not 100% sure on the used bytes figure tbh.\n. Would nsentering docker's mnt ns require cadvisor to run in the host ns? Does that have any implications when running cadvisor in a container?\n. I'd have to defer to someone who knows more about devicemapper tbh. @rhvgoyal @mrunalp opinions welcome on best approach.\n. AFAIK cadvisor won't be sharing pid ns with host as this can lead to zombie processes. pid 1 in container so gets reaped properly.\n. @rhvgoyal Forgot to say thanks for the update on dmsetup - so... thanks!\n@vishh Maybe early next week, yes.\n. Previously discussed in https://github.com/google/cadvisor/issues/822#issuecomment-135811901\n. @jthornber Sounds great!\n. So the use case for this is systems where systemd is running but docker is not run via systemd?\n. Thanks @vishh. LGTM\n. retest this please\n. Do you think it's time to put FS stats behind an interface & create appropriate implementation on startup? Would remove some of the switches & perhaps make it more extensible & easier to test going forward?\n. @vishh Any time to put FS stats behind an interface with separate impls?\n. @vishh Also needs an integration test set up for it please.\nIs this PR still alive btw?\n. @vishh OK so can you rebase & let's try to sort out integration tests ready to merge this.\n. It won't be this week sadly.\n. Can you rebase plesae?\n. LGTM. Thanks @vishh.\n. I wonder if we'd be better using build constraints for stuff like this? Tiny performance improvement, but possibly worth exploring in this case.\n. @yghannam Understood. LGTM. Thanks very much!\n. ok to test\n. retest this please\n. Merging. Thanks @yghannam!\n. Lgtm\n. Thank you for sending this PR in. Definitely needs to be done some time.\nThis seems to be changing more than just the runc dependencies. Can you confirm that those other dependency changes are required? Most importantly the downgrade of the docker packages is concerning.\nDo you know what impact this has on kubernetes, if any? Has kubernetes migrated to runc? I guess we need to keep that aligned, or doesn't it matter?\nDoes this have any impact on docker versions we support? I'm not sure what our compatibility policy is but runc is not used on older dockers. Will this cause problems?\n. @vishh Are you happy with these godeps bumps? I can't think of any problems but don't want to bump stuff it it's going to make it difficult/impossible to bump kubernetes' dependencies on next cadvisor release.\n. @yujuhong Any concerns about the Docker dependency bump, especially with kubernetes in mind?\n. ok to test\n. retest this please\n. LGTM. Thanks @afein!\n. I would never abandon cake ;)\nAnyway LGTM once you've signed the CLA. Thanks for sending this in.\n. Closed by #970 \n. Thanks for doing this. It might be easier to send in a PR so we can help you with review that way?\n. Thanks for this. What doesn't work? No metric or no values reported in the metric?\n. Can you share the whole /metrics endpoint output please? Perhaps in a gist.\n. And you're sure you built & replaced cadvisor binary with the newly built binary? Everything looks fine to me but the output.\n. OK I'll build with your PR locally tomorrow & see if I can see what's going on.\n. This works perfectly fine for me locally... How are you building? Just run make & you should get a binary in the root of the project dir to use.\n. Well when I say it works perfectly I mean I get output in the /metrics endpoint.\nBut in logs (running with --logtostderr):\nE1123 09:15:09.954624 11428 handler.go:149] raw driver: Failed to parse int \"-1\" from file \"/sys/fs/cgroup/cpu,cpuacct/cpu.cfs_quota_us\": strconv.ParseUint: parsing \"-1\": invalid syntax\nNeed a test for this as well if possible.\n. 1.4 should be fine. Do you get errors when building with 1.4?\n. Just pushed a fix so you can build on 1.4 as well (ldflags was 1.5 stylee). Rebase against master & you should be able to build on 1.4 with make.\n. Discussed on IRC: cadvisor needs to be cloned to $GOPATH/src/github.com/google/cadvisor & a remote set to your fork, rather than cloning to $GOPATH/src/github.com/f0/cadvisor.\n. So this works for you?\n. Would you be able to add a test to cadvisor/metrics/prometheus_test.go please?\n. Thank you. Works for me too btw.\n. That sounds good - if -1 then don't send the metric.\n. Sorry. You'll need to add the stats somewhere in https://github.com/google/cadvisor/blob/master/metrics/prometheus_test.go#L61-L151 & update https://github.com/google/cadvisor/blob/master/metrics/testdata/prometheus_metrics accordingly. It's a bit manual, but worthwhile.\n. retest this please\n. The order of the output is important. Think metrics are ordered alphabetically so you'll need to put the new metric in the right place in the test file.\n. Should be before container_start_time_seconds.\n. If you don't manage to get to it by Monday I can pull it into a separate PR & fix up the tests.\n. no LGTM\n. Self-merging as this is only ldflags tidy, no functional change.\n. Dupe of #966?\n. @yifan-gu If you could just check #966 & comment with anything you think should be included/different that would be awesome.\n. ok to test\n. Thanks @yifan-gu! Merging.\n. Can you explain a bit more? Have to admit to not knowing this area very well, but how is RSS more interesting/useful? FYI we delegate collection of this to libcontainer to be consistent with docker stats but I don't mind if we have to do our own processing if cadvisor users have different requirements.\n. @guoshimin Thanks for the explanation - now I understand :) Certainly very useful - any chance you could submit a PR for it?\n. That would be great - let us know if you need any pointers to get started.\n. @yujuhong Now you're making me work hard... you're right of course. Will add some tomorrow.\n. Refactored to use strings.Fields instead of fmt.Fscanf after benchmarking the two:\nBenchmarkFscanf-8         300000              5029 ns/op             240 B/op         10 allocs/op\nBenchmarkFields-8        1000000              1390 ns/op             304 B/op          3 allocs/op\nAdded tests as requested, including a case with the output value in the original report of this problem (kubernetes/kubernetes#17725).\nLink to DM kernel docs added.\n. retest this please\n. retest this please\n. @yujuhong @timstclair Review would be appreciated so we can get this fix into kubernetes ASAP.\n. > Is it not clearly documented?\nNot that I've found. I worked out the line format & confirmed with @rhvgoyal.\n. Thanks @yujuhong. Merging.\n. @yujuhong You mean at https://github.com/google/cadvisor/pull/976/files#diff-88a0b03cd7789db9964ce3da2ff9fd37R110 & following?\n0 409534464 thin-pool 64085 3705/4161600 88106/3199488 - rw no_discard_passdown queue_if_no_space -\nFrom @rhvgoyal description above, this is 88106 used data blocks out of total 3199488 data blocks as the test specifies?\n. Well now I actually read it, it does look different. The output I used in the tests was from my local box & from a bug report in Kubernetes. I guess there's some prefix that's different somehow but it is correct :)\n. @timstclair Thanks for a thorough code review - this is making it a much more worthwhile endeavour than it would have otherwise been :)\n. All comments addressed. Good review!\n. As an observation there is a lot we could do to lower resource utilisation. Regexps are hungry & need to be used judiciously. They're overused here where we could probably do some cheaper string parsing.\n. Integration tests pass, so it must be good right? :stuck_out_tongue_winking_eye: \n. Yeah we need to do more profiling of cadvisor in general. There have been quite a few performance issues when embedded in the kubelet & we need to identify areas for improvement. Regexes were a recurrent theme in those perf problems, hence me looking at them here. I think with the work we've done recently in cadvisor this is improved significantly, but further work needs to be done to keep cadvisor's footprint as low as possible.\n. retest this please\n. Self-merging after LGTM.\n. retest this please\n. retest this please\n. Same old very annoying flake... must look at it some time.\n. Self-merging after LGTM.\n. retest this please\n. @timstclair Any time for a quick review of this? This has a real impact on utilisation so want to get it into kubernetes ASAP.\n. @vishh @mvdan @yujuhong Anyone got time to review? This is the last PR I want to merge before 0.19.4 release & upgrading kubernetes dependency.\n. Thanks @mvdan! Comments addressed, waiting for green to merge.\n. Some gcloud flake...\n. retest this please\n. @vishh Integration testing infra problems... please can you try to sort it out as I have no access.\n. @dchen1107 Is the integration testing env anything you could help out with or know who could?\n. As this was passing before the stylistic improvements I'm merging so I can get this in to Kubernetes ASAP.\n. Ah yes I assumed it was a Docker hub build but you are correct. Thanks!\n. @vishh Tests passing - review when you get a chance please.\n. Self-merging as integration test cluster is broken & this is just unused code.\n. @vishh The integration test environment is screwed & needs fixing. I've tried to contact you before to fix but no response. I have no access to integration environment or I would have looks - perhaps you could sort me out with access? Also give me access to cadvisor docker hub builds?\n. Hope you had a great, well-deserved, break.\nYou should be able to add me as a collaborator on the Docker hub repository for just cadvisor - think that's now possible.\n. Perfect thanks @vishh - will have a nose around & see if it's anything obvious I can fix.\n. Seems to be a problem with coreos beta only. Investigating.\n. retest this please\n. retest this please\n. Merging after tests now pass.\n. Thanks for the PR. Did you do this manually or via a tool like goimports? If via a tool then can we also get the check added to the build scripts to verify subsequent PRs don't mess up ordering?\n. Why do you feel strongly about this package order? What different order does goimports produce? My issue with manually doing this is keeping it ordered going forward. Need tooling to support.\n. OK that makes sense.\nGoing to have to keep a close eye on import order in future.\nThanks!\n. ok to test\n. Merging as the integration testing environment is broken, but this change only affects import order & unit tests pass.\n. Thanks @carmark.\n. Understand about time constraints. Thanks for replying.\nI don't want to revert it as it will break with Docker >= 1.9 so need to come up with a way to handle it all, probably a custom Unmarshal function or something. I'll see if I can come up with something soon-ish.\n. @carmark awesome! Feel free to submit a PR now so we can help review, test, etc if you want.\n. Looks good but needs a couple of tests if possible with configs from < 1.8, 1.8.3 & >=1.9 just to check we haven't broken anything.\n. I'm wondering if we could just drop the throttled devices for now as we don't use that config AFAIK so processing for no need. I'm happy to do that & put that in a comment unless you can think of a reason why not to do that.\n. Minor nits. Really need some tests now just to confirm it's fixed & compatible.\n. ok to test\n. We have integration tests for multiple versions right now, this is why most PRs are broken now...\nCould you please add some unit tests & then we can merge?\nThanks very much for this - very much appreciated.\n. Thank you!\n. Flaky test\n. retest this please\n. retest this please\n. LGTM\n. Thanks @carmark!\n. @vishh Comments appreciated.\n. With the new Dockerfile in the root of the repo you can build with docker build ... & nothing more.\nI'd like to move to this approach & use Docker hub builds with builds from regex tags so we can ensure a Docker image is built for every tag automatically when we create a release tag.\n. With the latest Dockerfile it is just a case of docker build, no make required. Try it out!\n. Those warnings are due to the fact that cadvisor has to support with go 1.4 & 1.5. The only way to get rid of these warnings is to change the linker flags in build scripts in a way that would be incompatible with 1.4. I don't mind making go 1.5 a requirement personally.\n. Closing - this doesn't seem to have helped at all. Wondering now if the test is actually valid?\n. thanks @jonboulle - merging.\n. Nice catch - thanks @Mistermatt007!\n. Even better ;)\n. We'll have to remember to keep docs up to date if the source changes.\n. ok to test\n. LGTM\n. Thanks @timstclair.\n. ok to test\n. Can you please update docs with how to enable please?\n. add to whitelist\n. retest this please\n. LGTM. Thanks!\n. How about a Prometheus metric to hold metrics for number of specific goroutines? As goroutine starts, increment gauge; decrement on stop. That seems like a more foolproof way to do this?\n. Can you describe the use case please?\n. Shouldn't we just respect max results if it's specified?\n. Definitely most recent samples.\nI'm happy with the breaking change. cadvisor is still only a 0.x release so API stability not guaranteed :stuck_out_tongue_closed_eyes: \n. @cadvisorJenkinsBot has become sentient!\n. That's normally a transient error - let's try retesting...\n. retest this please\n. I haven't looked (lazy!) but are we defaulting max stats to -1 somewhere?\n. The failure is unrelated... Jenkins seems to get like this sometimes.\n. retest this please\n. Why 60? Assume that means 1 minute of stats but seems a strange default when housekeeping time is configurable.\nBut we can discuss that in a different PR. IMO -1 should be the default if unspecified.\n. Anyway LGTM\n. Unit tests run fine for me locally so happy to merge.\n. Are you looking to get this into kubernetes ASAP? I have a PR to bump cadvisor so I can tag it here & include this bump in that PR if you'd like?\n. So default is 60 & non-configurable via flags. 60 kind of makes sense if you're collecting every second, last minute of stats by default. However housekeeping interval is configurable. So if we change that to 10s, default stats is 60 = 10 minutes of stats. This makes no sense to me.\nIt's unusual to have an arbitrary value for default returned values. I would prefer to change the default to -1 (all stats in period) & make UI change queries to request appropriate number of stats.\n. ok to test\n. Jenkins PR status link is wrong, normal test flake when checking correct build. Merging.\nThanks @directxman12\n. Can you link to the discussion on separating out cadvisor from kubelet please?\n. @pwittrock:\n\nThe following files are not properly formatted:\nintegration/runner/runner.go integration/runner/runner.go\n. @vishh I do hope that cadvisor CI isn't moving to a non-public Jenkins... makes it hard for non-Google maintainers to help with contributions.\n. @pwittrock Aim to please :-b\n\nCan no longer see why Jenkins build is failing as it's been moved to internal google jenkins it seems... Will have to defer to @vishh for that sadly.\n. retest this please\n. Link to build log doesn't work btw...\n. LGTM! Self-merge when travis passes.\nNice work with the build logs link btw. Do you think we can disable the default jenkins build now?\n. @pwittrock Wonder if we can do this in a slightly smarter way? Perhaps extracting regexes into separate file to match multiple flaky tests in future rather than changing code?\n. LGTM after function renamed. Thanks!\n. LGTM - waiting on travis (being slow today). Feel free to self-merge if someone else doesn't get their first.\n. Travis has these blips every now & then. I'd rather use circleci tbh where I never see these kinds of issues.\n. One nit, but otherwise LGTM.\n. ok to test\n. @pwittrock How do I trigger Jenkins builds now?\n. Unrelated flakes... Travis failing because of go vet changes it seems & e2e failing with:\nERROR: (gcloud.compute.ssh) Your current active account [211744435552-compute@developer.gserviceaccount.com] does not have any valid credentials\\nPlease run:\\n\\n  $ gcloud auth login\\n\\nto obtain new credentials, or if you have already logged in with a\\ndifferent account:\\n\\n  $ gcloud config set account ACCOUNT\\n\\nto select an already authenticated account to use.\n. @carmark Unless you feel strongly about it I'd leave it as is.\n. This passing tests now - would like to get this merged ASAP\n. /cc @vishh \n. @marcellodesales Tests pass so please try locally when you can.\n@vishh Happy to merge then? We can clean up anything afterwards if need be but all LGTM.\n. @mnuessler Good spot!\n@marcellodesales Yay! Thanks for trying it out & confirming it all works as expected.\n. You will need to re-build cadvisor specific to the RPI arch as it's an ARM processor I believe. We only provide x86_64 arch builds I'm afraid.\n. No it is not possible to run cadvisor under Docker without being a privileged container.\nBut if you're running under Kubernetes you don't really need to as cadvisor is embedded inside the kubelet & metrics are accessible via the kubelet API. Unless you need to access cadvisor directly for some reason?\n. LGTM. Just waiting on Travis (as always...) then will merge. Thanks!\n. @pwittrock @vishh Problems with Jenkins e2e:\nF0112 12:48:17.258015   30970 runner.go:246] Error 0: failed to make remote testing directory: command \"gcloud\" [\"compute\" \"ssh\" \"--zone\" \"us-central1-f\" \"e2e-cadvisor-rhel-7-docker19\" \"--\" \"mkdir\" \"-p\" \"/tmp/cadvisor-30970\"] failed with error: exit status 1 and output: \"ERROR: (gcloud.compute.ssh) Your current active account [211744435552-compute@developer.gserviceaccount.com] does not have any valid credentials\\nPlease run:\\n\\n  $ gcloud auth login\\n\\nto obtain new credentials, or if you have already logged in with a\\ndifferent account:\\n\\n  $ gcloud config set account ACCOUNT\\n\\nto select an already authenticated account to use.\\n\"\nError 1: failed to make remote testing directory: command \"gcloud\" [\"compute\" \"ssh\" \"--zone\" \"us-central1-f\" \"e2e-cadvisor-container-vm-v20151215-docker18\" \"--\" \"mkdir\" \"-p\" \"/tmp/cadvisor-30970\"] failed with error: exit status 1 and output: \"ERROR: (gcloud.compute.ssh) Your current active account [211744435552-compute@developer.gserviceaccount.com] does not have any valid credentials\\nPlease run:\\n\\n  $ gcloud auth login\\n\\nto obtain new credentials, or if you have already logged in with a\\ndifferent account:\\n\\n  $ gcloud config set account ACCOUNT\\n\\nto select an already authenticated account to use.\\n\"\nError 2: failed to make remote testing directory: command \"gcloud\" [\"compute\" \"ssh\" \"--zone\" \"us-central1-f\" \"e2e-cadvisor-coreos-beta-docker19\" \"--\" \"mkdir\" \"-p\" \"/tmp/cadvisor-30970\"] failed with error: exit status 1 and output: \"ERROR: (gcloud.compute.ssh) Your current active account [211744435552-compute@developer.gserviceaccount.com] does not have any valid credentials\\nPlease run:\\n\\n  $ gcloud auth login\\n\\nto obtain new credentials, or if you have already logged in with a\\ndifferent account:\\n\\n  $ gcloud config set account ACCOUNT\\n\\nto select an already authenticated account to use.\\n\"\nCan someone fix that please as I don't want to merge this until I've got successful e2e tests at least.\n. @k8s-bot retest this please\n. @pwittrock @vishh Also how can I retrigger Jenkins GCE e2e tests?\n. OK thanks. Didn't think closing/opening issue worked with ghprb? Thought build result was tied to commit sha?\n. @pwittrock Thank you very much for looking at that.\n& e2e is green!\n. Still please don't merge until runc is tagged & we can reference a tagged version in godeps.\n. @vishh @pwittrock This is now ready to merge so we can fix up some Kubernetes issues.\n. And now I find out they're redoing the runc tag as they missed a version bump in code... Will update the dep when that's done in a new PR.\n. LGTM\nJust wait for Travis to pass & self-merge if someone doesn't get there first.\n. What version of cadvisor are you running?\n. @vishh Goroutines were leaked as detailed in kubernetes/kubernetes#19633. @timothysc found this was due to tracking filesystem usage. The prior version was starting tracking filesystem usage even if the container had already been seen - this was not being cleaned up. This PR ensures that the track usage goroutine is only started once.\n. @vishh I agree this was just yet another quick fix... Really need to review cadvisor's structure so we can tidy things up like this.\n. Discussed with @vishh on slack - e2e build runs unit tests on Jenkins as well so we're going to disable Travis completely.\n. Sounds like a good idea to me.\n. Closing - thanks @timothysc.\n. Self-merging - only a version change.\n. This doesn't include the libcontainer update I wanted to get in to next release so will have to cut another old next week once libcontainer is tagged.\n. Shouldn't this be picked up in tests? Do we have no coverage here?\n. Thanks!\n. LGTM thanks!\n. We have a few different fixes now we need in Kubernetes that should be in next release, although I'm waiting on runc tag before submitting update to Kubernetes (sadly runc tag is waiting on a bug fix too...). I don't mind if we keep doing small cadvisor releases at all but we need to make sure we don't have overlapping cadvisor updates to Kubernetes.\n. I can see that the commit has a different email to the one you used to raise the PR but that you're obviously the same person so CLA looks good to me.\nMerging :)\n. I don't think that tls is used for the unix socket - that's for TCP only. Are you running cadvisor as root or as a privileged container with docker socket mounted into container?\n. retest this please\n. @k8s-bot test this\n. Merging as discussed above with @vishh.\n. LGTM\nWhile this is a bit messy, I don't expect it to be refactored any better than you have done in this PR.\n. ok to test\n. Thanks @ncdc!\n. Ha - sorry I didn't mean your code I meant the whole labeling process tbh - I'd prefer to move this under the docker manager & request via that than inspect some free-form labels in this case. No, your code's beautiful, amazing, shiny. Kinda :-b\n. What problem does this solve?\n. This is a problem with godep not working when packages are removed from master branch but still exist in tagged versions obviously. In Kubernetes there is a convenience function preload-dep in hack/verify-godeps.sh that needs to be used in cases like this.\n. But there is nothing wrong with these godeps afaik. Why didn't you just use kubes preload deps like I pointed out?\n. go vet is still trying to vet vendored packages & failing. Can you update scripts accordingly?\n. I'm happy to see this move but I do wonder how this affects godep vendoring for consumers like Kubernetes? \n. Pulled this PR branch locally & something isn't right:\n$ glide list\n[ERROR] Dependency /home/jdyson/go/src/github.com/google/cadvisor/vendor/github.com/stretchr/testify/mock failed to resolve: lstat /home/jdyson/go/src/github.com/google/cadvisor/vendor/github.com/stretchr/testify/mock: no such file or directory.\nOops! lstat /home/jdyson/go/src/github.com/google/cadvisor/vendor/github.com/stretchr/testify/mock: no such file or directory\n. Package list in Makefile might be easier to construct with pkgs = $(shell $(GO) list ./... | grep -v /vendor/)\n. How about 0.20.5?\n. Thanks!\n. Can you share how to recreate the issue please? Not sure we support the new networking feature properly yet but shouldn't be too hard to add in.\n. Thanks for reporting back @damianopezzotti!\n. just waiting for another runc tag & update godeps with that & i will release ASAP after that - not too long i hope.\n. Yeah looking at merging now.\n. Fixed by #1063 \n. Self-merging as this is just a godep bump that we already agreed in #1046 \n. Self-merging to get this into Kubernetes.\n. We should probably sanitize labels then. That's a bit annoying tbh as you detailed in the Prometheus issue.\n. @MikeMichel What version of cadvisor are you running? Looks like labels are properly sanitized to me but just want to confirm if this is a recent change.\n. Labels were properly sanitized in 544b852a so only available in >= 0.20.4\n. I tagged 0.20.5 that includes that fix yesterday, but I have no access to push Docker images to Docker hub for this project I'm afraid.\n@vishh Is this something you could take care of please?\n. Closing this now as the labels are properly sanitized, just waiting on a docker image being tagged (come on @vishh :stuck_out_tongue:).\n. @vishh This needs reverting too. Checking the test logs it only passed because they timed out... This breaks stuff for devicemapper storage as it doesn't necessarily have a root storage dir.\n. It was still broken - think this is fixed now in #1094 I think.\n. Tests seemed to pass even though there were errors in the logs... Test killed with quit: ran too long (10m0s). So no confidence that this actually passed integration tests.\n. Does make build work for you? Confused how this can't work if it works on CI...\n. I can only think this is something in your environment - just cloned cadvisor to clean GOPATH, run make build & it works fine.\n. Are you building this on non-linux platform?\n. But there are buildable sources in there... for Linux at least - inotify_linux.go.\nAlso this hasn't changed in a long time.\n. Fixing up instead.\n. Fixing this up now.\n. Fixes #1094 \n. @zaa it should be it's just not implemented yet. Discussion in #959.\n. @vishh This PR now simplifies stuff around systemd cgroups by ignoring the fact that systemd might be being used. I think the previous logic wasn't necessary at all - we can just limit the container name to containing the full Docker container ID format ([a-z0-9]{64}) & then validate the container ID via Docker inspect as we did before. This gets rid the of the need for the nosystemd flag too which is a good thing.\n. Thanks for picking this up @pwittrock.\n. Personally if prefer to move to a matrix build for testing on multiple distros. That way is easier to identify problems & build output is more readable. What do you think?\n. For matrix build you would need to set up each node as a Jenkins slave & then set up the matrix to run on each slave.\n. LGTM\n. LGTM\n. @deads2k Sorry on vacation.\n@vishh Could you review when you get a chance please?\n. OK to test\n. Thanks @sjenning\n. LGTM. Merging without running tests as no code changes.\n. When running in a container, do we want to prefix the path with path of mounted host root dir?\n. @ncdc Ah OK that's fine then.\n. retest this please\n. @k8s-bot test again\n. @vishh @pwittrock How do we rekick tests on PRs now? Not working for me...\n. ok to test\n. Sorry this slipped off my radar. LGTM.\n. @timstclair Rumours... I'm still around, just not contributing as much as I'd like to be able to right now.\n. Would #1315 work for you?\n. This is a necessary fix that needs to go in IMO. Just need to protect against collisions & then LGTM.\n. @ZimboBoyd Are you still working on this PR? It not, then do you mind if I pick this up & send in a separate updated PR with the minor change detailed above?\n. @grobie What do you think about the constant prefix, even with no collisions? Happy with this approach?\n. Push gateway should only be used for ephemeral/batch jobs, not long running daemons like cadvisor. You can read more on when the push gateway is applicable at https://prometheus.io/docs/practices/pushing/\nI'm going to close this issue as it's not something that cadvisor should support.\n. Have you looked at using Prometheus jmx exporter & exposing Prometheus endpoint, reusing existing cadvisor custom metric collection?\nThis is what we do in fabric8, via Agent Bond, which combines both jolokia & jmx exporter into a single Java agent.\n. There is no policy per se, but adding in an extra custom endpoint type of course adds maintenance going forward. Jolokia only targets a small percentage of target users (JVM containers that use Jolokia). Both http & Prometheus cover a wider target. I'm not dead against adding this in, just want to make sure it's agreed & that there is agreement to maintain it going forward.\nDon't get me wrong: I LOVE Jolokia personally (my team mate & friend is the creator of Jolokia), but I'm not sure if it is suitable for cadvisor.\nThoughts @vishh @timstclair?\n. Could you split those fixes into separate PRs please, one for the IP address & one for the auth fixes? We can discuss the jolokia endpoint separately, but those fixes sound like ones we would like to get in please.\n. @k8s-bot ok to test\n. @k8s-bot Test this\n. @k8s-bot ok to test\n. @vishh Any idea on the e2e tests?\n. @timstclair I could be wrong but that sounds like someone has been playing with protected branches & required status checks. There's an option there that says PR branches must be up to date with master before merging, which is a PITA IMO. PRs should have no conflicts & as long as PR CI builds perform a merge before running tests (which I think they do) we're golden.\n. LGTM... but I no longer seem to be a maintainer so can't kick off test builds or merge PRs...\n@timstclair Would you be able to review if @vishh is unavailable?\n. @vishh Thanks!\n. @derekwaynecarr Sorry for being dense but could you explain the problem & how this fixes it? Got so much to learn!\n. Aha thanks @timstclair!\n. @timstclair Would you be able to take a look & merge if you're happy please?\n. Thanks @timstclair!\n. @timstclair This is affecting a number of users with Kubernetes-Prometheus integration when using base images with LABELs in them (e.g. CentOS). Shall I send in a PR to cherry-pick this into release-0.23 & get a new release tagged to be upgraded in Kubernetes?\n. Created cherry-picked PR #1405\n. The mount point shouldn't be hard coded but read from the mount table. Can't see anywhere this is hard coded so bit confused how directory is failing. Not at a computer right now to take a look though, sorry.\n. Dupe of #1404 \n. We need the pod name, container name, etc that come from these labels (https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/types/labels.go#L20). Perhaps we can just whitelist those labels & ignore others?\n. I'm not at a computer at all this week to check but would you be able to share a snippet from exported kubelet /metrics from a kubernetes build from master? Just want to check what the raw pod metrics labels look like.\n. I'm pretty sure this will affect pod discovery which uses the container labels from exported metrics for pod name, etc but as I'm out on PTO all well I can't check myself.\n. Doh I'm not talking about pod discovery but pod cgroup metrics. Pretty sure in kubernetes master, pod/container names will only be exposed via these container labels linked above. Just need to see a dump of /metrics from a kubelet built from master to confirm. We cleaned up duplicate label names iirc, relying on these container labels instead of deriving them from the docker container name.\n. SGTM. ContainerNameToLabelsFunc isn't used in kubernetes any more iirc. It was a stop gap until kubernetes implemented proper docker labels for this stuff.\n. LGTM - @timstclair?\n. @timstclair Can we get this in 0.24 please?\n. @timstclair Can we get this in 0.24 please?\n. LGTM\n. LGTM\n. :+1:\nI don't mind the loss of http_* metrics.\n. :+1:\nI don't mind the loss of http_* metrics.\n. @grobie Needs a rebase please.\n. @grobie Needs a rebase please.\n. mkdir: cannot create directory \u2018/tmp/cadvisor-768\u2019: No space left on device - @timstclair any chance you could sort that out?\n. mkdir: cannot create directory \u2018/tmp/cadvisor-768\u2019: No space left on device - @timstclair any chance you could sort that out?\n. @fabxc Would be you be able to take a look if you have some time? Switched to expfmt decoders & seems to work nicely (few tweaks to tests but nothing fundamental changing in functionality).\n. @fabxc Would be you be able to take a look if you have some time? Switched to expfmt decoders & seems to work nicely (few tweaks to tests but nothing fundamental changing in functionality).\n. Ready for a second round review.\n. Ready for a second round review.\n. @timstclair What's left to get this merged?\n. @timstclair What's left to get this merged?\n. Changed to that - good suggestion, thanks.\n. As @whosthatknocking said, please move this to a var outside of the func.\n. This regex can probably be locked to ([a-z0-9]{64})\\.scope$, including anchoring to end. I wonder if we could use that to get the parent cgroup too? Something like (.*)-([a-z0-9]{64})\\.scope$?\n. labelKey will need to be sanitized as Prometheus labels need to match this regexp.\nYou can copy the sanitize method from here & please add a test for this.\n. AFAIK format is <cgroupParent>-<containerId>.scope. Container ID is 64 characters (hence the {64} so from this regex we should be able to extract the cgroup parent (normally docker) & the container ID. I can't imagine any conflict with other scope units so this should be pretty safe.\n. Ah I missed @whosthatknocking's + as opposed to my * - much better idea. Will update #896 with that.\n. Yes. Fedora 22 with Docker 1.8.2\n. How does removing this affect non-systemd Docker installs?\n. This is actually redundant as this is only used for non-systemd Dockers where self.cgroupParent is empty.\n. Coincentally I've just run cadvisor on Ubnutu 14.04 & it does actually use /docker...\ncondescending_bhabha (/docker/f5a39e9c98788da89c050ca45d266ce0ddc31bd0a7e3ecbc618e166c17ade17d)\n. The docker ID is extracted from the container name - https://github.com/google/cadvisor/blob/499737d7ad3f754a63036e9c4d0d5f77f2889fed/container/docker/factory.go#L118. It is that ID that is used in the docker inspect.\nAgain, this is tested & working properly on Ubuntu 14.04.\n. omg you're totally right - sorry i misread the condition somehow. Doh!\nYes I agree - redundant code to be removed.\n. Why is this deleted? Isn't this used by the web pages?\n. OK it's redundant too. I like this PR - lots deleted :)\n. Agreed. Also using the regexp rather than HasSuffix to check the container name would make more sense.\n. Isn't it the larger docker containers that will take longer? Setting the cpu time this low will likely mean no fs stats for large containers, which reduces the usefulness of this check: finding which containers are using up your disk.\n. Perhaps niceing it would be better?\n. Probably don't need to wrap it in /bin/sh now as nice is a binary, not a shell builtin. This will also help with the args - dir is outside of the -c param to sh right now & probably won't be used at all in this call.\nI'd go with out, err := exec.Command(\"nice\", \"-n\", \"19\", \"du\", \"-s\", dir).CombinedOutput()\nUnless I'm missing it, could you add a test for this too? Should be pretty easy to set up a file with known file in an otherwise empty dir & run GetDirUsage on it to check.\n. Wouldn't this be better off using a select on time.After & stopChan? Otherwise we're waiting for shutdown for this thread to finish sleeping, a minute at worst case.\n. I'll send in a PR later\n. I'm missing where stop is actually called?\n. The free form options are a bit scary but there is no other way AFAIK to get this info out of Docker. The use of Contains was previously unnecessary - the keys/values should never have changed... I'd prefer to list the options more explicitly rather than using Contains anyway if need be, but this should work fine.\n. No... damn that's killed this approach really then. Or can we add in dmsetup to the cadvisor image?\n. Yep\n. I don't think so. we've handled the file read error so we have a string that we're just trying to match.\n. Yeah golang conventions\n. https://blog.golang.org/package-names#TOC_2.\n. Doesn't this cause compilation failure for you?\n. How about something like Explicitly disable  systemd support for Docker cgroups?\n. Isn't this a noop consisting when we run this?\n. Can we just return here rather than inventing the whole next block?\n. Actually it might be worth adding the actual slack address, kubernetes.slack.com, with a note to sign up at slack.kubernetes.io\n. Why is the docker package being downgraded?\n. Why not use the readInt64 method as above?\n. What value will this have if quota is unset (-1)? I assume zero - is zero really a value we want to send through?\n. I didn't check that - so really readInt64 should really be readUInt64...\n. What omitifempty? You're sending through a metric to Prometheus here with value 0 if container.Spec.Cpu.Quota is unset.\n. Ah you mean the omitempty tag on the struct field? That's only for JSON serialization of the specific struct, not sending metrics to Prometheus.\n. https://golang.org/ref/spec#The_zero_value\n. Is 0 a valid value for CPU quota - I'm not sure tbh.\n. Putting it in the function means the RE gets wastefully compiled every time the function is called. In this case probably not called more than once, but normally best to move it outside so trying to keep it consistent.\n. Good catch,\n. Probably best to return an error - can you trust stats from a corrupt file?\n. Seems as though it was always unused... removing.\n. Ah hang on messed up removing stuff below. Will add it back in.\n. Great minds...\n. Done.\n. Agreed. Not going to do it here, luckily AFAICT this is only called once on start of cadvisor so efficiency isn't paramount. Would like to improve it though, for vanity more than anything else :-b\n. This is doing the same bizarre single quotes for me locally too. Any idea why? I assume it's not going to cause any problems going forward?\n. Good catch!\n. Should we check the type of this is a json.UnmarshalTypeError instead of checking the string contents?\n. Can we take this out into a separate function? Better readability & easier to test.\n. I mean this whole convert old Config to new block.\n. Can you link to where this is documented?\n. These are all redundant assignments aren't they?\n. Not sure this would be a pointer type?\n. The wget req on line 253 (https://github.com/google/cadvisor/pull/993/files#diff-6333e565f1256ee66767acf24186c52cR253). That's what should be causing network traffic. The problem I think that's happening is there's a race between container is up check via waitForContainer & first wget requests.\n. The Dockerfile has moved in this PR to the top level dir & you can use just make docker-build to build the image.\n. No - that was my original intention but was a bit tricky to achieve while still running on alpine due to glibc linking issues for runc. I'll take another look & see what I can do.\n. What do you mean? At the end of this I'd like to see a single Dockerfile that we can use Docker hub to build on commit to master (canary tag) & any tag (tagged with appropriate tag).\n. Shouldn't this be deferred straight after framework.New() to make sure we don't leave any containers lying around if it bombs out earlier?\n. This seems really fragile. Is this the only way to get this info?\n. Check the format of the lines - HELP should be at the start of the comment. You can run tests locally btw with make test.\n. This would end up with a bigger final image as the build dependencies will still be present in lower layers.\n. Because these env vars aren't needed after the image build & all go build tools are cleaned up anyway. Not expecting any images to extend this one & build go stuff.\n. Docker hub can do builds on any push to any branch/tag & tag the docker image appropriately.\n. So this change makes the prod image larger as it still includes source in the final image - no way to delete that unfortunately as it's in a lower layer. If we want to keep small image size without source then perhaps we should go with an external build via jenkins/circleci? Pretty easy to set up.\n. I'd expect most tests to go through a second docker image & use the cadvisor API rather than extending this image.\n. We should sanitize rather than drop. This is normal practice for prometheus metrics.\n. initRetryWhitelist?\n. nit: can we just make this name?\n. Using - may mean users have to quote everything in their queries, otherwise might be seen as operator.\n. Cumulative dropped to be consistent with other serCpuUsage* vars below.\n. Yeah device would be better here - not my code originally ;)\n. Interesting question. Wonder if we need to worry?\n. This whole type switch needs tidying up anyway so doing that now.\n. The guard is really between createContainer & destroyContainer.\n. handler.Cleanup() should only be called when a container is removed. Not sure why you would do that here?\n. Yeah it would.\n. Fixed.\n. Agreed. I assume the point of this is to spread the collection across the housekeeping interval? I don't think this should affect the actual housekeeping interval but instead just add a random sleep (e.g. with timer.After) in to the housekeeping loop?\nThe aim must be to collect from all containers in the effective period between housekeeping runs?\n. Doesn't that have the potential of skipping a housekeeping period as @vishh detailed above?\n. Why? If we're jittering in housekeeping then this seems to be enough to spread load across the housekeeping period.\n. This is a bit redundant isn't it? Remove for now, add back in later if different from constant 1?\n. Drop maxFactor? Always set to 1 in this PR.\n. Why downgrade to an RC of Docker?\n. Wouldn't this be better done using the StorageDriver value from docker info?\n. But refactoring these checks in combination with the check on line 373 that should work.\nProblem I can see with the way you've currently done it is if anyone mounts a non-Docker devicemapper partition with Docker running on non-devicemapper storage things won't be labelled properly. A niche case of course but you know someone out there's doing it :)\n. You need to add in HasCpu: true, in here so that the CPU specs are output as metrics. You'll probably find then that there are a couple of other metrics which haven't been tested for that you'll need to insert into metrics/testdata/prometheus_metrics to get this to pass.\n. :+1: \n. I can't think of another option - the fact these keys/values are free text is a bit loose for me, but no other option.\n. config.Cgroups.Resources.Memory is int64, spec.Memory.Limit is uint64\n. Very weird.\n. @f0 Any thoughts on this?\n. Yes, hence this PR.\n. I run fedora & this works for me. Docker has changed cgroup hierarchy in 1.10 it seems.\n. Can we not run this fit devicemapper storage?\n. What about zfs storage that I think we support now?\n. Is that extra - required?\n. Don't think so - exec replaces current shell process & doesn't return to caller iirc.\nNot sure why you need to exec it though? Just call it normally? Or are you relying on the exit code of the exec script?\n. I didn't know that - thanks! Will take a look in the next few days.\n. I have no problem with exec here.\n. What causes this to happen?\n. Can we use a ring buffer here as a fixed size list?\n. Can we only do this once when starting & remove the need for this check when running normally?\n. Probably need to check for existing matching labels so we don't get any collisions?\n. How about just container_{label,env}_? At least that will be consistent for dashboards, etc across docker/rkt envs.\n. Changed to container_{label,env}_, which makes sense to me at least.\n. Should containerLabelPrefix be exported so custom mills can use it?\n. So this now handles all labels, including id name etc. I personally like that control for custom impls but it needs to be documented somewhere as well as release noted.\n. Let's just makes it clear in the godoc comment for now & make sure it's clear when used in kubernetes too.\n. That's good enough for me - thanks!\n. > the env labels have never worked for me so far, they're always empty\nIIRC env vars are whitelisted by the docker_env_metadata_whitelist flag. By default that is empty as env vars could potentially contain sensitive data so must be consciously whitelisted.\n. Ah yeah I forgot to ask what the purpose of this label was. Considering there are likely to be multiple labels on each metric, what should this be set to? @timstclair?\n. @timstclair @DirectXMan12 What should the value of label be in this case? A concatenation of all Prometheus labels?\n. Types aren't arbitrary, it's just that cadvisor doesn't support the same metric types as Prometheus & there's no way to map e.g. a Prometheus histogram to a cadvisor metric type.\n. Done.\n. Still not sure how to handle this nicely... This PR doesn't fix the existing problem, but doesn't make it worse so will raise a separate issue to figure out how to handle it, but assume that to be in a separate PR.\n. Done.\n. Done.\n. Hope what I've done it right... always confused by unicode stuff like this. Added tests too.\n. ",
    "pmoust": "@gregbkr In Kibana 4, go to Settings -> Indicies -> Select your index pattern -> hit Refresh to repopulate, find the appropriate fields and index them as needed.\n\n. ",
    "patrick-bark": "I like where this is headed, but is there no way to send this to logstash? Otherwise, creating daily indices is difficult, making retiring old data even more difficult. Furthermore that would take care of the .raw value issue. I rarely see ES ran without logstash and they really always should be paired. I think this is a real miss by going straight to ES.\n. ",
    "fiunchinho": "@gregbkr I have your same problem, but your solution doesn't seem to work for me. I still get the '/' container name after doing your proposed solution. Any help?\n. I'd love to hear more details on your problems. We are using ELK for the logs, so having system metrics on ElasticSearch looks the way to go for me, instead of adding yet another tool (presumably prometheus)\n. ",
    "JamieCressey": "Sorry to drag this back up, but I can see a LogStash driver being hugely beneficial here. \nThe use case I'm thinking of being LogStash has native support to insert data into dynamic indexes, so metrics could be stored in a multitude of ways based on a user's requirements. \nE.g. Two departments are running docker services, each get a department=foo or department=barlabel assigned to their containers. With labels being passed through with each metric to LogStash the output index by can be dynamically chosen, either foo_metrics_2017-02-08 or bar_metrics_2017-02-08 dependant on the metric's origin. Using something along the lines of;\ninput {\n...\n}\nfilter {\n  json {\n    add_field => { \"department\" => ... }\n  }\n}\noutput {  \n    elasticsearch {  \n        index => \"%{department}_metrics_%{index_day}\"   \n    }\n}\nThis also means the processing of metrics then becomes outside the scope of CAdvisor.. ",
    "justinsb": "Oh, weird.... the failure looks real, though unrelated:\n$ go vet github.com/google/cadvisor/...\nhttp/handlers.go:75: result of fmt.Errorf call not used\nexit status 1\nI'll submit another trivial PR for that one.  Then this PR should pass tests.\n. LGTM, except I don't think there's any DNS involved, as on EC2 the metadata service is on 169.254.169.254.  If I'm correct the comment is a bit misleading...\nThere is also this alternative approach, though I haven't tried it in production (it seems to be accurate though): http://serverfault.com/a/700771\nI'm fine with PR as is.  I don't think I've ever seen the metadata service fail to respond on AWS... I'm pretty sure it is implemented on the host machine, so it seems the best option, particularly if we want to get it into 1.2\n. I'd say 2 seconds is likely fine, but I'm going to open an issue to detect using the hypervisor uuid trick so we can raise this back to the default values.\n. Is there any official documentation about this from AWS?  It seems the sort of thing that could break and be hard to diagnose & fix \"in the field\".  I'd be happier if there were others in the same boat as us, but I couldn't find anyone.\nAlso, is there some way to actively detect OpenStack?  Or to differentiate OpenStack's metadata service from AWS's (e.g. could we look at the availability-zone)?\nOne alternative: have a fast-path, and then slower but more complete fallback logic.  So dmi.product_version=amazon => AWS, dmi.product_name=Google => GCE etc, but if we don't identify any of them then we go through the official procedure.  I know it's ugly, but how about something like this:\n```\nfunc detectCloudProvider() info.CloudProvider {\n       switch {\n        case fastOnGCE():\n                return info.GCE\n        case fastOnAWS():\n                return info.AWS\n        case fastOnBaremetal():\n                return info.Baremetal\n        }\n        glog.Infof(\"Fast cloud-provider detection could not determine the provider; falling back to slower approaches\")\n        switch {\n        case onGCE():\n                return info.GCE\n        case onAWS():\n                return info.AWS\n        case onBaremetal():\n                return info.Baremetal\n        }\n        return info.UnkownProvider\n}\n```\n. ",
    "humbargs": "Thanks for the info!\n. ",
    "artjomzab": "What I find confusing when using cAdvisor the first time is that it says 16Gib of a total of 24 Gib memory is used.\nVia free -m I found out that I actually have 22 Gib free memory and that 14 Gib are just cached by linux.\n. ",
    "Vanuan": "Is working set shared between containers? Because if I sum up all container's working sets it's well above the total memory. And all containers' working sets are exactly the same.\n. Ah, it's probably heapster's bug then. I use memory/working_set_bytes_gauge which is the same, no matter which container I choose.\n. Posted kubernetes/heapster#1005\n. Opened chrome tab takes 60% CPU. No wonder, if it's loading and visualizing 400 KiB of data each second.\n. So, to recap this 3 year old discussion:\n\n100% CPU is due to cadvisor calculating disk usage of each container\nthere is a way to set so called \"project quota\" in some filesystems and it can be done in cadvisor out of the box\ncadvisor devs don't want to implement it until project quota is supported on ext4\n\nSo here are the questions:\nIs there a way to disable calculating disk usage per container?\nWill disabling du calculation reduce cadvisor CPU load?\nIs it specific to ext4? Why can't it be implemented on on xfs only?\nCan't container runtime limit CPU usage or project qouta? Is it because cadvisor runs in privileged mode?. Ok, so the blocker is not in cAdvisor, but in container runtimes, right?\nIs there a tracking issue in any of these projects:\nhttps://github.com/opencontainers/runc/ \nhttps://github.com/containerd/containerd\nhttps://github.com/moby/moby\n?\nIs it something that should be handled by OCI runtime spec?. I'm using docker swarm.\nThinking more about this, since cadvisor is using bind mounts to determine disk usage, it doesn't look like container runtime can do much about the issue. For example, fstab file must be edited to support disk quota. Also volume drivers need to be exposed to that. Though I'm not sure whether cadvisor can query disk usage of remote volumes.\nRegarding the issue in moby you linked it appears that VFS only applies to images and containers. It doesn't apply to bind mounts and volumes. Correct me if I'm wrong\nCreated an issue here: https://github.com/moby/moby/issues/37574. Ok, it looks like I've missed the point. As far as I understood, cadvisor runs du command only on /var/lib/docker/overlay/, to report which containers and images consume the most space. Which appears to be VFS. Is it correct? \nBut why use du at all? Doesn't docker expose container disk metrics on Unix socket?.. But that looks to be expensive too: https://github.com/moby/moby/issues/31951\nWith --disable-metrics=disk, would cadvisor still report disk free metrics?. I don't think that docker supports dynamic EXPOSE option.\n. ",
    "cboettig": "It would be really nice if cadvisor didn't include cached memory as \"used\" in it's overview / summary graphs.  This results in confusing / inconsistent display on most linux hosts, e.g. the \"overview\" will show 90% memory used while the table directly below will list no process in which memory use is high (since it is mostly in cache).  . ",
    "amos6224": "Also getting this issue on boot2docker even after i upgrade.\n. ",
    "radcheb": "+1\nGetting same issue on Ubuntu 14.04 with no boot2docker.\n. ",
    "cgswong": "I'm on CoreOS 647.2.0 and getting the error below when running cAdvisor (Google Docker image) in a fleet/systemd unit:\nJun 09 13:55:41 stluengtst03.monsanto.com docker[22924]: I0609 13:55:41.040406 00001 manager.go:151] Version: {KernelVersion:4.0.1 ContainerOsVersion:Buildroot 2014.02 DockerVersion:1.5.0 CadvisorVersion:0.14.0}\nJun 09 13:55:41 stluengtst03.monsanto.com docker[22924]: I0609 13:55:41.137500 00001 factory.go:220] System is using systemd\nJun 09 13:55:41 stluengtst03.monsanto.com docker[22924]: I0609 13:55:41.140705 00001 factory.go:228] Registering Docker factory\nJun 09 13:55:41 stluengtst03.monsanto.com docker[22924]: I0609 13:55:41.143247 00001 factory.go:64] Registering Raw factory\nJun 09 13:55:41 stluengtst03.monsanto.com docker[22924]: I0609 13:55:41.348053 00001 manager.go:930] Started watching for new ooms in manager\nJun 09 13:55:41 stluengtst03.monsanto.com docker[22924]: E0609 13:55:41.348260 00001 manager.go:217] Failed to start OOM watcher, will not get OOM events: exec: \"journalctl\": executable file not found in $PATH\nJun 09 13:55:41 stluengtst03.monsanto.com docker[22924]: I0609 13:55:41.348930 00001 manager.go:230] Starting recovery of all containers\nJun 09 13:55:41 stluengtst03.monsanto.com docker[22924]: I0609 13:55:41.456439 00001 manager.go:235] Recovery completed\nJun 09 13:55:41 stluengtst03.monsanto.com docker[22924]: I0609 13:55:41.510994 00001 cadvisor.go:90] Starting cAdvisor version: \"0.14.0\" on port 8080\ncAdvisor remains up and running.\n. How should the image be run such that journalctl from the CoreOS host is made available to the container please? Below is my Docker run statement:\ndocker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker:/var/lib/docker:ro --publish=8080:8080 --name=%p google/cadvisor:latest --logtostderr -storage_driver=influxdb -storage_driver_host=stluengtst03.monsanto.com:8086 -storage_driver_db=cadvisor -storage_driver_user=root -storage_driver_password=root\n. ",
    "trinitronx": "Just a hint:  It's probably good to gracefully degrade here, and not require journalctl because older distro versions (e.g.: CentOS / RHEL 6) do not have systemd or journalctl.\n. Just a hint:  It's probably good to gracefully degrade here, and not require journalctl because older distro versions (e.g.: CentOS / RHEL 6) do not have systemd or journalctl.\n. @vmarmol:  To answer your question earlier in the thread. boot2docker does not have systemd or journalctl installed.  boot2docker is based on Tiny Core Linux.  A quick look at this wiki page tells me that it's using the tried and true inittab / /etc/init.d / SysV-style init process.  So no systemd there.\n. @vmarmol:  To answer your question earlier in the thread. boot2docker does not have systemd or journalctl installed.  boot2docker is based on Tiny Core Linux.  A quick look at this wiki page tells me that it's using the tried and true inittab / /etc/init.d / SysV-style init process.  So no systemd there.\n. My thoughts are that this is \"not a bug\"...    It appears that @pbhavsar211 just may have not understood that boot2docker is not using systemd as it's init system.  It also is good to know that the message: \n\nFailed to start OOM watcher, will not get OOM events: exec: \"journalctl\": executable file not found in $PATH\n\nIs really just a warning and that cAdvisor gracefully degrades.\nPerhaps really there is a hidden desire for a feature request here to add OOM-watcher support for old SysV-style init systems?\n. My thoughts are that this is \"not a bug\"...    It appears that @pbhavsar211 just may have not understood that boot2docker is not using systemd as it's init system.  It also is good to know that the message: \n\nFailed to start OOM watcher, will not get OOM events: exec: \"journalctl\": executable file not found in $PATH\n\nIs really just a warning and that cAdvisor gracefully degrades.\nPerhaps really there is a hidden desire for a feature request here to add OOM-watcher support for old SysV-style init systems?\n. Seems that some folks are running into this on CentOS 7 and CoreOS... but my intuition tells me that they're probably not running the container in a way that makes the host's journalctl available inside the container.\n. Seems that some folks are running into this on CentOS 7 and CoreOS... but my intuition tells me that they're probably not running the container in a way that makes the host's journalctl available inside the container.\n. After investigating a bit, I found that it appears the oomparser already tries to look at /var/log/messages and /var/log/syslog files first, then falls back to trying systemd.   Seems that there is no bug here.\nHowever, when I attempted to start the cadvisor container with -v /var/log/messages:/var/log/messages:ro, or with -v /var/log:/var/log:ro, the /var/log/messages file was not available inside the container.  Something else going on here that I'm missing?\n. After investigating a bit, I found that it appears the oomparser already tries to look at /var/log/messages and /var/log/syslog files first, then falls back to trying systemd.   Seems that there is no bug here.\nHowever, when I attempted to start the cadvisor container with -v /var/log/messages:/var/log/messages:ro, or with -v /var/log:/var/log:ro, the /var/log/messages file was not available inside the container.  Something else going on here that I'm missing?\n. ",
    "aecolley": "Mounting the host's /var/log onto the container's /var/log fails (at least, on docker 1.6.2 and centos 6.6). The container's /var/log is a symlink to /tmp for some reason, and the mounted directory fails to appear. The same thing obstructs mounting /var/log/messages directly (not that you'd want to do that), as the file is quickly unlinked mysteriously.\nI resorted to docker run ... --entrypoint=/bin/sh google/cadvisor:0.16.0 -c 'ln -s -f /rootfs/var/log/messages /var/log/messages && exec /usr/bin/cadvisor'\nAll of this is, strictly speaking, irrelevant. The original problem behind this bug report is that cadvisor chooses the less helpful of two possible error messages when it fails to find a kernel log file and then fails to run journalctl. If you fix that error, you can close this bug. I'll then file the centos issues in a new bug report.\n. Mangled by markdown. Let me try one more time: \".*?\\\\w:(/[^,;]*)[,;].*\"\n. ",
    "allencloud": "configure which of the existing metrics to export @vmarmol \n. Yes, it is what I am interested in. And I can prepare PR for it. @vishh \n. Done, PTAL @dashpole . ",
    "frioux": "I can repro it as well; here's what I get:\n```\nI0428 15:16:37.709945 00001 storagedriver.go:92] No backend storage selected\nI0428 15:16:37.710220 00001 storagedriver.go:94] Caching %d stats in memory60\nI0428 15:16:37.710441 00001 manager.go:105] cAdvisor running in container: \"/system.slice/docker-6489e42b05cf5e27eda698ee17fb3782948d30fbaa3cfa4503812df9c608687a.scope\"\nI0428 15:16:37.720195 00001 fs.go:87] Filesystem partitions: map[/dev/mapper/tungsten-home:{mountpoint:/rootfs/home major:252 minor:1} /dev/sda1:{mountpoint:/rootfs/boot major:8 minor:1} /dev/mapper/t\nungsten-root_x64:{mountpoint:/rootfs major:252 minor:4} /dev/mapper/tungsten-games:{mountpoint:/rootfs/games major:252 minor:3}]\nE0428 15:16:37.731583 00001 machine.go:267] Failed to get system UUID: open /sys/class/dmi/id/product_uuid: no such file or directory\nI0428 15:16:37.731756 00001 machine.go:223] Couldn't collect info from any of the files in \"/etc/machine-id,/var/lib/dbus/machine-id\"\nI0428 15:16:37.731846 00001 manager.go:126] Machine: {NumCores:2 CpuFrequency:2933000 MemoryCapacity:8238407680 MachineID: SystemUUID: BootID:304919a0-724a-4b52-abd2-d20669a649fa Filesystems:[{Device:\n/dev/sda1 Capacity:238787584} {Device:/dev/mapper/tungsten-root_x64 Capacity:27345022976} {Device:/dev/mapper/tungsten-games Capacity:21003628544} {Device:/dev/mapper/tungsten-home Capacity:8996602675\n2}] DiskMap:map[8:16:{Name:sdb Major:8 Minor:16 Size:80026361856 Scheduler:deadline} 8:32:{Name:sdc Major:8 Minor:32 Size:150039945216 Scheduler:deadline} 252:0:{Name:dm-0 Major:252 Minor:0 Size:84137\n73824 Scheduler:none} 252:2:{Name:dm-2 Major:252 Minor:2 Size:322122547200 Scheduler:none} 252:4:{Name:dm-4 Major:252 Minor:4 Size:27917287424 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:500107\n862016 Scheduler:deadline} 252:1:{Name:dm-1 Major:252 Minor:1 Size:91536490496 Scheduler:none} 252:3:{Name:dm-3 Major:252 Minor:3 Size:21474836480 Scheduler:none} 252:5:{Name:dm-5 Major:252 Minor:5 Si\nze:136365211648 Scheduler:none}] NetworkDevices:[{Name:eth0 MacAddress:00:23:ae:7a:96:20 Speed:100 Mtu:1500} {Name:lxcbr0 MacAddress:9e:93:dd:cc:f2:23 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:8238407\n680 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1}]} {Id:1 Threads:[1] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:\n1}]}] Caches:[]}]}\nI0428 15:16:37.734790 00001 manager.go:133] Version: {KernelVersion:3.13.0-34-generic ContainerOsVersion:Buildroot 2014.02 DockerVersion:1.6.0 CadvisorVersion:0.11.0}\npanic: close of closed channel\ngoroutine 18 [running]:\ngithub.com/godbus/dbus.(Conn).Close(0x4c2080b3560, 0x0, 0x0)\n        /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/godbus/dbus/conn.go:178 +0x62\ngithub.com/godbus/dbus.(Conn).inWorker(0x4c2080b3560)\n        /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/godbus/dbus/conn.go:334 +0x10be\ncreated by github.com/godbus/dbus.(*Conn).Auth\n        /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/godbus/dbus/auth.go:118 +0xe84\ngoroutine 1 [runnable]:\nfmt.(stringReader).Read(0x4c208029e00, 0x4c2080cbfd0, 0x1, 0x4, 0x1, 0x0, 0x0)\n        /usr/lib/google-golang/src/fmt/scan.go:90\nio.ReadAtLeast(0x7fedca1c7610, 0x4c208029e00, 0x4c2080cbfd0, 0x1, 0x4, 0x1, 0x0, 0x0, 0x0)\n        /usr/lib/google-golang/src/io/io.go:298 +0xf1\nio.ReadFull(0x7fedca1c7610, 0x4c208029e00, 0x4c2080cbfd0, 0x1, 0x4, 0x1, 0x0, 0x0)\n        /usr/lib/google-golang/src/io/io.go:316 +0x6d\nfmt.(readRune).readByte(0x4c2080cbfb0, 0x40e665, 0x0, 0x0)\n        /usr/lib/google-golang/src/fmt/scan.go:341 +0x142\nfmt.(readRune).ReadRune(0x4c2080cbfb0, 0x0, 0x0, 0x0, 0x0)\n        /usr/lib/google-golang/src/fmt/scan.go:357 +0x5d\nfmt.(ss).ReadRune(0x4c208120000, 0x4c200000000, 0x0, 0x0, 0x0)\n        /usr/lib/google-golang/src/fmt/scan.go:201 +0xe5\nfmt.(ss).getRune(0x4c208120000, 0x4c200000065)\n        /usr/lib/google-golang/src/fmt/scan.go:221 +0x28\nfmt.(ss).token(0x4c208120000, 0x1, 0xb1e6c8, 0x0, 0x0, 0x0)\n        /usr/lib/google-golang/src/fmt/scan.go:472 +0x5d\nfmt.(ss).convertString(0x4c208120000, 0x73, 0x0, 0x0)\n        /usr/lib/google-golang/src/fmt/scan.go:832 +0x11b\nfmt.(ss).scanOne(0x4c208120000, 0x73, 0x81d340, 0x4c208090430)\n        /usr/lib/google-golang/src/fmt/scan.go:989 +0x1132\nfmt.(ss).doScanf(0x4c208120000, 0xa24370, 0x17, 0x4c2080dd758, 0x8, 0x8, 0x6, 0x0, 0x0)\n        /usr/lib/google-golang/src/fmt/scan.go:1161 +0x373\nfmt.Fscanf(0x7fedca1c7610, 0x4c208029e00, 0xa24370, 0x17, 0x4c2080dd758, 0x8, 0x8, 0x7fedca1b5000, 0x0, 0x0)\n        /usr/lib/google-golang/src/fmt/scan.go:145 +0xc0\nfmt.Sscanf(0x4c20802b720, 0x50, 0xa24370, 0x17, 0x4c2080dd758, 0x8, 0x8, 0x80, 0x0, 0x0)\n        /usr/lib/google-golang/src/fmt/scan.go:117 +0xcb\ngithub.com/docker/docker/pkg/mount.parseInfoFile(0x7fedca1c6fb0, 0x4c208124250, 0x0, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/docker/docker/pkg/mount/mountinfo_linux.go:61 +0x69d\ngithub.com/docker/docker/pkg/mount.parseMountTable(0x0, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/docker/docker/pkg/mount/mountinfo_linux.go:39 +0x135\ngithub.com/docker/docker/pkg/mount.GetMounts(0x0, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/docker/docker/pkg/mount/mount.go:8 +0x4c\ngithub.com/docker/libcontainer/cgroups.FindCgroupMountpoint(0x9e4bb0, 0x3, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/utils.go:19 +0x5a\ngithub.com/google/cadvisor/container/docker.func\u00b7001()\n        /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/container/docker/factory.go:55 +0x50\nsync.(Once).Do(0xfec938, 0xb1ea38)\n        /usr/lib/google-golang/src/sync/once.go:44 +0xd4\ngithub.com/google/cadvisor/container/docker.UseSystemd(0x4c208028990)\n        /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/container/docker/factory.go:63 +0x36\ngithub.com/google/cadvisor/container/docker.Register(0x7fedca1c9a70, 0x4c208148000, 0x7fedca1c80b8, 0x4c20813aac0, 0x0, 0x0)\n        /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/container/docker/factory.go:211 +0x887\ngithub.com/google/cadvisor/manager.New(0x4c20810a1c0, 0x7fedca1c7f48, 0xfec8b8, 0x0, 0x0, 0x0, 0x0)\n        /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/manager/manager.go:138 +0xa20\nmain.main()\n        /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/cadvisor.go:69 +0x454\ngoroutine 5 [chan receive]:\ngithub.com/golang/glog.(*loggingT).flushDaemon(0xfe3ae0)\n        /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:839 +0x78\ncreated by github.com/golang/glog.init\u00b71\n        /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:406 +0x2a7\ngoroutine 17 [syscall, locked to thread]:\nruntime.goexit()\n        /usr/lib/google-golang/src/runtime/asm_amd64.s:2232 +0x1\ngoroutine 10 [syscall]:\nos/signal.loop()\n        /usr/lib/google-golang/src/os/signal/signal_unix.go:21 +0x1f\ncreated by os/signal.init\u00b71\n        /usr/lib/google-golang/src/os/signal/signal_unix.go:27 +0x35\n```\nNot to sound like I'm stating the obvious here, but I'd guess it's a kernel setting; /sys/class/dmi/id/product_uuid doesn't exist on my system.\n. Yep, it is the very beginning of systemd support for ubuntu.  Might be interesting to see if Debian jessie has the same problem, as they just added systemd too (and it's likely the same.)\n. ",
    "mnuessler": "Having the same problem (also using Ubuntu 15.04). Container exits. Any updates on this, by any chance?\n. @rjnagal I'm very interested in support for InfluxDB 0.9 too, so I gave it a try last week and implemented a prototype that can already write stats to InfluxDB 0.9. It is not ready to be merged yet, so I did not open a PR yet. (I still have to fix the RecentStats() function and the tests.)\nI think I could finish this up during the next few days. In case you would like to have a look at the code already, please refer to this branch.\n. Thanks for the info! With RecentStats() removed there is less work left to do for me.\nSo far I have tested against the API of 0.9.0-rc31, but I will make sure to test against the alpha1 branch also. Good point!\n. @benfb That's great news! I will test against the 0.9.0 release version.\n. I think I will be able to open a PR tomorrow. I had to change a lot when upgrading from  0.9.0-rc31 to the 0.9.0 release version.\n. @jmaitrehenry Thanks a lot :) I have just opened the PR, but I'm sure there are still many improvements to make!\n. Not sure why the CI build failed. I can't reproduce the problem locally. Looks like dependency is missing?\n. Ok, I split the commit.\n. @vmarmol Any feedback on this PR so far? :)\n. Can someone with more experience in godep please give me a hint on how to fix that? I'm kind of stuck here...\n. @mikedanese  I have fixed the Godeps.json according to your comments.\nI tried several ways of updating godeps, but none seem to work as I expected. Maybe you could point me to the correct way.\nActually I just need to upgrade the package github.com/influxdb/influxdb/client to the latest release (v0.9.1 or at least anything > v0.9.0). Starting off with a clean GOPATH, I tried the following steps:\n-  Fetch cadvisor via go get github.com/google/cadvisor.\n- Restore dependencies in GOPATH with godep restore. (Not sure if this is actually necessary.)\n- Use git to pull my cadvisor branch and switch to it.\n- Update the influxdb package in the GOPATH: go get -u github.com/influxdb/influxdb\n- Now when trying to update the godeps with godeps save or godeps save ./... I get an error like such as:\ngodep: github.com/influxdb/influxdb/influxql: revision is 8b3219e74fcc3843a6f4901bdf00e905642b6bd6, want 3284662b350688b651359f9124928856071bd3f5\n- When I do godep save github.com/influxdb/influxdb/client or `godep save github.com/influxdb/influxdb then godeps are updated, but a lot of dependencies also get deleted which caused the CI build to fail.\nThe only way I could get it to work with the CI was committing the added godeps but ignoring the deletions. It worked locally because the missing dependencies were still in the GOPATH. Any idea what I am doing wrong here?\n. Any comments on this? It's been a month already since I opened this PR...\n. @vishh I already split the change into two commits for that reason. There is one commit containing only the godep changes and another one containing only the code changes. Please have a look, maybe that is sufficient?\nUnfortunately the InfluxDB client pulls in a plethora of additional dependencies...\n. @vishh Ok, squashed commits as suggested.\n. @mikedanese Looks promising! I will do a test with the InfluxDB client from HEAD.\n. @vishh Thanks for the review! Will reply to your comments tomorrow.\n. @vishh Using the latest InfluxDB client from master we really don't need all those extra dependencies anymore, that's nice. I still need to do a test for some changes according to your comments.\n. @onlyjob Unfortunately the client of the latest release still requires that huge number of extra dependencies. If we cannot use the latest master and want to avoid pulling in all those dependencies, then we have to wait for the 0.9.5 release. \n. @mikedanese @vishh What do you think? Should we use 0.9.4.2 then and pull in all those dependencies?\n. Thanks for the info. I will update the PR for InfluxDB 0.9.5. So we can rid of the extra dependencies now and still refer to a regular release.\n. Sorry for delay. I managed to upgrade the InfluxDB client dependency to v0.9.5.1, but I had to rewrite commits in this branch. I also haven't tested it yet and I won't have time to do so before Monday.\nThanks @christianhuening and @carmark for your offer to help. I'm afraid I need to have a look first what still has to be done as I haven't worked on this for a while. But feel free to have a look at the code or do some testing if you like.\n. Rebased on master and resolved merge conflicts due to recent storage driver refactoring.\n. I think you may be missing the -storage_driver=influxdb option when running cAdvisor.\n. >  tutum/influxdb:0.8.8\nPlease note also that it won't work InfluxDB 0.8.8. The InfluxDB API changed completely with 0.9. You will have to use a recent version of InfluxDB.. ",
    "wololock": "I have the same issue on the Ubuntu 15.04 server. Here are the details:\ndocker-compose run cadvisor\nThe output:\n```\npanic: close of closed channel\ngoroutine 13 [running]:\ngithub.com/godbus/dbus.(Conn).Close(0x4c2080cd7a0, 0x0, 0x0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/godbus/dbus/conn.go:178 +0x62\ngithub.com/godbus/dbus.(Conn).inWorker(0x4c2080cd7a0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/godbus/dbus/conn.go:334 +0x10be\ncreated by github.com/godbus/dbus.(*Conn).Auth\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/godbus/dbus/auth.go:118 +0xe84\ngoroutine 1 [runnable]:\nsyscall.Syscall(0x0, 0x6, 0x4c208200000, 0x1000, 0xf52, 0x1000, 0x0)\n    /usr/lib/google-golang/src/syscall/asm_linux_amd64.s:21 +0x5\nsyscall.read(0x6, 0x4c208200000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /usr/lib/google-golang/src/syscall/zsyscall_linux_amd64.go:860 +0x6e\nsyscall.Read(0x6, 0x4c208200000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /usr/lib/google-golang/src/syscall/syscall_unix.go:136 +0x58\nos.(File).read(0x4c2081fe000, 0x4c208200000, 0x1000, 0x1000, 0x7f6f4811f188, 0x0, 0x0)\n    /usr/lib/google-golang/src/os/file_unix.go:191 +0x5e\nos.(File).Read(0x4c2081fe000, 0x4c208200000, 0x1000, 0x1000, 0x875680, 0x0, 0x0)\n    /usr/lib/google-golang/src/os/file.go:95 +0x91\nbufio.(Scanner).Scan(0x4c208202000, 0x4c208239050)\n    /usr/lib/google-golang/src/bufio/scan.go:180 +0x688\ngithub.com/docker/docker/pkg/mount.parseInfoFile(0x7f6f48117fb0, 0x4c2081fe000, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/docker/docker/pkg/mount/mountinfo_linux.go:48 +0x1f0\ngithub.com/docker/docker/pkg/mount.parseMountTable(0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/docker/docker/pkg/mount/mountinfo_linux.go:39 +0x135\ngithub.com/docker/docker/pkg/mount.GetMounts(0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/docker/docker/pkg/mount/mount.go:8 +0x4c\ngithub.com/docker/libcontainer/cgroups.FindCgroupMountpoint(0xa315b0, 0x3, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/utils.go:19 +0x5a\ngithub.com/google/cadvisor/container/docker.func\u00b7001()\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/container/docker/factory.go:62 +0x50\nsync.(Once).Do(0x1060c38, 0xb7c998)\n    /usr/lib/google-golang/src/sync/once.go:44 +0xd4\ngithub.com/google/cadvisor/container/docker.UseSystemd(0x4c2080d51f0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/container/docker/factory.go:70 +0x36\ngithub.com/google/cadvisor/container/docker.Register(0x7f6f4811ee80, 0x4c208078180, 0x7f6f4811d330, 0x4c208111c10, 0x0, 0x0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/container/docker/factory.go:219 +0x887\ngithub.com/google/cadvisor/manager.New(0x4c2080c05c0, 0x7f6f481190f8, 0x1060bb8, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/manager/manager.go:145 +0xa76\nmain.main()\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/cadvisor.go:69 +0x454\ngoroutine 5 [chan receive]:\ngithub.com/golang/glog.(*loggingT).flushDaemon(0x1057ce0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:839 +0x78\ncreated by github.com/golang/glog.init\u00b71\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:406 +0x2a7\ngoroutine 17 [syscall, locked to thread]:\nruntime.goexit()\n    /usr/lib/google-golang/src/runtime/asm_amd64.s:2232 +0x1\ngoroutine 11 [syscall]:\nos/signal.loop()\n    /usr/lib/google-golang/src/os/signal/signal_unix.go:21 +0x1f\ncreated by os/signal.init\u00b71\n    /usr/lib/google-golang/src/os/signal/signal_unix.go:27 +0x35\n```\nKernel:\n$ uname -a\nLinux sd-77353 3.19.0-16-generic #16-Ubuntu SMP Thu Apr 30 16:09:58 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\nDmesg:\n[113246.872573] aufs au_opts_verify:1612:docker[24990]: dirperm1 breaks the protection by the permission bits on the lower branch\n[113246.923451] aufs au_opts_verify:1612:docker[24990]: dirperm1 breaks the protection by the permission bits on the lower branch\n[113246.976977] aufs au_opts_verify:1612:docker[25601]: dirperm1 breaks the protection by the permission bits on the lower branch\n[113246.991649] device veth737ddc2 entered promiscuous mode\n[113246.991685] IPv6: ADDRCONF(NETDEV_UP): veth737ddc2: link is not ready\n[113247.015784] eth0: renamed from veth4bf4b54\n[113247.030776] IPv6: ADDRCONF(NETDEV_CHANGE): veth737ddc2: link becomes ready\n[113247.030791] docker0: port 17(veth737ddc2) entered forwarding state\n[113247.030794] docker0: port 17(veth737ddc2) entered forwarding state\n[113247.076121] audit: type=1107 audit(1432224892.577:85): pid=684 uid=105 auid=4294967295 ses=4294967295 msg='apparmor=\"DENIED\" operation=\"dbus_method_call\"  bus=\"system\" path=\"/org/freedesktop/DBus\" interface=\"org.freedesktop.DBus\" member=\"Hello\" mask=\"send\" name=\"org.freedesktop.DBus\" pid=32599 label=\"docker-default\" peer_label=\"unconfined\"\n exe=\"/usr/bin/dbus-daemon\" sauid=105 hostname=? addr=? terminal=?'\n[113247.102955] docker0: port 17(veth737ddc2) entered disabled state\n[113247.103865] device veth737ddc2 left promiscuous mode\n[113247.103868] docker0: port 17(veth737ddc2) entered disabled state\n[113247.446419] device veth7bd7d56 entered promiscuous mode\n[113247.446451] IPv6: ADDRCONF(NETDEV_UP): veth7bd7d56: link is not ready\n[113247.446453] docker0: port 17(veth7bd7d56) entered forwarding state\n[113247.446456] docker0: port 17(veth7bd7d56) entered forwarding state\n[113247.475658] eth0: renamed from vethfe713ef\n[113247.495000] IPv6: ADDRCONF(NETDEV_CHANGE): veth7bd7d56: link becomes ready\n[113247.546881] audit: type=1107 audit(1432224893.049:86): pid=684 uid=105 auid=4294967295 ses=4294967295 msg='apparmor=\"DENIED\" operation=\"dbus_method_call\"  bus=\"system\" path=\"/org/freedesktop/DBus\" interface=\"org.freedesktop.DBus\" member=\"Hello\" mask=\"send\" name=\"org.freedesktop.DBus\" pid=32636 label=\"docker-default\" peer_label=\"unconfined\"\n exe=\"/usr/bin/dbus-daemon\" sauid=105 hostname=? addr=? terminal=?'\n[113247.583168] docker0: port 17(veth7bd7d56) entered disabled state\n[113247.584160] device veth7bd7d56 left promiscuous mode\n[113247.584163] docker0: port 17(veth7bd7d56) entered disabled state\n[113248.018194] device veth5aaa735 entered promiscuous mode\n[113248.018224] IPv6: ADDRCONF(NETDEV_UP): veth5aaa735: link is not ready\n[113248.018226] docker0: port 17(veth5aaa735) entered forwarding state\n[113248.018228] docker0: port 17(veth5aaa735) entered forwarding state\n[113248.039857] eth0: renamed from veth78b60ba\n[113248.055183] IPv6: ADDRCONF(NETDEV_CHANGE): veth5aaa735: link becomes ready\n[113248.104497] audit: type=1107 audit(1432224893.605:87): pid=684 uid=105 auid=4294967295 ses=4294967295 msg='apparmor=\"DENIED\" operation=\"dbus_method_call\"  bus=\"system\" path=\"/org/freedesktop/DBus\" interface=\"org.freedesktop.DBus\" member=\"Hello\" mask=\"send\" name=\"org.freedesktop.DBus\" pid=32666 label=\"docker-default\" peer_label=\"unconfined\"\n exe=\"/usr/bin/dbus-daemon\" sauid=105 hostname=? addr=? terminal=?'\n[113248.131444] docker0: port 17(veth5aaa735) entered disabled state\n[113248.132402] device veth5aaa735 left promiscuous mode\n[113248.132405] docker0: port 17(veth5aaa735) entered disabled state\nI'm not the Ubuntu user, but this apparmor=DENIED thing looks suspicious. I heard that people had some problems with docker due to this AppArmor thing. What do you think about it?\n. I have the same issue on the Ubuntu 15.04 server. Here are the details:\ndocker-compose run cadvisor\nThe output:\n```\npanic: close of closed channel\ngoroutine 13 [running]:\ngithub.com/godbus/dbus.(Conn).Close(0x4c2080cd7a0, 0x0, 0x0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/godbus/dbus/conn.go:178 +0x62\ngithub.com/godbus/dbus.(Conn).inWorker(0x4c2080cd7a0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/godbus/dbus/conn.go:334 +0x10be\ncreated by github.com/godbus/dbus.(*Conn).Auth\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/godbus/dbus/auth.go:118 +0xe84\ngoroutine 1 [runnable]:\nsyscall.Syscall(0x0, 0x6, 0x4c208200000, 0x1000, 0xf52, 0x1000, 0x0)\n    /usr/lib/google-golang/src/syscall/asm_linux_amd64.s:21 +0x5\nsyscall.read(0x6, 0x4c208200000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /usr/lib/google-golang/src/syscall/zsyscall_linux_amd64.go:860 +0x6e\nsyscall.Read(0x6, 0x4c208200000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /usr/lib/google-golang/src/syscall/syscall_unix.go:136 +0x58\nos.(File).read(0x4c2081fe000, 0x4c208200000, 0x1000, 0x1000, 0x7f6f4811f188, 0x0, 0x0)\n    /usr/lib/google-golang/src/os/file_unix.go:191 +0x5e\nos.(File).Read(0x4c2081fe000, 0x4c208200000, 0x1000, 0x1000, 0x875680, 0x0, 0x0)\n    /usr/lib/google-golang/src/os/file.go:95 +0x91\nbufio.(Scanner).Scan(0x4c208202000, 0x4c208239050)\n    /usr/lib/google-golang/src/bufio/scan.go:180 +0x688\ngithub.com/docker/docker/pkg/mount.parseInfoFile(0x7f6f48117fb0, 0x4c2081fe000, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/docker/docker/pkg/mount/mountinfo_linux.go:48 +0x1f0\ngithub.com/docker/docker/pkg/mount.parseMountTable(0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/docker/docker/pkg/mount/mountinfo_linux.go:39 +0x135\ngithub.com/docker/docker/pkg/mount.GetMounts(0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/docker/docker/pkg/mount/mount.go:8 +0x4c\ngithub.com/docker/libcontainer/cgroups.FindCgroupMountpoint(0xa315b0, 0x3, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/docker/libcontainer/cgroups/utils.go:19 +0x5a\ngithub.com/google/cadvisor/container/docker.func\u00b7001()\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/container/docker/factory.go:62 +0x50\nsync.(Once).Do(0x1060c38, 0xb7c998)\n    /usr/lib/google-golang/src/sync/once.go:44 +0xd4\ngithub.com/google/cadvisor/container/docker.UseSystemd(0x4c2080d51f0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/container/docker/factory.go:70 +0x36\ngithub.com/google/cadvisor/container/docker.Register(0x7f6f4811ee80, 0x4c208078180, 0x7f6f4811d330, 0x4c208111c10, 0x0, 0x0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/container/docker/factory.go:219 +0x887\ngithub.com/google/cadvisor/manager.New(0x4c2080c05c0, 0x7f6f481190f8, 0x1060bb8, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/manager/manager.go:145 +0xa76\nmain.main()\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/cadvisor.go:69 +0x454\ngoroutine 5 [chan receive]:\ngithub.com/golang/glog.(*loggingT).flushDaemon(0x1057ce0)\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:839 +0x78\ncreated by github.com/golang/glog.init\u00b71\n    /usr/local/google/home/vmarmol/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:406 +0x2a7\ngoroutine 17 [syscall, locked to thread]:\nruntime.goexit()\n    /usr/lib/google-golang/src/runtime/asm_amd64.s:2232 +0x1\ngoroutine 11 [syscall]:\nos/signal.loop()\n    /usr/lib/google-golang/src/os/signal/signal_unix.go:21 +0x1f\ncreated by os/signal.init\u00b71\n    /usr/lib/google-golang/src/os/signal/signal_unix.go:27 +0x35\n```\nKernel:\n$ uname -a\nLinux sd-77353 3.19.0-16-generic #16-Ubuntu SMP Thu Apr 30 16:09:58 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\nDmesg:\n[113246.872573] aufs au_opts_verify:1612:docker[24990]: dirperm1 breaks the protection by the permission bits on the lower branch\n[113246.923451] aufs au_opts_verify:1612:docker[24990]: dirperm1 breaks the protection by the permission bits on the lower branch\n[113246.976977] aufs au_opts_verify:1612:docker[25601]: dirperm1 breaks the protection by the permission bits on the lower branch\n[113246.991649] device veth737ddc2 entered promiscuous mode\n[113246.991685] IPv6: ADDRCONF(NETDEV_UP): veth737ddc2: link is not ready\n[113247.015784] eth0: renamed from veth4bf4b54\n[113247.030776] IPv6: ADDRCONF(NETDEV_CHANGE): veth737ddc2: link becomes ready\n[113247.030791] docker0: port 17(veth737ddc2) entered forwarding state\n[113247.030794] docker0: port 17(veth737ddc2) entered forwarding state\n[113247.076121] audit: type=1107 audit(1432224892.577:85): pid=684 uid=105 auid=4294967295 ses=4294967295 msg='apparmor=\"DENIED\" operation=\"dbus_method_call\"  bus=\"system\" path=\"/org/freedesktop/DBus\" interface=\"org.freedesktop.DBus\" member=\"Hello\" mask=\"send\" name=\"org.freedesktop.DBus\" pid=32599 label=\"docker-default\" peer_label=\"unconfined\"\n exe=\"/usr/bin/dbus-daemon\" sauid=105 hostname=? addr=? terminal=?'\n[113247.102955] docker0: port 17(veth737ddc2) entered disabled state\n[113247.103865] device veth737ddc2 left promiscuous mode\n[113247.103868] docker0: port 17(veth737ddc2) entered disabled state\n[113247.446419] device veth7bd7d56 entered promiscuous mode\n[113247.446451] IPv6: ADDRCONF(NETDEV_UP): veth7bd7d56: link is not ready\n[113247.446453] docker0: port 17(veth7bd7d56) entered forwarding state\n[113247.446456] docker0: port 17(veth7bd7d56) entered forwarding state\n[113247.475658] eth0: renamed from vethfe713ef\n[113247.495000] IPv6: ADDRCONF(NETDEV_CHANGE): veth7bd7d56: link becomes ready\n[113247.546881] audit: type=1107 audit(1432224893.049:86): pid=684 uid=105 auid=4294967295 ses=4294967295 msg='apparmor=\"DENIED\" operation=\"dbus_method_call\"  bus=\"system\" path=\"/org/freedesktop/DBus\" interface=\"org.freedesktop.DBus\" member=\"Hello\" mask=\"send\" name=\"org.freedesktop.DBus\" pid=32636 label=\"docker-default\" peer_label=\"unconfined\"\n exe=\"/usr/bin/dbus-daemon\" sauid=105 hostname=? addr=? terminal=?'\n[113247.583168] docker0: port 17(veth7bd7d56) entered disabled state\n[113247.584160] device veth7bd7d56 left promiscuous mode\n[113247.584163] docker0: port 17(veth7bd7d56) entered disabled state\n[113248.018194] device veth5aaa735 entered promiscuous mode\n[113248.018224] IPv6: ADDRCONF(NETDEV_UP): veth5aaa735: link is not ready\n[113248.018226] docker0: port 17(veth5aaa735) entered forwarding state\n[113248.018228] docker0: port 17(veth5aaa735) entered forwarding state\n[113248.039857] eth0: renamed from veth78b60ba\n[113248.055183] IPv6: ADDRCONF(NETDEV_CHANGE): veth5aaa735: link becomes ready\n[113248.104497] audit: type=1107 audit(1432224893.605:87): pid=684 uid=105 auid=4294967295 ses=4294967295 msg='apparmor=\"DENIED\" operation=\"dbus_method_call\"  bus=\"system\" path=\"/org/freedesktop/DBus\" interface=\"org.freedesktop.DBus\" member=\"Hello\" mask=\"send\" name=\"org.freedesktop.DBus\" pid=32666 label=\"docker-default\" peer_label=\"unconfined\"\n exe=\"/usr/bin/dbus-daemon\" sauid=105 hostname=? addr=? terminal=?'\n[113248.131444] docker0: port 17(veth5aaa735) entered disabled state\n[113248.132402] device veth5aaa735 left promiscuous mode\n[113248.132405] docker0: port 17(veth5aaa735) entered disabled state\nI'm not the Ubuntu user, but this apparmor=DENIED thing looks suspicious. I heard that people had some problems with docker due to this AppArmor thing. What do you think about it?\n. ",
    "pierreozoux": "@wololock would you mind sharing a working nginx config? Thanks!\n. ",
    "AnthonyWC": "Looks like it is still broken, see https://github.com/google/cadvisor/pull/680. Re: https://github.com/google/cadvisor/issues/667\nI tested (via https://x.x.x.x/cadvisor/):\nlocation /cadvisor/ {\n      proxy_pass         http://prometheus_cadvisor_1:8080/;\n      proxy_set_header   Host $host;\n      proxy_set_header   X-Forwarded-For $remote_addr;\n    }\nResults in 404 but this work\nlocation / {\n      proxy_pass         http://prometheus_cadvisor_1:8080/;\n      proxy_set_header   Host $host;\n      proxy_set_header   X-Forwarded-For $remote_addr;\n    }\nSo issue still exists as they reverted the change earlier and never fix it.\nI can manually specify the url path, like this:\nlocation / {\n      proxy_pass         http://prometheus_cadvisor_1:8080/containers/;\n      proxy_set_header   Host $host;\n      proxy_set_header   X-Forwarded-For $remote_addr;\n    }\nIt doesn't display properly but doesn't show 404 error, and any non /containers/ links will break.\n. ",
    "remoteur": "I tried that with SELinux set to permissive. Looks like I had to remove all the mounts for it to start :)  When I switched it to disabled it worked. I'll look further into what's SELinux blocking.\nThanks\n. ",
    "matschaffer": "For anyone who comes looking in the short term here's a dump from the initial \"kick-the-tires\" attempt I just did: https://gist.github.com/matschaffer/328e891e0ebc1eeb9705\nAlso I thought it was worth mention that the current design could have some issues. The influx 0.8 schema design docs recommend encoding most of the metadata into the series name.\nThe current approach would probably require a fair bit of queries like where machine = '...' which apparently requires a range scan.\nIt's possible 0.9 won't have this issue, but I haven't tried using it yet.\nAlso here's a screenshot which may be easier for folks to use to scan the structure:\n\n. ",
    "pchico83": "The wrong value was for the total machine memory [\"NodeInfo\"][\"Stats\"][0][\"memory\"][\"working_set\"]:\nThis is the output of the commands:\n```\n$ cat /proc/meminfo\nMemTotal:         501800 kB\nMemFree:           13808 kB\nBuffers:           29144 kB\nCached:            91004 kB\nSwapCached:       120604 kB\nActive:            73880 kB\nInactive:         261580 kB\nActive(anon):      22844 kB\nInactive(anon):   193448 kB\nActive(file):      51036 kB\nInactive(file):    68132 kB\nUnevictable:           0 kB\nMlocked:               0 kB\nSwapTotal:       2097148 kB\nSwapFree:        1756088 kB\nDirty:              2212 kB\nWriteback:             0 kB\nAnonPages:        130436 kB\nMapped:            34400 kB\nShmem:               968 kB\nSlab:             102416 kB\nSReclaimable:      66384 kB\nSUnreclaim:        36032 kB\nKernelStack:        3608 kB\nPageTables:         9528 kB\nNFS_Unstable:          0 kB\nBounce:                0 kB\nWritebackTmp:          0 kB\nCommitLimit:     2348048 kB\nCommitted_AS:    3183124 kB\nVmallocTotal:   34359738367 kB\nVmallocUsed:        8568 kB\nVmallocChunk:   34359710588 kB\nHardwareCorrupted:     0 kB\nAnonHugePages:         0 kB\nHugePages_Total:       0\nHugePages_Free:        0\nHugePages_Rsvd:        0\nHugePages_Surp:        0\nHugepagesize:       2048 kB\nDirectMap4k:       40952 kB\nDirectMap2M:      483328 kB\nDirectMap1G:           0 kB\n$ cat /sys/devices/system/node/node*/meminfo\nNode 0 MemTotal:         501800 kB\nNode 0 MemFree:           13388 kB\nNode 0 MemUsed:          488412 kB\nNode 0 Active:            73944 kB\nNode 0 Inactive:         261936 kB\nNode 0 Active(anon):      22824 kB\nNode 0 Inactive(anon):   193444 kB\nNode 0 Active(file):      51120 kB\nNode 0 Inactive(file):    68492 kB\nNode 0 Unevictable:           0 kB\nNode 0 Mlocked:               0 kB\nNode 0 Dirty:                 0 kB\nNode 0 Writeback:             0 kB\nNode 0 FilePages:        241176 kB\nNode 0 Mapped:            34404 kB\nNode 0 AnonPages:        130952 kB\nNode 0 Shmem:               968 kB\nNode 0 KernelStack:        3608 kB\nNode 0 PageTables:         9528 kB\nNode 0 NFS_Unstable:          0 kB\nNode 0 Bounce:                0 kB\nNode 0 WritebackTmp:          0 kB\nNode 0 Slab:             102244 kB\nNode 0 SReclaimable:      66212 kB\nNode 0 SUnreclaim:        36032 kB\nNode 0 AnonHugePages:         0 kB\nNode 0 HugePages_Total:     0\nNode 0 HugePages_Free:      0\nNode 0 HugePages_Surp:      0\n```\n. Yes, it is the root container, the output is:\n$ cat /sys/fs/cgroup/memory/memory.stat\ncache 47058944\nrss 53981184\nrss_huge 0\nmapped_file 8974336\nwriteback 0\npgpgin 62813689\npgpgout 62789021\npgfault 300595765\npgmajfault 160110\ninactive_anon 126111744\nactive_anon 8310784\ninactive_file 23064576\nactive_file 22962176\nunevictable 0\nhierarchical_memory_limit 18446744073709551615\ntotal_cache 103493632\ntotal_rss 163254272\ntotal_rss_huge 0\ntotal_mapped_file 33714176\ntotal_writeback 0\ntotal_pgpgin 132762474\ntotal_pgpgout 132697350\ntotal_pgfault 681232772\ntotal_pgmajfault 408644\ntotal_inactive_anon 225005568\ntotal_active_anon 18759680\ntotal_inactive_file 56545280\ntotal_active_file 45862912\ntotal_unevictable 0\n$ cat /sys/fs/cgroup/memory/memory.usage_in_bytes\n267001856\n. ",
    "MrMMorris": "yes please! I need to be able to differentiate containers as I don't name them and docker labels seems to be the best way to do it :smile: \n. @Marmelatze thanks so much for taking this on. It is basically a requirement when using Prometheus with Consul if I want to populate graphs dynamically. Now I just have to LABEL my images and it will Just Work\u2122 :grinning:\n. ",
    "Colstuwjx": "I have some docker envs which need to inject as prom labels, how to achieve that goal?\nIt seems that we need a more flexible way to determine prom labels.... I have been checked the source code, and found that cadvisor will set some default labels, and envs are already exist.\nBTW,  I also found that there is a ContainerSpec struct to determine the container metadata, but the env will be whitelist? How to setup these configs? \nI cannot find any documentation about this, and I also suggest docker inspect result should have a simple  way to inject as part of ContainerSpect, e.g Network IPAddress.. Thanks for your reply.\nAlthough cadvisor support custom application metrics, app container needs to expose an endpoint which contains these metrics, but e.g. tcp connections, cpu load, process stats, these are basic system level metrics, and should have a pretty way to achieve, rather than binding into application's code logical.. As #1674 merged, I'd like to close this, since each metrics is included in prom /metrics endpoint.. Sorry, since the author config is wrong and cannot take a sign, I'd like to recreate one.. Ping @dashpole  @jimmidyson . Yeah, I mean that due to no ignoreMetrics in ContainerSpec, I cannot determine whether the metric is disabled. In the code level, ignoreMetrics should not appeared in that, so, just expose zero values if they are disabled.... @dashpole  Hi, I have made things clear, pls let me know if there is something need to change.. Done.. I've already update to date, let me know if there is anything need to correct, @dashpole .. BTW, after tested on the host, I confirmed that cadvisor run in container would not show the metrics, and on the host would be OK.  @mindprince . Hi @mindprince , I have been almost solved the problem \"collect GPU metrics via cadvisor running inside container\", but there still have some questions need to answer or document:\n\n\nI need to set cadvisor container running with --privileged, is there any way, such as fulfilled with other specified capacity or sysctl arguments  rather than full privileged unsafe container;\n\n\nI have been added -e LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64   --volume=/usr/local/lib/nvidia_drivers:/usr/local/nvidia/lib64 while run up the cadvisor, we should document it, I'm not sure whether these are full requirements, but it works;\n\n\nwhile I cannot collect the gpu metrics from cadvisor, the log in cadvisor I can see above is ONLY NVML initialized. Number of nvidia devices: 4, we should make it more clear, such as adding more log about the initialize devices -> add nvidia collector -> containerData updateStats, to told us that we have some nvidia devices, and can NOT collect metrics, or we cannot recognize the nvidia device, things like adding logging at line 132 should be good.\n\n\nBTW, after custom build cadvisor with more debug logging, I found that building the cadvisor would also need nvidia libraries?  After moving to GPU host, I build it ok. maybe we also need to update the build page? I'm not sure, haha.\n\n\nAnyway, thanks for your PR again, it's great to naturally use cadvisor to monitor GPU container! . Yeah, I'm using the /metrics exposed by cadvisor and collect them into prometheus. I think image size or total container disk usage in the other word, is really needed for container disk usage monitoring.. ",
    "superzhaoyy": "I signed it!\n.  hi  @vmarmol     why I can't  add the redis godep   on Godeps?\nwhen I run [ go get github.com/garyburd/redigo/redis] [ godep save ] ,I haven't changed anything in Godeps/Godeps.json and Godeps/_workspace\nwhen I run [ go get github.com/garyburd/redigo/redis] [ godep save ] \nresult is \uff1a\ngodep: github.com/docker/docker/pkg/symlink: revision is af9dac9627e7b04a803df152cc24f0db073ea17a, want 8e107a93210c54f22ec1354d969c771b1abfbe058e107a93210c54f22ec1354d969c771b1abfbe05\n. thanks@vmarmol\nAs you said, I successfully added redis in Godeps.\n. hi@rjnagal\nI have modified some mistakes that you had pointed out\uff0cbut how can i squash my commits\nthank you.\n. hi @vmarmol \noh ,it's sad .I add more commits because i make mistakes by using git .\nThis article  http://gitready.com/advanced/2009/02/10/squashing-commits-with-rebase.html\nsays \"A word of caution: Only do this on commits that haven\u2019t been pushed an external repository\"\nbut I have pushed commits to remote master, so can't I make my commits into less commits ?\n can i give up this PR, then I give another PR ?\n. @vmarmol \nyes.it is sad .I am not able to skillfully use Git. I have to delete my repository,i refork google/cadvisor.\ncan i give up this PR, then I give another PR ? \nThank you for your patience to answer.\n:+1: \n. thanks@vmarmol\nI have make a new PR. Please close this PR.\nIt's nice talking with you.\n. hi@vmarmol .\nyou can pass some additional flags to cAdvisor telling it what's the key when you run a cAdvisor.\nwhen a cAdvisor is running ,the key is unique,we use a list to store the value.\nAt set intervals, we add the newest values to the list every time .you can use redis's \"LRANGE\" to get any range of the list. It's flexible.\nfor example\uff1a\nredis> LRANGE cadvisor 0 9      // we can get ten of the latest data\n. ",
    "shahriarb": "I'm experiencing this issue as well .\n@vmarmol I've tested your suggestion : \n- Don't mount /sys (but leave /)  :  It will raise this error \n  docker: Invalid bind mount: destination can't be '/'\n- Run with --privileged   : It will raise this error : \n  System error: \"/var/lib/docker/aufs/mnt/89d25450b19ef9abfaa56ee9bb5389b9a2bdfccee2ebb4d6fdc68505f78d659c/sys\" cannot be mounted because it is located inside \"/sys\" \nAny other suggestion or fix ?\n. ",
    "prologic": "Same issue here:\n#!bash\nStatus: Downloaded newer image for google/cadvisor:latest\nCannot start container 18d3d8ffd585d81cce22a24f8f207dec04075867b0b4a5801a177f3dca110549: [8] System error: \"/var/lib/docker/devicemapper/mnt/18d3d8ffd585d81cce22a24f8f207dec04075867b0b4a5801a177f3dca110549/rootfs/sys\" cannot be mounted because it is located inside \"/sys\"\n. Is there an upstream Docker issue we can cross-link to for reference?\n. Beautiful :)\n. Sweet :) Can't wait!\n. :+1:\n. ",
    "jumanjiman": "\nIs there an upstream Docker issue we can cross-link to for reference?\n\nhttps://github.com/docker/docker/issues/13097\n. ",
    "Shinzu": "even if i set --enable_load_reader=true the 'has_load' label is still set to false:\n```\n/ # cadvisor --version\ncAdvisor version 0.14.0\n/ # ps\nPID   USER     COMMAND\n    1 root     /usr/bin/cadvisor --enable_load_reader=true\ncurl http://192.168.59.103:8080/api/v2.0/stats\\?count\\=1 | jq .\n...\n          \"weighted_io_time\": 0\n        }\n      ],\n      \"has_load\": false,\n      \"load_stats\": {\n        \"nr_sleeping\": 0,\n...\n```\n. ",
    "bdanofsky": "Victor:\nI was using the manager code to access container stats directly.  What caught me off guard was I called exists() on a docker container and got back false.  After investigating I say the name space was not set.  I should have noted all the docker Api calls have docker in the name, I should have caught that faster.\nRegards\nBrad\n\nOn May 20, 2015, at 10:22 AM, Victor Marmol notifications@github.com wrote:\n@bdanofsky I'm curious, are you consuming the cAdvisor manager as an API instead of using the remote one? In the remote one we are starting to have the concept of \"Docker\" container, the internal interfaces are all raw containers pretty much. We could do a better job in that interface as @rjnagal mentioned. No one has used that API till now so it wasn't prioritized, which is why I ask :)\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "rosskukulinski": "We aren't sending any container data to statsd currently.  We briefly played with some collectd plugins that supposedly get cgroup data and pipe it to graphite directly -- but were not successful in a quick test.\nSome background on statsd:\nStatsd is actually a pretty simple nodejs metric aggregation daemon.  It listens for udp or tcp messages of the format: my.metric:1|c.  That would increment a counter called 'my.metric' by 1.  Other types exist as well.  I think most of the data coming out of cAdvisor would correspond with the 'gauge' metric type.\nStatsd receives and aggregates metrics before sending them to a storage backend like graphite.  Graphite has been around for a long time - it provides metric storage and graph rendering and supports clustering natively.  Statsd does have a number of 3rd party backends as well.\nI do not believe you can use slashes in the metric name as Whisper (graphite's underlying storage mechanism) uses the filesystem.  Quick googling has confirmed this.  We replace the slashes in all of our HTTP metrics with underscores before sending to statsd.\n. Amazing!  I'll try to find some time and take this for a spin tomorrow\nOn Sunday, July 5, 2015, Julien Maitrehenry notifications@github.com\nwrote:\n\nHi, I just finish a statsd storage for cadvisor, if you wish try it and\nsend me some feedback about how I did it, you can test it by running my\ndocker image jmaitrehenry/cadvisor or by checking the PR #798\nhttps://github.com/google/cadvisor/pull/798\nThis is how I run it:\ndocker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  jmaitrehenry/cadvisor \\\n  -storage_driver=statsd \\\n  -storage_driver_host=192.168.59.3:8125 \\\n  -storage_driver_db=docker_node_001\nWith storage_driver_db is use as a prefix for all stats, I use it for\nprefixing stats with the docker node hostname for example.\nThis is a sample of gauge receive by statsd :\ngauges:\n   { [...]\n     'docker_001.lonely_yonath.memory_working_set': 47902720,\n     'docker_001.lonely_yonath.rx_bytes': 8915403,\n     'docker_001.lonely_yonath.rx_errors': 0,\n     'docker_001.lonely_yonath.tx_bytes': 1473345,\n     'docker_001.lonely_yonath.tx_errors': 0,\n     'docker_001.lonely_yonath.cpu_cumulative_usage': 15310631766,\n     'docker_001.lonely_yonath.memory_usage': 61747200,\n     'docker_001.lonely_yonath.-dev-sda1.fs_limit': 19507089408,\n     'docker_001.lonely_yonath.-dev-sda1.fs_usage': 12288,\n     'docker_001.lonely_yonath.fs_summary.fs_limit': 19507089408,\n     'docker_001.lonely_yonath.fs_summary.fs_usage': 12288,\n     'docker_001.cadvisor.tx_bytes': 7236437,\n     'docker_001.cadvisor.tx_errors': 0,\n     'docker_001.cadvisor.cpu_cumulative_usage': 106884766265,\n     'docker_001.cadvisor.memory_usage': 26087424,\n     'docker_001.cadvisor.memory_working_set': 26087424,\n     'docker_001.cadvisor.rx_bytes': 5218,\n     'docker_001.cadvisor.rx_errors': 0,\n     'docker_001.cadvisor.-dev-sda1.fs_limit': 19507089408,\n     'docker_001.cadvisor.-dev-sda1.fs_usage': 28672,\n     'docker_001.cadvisor.fs_summary.fs_limit': 19507089408,\n     'docker_001.cadvisor.fs_summary.fs_usage': 28672,\n     [...]\n },\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/724#issuecomment-118653232.\n. \n",
    "jmaitrehenry": "Hi, I just finish a statsd storage for cadvisor, if you wish try it and send me some feedback about how I did it, you can test it by running my docker image jmaitrehenry/cadvisor or by checking the PR #798\nThis is how I run it:\ndocker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  jmaitrehenry/cadvisor \\\n  -storage_driver=statsd \\\n  -storage_driver_host=192.168.59.3:8125 \\\n  -storage_driver_db=docker_node_001\nWith storage_driver_db is use as a prefix for all stats, I use it for prefixing stats with the docker node hostname for example.\nThis is a sample of gauge receive by statsd :\ngauges: \n   { [...]\n     'docker_001.lonely_yonath.memory_working_set': 47902720,\n     'docker_001.lonely_yonath.rx_bytes': 8915403,\n     'docker_001.lonely_yonath.rx_errors': 0,\n     'docker_001.lonely_yonath.tx_bytes': 1473345,\n     'docker_001.lonely_yonath.tx_errors': 0,\n     'docker_001.lonely_yonath.cpu_cumulative_usage': 15310631766,\n     'docker_001.lonely_yonath.memory_usage': 61747200,\n     'docker_001.lonely_yonath.-dev-sda1.fs_limit': 19507089408,\n     'docker_001.lonely_yonath.-dev-sda1.fs_usage': 12288,\n     'docker_001.lonely_yonath.fs_summary.fs_limit': 19507089408,\n     'docker_001.lonely_yonath.fs_summary.fs_usage': 12288,\n     'docker_001.cadvisor.tx_bytes': 7236437,\n     'docker_001.cadvisor.tx_errors': 0,\n     'docker_001.cadvisor.cpu_cumulative_usage': 106884766265,\n     'docker_001.cadvisor.memory_usage': 26087424,\n     'docker_001.cadvisor.memory_working_set': 26087424,\n     'docker_001.cadvisor.rx_bytes': 5218,\n     'docker_001.cadvisor.rx_errors': 0,\n     'docker_001.cadvisor.-dev-sda1.fs_limit': 19507089408,\n     'docker_001.cadvisor.-dev-sda1.fs_usage': 28672,\n     'docker_001.cadvisor.fs_summary.fs_limit': 19507089408,\n     'docker_001.cadvisor.fs_summary.fs_usage': 28672,\n     [...]\n },\n. @schneidexe something like prefix.container_id.container_name.metric ?\nI can update my PR for adding the container_id in the path.\nIf you wish more metrics, I can do that in a new PR after this one will be merge without problem :)\n. @mnuessler if you need some help, I'm in :)\n. Hi,\nYou are using influxdb v0.9 that is not currently supported by cadvisor, see #743.\nYou can use influxdb 0.8 just by using tutum/influxdb:0.8.8\n. It's really look like you use influxdb 0.9 and not 0.8 because on the 0.8.8 web ui, when you create a database you have Shard Spaces box, but it's more a problem with influxdb than cadvisor IMO.  \nI juste made a test with by running influxdb with:\n    docker run -d --name=influxdb -p 8083:8083 -p 8086:8086 --restart=always -expose 8090 --expose 8099 tutum/influxdb\nAnd with a force refresh (remove my browser cache).\n. Squash done!\n. No problem, i will update it soon, maybe today.\n. ",
    "schneidexe": "Works great! The metrics paths are still a bit messy when forwarding them from statsd to graphite with whisper storage, but definitely useful. Might be worth to get aligned with #474...\n. ",
    "stevezau": "any updates on this? I'd like to forward via graphite. It sounds like it might be possible now but i can't find any docs on it\n. ",
    "bpradipt": "Thanks @rjnagal. I have created two PRs #730 and #731. Please review and provide your comments\n. Thanks @rjnagal for the review. I'm closing the issue now since the patches have been merged.\n. @rjnagal, I have uploaded new version based on your suggestions. Can you please review.\nThanks\n. Hello I'm working in IBM and covered as part of Google Corporate CLA. Is there any additional info required?\n. @vmarmol, I have uploaded new version based on your suggestions. Can you please review.\nThanks\n. Thanks for the review. I'll send a new version removing the arch detection.\n. Thanks @vmarmol. I'll resend incorporating the suggestion.\n. ",
    "tweakmy": "root@mesos-slave:/var/run/docker/execdriver/native/0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f# ls -ltr\ntotal 8\n-rw-r--r-- 1 root root 7333 May 21 14:38 state.json\nroot@mesos-slave:/var/run/docker/execdriver/native/0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f# cat state.json \n{\"id\":\"0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f\",\"init_process_pid\":16455,\"init_process_start\":\"984420\",\"cgroup_paths\":{\"blkio\":\"/sys/fs/cgroup/blkio/docker/0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f\",\"cpu\":\"/sys/fs/cgroup/cpu/docker/0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f\",\"cpuacct\":\"/sys/fs/cgroup/cpuacct/docker/0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f\",\"cpuset\":\"/sys/fs/cgroup/cpuset/docker/0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f\",\"devices\":\"/sys/fs/cgroup/devices/docker/0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f\",\"freezer\":\"/sys/fs/cgroup/freezer/docker/0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f\",\"memory\":\"/sys/fs/cgroup/memory/docker/0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f\",\"perf_event\":\"/sys/fs/cgroup/perf_event/docker/0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f\"},\"namespace_paths\":{\"NEWIPC\":\"/proc/16455/ns/ipc\",\"NEWNET\":\"/proc/16455/ns/net\",\"NEWNS\":\"/proc/16455/ns/mnt\",\"NEWPID\":\"/proc/16455/ns/pid\",\"NEWUSER\":\"/proc/16455/ns/user\",\"NEWUTS\":\"/proc/16455/ns/uts\"},\"config\":{\"no_pivot_root\":false,\"parent_death_signal\":0,\"pivot_dir\":\"\",\"rootfs\":\"/var/lib/docker/aufs/mnt/0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f\",\"readonlyfs\":false,\"mounts\":[{\"source\":\"proc\",\"destination\":\"/proc\",\"device\":\"proc\",\"flags\":14,\"data\":\"\",\"relabel\":\"\"},{\"source\":\"tmpfs\",\"destination\":\"/dev\",\"device\":\"tmpfs\",\"flags\":16777218,\"data\":\"mode=755\",\"relabel\":\"\"},{\"source\":\"devpts\",\"destination\":\"/dev/pts\",\"device\":\"devpts\",\"flags\":10,\"data\":\"newinstance,ptmxmode=0666,mode=0620,gid=5\",\"relabel\":\"\"},{\"source\":\"shm\",\"destination\":\"/dev/shm\",\"device\":\"tmpfs\",\"flags\":14,\"data\":\"mode=1777,size=65536k\",\"relabel\":\"\"},{\"source\":\"mqueue\",\"destination\":\"/dev/mqueue\",\"device\":\"mqueue\",\"flags\":14,\"data\":\"\",\"relabel\":\"\"},{\"source\":\"/\",\"destination\":\"/var/lib/docker/aufs/mnt/0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f/rootfs\",\"device\":\"bind\",\"flags\":20481,\"data\":\"\",\"relabel\":\"\"},{\"source\":\"/sys\",\"destination\":\"/var/lib/docker/aufs/mnt/0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f/sys\",\"device\":\"bind\",\"flags\":20481,\"data\":\"\",\"relabel\":\"\"},{\"source\":\"/var/lib/docker\",\"destination\":\"/var/lib/docker/aufs/mnt/0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f/var/lib/docker\",\"device\":\"bind\",\"flags\":20481,\"data\":\"\",\"relabel\":\"\"},{\"source\":\"/run/docker/execdriver/native\",\"destination\":\"/var/lib/docker/aufs/mnt/0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f/var/lib/docker/execdriver/native\",\"device\":\"bind\",\"flags\":20480,\"data\":\"\",\"relabel\":\"\"},{\"source\":\"/run\",\"destination\":\"/var/lib/docker/aufs/mnt/0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f/tmp\",\"device\":\"bind\",\"flags\":20480,\"data\":\"\",\"relabel\":\"\"},{\"source\":\"/var/lib/docker/containers/0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f/resolv.conf\",\"destination\":\"/var/lib/docker/aufs/mnt/0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f/etc/resolv.conf\",\"device\":\"bind\",\"flags\":20480,\"data\":\"\",\"relabel\":\"\"},{\"source\":\"/var/lib/docker/containers/0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f/hostname\",\"destination\":\"/var/lib/docker/aufs/mnt/0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f/etc/hostname\",\"device\":\"bind\",\"flags\":20480,\"data\":\"\",\"relabel\":\"\"},{\"source\":\"/var/lib/docker/containers/0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f/hosts\",\"destination\":\"/var/lib/docker/aufs/mnt/0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f/etc/hosts\",\"device\":\"bind\",\"flags\":20480,\"data\":\"\",\"relabel\":\"\"}],\"devices\":[{\"type\":99,\"path\":\"/dev/fuse\",\"major\":10,\"minor\":229,\"permissions\":\"rwm\",\"file_mode\":0,\"uid\":0,\"gid\":0},{\"type\":99,\"path\":\"/dev/null\",\"major\":1,\"minor\":3,\"permissions\":\"rwm\",\"file_mode\":438,\"uid\":0,\"gid\":0},{\"type\":99,\"path\":\"/dev/zero\",\"major\":1,\"minor\":5,\"permissions\":\"rwm\",\"file_mode\":438,\"uid\":0,\"gid\":0},{\"type\":99,\"path\":\"/dev/full\",\"major\":1,\"minor\":7,\"permissions\":\"rwm\",\"file_mode\":438,\"uid\":0,\"gid\":0},{\"type\":99,\"path\":\"/dev/tty\",\"major\":5,\"minor\":0,\"permissions\":\"rwm\",\"file_mode\":438,\"uid\":0,\"gid\":0},{\"type\":99,\"path\":\"/dev/urandom\",\"major\":1,\"minor\":9,\"permissions\":\"rwm\",\"file_mode\":438,\"uid\":0,\"gid\":0},{\"type\":99,\"path\":\"/dev/random\",\"major\":1,\"minor\":8,\"permissions\":\"rwm\",\"file_mode\":438,\"uid\":0,\"gid\":0}],\"mount_label\":\"\",\"hostname\":\"0657e582b849\",\"namespaces\":[{\"type\":\"NEWNS\",\"path\":\"\"},{\"type\":\"NEWUTS\",\"path\":\"\"},{\"type\":\"NEWIPC\",\"path\":\"\"},{\"type\":\"NEWPID\",\"path\":\"\"},{\"type\":\"NEWNET\",\"path\":\"\"}],\"capabilities\":[\"CHOWN\",\"DAC_OVERRIDE\",\"FSETID\",\"FOWNER\",\"MKNOD\",\"NET_RAW\",\"SETGID\",\"SETUID\",\"SETFCAP\",\"SETPCAP\",\"NET_BIND_SERVICE\",\"SYS_CHROOT\",\"KILL\",\"AUDIT_WRITE\"],\"networks\":[{\"type\":\"loopback\",\"name\":\"\",\"bridge\":\"\",\"mac_address\":\"\",\"address\":\"\",\"gateway\":\"\",\"ipv6_address\":\"\",\"ipv6_gateway\":\"\",\"mtu\":0,\"txqueuelen\":0,\"host_interface_name\":\"\",\"hairpin_mode\":false},{\"type\":\"veth\",\"name\":\"eth0\",\"bridge\":\"docker0\",\"mac_address\":\"02:42:ac:11:00:0d\",\"address\":\"172.17.0.13/16\",\"gateway\":\"172.17.42.1\",\"ipv6_address\":\"\",\"ipv6_gateway\":\"\",\"mtu\":1500,\"txqueuelen\":0,\"host_interface_name\":\"veth81dec23\",\"hairpin_mode\":false}],\"routes\":null,\"cgroups\":{\"name\":\"0657e582b84986174ee26737321b3c726929a11ff871519c5d888e880efd389f\",\"parent\":\"docker\",\"allow_all_devices\":false,\"allowed_devices\":[{\"type\":99,\"path\":\"\",\"major\":-1,\"minor\":-1,\"permissions\":\"m\",\"file_mode\":0,\"uid\":0,\"gid\":0},{\"type\":98,\"path\":\"\",\"major\":-1,\"minor\":-1,\"permissions\":\"m\",\"file_mode\":0,\"uid\":0,\"gid\":0},{\"type\":99,\"path\":\"/dev/console\",\"major\":5,\"minor\":1,\"permissions\":\"rwm\",\"file_mode\":0,\"uid\":0,\"gid\":0},{\"type\":99,\"path\":\"/dev/tty0\",\"major\":4,\"minor\":0,\"permissions\":\"rwm\",\"file_mode\":0,\"uid\":0,\"gid\":0},{\"type\":99,\"path\":\"/dev/tty1\",\"major\":4,\"minor\":1,\"permissions\":\"rwm\",\"file_mode\":0,\"uid\":0,\"gid\":0},{\"type\":99,\"path\":\"\",\"major\":136,\"minor\":-1,\"permissions\":\"rwm\",\"file_mode\":0,\"uid\":0,\"gid\":0},{\"type\":99,\"path\":\"\",\"major\":5,\"minor\":2,\"permissions\":\"rwm\",\"file_mode\":0,\"uid\":0,\"gid\":0},{\"type\":99,\"path\":\"\",\"major\":10,\"minor\":200,\"permissions\":\"rwm\",\"file_mode\":0,\"uid\":0,\"gid\":0},{\"type\":99,\"path\":\"/dev/null\",\"major\":1,\"minor\":3,\"permissions\":\"rwm\",\"file_mode\":438,\"uid\":0,\"gid\":0},{\"type\":99,\"path\":\"/dev/zero\",\"major\":1,\"minor\":5,\"permissions\":\"rwm\",\"file_mode\":438,\"uid\":0,\"gid\":0},{\"type\":99,\"path\":\"/dev/full\",\"major\":1,\"minor\":7,\"permissions\":\"rwm\",\"file_mode\":438,\"uid\":0,\"gid\":0},{\"type\":99,\"path\":\"/dev/tty\",\"major\":5,\"minor\":0,\"permissions\":\"rwm\",\"file_mode\":438,\"uid\":0,\"gid\":0},{\"type\":99,\"path\":\"/dev/urandom\",\"major\":1,\"minor\":9,\"permissions\":\"rwm\",\"file_mode\":438,\"uid\":0,\"gid\":0},{\"type\":99,\"path\":\"/dev/random\",\"major\":1,\"minor\":8,\"permissions\":\"rwm\",\"file_mode\":438,\"uid\":0,\"gid\":0}],\"memory\":0,\"memory_reservation\":0,\"memory_swap\":0,\"cpu_shares\":0,\"cpu_quota\":0,\"cpu_period\":0,\"cpuset_cpus\":\"\",\"cpuset_mems\":\"\",\"blkio_weight\":0,\"freezer\":\"\",\"slice\":\"\",\"oom_kill_disable\":false},\"apparmor_profile\":\"docker-default\",\"process_label\":\"\",\"rlimits\":null,\"additional_groups\":null,\"uid_mappings\":null,\"gid_mappings\":null,\"mask_paths\":[\"/proc/kcore\"],\"readonly_paths\":[\"/proc/sys\",\"/proc/sysrq-trigger\",\"/proc/irq\",\"/proc/bus\"]}}\n. funny, I am using :latest\nvagrant@mesos-slave:~/slave$ docker images\nREPOSITORY              TAG                            IMAGE ID            CREATED             VIRTUAL SIZE\nmesos                   latest                         3e90ce5ead46        5 days ago          504.4 MB\nk8sm                    latest                         256c918bf7d7        5 days ago          371 MB\nmesosphere/kubernetes   skydns-2015-05-10T0351350000   47dfd6d4f20a        11 days ago         8.873 MB\ngoogle/cadvisor         latest                         e34bb5f6a0ba        2 weeks ago         20.01 MB\nnginx                   latest                         42a3cf88f3f0        2 weeks ago         132.8 MB\nubuntu                  14.04.2                        07f8e8c5e660        2 weeks ago         188.3 MB\nquay.io/coreos/etcd     v2.0.3                         201a4476d171        3 months ago        12.72 MB\nkubernetes/skydns       latest                         93f9564eaa33        4 months ago        8.971 MB\nkubernetes/kube2sky     1.0                            3ea44ca9f1bd        6 months ago        7.724 MB\nkubernetes/kube2sky     latest                         3ea44ca9f1bd        6 months ago        7.724 MB\nkubernetes/pause        latest                         6c4579af347b        10 months ago       239.8 kB\n. Sorry false alarm. The Kubernetes cadvisor port is clashing with the cadvisor container I just ran hence all the wrong indication. Sorry for the confusion.\n. ",
    "pnovotnak": "Can confirm this issue on a MBP running atop boot2docker, though now that I'm ready to profile it's slunk back into the abyss.\n. ",
    "teon": "No, this is on Docker 1.6.2 - Debian Jessie (as docker host).\n. Also, I've installed lately Rancher (https://github.com/rancherio/rancher) a service for managing Docker containers which also uses cadvisor - same issue here, had to remove rancher since cadvisor took 50% of my 16 CPUs...\n. @vmarmol whar do you mean by kocalhost, should I run the command from docker container or host?\n. go tool pprof http://localhost:8080/debug/pprof/profile\nPossible precedence issue with control flow operator at /usr/lib/go/pkg/tool/linux_amd64/pprof line 3008.\nRead http://localhost:8080/debug/pprof/symbol\nGathering CPU profile from http://localhost:8080/debug/pprof/profile?seconds=30 for 30 seconds to\n  /tmp/qtt6Z8he7k\nBe patient...\nWrote profile to /tmp/qtt6Z8he7k\nWelcome to pprof!  For help, type 'help'.\n(pprof)  png > output.png\nUnknown command: try 'help'.\n. go tool pprof http://localhost:8080/debug/pprof/profile\nPossible precedence issue with control flow operator at /usr/lib/go/pkg/tool/linux_amd64/pprof line 3008.\nRead http://localhost:8080/debug/pprof/symbol\nGathering CPU profile from http://localhost:8080/debug/pprof/profile?seconds=30 for 30 seconds to\n  /tmp/qtt6Z8he7k\nBe patient...\nWrote profile to /tmp/qtt6Z8he7k\nWelcome to pprof!  For help, type 'help'.\n(pprof)  png > output.png\nUnknown command: try 'help'.\n. go version go1.3.3 linux/amd64\n. \n. ",
    "cdrcnm": "Same problem here.\nDebian 7.8\nDocker version 1.7.0, build 0baf609\nThe graph :\n\nThe problem seem related with the number of containers running, the problem appears on servers with around 20 containers, but not on server with a couple of containers.\n. ",
    "md5": "I think I may be running into this as well. I'm also on a Macbook Pro with a boot2docker VM.\nIn my case, I noticed high CPU soon after waking my laptop. After seeing VBoxHeadless consuming 400% CPU (4 core machine), I poked around and saw that .cadvisor was doing most of the fan spinning in my Rancher agent container.\n@vmarmol I tried running a slight variant of the command you suggested, but I'm getting this response:\nconsole\n$ go tool pprof -png -output=cadvisor-prof.png \"http://$(docker-machine ip dev):8080/debug/pprof/profile\"\nFetching profile from http://192.168.99.100:8080/debug/pprof/profile\nPlease wait... (30s)\nserver response: 422 status code 422\n. P.S. Let me know if I should be reporting this to the Rancher folks instead or if you need any more details.\n. Nevermind the 422 error... I was being dumb and connecting to the wrong container's port 8080.\n. ",
    "tasosz": "I'm also running on Macbook Pro (but Ubuntu VMs on VBox) and have the same issue after wake-up of the laptop - all VM around 100% each.\nThe issue looks to get resolved once I restart the agent processes.\nSo, the quick solution I have been using until a fix is found is this (inside each host if you have multiple):\n  kill $(ps aux | grep '[/]var/log/rancher/.cadvisor' | awk '{print $2}')\nThis results in this process instantly restarting and the issue is resolved until the next sleep.\nIt also doesn't require any manual restart of the agent(s).\n. ",
    "xiaods": "+1, indeed need inspect the root cause\n. ```\nstatfs(\"/roots/var/lib/docker/devicemapper/mnt/70d15e9045b9fa71d7703d57eb7b85301de9943671472e0eb7fb858c944bd470\", {f_type=0x58465342, f_bsize=4096, f_blocks=26201344, f_bfree=26116217, f_bavail=26116217, f_files=104856576, f_ffree=104843211, f_fsid={64769, 0}, f_namelen=255, f_frsize=4096}) = 0\nfutex(0x12f81d0, FUTEX_WAIT, 0, NULL)   = 0\nfutex(0xc82250a110, FUTEX_WAKE, 1)      = 1\nfutex(0x12f6e78, FUTEX_WAIT, 0, {0, 160996}) = -1 ETIMEDOUT (Connection timed out)\nfutex(0xc82250a110, FUTEX_WAKE, 1)      = 1\nfutex(0xc820459310, FUTEX_WAKE, 1)      = 1\nfutex(0x12f6e78, FUTEX_WAIT, 0, {0, 263095}) = -1 ETIMEDOUT (Connection timed out)\nfutex(0xc821a52110, FUTEX_WAKE, 1)      = 1\nfutex(0xc82188a110, FUTEX_WAKE, 1)      = 1\nfutex(0x12f6e78, FUTEX_WAIT, 0, {0, 1240}) = -1 ETIMEDOUT (Connection timed out)\nfutex(0xc821a52110, FUTEX_WAKE, 1)      = 1\nfutex(0xc820b7e110, FUTEX_WAKE, 1)      = 1\nfutex(0x12f6e78, FUTEX_WAIT, 0, {0, 62243}) = -1 ETIMEDOUT (Connection timed out)\nfutex(0xc821a52110, FUTEX_WAKE, 1)      = 1\n```\nget timed out from strace cadvisor process\n. good point. ",
    "mathpl": "I'm seeing this as well on v0.21.1. stracing du I noticed it was calculating space usage under its own overlay.  (c4e306db947e is cadvisor) It's going through everything that's under /rootfs as well which includes the host's /var/lib/docker. You can see where this is going. Is this a configuration issue? I'm running cadvisor as defined in the README.\nlstat(\"/var/lib/docker/overlay/c4e306db947ecb57dbce88386cfbda7bfb5e4afe9c313902134ee72185cace14/merged/rootfs/var/lib/docker/overlay/5ae48109811049b8b149683728917381e3468375728b42e8c2ff414557c519f4/root/var/lib/dpkg/info/libudev1:amd64.md5sums\", {st_mode=S_IFREG|0644, st_size=287, ...}) = 0\n. ",
    "dpgeekzero": "Seeing this on v0.24.1 (ae6934c)\nI have a fairly large machine (50 cores, 256GB) with about 60 containers running on it currently. (CoreOS 1235.9.0 / Docker 1.12.6)\nRunning cadvisor with the quickstart command line here: https://github.com/google/cadvisor results in full utilization of 2 CPU's and such excessive IO that the server became unresponsive with docker ps hanging.\nI was able to fix the problem by manually killing the process ID of the cadvisor process.\nAs a further datapoint, I saw the same behavior downloading the binary and running it outside of docker.\n. ",
    "kainz": "So i've dealt with this in a decent number of environments before (pre-docker as well), it looks like in mesos-land they are addressing this sort of thing in mesos-1.3 via the xfs-project-quota/group quota isolator, which newer dockers also support through corresponding controls -- maybe the solution is to detect such a quota isolator and use that, or implemnt that as a toggleable metric vis-a-vis the default disk metrics?. Dockers or the cni apis dont necessarily have any great options on top of that, yet. If I had to roll out a system for that today, you have to do something to acquire quota knowledge both on image layers (particularly the per container layer), and on volume drivers. The latter is actually slightly simpler as you could get as simple as implementing a bind mount wrapper that does prjquota and exposes those metrics elsewhere.\nFor container fs and/or layers, I suppose the easiest thing to do would be to dig into runc and add hooks for setting up then querying a prjquota which you would then need to bubble up to the docker API through however many layers. This is easier on some graph drivers (say btrfs/xfs/zfs) than others.\nI'd actually recommend starting with the btrfs graph driver as you get automatic subvolume quotas when you enable quota on a parent volume. Use xfs if you want to get a good feel for everywhere you'll need to do a system API integration though, it's more manual there. Anyways once you have those you want an api to query use, and depending on your clients/needs you may also need to log max quota used and/or have something periodically querying usage to get p95 usage or the like.\nI'd recommend getting something working first and then deal with the huge pile of edge case enablement later. That or stick to block device provisioning/mounts like most of the cloud provider solutions.\nOn July 31, 2018 6:12:30 PM PDT, John Yani notifications@github.com wrote:\n\nI'm using docker swarm.\nThinking more about this, since cadvisor is using bind mounts to\ndetermine disk usage, it doesn't look like container runtime can do\nmuch about this. For example, fstab file must be edited to support disk\nquota. Also volume drivers need to be exposed to that. Though I'm not\nsure whether cadvisor can query disk usage of remote volumes.\nRegarding the issue in moby you linked it appears that VFS only applies\nto images and containers. It doesn't apply to bind mounts and volumes.\nCorrect me if I'm wrong\nCreated an issue here: https://github.com/moby/moby/issues/37574\n-- \nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub:\nhttps://github.com/google/cadvisor/issues/735#issuecomment-409416384\n\n-- \nSent from my Android device with K-9 Mail. Please excuse my brevity.. Volume metrics and per-container disk metrics aren't a very clear distinction. Prometheus node-exporter for example can get away with statvfs, but disk usage is not tracked say per-overlay, thus the need for the per-container du call in cadvisor. That's what the project quota idea is meant to mitigate. You could also do it with more conventional (aka it's been around 10-20y longer) user/group quota if you rig the right things up with PID/GID namespace mapping. That breaks all sorts of abstractions though, thus the leaning towards project quota.\nOn July 31, 2018 6:33:55 PM PDT, John Yani notifications@github.com wrote:\n\nOk, it looks like I've missed the point. As far as I understood,\ncadvisor runs du command only on /var/lib/docker/overlay/, to report\nwhich containers and images consume the most space. Which appears to be\nVFS. Is it correct? \nBut why use du at all? Doesn't docker expose container disk metrics on\nUnix socket? But that looks to be expensive too:\nhttps://github.com/moby/moby/issues/31951\nWith --disable-metrics=disk, would cadvisor still report disk free\nmetrics?\n-- \nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub:\nhttps://github.com/google/cadvisor/issues/735#issuecomment-409419785\n\n-- \nSent from my Android device with K-9 Mail. Please excuse my brevity.. To add though, maybe a 'per-container' disk free metric would be useful, even if that free space happens to be shared amongst other containers. You can sort of rig that up already but maybe having that available would be a useful option? (I.e. you could use a system wide df metric to flag containers on a given host as 'in danger', but that's certainly a more expensive query)\nOn July 31, 2018 6:33:55 PM PDT, John Yani notifications@github.com wrote:\n\nOk, it looks like I've missed the point. As far as I understood,\ncadvisor runs du command only on /var/lib/docker/overlay/, to report\nwhich containers and images consume the most space. Which appears to be\nVFS. Is it correct? \nBut why use du at all? Doesn't docker expose container disk metrics on\nUnix socket? But that looks to be expensive too:\nhttps://github.com/moby/moby/issues/31951\nWith --disable-metrics=disk, would cadvisor still report disk free\nmetrics?\n-- \nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub:\nhttps://github.com/google/cadvisor/issues/735#issuecomment-409419785\n\n-- \nSent from my Android device with K-9 Mail. Please excuse my brevity.. ",
    "arno01": "@vishh any updates from Google on this issue?\n```\ntimeout 10s strace -c -f -p 30402\nstrace: Process 30402 attached with 13 threads\nstrace: Process 317 attached\nstrace: Process 371 attached\nstrace: Process 386 attached\nstrace: Process 30402 detached\nstrace: Process 30467 detached\nstrace: Process 30468 detached\nstrace: Process 30469 detached\nstrace: Process 30470 detached\nstrace: Process 30471 detached\nstrace: Process 30483 detached\nstrace: Process 30484 detached\nstrace: Process 30485 detached\nstrace: Process 30607 detached\nstrace: Process 30630 detached\nstrace: Process 30769 detached\nstrace: Process 32358 detached\n% time     seconds  usecs/call     calls    errors syscall\n\n65.29   15.278506       11964      1277       147 futex\n 10.57    2.473316         276      8977        15 read\n  8.13    1.901699         384      4957           pselect6\n  3.84    0.899657         471      1912           epoll_wait\n  3.31    0.775641      258547         3           waitid\n  1.87    0.438267          34     13010           lstat\n  1.60    0.374308          37     10237           close\n  1.20    0.281473          14     20762           fcntl\n  1.20    0.279661          44      6342       369 openat\n  1.19    0.279386          29      9695       756 epoll_ctl\n  0.52    0.120677          24      5016           getdents64\n  0.47    0.110231          49      2270       263 stat\n  0.41    0.095861          21      4540           fstat\n  0.20    0.047173          22      2124           newfstatat\n  0.13    0.031263          29      1077           open\n  0.02    0.005554        1851         3           clone\n  0.01    0.002782          87        32           sched_yield\n  0.01    0.001487         297         5           write\n  0.00    0.000709          15        48           brk\n  0.00    0.000403          45         9           pipe2\n  0.00    0.000296          37         8           writev\n  0.00    0.000277          25        11         6 execve\n  0.00    0.000191          19        10           mprotect\n  0.00    0.000152           8        20           statfs\n  0.00    0.000119           1       168           rt_sigaction\n  0.00    0.000117          59         2         1 restart_syscall\n  0.00    0.000104          35         3           wait4\n  0.00    0.000099           8        12           rt_sigprocmask\n  0.00    0.000052          10         5           arch_prctl\n  0.00    0.000021           4         5         5 ioctl\n  0.00    0.000019           2         9           dup2\n  0.00    0.000017           4         4           getuid\n  0.00    0.000014           7         2           getpriority\n  0.00    0.000011           6         2           setpriority\n  0.00    0.000011           2         5           set_tid_address\n  0.00    0.000007           2         3           uname\n  0.00    0.000007           7         1           fchdir\n  0.00    0.000006           2         3           getpid\n  0.00    0.000003           1         3         1 rt_sigreturn\n  0.00    0.000000           0         1           getsockname\n  0.00    0.000000           0         4           setsockopt\n  0.00    0.000000           0         2         1 accept4\n\n100.00   23.399577                 92579      1564 total\n```\n```\nstrace -f -p 30402 |& grep execve\n[pid  2490] execve(\"/bin/nice\", [\"nice\", \"-n\", \"19\", \"du\", \"-s\", \"/rootfs/var/lib/docker/overlay2/\"...], 0xc422891200 / 4 vars / \n[pid  2490] <... execve resumed> )      = 0\n[pid  2490] execve(\"/usr/local/sbin/du\", [\"du\", \"-s\", \"/rootfs/var/lib/docker/overlay2/\"...], 0x7ffc44c234d0 / 4 vars / \n[pid  2490] <... execve resumed> )      = -1 ENOENT (No such file or directory)\n[pid  2490] execve(\"/usr/local/bin/du\", [\"du\", \"-s\", \"/rootfs/var/lib/docker/overlay2/\"...], 0x7ffc44c234d0 / 4 vars / \n[pid  2490] <... execve resumed> )      = -1 ENOENT (No such file or directory)\n[pid  2490] execve(\"/usr/sbin/du\", [\"du\", \"-s\", \"/rootfs/var/lib/docker/overlay2/\"...], 0x7ffc44c234d0 / 4 vars / \n[pid  2490] <... execve resumed> )      = -1 ENOENT (No such file or directory)\n[pid  2490] execve(\"/usr/bin/du\", [\"du\", \"-s\", \"/rootfs/var/lib/docker/overlay2/\"...], 0x7ffc44c234d0 / 4 vars / \n[pid  2490] <... execve resumed> )      = 0\n[pid  2491] execve(\"/bin/nice\", [\"nice\", \"-n\", \"19\", \"du\", \"-s\", \"/rootfs/var/lib/docker/overlay2/\"...], 0xc422891470 / 4 vars / \n[pid  2491] <... execve resumed> )      = 0\n```\n\nPID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND   \n30402 root      20   0 1596616 744408      0 S 120.7 36.5   5:47.19 cadvisor  \n30777 nobody    20   0  246996 198360      0 S   0.0  9.7   1:53.75 prometheus\n22319 999       20   0 1201932 180264      0 S   0.0  8.8   0:11.85 mysqld    \n22179 arno      20   0  698808  72788      0 S   0.0  3.6   1:04.36 node      \n24782 arno      20   0  123180  55024   4268 S   1.2  2.7   3:10.16 gitea     \n22265 arno      20   0  125128  40988      0 S   0.0  2.0   0:05.95 ruby      \n25664 991       20   0  232432  38220  35236 S   0.0  1.9   0:00.78 php-fpm7  \n23566 www-data  20   0  454808  30668  16860 S   0.0  1.5   0:16.89 apache2\nIn the Grafana dashboard you can see I was restarting cadvisor once, then it starts eating all CPU/RAM again:\n\ncadvisor:\n    restart: unless-stopped\n    image: google/cadvisor:latest\n    networks:\n      - backend\n    volumes:\n      - /:/rootfs:ro\n      - /var/run:/var/run:rw\n      - /sys:/sys:ro\n      - /var/lib/docker:/var/lib/docker:ro\n\nUbuntu Linux 4.15\ncAdvisor version v0.28.3 (1e567c2)\nDocker 18.03.1-ce with overlay2 at ext4.. \n",
    "WnP": "I've got an issue which looks really similar to this one:\n\nBut as far as I can understand it looks more related to memory stats, not?. Thanks a lot @bmerry dropping the dentry cache fully fix my cadvisor cpu issue.\nI've done that using the following command:\necho 2 > /proc/sys/vm/drop_caches\nMore information about drop_caches here.. @Aschenbecher then your high CPU load issue is not related to memory stats, you need to profile cadvisor using pprof in order to find what cause those high CPU loads.\n@zerthimon +1 comment are always useless, use github comments thumbs up feature. Please do not pollute issues with useless comments, Thanks. ",
    "neonstalwart": "fyi - InfluxDB 0.9 is still experiencing some API churn.  be sure to target the alpha1 branch (if it has not been merged to master).  that merge is anticipated to happen some time later this week.\n. ",
    "benfb": "InfluxDB v0.9 is officially released now, see here: http://influxdb.com/blog/2015/06/11/InfluxDB-v0_9_0-released-with-developer-and-production-support.html\nThe release notes claim \"going forward in the 0.9.x series of releases we will not make breaking API changes or breaking changes to the underlying data storage.\"\n. Any updates on this?\n. ",
    "SlashmanX": "Has there been any movement on this?\n. Any progress on getting this merged?\n. InfluxDB v0.9.5 has been released, which should help speed this up, correct?\n. ",
    "hectorj2f": "Are there any expectations of having this PR merged soon ? \n. Great! :+1: \n. @vishh Do we have any news on this ?\n. ",
    "martinm82": "Hi guys, just wondering whether the PR will be merged into master soon? :)\n. +1\n. ",
    "jwthomp": "Also following up to check on status. \n. Also following up to check on status. \n. ",
    "clanstyles": "Any updates, this is one of the only reasons I haven't upgraded influxdb.\n. Any update? I'd love to push this into prod, CAdvisor is one of the last reasons we haven't pushed Influx 0.9. We've begun looking into telegraf.\n. ",
    "harunyardimci": "Hi, \nis there any due date for this? \n. +1\n. ",
    "hkeeler": "Any update on this?\n. ",
    "aheusingfeld": "\nLooks like influxdb broke the older version, so we can ignore that too.\n\n@rjnagal I'm not sure I get what you meant to say. The InfluxDB API shouldn't have changed since 0.9. Could you please have another look at this and explain what is missing from being merged? We're really looking forward to the InfluxDB 0.9 support. Thanks for your time!\n. ",
    "leonty": "Hi guys, any observable release date for this PR?\nI'm looking forward supporing Influx 0.9 because my infrastructure already uses this version and I can't thus integrace cAdvisor into it :(\n. ",
    "svenmueller": "Hi...what is the current schedule for merging the PR?\n. +1\n. we also encounter the same problem (cadvisor:lastest, ubuntu 14.04)\n. any updates regarding this?\n. is influxdb 0.9.x still not supported by cadvisor?\n. anyone else having the same issue? what could cause this error log?\n. it seems to work (beside that we are seeing #771). but there are no other logs shown..so logging might be affected?\n. how can i set the native flag --v=8 when using docker run...?\n. Thx. I just tried it. But there is no change in the logged data:\nbash\n$ docker logs cadvisor\nE1022 12:58:03.536020 00001 machine.go:87] Failed to get system UUID: open /proc/device-tree/vm,uuid: no such file or directory\n. this is the command i'm using\nbash\ndocker run --net bridge -m 0b --detach=true -p 9090:8080 -v /var/run:/var/run:rw -v /sys:/sys:ro  --name cadvisor google/cadvisor:latest --v=8\n. i even can't see the full logs using --v=8\n. +1\n. problem was caused by another issue\n. which version does contain this commit?\n. just pulled the current latest tag. when running the container, the logs shows:\nbash\nI0125 21:31:10.290451 00001 cadvisor.go:94] Starting cAdvisor version: \"0.18.0\" on port 8080\nis that the latest stable version? when looking at the \"releases\" section, i expected version 0.20.1 (https://github.com/google/cadvisor/releases)\nThx,\nSven\n. Hi,\nStill the same version for me (although a new iamge was pulled).\nbash\n$ docker images\nREPOSITORY                                 TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\ngoogle/cadvisor                            latest              3f6443d29d3e        11 minutes ago      35 MB\nbash\ndocker logs -f cadvisor\n...\nI0125 21:56:03.652602 00001 cadvisor.go:94] Starting cAdvisor version: \"0.18.0\" on port 8080\n...\n. ",
    "ChamaraG": "Hi, any update on this? Thanks!\n. ",
    "mecton": "+1 v0.9.5  released yesterday !\n. ",
    "jvgutierrez": "Any updates on this?\n. ",
    "carmark": "@jimmidyson \nI can take this if the contributor is busy on more important issues.\n. @jimmidyson  sure.\n. @mnuessler \nI would like to help you do something if you are busy on other things.  It seems this patch is aspired by many guys in https://github.com/google/cadvisor/issues/743.\n. @jimmidyson Thanks, it is my pleasure to do the code review.\n. @jimmidyson \nI read the code, and modify the import packages manually at the same time.  The tool goimports seems not working for this change.\n. This order make the packages imported very clear, the tool goimports will only separate the golang packages and other packages into two parts.  However, the order of this patch will have another parts.\nIt is truly that there is no any tool to support this, and I do not very strongly feel this patch is very necessary, but kubernetes uses this kind of order.\n. Thanks @jimmidyson.  I will also try to keep a close eye on the new import packages.\n. @jimmidyson \nI have a simple fix, it works well after basic test.  I will submit a PR later when I do more test.\n. @jimmidyson \nYeah, I will do more tests to make sure there is no any regression.\nThanks\n. @jimmidyson \nWhen I implemented this, I also want to drop these throttled devices, but I am afraid I missed something.  So I will remove those codes and put the new changes into a new function.\nThanks,\n. @jimmidyson \nI updated the changes, and tested it with docker 1.8.2, 1.8.3 and 1.9.1.  All of them works well.  Could you please review them again?\n. @jimmidyson \n\nMinor nits. Really need some tests now just to confirm it's fixed & compatible.\n\nI also think so, it will be better to have more integration test cases for different docker version.\n. Sure, I will try to add some unit tests on this.\n. @vishh  @jimmidyson \nMade some changes based on your comments and add the basic unit tests.  I am not sure why the default test fails.\n. Thanks @vishh \n. @jimmidyson \nYeah, thanks for your reminder, I will update it.\n. retest this please\n. ok to test\n. @timstclair \nThanks for your review, I made the error message more useful by adding test case ID and fixed spelling mistakes.  I kept the standardDesc and errorDesc in validate/validate_test.go, since they are only used in test scenario.\n. @timstclair \nNo, I have no merge permission, could you please merge this?\nThanks,\n. could you please re-order the import packages as the follow order:\n- golang self package\n- cadvisor package\n- third-party package\n. Shall we refactor the function of StatsEq with reflect.DeepEqual in storage/influxdb/influxdb_test.go?\n. @jimmidyson Fine for me, may modify them later.\n. Thanks @jimmidyson \n. How about using path/filepath instead of path?  path is intended for forward slash-separated paths only (such as those used in URLs), while path/filepath manipulates paths across different operating systems, although the cadvisor only support linux system. \n. Yeah, I checked most of codes which used json.UnmarshalTypeError, it should be pointer type.\n. Yeah, I will rename it and add some comments in next commit with the unit test.\n. It looks like a merge issue, shouldn't change it.\n. ",
    "thanhson1085": "+1\n. ",
    "jcoloh": "+1\n. ",
    "sensimilla": "+1\n. ",
    "bmlynarczyk": "+1\n. +1\n. ",
    "asa511": "+1\n. ",
    "ohmystack": ":thumbsup:\n. Webpage on port 8080 is provided by cAdvisor, which means cAdvisor is up. However, from your log, it seems that the cAdvisor cannot connect to the Docker's socket. \nYou are using docker on Mac, all containers are running in VMs on your Mac. Maybe you should find out which /var/run path is really mounted, is the same one on the same VM of other containers?\n. However, the tags v0.20.4 & 0.19.3 are still packaged with a wrong version of cAdvisor bin, \"0.18.0\".\nsudo docker run -it --rm --entrypoint='/bin/sh' google/cadvisor:v0.20.4\n/ # cadvisor -version\ncAdvisor version 0.18.0\nsudo docker run -it --rm --entrypoint='/bin/sh' google/cadvisor:0.19.3\n/ # cadvisor -version\ncAdvisor version 0.18.0\n. @hridyeshpant \nYour running cmd misses a few volumes.\nThe cmd you privided above:\n\nCOMMAND run : docker run --volume=/var/run:/var/run:rw --volume=/var/lib/docker/:/var/lib/docker:ro --publish=8080:8080 --detach=true --name=cadvisor google/cadvisor:latest\n\nYou need to mount:\n/:/rootfs:ro and /sys/fs/cgroup:/sys/fs/cgroup:ro (OR on CentOS 6, /cgroup:/cgroup:ro)\ncAdvisor uses libcontainer lib to read the \"mount info\" at /proc/self/mountinfo, which tells cAdvisor who the subcontainers are. If you don't mount the root filesystem, cAdvisor cannot know the \"mount info\", so that there is no subcontainer.\n. Newer version (about >= v0.20.4) cAdvisor can support for InfluxDB 0.9, but I'm not sure with 0.10.\n. You should make cadvisor also listen to 2020. Add --port=2020. For more details, I have answered there.\n. ",
    "AnanyaKumar": "A clarification: do we want to remove the network stats panel from the cAdvisor GUI when Docker containers are run with --net=host? Or do we want to find a way to profile the container's network throughput even when it's run with --net=host?\n. I signed the CLA!\n. Thanks for identifying a great issue for me to get some experience programming in Go and with the cAdvisor/Kubernetes dev workflow! I have a (short) outline of what I plan to do, and a couple of clarifications on what we expect out of this feature.\nWhat we want:\nModule that detects what cloud provider we're on (for now, GCE and AWS)\nMachine info data structures should contain cloud provider and instance type info\nhttp://localhost:8080/api/v1.3/machine should should show cloud provider info\n(Should the GUI show cloud provider information?)\nOutline:\n- Add a new module cloudproviderinfo in utils\n- Add constants cloudproviderinfo.GCE, cloudproviderinfo.AWS, cloudproviderinfo.OTHER\n- Add a function (and helper functions) that gets cloud provider name and instance type\n- Add a unit test with a fake for the cloudproviderinfo module\n- Modify machine info/attribute structures in both v1 and v2 to add a cloudProvider and instanceType field (I wasn't 100% sure if both versions need to be changed, but I'd guess they do)\n- manager/machine.go getMachineInfo will populate and return cloud provider info in machineInfo\nIs this roughly what you had in mind in terms of what the cloud provider detection feature involves? I'm going to start by implementing the cloudproviderinfo module.\n. Cool, thanks @rjnagal !\n. Sure, I'll send a PR after I'm done with the net=host issue!\n. I authored these commits.\n. Another question: should I add test cases for the code? I could use a fake for the dockerContainerHandler and examine the returned spec.\nI've tested the code on the GUI (the network tab does not show up when a docker container is run with --net=host). What's the cAdvisor policy for testing small changes?\n. I checked libcontainer config (https://github.com/docker/libcontainer/blob/master/configs/config.go), but it didn't seem like it was keeping track of the HostConfig or the NetworkMode. I also logged libcontainer.config.Networks and libcontainer.config.SystemProperties and they did not seem to contain information about the NetworkMode. However, I might have missed something!\nHowever, I realized that there's an issue with my code. The code I inserted has nothing to do with libcontainerConfig and so it should not be in the function \"libcontainerConfigToContainerSpec\". I'll extract it to GetSpec and send a revised PR.\nOh and no problem about the \"1 line\" statement :) I didn't spend too much time trying to see if there's a 1 line solution, but I realized that I might have missed some property in libcontainer.config or in some other struct.\n. I'm using Docker.runconfig's method to check if the network mode is a bridge - I think this is better because the string representation of \"bridge\" could potentially change.\n. Checking the size of the Network attribute in libContainerConfig was a good idea! I logged the output of libContainerConfig when net=host was set to true and when it wasn't, and checked for the differences in output, but I seem to have missed the Network attribute :(\n. I'll push a fix - thanks @rjnagal for helping me out!\n. Ok, I've removed the extra commits, and squashed my commits!\n. @rjnagal Can/should this PR be merged in? Just pinging you so that we don't forget about the PR :)\n. I authored these commits.\n. Thanks for your feedback @vishh! I've made the changes, and I'll send a PR with more changes later on :)\n. The feature has been implemented. Couple of notes:\n1. I tested the code on my local machine (by accessing v1.3 and v2.0 of the API) and the API contained correctly populated values for cloud_provider and instance_type. I'm in the process of testing the code on GCE (I'm installing cAdvisor on a GCE instance right now).\n2. Errors were, for the most part, removed in the latest commit. If there is an error in getting the cloud provider, the cloud provider will be set to \"UNKNOWN\". In other words, the errors are contained in the return value.\n3. I've changed the types of the constants to strings, so that they're more readable in the API output.\nLet me know if you have any feedback!\n. I didn't detect the instance type correctly in GCE in the previous commit. I've corrected this, and tested on an n1-standard-1 instance (it outputs the correct instance type).\n. @rjnagal Does this look fine?\n. @rjnagal Awesome, thanks for your feedback. I've squashed the commits!\n. @vmarmol Sorry I missed your comments until you spoke to me IRL a couple of days ago. I've pushed a diff based on the corrections to the code that you suggested. I'll respond faster if you have any follow ups!\n. @rjnagal Cool, done!\n. Sorry I didn't comment on this - I was trying to figure the code out, and understand what the test was doing, but looks like Anushree got to it first! \nWhat happens if the first wget is executed after the asserts in lines 264 to 270? Would the test fail because the busy box container hasn't yet sent a packet? (Sorry I probably missed something)\n. Thanks for the clarification - besides that it looked fine to me :)\n. Changed.\n. Removed.\n. Added.\n. Great idea! Changed the code.\n. True!\n. That's true - changed!\n. Sorry, I didn't understand why the data structures should go into machine.go Why shouldn't consumers of the API have to include utils/cloudinfo? Aren't the methods that retrieve information about the cloud provider in utils/cloudinfo anyway? Thanks!\n. ",
    "priyanka5": "@vmarmol yeah but docker is running fine, i am able to run other containers on the same machine.\n. its working now thanks a lot ! just wanted to know where is the image (Dockerfile) available for this image? wanted to have a look\n. ",
    "AJNOURI": "Hi guys, I am running an openvswitch container with 16 interfaces and I need to monitor all interfaces. Only 6 are shown in the dropdown menu, is there a reason behind the limitation?\n. OK, got it! I was running it with \"sudo\".\n. ",
    "fanfuxiaoran": "My Docker is 1.7.0-dev\n. ",
    "wpbgent": "Same issue with docker 1.7.0-dev\n. ",
    "cornelius-keller": "@rjnagal \nCadvisor version is:\n\n[root@583274-app35 ~]# docker images\nREPOSITORY                                      TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\ndocker.io/google/cadvisor                       latest              399ae3c46a0e        47 hours ago        19.89 MB\n[root@583274-app35 ~]# \n\nThis is a permanent situation. The container fs stays busy untill I delete cadvisor.\nWhat do you mean by getting host:port/validate for cadvisor? Cadvisor was still running and responsive on the web ui if that is what you mean. Unfortunately I can't give you any public host port to validate as cadvisor is only exposed via a vpn.\n. Sorry was a long day, did not get that this was an endpoint. I added the output to the gist.\n. ",
    "gianlucaborello": "I am facing this same issue. Essentially, running cadvisor with --volume=/:/rootfs:ro causes other containers' devicemapper mounts to be mounted inside the cadvisor container, so they can't be properly destroyed when issuing docker rm on the target container as they will appear in use.\nHow can this be solved?\n. ",
    "hoeghh": "When i run it on Fedora 21, it works fine. But when i run it on Ubuntu 14.04.2 LTS I get the same error as described above.\nError response from daemon: Cannot destroy container xxx_jenkinsMaster_1230: Driver aufs failed to remove root filesystem 13b421d0458e740e42e5fa5ac1cb68f32638f0bc723d9ba16718955214d79b7d: rename /var/lib/docker/aufs/mnt/13b421d0458e740e42e5fa5ac1cb68f32638f0bc723d9ba16718955214d79b7d /var/lib/docker/aufs/mnt/13b421d0458e740e42e5fa5ac1cb68f32638f0bc723d9ba16718955214d79b7d-removing: device or resource busy\nThe main difference is, that Ubuntu uses AUFS, where Fedora uses Devicemapper. Maby thats the problem. \n. ",
    "shredder12": "@rjnagal I can confirm that this issue happens on Ubuntu trusty x64 with Doceker 1.8.1, cadvisor:latest and devicemapper.\n'1cb6051b30a1' being the container ID.\n```\ngrep -l 1cb6051b30a1 /proc/*/mountinfo\n/proc/1963/mountinfo\nps aux | grep -i 1963\nroot      1963  1.9  0.8 588740 71688 ?        Ssl  Aug26  30:08 /usr/bin/cadvisor\nroot     14767  0.0  0.0  11744   952 pts/0    S+   00:56   0:00 grep --color=auto -i 1963\n```\nPlease suggest a workaround for this.\n. @rjnagal I can confirm that this issue happens on Ubuntu trusty x64 with Doceker 1.8.1, cadvisor:latest and devicemapper.\n'1cb6051b30a1' being the container ID.\n```\ngrep -l 1cb6051b30a1 /proc/*/mountinfo\n/proc/1963/mountinfo\nps aux | grep -i 1963\nroot      1963  1.9  0.8 588740 71688 ?        Ssl  Aug26  30:08 /usr/bin/cadvisor\nroot     14767  0.0  0.0  11744   952 pts/0    S+   00:56   0:00 grep --color=auto -i 1963\n```\nPlease suggest a workaround for this.\n. ",
    "hourliert": "Same problem here with Ubuntu 14.04.3.\n@difro solution works but cadvisor can't provide docker stats anymore.\nAny workaround?\n. ",
    "rmetzler": "The last time I ran into this problem, I digged a little bit into the cAdvisor source code. I'm not 100% sure - because it was a few weeks ago - but this is essentially the gist:\nIf you use cAdvisor like it is shown in README.md you'll mount /var/lib/docker as a volume into the container. This will create dead containers.\nThe reason, cAdvisor wants you to mount /var/lib/docker is - as far as I could see - only to display a certain info that is only interesting for admins and should be known before hand.\n. ",
    "tuxknight": "Same situation.\nMy Docker Version is 1.9.1\nCadvisor  version 0.18.0\nAnd when docker rm container fails, the status of that container change to \"dead\" .\nIs it possible to umount that specific mountpoint when  container status changed to \"exit\" or \"dead\"  ?\n. ",
    "arhea": "+1\n. ",
    "tonysickpony": "running cAdvisor without --volume=/:/rootfs:ro seems to fix it. \nAs pointed out in https://github.com/google/cadvisor/blob/master/docs/running.md\nI haven't fully tested it yet, but works fine up to now\n. ",
    "xbglowx": "I had to remove the following volume mounts:\n- /:/rootfs:ro\n- /var/lib/docker/:/var/lib/docker:ro\nSetup:\n- Ubuntu 14.04.3 LTS\n- docker 1.9.1 with aufs\n- cAdvisor 0.20.5\n. Upgraded docker to 1.10.3 and now cAdvisor can only see the docker images, but no containers, if I only use volume mounts:\n- /var/run:/var/run:rw\n- /sys:/sys:ro\n- /var/lib/docker/:/var/lib/docker:ro\nIf I add /:/rootfs:ro, cAdvisor can see the containers, but I get device or resource busy, when trying to remove any container.\n. Using google/cadvisor:v0.22.0\n. If I try using --disable_metrics=\"tcp,disk\" I get the following:\n```\nsudo docker run -ti -v /var/lib/docker/:/var/lib/docker:ro -v /var/run:/var/run:rw -v /sys:/sys:ro -v /:/rootfs:ro google/cadvisor --disable_metrics=\"tcp,disk\"\npanic: assignment to entry in nil map\ngoroutine 1 [running]:\npanic(0xb0c8c0, 0xc8201c0440)\n    /usr/local/go/src/runtime/panic.go:481 +0x3e6\nmain.(metricSetValue).Set(0x15ac528, 0x7ffe3cea1f59, 0x8, 0x0, 0x0)\n    /go/src/github.com/google/cadvisor/cadvisor.go:85 +0x1da\nflag.(FlagSet).parseOne(0xc82004e060, 0xc82005e901, 0x0, 0x0)\n    /usr/local/go/src/flag/flag.go:881 +0xdd9\nflag.(*FlagSet).Parse(0xc82004e060, 0xc82000a100, 0x2, 0x2, 0x0, 0x0)\n    /usr/local/go/src/flag/flag.go:900 +0x6e\nflag.Parse()\n    /usr/local/go/src/flag/flag.go:928 +0x6f\nmain.main()\n    /go/src/github.com/google/cadvisor/cadvisor.go:99 +0x68\n```\nThis is with cAdvisor version 0.23.0 (750f18e). Works fine with 0.22.0. \nI still need to see if using --disable_metrics=\"tcp,disk\" fixes the problem.\n. One should not have to worry about starting/stopping containers in order to properly run cAdvisor. Monitoring should have no affect on the running of containers.\n. This is no longer an issue for me, since I switched to:\n\nKernel: 4.4.x\nStorage driver: overlay2. \n",
    "jordic": "Any ideas or suggestions how can i dig inside the issue?\n. With docker 1.11.1 the is issue is gone.  With the latest fixes from docker part, seems working now.\n. ",
    "ashkop": "I'm still able to reproduce this with docker 1.11.1 and cAdvisor 0.23.0. Ubuntu 14.04. \n. @vishh Unfortunately the flag didn't help. As @xbglowx mentioned, this option causes 0.23.0 to crash, so I tried 0.22.0 and canary. Both still prevent me from removing containers. Here's the error message I get:\nError response from daemon: Unable to remove filesystem for 9e96817fba0a443f75d1426b6d7a586f4bc84217b06eb021f6d28bae4f341473: remove /var/lib/docker/containers/9e96817fba0a443f75d1426b6d7a586f4bc84217b06eb021f6d28bae4f341473/shm: device or resource busy\n. I tried to reproduce it on fresh VM, but failed. I'll try to find the difference that is actually causing the issue. Meanwhile I did lsof inside the cadvisor container of the file that is being blocked. Here's what I got:\n1   /usr/bin/cadvisor   pipe:[70918923]\n1   /usr/bin/cadvisor   pipe:[70918924]\n1   /usr/bin/cadvisor   pipe:[70918925]\n1   /usr/bin/cadvisor   socket:[70919220]\n1   /usr/bin/cadvisor   anon_inode:[eventpoll]\n1   /usr/bin/cadvisor   anon_inode:inotify\n1   /usr/bin/cadvisor   socket:[70919240]\n. I also noticed that issue occurs only if I start cadvisor after my own containers. If cadvisor is the first one started, then I can restart my containers without any issue.\n. cadvisor should be the first container to start.\n. 100% agreed. I'm a telling that to workaround the issue you can start cAdvisor before other containers.\n. ",
    "infiniteproject": "Same here on Debian 8, Docker 1.11.1 and latest cAdvisor.\n. ",
    "moortimis": "I am experiencing the same issue with the following versions\n\"cAdvisor version: 0.23.0-750f18e\"\ngoogle/cadvisor               latest              5cda8139955b        8 days ago          48.92 MB\nCentOS Linux release 7.2.1511 (Core) \nDocker version 1.11.1, build 5604cbe\nWork around was to remove /var/lib/docker from the shared volume. \n. ",
    "ceecko": "The issue persists in v0.23.1 on CentOS7, Docker 1.10.1, devicemapper\ndocker run \\\n  --rm \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  google/cadvisor:v0.23.1 \\\n  -docker_only \\\n  --disable_metrics=\"tcp,disk\"\n. To add more info - the issue persists on v0.23.1 and v0.23.2 on CentOS7, Docker 1.11.1, devicemapper.\nHowever the issue only occurs when cadvisor is run from docker.\nRunning cadvisor directly on CentOS7 works without issues.\n. @ashkop That's actually correct. I tried to reproduce the error, but couldn't. If the other containers are started first, only then cadvisor blocks removal.\n. Here's a script to replicate the error on CentOS 7.\nYou will need a machine with an empty block device (just replace the path to the device in DOCKER_DATA_DISK) and it will setup docker with devicemapper through lvm's thin-pool, run a container, then cadvisor and then stop & rm the first container.\n```\n!/bin/bash\nDOCKER_DATA_DISK=/dev/vdb\nset -exo pipefail\nsetenforce Permissive\nyum update -y\nyum install -y lvm2\nsystemctl enable lvm2-lvmetad\nsystemctl start lvm2-lvmetad\npvcreate $DOCKER_DATA_DISK\nvgcreate data $DOCKER_DATA_DISK\nlvcreate -l 100%free -T data/docker_thin\ncurl -sSL https://get.docker.com/ | sh\nmkdir -p /etc/systemd/system/docker.service.d\ncat < /etc/systemd/system/docker.service.d/docker-lvm.conf\n[Service]\nExecStart=\nExecStart=/usr/bin/docker daemon -H fd:// \\\n    -s devicemapper \\\n    --storage-opt dm.thinpooldev=/dev/mapper/data-docker_thin\nTimeoutStartSec=3000\nEOF\nsystemctl daemon-reload\nsystemctl enable docker\nsystemctl start docker\nsleep 3\ndocker run \\\n    --name=test \\\n    -d \\\n    debian:jessie \\\n    /bin/sh -c \"while true; do foo; sleep 1; done\"\ndocker run \\\n  -d \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --name=cadvisor \\\n  google/cadvisor:v0.23.1 \\\n  -docker_only \\\n  --disable_metrics=\"tcp,disk\"\ndocker stop test\ndocker rm test\n```\nThe output is:\n```\n... some data ...\n\ndocker stop test\ntest\ndocker rm test\nError response from daemon: Unable to remove filesystem for 7d7513b0c3310f26e7425728f9c34e219db53a5e4dbb6e0e4259c2e6eb760044: remove /var/lib/docker/containers/7d7513b0c3310f26e7425728f9c34e219db53a5e4dbb6e0e4259c2e6eb760044/shm: device or resource busy\n\n```\n. ",
    "amcrn": "On Ubuntu 14.04, using --disable_metrics=\"tcp,disk\" still does not fix the problem. I've confirmed @ashkop 's observation: If cAdvisor is started after another container, then removing said container fails.\n. Hope this helps someone else:\nUbuntu 16.X (kernel 4.4.X)   and Docker 1.11.2 w/ AUFS works fine.\nUbuntu 14.X (kernel 3.13.X) and Docker 1.11.2 w/ AUFS exhibits the problem.\nSo, it looks like overlay isn't necessary, a kernel upgrade is all that's required.. #938 was closed, but I don't see any 0.19.x releases on docker hub: https://hub.docker.com/r/google/cadvisor/tags/ . is there a different bug holding up the push?\n. @vishh wow, that was fast! pulled, ran, and looks good. thanks a bunch!\n. This looks to have been solved (Docker Hub is up to date as of now). Closing.\n. Still seeing this with cAdvisor 0.23.1 and Docker 1.11.1. @vishh any ideas?\n. > The log reports that this is a fatal error, but it looks like cAdvisor still starts up. Are you actually seeing any issues from this, other than the scary log message?\nI assumed this was because we have the Docker restart policy set to always, which at some point, it does run. Might have been a poor assumption, apologies. Let me confirm this is the case.\n\nAlso, what OS / distro are you running on? I couldn't reproduce on my local workstation.\n\nUbuntu 14.04 LTS w/ Kernel at 3.13.0-24-generic.\n. @timstclair I confirmed that the retries are from cAdvisor and not due to our Docker restart policy. So it definitely recovers, but these retries account for almost 10 seconds added to startup:\nI0927 18:58:03.959701       1 storagedriver.go:50] Caching stats in memory for 2m0s\n...\nI0927 18:58:12.004511       1 cadvisor.go:157] Starting cAdvisor version: v0.24.0-0cdf491 on port 8080\nThis came to our attention because we have health checks on the cAdvisor container after we start it that were failing (We hit /api/ looking for 'Supported API versions', 3 retries, exponential backoff starting at 1 second).\n10 seconds is pretty significant in terms of container startup, so https://github.com/google/cadvisor/issues/1483#issuecomment-250032379 would definitely be a welcome addition :)\n. @timstclair I don't see the same 10 second delay when using cAdvisor 0.23.8\nUsing cAdvisor 0.23.8, the logs after a docker restart cadvisor:\nI0930 02:01:28.321492       1 manager.go:1062] Exiting thread watching subcontainers\nI0930 02:01:28.321706       1 manager.go:347] Exiting global housekeeping thread\nI0930 02:01:28.321744       1 cadvisor.go:185] Exiting given signal: terminated\nI0930 02:01:28.548682       1 storagedriver.go:50] Caching stats in memory for 2m0s\nI0930 02:01:28.548903       1 manager.go:138] cAdvisor running in container: \"/docker/d5612ca931170142e0dc73207b3a33627b225511fef2864835f3e0f84e195f2b\"\nW0930 02:01:28.555835       1 manager.go:146] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp 127.0.0.1:15441: getsockopt: connection refused\nI0930 02:01:28.565094       1 fs.go:139] Filesystem partitions: map[/dev/vdb:{mountpoint:/rootfs/mnt major:253 minor:16 fsType: blockSize:0} /dev/vda:{mountpoint:/var/lib/docker/aufs major:253 minor:0 fsType:ext4 blockSize:0}]\nI0930 02:01:28.627006       1 info.go:47] Couldn't collect info from any of the files in \"/rootfs/etc/machine-id,/var/lib/dbus/machine-id\"\nI0930 02:01:28.627087       1 manager.go:192] Machine: {NumCores:16 CpuFrequency:2593748 MemoryCapacity:126749863936 MachineID: SystemUUID:49A2923F-CAD1-4203-A4A2-BBBC04508309 BootID:51251646-7302-45c1-a004-6c828c5c5aea Filesystems:[{Device:none Capacity:31572619264 Type:vfs Inodes:1966080} {Device:/dev/vda Capacity:31572619264 Type:vfs Inodes:1966080} {Device:/dev/vdb Capacity:845381279744 Type:vfs Inodes:52428800}] DiskMap:map[253:0:{Name:vda Major:253 Minor:0 Size:32212254720 Scheduler:none} 253:16:{Name:vdb Major:253 Minor:16 Size:858993459200 Scheduler:none} 253:32:{Name:vdc Major:253 Minor:32 Size:67108864 Scheduler:none}] NetworkDevices:[{Name:eth0 MacAddress:74:db:d1:a0:a4:a3 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:7845466112 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:1 Memory:7926992896 Cores:[{Id:0 Threads:[1] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:2 Memory:7926988800 Cores:[{Id:0 Threads:[2] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:3 Memory:7926992896 Cores:[{Id:0 Threads:[3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:4 Memory:7926988800 Cores:[{Id:0 Threads:[4] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:5 Memory:7926992896 Cores:[{Id:0 Threads:[5] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:6 Memory:7926988800 Cores:[{Id:0 Threads:[6] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:7 Memory:7926992896 Cores:[{Id:0 Threads:[7] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:8 Memory:7926984704 Cores:[{Id:0 Threads:[8] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:9 Memory:7926992896 Cores:[{Id:0 Threads:[9] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:10 Memory:7926988800 Cores:[{Id:0 Threads:[10] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:11 Memory:7926992896 Cores:[{Id:0 Threads:[11] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:12 Memory:7926988800 Cores:[{Id:0 Threads:[12] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:13 Memory:7926992896 Cores:[{Id:0 Threads:[13] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:14 Memory:7926988800 Cores:[{Id:0 Threads:[14] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:15 Memory:7926530048 Cores:[{Id:0 Threads:[15] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}\nI0930 02:01:28.627909       1 manager.go:198] Version: {KernelVersion:3.13.0-24-generic ContainerOsVersion:Alpine Linux v3.2 DockerVersion:1.11.2 CadvisorVersion:0.23.8 CadvisorRevision:5c5b2b8}\nI0930 02:01:28.637104       1 factory.go:228] Registering Docker factory\nE0930 02:01:28.637203       1 manager.go:240] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp 127.0.0.1:15441: getsockopt: connection refused\nI0930 02:01:28.637227       1 factory.go:54] Registering systemd factory\nI0930 02:01:28.641716       1 factory.go:86] Registering Raw factory\nI0930 02:01:28.646170       1 manager.go:1072] Started watching for new ooms in manager\nW0930 02:01:28.646346       1 manager.go:268] Could not configure a source for OOM detection, disabling OOM events: unable to find any kernel log file available from our set: [/var/log/kern.log /var/log/messages /var/log/syslog]\nI0930 02:01:28.647367       1 manager.go:281] Starting recovery of all containers\nI0930 02:01:28.675280       1 manager.go:286] Recovery completed\nI0930 02:01:28.706153       1 cadvisor.go:151] Starting cAdvisor version: 0.23.8-5c5b2b8 on port 8080\n\nDo you have a lot of containers running? \n\nOnly 3 containers on this machine.\nAnother update: I can't cause the delay/fault behavior in cAdvisor 0.24.0 consistently. It happens on some restarts, but not on others :/\n. @carlpett I think you're onto something, because we don't consistently see this either.\n. given the above, shouldn't the official cadvisor docker images now be built with the netgo tags?\n. any update on this? we're still stuck running v0.23.8 because of it. thanks!. ",
    "theroys": "To get around this issue i have tried running cadvisor as standalone..however it does not get data while i am using  RHEL , \ncadvisor complains \"unable to get fs usage from thin pool for device\".. it seems it cant get right information about the storage driver.\nUsing RHEL 7.1\n  version 0.23.3 (6607e7c)\n docker 1.9.1     \nAnybody tried similar\n. The workaround that always works as per our experience running CAdvisor in host as a process , rather than as a docker container. We have it running in Production without any incidence.\n. ",
    "shane-axiom": "This issue is hitting us often and affecting production container deployments (Debian 8.5 hosts, Docker 1.11.1).\nCan anyone spell out what we lose by omitting the /:/rootfs:ro mount? Is it just disk usage metrics?\n. Wanted to add that removing the root volume (/:/rootfs:ro) did not solve this issue for us. We ended up removing cadvisor from our deployment ecosystem until this issue is resolved as it was causing too much pain in our deployment scheme.\n. ",
    "zevarito": "So, it is possible to stop cadvisor before stop/start any other containers and then start cadvisor again?\n. But once Cadvisor is monitoring the other containers you are not able to\nremove one of the monitored ones until you remove Cadvisor, at least that\nhappened to me a lot until now that I stop Cadvisor update containers and\nstart it again. I am doing this wrong?\n2016-07-24 13:25 GMT-03:00 Alex notifications@github.com:\n\n100% agreed. I'm a telling that to workaround the issue you can start\ncAdvisor before other containers.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/771#issuecomment-234786522,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AACgalxt67a6n9U7PMSW2nyKXJRWjjP5ks5qY5IBgaJpZM4FBIxe\n.\n\n\nAlvaro\n. I've opted for put down cadvisor while deploying/removing containers and\nthen put it up again. Not liked much but works.\n2016-08-03 17:09 GMT-03:00 Chad McElligott notifications@github.com:\n\nupgrading to Docker 1.12 also did not make any difference\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/771#issuecomment-237356597,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AACgatIXetobsMlE0PJR7LKsnGO454m2ks5qcPWRgaJpZM4FBIxe\n.\n\n\nAlvaro\n. Evert, to which tool have you moved to monitor containers?\n2016-08-05 10:17 GMT-03:00 EvertMDC notifications@github.com:\n\nHello Chadxz,\nDid you build the latest release or the master? Because the current master\nbranch has issues I have the impression, or I'm doing something wrong.\n../../../golang.org/x/oauth2/jws/jws.go:67:17: error: reference to\nundefined identifier \u2018base64.RawURLEncoding\u2019\nreturn base64.RawURLEncoding.EncodeToString(b), nil\n^\n../../../golang.org/x/oauth2/jws/jws.go:85:16: error: reference to\nundefined identifier \u2018base64.RawURLEncoding\u2019\nreturn base64.RawURLEncoding.EncodeToString(b), nil\n^\n../../../golang.org/x/oauth2/jws/jws.go:105:16: error: reference to\nundefined identifier \u2018base64.RawURLEncoding\u2019\nreturn base64.RawURLEncoding.EncodeToString(b), nil\n^\n../../../golang.org/x/oauth2/jws/jws.go:116:25: error: reference to\nundefined identifier \u2018base64.RawURLEncoding\u2019\ndecoded, err := base64.RawURLEncoding.DecodeString(s[1])\n^\nand it goes on like that.\nWe too removed cadvisor from our dev systems as it created 'dead'\ncontainers when we tried to remove others.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/771#issuecomment-237847659,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AACgarT3UVvwHModKzo533_6tykLHwPZks5qczf_gaJpZM4FBIxe\n.\n\n\nAlvaro\n. Hi Evert,\nI have all with Cadvisor, but I will stop monitoring the Host itself with\nCadvisor and just monitor the containers. For the Host I think Node\nExporter should be the safest bet.\n2016-08-05 11:19 GMT-03:00 EvertMDC notifications@github.com:\n\nThanks @chadxz https://github.com/chadxz . I overlooked that.\nHello @zevarito https://github.com/zevarito . None at the moment but I\nhave used the container exporter images and they worked fine. They\nsuggested to use cadvisor however as they are no longer maintaining it.\nGoing to run it on the system itself now as Chadxz suggested and see how\nit goes.\nhttps://github.com/docker-infra/container_exporter\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/771#issuecomment-237862836,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AACgahA5bHNx71HVm7ZWjAMOsG9OPY7sks5qc0Z3gaJpZM4FBIxe\n.\n\n\nAlvaro\n. @keylok yeah, with node_exporter is even worse, I'd suggest to move it to\nthe host (as developers say) to avoid any troubles.\n2017-06-15 5:28 GMT-03:00 keyolk notifications@github.com:\n\nI got similar issue with prometheus node_exporter also\nprometheus/node_exporter#602\nhttps://github.com/prometheus/node_exporter/issues/602\nseems bind mounting the path which including /var/lib/docker\nmake mount name space leaking.\nboth are resolved with running it on host directly.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/771#issuecomment-308664996,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AACgaqx351IfzBjKIr-_9DW01UyHRmngks5sEOtKgaJpZM4FBIxe\n.\n\n\n-- \nAlvaro\n. ",
    "chadxz": "i ran into this issue as well. removing the /:/rootfs:ro volume works around the issue for me, but i do lose some stats... looks like network inside the containers, and process lists... maybe others that i haven't noticed right off.\nDocker 1.11.2, cAdvisor 0.23.1\n. i am able to confirm that having cAdvisor loaded first, any containers loaded after are able to be removed without the 'device or resource busy' error\n. upgrading to Docker 1.12 also did not make any difference\n. I ended up just running cAdvisor on the host instead of as a container, and it is working well that way.\n. @EvertMDC I downloaded the prebuilt binary from the latest stable release on the releases page https://github.com/google/cadvisor/releases/tag/v0.23.2\n. @jaybennett89 cadvisor running on host has been working fine. The \"Unable to remove filesystem\" error never occurs.\n. I noticed today that my systems were using devicemapper, so i switched them to using aufs. So far I have not had any issue with being unable to remove containers when they are started before the cAdvisor container is started. Using all volumes also... docker 1.11.1 / aufs / cAdvisor 0.24.1. ",
    "Thorndike": "Removing the root volume did not make any difference for me. Still blocking containers from removal.\ndocker 1.11.1\nGoing to remove cAdvisor from all systems as it is blocking my deployment.\n. ",
    "Tiduster": "We have this issue too in our production environment.\nIt's very frustrating because it blocks our upgrade process.\nWe use Debian 8, Docker 1.10.3 and cadvisor 0.23.2.\n. ",
    "EvertMDC": "Hello Chadxz,\nDid you build the latest release or the master? Because the current master branch has issues I have the impression, or I'm doing something wrong.\n../../../golang.org/x/oauth2/jws/jws.go:67:17: error: reference to undefined identifier \u2018base64.RawURLEncoding\u2019\n   return base64.RawURLEncoding.EncodeToString(b), nil\n                 ^\n../../../golang.org/x/oauth2/jws/jws.go:85:16: error: reference to undefined identifier \u2018base64.RawURLEncoding\u2019\n  return base64.RawURLEncoding.EncodeToString(b), nil\n                ^\n../../../golang.org/x/oauth2/jws/jws.go:105:16: error: reference to undefined identifier \u2018base64.RawURLEncoding\u2019\n  return base64.RawURLEncoding.EncodeToString(b), nil\n                ^\n../../../golang.org/x/oauth2/jws/jws.go:116:25: error: reference to undefined identifier \u2018base64.RawURLEncoding\u2019\n  decoded, err := base64.RawURLEncoding.DecodeString(s[1])\n                         ^\nand it goes on like that.\nWe too removed cadvisor from our dev systems as it created 'dead' containers when we tried to remove others.\n. Thanks @chadxz . I overlooked that.\nHello @zevarito . None at the moment but I have used the container exporter images and they worked fine. They suggested to use cadvisor however as they are no longer maintaining it.\nGoing to run it on the system itself now as Chadxz suggested and see how it goes.\nhttps://github.com/docker-infra/container_exporter\n. Has the new release something to do with this bug? The description seems like it does.\nhttps://github.com/google/cadvisor/releases/tag/v0.24.1\n. I noticed something different the other day. My lvm volume couldn't be removed until I stopped the cadvisor container. This must be related.\n. I haven't seen any solution to this yet, apart from installing cadvisor on the host itself without a container. But your machines have to support that.\nIt's a difficult one to solve in my opinion since cadvisor actually works. It's only in the runtime environment of docker that issues arise.. ",
    "jaybennett89": "Hey @chadxz, have you continued to have success running cadvisor on the host directly?\nI am facing the same problem production when running cadvisor in a container. It's hard to validate in a short period of time whether the baremetal approach will work to fix this bug when it appears so rarely. \n. maybe I am just missing something, but I agree with this issue. The v2.0 API does not seem to offer an obvious equivalent to v1 client's AllDockerContainers() query. How do you search a container stats without knowing the name of your container already?\n. ",
    "rhuddleston": "I suggest adding dm.use_deferred_removal=true and dm.use_deferred_deletion=true to /etc/docker/daemon.json as a possible workaround:\n{\n  \"live-restore\": true,\n  \"storage-driver\": \"devicemapper\",\n  \"storage-opts\": [\n    \"dm.use_deferred_removal=true\",\n    \"dm.use_deferred_deletion=true\"\n  ]\n}\n. Same issue here:\nNov  1 20:55:44 prom1 dockerd[25267]: time=\"2016-11-01T20:55:44.822064076Z\" level=error msg=\"Handler for GET /containers/bdd3ca44b528d9fd5704fe5781f5f375d63e5103e0c7c81daa4377bfbb1d68d1/json returned error: No such container: bdd3ca44b528d9fd5704fe5781f5f375d63e5103e0c7c81daa4377bfbb1d68d1\"\nNov  1 20:55:44 prom1 dockerd[25267]: time=\"2016-11-01T20:55:44.823293659Z\" level=error msg=\"Handler for GET /containers/03458bf30794c6cff51fa38946e0067655191b3087cf1012facfeec71e02e44e/json returned error: No such container: 03458bf30794c6cff51fa38946e0067655191b3087cf1012facfeec71e02e44e\"\nNov  1 20:55:44 prom1 dockerd[25267]: time=\"2016-11-01T20:55:44.823722987Z\" level=error msg=\"Handler for GET /containers/6379b7b68f5b7261e70de3f676923d9eca0bca1ef3123f255e90b8e9c702daa2/json returned error: No such container: 6379b7b68f5b7261e70de3f676923d9eca0bca1ef3123f255e90b8e9c702daa2\"\nNov  1 20:55:44 prom1 dockerd[25267]: time=\"2016-11-01T20:55:44.824549578Z\" level=error msg=\"Handler for GET /containers/30d21f7cf69be1760be6f1f65762e863c88f87e6142854dfe6bdfe5ffef51e98/json returned error: No such container: 30d21f7cf69be1760be6f1f65762e863c88f87e6142854dfe6bdfe5ffef51e98\"\nNov  1 20:55:44 prom1 dockerd[25267]: time=\"2016-11-01T20:55:44.824986987Z\" level=error msg=\"Handler for GET /containers/d8625f2e50ea94ed315718980ffb0a9581d27dca1cccf236c90b01b4b483b21b/json returned error: No such container: d8625f2e50ea94ed315718980ffb0a9581d27dca1cccf236c90b01b4b483b21b\"\nroot@prom1:~# df | grep mnt\n/dev/dm-3       10474496    91188  10383308   1% /var/lib/docker/devicemapper/mnt/bdd3ca44b528d9fd5704fe5781f5f375d63e5103e0c7c81daa4377bfbb1d68d1\n/dev/dm-2       10474496    81224  10393272   1% /var/lib/docker/devicemapper/mnt/03458bf30794c6cff51fa38946e0067655191b3087cf1012facfeec71e02e44e\n/dev/dm-6       10474496    49392  10425104   1% /var/lib/docker/devicemapper/mnt/6379b7b68f5b7261e70de3f676923d9eca0bca1ef3123f255e90b8e9c702daa2\n/dev/dm-5       10474496    53236  10421260   1% /var/lib/docker/devicemapper/mnt/30d21f7cf69be1760be6f1f65762e863c88f87e6142854dfe6bdfe5ffef51e98\n/dev/dm-4       10474496   317108  10157388   4% /var/lib/docker/devicemapper/mnt/d8625f2e50ea94ed315718980ffb0a9581d27dca1cccf236c90b01b4b483b21b\nIt my case it's a devicemapper mounts\n. https://github.com/google/cadvisor/pull/1847 any plans on a new release soon?. ",
    "soumyadipDe": "@rhuddleston tried using dm.use_deferred_removal=true and dm.use_deferred_deletion=true. Still Resource busy error is throwing but containers are getting removed which were being in Dead state earlier. Is that the same with you?\n. ",
    "psychok7": "I had the same problem using docker-compose. As a Workaround (and like it has been mentioned before) i removed my Cadvisor service from the compose file and started the container manually using just docker before anything else. \nAlso, i had to connect this container to the same network as my compose services using docker network connect default cadvisor, this way my services can now see the container. I can now restart my services without running into this nasty error.\n. @dashpole i needed to restrict unautorhized access to cAdvisor. At the momento i am using my Nginx Basic Authentication so that only admins access it, but i thought Cadvisor had that feature built in as well (in case i don't have a reverse proxy).. ",
    "toussa": "Same problem for us too... We had to stop to use cAdvisor in production until this is fixed.\nThe workaround that says to install cAdivsor directly to the host is not possible for us.\n. ",
    "Nightbr": "Hey, \nany news from this issue ? I'm still stuck with this:\n\ndocker 1.12.3\ncadvisor latest (v0.24.1)\nDebian Jessie64\n\nThe bug \"device is busy\" appears only if cadvisor is start after other containers we want to manage (restart, remove, ...).\nI tried all workaround but I'm not completely satified:\n\n\nRemove this two volumes\n            - /:/rootfs:ro\n            - /var/lib/docker/:/var/lib/docker:ro\nProblem: We loose most container metrics...\n\n\nStart cadvisor first\nThis is the easier workaround to put into practice but it is not really convenient and scallable...\n\n\nStop all container, start cadvisor, restart all container:\n```\n\n\n!/bin/sh\nStop all containers\ndocker stop $(docker ps -a -q)\nStart cadvisor\ndocker run \\\n    -d \\\n    --name=cadvisor \\\n    --volume=/:/rootfs:ro \\\n    --volume=/var/run:/var/run:rw \\\n    --volume=/sys:/sys:ro \\\n    --volume=/var/lib/docker/:/var/lib/docker:ro \\\n    --net logs_back-tier \\\n    google/cadvisor\nRestart all container\ndocker start $(docker ps -a -q)\n```\nVery slow if you have a lots of containers... And not working well if you use docker-compose and depends_on for starting order...\n\nInstall cadvisor and start it on the host\n\nAny documentation or advice with this ? Because it is really painfull with docker to link container with service on host (you need to find the host ip for the container network, ...). If someone has anything on this I would appreciate.\nThat's it ;) The best is to fix this issue perhaps in future version but it seems to be a bit critical and reproductible. \nThanks in advance!\n. ",
    "RRAlex": "Seems like (f)statfs are the problem, according to docker at least:\nhttps://docs.docker.com/engine/admin/troubleshooting_volume_errors/. I'm also wondering if there is a way to run cadvisor so it is more light weight (like other prometheus containers).\nThe fact that it consumes quite a lot of CPU & RAM, while not being probed or looked at, is concerning and maybe my setup is wrong.... Thanks all, I indeed started playing with the housekeeping settings after my initial comment and realized I could tone down it's requirements quite a lot...\nEverything is much smoother now! :-)\nedit: @mboussaa bellow:\n --allow_dynamic_housekeeping=true --housekeeping_interval=10s\nIt might vary over time, but that worked for now.\nI'm just starting setting up Prometheus as I need to integrate cadvisor & al. on prod instances. :). ",
    "keyolk": "I got similar issue with prometheus node_exporter also\nhttps://github.com/prometheus/node_exporter/issues/602\nseems bind mounting the path including /var/lib/docker\nmakes mount namespace leaking.\nboth are resolved with running it on host directly.. @zevarito \nI think it can be mitigated.\nif I can put exact volumes to be used to the container.\nwhat I means just take off /var/lib/docker/devicemapper being mounted.\ncould you inform me what of exact host data it uses ?. ",
    "jindov": "Same issue with my system:\nOS: ubuntu 14.04 LTS\nKernel: 3.13.0-48-generic\nDocker: 17.04.0-ce\nGot this issue when run cadvisor v:0.26 with docker (even cadvisor:latest). Everything seems ok with node_exporter. Thank @viossat, I will try to upgrade on dev env, but with prod env, we can't do this, so I decide to run on host directly. It's worked well. You can use supervisord to run directly, this is my configuration to run cadvisor:\n[program:cadvisor]\ndirectory=/build/metric_exporter/cadvisor/src/github.com/google/cadvisor\ncommand=/build/metric_exporter/cadvisor/src/github.com/google/cadvisor/cadvisor -port 9080\nautostart=true\nautorestart=unexpected\nredirect_stderr=true\nenvironment=GOROOT=\"/usr/local/go\",GOPATH=\"GOPATH=/build/metric_exporter/cadvisor\",PATH=\"$GOPATH/bin:$GOROOT/bin:$PATH\"\nJin. Waiting for this support, I use ELK 5.5 stack on system. ",
    "viossat": "@jindov Try to upgrade your kernel, you need to switch to the overlay driver. See my previous comment.. ",
    "garyden": "How to run cadvisor on the host directly? \nGary. ",
    "stephan2012": "Same problem with RHEL 7.4, Docker 17.06.2. Doesn't matter if I'm using ZFS or Overlay2.\nAny solution for this by now? Or just run cAdvisor directly on the host?. ",
    "gengwg": "I'm having this issue (can't remove container) on latest version of cadvisor on Centos 7 with kernel version:\n$ uname -r\n3.10.0-514.26.2.el7.x86_64\nUnfortunately I can't upgrade the kernel version, as it is provisioned by our infra team. And we can't upgrade the OS ourselves.\nI bypassed this issue by systemctl restart cadvisor, then docker rm <container id> worked. \n. ",
    "sjkeerthi": "In that case my cgroup is not under the /cgroup since its CentOS 7.x its under /sys/fs/cgroup even in that case do you want me to still point volume=/cgroup:/cgroup:ro\n. ",
    "GregoryKutuzov": "Axibase Corporation\n2015-06-18 21:28 GMT+03:00 googlebot notifications@github.com:\n\nThanks for your pull request. It looks like this may be your first\ncontribution to a Google open source project, in which case you'll need to\nsign a Contributor License Agreement (CLA).\n[image: :memo:] Please visit https://cla.developers.google.com/\nhttps://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll\nverify. Thanks.\n\nIf you've already signed a CLA, it's possible we don't have your\n  GitHub username or you're using a different email address. Check your\n  existing CLA data https://cla.developers.google.com/clas and verify\n  that your email is set on your git commits\n  https://help.github.com/articles/setting-your-email-in-git/.\nIf you signed the CLA as a corporation, please let us know the\n  company's name.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/774#issuecomment-113250695.\n\n\n\u0421 \u0443\u0432\u0430\u0436\u0435\u043d\u0438\u0435\u043c,\n\u0413\u0440\u0438\u0433\u043e\u0440\u0438\u0439 \u041a\u0443\u0442\u0443\u0437\u043e\u0432_._\n. Can we get an update on our pull request?\n. Can we get an update on our pull request?\n. Can we get an update on our pull request?\n. Sure, I've done it. I will be happy to answer any questions you have.\n. Can we get an update on our pull request?\n. Can we get an update on our pull request?\n. Can we get an update on our pull request?\n. This flag is to include statistics about containers with name prefix \"/user\"\n. Docker host is the entity in ATSD. It is useful to set this flag as the machine hostname where docker daemon is running. \n. Lgtm\n. We think that 15 second collection interval is more appropriate for a long-term retention. Since the metrics will be sent to ATSD at this frequency it does not make sense to collect them more frequently than necessary. \n. Time duration between two AddStats calls can be less than housekeeping interval. If we setup our storage driver with a sampling interval = housekeeping interval it will send series to ATSD with reduced frequency. To fix this we add max possible error. \n. connectionLimit param possible values must be > 0. Why shouldn't we use uint to force these boundaries?\n. In case multiple cadvisors storing data to ATSD this helps as differentiate cadvisors from each other. From container we can't get docker engine hostname.\n. deduplicationParamsList must implement flag.Value\n. I have tried to replace these maps with a single map but after that I change my point of view to remain this code unchanged due to rising new additional checks.\n. No it wouldn't. All these SendSeriesCommands actually only stores commands to a buffer.\n. The main purpose of this test is to check that we sends only expected commands. If some command conversion logic changes we will see it with this test. \n. 8088/8443 - web interface + RESTful api (http/https)\n8081 - tcp network commands input\n8082 - udp network commands input\n. Here I need to delegate String()method to map implementation. If I type fmt.Sprint(self) I get stack overflow error because of infinite recursion (Sprint calls self.String()). \nIf there is a more elegant way to do this I would be happy to know about it.\n. Yes, but I can't use reference here. Default toml marshaller use plain struct and ignores references.\n. ",
    "heinrichvk": "I signed the CLA agreement.\n. ",
    "rodionos": "This looks like a build issue by @k8s-bot again, or a CLA licensing confirmation issue. @KutuzovGregory - please review.\n. @timstclair Tim, I wanted to reach out and see which direction you want to take with respect to this ATSD storage driver PR. We now officially hold the honor of having the oldest PR with an open status in the cadvisor project. Let us know how would you like to proceed, thanks!. @timstclair #1458 advocates for major changes in the storage adapter architecture. Looks like it'll be a show-stopper for this PR given incompatibilities. It's somewhat discouraging given the effort spent (code, testing, reviews, maintenance) especially given the fact that this integration was discussed early on with the cadvisor team. Storage adapters are created not equal and there are perhaps implementations that increase the maintenance load on the core cadvisor team. In our case you have the company backing the adapter with necessary resources and we're committed to resolve ATSD adapter issues raised through the cadvisor master project. In fact we can provide additional assurances in terms of issue ETAs for the ATSD adapter, if necessary. Having this PR accepted would make use of the code that's already developed and tested, without increasing overhead on the cadvisor team.\n. ",
    "anushree-n": "I signed it.\n. Thank you @vmarmol  and @rjnagal ! I will make the necessary modifications.\n. I signed it.\n. I signed it.\n. I have made the changes you suggested, @vmarmol :)\n. Looks good! Thanks!\n. I signed it.\n. Done, @rjnagal  :)\n. Made the changes, @rjnagal.\n. I believe the earlier model took the minimum of next stats collection and next custom metrics collection as the next housekeeping time.\nIf the minimum supported frequency is 1s, yes, this looks good! :)\n. I agree, @rjnagal, @vmarmol .\nAppend metrics under ContainerStats\n     - Already in place\n     - Logically , custom metrics follows cpu, memory stats\n     - However, if polling frequency of custom metrics is higher than that of regular stats, containerStats can be uneven.\nDifferent timeStore\n     - Handling of data(to display in API and UI) may be easier\n     - Extra overhead\nI can start working on a different timestore if we have something along these lines :\ntype MetricSpec struct {\n     name string,\n     type string,\n     labels map[string][string],\n     metricsInt []MetricInt,\n     metricsFloat []MetricFloat\n}\ntype MetricInt struct {\n      timestamp time.Time,\n      intValue int\n}\n. @rjnagal Let me know if any changes need to be done. Thanks!\n. Squashed. :)\n. Some issues need to be addressed here though \n- The maximum size of IntPoints/FloatPoints that metrics can hold.\n- How collector manager can gather metrics since Collect() method would return historic data too.\n. Will reopen once #852 is finalized.\n. @rjnagal , Squashed.\n. There was a name collision between the Collector struct and the Collector interface(temporarily renamed as CollectorInterface). Once we finalize on the names for these, we can make the necessary changes in all places(eg manager.go uses Collector as an interface) and put back the test.\n. Done. :)\n. This can be skipped if we want to continue collection of other valid metrics.\n. Do we need to re-calculate nextCollectionTime here?\n. Used this approach in #810\n. My concern was the way in which the metric would be expressed : int, integer, int32, number ... \nBut yes, I think this is a better approach. We can use enum for metric units if necessary\n. The idea was to allow collection of other valid metrics by creating the constructor even if a few regexps errored. Can we can remove the entire metric whose regex is invalid and proceed with creation of a collector if there is atleast one valid metric?\n. I see the point It might be  unclear to the user how only a few metrics have been collected. Thanks @rjnagal @vmarmol . Will fix it.\n. This condition should not have been here! Will remove it.\nThe intent was to  enable collection of metrics within 'housekeepingTick' so that the housekeeping time includes the time taken for metric collection.\n. Do we need to retain the metric collection here, @rjnagal , @vmarmol ? Only issue is that the metric collection time will not be included in housekeeping time in this case. \nHowever, the current approach of pushing metric collection to updateStats seems more convoluted.\n. Added the change. :)\n. Just a small query : Can \nerr = cont.collectorManager.RegisterCollector(newCollector)\nbe used instead of passing cm as an argument ?\n. Addition of customMetrics to ResourcesAvailable is probably not necessary, as cpu, memory, network, fs are the standard metrics which need to be present.\n. Assumption: PollingFrequency is same for all custom metrics which leads to custom_metrics.length being same in all stats corresponding to different timestamps.\n. Do we need to add the following after line 780 ?\nif collectorConfigs != nil {                                     //err != nil check for m.registerCollectors is already present\n        contSpec.HasCustomMetrics = true\n}\n. Oh, right! The spec gets re-written. Thanks for poiniting it out! I shall close the PR. \n. There is redundancy here as we store metrics as part of the Collector struct as well as return it in the Collect() method.\n. This function needs to be re-written when we have different labels for one metric. Maybe along with the prometheus PR.\n@rjnagal , @vmarmol \n. ",
    "konukhov": "Hi @rjnagal\nThanks for the fast feedback! \nI just pushed updated documentation + gofmt'd my code. I also added some TCP connection recovery logic to the Riemann client in order to make events delivery more reliable.\n. Thanks for the update, @rjnagal!\nI'm quite busy this week too, so I'll try to provide containers/configs for testing by the end of this week.\n. @szubster Hi!\nSorry I've been quite busy lately, so I haven't got time to fix/provide things @rjnagal asked for. I'll try to finish this in a week or so.\n. ",
    "szubster": "Any news on this?\n. ",
    "edannenberg": ":+1: \n. If this is on hold because of a missing test setup via docker-compose I can put something together. Would really like to see this merged.\n. nvm, there is already a PR for this: https://github.com/google/cadvisor/pull/800\n. Is Go 1.7 support in yet? I remember it was planned for v0.24. Distributions like Gentoo don't have Go 1.6 in their package repository anymore.. ",
    "jfoy": "FYI https://github.com/kubernetes/heapster/pull/379\n. ",
    "GlobalHelp": "I signed it!\n. Hi,I think there are other contributing French who can update the documents but it is true that the French version will have a few days/weeks late.\n. OK.I understand\n. ",
    "ynnt": "I see other services now but it's ok\nOh, sorry, I figured out that I didn't fill issue text.\nMy primary question is why do I see null values about docker containers.\n. 127.0.0.1:8080/metrics\nOn Jun 23, 2015 10:16 PM, \"Victor Marmol\" notifications@github.com wrote:\n\n@a-pastushenko https://github.com/a-pastushenko where do you see these\nnull values?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/779#issuecomment-114614767.\n. Also in the cAdvisor web page there are 0 values for RAM usage\nOn Jun 23, 2015 10:16 PM, \"Victor Marmol\" notifications@github.com wrote:\n@a-pastushenko https://github.com/a-pastushenko where do you see these\nnull values?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/779#issuecomment-114614767.\n. http://pastebin.com/raw.php?i=TGjFCX5a\n. \n",
    "Marmelatze": "I have the same use case as the guy in #546. I have some Mesos/Marathon hosts and Mesos assigns some random id as container name (I expect Mesos/kubernetes and Mesos/Swarm to do the same).\nIn my dashboard (cadvisor on every host -> prometheus -> grafana) I can see that a container uses 100% CPU but I only can see the host its running on and the name (mesos-1234-5678...). To determine what application this container is running I have to SSH into that host and inspect the container. There I can see the mesos task-id, the marathon app name and version.\nThe reverse way is also time consuming: I have more than one instance of this app running and I want to know how these are performing (CPU, RAM, Network). So I have to check Marathon or Mesos to get a list of hosts, that app is deployed to. Than I have to ssh into these. List all containers and look which could be my app. Inspect these to be sure. Go back to the dashboard and filter for the names.\nI know some sensitive data, like passwords and access tokens, are stored in env variables, so I made this optional and the exposed variables configurable. \nFor Mesos/Marathon it would be:\n--docker_metadata_env=MESOS_TASK_ID,MARATHON_APP_ID,MARATHON_APP_VERSION\n. Thanks for the feedback. With all container labels exported as prometheus labels it would also fix #688\n. Rebased to current master and tests seem to pass\n. OK thanks, but unfortunately mesos doesn't seem to set labels or something else. Just the env variables.\n. Rebased to current master and removed the image name parts.\n. ",
    "gtmtech": "+1 I have exactly the same usecase (mesos, classifying containers by appname for aggregation and averaging reasons across a cluster). Hopefully this can be merged ASAP\n. ",
    "jzaefferer": "@Marmelatze could you rebase this to remove the changes already introduced by #869? Unless you've found an alternative solution by now; if so, could you share?\n@rjnagal was the answer to your question satisfying? We've got the same usecase as @Marmelatze and would like to see this addressed.\n. I actually tried rebasing myself, but with my very limited understanding of Go, I couldn't figure out how to resolve the conflicts in metrics/prometheus.go.\n. The commit message still mentions the image, but that should be trivial to fix when landing this.\n. ",
    "MikeMichel": "+1 for rebase\n. @Marmelatze awesome, works perfect.\nthx\n. yes https://hub.docker.com/r/mikemichel/cadvisor/tags/\ndocker run --name cadvisor --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys\n:ro --volume=/mnt/docker/:/var/lib/docker:ro --publish=8080:8080 --detach=true   mikemichel/cadvisor:metaenvs --docker_metadata_env=MESOS_TASK_ID,MARATHO\nN_APP_ID,MARATHON_APP_VERSION\n. @jimmidyson i'm using a patched < 0.20.4 version because i need https://github.com/google/cadvisor/issues/1078 working before i can move to a newer official version\n. ",
    "koboltmarky": "+1 for rebase\n. ",
    "dqminh": "@vishh @Marmelatze  im also interested to see this and ability to specify container labels as tags. Let me know if you want some help with this ( rebase / take over etc. )\n. @vishh @rjnagal @jimmidyson  i like to use this patch. Should i take over and do a rebase on behalf of @Marmelatze ?\n. Not only rss but i think that all statistics from memory.stat should also be exposed. They are very helpful in many cases.\n@jimmidyson @vishh wdyt ? Do we worry about exposing too many metrics by default without knobs for turning them off ?\n. >  As long as the cost for acquiring these metrics isn't high, I think adding them is fine by me.\nRight, then i think its fine. We already have them in memory metrics, we just need to expose them to the world.\n. I would be fine with exposing all of them.\n. @vishh  i've checked our CLAs and i think we are ok here. Let me know what you think.\n. ping @vishh \n. @vishh added references to API v2 and changed the flag.\nI'm keeping the env whitelist i.e. not collecting all env variables by default. PTAL.\n. @vishh updated\n. @vishh squashed the changes to envs into 1 commits.\n. Makes sense. Its sad that there isnt a nice way to auto-detect this though\n. It isnt, but i think it should. There's already a flag to choose the cgroupfs driver explicitly so it makes sense to expose it as the daemon's info. Meanwhile i will close this since we have a workaround.\n. confirmed that CLA should be in place.\nping @vishh \n. > The email you used does not show up in the CLA list :( ?\ni signed with a company email. I asked legal and they said the CLA should be in place :(\n. hmm is it about cadvisor needs to run with nosystemd when using the cgroupfs ( the default ) https://github.com/google/cadvisor/issues/1062 ?\n. @dashpole we already have ondemand housekeeping, wouldnt a reasonable quick workaround/compromise to alleviate CPU issues be that:\n\nuser need to increase the housekeeping interval\nadd a plug into prometheus endpoint to start ondemand housekeeping when collecting metrics. ( perhaps with optional maxAge so that if we happen to land closely on a housekeeping tick, dont perform additional work ). SGTM, i will take a look at this.. ping @dashpole . hmm in the end they all come back as metrics labels so i dont really think they are necessary.\nLabels makes a lot of sense for metrics, env much less so. We are just stuck with env here because some orchestration platform doesnt support writing labels to docker container.\n. agreed. Updated it to add Envs as a source of metadata to ContainerSpec. PTAL\n. main reason was noted here: https://github.com/google/cadvisor/pull/78\n\ntldr: the orchestrator doesnt understand docker labels, and set their metadata as env variable. Until that's fixed, we need this to make sense of those containers.\n. this is to select the env that we want to publish as metrics labels ( I made a similar flag for logging in docker https://github.com/docker/docker/pull/16961 )\nFor example, a mesos container has the following configs:\n\"Env\": [\n            \"GOMAXPROCS=64\",\n            \"MARATHON_APP_VERSION=2015-11-18T22:55:53.442Z\",\n            \"MARATHON_APP_DOCKER_IMAGE=docker/service\",\n            \"MESOS_TASK_ID=service.3f90dfce-12314235\",\n            \"MARATHON_APP_ID=/service\",\n            \"MESOS_SANDBOX=/mnt/mesos/sandbox\",\n            \"MESOS_CONTAINER_NAME=mesos-asdsfdfds\",\n            \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n            \"container=docker\"\n],\n\"Labels\": {}, // sad :(\nwe only interest in MARATHON_APP_ID,MESOS_TASK_ID,MESOS_CONTAINER_NAME as the container identifiers.\nA flag is reasonably useful for use as we only use 1 orchestrator i.e. mesos, so all containers will have the same set of env keys. This is the chosen approach in the original PR.\nI can imagine an alternative approach would be to let the administrator defined their desired env keys as a well known label, for example:\n// this will not be set as metrics in prometheus\n\"Labels\": { \"cadvisor.metadata_env\": \"MARATHON_APP_ID,MESOS_TASK_ID,MESOS_CONTAINER_NAME\" }\n. Label names may contain ASCII letters, numbers, as well as underscores.\nFrom: http://prometheus.io/docs/concepts/data_model/#metric-names-and-labels\nI think this makes sense as the default. The other option is to ignore the invalid labels.\n. SGTM. What do you think of\n-allowed-docker-env-as-metadata: a comma-separated list of allowed environment variable names for docker container that should have their name values set as the container's metadata\n. --docker-env-metadata-whitelist: a comma-separated list of environment variable keys that needs to be collected for docker containers\n:+1: \nHmm, i thinkcollecting all env variables as the default is debatable.\nIn term of metrics collection, many env variables are strictly for runtime usage and doesn't make much sense in term of metrics which is why the whitelist approach ( i.e. disabled by default ) feels right. I think enabling all env variables by default will likely bite some users in their foot if they are not careful.\nHowever, if we are talking about collecting information of container for exposing to others i.e. cadvisor is the only place to discover containers' information on the system then collecting all env by default makes sense.\n. I would like to also see at least cache exposed.\n. can we return an error here if the user specified an invalid value ?\n. Sorry for my own delay :cry: !\nYes it does. In general i think its not costly, but i think we can also just initialize the collector outside of the handler, register with the registry, and then customize the option ( perhaps with new exported option like func (c *PrometheusCollector) SetRequestOptions(opt) before running promhttp.HandlerFor which triggers Collect for each registered collector.. ",
    "deedubs": "Is anyone running this yet?\n. @rjnagal feels like the security concerns are minimized by only exporting whitelisted ENV vars no?\n. ",
    "r0fls": "\nFixed by #780.\n\nWell hey now, that's a circular reference! \ud83d\ude04  Do you mean #1023 ?. ",
    "discostur": "Yes, running the latest version, here is the output of /validate:\n```\ncAdvisor version: 0.15.1\nOS version: CentOS Linux 7 (Core)\nKernel version: [Supported and recommended]\n    Kernel version is 3.10.0-229.4.2.el7.x86_64. Versions >= 2.6 are supported. 3.0+ are recommended.\nCgroup setup: [Supported and recommended]\n    Available cgroups: map[memory:1 freezer:1 blkio:1 perf_event:1 cpuset:1 cpu:1 cpuacct:1 devices:1 net_cls:1 hugetlb:1]\n    Following cgroups are required: [cpu cpuacct]\n    Following other cgroups are recommended: [memory blkio cpuset devices freezer]\n    Hierarchical memory accounting enabled. Reported memory usage includes memory used by child containers.\nCgroup mount setup: [Supported and recommended]\n    Cgroups are mounted at /sys/fs/cgroup.\n    Cgroup mount directories: blkio cpu cpu,cpuacct cpuacct cpuset devices freezer hugetlb memory net_cls perf_event systemd \n    Any cgroup mount point that is detectible and accessible is supported. /sys/fs/cgroup is recommended as a standard location.\n    Cgroup mounts:\n    cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpuacct,cpu 0 0\n    cgroup /sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0\n    cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /sys/fs/cgroup/net_cls cgroup rw,nosuid,nodev,noexec,relatime,net_cls 0 0\n    cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0\nDocker version: [Unknown]\n    Could not parse docker version. Docker version is Unknown. Versions >= 1.0 are supported. 1.2+ are recommended.\nDocker driver setup: [Unknown]\n    Docker remote API not reachable\nBlock device setup: [Supported, but not recommended]\n    None of the devices support 'cfq' I/O scheduler. No disk stats can be reported.\n     Disk \"zram1\" Scheduler type \"none\".\n     Disk \"zram2\" Scheduler type \"none\".\n     Disk \"zram3\" Scheduler type \"none\".\n     Disk \"sda\" Scheduler type \"deadline\".\n     Disk \"sdb\" Scheduler type \"noop\".\n     Disk \"sdc\" Scheduler type \"noop\".\n     Disk \"zram0\" Scheduler type \"none\".\nManaged containers: \n    /machine.slice/machine-lxc\\x2dISACfa31fad88f5ca2c11b67d6ba4ab661ag.scope\n    /system.slice\n    /user.slice\n    /machine.slice/machine-lxc\\x2dISACfa31fad88f5ca2c11b67d6ba4ab661af.scope\n    /machine.slice\n    /\nInotify watches: \n    /machine.slice:\n        /sys/fs/cgroup/cpuset/machine.slice\n        /sys/fs/cgroup/memory/machine.slice\n        /sys/fs/cgroup/blkio/machine.slice\n        /sys/fs/cgroup/cpu,cpuacct/machine.slice\n    /user.slice:\n        /sys/fs/cgroup/cpu,cpuacct/user.slice\n        /sys/fs/cgroup/blkio/user.slice\n    /machine.slice/machine-lxc\\x2dISACfa31fad88f5ca2c11b67d6ba4ab661ag.scope:\n        /sys/fs/cgroup/cpu,cpuacct/machine.slice/machine-lxc\\x2dISACfa31fad88f5ca2c11b67d6ba4ab661ag.scope\n        /sys/fs/cgroup/cpuset/machine.slice/machine-lxc\\x2dISACfa31fad88f5ca2c11b67d6ba4ab661ag.scope\n        /sys/fs/cgroup/memory/machine.slice/machine-lxc\\x2dISACfa31fad88f5ca2c11b67d6ba4ab661ag.scope\n        /sys/fs/cgroup/blkio/machine.slice/machine-lxc\\x2dISACfa31fad88f5ca2c11b67d6ba4ab661ag.scope\n    /machine.slice/machine-lxc\\x2dISACfa31fad88f5ca2c11b67d6ba4ab661af.scope:\n        /sys/fs/cgroup/cpu,cpuacct/machine.slice/machine-lxc\\x2dISACfa31fad88f5ca2c11b67d6ba4ab661af.scope\n        /sys/fs/cgroup/cpuset/machine.slice/machine-lxc\\x2dISACfa31fad88f5ca2c11b67d6ba4ab661af.scope\n        /sys/fs/cgroup/memory/machine.slice/machine-lxc\\x2dISACfa31fad88f5ca2c11b67d6ba4ab661af.scope\n        /sys/fs/cgroup/blkio/machine.slice/machine-lxc\\x2dISACfa31fad88f5ca2c11b67d6ba4ab661af.scope\n    /system.slice:\n        /sys/fs/cgroup/cpu,cpuacct/system.slice\n        /sys/fs/cgroup/blkio/system.slice\n       /:\n        /sys/fs/cgroup/memory\n        /sys/fs/cgroup/blkio\n        /sys/fs/cgroup/cpu,cpuacct\n        /sys/fs/cgroup/cpuset\n```\n. In the Javascript-Console i the following error (repeating each second):\n[Error] Failed to load resource: the server responded with a status of 500 (Internal Server Error) (x2dISACfa31fad88f5ca2c11b67d6ba4ab661ag.scope, line 0)\nfailed to get container \"/machine.slice/machine-lxc/x2dISACfa31fad88f5ca2c11b67d6ba4ab661ag.scope\" with error: unknown container \"/machine.slice/machine-lxc/x2dISACfa31fad88f5ca2c11b67d6ba4ab661ag.scope\"\nIs there an error within the path? I think it should be machine-lxc\\\\x2d... instead of  machine-lxc/x2d ?\nHere is the output of /api/v1.3...:\n{\"name\":\"/machine.slice/machine-lxc\\\\x2dISACfa31fad88f5ca2c11b67d6ba4ab661ag.scope\",\"spec\":{\"creation_time\":\"2015-06-26T16:18:03.681326357+02:00\",\"has_cpu\":true,\"cpu\":{\"limit\":1024,\"max_limit\":0,\"mask\":\"0-3\"},\"has_memory\":true,\"memory\":{\"limit\":8192000000,\"swap_limit\":10240000000},\"has_network\":false,\"has_filesystem\":false,\"has_diskio\":true},\"stats\":[{\"timestamp\":\"2015-06-26T16:23:17.685000799+02:00\",\"cpu\":{\"usage\":{\"total\":836406546,\"per_cpu_usage\":[195997452,224456541,176927699,239024854],\"user\":280000000,\"system\":510000000},\"load_average\":0},\"diskio\":{\"io_service_bytes\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":10493952,\"Read\":10493952,\"Sync\":0,\"Total\":10493952,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":8966144,\"Read\":8966144,\"Sync\":0,\"Total\":8966144,\"Write\":0}}],\"io_serviced\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":449,\"Read\":449,\"Sync\":0,\"Total\":449,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":384,\"Read\":384,\"Sync\":0,\"Total\":384,\"Write\":0}}]},\"memory\":{\"usage\":51695616,\"working_set\":23834624,\"container_data\":{\"pgfault\":100872,\"pgmajfault\":288},\"hierarchical_data\":{\"pgfault\":100872,\"pgmajfault\":288}},\"network\":{\"name\":\"\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},\"task_stats\":{\"nr_sleeping\":0,\"nr_running\":0,\"nr_stopped\":0,\"nr_uninterruptible\":0,\"nr_io_wait\":0}},{\"timestamp\":\"2015-06-26T16:23:18.68491176+02:00\",\"cpu\":{\"usage\":{\"total\":836406546,\"per_cpu_usage\":[195997452,224456541,176927699,239024854],\"user\":280000000,\"system\":510000000},\"load_average\":0},\"diskio\":{\"io_service_bytes\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":10493952,\"Read\":10493952,\"Sync\":0,\"Total\":10493952,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":8966144,\"Read\":8966144,\"Sync\":0,\"Total\":8966144,\"Write\":0}}],\"io_serviced\":[{\"major\":8,\"minor\":32,\"stats\":{\"Async\":384,\"Read\":384,\"Sync\":0,\"Total\":384,\"Write\":0}},{\"major\":8,\"minor\":16,\"stats\":{\"Async\":449,\"Read\":449,\"Sync\":0,\"Total\":449,\"Write\":0}}]},\"memory\":{\"usage\":51695616,\"working_set\":23834624,\"container_data\":{\"pgfault\":100872,\"pgmajfault\":288},\"hierarchical_data\":{\"pgfault\":100872,\"pgmajfault\":288}},\"network\":{\"name\":\"\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},\"task_stats\":{\"nr_sleeping\":0,\"nr_running\":0,\"nr_stopped\":0,\"nr_uninterruptible\":0,\"nr_io_wait\":0}},{\"timestamp\":\"2015-06-26T16:23:19.684884906+02:00\",\"cpu\":{\"usage\":{\"total\":836406546,\"per_cpu_usage\":[195997452,224456541,176927699,239024854],\"user\":280000000,\"system\":510000000},\"load_average\":0},\"diskio\":{\"io_service_bytes\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":10493952,\"Read\":10493952,\"Sync\":0,\"Total\":10493952,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":8966144,\"Read\":8966144,\"Sync\":0,\"Total\":8966144,\"Write\":0}}],\"io_serviced\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":449,\"Read\":449,\"Sync\":0,\"Total\":449,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":384,\"Read\":384,\"Sync\":0,\"Total\":384,\"Write\":0}}]},\"memory\":{\"usage\":51695616,\"working_set\":23834624,\"container_data\":{\"pgfault\":100872,\"pgmajfault\":288},\"hierarchical_data\":{\"pgfault\":100872,\"pgmajfault\":288}},\"network\":{\"name\":\"\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},\"task_stats\":{\"nr_sleeping\":0,\"nr_running\":0,\"nr_stopped\":0,\"nr_uninterruptible\":0,\"nr_io_wait\":0}},{\"timestamp\":\"2015-06-26T16:23:20.684894832+02:00\",\"cpu\":{\"usage\":{\"total\":836406546,\"per_cpu_usage\":[195997452,224456541,176927699,239024854],\"user\":280000000,\"system\":510000000},\"load_average\":0},\"diskio\":{\"io_service_bytes\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":10493952,\"Read\":10493952,\"Sync\":0,\"Total\":10493952,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":8966144,\"Read\":8966144,\"Sync\":0,\"Total\":8966144,\"Write\":0}}],\"io_serviced\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":449,\"Read\":449,\"Sync\":0,\"Total\":449,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":384,\"Read\":384,\"Sync\":0,\"Total\":384,\"Write\":0}}]},\"memory\":{\"usage\":51695616,\"working_set\":23834624,\"container_data\":{\"pgfault\":100872,\"pgmajfault\":288},\"hierarchical_data\":{\"pgfault\":100872,\"pgmajfault\":288}},\"network\":{\"name\":\"\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},\"task_stats\":{\"nr_sleeping\":0,\"nr_running\":0,\"nr_stopped\":0,\"nr_uninterruptible\":0,\"nr_io_wait\":0}},{\"timestamp\":\"2015-06-26T16:23:22.684996159+02:00\",\"cpu\":{\"usage\":{\"total\":836406546,\"per_cpu_usage\":[195997452,224456541,176927699,239024854],\"user\":280000000,\"system\":510000000},\"load_average\":0},\"diskio\":{\"io_service_bytes\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":10493952,\"Read\":10493952,\"Sync\":0,\"Total\":10493952,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":8966144,\"Read\":8966144,\"Sync\":0,\"Total\":8966144,\"Write\":0}}],\"io_serviced\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":449,\"Read\":449,\"Sync\":0,\"Total\":449,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":384,\"Read\":384,\"Sync\":0,\"Total\":384,\"Write\":0}}]},\"memory\":{\"usage\":51695616,\"working_set\":23834624,\"container_data\":{\"pgfault\":100872,\"pgmajfault\":288},\"hierarchical_data\":{\"pgfault\":100872,\"pgmajfault\":288}},\"network\":{\"name\":\"\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},\"task_stats\":{\"nr_sleeping\":0,\"nr_running\":0,\"nr_stopped\":0,\"nr_uninterruptible\":0,\"nr_io_wait\":0}},{\"timestamp\":\"2015-06-26T16:23:26.684999633+02:00\",\"cpu\":{\"usage\":{\"total\":836406546,\"per_cpu_usage\":[195997452,224456541,176927699,239024854],\"user\":280000000,\"system\":510000000},\"load_average\":0},\"diskio\":{\"io_service_bytes\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":10493952,\"Read\":10493952,\"Sync\":0,\"Total\":10493952,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":8966144,\"Read\":8966144,\"Sync\":0,\"Total\":8966144,\"Write\":0}}],\"io_serviced\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":449,\"Read\":449,\"Sync\":0,\"Total\":449,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":384,\"Read\":384,\"Sync\":0,\"Total\":384,\"Write\":0}}]},\"memory\":{\"usage\":51695616,\"working_set\":23834624,\"container_data\":{\"pgfault\":100872,\"pgmajfault\":288},\"hierarchical_data\":{\"pgfault\":100872,\"pgmajfault\":288}},\"network\":{\"name\":\"\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},\"task_stats\":{\"nr_sleeping\":0,\"nr_running\":0,\"nr_stopped\":0,\"nr_uninterruptible\":0,\"nr_io_wait\":0}},{\"timestamp\":\"2015-06-26T16:23:34.684966225+02:00\",\"cpu\":{\"usage\":{\"total\":836406546,\"per_cpu_usage\":[195997452,224456541,176927699,239024854],\"user\":280000000,\"system\":510000000},\"load_average\":0},\"diskio\":{\"io_service_bytes\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":10493952,\"Read\":10493952,\"Sync\":0,\"Total\":10493952,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":8966144,\"Read\":8966144,\"Sync\":0,\"Total\":8966144,\"Write\":0}}],\"io_serviced\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":449,\"Read\":449,\"Sync\":0,\"Total\":449,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":384,\"Read\":384,\"Sync\":0,\"Total\":384,\"Write\":0}}]},\"memory\":{\"usage\":51695616,\"working_set\":23834624,\"container_data\":{\"pgfault\":100872,\"pgmajfault\":288},\"hierarchical_data\":{\"pgfault\":100872,\"pgmajfault\":288}},\"network\":{\"name\":\"\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},\"task_stats\":{\"nr_sleeping\":0,\"nr_running\":0,\"nr_stopped\":0,\"nr_uninterruptible\":0,\"nr_io_wait\":0}},{\"timestamp\":\"2015-06-26T16:23:50.68494986+02:00\",\"cpu\":{\"usage\":{\"total\":836406546,\"per_cpu_usage\":[195997452,224456541,176927699,239024854],\"user\":280000000,\"system\":510000000},\"load_average\":0},\"diskio\":{\"io_service_bytes\":[{\"major\":8,\"minor\":32,\"stats\":{\"Async\":8966144,\"Read\":8966144,\"Sync\":0,\"Total\":8966144,\"Write\":0}},{\"major\":8,\"minor\":16,\"stats\":{\"Async\":10493952,\"Read\":10493952,\"Sync\":0,\"Total\":10493952,\"Write\":0}}],\"io_serviced\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":449,\"Read\":449,\"Sync\":0,\"Total\":449,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":384,\"Read\":384,\"Sync\":0,\"Total\":384,\"Write\":0}}]},\"memory\":{\"usage\":51695616,\"working_set\":23834624,\"container_data\":{\"pgfault\":100872,\"pgmajfault\":288},\"hierarchical_data\":{\"pgfault\":100872,\"pgmajfault\":288}},\"network\":{\"name\":\"\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},\"task_stats\":{\"nr_sleeping\":0,\"nr_running\":0,\"nr_stopped\":0,\"nr_uninterruptible\":0,\"nr_io_wait\":0}},{\"timestamp\":\"2015-06-26T16:23:51.684940338+02:00\",\"cpu\":{\"usage\":{\"total\":836406546,\"per_cpu_usage\":[195997452,224456541,176927699,239024854],\"user\":280000000,\"system\":510000000},\"load_average\":0},\"diskio\":{\"io_service_bytes\":[{\"major\":8,\"minor\":32,\"stats\":{\"Async\":8966144,\"Read\":8966144,\"Sync\":0,\"Total\":8966144,\"Write\":0}},{\"major\":8,\"minor\":16,\"stats\":{\"Async\":10493952,\"Read\":10493952,\"Sync\":0,\"Total\":10493952,\"Write\":0}}],\"io_serviced\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":449,\"Read\":449,\"Sync\":0,\"Total\":449,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":384,\"Read\":384,\"Sync\":0,\"Total\":384,\"Write\":0}}]},\"memory\":{\"usage\":51695616,\"working_set\":23834624,\"container_data\":{\"pgfault\":100872,\"pgmajfault\":288},\"hierarchical_data\":{\"pgfault\":100872,\"pgmajfault\":288}},\"network\":{\"name\":\"\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},\"task_stats\":{\"nr_sleeping\":0,\"nr_running\":0,\"nr_stopped\":0,\"nr_uninterruptible\":0,\"nr_io_wait\":0}},{\"timestamp\":\"2015-06-26T16:23:53.685019705+02:00\",\"cpu\":{\"usage\":{\"total\":836406546,\"per_cpu_usage\":[195997452,224456541,176927699,239024854],\"user\":280000000,\"system\":510000000},\"load_average\":0},\"diskio\":{\"io_service_bytes\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":10493952,\"Read\":10493952,\"Sync\":0,\"Total\":10493952,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":8966144,\"Read\":8966144,\"Sync\":0,\"Total\":8966144,\"Write\":0}}],\"io_serviced\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":449,\"Read\":449,\"Sync\":0,\"Total\":449,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":384,\"Read\":384,\"Sync\":0,\"Total\":384,\"Write\":0}}]},\"memory\":{\"usage\":51695616,\"working_set\":23834624,\"container_data\":{\"pgfault\":100872,\"pgmajfault\":288},\"hierarchical_data\":{\"pgfault\":100872,\"pgmajfault\":288}},\"network\":{\"name\":\"\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},\"task_stats\":{\"nr_sleeping\":0,\"nr_running\":0,\"nr_stopped\":0,\"nr_uninterruptible\":0,\"nr_io_wait\":0}},{\"timestamp\":\"2015-06-26T16:23:54.684954881+02:00\",\"cpu\":{\"usage\":{\"total\":836406546,\"per_cpu_usage\":[195997452,224456541,176927699,239024854],\"user\":280000000,\"system\":510000000},\"load_average\":0},\"diskio\":{\"io_service_bytes\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":10493952,\"Read\":10493952,\"Sync\":0,\"Total\":10493952,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":8966144,\"Read\":8966144,\"Sync\":0,\"Total\":8966144,\"Write\":0}}],\"io_serviced\":[{\"major\":8,\"minor\":32,\"stats\":{\"Async\":384,\"Read\":384,\"Sync\":0,\"Total\":384,\"Write\":0}},{\"major\":8,\"minor\":16,\"stats\":{\"Async\":449,\"Read\":449,\"Sync\":0,\"Total\":449,\"Write\":0}}]},\"memory\":{\"usage\":51695616,\"working_set\":23834624,\"container_data\":{\"pgfault\":100872,\"pgmajfault\":288},\"hierarchical_data\":{\"pgfault\":100872,\"pgmajfault\":288}},\"network\":{\"name\":\"\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},\"task_stats\":{\"nr_sleeping\":0,\"nr_running\":0,\"nr_stopped\":0,\"nr_uninterruptible\":0,\"nr_io_wait\":0}},{\"timestamp\":\"2015-06-26T16:23:55.684887395+02:00\",\"cpu\":{\"usage\":{\"total\":836406546,\"per_cpu_usage\":[195997452,224456541,176927699,239024854],\"user\":280000000,\"system\":510000000},\"load_average\":0},\"diskio\":{\"io_service_bytes\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":10493952,\"Read\":10493952,\"Sync\":0,\"Total\":10493952,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":8966144,\"Read\":8966144,\"Sync\":0,\"Total\":8966144,\"Write\":0}}],\"io_serviced\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":449,\"Read\":449,\"Sync\":0,\"Total\":449,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":384,\"Read\":384,\"Sync\":0,\"Total\":384,\"Write\":0}}]},\"memory\":{\"usage\":51695616,\"working_set\":23834624,\"container_data\":{\"pgfault\":100872,\"pgmajfault\":288},\"hierarchical_data\":{\"pgfault\":100872,\"pgmajfault\":288}},\"network\":{\"name\":\"\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},\"task_stats\":{\"nr_sleeping\":0,\"nr_running\":0,\"nr_stopped\":0,\"nr_uninterruptible\":0,\"nr_io_wait\":0}},{\"timestamp\":\"2015-06-26T16:23:56.684960902+02:00\",\"cpu\":{\"usage\":{\"total\":836406546,\"per_cpu_usage\":[195997452,224456541,176927699,239024854],\"user\":280000000,\"system\":510000000},\"load_average\":0},\"diskio\":{\"io_service_bytes\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":10493952,\"Read\":10493952,\"Sync\":0,\"Total\":10493952,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":8966144,\"Read\":8966144,\"Sync\":0,\"Total\":8966144,\"Write\":0}}],\"io_serviced\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":449,\"Read\":449,\"Sync\":0,\"Total\":449,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":384,\"Read\":384,\"Sync\":0,\"Total\":384,\"Write\":0}}]},\"memory\":{\"usage\":51695616,\"working_set\":23834624,\"container_data\":{\"pgfault\":100872,\"pgmajfault\":288},\"hierarchical_data\":{\"pgfault\":100872,\"pgmajfault\":288}},\"network\":{\"name\":\"\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},\"task_stats\":{\"nr_sleeping\":0,\"nr_running\":0,\"nr_stopped\":0,\"nr_uninterruptible\":0,\"nr_io_wait\":0}},{\"timestamp\":\"2015-06-26T16:23:58.684909636+02:00\",\"cpu\":{\"usage\":{\"total\":836406546,\"per_cpu_usage\":[195997452,224456541,176927699,239024854],\"user\":280000000,\"system\":510000000},\"load_average\":0},\"diskio\":{\"io_service_bytes\":[{\"major\":8,\"minor\":32,\"stats\":{\"Async\":8966144,\"Read\":8966144,\"Sync\":0,\"Total\":8966144,\"Write\":0}},{\"major\":8,\"minor\":16,\"stats\":{\"Async\":10493952,\"Read\":10493952,\"Sync\":0,\"Total\":10493952,\"Write\":0}}],\"io_serviced\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":449,\"Read\":449,\"Sync\":0,\"Total\":449,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":384,\"Read\":384,\"Sync\":0,\"Total\":384,\"Write\":0}}]},\"memory\":{\"usage\":51695616,\"working_set\":23834624,\"container_data\":{\"pgfault\":100872,\"pgmajfault\":288},\"hierarchical_data\":{\"pgfault\":100872,\"pgmajfault\":288}},\"network\":{\"name\":\"\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},\"task_stats\":{\"nr_sleeping\":0,\"nr_running\":0,\"nr_stopped\":0,\"nr_uninterruptible\":0,\"nr_io_wait\":0}},{\"timestamp\":\"2015-06-26T16:23:59.684873595+02:00\",\"cpu\":{\"usage\":{\"total\":836406546,\"per_cpu_usage\":[195997452,224456541,176927699,239024854],\"user\":280000000,\"system\":510000000},\"load_average\":0},\"diskio\":{\"io_service_bytes\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":10493952,\"Read\":10493952,\"Sync\":0,\"Total\":10493952,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":8966144,\"Read\":8966144,\"Sync\":0,\"Total\":8966144,\"Write\":0}}],\"io_serviced\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":449,\"Read\":449,\"Sync\":0,\"Total\":449,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":384,\"Read\":384,\"Sync\":0,\"Total\":384,\"Write\":0}}]},\"memory\":{\"usage\":51695616,\"working_set\":23834624,\"container_data\":{\"pgfault\":100872,\"pgmajfault\":288},\"hierarchical_data\":{\"pgfault\":100872,\"pgmajfault\":288}},\"network\":{\"name\":\"\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},\"task_stats\":{\"nr_sleeping\":0,\"nr_running\":0,\"nr_stopped\":0,\"nr_uninterruptible\":0,\"nr_io_wait\":0}},{\"timestamp\":\"2015-06-26T16:24:00.684911014+02:00\",\"cpu\":{\"usage\":{\"total\":836406546,\"per_cpu_usage\":[195997452,224456541,176927699,239024854],\"user\":280000000,\"system\":510000000},\"load_average\":0},\"diskio\":{\"io_service_bytes\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":10493952,\"Read\":10493952,\"Sync\":0,\"Total\":10493952,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":8966144,\"Read\":8966144,\"Sync\":0,\"Total\":8966144,\"Write\":0}}],\"io_serviced\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":449,\"Read\":449,\"Sync\":0,\"Total\":449,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":384,\"Read\":384,\"Sync\":0,\"Total\":384,\"Write\":0}}]},\"memory\":{\"usage\":51695616,\"working_set\":23834624,\"container_data\":{\"pgfault\":100872,\"pgmajfault\":288},\"hierarchical_data\":{\"pgfault\":100872,\"pgmajfault\":288}},\"network\":{\"name\":\"\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},\"task_stats\":{\"nr_sleeping\":0,\"nr_running\":0,\"nr_stopped\":0,\"nr_uninterruptible\":0,\"nr_io_wait\":0}},{\"timestamp\":\"2015-06-26T16:24:02.68490311+02:00\",\"cpu\":{\"usage\":{\"total\":836406546,\"per_cpu_usage\":[195997452,224456541,176927699,239024854],\"user\":280000000,\"system\":510000000},\"load_average\":0},\"diskio\":{\"io_service_bytes\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":10493952,\"Read\":10493952,\"Sync\":0,\"Total\":10493952,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":8966144,\"Read\":8966144,\"Sync\":0,\"Total\":8966144,\"Write\":0}}],\"io_serviced\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":449,\"Read\":449,\"Sync\":0,\"Total\":449,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":384,\"Read\":384,\"Sync\":0,\"Total\":384,\"Write\":0}}]},\"memory\":{\"usage\":51695616,\"working_set\":23834624,\"container_data\":{\"pgfault\":100872,\"pgmajfault\":288},\"hierarchical_data\":{\"pgfault\":100872,\"pgmajfault\":288}},\"network\":{\"name\":\"\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},\"task_stats\":{\"nr_sleeping\":0,\"nr_running\":0,\"nr_stopped\":0,\"nr_uninterruptible\":0,\"nr_io_wait\":0}},{\"timestamp\":\"2015-06-26T16:24:06.684939938+02:00\",\"cpu\":{\"usage\":{\"total\":836406546,\"per_cpu_usage\":[195997452,224456541,176927699,239024854],\"user\":280000000,\"system\":510000000},\"load_average\":0},\"diskio\":{\"io_service_bytes\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":10493952,\"Read\":10493952,\"Sync\":0,\"Total\":10493952,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":8966144,\"Read\":8966144,\"Sync\":0,\"Total\":8966144,\"Write\":0}}],\"io_serviced\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":449,\"Read\":449,\"Sync\":0,\"Total\":449,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":384,\"Read\":384,\"Sync\":0,\"Total\":384,\"Write\":0}}]},\"memory\":{\"usage\":51695616,\"working_set\":23834624,\"container_data\":{\"pgfault\":100872,\"pgmajfault\":288},\"hierarchical_data\":{\"pgfault\":100872,\"pgmajfault\":288}},\"network\":{\"name\":\"\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},\"task_stats\":{\"nr_sleeping\":0,\"nr_running\":0,\"nr_stopped\":0,\"nr_uninterruptible\":0,\"nr_io_wait\":0}},{\"timestamp\":\"2015-06-26T16:24:14.684956792+02:00\",\"cpu\":{\"usage\":{\"total\":836406546,\"per_cpu_usage\":[195997452,224456541,176927699,239024854],\"user\":280000000,\"system\":510000000},\"load_average\":0},\"diskio\":{\"io_service_bytes\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":10493952,\"Read\":10493952,\"Sync\":0,\"Total\":10493952,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":8966144,\"Read\":8966144,\"Sync\":0,\"Total\":8966144,\"Write\":0}}],\"io_serviced\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":449,\"Read\":449,\"Sync\":0,\"Total\":449,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":384,\"Read\":384,\"Sync\":0,\"Total\":384,\"Write\":0}}]},\"memory\":{\"usage\":51695616,\"working_set\":23834624,\"container_data\":{\"pgfault\":100872,\"pgmajfault\":288},\"hierarchical_data\":{\"pgfault\":100872,\"pgmajfault\":288}},\"network\":{\"name\":\"\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},\"task_stats\":{\"nr_sleeping\":0,\"nr_running\":0,\"nr_stopped\":0,\"nr_uninterruptible\":0,\"nr_io_wait\":0}},{\"timestamp\":\"2015-06-26T16:24:30.6849831+02:00\",\"cpu\":{\"usage\":{\"total\":836406546,\"per_cpu_usage\":[195997452,224456541,176927699,239024854],\"user\":280000000,\"system\":510000000},\"load_average\":0},\"diskio\":{\"io_service_bytes\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":10493952,\"Read\":10493952,\"Sync\":0,\"Total\":10493952,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":8966144,\"Read\":8966144,\"Sync\":0,\"Total\":8966144,\"Write\":0}}],\"io_serviced\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":449,\"Read\":449,\"Sync\":0,\"Total\":449,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":384,\"Read\":384,\"Sync\":0,\"Total\":384,\"Write\":0}}]},\"memory\":{\"usage\":51695616,\"working_set\":23834624,\"container_data\":{\"pgfault\":100872,\"pgmajfault\":288},\"hierarchical_data\":{\"pgfault\":100872,\"pgmajfault\":288}},\"network\":{\"name\":\"\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},\"task_stats\":{\"nr_sleeping\":0,\"nr_running\":0,\"nr_stopped\":0,\"nr_uninterruptible\":0,\"nr_io_wait\":0}},{\"timestamp\":\"2015-06-26T16:25:02.684946556+02:00\",\"cpu\":{\"usage\":{\"total\":836406546,\"per_cpu_usage\":[195997452,224456541,176927699,239024854],\"user\":280000000,\"system\":510000000},\"load_average\":0},\"diskio\":{\"io_service_bytes\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":10493952,\"Read\":10493952,\"Sync\":0,\"Total\":10493952,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":8966144,\"Read\":8966144,\"Sync\":0,\"Total\":8966144,\"Write\":0}}],\"io_serviced\":[{\"major\":8,\"minor\":16,\"stats\":{\"Async\":449,\"Read\":449,\"Sync\":0,\"Total\":449,\"Write\":0}},{\"major\":8,\"minor\":32,\"stats\":{\"Async\":384,\"Read\":384,\"Sync\":0,\"Total\":384,\"Write\":0}}]},\"memory\":{\"usage\":51695616,\"working_set\":23834624,\"container_data\":{\"pgfault\":100872,\"pgmajfault\":288},\"hierarchical_data\":{\"pgfault\":100872,\"pgmajfault\":288}},\"network\":{\"name\":\"\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},\"task_stats\":{\"nr_sleeping\":0,\"nr_running\":0,\"nr_stopped\":0,\"nr_uninterruptible\":0,\"nr_io_wait\":0}}]}\n. ",
    "kenzodeluxe": "Would it be possible to release a new version including the fix?\n. Seems this one didn't make it into 0.16.0 - any ETA when this can be expected?\nThanks!\n. Sorry - I didn't find it in the list of fixed items and hence thought it didn't make it. Tried, metrics aren't mirrored anymore. Looking into some showing up as 0 now :)\nThanks!\n. 0 was caused by host mode which makes sense.\n. Found https://github.com/docker/libcontainer/issues/165.\n. ",
    "pacuna": "@vmarmol thanks!!\n. ",
    "exetico": "I just want to add some information. @vmarmol 's answer is just fine, but, sadly there is a typing error. \nIt should be: \nENTRYPOINT [\"/usr/bin/cadvisor\", \"--http_auth_file\", \"auth.htpasswd\", \"--http_auth_realm\u201d, \u201clocalhost\"]\nI'm totally new with Docker, so it was hard to see what actually went wrong. After attaching to the container, i was able to see that something wasn't added like it should (Well.. i just took two hours to figure that out - Rofl):\nError response from daemon: no such id: 10636eca00c7\nroot@ docker attach 084e540e96a9\nflag provided but not defined: -http_auth_realm localhost\nSo in the last line, you need to seperate --http_auth_realm and localhost.\nAnd... For all the other people, trying to find a nice solution with Google searches and stuff - Here is what i thing is the most easy way to add this:\n- Make a dir like \"docker_cadvisor\" in your home destination.\n- Navigate to the folder.\n- Make a htpasswd file and call it \"auth.htpasswd\" (More info at http://www.cyberciti.biz/faq/create-update-user-authentication-files/)\nhtpasswd -c auth.htpasswd WEBUSERNAME\n- Make the Dockerfile, and add the information\nnano Dockerfile\nPaste in the informations:\n```\nFROM google/cadvisor:latest\nADD auth.htpasswd /auth.htpasswd\nEXPOSE 8080\nENTRYPOINT [\"/usr/bin/cadvisor\", \"--http_auth_file\", \"auth.htpasswd\", \"--http_auth_realm\", \"localhost\"]\n```\n- Build the container, and call it somehing - I personally just called it \"cadvisor\". Please note that you have to keep the dot in the code.\ndocker build -t cadvisor .\n- Give it some time.\n- Run the container - Note that you NEED to CHANGE the buttomline, to the container ID you will get just after docker is done with working out all the good stuff. Actually i guess you are able to put in \"cadvisor\" as well the ID..\nsudo docker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  --restart=always \\\nbdc1c2d18ab5\nNote that i have added \"restart=always\". This means that cAdvisor will start with the system. I personally changed publish from the default to \"--publish=8383:8080 \\\" cause of another resource, running at port 8080.\nHopefully this can help other people, trying to get cAdvisor working with Docker WITH PASSWORD support. Please ask, if you read this and have any questions - and if you spot typing errors. \nUpdate:\nI just saw my own post - If you like dirty stuff - this is just a bit easier ;-) Replace USERNAME and PASSWORD.\n```\ncd /home/USERNAME \\\n&& htpasswd -c -i -b auth.htpasswd USERNAME PASSWORD \\\n&& touch newfile \\\n&& cat < Dockerfile\nFROM google/cadvisor:latest\nADD auth.htpasswd /auth.htpasswd\nEXPOSE 8080\nENTRYPOINT [\"/usr/bin/cadvisor\", \"--http_auth_file\", \"auth.htpasswd\", \"--http_auth_realm\", \"localhost\"]\nEOF\ndocker build -t cadvisor . \\\n&& docker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  --restart=always \\\n cadvisor\n```\n. Hi @tim545. I will give it a try, in my new ESXi setup at home. I think it will be a good idea to point out, how to use the arguments in the information.\n. ",
    "tim545": "Thanks @vmarmol and @exetico! your answers helped me out a lot on this one.\nIf you guys are by chance interested I took your advise and put it into a container on docker hub - https://hub.docker.com/r/tim545/cadvisor-basicauth It works pretty well, I'm using it on a personal server at the moment. you can get it going in a few commands:\ngit clone https://github.com/tim545/docker-cadvisor-basicauth.git\ndocker build --build-arg USERNAME=admin --build-arg PASSWORD=Password1 -t tim545/cadvisor-basicauth .\ndocker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor-basicauth \\\n  --restart=always \\\n tim545/cadvisor-basicauth:latest\nThe only spot where I fell short was parsing the environment variables in straight from the run command instead of having to manually clone->build->run using the --build-args's.\nFrom what I understand it could be done by replacing ENTRYPOINT [\"/usr/bin/cadvisor\", \"--http_auth_file\", \"auth.htpasswd\", \"--http_auth_realm\", \"localhost\"] with something like ENTRYPOINT [\"entrypoint.sh\"] to run a bash script a bit like:\n```\n!/bin/bash\nhtpasswd -c -i -b auth.htpasswd $USERNAME $PASSWORD\n/usr/bin/cadvisor --http_auth_file auth.htpasswd --http_auth_realm localhost\n```\nI'm not very good with writing bash scripts and I think there's some extra things you need to do to make it work when being run from a docker container, but I think my main issue was being able to parse the USERNAME and PASSWORD to entrypoint.sh via the run command using environment variables like this:\ndocker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor-basicauth \\\n  --restart=always \\\n  -e USERNAME=admin \\\n  -e PASSWORD=Password1 \\\n tim545/cadvisor-basicauth:latest\nNote: For anyone who just skipped to here, the above command does not work\nWhen I get some more time to spend on it I'll try again, but any help/pointers or even a PR would be appreciated.\n. Thanks @vmarmol and @exetico! your answers helped me out a lot on this one.\nIf you guys are by chance interested I took your advise and put it into a container on docker hub - https://hub.docker.com/r/tim545/cadvisor-basicauth It works pretty well, I'm using it on a personal server at the moment. you can get it going in a few commands:\ngit clone https://github.com/tim545/docker-cadvisor-basicauth.git\ndocker build --build-arg USERNAME=admin --build-arg PASSWORD=Password1 -t tim545/cadvisor-basicauth .\ndocker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor-basicauth \\\n  --restart=always \\\n tim545/cadvisor-basicauth:latest\nThe only spot where I fell short was parsing the environment variables in straight from the run command instead of having to manually clone->build->run using the --build-args's.\nFrom what I understand it could be done by replacing ENTRYPOINT [\"/usr/bin/cadvisor\", \"--http_auth_file\", \"auth.htpasswd\", \"--http_auth_realm\", \"localhost\"] with something like ENTRYPOINT [\"entrypoint.sh\"] to run a bash script a bit like:\n```\n!/bin/bash\nhtpasswd -c -i -b auth.htpasswd $USERNAME $PASSWORD\n/usr/bin/cadvisor --http_auth_file auth.htpasswd --http_auth_realm localhost\n```\nI'm not very good with writing bash scripts and I think there's some extra things you need to do to make it work when being run from a docker container, but I think my main issue was being able to parse the USERNAME and PASSWORD to entrypoint.sh via the run command using environment variables like this:\ndocker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor-basicauth \\\n  --restart=always \\\n  -e USERNAME=admin \\\n  -e PASSWORD=Password1 \\\n tim545/cadvisor-basicauth:latest\nNote: For anyone who just skipped to here, the above command does not work\nWhen I get some more time to spend on it I'll try again, but any help/pointers or even a PR would be appreciated.\n. Thanks @exetico, let me know how it goes. I updated the readme a bit to try and make the instructions clearer like you mentioned.. Thanks @exetico, let me know how it goes. I updated the readme a bit to try and make the instructions clearer like you mentioned.. ",
    "gustavomcarmo": "Thank you guys your comments were really useful. What I unfortunately miss is the implementation of basic auth in the Prometheus metrics endpoint (/metrics). The code has nothing about it and it is a feature I really need. Maybe I can contribute with it soon.. ",
    "KostyaSha": "Any updates?\n. ",
    "hookenz": "Hi @vmarmol ,\nThe output of ls /sys/class/net is:\ndocker0 en01 ib0 lo veth16589f3 veth428c6db\nIt turns out I had gccgo-go installed instead of golang-go 1.4.  \nBasically, nothing in the GetNetworkAdapters code looks broken for IB as far as I can tell.  But I'll take a closer look.\n. @vmarmol - I figured out the issue.  While the output of /sys/class/net shows the ib0 device, it wasn't appearing in the UI.  This is because it's not seen in the namespace of the docker image that is running cadvisor.\nTwo solutions to this:\n1) use --volume=/sys:/sys:ro instead of --volume=/sys/fs/cgroup/:/sys/fs/cgroup\nNote: for some reason you can't have both /sys/fs/cgroup and /sys/class/net mapped?!\n2) Use --net=host\n. @vmarmol -hmm, actually I realise the CoreOS instructions are the issue.\nI've changed my cadvisor.service file to contain:\ndocker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  google/cadvisor:latest\nThe key being to pass -volume=/:/rootfs:ro.  That fixes the issue where I also had the process list missing.\n. @vmarmol - Looks like I was following the heapster docs which are perhaps outdated in that section.  But ensuring the rootfs mount is present fixes it.\n. The cadvisor.service file suggested for CoreOS is missing:\n--volume=/:/rootfs:ro\nWhich fixes all these problems.\n. My mistake!  I was using a config file found under the heapster documentation.  You're all good.\nNo I don't see a cadvisor.service file in the repo.\n. ",
    "mikedanese": "@mnuessler The godep changes broke the build.\ngodep go test -v -race -test.short github.com/google/cadvisor/...\nutils/cloudinfo/gce.go:20:2: cannot find package \"github.com/GoogleCloudPlatform/gcloud-golang/compute/metadata\" in any of:\n    /home/travis/.gimme/versions/go1.4.linux.amd64/src/github.com/GoogleCloudPlatform/gcloud-golang/compute/metadata (from $GOROOT)\n    /home/travis/gopath/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/GoogleCloudPlatform/gcloud-golang/compute/metadata (from $GOPATH)\n    /home/travis/gopath/src/github.com/GoogleCloudPlatform/gcloud-golang/compute/metadata\nstorage/bigquery/client/example/example.go:22:2: cannot find package \"github.com/SeanDolphin/bqschema\" in any of:\n    /home/travis/.gimme/versions/go1.4.linux.amd64/src/github.com/SeanDolphin/bqschema (from $GOROOT)\n    /home/travis/gopath/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/SeanDolphin/bqschema (from $GOPATH)\n    /home/travis/gopath/src/github.com/SeanDolphin/bqschema\nutils/fs/mockfs/mockfs.go:21:2: cannot find package \"code.google.com/p/gomock/gomock\" in any of:\n    /home/travis/.gimme/versions/go1.4.linux.amd64/src/code.google.com/p/gomock/gomock (from $GOROOT)\n    /home/travis/gopath/src/github.com/google/cadvisor/Godeps/_workspace/src/code.google.com/p/gomock/gomock (from $GOPATH)\n    /home/travis/gopath/src/code.google.com/p/gomock/gomock\n. The huge number of dependencies for the client is actually fixed in HEAD of influx.\nhttps://github.com/influxdb/influxdb/pull/4134\nhttps://github.com/influxdb/influxdb/issues/3447\n. I would suggest using logrotate. glog is closed for features and it seems like overkill to include in cadvisor.\nhttp://linux.die.net/man/8/logrotate\n. It's also important that we separate the responsibility of exporting raw metrics and transforming/aggregating/deriving statistics. Since cAdvisor is a node agent, cpu/memory is at a premium and many use cases will prefer to move raw metrics off the node before applying transformations. Maybe this could be implemented in a storage backend?\n. It's also possible that this info is already available from accounting in which case we should just expose it...\n. @jimonreal it is very tricky to squash when you are working off master branch (and impossible now that you have merge the remote master into your local master). You should try to work on a feature branch and keep your master branch in sync with the remote master, then you can easily rebase your feature branch ontop of your local master\n. Awesome!\n. This should not be removed\n. These imports are not valid json. Different imports need to be in separate objects.\n. https://github.com/golang/go/wiki/CodeReviewComments#receiver-names\n. ",
    "mangalaman93": "Why doesn't influxdb show any network interfaces? Is that the expected behavior? I was trying to use these changes in my cadvisor-influxdb-grafana setup.\n. ",
    "christianhuening": "Is there anything one could do to help getting this done? I've not looked into the sources or anything, but if it's merely a case of updating stuff and checking it, I might be able to do so.\n. sorry was very busy over the holidays. I understand that the majority of the work has been done?\n. ",
    "normanjoyner": "Submitted PR to fix missing InfluxDB models dependency. Tests failing on TestGetProcessNamePid which is fixed in dee3f07b0afc36bd41014679989e9610a4c1e8a7.\n. :+1:\n. ",
    "alexmavr": "comment addressed in the latest commit. Let me know when to squash it!\n. TestDockerContainerNetworkStats and TestDockerContainerSpec are failing in the master branch as well.\n. squashed!\n. yes! there was only one usage:\nhttps://github.com/google/cadvisor/pull/811/files#diff-b99eec7556cdc85a8fbee378f4131125L21\n. I guess this choice boils down to the design choice of functionality separation versus usability and consistency.\nCadvisor is already performing more advanced node-level derivations than this, such as 90th percentiles and moving averages, with more such stats coming up in the following weeks.\nIn an ideal world, CAdvisor would only export raw metrics, and there would be another layer on top of that that performs derivation and aggregation, in the similar way that is now partially performed in CAdvisor, Heapster, Kubelets and timeseries database clients.\nHowever, in terms of consistency and usability, cpu_cumulative_usage is the only cumulative stat that CAdvisor exposes. If data precision and functionality isolation are really a primary concern, then it would only make sense to export all other stats to their cumulative versions as well; and implement a configurable layer on top of CAdvisor that can extract instantaneous and derived stats from these cumulative metrics.\nHowever, since that layer does not exist, CAdvisor has already been bloated up with metric derivations for hardcoded time periods, as these stats are what's useful to a large set users of cadvisor.\nA simple search of CAdvisor and Heapster issues with \"cpu usage\" or \"cpu_cumulative_usage\" reveals that the case of the cpu usage stat is problematic with many users, as it causes them to have to implement their own handling logic only for this metric. Needless to mention that there are issues, such as crash-looping containers, where the end user would also have to check a container's uptime to be able to derive the instantaneous CPU usage, whereas instantaneous memory, network and filesystem metrics are readily available.\nFinally, the existing approach of exporting only cpu usage as a cumulative stat propagates a design problem of cadvisor (separation of raw data exporting and metric derivation) to the codebase of each consumer. For that reason, CAdvisor should be able to serve both types of users in a consistent pattern: \n- Derived stats users and users who deal with low-resolution metrics would mostly need instantaneous metrics and derived stats.\n- High-resolution metric users, such as timeseries databases, would need cumulative data for all metrics to allow derivations and transformations to be as precise as needed.\n\\cc @rjnagal @vmarmol @vishh\n. It appears that the docker downgrade to v1.4.1-4831-g0f5c9d3 was performed because of runc's dependency on that version. I will edit this PR to disregard the downgrade, even though it seems that the imported packages are not affected. The other package upgrades were, again, performed due to runc's dependencies on these versions.\nHowever, from a quick look, I can see that Kubernetes has not migrated to runc yet. Therefore, introducing this set of changes to CAdvisor at this point would require Kubernetes to migrate to runc during their next godep update of CAdvisor. I am not aware of the current release plans of both projects so this PR could be postponed until Kubernetes is ready to migrate.\nAs far as I'm aware, there should be no compatibility concerns with docker, as the previous version that was being linked with CAdvisor (v1.8.3) already uses runc/libcontainer internally.\nPlease let me know if you feel any other changes are needed to this PR, aside from retaining the docker version.\n. Updated this PR to include the latest available tag for docker/docker, v1.9.1\n. I will not have the time to actually dig into this in detail any time soon.\nWe can revert #966 and I can resubmit the Godep bump PR with the exact version of runc that corresponds to Docker 1.8.3. Would that be an adequate fix?\n. ",
    "kgrvamsi": "so here are my requirements \n1)To gather the server metrics and container metrics and use a centralized dashboard where i can project those from the dashboard we are building .\nSo inorder not to make use of the gui we would like to use the API way of fetching the metrics.\nThanks\n. so here are my requirements \n1)To gather the server metrics and container metrics and use a centralized dashboard where i can project those from the dashboard we are building .\nSo inorder not to make use of the gui we would like to use the API way of fetching the metrics.\nThanks\n. so there is no possibility  in not running the cadvisor as a web application ?\n. so there is no possibility  in not running the cadvisor as a web application ?\n. ",
    "ciuncan": "Could this issue be related? https://github.com/google/cadvisor/issues/808\n. Oh, this was a very long time ago, and I was just experimenting with it, nothing serious. If nobody else met with such an error, it may as well be closed.\n. ",
    "elephunt": "Did you find a solution?\n. @mynameisguy Hey ,sorry for delay.My solution was just to switch between influxDb to Prometheus.Everything started worked fine.\n. ",
    "mynameisguy": "I'm having the same issue.\nI used this docker-compose  script.\nReplaced cadvisor version to v0.23.0 and latest. both couldn't write to influxdb (v0.8.8).\nversion v0.20.1 worked but didn't return the containers name. (solved in 0.23.0 according to #1127)\n. ",
    "Christopher-Bui": "I just reproduced this with the same commands using today's latest containers.\nInflux DB seems to be returning a 404 page now so error is slightly different: E0518 13:28:33.621145       1 memory.go:91] failed to write stats to influxDb - 404 page not found \nEDIT: Actually, doesn't this mean the host is found?. We're also needing this for the same reason. Anything I can do to help move this forward?\n. Looks like prometheus only exposes per-container metrics right now. Would it be appropriate to add the docker driver status metrics here?. I started looking into this and it looks like there are a couple potential issues.\nOne is that we have to parse the string docker daemon exposes to get the storage used from docker info, which can be a little tricky because docker scales the unit shown eg. GB / MBs. \nSecondly, this logic would be very specific to device mapper running on direct-lvm. Other storage drives don't expose the usage with docker info.. Are you using direct or loop lvm?. Data Space Used 1.409 GB\nData Space Total 23.35 GB\nIs that not it?. I did some profiling, not sure if I did it right or if it'll be useful:  \n\n. I signed it!. Do I have to squash the merge commit?. I don't think coupling consul and cadvisor is appropriate. Is there a reason you can't just link cadvisor to your consul container?. ",
    "mvdan": "Initially these were going to be used by kubelet, but in the end we're doing them in heapster with @afein. If this is in general useful, like you say, it can of course be merged. If not, I'm fine with closing it too.\nI can have a look at the UI part, yeah. Should that be on this PR?\n. nit fixed. Also, note the TODO added. I was just confused by that code and the 90p comment so was hoping someone could shed some light on that.\n. Argument against this makefile in particular - running deps, clean and build in test before the actual godep go test will make the whole thing noticeably slower than just running godep go test directly.\nWhy are those dependencies for test anyway?\nI generally agree with @rjnagal here - a Makefile to share targets as if they were aliases amongst people is fine, as long as running the simple go/godep commands will still be straightforward :)\n. @mikedanese all worked out on IRC :)\n. Will be back in a new, cleaned up PR soon.\n. Besides the constants and error comments, LGTM.\n@cadvisorJenkinsBot ok to test\nCC @vmarmol\n. It appears I'm not an admin. Could someone trigger the tests?\n. LGTM\n. LGTM\n. Ping @rjnagal \n. Doing the PR with only the data structure first so that if anyone is strongly against this, it can be said before I actually implement it.\n. @vmarmol @rjnagal is this a terrible idea?\n. @rjnagal how does this look? Doing a unit test for instCpuStats() now.\n. Finished including extensive unit tests. Doing integration tests now.\n. @rjnagal done. I did not do the integration tests because the v2 client is lacking, and I did not add the DockerContainer method to the v2 client because the v2.ContainerInfo type is missing. This PR would get rather large if I did all of that, and I don't know what you want to do with v2.ContainerInfo either.\n. @rjnagal I can't just switch kubelet to using cadvisor's info/v2 because some types are missing. Saw your TODO, so I copied whatever types were necessary and weren't in v2 already.\n. Note that I left v1.FsInfo instead of using the modified FsInfo in v2. Should I change that?\n. Actually, more stuff is missing. Would you move it to v1 and keep the kubelet as-is, or keep it in v2 and make all the modifications in cadvisor and kubelet for it to work?\n. The /stats api in kubelet will be removed, I'm ading /experimental/v1/nodeMetrics which translates from the cadvisor stats into its own api types.\nOkay, I'll continue with v2. Just wanted to ask since I wasn't expecting v2 to be this lacking.\n. @rjnagal comments addressed. Removed the v2 stuff, I'll do that on another PR once I have everything working in kubelet.\n. all green :)\n. @shahidhs-ibm as a start, the gofmt checks are failing. Fix those first.\n. Jenkins does that for some reason sometimes (picking up the wrong set of commits to merge). We never quite figured out why.\nI assume this godeps cleanup is similar to those done in heapster? At a quick glance LGTM.\n. Thanks for the cleanup @jimmidyson!\nEdit: was about to merge, then remembered I can't.\n. @vishh the issue is that docker info outputs human readable info, not raw info in bytes.\n. A few comments, otherwise LGTM.\n. Sure.\n. It's a bit more idiomatic to do:\ngo\ncontainerName := ref.Name\nif len(ref.Aliases) > 0 {\n    containerName = ref.Aliases[0]\n}\n. If this data can be very long - i.e. if this line can be very long - consider splitting the output into multiple lines.\nThis has several advantages:\n- Better readability\n- No need to use a buffer, just write directly line by line to stdout\n- Writing very long chunks of text to stdout in one go is not a very good idea on most systems\n. Do you need to specify that these consts are strings? (probably not)\n. We generally group and separate imports, so add an empty line before the info import since it's not a standard library import.\n. This block of code is a bit weird. As a start, you could do the string concatenation only once.\nIs the map dereference necessary? I thought Go took care of this, although I'm right now unsure about maps.\n. So you're saying that if you do line by line instead of writing it all at once, different goroutines might work at the same time and thus their lines might get mixed?\nIf you are worried about that, you could do the writes to stdout with a worker and a channel, something like chan []string, acting as a queue of chunks of lines.\nIf there is still a strong reason why a single line is preferred over multiple lines, then sticking with the current code sounds fine to me.\n. Whoops, you're right. Read too fast.\n. Constants don't need to be typed. You can just do foo = \"bar\" instead of foo string = \"bar\"``\n. Clarification - you might need a constant to be typed for some reason, but I highly doubt this is the case.\n. If the intention is to write only one line perAddStats()call, then this code is fine. If it ever becomes slow due to the buffers getting big, we can look into ways of improving it.\n. You should probably returnPrintln`'s error:\ngo\n_, err := fmt.Println(buffer.String())\nreturn err\n. Just remove the string word from all of these const declarations. It should still build and work the same.\n. Since aggregated is in nanoseconds, I thought I'd stay consistent and not lose any precision.\n. Yes, going to fix that.\n. Fair enough. If I split them, should the new definition be in v1 or v2?\nThanks for your input!\n. I would have assumed that v1 was frozen by now. Otherwise, shouldn't changes in v1 be in v2 too?\nWould moving kubelet to v2 be an issue?\n. Why is that? Is it never supposed to be that small?\n. Sure.\n. I used nanocores/s for both precision and consistency with the other metrics, which are also nano.\nKubelet will expose milli, so yes, it will divide the numbers. Although it's not converting \"back\", since I obtain the instantaneous metrics from accumulated nanoseconds, not milliseconds.\n. OK.\n. this can be if b := scanner.Scan(); !b {\n. I'd invert this condition and use continue instead. This will mean one less indentation level for the rest of the code. See https://github.com/golang/go/wiki/CodeReviewComments#indent-error-flow\n. Just curious, but why not return a nil slice? We didn't before, but it should be fine.\n. Unless this is later marshaled into json, in which case it would become null instead of [].\n. ",
    "basvdlei": "I'm pretty sure containers are started as a .scope unit. This is good enough for filtering on my systemd based machines (Debian and CoreOS), where I don't have any other scope units. I've also changed the ContainerNameToDockerId to handle containers started with --cgroup-parent.\n. Hi, is there something I can do to get this in?\n. Moved the regexp out of the function as requested. Squashed and rebased so the automated builds won't fail (on changed dependencies.)\n. The first thing that comes to mind is to ask Docker for the cgroup parent name. It seems to be part of the inspect result. The current FullContainerName implementation however doesn't have a reference to the Docker client.\nWhen scanning the codebase, FullContainerName is only used in dockerContainerHandler.ListContainers. So adding an extra argument that passes a Docker client reference shouldn't have that big of an impact.\nBut I don't like the amount of extra API call to Docker this will generate, for a case most users won't ever need. \nWe could first try to test if docker-%s.scope cgroup exists before bugging Docker about to cgroup parent name?\n. Haven't spend much time on it, and when I did, I've mostly been banging my head on finding a clean solution for FullContainerName. The more I look at it, the more it seems like dead code. I'll take a look at your PR in a moment.\nI like whosthatknocking's regex (.+) for the parent name, since it's never nil. AFAIK docker does not place any restrictions on the name, as far as it results in a valid unit name.\n. You are correct. I've not used that feature before :) So I just did a simple test and the prefix docker- is indeed replaced with the parent name. But it also didn't show up as a docker container in a build of master though. Probably because the docker factory is already relying on the docker- prefix when Systemd is detected :(\n. Is this not setting the cgroup parent the same for all containers? Since this for is looping over all docker containers (self.client is a reference to the Docker client), but self.cgroupParent is only for the current container.\n. In all the tests I've done, it doesn't. I have tested this on Ubuntu Precise (upstart), Debian Jessie/Sid (sysv init) and Debian Jessie (systemd).\nWhen the manager is started it registers the / container and starts searching for containers using detectSubcontainers(\"/\"). Since / is technically a raw container, all Docker containers are discovered using their entries in the cgroup mount. \nI have not found any place where the dockerContainerHandler with the name /docker is created. Because it doesn't exist, this if will never be false\n. Yes, docker will create a cgroup named docker, and is found as subcontainer of /. But CanHandleAndAccept will be false when the manager tries to get a dockerContainerHandler for it. Because the inspect will fail: https://github.com/google/cadvisor/blob/master/container/docker/factory.go#L147\nYou can also see this happening in the Docker daemon logs:\nlog\nERRO[437132] Handler for GET /containers/{name:.*}/json returned error: no such id: docker \nERRO[437132] HTTP Error                                    err=no such id: docker statusCode=404\nOnly when the container /docker/f5a39e9c98788da89c050ca45d266ce0ddc31bd0a7e3ecbc618e166c17ade17d is found it will actually create a handler.\n. Maybe we are saying the same thing. Or are you saying this code is still needed? If so, do you know where the dockerContainerHandler named only /docker is created? Because only then will ListContainers actually do something.\nIn my tests, with this function stubbed, everything also still works on a non-systemd machine and also see Docker containers with the /docker cgroup eg.\nclever_wright (/docker/64f26ce9d60219c5d730f31abeabc39c47fe7aa282343c3a3a8c820a80db5b70)\n. ",
    "whosthatknocking": "Just wondering, it looks like the scope of this PR is,\n- enabling support for --cgroup-parent\n- filter accept to only .scope\nPerhaps you can consider,\n- Two PRs?\n- To minimize any performance impact should the regexp.MustCompile be moved outside the func\n- To be consistent, your changes are not covering FullContainerName func\n. (.+)-([a-z0-9]{64})\\.scope$?\n. Feels appropriate to follow existing code style and express the check under isContainerName or similar function?\n. RH 7.1 with Docker 1.8.2\n. ",
    "ZeroGraviti": "I am seeing the same behavior (no blkio and zero values for network metrics) on cAdvisor when it is running in a docker container. Here are the top 10 lines (or so) of my /validate :\n```\ncAdvisor version: 0.16.0\nOS version: Buildroot 2014.02\nKernel version: [Supported and recommended]\n    Kernel version is 4.0.5-boot2docker. Versions >= 2.6 are supported. 3.0+ are recommended.\nCgroup setup: [Supported and recommended]\n    Available cgroups: map[cpu:1 net_cls:1 perf_event:1 net_prio:1 cpuset:1 blkio:1 memory:1 devices:1 freezer:1 cpuacct:1]\n    Following cgroups are required: [cpu cpuacct]\n    Following other cgroups are recommended: [memory blkio cpuset devices freezer]\n    Hierarchical memory accounting disabled. Memory usage does not include usage from child containers.\n```\nPlease let me know if you want the full output ?\nTo confirm I ran docker stats and --no-stream=true, with the following results which at least for network i/o show zero values for each of the containers. Sample output for one of the containers is :\nCONTAINER           CPU %               MEM USAGE/LIMIT     MEM %          NET I/O\nfoobar1             1.54%               87.71 MB/1.044 GB   8.40%          0 B/0 B\n. ",
    "lukaf": "Hi,\nSame results (no network statistics from containers) on Ubuntu 14.04 LTS, Docker 1.7.1 and cAdvisor 0.16.0.\nStatistics are shown using docker stats but not exported by cAdvisor.\npensive_thompson is a nginx container:\nCONTAINER           CPU %               MEM USAGE/LIMIT     MEM %               NET I/O\npensive_thompson    0.03%               1.389 MB/1.041 GB   0.13%               267.7 kB/578.3 kB\nbash\n$ curl localhost:8080/metrics -s | awk '/pensive_thompson/ && /network/'\ncontainer_network_receive_bytes_total{id=\"/docker/b29db2ffebdf165730b3585085caa459907cda32ffb02a0f2d2e2e86255a6bd7\",name=\"pensive_thompson\"} 0\ncontainer_network_receive_errors_total{id=\"/docker/b29db2ffebdf165730b3585085caa459907cda32ffb02a0f2d2e2e86255a6bd7\",name=\"pensive_thompson\"} 0\ncontainer_network_receive_packets_dropped_total{id=\"/docker/b29db2ffebdf165730b3585085caa459907cda32ffb02a0f2d2e2e86255a6bd7\",name=\"pensive_thompson\"} 0\ncontainer_network_receive_packets_total{id=\"/docker/b29db2ffebdf165730b3585085caa459907cda32ffb02a0f2d2e2e86255a6bd7\",name=\"pensive_thompson\"} 0\ncontainer_network_transmit_bytes_total{id=\"/docker/b29db2ffebdf165730b3585085caa459907cda32ffb02a0f2d2e2e86255a6bd7\",name=\"pensive_thompson\"} 0\ncontainer_network_transmit_errors_total{id=\"/docker/b29db2ffebdf165730b3585085caa459907cda32ffb02a0f2d2e2e86255a6bd7\",name=\"pensive_thompson\"} 0\ncontainer_network_transmit_packets_dropped_total{id=\"/docker/b29db2ffebdf165730b3585085caa459907cda32ffb02a0f2d2e2e86255a6bd7\",name=\"pensive_thompson\"} 0\ncontainer_network_transmit_packets_total{id=\"/docker/b29db2ffebdf165730b3585085caa459907cda32ffb02a0f2d2e2e86255a6bd7\",name=\"pensive_thompson\"} 0\n. While testing 0.16.0.1 release which should include this changes, I've noticed network metrics collection works only if cadvisor is running outside container.\nOn Ubuntu 14.04.03 LTS with Docker 1.8.1.\nDownloaded 0.16.0.1 release and built using:\n$ godep go build -a\nPackaged in an image with Dockerfile:\nFROM debian:8\nCOPY cadvisor-0.16.0.1 /cadvisor\nENTRYPOINT [\"/cadvisor\"]\nBuild and run:\n$ docker build -t test/cadvisor .\n$ docker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro  --volume=/var/lib/docker/:/var/lib/docker:ro --publish=8080:8080 --detach=true --name=cadvisor test/cadvisor\nMetrics for network are empty:\n$ curl localhost:8080/metrics -s | awk '/nginx/ && /network/'\ncontainer_network_receive_bytes_total{id=\"/docker/2a0706ff57a311c3056d55ef0f9fc70f0526a3b5e8a2f0b5840f620553b47e0e\",name=\"nginx\"} 0\ncontainer_network_receive_errors_total{id=\"/docker/2a0706ff57a311c3056d55ef0f9fc70f0526a3b5e8a2f0b5840f620553b47e0e\",name=\"nginx\"} 0\ncontainer_network_receive_packets_dropped_total{id=\"/docker/2a0706ff57a311c3056d55ef0f9fc70f0526a3b5e8a2f0b5840f620553b47e0e\",name=\"nginx\"} 0\ncontainer_network_receive_packets_total{id=\"/docker/2a0706ff57a311c3056d55ef0f9fc70f0526a3b5e8a2f0b5840f620553b47e0e\",name=\"nginx\"} 0\ncontainer_network_transmit_bytes_total{id=\"/docker/2a0706ff57a311c3056d55ef0f9fc70f0526a3b5e8a2f0b5840f620553b47e0e\",name=\"nginx\"} 0\ncontainer_network_transmit_errors_total{id=\"/docker/2a0706ff57a311c3056d55ef0f9fc70f0526a3b5e8a2f0b5840f620553b47e0e\",name=\"nginx\"} 0\ncontainer_network_transmit_packets_dropped_total{id=\"/docker/2a0706ff57a311c3056d55ef0f9fc70f0526a3b5e8a2f0b5840f620553b47e0e\",name=\"nginx\"} 0\ncontainer_network_transmit_packets_total{id=\"/docker/2a0706ff57a311c3056d55ef0f9fc70f0526a3b5e8a2f0b5840f620553b47e0e\",name=\"nginx\"} 0\nBut if cadvisor is ran outside the container:\n$ sudo ./cadvisor-0.16.0.1 &\n[1] 11325\n$ curl localhost:8080/metrics -s | awk '/nginx/ && /network/'\ncontainer_network_receive_bytes_total{id=\"/docker/2a0706ff57a311c3056d55ef0f9fc70f0526a3b5e8a2f0b5840f620553b47e0e\",name=\"nginx\"} 752392\ncontainer_network_receive_errors_total{id=\"/docker/2a0706ff57a311c3056d55ef0f9fc70f0526a3b5e8a2f0b5840f620553b47e0e\",name=\"nginx\"} 0\ncontainer_network_receive_packets_dropped_total{id=\"/docker/2a0706ff57a311c3056d55ef0f9fc70f0526a3b5e8a2f0b5840f620553b47e0e\",name=\"nginx\"} 0\ncontainer_network_receive_packets_total{id=\"/docker/2a0706ff57a311c3056d55ef0f9fc70f0526a3b5e8a2f0b5840f620553b47e0e\",name=\"nginx\"} 3204\ncontainer_network_transmit_bytes_total{id=\"/docker/2a0706ff57a311c3056d55ef0f9fc70f0526a3b5e8a2f0b5840f620553b47e0e\",name=\"nginx\"} 353296\ncontainer_network_transmit_errors_total{id=\"/docker/2a0706ff57a311c3056d55ef0f9fc70f0526a3b5e8a2f0b5840f620553b47e0e\",name=\"nginx\"} 0\ncontainer_network_transmit_packets_dropped_total{id=\"/docker/2a0706ff57a311c3056d55ef0f9fc70f0526a3b5e8a2f0b5840f620553b47e0e\",name=\"nginx\"} 0\ncontainer_network_transmit_packets_total{id=\"/docker/2a0706ff57a311c3056d55ef0f9fc70f0526a3b5e8a2f0b5840f620553b47e0e\",name=\"nginx\"} 4538\nAm I missing something?\nThanks!\n. @jimmidyson That worked, thanks!\n. ",
    "zenlint": "@vmarmol , If I want to realize to export data to the native file on system,  how can I do ,plz do me some favour, thank u very much.\n. ",
    "piosz": "The changed is required to change the value of max_housekeeping_interval in Kubernetes. PTAL\n. I definitely prefer typing make test instead of godep go test ./... -test.short, but this is just my private feeling;)\n. cc @dchen1107 @fgrzadkowski @jszczepkowski @mwielgus @vishh \n. Yes, that would be easier but we have already defined semantic of the request (see container.go) and this would be a breaking change. @rjnagal @vishh @vmarmol any thoughts?\nSince we are downsampling the stats to exactly one in Heapster we can do it on cadvisor side and reduce the network traffic.\n. The most recent one.\n. cadvisorJenkinsBot writing comments... sounds interesting ;)\n. Changed the code to make a breaking change.\n. @rjnagal @vishh could you please grant LGTM to the PR?\nThe integration test is broken:\nError 0: failed to make remote testing directory: command \"gcloud\" [\"compute\" \"ssh\" \"--zone\" \"us-central1-f\" \"cadvisor-ubuntu-trusty\" \"--\" \"mkdir\" \"-p\" \"/tmp/cadvisor-5471\"] failed with error: exit status 1 and output: \"ERROR: (gcloud.compute.ssh) The required property [project] is not currently set.\\nYou may set it for your current workspace by running:\\n\\n  $ gcloud config set project VALUE\\n\\nor it can be set temporarily by the environment variable [CLOUDSDK_CORE_PROJECT]\\n\"\nBuild step 'Execute shell' marked build as failure\n. thanks @jimmidyson \n. According to the documentation the default is 60 (see https://github.com/google/cadvisor/blob/master/info/v1/container.go#L95). \n. create an issue #1011 for this purpose\n. Thanks @jimmidyson. What is the release process of cadvisor?\n. Yes, please. Thank you very much in advance.\n. cc @jszczepkowski\n. @timstclair @dchen1107 \n. Thanks @timstclair!\n. ",
    "Naresht-vedams": "@vmarmol \nI tried localhost:4194/api/v1.3/events/start_time but it is showing nothing.\nCould you provide the REST API url..?\n. ",
    "KevinPike": "Thanks @rjnagal for quickly addressing it. I think, with my very limited cAdvisor knowledge, that the PR will resolve this.\n. ```\ncAdvisor version: 0.16.0\nOS version: Buildroot 2014.02\nKernel version: [Supported and recommended]\n    Kernel version is 3.13.0-52-generic. Versions >= 2.6 are supported. 3.0+ are recommended.\nCgroup setup: [Supported and recommended]\n    Available cgroups: map[blkio:1 hugetlb:1 devices:1 freezer:1 perf_event:1 cpuset:1 cpu:1 cpuacct:1 memory:1]\n    Following cgroups are required: [cpu cpuacct]\n    Following other cgroups are recommended: [memory blkio cpuset devices freezer]\n    Hierarchical memory accounting disabled. Memory usage does not include usage from child containers.\nCgroup mount setup: [Supported, but not recommended]\n    Cgroups are mounted at /rootfs/sys/fs/cgroup.\n    Cgroup mount directories: blkio cpu cpuacct cpuset devices freezer hugetlb memory perf_event systemd \n    Any cgroup mount point that is detectible and accessible is supported. /sys/fs/cgroup is recommended as a standard location.\n    Cgroup mounts:\n    cgroup /rootfs/sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /rootfs/sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /rootfs/sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /rootfs/sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /rootfs/sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /rootfs/sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /rootfs/sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/rootfs/sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/rootfs/sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/rootfs/sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/rootfs/sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/rootfs/sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/rootfs/sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/rootfs/sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/rootfs/sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/rootfs/sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\nDocker version: [Supported and recommended]\n    Docker version is 1.6.2. Versions >= 1.0 are supported. 1.2+ are recommended.\nDocker driver setup: [Supported and recommended]\n    Docker exec driver is native-0.2. Storage driver is aufs.\n    Cgroups are being created through cgroup filesystem.\n    Docker container state directory is at \"/var/lib/docker/containers\" and is accessible.\nBlock device setup: [Supported, but not recommended]\n    None of the devices support 'cfq' I/O scheduler. No disk stats can be reported.\n     Disk \"xvda\" Scheduler type \"deadline\".\nInotify watches: \n    /docker/4aad8783588b4f9c01c8de1267268c849f0e5d56004c2308a67270dabe7e8770:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/4aad8783588b4f9c01c8de1267268c849f0e5d56004c2308a67270dabe7e8770\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/4aad8783588b4f9c01c8de1267268c849f0e5d56004c2308a67270dabe7e8770\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/4aad8783588b4f9c01c8de1267268c849f0e5d56004c2308a67270dabe7e8770\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/4aad8783588b4f9c01c8de1267268c849f0e5d56004c2308a67270dabe7e8770\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/4aad8783588b4f9c01c8de1267268c849f0e5d56004c2308a67270dabe7e8770\n    /docker/9fe0558b8c6dcd3a264a5ff346997be3c999ffe88cf2945fea8dd310572edc3c:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/9fe0558b8c6dcd3a264a5ff346997be3c999ffe88cf2945fea8dd310572edc3c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/9fe0558b8c6dcd3a264a5ff346997be3c999ffe88cf2945fea8dd310572edc3c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/9fe0558b8c6dcd3a264a5ff346997be3c999ffe88cf2945fea8dd310572edc3c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/9fe0558b8c6dcd3a264a5ff346997be3c999ffe88cf2945fea8dd310572edc3c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/9fe0558b8c6dcd3a264a5ff346997be3c999ffe88cf2945fea8dd310572edc3c\n    /docker/99b235fd62a8a991cd3dcdb858e0a68dd47abd2992e718da0dcc7bb302f0e9fe:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/99b235fd62a8a991cd3dcdb858e0a68dd47abd2992e718da0dcc7bb302f0e9fe\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/99b235fd62a8a991cd3dcdb858e0a68dd47abd2992e718da0dcc7bb302f0e9fe\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/99b235fd62a8a991cd3dcdb858e0a68dd47abd2992e718da0dcc7bb302f0e9fe\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/99b235fd62a8a991cd3dcdb858e0a68dd47abd2992e718da0dcc7bb302f0e9fe\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/99b235fd62a8a991cd3dcdb858e0a68dd47abd2992e718da0dcc7bb302f0e9fe\n    /docker/6422b44eb03096c62b9320f99199c842e1053f206fb1496835231e209407c87e:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/6422b44eb03096c62b9320f99199c842e1053f206fb1496835231e209407c87e\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/6422b44eb03096c62b9320f99199c842e1053f206fb1496835231e209407c87e\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/6422b44eb03096c62b9320f99199c842e1053f206fb1496835231e209407c87e\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/6422b44eb03096c62b9320f99199c842e1053f206fb1496835231e209407c87e\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/6422b44eb03096c62b9320f99199c842e1053f206fb1496835231e209407c87e\n    /docker/961f26eabb59e163a9d250e7a29926de99f5fe1c11820e855c414f1765c3347d:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/961f26eabb59e163a9d250e7a29926de99f5fe1c11820e855c414f1765c3347d\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/961f26eabb59e163a9d250e7a29926de99f5fe1c11820e855c414f1765c3347d\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/961f26eabb59e163a9d250e7a29926de99f5fe1c11820e855c414f1765c3347d\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/961f26eabb59e163a9d250e7a29926de99f5fe1c11820e855c414f1765c3347d\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/961f26eabb59e163a9d250e7a29926de99f5fe1c11820e855c414f1765c3347d\n    /docker/f5669f8c6d4a846c63b3976965f6cd8467984eb6e5bd7997ca7e7597d2805806:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/f5669f8c6d4a846c63b3976965f6cd8467984eb6e5bd7997ca7e7597d2805806\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/f5669f8c6d4a846c63b3976965f6cd8467984eb6e5bd7997ca7e7597d2805806\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/f5669f8c6d4a846c63b3976965f6cd8467984eb6e5bd7997ca7e7597d2805806\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/f5669f8c6d4a846c63b3976965f6cd8467984eb6e5bd7997ca7e7597d2805806\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/f5669f8c6d4a846c63b3976965f6cd8467984eb6e5bd7997ca7e7597d2805806\n    /docker/0e549d1609b03959fcac6027d3a53c339fe8b6a3d6bee57886679013969186a6:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/0e549d1609b03959fcac6027d3a53c339fe8b6a3d6bee57886679013969186a6\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/0e549d1609b03959fcac6027d3a53c339fe8b6a3d6bee57886679013969186a6\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/0e549d1609b03959fcac6027d3a53c339fe8b6a3d6bee57886679013969186a6\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/0e549d1609b03959fcac6027d3a53c339fe8b6a3d6bee57886679013969186a6\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/0e549d1609b03959fcac6027d3a53c339fe8b6a3d6bee57886679013969186a6\n    /docker/c9647f5f0486d61bc3d644df3a385f33d1108f00828fdd4a3dfe41f9259db69f:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/c9647f5f0486d61bc3d644df3a385f33d1108f00828fdd4a3dfe41f9259db69f\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/c9647f5f0486d61bc3d644df3a385f33d1108f00828fdd4a3dfe41f9259db69f\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/c9647f5f0486d61bc3d644df3a385f33d1108f00828fdd4a3dfe41f9259db69f\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/c9647f5f0486d61bc3d644df3a385f33d1108f00828fdd4a3dfe41f9259db69f\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/c9647f5f0486d61bc3d644df3a385f33d1108f00828fdd4a3dfe41f9259db69f\n    /docker/8a578abd29af1b7bd4289e73fb347af6b0b8bcf4137cfc3242d1473f7b5d6fa4:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/8a578abd29af1b7bd4289e73fb347af6b0b8bcf4137cfc3242d1473f7b5d6fa4\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/8a578abd29af1b7bd4289e73fb347af6b0b8bcf4137cfc3242d1473f7b5d6fa4\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/8a578abd29af1b7bd4289e73fb347af6b0b8bcf4137cfc3242d1473f7b5d6fa4\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/8a578abd29af1b7bd4289e73fb347af6b0b8bcf4137cfc3242d1473f7b5d6fa4\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/8a578abd29af1b7bd4289e73fb347af6b0b8bcf4137cfc3242d1473f7b5d6fa4\n    /docker/9b4330ef04a359c014cac2bc86275fa7c9140535952f0ee467bd080b8cf4fed9:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/9b4330ef04a359c014cac2bc86275fa7c9140535952f0ee467bd080b8cf4fed9\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/9b4330ef04a359c014cac2bc86275fa7c9140535952f0ee467bd080b8cf4fed9\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/9b4330ef04a359c014cac2bc86275fa7c9140535952f0ee467bd080b8cf4fed9\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/9b4330ef04a359c014cac2bc86275fa7c9140535952f0ee467bd080b8cf4fed9\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/9b4330ef04a359c014cac2bc86275fa7c9140535952f0ee467bd080b8cf4fed9\n    /user/1001.user:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/user/1001.user\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/user/1001.user\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/user/1001.user\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/user/1001.user\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/user/1001.user\n    /docker/9046e31cabb699df8f8aca664b7a55fa5d2456f4f25376ca01a042cf1446cb55:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/9046e31cabb699df8f8aca664b7a55fa5d2456f4f25376ca01a042cf1446cb55\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/9046e31cabb699df8f8aca664b7a55fa5d2456f4f25376ca01a042cf1446cb55\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/9046e31cabb699df8f8aca664b7a55fa5d2456f4f25376ca01a042cf1446cb55\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/9046e31cabb699df8f8aca664b7a55fa5d2456f4f25376ca01a042cf1446cb55\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/9046e31cabb699df8f8aca664b7a55fa5d2456f4f25376ca01a042cf1446cb55\n    /docker/dabab65e3bd018c1eff13d974d2b9679dee78308e9a8f531a50d0b5e95c81a49:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/dabab65e3bd018c1eff13d974d2b9679dee78308e9a8f531a50d0b5e95c81a49\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/dabab65e3bd018c1eff13d974d2b9679dee78308e9a8f531a50d0b5e95c81a49\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/dabab65e3bd018c1eff13d974d2b9679dee78308e9a8f531a50d0b5e95c81a49\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/dabab65e3bd018c1eff13d974d2b9679dee78308e9a8f531a50d0b5e95c81a49\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/dabab65e3bd018c1eff13d974d2b9679dee78308e9a8f531a50d0b5e95c81a49\n    /docker/77163430fd443191da82e20fc5c45e78e4adaa96151982f01677d6ea9114628a:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/77163430fd443191da82e20fc5c45e78e4adaa96151982f01677d6ea9114628a\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/77163430fd443191da82e20fc5c45e78e4adaa96151982f01677d6ea9114628a\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/77163430fd443191da82e20fc5c45e78e4adaa96151982f01677d6ea9114628a\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/77163430fd443191da82e20fc5c45e78e4adaa96151982f01677d6ea9114628a\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/77163430fd443191da82e20fc5c45e78e4adaa96151982f01677d6ea9114628a\n    /docker/2f5edc676c327406b00be7b8e0cb888252028cecb36da7f08ea88382f25819ae:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/2f5edc676c327406b00be7b8e0cb888252028cecb36da7f08ea88382f25819ae\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/2f5edc676c327406b00be7b8e0cb888252028cecb36da7f08ea88382f25819ae\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/2f5edc676c327406b00be7b8e0cb888252028cecb36da7f08ea88382f25819ae\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/2f5edc676c327406b00be7b8e0cb888252028cecb36da7f08ea88382f25819ae\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/2f5edc676c327406b00be7b8e0cb888252028cecb36da7f08ea88382f25819ae\n    /docker/d11a990b17f26f5b733ef2196e6842aab2c97141afcce6e71cdf5dbddccfbac4:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/d11a990b17f26f5b733ef2196e6842aab2c97141afcce6e71cdf5dbddccfbac4\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/d11a990b17f26f5b733ef2196e6842aab2c97141afcce6e71cdf5dbddccfbac4\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/d11a990b17f26f5b733ef2196e6842aab2c97141afcce6e71cdf5dbddccfbac4\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/d11a990b17f26f5b733ef2196e6842aab2c97141afcce6e71cdf5dbddccfbac4\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/d11a990b17f26f5b733ef2196e6842aab2c97141afcce6e71cdf5dbddccfbac4\n    /user/1001.user/14724.session:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/user/1001.user/14724.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/user/1001.user/14724.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/user/1001.user/14724.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/user/1001.user/14724.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/user/1001.user/14724.session\n    /docker/3e9f54b61cc0423f9fb8f4fc3bde2ed026651a9c8485246e13408763fafc9a0b:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/3e9f54b61cc0423f9fb8f4fc3bde2ed026651a9c8485246e13408763fafc9a0b\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/3e9f54b61cc0423f9fb8f4fc3bde2ed026651a9c8485246e13408763fafc9a0b\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/3e9f54b61cc0423f9fb8f4fc3bde2ed026651a9c8485246e13408763fafc9a0b\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/3e9f54b61cc0423f9fb8f4fc3bde2ed026651a9c8485246e13408763fafc9a0b\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/3e9f54b61cc0423f9fb8f4fc3bde2ed026651a9c8485246e13408763fafc9a0b\n    /docker/05372d45d781d9a4a858ab084fa015e775d6d0a8f2c1effe36462f59022bb492:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/05372d45d781d9a4a858ab084fa015e775d6d0a8f2c1effe36462f59022bb492\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/05372d45d781d9a4a858ab084fa015e775d6d0a8f2c1effe36462f59022bb492\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/05372d45d781d9a4a858ab084fa015e775d6d0a8f2c1effe36462f59022bb492\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/05372d45d781d9a4a858ab084fa015e775d6d0a8f2c1effe36462f59022bb492\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/05372d45d781d9a4a858ab084fa015e775d6d0a8f2c1effe36462f59022bb492\n    /docker/a0740256c9bccb2d7a5d810ecc20d24c7b1cf4cf524179f185bcafc8b949fd2c:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/a0740256c9bccb2d7a5d810ecc20d24c7b1cf4cf524179f185bcafc8b949fd2c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/a0740256c9bccb2d7a5d810ecc20d24c7b1cf4cf524179f185bcafc8b949fd2c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/a0740256c9bccb2d7a5d810ecc20d24c7b1cf4cf524179f185bcafc8b949fd2c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/a0740256c9bccb2d7a5d810ecc20d24c7b1cf4cf524179f185bcafc8b949fd2c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/a0740256c9bccb2d7a5d810ecc20d24c7b1cf4cf524179f185bcafc8b949fd2c\n    /docker/735c9fb5262a9e6dbd7ca89f41a41b6e622797fe90e345fdc29b67f6d6394416:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/735c9fb5262a9e6dbd7ca89f41a41b6e622797fe90e345fdc29b67f6d6394416\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/735c9fb5262a9e6dbd7ca89f41a41b6e622797fe90e345fdc29b67f6d6394416\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/735c9fb5262a9e6dbd7ca89f41a41b6e622797fe90e345fdc29b67f6d6394416\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/735c9fb5262a9e6dbd7ca89f41a41b6e622797fe90e345fdc29b67f6d6394416\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/735c9fb5262a9e6dbd7ca89f41a41b6e622797fe90e345fdc29b67f6d6394416\n    /docker/b79605c1bd4c53f54637781e963cb98d4a45af1aef235f0a6dde7b5550c2d6e4:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/b79605c1bd4c53f54637781e963cb98d4a45af1aef235f0a6dde7b5550c2d6e4\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/b79605c1bd4c53f54637781e963cb98d4a45af1aef235f0a6dde7b5550c2d6e4\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/b79605c1bd4c53f54637781e963cb98d4a45af1aef235f0a6dde7b5550c2d6e4\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/b79605c1bd4c53f54637781e963cb98d4a45af1aef235f0a6dde7b5550c2d6e4\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/b79605c1bd4c53f54637781e963cb98d4a45af1aef235f0a6dde7b5550c2d6e4\n    /docker/1a1a60a24db12c1a4e93442e5107b6978ee04fdcd1d000fdc22d829f5e1f48cc:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/1a1a60a24db12c1a4e93442e5107b6978ee04fdcd1d000fdc22d829f5e1f48cc\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/1a1a60a24db12c1a4e93442e5107b6978ee04fdcd1d000fdc22d829f5e1f48cc\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/1a1a60a24db12c1a4e93442e5107b6978ee04fdcd1d000fdc22d829f5e1f48cc\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/1a1a60a24db12c1a4e93442e5107b6978ee04fdcd1d000fdc22d829f5e1f48cc\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/1a1a60a24db12c1a4e93442e5107b6978ee04fdcd1d000fdc22d829f5e1f48cc\n    /docker/4c56e1d8e67a4c899fa1f129c46e7dda0307c866a598ffb17d36e90ec41492c9:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/4c56e1d8e67a4c899fa1f129c46e7dda0307c866a598ffb17d36e90ec41492c9\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/4c56e1d8e67a4c899fa1f129c46e7dda0307c866a598ffb17d36e90ec41492c9\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/4c56e1d8e67a4c899fa1f129c46e7dda0307c866a598ffb17d36e90ec41492c9\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/4c56e1d8e67a4c899fa1f129c46e7dda0307c866a598ffb17d36e90ec41492c9\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/4c56e1d8e67a4c899fa1f129c46e7dda0307c866a598ffb17d36e90ec41492c9\n    /docker/12376364e1b9663dfe37605670aa2d8e356b6d08c406a94e09b2a455f1b95b76:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/12376364e1b9663dfe37605670aa2d8e356b6d08c406a94e09b2a455f1b95b76\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/12376364e1b9663dfe37605670aa2d8e356b6d08c406a94e09b2a455f1b95b76\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/12376364e1b9663dfe37605670aa2d8e356b6d08c406a94e09b2a455f1b95b76\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/12376364e1b9663dfe37605670aa2d8e356b6d08c406a94e09b2a455f1b95b76\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/12376364e1b9663dfe37605670aa2d8e356b6d08c406a94e09b2a455f1b95b76\n    /docker/8e4af215c6fe9564e648824209330e50381750fa84e9b222a13ae653a0407880:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/8e4af215c6fe9564e648824209330e50381750fa84e9b222a13ae653a0407880\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/8e4af215c6fe9564e648824209330e50381750fa84e9b222a13ae653a0407880\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/8e4af215c6fe9564e648824209330e50381750fa84e9b222a13ae653a0407880\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/8e4af215c6fe9564e648824209330e50381750fa84e9b222a13ae653a0407880\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/8e4af215c6fe9564e648824209330e50381750fa84e9b222a13ae653a0407880\n    /docker/5aac9caa327d850304a07e4014140bbadd6dcfdb2ae0ae7aceea72b562e5cfcc:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/5aac9caa327d850304a07e4014140bbadd6dcfdb2ae0ae7aceea72b562e5cfcc\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/5aac9caa327d850304a07e4014140bbadd6dcfdb2ae0ae7aceea72b562e5cfcc\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/5aac9caa327d850304a07e4014140bbadd6dcfdb2ae0ae7aceea72b562e5cfcc\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/5aac9caa327d850304a07e4014140bbadd6dcfdb2ae0ae7aceea72b562e5cfcc\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/5aac9caa327d850304a07e4014140bbadd6dcfdb2ae0ae7aceea72b562e5cfcc\n    /docker/820d3ee57c911f929f782d533b322965d772b30c758b12d9efa3618a231c083d:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/820d3ee57c911f929f782d533b322965d772b30c758b12d9efa3618a231c083d\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/820d3ee57c911f929f782d533b322965d772b30c758b12d9efa3618a231c083d\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/820d3ee57c911f929f782d533b322965d772b30c758b12d9efa3618a231c083d\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/820d3ee57c911f929f782d533b322965d772b30c758b12d9efa3618a231c083d\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/820d3ee57c911f929f782d533b322965d772b30c758b12d9efa3618a231c083d\n    /docker/e2e545d6bf402778d334a03fa0a4575e29a96c82af4f474f1267cdf83df40c9f:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/e2e545d6bf402778d334a03fa0a4575e29a96c82af4f474f1267cdf83df40c9f\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/e2e545d6bf402778d334a03fa0a4575e29a96c82af4f474f1267cdf83df40c9f\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/e2e545d6bf402778d334a03fa0a4575e29a96c82af4f474f1267cdf83df40c9f\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/e2e545d6bf402778d334a03fa0a4575e29a96c82af4f474f1267cdf83df40c9f\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/e2e545d6bf402778d334a03fa0a4575e29a96c82af4f474f1267cdf83df40c9f\n    /docker/b54e2858e7ef98dc8ed8d1bdac6f623e5bf66b77535c14dfe65233e035484fe6:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/b54e2858e7ef98dc8ed8d1bdac6f623e5bf66b77535c14dfe65233e035484fe6\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/b54e2858e7ef98dc8ed8d1bdac6f623e5bf66b77535c14dfe65233e035484fe6\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/b54e2858e7ef98dc8ed8d1bdac6f623e5bf66b77535c14dfe65233e035484fe6\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/b54e2858e7ef98dc8ed8d1bdac6f623e5bf66b77535c14dfe65233e035484fe6\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/b54e2858e7ef98dc8ed8d1bdac6f623e5bf66b77535c14dfe65233e035484fe6\n    /docker/206d8ff33742a58d4267a5146fe4a2ba2a49e2244ebe47f3424e1a5c22af9e06:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/206d8ff33742a58d4267a5146fe4a2ba2a49e2244ebe47f3424e1a5c22af9e06\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/206d8ff33742a58d4267a5146fe4a2ba2a49e2244ebe47f3424e1a5c22af9e06\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/206d8ff33742a58d4267a5146fe4a2ba2a49e2244ebe47f3424e1a5c22af9e06\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/206d8ff33742a58d4267a5146fe4a2ba2a49e2244ebe47f3424e1a5c22af9e06\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/206d8ff33742a58d4267a5146fe4a2ba2a49e2244ebe47f3424e1a5c22af9e06\n    /user/1002.user/153.session:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/user/1002.user/153.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/user/1002.user/153.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/user/1002.user/153.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/user/1002.user/153.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/user/1002.user/153.session\n    /docker/1f3d7a107248048e7c85a02a9156213ab9072fca75368967c658b77ee7f1a681:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/1f3d7a107248048e7c85a02a9156213ab9072fca75368967c658b77ee7f1a681\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/1f3d7a107248048e7c85a02a9156213ab9072fca75368967c658b77ee7f1a681\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/1f3d7a107248048e7c85a02a9156213ab9072fca75368967c658b77ee7f1a681\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/1f3d7a107248048e7c85a02a9156213ab9072fca75368967c658b77ee7f1a681\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/1f3d7a107248048e7c85a02a9156213ab9072fca75368967c658b77ee7f1a681\n    /user/1003.user:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/user/1003.user\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/user/1003.user\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/user/1003.user\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/user/1003.user\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/user/1003.user\n    /user/1002.user:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/user/1002.user\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/user/1002.user\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/user/1002.user\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/user/1002.user\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/user/1002.user\n    /docker/f8f22a0fa6c9a58a6efea94901b03e7075b97f23fbec7962d625c2547b21e029:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/f8f22a0fa6c9a58a6efea94901b03e7075b97f23fbec7962d625c2547b21e029\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/f8f22a0fa6c9a58a6efea94901b03e7075b97f23fbec7962d625c2547b21e029\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/f8f22a0fa6c9a58a6efea94901b03e7075b97f23fbec7962d625c2547b21e029\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/f8f22a0fa6c9a58a6efea94901b03e7075b97f23fbec7962d625c2547b21e029\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/f8f22a0fa6c9a58a6efea94901b03e7075b97f23fbec7962d625c2547b21e029\n    /docker/0ba20126fcc10cdda9f5067284037f95197610f67b8a63c76014d86110a7e546:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/0ba20126fcc10cdda9f5067284037f95197610f67b8a63c76014d86110a7e546\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/0ba20126fcc10cdda9f5067284037f95197610f67b8a63c76014d86110a7e546\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/0ba20126fcc10cdda9f5067284037f95197610f67b8a63c76014d86110a7e546\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/0ba20126fcc10cdda9f5067284037f95197610f67b8a63c76014d86110a7e546\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/0ba20126fcc10cdda9f5067284037f95197610f67b8a63c76014d86110a7e546\n    /docker/7cc5d267c5d49ff3d3725aca475177824c2aeb889a62baf0c3ade55f71ba4899:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/7cc5d267c5d49ff3d3725aca475177824c2aeb889a62baf0c3ade55f71ba4899\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/7cc5d267c5d49ff3d3725aca475177824c2aeb889a62baf0c3ade55f71ba4899\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/7cc5d267c5d49ff3d3725aca475177824c2aeb889a62baf0c3ade55f71ba4899\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/7cc5d267c5d49ff3d3725aca475177824c2aeb889a62baf0c3ade55f71ba4899\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/7cc5d267c5d49ff3d3725aca475177824c2aeb889a62baf0c3ade55f71ba4899\n    /docker/d2a239bbd9ce62a7aee6636196f8f52dc61100bbac48cca6f121b1b12ff41c89:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/d2a239bbd9ce62a7aee6636196f8f52dc61100bbac48cca6f121b1b12ff41c89\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/d2a239bbd9ce62a7aee6636196f8f52dc61100bbac48cca6f121b1b12ff41c89\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/d2a239bbd9ce62a7aee6636196f8f52dc61100bbac48cca6f121b1b12ff41c89\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/d2a239bbd9ce62a7aee6636196f8f52dc61100bbac48cca6f121b1b12ff41c89\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/d2a239bbd9ce62a7aee6636196f8f52dc61100bbac48cca6f121b1b12ff41c89\n    /docker/feae778b29b082333b80d155555fb757d1756dc5fcbbb39c094b3aabce5f0ae8:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/feae778b29b082333b80d155555fb757d1756dc5fcbbb39c094b3aabce5f0ae8\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/feae778b29b082333b80d155555fb757d1756dc5fcbbb39c094b3aabce5f0ae8\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/feae778b29b082333b80d155555fb757d1756dc5fcbbb39c094b3aabce5f0ae8\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/feae778b29b082333b80d155555fb757d1756dc5fcbbb39c094b3aabce5f0ae8\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/feae778b29b082333b80d155555fb757d1756dc5fcbbb39c094b3aabce5f0ae8\n    /docker/a572e9c8386240d8328c315e853f08411803c9bccac53693c4091e3cf0cfdbe0:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/a572e9c8386240d8328c315e853f08411803c9bccac53693c4091e3cf0cfdbe0\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/a572e9c8386240d8328c315e853f08411803c9bccac53693c4091e3cf0cfdbe0\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/a572e9c8386240d8328c315e853f08411803c9bccac53693c4091e3cf0cfdbe0\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/a572e9c8386240d8328c315e853f08411803c9bccac53693c4091e3cf0cfdbe0\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/a572e9c8386240d8328c315e853f08411803c9bccac53693c4091e3cf0cfdbe0\n    /user/1003.user/5468.session:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/user/1003.user/5468.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/user/1003.user/5468.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/user/1003.user/5468.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/user/1003.user/5468.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/user/1003.user/5468.session\n    /docker/17b663f78597e16149470e45d741a30ee3a3e77768cdfd61a5049a4459b0d3c7:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/17b663f78597e16149470e45d741a30ee3a3e77768cdfd61a5049a4459b0d3c7\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/17b663f78597e16149470e45d741a30ee3a3e77768cdfd61a5049a4459b0d3c7\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/17b663f78597e16149470e45d741a30ee3a3e77768cdfd61a5049a4459b0d3c7\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/17b663f78597e16149470e45d741a30ee3a3e77768cdfd61a5049a4459b0d3c7\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/17b663f78597e16149470e45d741a30ee3a3e77768cdfd61a5049a4459b0d3c7\n    /docker/073a1ae3f3a0dfef58dc747f1e596ee8e31168bbcd9dbd07c7186ec8c63f7eff:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/073a1ae3f3a0dfef58dc747f1e596ee8e31168bbcd9dbd07c7186ec8c63f7eff\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/073a1ae3f3a0dfef58dc747f1e596ee8e31168bbcd9dbd07c7186ec8c63f7eff\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/073a1ae3f3a0dfef58dc747f1e596ee8e31168bbcd9dbd07c7186ec8c63f7eff\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/073a1ae3f3a0dfef58dc747f1e596ee8e31168bbcd9dbd07c7186ec8c63f7eff\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/073a1ae3f3a0dfef58dc747f1e596ee8e31168bbcd9dbd07c7186ec8c63f7eff\n    /docker/ccf6cfa182aa618be90d39f27f4d71f2f31d1e353ca9f9c6b1ad0cee8f1e4478:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/ccf6cfa182aa618be90d39f27f4d71f2f31d1e353ca9f9c6b1ad0cee8f1e4478\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/ccf6cfa182aa618be90d39f27f4d71f2f31d1e353ca9f9c6b1ad0cee8f1e4478\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/ccf6cfa182aa618be90d39f27f4d71f2f31d1e353ca9f9c6b1ad0cee8f1e4478\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/ccf6cfa182aa618be90d39f27f4d71f2f31d1e353ca9f9c6b1ad0cee8f1e4478\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/ccf6cfa182aa618be90d39f27f4d71f2f31d1e353ca9f9c6b1ad0cee8f1e4478\n    /docker/0a3e932bbb933d544cf646d0591f15e4c55a31540aa8089fc9a6848d2956eb81:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/0a3e932bbb933d544cf646d0591f15e4c55a31540aa8089fc9a6848d2956eb81\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/0a3e932bbb933d544cf646d0591f15e4c55a31540aa8089fc9a6848d2956eb81\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/0a3e932bbb933d544cf646d0591f15e4c55a31540aa8089fc9a6848d2956eb81\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/0a3e932bbb933d544cf646d0591f15e4c55a31540aa8089fc9a6848d2956eb81\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/0a3e932bbb933d544cf646d0591f15e4c55a31540aa8089fc9a6848d2956eb81\n    /docker/65c4848df58453224f08a51bbb4274b307beb263efa676faec777e9d06b1c5d6:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/65c4848df58453224f08a51bbb4274b307beb263efa676faec777e9d06b1c5d6\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/65c4848df58453224f08a51bbb4274b307beb263efa676faec777e9d06b1c5d6\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/65c4848df58453224f08a51bbb4274b307beb263efa676faec777e9d06b1c5d6\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/65c4848df58453224f08a51bbb4274b307beb263efa676faec777e9d06b1c5d6\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/65c4848df58453224f08a51bbb4274b307beb263efa676faec777e9d06b1c5d6\n    /docker/5ee774a67313cd2cfc61aae603f849a99cc0fe025b836d3b925b794ffd3615a7:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/5ee774a67313cd2cfc61aae603f849a99cc0fe025b836d3b925b794ffd3615a7\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/5ee774a67313cd2cfc61aae603f849a99cc0fe025b836d3b925b794ffd3615a7\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/5ee774a67313cd2cfc61aae603f849a99cc0fe025b836d3b925b794ffd3615a7\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/5ee774a67313cd2cfc61aae603f849a99cc0fe025b836d3b925b794ffd3615a7\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/5ee774a67313cd2cfc61aae603f849a99cc0fe025b836d3b925b794ffd3615a7\n    /user/1003.user/205.session:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/user/1003.user/205.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/user/1003.user/205.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/user/1003.user/205.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/user/1003.user/205.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/user/1003.user/205.session\n    /user/0.user/14096.session:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/user/0.user/14096.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/user/0.user/14096.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/user/0.user/14096.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/user/0.user/14096.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/user/0.user/14096.session\n    /docker/3c305d1d145b6d3b5000d6a6698f7dc05767f51c0c8605195b79faa3fdfc0b0b:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/3c305d1d145b6d3b5000d6a6698f7dc05767f51c0c8605195b79faa3fdfc0b0b\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/3c305d1d145b6d3b5000d6a6698f7dc05767f51c0c8605195b79faa3fdfc0b0b\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/3c305d1d145b6d3b5000d6a6698f7dc05767f51c0c8605195b79faa3fdfc0b0b\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/3c305d1d145b6d3b5000d6a6698f7dc05767f51c0c8605195b79faa3fdfc0b0b\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/3c305d1d145b6d3b5000d6a6698f7dc05767f51c0c8605195b79faa3fdfc0b0b\n    /docker/64f1e8fb112a40d156020ccdebe92b0878a734aa3f80420e306bae09d92bc51a:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/64f1e8fb112a40d156020ccdebe92b0878a734aa3f80420e306bae09d92bc51a\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/64f1e8fb112a40d156020ccdebe92b0878a734aa3f80420e306bae09d92bc51a\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/64f1e8fb112a40d156020ccdebe92b0878a734aa3f80420e306bae09d92bc51a\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/64f1e8fb112a40d156020ccdebe92b0878a734aa3f80420e306bae09d92bc51a\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/64f1e8fb112a40d156020ccdebe92b0878a734aa3f80420e306bae09d92bc51a\n    /docker/1874bc34c1a1a34abb6fdf0d66c0e52cee12acf55e5553ec1bd24dfce5a4b323:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/1874bc34c1a1a34abb6fdf0d66c0e52cee12acf55e5553ec1bd24dfce5a4b323\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/1874bc34c1a1a34abb6fdf0d66c0e52cee12acf55e5553ec1bd24dfce5a4b323\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/1874bc34c1a1a34abb6fdf0d66c0e52cee12acf55e5553ec1bd24dfce5a4b323\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/1874bc34c1a1a34abb6fdf0d66c0e52cee12acf55e5553ec1bd24dfce5a4b323\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/1874bc34c1a1a34abb6fdf0d66c0e52cee12acf55e5553ec1bd24dfce5a4b323\n    /docker/934739704e1f74efdd627f14a36cb691bb80c26d553b6cfdeb14bf1758b4a614:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/934739704e1f74efdd627f14a36cb691bb80c26d553b6cfdeb14bf1758b4a614\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/934739704e1f74efdd627f14a36cb691bb80c26d553b6cfdeb14bf1758b4a614\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/934739704e1f74efdd627f14a36cb691bb80c26d553b6cfdeb14bf1758b4a614\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/934739704e1f74efdd627f14a36cb691bb80c26d553b6cfdeb14bf1758b4a614\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/934739704e1f74efdd627f14a36cb691bb80c26d553b6cfdeb14bf1758b4a614\n    /docker/a183ebeab1f3abd6e8cb7a48e11a81dc5b1b95c818b914a9eb0988975a2cc807:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/a183ebeab1f3abd6e8cb7a48e11a81dc5b1b95c818b914a9eb0988975a2cc807\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/a183ebeab1f3abd6e8cb7a48e11a81dc5b1b95c818b914a9eb0988975a2cc807\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/a183ebeab1f3abd6e8cb7a48e11a81dc5b1b95c818b914a9eb0988975a2cc807\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/a183ebeab1f3abd6e8cb7a48e11a81dc5b1b95c818b914a9eb0988975a2cc807\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/a183ebeab1f3abd6e8cb7a48e11a81dc5b1b95c818b914a9eb0988975a2cc807\n    /docker/6cc4ddef9728d1b427cf22374267a87c1b7739f0673e44f08a4fcd457246f595:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/6cc4ddef9728d1b427cf22374267a87c1b7739f0673e44f08a4fcd457246f595\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/6cc4ddef9728d1b427cf22374267a87c1b7739f0673e44f08a4fcd457246f595\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/6cc4ddef9728d1b427cf22374267a87c1b7739f0673e44f08a4fcd457246f595\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/6cc4ddef9728d1b427cf22374267a87c1b7739f0673e44f08a4fcd457246f595\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/6cc4ddef9728d1b427cf22374267a87c1b7739f0673e44f08a4fcd457246f595\n    /docker/43271ec0e117db15c2afcb6f40515e4c9ddc6d5d7c5205ec880a37def79df114:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/43271ec0e117db15c2afcb6f40515e4c9ddc6d5d7c5205ec880a37def79df114\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/43271ec0e117db15c2afcb6f40515e4c9ddc6d5d7c5205ec880a37def79df114\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/43271ec0e117db15c2afcb6f40515e4c9ddc6d5d7c5205ec880a37def79df114\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/43271ec0e117db15c2afcb6f40515e4c9ddc6d5d7c5205ec880a37def79df114\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/43271ec0e117db15c2afcb6f40515e4c9ddc6d5d7c5205ec880a37def79df114\n    /docker/9bd781da738343d8939c6d65e730370ff0490588909f9892e725c4521838dedb:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/9bd781da738343d8939c6d65e730370ff0490588909f9892e725c4521838dedb\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/9bd781da738343d8939c6d65e730370ff0490588909f9892e725c4521838dedb\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/9bd781da738343d8939c6d65e730370ff0490588909f9892e725c4521838dedb\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/9bd781da738343d8939c6d65e730370ff0490588909f9892e725c4521838dedb\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/9bd781da738343d8939c6d65e730370ff0490588909f9892e725c4521838dedb\n    /docker/f631f589864b89c294dd4f8a0220a81ae489fa36ca108e17422e9ee1ac2765ed:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/f631f589864b89c294dd4f8a0220a81ae489fa36ca108e17422e9ee1ac2765ed\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/f631f589864b89c294dd4f8a0220a81ae489fa36ca108e17422e9ee1ac2765ed\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/f631f589864b89c294dd4f8a0220a81ae489fa36ca108e17422e9ee1ac2765ed\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/f631f589864b89c294dd4f8a0220a81ae489fa36ca108e17422e9ee1ac2765ed\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/f631f589864b89c294dd4f8a0220a81ae489fa36ca108e17422e9ee1ac2765ed\n    /docker/a717f0fcc29dcc2f61156c883a4febc91f3f307cbabcba1456e89198a3783268:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/a717f0fcc29dcc2f61156c883a4febc91f3f307cbabcba1456e89198a3783268\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/a717f0fcc29dcc2f61156c883a4febc91f3f307cbabcba1456e89198a3783268\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/a717f0fcc29dcc2f61156c883a4febc91f3f307cbabcba1456e89198a3783268\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/a717f0fcc29dcc2f61156c883a4febc91f3f307cbabcba1456e89198a3783268\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/a717f0fcc29dcc2f61156c883a4febc91f3f307cbabcba1456e89198a3783268\n    /docker/201c9549b6c773cba97e94c1cdc6ae5893f818fa7f5622cfd1219169c63e8595:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/201c9549b6c773cba97e94c1cdc6ae5893f818fa7f5622cfd1219169c63e8595\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/201c9549b6c773cba97e94c1cdc6ae5893f818fa7f5622cfd1219169c63e8595\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/201c9549b6c773cba97e94c1cdc6ae5893f818fa7f5622cfd1219169c63e8595\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/201c9549b6c773cba97e94c1cdc6ae5893f818fa7f5622cfd1219169c63e8595\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/201c9549b6c773cba97e94c1cdc6ae5893f818fa7f5622cfd1219169c63e8595\n    /docker/c3a62a0de5a4b9c40b38a774f7c146c6374b426ae02ca2622fbef8fa3a97bcc0:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/c3a62a0de5a4b9c40b38a774f7c146c6374b426ae02ca2622fbef8fa3a97bcc0\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/c3a62a0de5a4b9c40b38a774f7c146c6374b426ae02ca2622fbef8fa3a97bcc0\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/c3a62a0de5a4b9c40b38a774f7c146c6374b426ae02ca2622fbef8fa3a97bcc0\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/c3a62a0de5a4b9c40b38a774f7c146c6374b426ae02ca2622fbef8fa3a97bcc0\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/c3a62a0de5a4b9c40b38a774f7c146c6374b426ae02ca2622fbef8fa3a97bcc0\n    /docker/2533f18ca64069cb09ea33e166e8f078f3b5421e8c76c06b635a57e9b4a135a8:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/2533f18ca64069cb09ea33e166e8f078f3b5421e8c76c06b635a57e9b4a135a8\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/2533f18ca64069cb09ea33e166e8f078f3b5421e8c76c06b635a57e9b4a135a8\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/2533f18ca64069cb09ea33e166e8f078f3b5421e8c76c06b635a57e9b4a135a8\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/2533f18ca64069cb09ea33e166e8f078f3b5421e8c76c06b635a57e9b4a135a8\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/2533f18ca64069cb09ea33e166e8f078f3b5421e8c76c06b635a57e9b4a135a8\n    /docker/333a720029089d3020a6f86fe5c08c34793021acfa6da6bec8f5f59c5ba6534a:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/333a720029089d3020a6f86fe5c08c34793021acfa6da6bec8f5f59c5ba6534a\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/333a720029089d3020a6f86fe5c08c34793021acfa6da6bec8f5f59c5ba6534a\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/333a720029089d3020a6f86fe5c08c34793021acfa6da6bec8f5f59c5ba6534a\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/333a720029089d3020a6f86fe5c08c34793021acfa6da6bec8f5f59c5ba6534a\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/333a720029089d3020a6f86fe5c08c34793021acfa6da6bec8f5f59c5ba6534a\n    /docker/801f69a05706bdabe196c906a86bf6a8e87f8f39b56cea78f0a6a26e53b9967d:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/801f69a05706bdabe196c906a86bf6a8e87f8f39b56cea78f0a6a26e53b9967d\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/801f69a05706bdabe196c906a86bf6a8e87f8f39b56cea78f0a6a26e53b9967d\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/801f69a05706bdabe196c906a86bf6a8e87f8f39b56cea78f0a6a26e53b9967d\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/801f69a05706bdabe196c906a86bf6a8e87f8f39b56cea78f0a6a26e53b9967d\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/801f69a05706bdabe196c906a86bf6a8e87f8f39b56cea78f0a6a26e53b9967d\n    /docker/71c481eb8872046f45579f0547de104906fa92f1a7e3e98774af4459d5fceb5a:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/71c481eb8872046f45579f0547de104906fa92f1a7e3e98774af4459d5fceb5a\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/71c481eb8872046f45579f0547de104906fa92f1a7e3e98774af4459d5fceb5a\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/71c481eb8872046f45579f0547de104906fa92f1a7e3e98774af4459d5fceb5a\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/71c481eb8872046f45579f0547de104906fa92f1a7e3e98774af4459d5fceb5a\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/71c481eb8872046f45579f0547de104906fa92f1a7e3e98774af4459d5fceb5a\n    /docker/216b7daf60bb4c1b4d62a4944eb330f7be5ed8322dee4a183b4ee5fc0a2f2f74:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/216b7daf60bb4c1b4d62a4944eb330f7be5ed8322dee4a183b4ee5fc0a2f2f74\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/216b7daf60bb4c1b4d62a4944eb330f7be5ed8322dee4a183b4ee5fc0a2f2f74\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/216b7daf60bb4c1b4d62a4944eb330f7be5ed8322dee4a183b4ee5fc0a2f2f74\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/216b7daf60bb4c1b4d62a4944eb330f7be5ed8322dee4a183b4ee5fc0a2f2f74\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/216b7daf60bb4c1b4d62a4944eb330f7be5ed8322dee4a183b4ee5fc0a2f2f74\n    /docker/5f56cc399ea1c3f4c8cf2f249ff5721bfd5c1c709a8d9925e0166a668ad24dba:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/5f56cc399ea1c3f4c8cf2f249ff5721bfd5c1c709a8d9925e0166a668ad24dba\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/5f56cc399ea1c3f4c8cf2f249ff5721bfd5c1c709a8d9925e0166a668ad24dba\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/5f56cc399ea1c3f4c8cf2f249ff5721bfd5c1c709a8d9925e0166a668ad24dba\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/5f56cc399ea1c3f4c8cf2f249ff5721bfd5c1c709a8d9925e0166a668ad24dba\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/5f56cc399ea1c3f4c8cf2f249ff5721bfd5c1c709a8d9925e0166a668ad24dba\n    /docker/15a03de6f856f1ad427edf8df46b46c6a76820210a426f910745c07c248a8982:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/15a03de6f856f1ad427edf8df46b46c6a76820210a426f910745c07c248a8982\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/15a03de6f856f1ad427edf8df46b46c6a76820210a426f910745c07c248a8982\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/15a03de6f856f1ad427edf8df46b46c6a76820210a426f910745c07c248a8982\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/15a03de6f856f1ad427edf8df46b46c6a76820210a426f910745c07c248a8982\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/15a03de6f856f1ad427edf8df46b46c6a76820210a426f910745c07c248a8982\n    /docker/b82a11df73ffa474cee6fb9d2fa48504d161b99f6ab2b72a1bd2a1a6244035be:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/b82a11df73ffa474cee6fb9d2fa48504d161b99f6ab2b72a1bd2a1a6244035be\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/b82a11df73ffa474cee6fb9d2fa48504d161b99f6ab2b72a1bd2a1a6244035be\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/b82a11df73ffa474cee6fb9d2fa48504d161b99f6ab2b72a1bd2a1a6244035be\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/b82a11df73ffa474cee6fb9d2fa48504d161b99f6ab2b72a1bd2a1a6244035be\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/b82a11df73ffa474cee6fb9d2fa48504d161b99f6ab2b72a1bd2a1a6244035be\n    /docker/3ee96de8255aaada7c4bbc637aac319f3a71660d01e1e4e767801b7961e894f4:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/3ee96de8255aaada7c4bbc637aac319f3a71660d01e1e4e767801b7961e894f4\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/3ee96de8255aaada7c4bbc637aac319f3a71660d01e1e4e767801b7961e894f4\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/3ee96de8255aaada7c4bbc637aac319f3a71660d01e1e4e767801b7961e894f4\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/3ee96de8255aaada7c4bbc637aac319f3a71660d01e1e4e767801b7961e894f4\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/3ee96de8255aaada7c4bbc637aac319f3a71660d01e1e4e767801b7961e894f4\n    /docker/a20c5f5eee36e1ac43dce7196da9d024cb649f12701e6ff4515c05224b2a0744:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/a20c5f5eee36e1ac43dce7196da9d024cb649f12701e6ff4515c05224b2a0744\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/a20c5f5eee36e1ac43dce7196da9d024cb649f12701e6ff4515c05224b2a0744\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/a20c5f5eee36e1ac43dce7196da9d024cb649f12701e6ff4515c05224b2a0744\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/a20c5f5eee36e1ac43dce7196da9d024cb649f12701e6ff4515c05224b2a0744\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/a20c5f5eee36e1ac43dce7196da9d024cb649f12701e6ff4515c05224b2a0744\n    /docker/5b8af4f21a6e516615f25e3900cf1918e405f4a61f61c473415ccb9f706ff47d:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/5b8af4f21a6e516615f25e3900cf1918e405f4a61f61c473415ccb9f706ff47d\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/5b8af4f21a6e516615f25e3900cf1918e405f4a61f61c473415ccb9f706ff47d\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/5b8af4f21a6e516615f25e3900cf1918e405f4a61f61c473415ccb9f706ff47d\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/5b8af4f21a6e516615f25e3900cf1918e405f4a61f61c473415ccb9f706ff47d\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/5b8af4f21a6e516615f25e3900cf1918e405f4a61f61c473415ccb9f706ff47d\n    /docker/40cfeb35bddd160076ec87102054f25ac47237acabc6e5d4f2a89e8e4b821f16:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/40cfeb35bddd160076ec87102054f25ac47237acabc6e5d4f2a89e8e4b821f16\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/40cfeb35bddd160076ec87102054f25ac47237acabc6e5d4f2a89e8e4b821f16\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/40cfeb35bddd160076ec87102054f25ac47237acabc6e5d4f2a89e8e4b821f16\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/40cfeb35bddd160076ec87102054f25ac47237acabc6e5d4f2a89e8e4b821f16\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/40cfeb35bddd160076ec87102054f25ac47237acabc6e5d4f2a89e8e4b821f16\n    /docker/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc\n    /docker/45dd95723c755d1b2970bb5403841341f176e7fe5d129edb1d205ea03299e16b:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/45dd95723c755d1b2970bb5403841341f176e7fe5d129edb1d205ea03299e16b\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/45dd95723c755d1b2970bb5403841341f176e7fe5d129edb1d205ea03299e16b\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/45dd95723c755d1b2970bb5403841341f176e7fe5d129edb1d205ea03299e16b\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/45dd95723c755d1b2970bb5403841341f176e7fe5d129edb1d205ea03299e16b\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/45dd95723c755d1b2970bb5403841341f176e7fe5d129edb1d205ea03299e16b\n    /docker/e3497bcedfaf4d0ec65d20aec669c11551f834f54ae06aaeb81b78f59f37a3d8:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/e3497bcedfaf4d0ec65d20aec669c11551f834f54ae06aaeb81b78f59f37a3d8\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/e3497bcedfaf4d0ec65d20aec669c11551f834f54ae06aaeb81b78f59f37a3d8\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/e3497bcedfaf4d0ec65d20aec669c11551f834f54ae06aaeb81b78f59f37a3d8\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/e3497bcedfaf4d0ec65d20aec669c11551f834f54ae06aaeb81b78f59f37a3d8\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/e3497bcedfaf4d0ec65d20aec669c11551f834f54ae06aaeb81b78f59f37a3d8\n    /user/0.user:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/user/0.user\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/user/0.user\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/user/0.user\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/user/0.user\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/user/0.user\n    /docker/69f80e655409f180f39c27d1aa9cca3f1b17e0d07d44081dbf994fe4cb177b41:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/69f80e655409f180f39c27d1aa9cca3f1b17e0d07d44081dbf994fe4cb177b41\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/69f80e655409f180f39c27d1aa9cca3f1b17e0d07d44081dbf994fe4cb177b41\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/69f80e655409f180f39c27d1aa9cca3f1b17e0d07d44081dbf994fe4cb177b41\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/69f80e655409f180f39c27d1aa9cca3f1b17e0d07d44081dbf994fe4cb177b41\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/69f80e655409f180f39c27d1aa9cca3f1b17e0d07d44081dbf994fe4cb177b41\n    /docker/be7973510f1b1dbe364c2fbc9f42076cdb86304b79dfe2838498400b9c23c5b3:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/be7973510f1b1dbe364c2fbc9f42076cdb86304b79dfe2838498400b9c23c5b3\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/be7973510f1b1dbe364c2fbc9f42076cdb86304b79dfe2838498400b9c23c5b3\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/be7973510f1b1dbe364c2fbc9f42076cdb86304b79dfe2838498400b9c23c5b3\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/be7973510f1b1dbe364c2fbc9f42076cdb86304b79dfe2838498400b9c23c5b3\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/be7973510f1b1dbe364c2fbc9f42076cdb86304b79dfe2838498400b9c23c5b3\n    /docker/8eda71523fb1ccd6821993a011fe975f620ad6e99f80323a11609fc3949aa615:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/8eda71523fb1ccd6821993a011fe975f620ad6e99f80323a11609fc3949aa615\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/8eda71523fb1ccd6821993a011fe975f620ad6e99f80323a11609fc3949aa615\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/8eda71523fb1ccd6821993a011fe975f620ad6e99f80323a11609fc3949aa615\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/8eda71523fb1ccd6821993a011fe975f620ad6e99f80323a11609fc3949aa615\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/8eda71523fb1ccd6821993a011fe975f620ad6e99f80323a11609fc3949aa615\n    /docker/0c9d50c1fe120f3e3e68384d08d76f09f370946893d7b96e04e4587f07ca1a2a:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/0c9d50c1fe120f3e3e68384d08d76f09f370946893d7b96e04e4587f07ca1a2a\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/0c9d50c1fe120f3e3e68384d08d76f09f370946893d7b96e04e4587f07ca1a2a\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/0c9d50c1fe120f3e3e68384d08d76f09f370946893d7b96e04e4587f07ca1a2a\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/0c9d50c1fe120f3e3e68384d08d76f09f370946893d7b96e04e4587f07ca1a2a\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/0c9d50c1fe120f3e3e68384d08d76f09f370946893d7b96e04e4587f07ca1a2a\n    /docker/1552cb6aef46932c64bc91bd105bdc81a365cd205a1db153f4a0ec231f88286c:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/1552cb6aef46932c64bc91bd105bdc81a365cd205a1db153f4a0ec231f88286c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/1552cb6aef46932c64bc91bd105bdc81a365cd205a1db153f4a0ec231f88286c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/1552cb6aef46932c64bc91bd105bdc81a365cd205a1db153f4a0ec231f88286c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/1552cb6aef46932c64bc91bd105bdc81a365cd205a1db153f4a0ec231f88286c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/1552cb6aef46932c64bc91bd105bdc81a365cd205a1db153f4a0ec231f88286c\n    /docker/68e36ad22c98f74bcdc9c078370974804a2e0554cb105cf550c6004c7f949221:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/68e36ad22c98f74bcdc9c078370974804a2e0554cb105cf550c6004c7f949221\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/68e36ad22c98f74bcdc9c078370974804a2e0554cb105cf550c6004c7f949221\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/68e36ad22c98f74bcdc9c078370974804a2e0554cb105cf550c6004c7f949221\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/68e36ad22c98f74bcdc9c078370974804a2e0554cb105cf550c6004c7f949221\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/68e36ad22c98f74bcdc9c078370974804a2e0554cb105cf550c6004c7f949221\n    /docker/f8c6b2b9a3297563395fd10e6745098cf8d6dc401f1c017733663e788daf721c:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/f8c6b2b9a3297563395fd10e6745098cf8d6dc401f1c017733663e788daf721c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/f8c6b2b9a3297563395fd10e6745098cf8d6dc401f1c017733663e788daf721c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/f8c6b2b9a3297563395fd10e6745098cf8d6dc401f1c017733663e788daf721c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/f8c6b2b9a3297563395fd10e6745098cf8d6dc401f1c017733663e788daf721c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/f8c6b2b9a3297563395fd10e6745098cf8d6dc401f1c017733663e788daf721c\n    /docker/f587fb2c575023ae888366001dfca756e65204e90c644470c6ffde3828e93fa0:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/f587fb2c575023ae888366001dfca756e65204e90c644470c6ffde3828e93fa0\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/f587fb2c575023ae888366001dfca756e65204e90c644470c6ffde3828e93fa0\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/f587fb2c575023ae888366001dfca756e65204e90c644470c6ffde3828e93fa0\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/f587fb2c575023ae888366001dfca756e65204e90c644470c6ffde3828e93fa0\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/f587fb2c575023ae888366001dfca756e65204e90c644470c6ffde3828e93fa0\n    /docker/9319cb7b5eecbbf96a5e6ad8771064564581dc78b0bcf88b7c236489a9e189a3:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/9319cb7b5eecbbf96a5e6ad8771064564581dc78b0bcf88b7c236489a9e189a3\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/9319cb7b5eecbbf96a5e6ad8771064564581dc78b0bcf88b7c236489a9e189a3\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/9319cb7b5eecbbf96a5e6ad8771064564581dc78b0bcf88b7c236489a9e189a3\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/9319cb7b5eecbbf96a5e6ad8771064564581dc78b0bcf88b7c236489a9e189a3\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/9319cb7b5eecbbf96a5e6ad8771064564581dc78b0bcf88b7c236489a9e189a3\n    /docker/373ef176c884d24a7beee11317863518037ab38a7011d691e4aa2d303b04eb49:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/373ef176c884d24a7beee11317863518037ab38a7011d691e4aa2d303b04eb49\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/373ef176c884d24a7beee11317863518037ab38a7011d691e4aa2d303b04eb49\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/373ef176c884d24a7beee11317863518037ab38a7011d691e4aa2d303b04eb49\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/373ef176c884d24a7beee11317863518037ab38a7011d691e4aa2d303b04eb49\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/373ef176c884d24a7beee11317863518037ab38a7011d691e4aa2d303b04eb49\n    /docker/9bc2f6ffda071f9fa37a2902c50f7c79a6bed8760320a9676e2268921368a015:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/9bc2f6ffda071f9fa37a2902c50f7c79a6bed8760320a9676e2268921368a015\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/9bc2f6ffda071f9fa37a2902c50f7c79a6bed8760320a9676e2268921368a015\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/9bc2f6ffda071f9fa37a2902c50f7c79a6bed8760320a9676e2268921368a015\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/9bc2f6ffda071f9fa37a2902c50f7c79a6bed8760320a9676e2268921368a015\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/9bc2f6ffda071f9fa37a2902c50f7c79a6bed8760320a9676e2268921368a015\n    /docker/c164c683ad17574abce349544a24b07235a94bbfcfa5d7dbe61ff31bd39ff464:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/c164c683ad17574abce349544a24b07235a94bbfcfa5d7dbe61ff31bd39ff464\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/c164c683ad17574abce349544a24b07235a94bbfcfa5d7dbe61ff31bd39ff464\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/c164c683ad17574abce349544a24b07235a94bbfcfa5d7dbe61ff31bd39ff464\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/c164c683ad17574abce349544a24b07235a94bbfcfa5d7dbe61ff31bd39ff464\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/c164c683ad17574abce349544a24b07235a94bbfcfa5d7dbe61ff31bd39ff464\n    /docker/3c3b79b922aab0e8e204f31bf3d166f1210102c71cfa412a2e02980f0dd4efb0:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/3c3b79b922aab0e8e204f31bf3d166f1210102c71cfa412a2e02980f0dd4efb0\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/3c3b79b922aab0e8e204f31bf3d166f1210102c71cfa412a2e02980f0dd4efb0\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/3c3b79b922aab0e8e204f31bf3d166f1210102c71cfa412a2e02980f0dd4efb0\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/3c3b79b922aab0e8e204f31bf3d166f1210102c71cfa412a2e02980f0dd4efb0\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/3c3b79b922aab0e8e204f31bf3d166f1210102c71cfa412a2e02980f0dd4efb0\n    /docker/2c22e19db514bf852d0c9564e65d45a72f2f473a3faf6e4ea6569a000a933d9b:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/2c22e19db514bf852d0c9564e65d45a72f2f473a3faf6e4ea6569a000a933d9b\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/2c22e19db514bf852d0c9564e65d45a72f2f473a3faf6e4ea6569a000a933d9b\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/2c22e19db514bf852d0c9564e65d45a72f2f473a3faf6e4ea6569a000a933d9b\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/2c22e19db514bf852d0c9564e65d45a72f2f473a3faf6e4ea6569a000a933d9b\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/2c22e19db514bf852d0c9564e65d45a72f2f473a3faf6e4ea6569a000a933d9b\n    /user:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/user\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/user\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/user\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/user\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/user\n    /docker:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker\n    /docker/04e19a02229b6b3b7e1bf2be5ebbb2e1ecf816d29b44a87975c27c42b7cbff56:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/04e19a02229b6b3b7e1bf2be5ebbb2e1ecf816d29b44a87975c27c42b7cbff56\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/04e19a02229b6b3b7e1bf2be5ebbb2e1ecf816d29b44a87975c27c42b7cbff56\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/04e19a02229b6b3b7e1bf2be5ebbb2e1ecf816d29b44a87975c27c42b7cbff56\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/04e19a02229b6b3b7e1bf2be5ebbb2e1ecf816d29b44a87975c27c42b7cbff56\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/04e19a02229b6b3b7e1bf2be5ebbb2e1ecf816d29b44a87975c27c42b7cbff56\n    /docker/94f7e4709273ffa4ca421555abecb5ca31dca5374e8209e981b92932ab24233c:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/94f7e4709273ffa4ca421555abecb5ca31dca5374e8209e981b92932ab24233c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/94f7e4709273ffa4ca421555abecb5ca31dca5374e8209e981b92932ab24233c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/94f7e4709273ffa4ca421555abecb5ca31dca5374e8209e981b92932ab24233c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/94f7e4709273ffa4ca421555abecb5ca31dca5374e8209e981b92932ab24233c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/94f7e4709273ffa4ca421555abecb5ca31dca5374e8209e981b92932ab24233c\n    /docker/986065790a08f75ae0748933728e960b73fcd358232db6dea2bbadd942df8df4:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/986065790a08f75ae0748933728e960b73fcd358232db6dea2bbadd942df8df4\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/986065790a08f75ae0748933728e960b73fcd358232db6dea2bbadd942df8df4\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/986065790a08f75ae0748933728e960b73fcd358232db6dea2bbadd942df8df4\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/986065790a08f75ae0748933728e960b73fcd358232db6dea2bbadd942df8df4\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/986065790a08f75ae0748933728e960b73fcd358232db6dea2bbadd942df8df4\n    /docker/19fa6b3b67886869a61fab397d606aad0c2bd40c5eabe94fe7d2eec7c52ac0b7:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/19fa6b3b67886869a61fab397d606aad0c2bd40c5eabe94fe7d2eec7c52ac0b7\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/19fa6b3b67886869a61fab397d606aad0c2bd40c5eabe94fe7d2eec7c52ac0b7\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/19fa6b3b67886869a61fab397d606aad0c2bd40c5eabe94fe7d2eec7c52ac0b7\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/19fa6b3b67886869a61fab397d606aad0c2bd40c5eabe94fe7d2eec7c52ac0b7\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/19fa6b3b67886869a61fab397d606aad0c2bd40c5eabe94fe7d2eec7c52ac0b7\n    /docker/b839f2e75a2b9cfbe87506e970583570d37c9e79b5851d43b0237bc45ee2ee57:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/b839f2e75a2b9cfbe87506e970583570d37c9e79b5851d43b0237bc45ee2ee57\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/b839f2e75a2b9cfbe87506e970583570d37c9e79b5851d43b0237bc45ee2ee57\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/b839f2e75a2b9cfbe87506e970583570d37c9e79b5851d43b0237bc45ee2ee57\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/b839f2e75a2b9cfbe87506e970583570d37c9e79b5851d43b0237bc45ee2ee57\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/b839f2e75a2b9cfbe87506e970583570d37c9e79b5851d43b0237bc45ee2ee57\n    /docker/2c79e3818e2d0c96f9b56a0b71a5c9674d13d1b81eb14d402e8356b3424f82d7:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/2c79e3818e2d0c96f9b56a0b71a5c9674d13d1b81eb14d402e8356b3424f82d7\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/2c79e3818e2d0c96f9b56a0b71a5c9674d13d1b81eb14d402e8356b3424f82d7\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/2c79e3818e2d0c96f9b56a0b71a5c9674d13d1b81eb14d402e8356b3424f82d7\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/2c79e3818e2d0c96f9b56a0b71a5c9674d13d1b81eb14d402e8356b3424f82d7\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/2c79e3818e2d0c96f9b56a0b71a5c9674d13d1b81eb14d402e8356b3424f82d7\n    /:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio\n    /docker/3a3d92519b9d3bdd64c3dd0cad459c4165a1d358eed4eb51cd85515c26996070:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/3a3d92519b9d3bdd64c3dd0cad459c4165a1d358eed4eb51cd85515c26996070\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/3a3d92519b9d3bdd64c3dd0cad459c4165a1d358eed4eb51cd85515c26996070\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/3a3d92519b9d3bdd64c3dd0cad459c4165a1d358eed4eb51cd85515c26996070\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/3a3d92519b9d3bdd64c3dd0cad459c4165a1d358eed4eb51cd85515c26996070\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/3a3d92519b9d3bdd64c3dd0cad459c4165a1d358eed4eb51cd85515c26996070\n    /docker/c7502659491833b4c1730e30858eb79e7eb89cf49b13ffc8d9aa4820a2817c6f:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/c7502659491833b4c1730e30858eb79e7eb89cf49b13ffc8d9aa4820a2817c6f\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/c7502659491833b4c1730e30858eb79e7eb89cf49b13ffc8d9aa4820a2817c6f\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/c7502659491833b4c1730e30858eb79e7eb89cf49b13ffc8d9aa4820a2817c6f\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/c7502659491833b4c1730e30858eb79e7eb89cf49b13ffc8d9aa4820a2817c6f\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/c7502659491833b4c1730e30858eb79e7eb89cf49b13ffc8d9aa4820a2817c6f\n    /user/0.user/14103.session:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/user/0.user/14103.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/user/0.user/14103.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/user/0.user/14103.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/user/0.user/14103.session\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/user/0.user/14103.session\n    /docker/f7b0867d255b6c4b42fbca2ff60e660da310a0a137e21c12bf71816ce1240129:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/f7b0867d255b6c4b42fbca2ff60e660da310a0a137e21c12bf71816ce1240129\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/f7b0867d255b6c4b42fbca2ff60e660da310a0a137e21c12bf71816ce1240129\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/f7b0867d255b6c4b42fbca2ff60e660da310a0a137e21c12bf71816ce1240129\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/f7b0867d255b6c4b42fbca2ff60e660da310a0a137e21c12bf71816ce1240129\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/f7b0867d255b6c4b42fbca2ff60e660da310a0a137e21c12bf71816ce1240129\n    /docker/53d7951a9d25019ecb1fe80bb62aa27c39307c1f1cacc89d523037756123df96:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/53d7951a9d25019ecb1fe80bb62aa27c39307c1f1cacc89d523037756123df96\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/53d7951a9d25019ecb1fe80bb62aa27c39307c1f1cacc89d523037756123df96\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/53d7951a9d25019ecb1fe80bb62aa27c39307c1f1cacc89d523037756123df96\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/53d7951a9d25019ecb1fe80bb62aa27c39307c1f1cacc89d523037756123df96\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/53d7951a9d25019ecb1fe80bb62aa27c39307c1f1cacc89d523037756123df96\n    /docker/52a78e1eb62e6fd5b75db640271ad7319a4f71c7ce52762aa5d584512e474b54:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/52a78e1eb62e6fd5b75db640271ad7319a4f71c7ce52762aa5d584512e474b54\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/52a78e1eb62e6fd5b75db640271ad7319a4f71c7ce52762aa5d584512e474b54\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/52a78e1eb62e6fd5b75db640271ad7319a4f71c7ce52762aa5d584512e474b54\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/52a78e1eb62e6fd5b75db640271ad7319a4f71c7ce52762aa5d584512e474b54\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/52a78e1eb62e6fd5b75db640271ad7319a4f71c7ce52762aa5d584512e474b54\n    /docker/13a46c66e7975194d69c2d9ff524608e51ab83e1ce96be063c0d1fcec35dfd24:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/13a46c66e7975194d69c2d9ff524608e51ab83e1ce96be063c0d1fcec35dfd24\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/13a46c66e7975194d69c2d9ff524608e51ab83e1ce96be063c0d1fcec35dfd24\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/13a46c66e7975194d69c2d9ff524608e51ab83e1ce96be063c0d1fcec35dfd24\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/13a46c66e7975194d69c2d9ff524608e51ab83e1ce96be063c0d1fcec35dfd24\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/13a46c66e7975194d69c2d9ff524608e51ab83e1ce96be063c0d1fcec35dfd24\n    /docker/0a06bda70da5f699bf9b3538a2fdad0c387fbe6f89552e0596ca3ccedc366b1b:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/0a06bda70da5f699bf9b3538a2fdad0c387fbe6f89552e0596ca3ccedc366b1b\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/0a06bda70da5f699bf9b3538a2fdad0c387fbe6f89552e0596ca3ccedc366b1b\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/0a06bda70da5f699bf9b3538a2fdad0c387fbe6f89552e0596ca3ccedc366b1b\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/0a06bda70da5f699bf9b3538a2fdad0c387fbe6f89552e0596ca3ccedc366b1b\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/0a06bda70da5f699bf9b3538a2fdad0c387fbe6f89552e0596ca3ccedc366b1b\n    /docker/e99d71f8ee002b06a110fdee0f0659cf5cfad55e399fd680c0a4080d8fc9b75c:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/e99d71f8ee002b06a110fdee0f0659cf5cfad55e399fd680c0a4080d8fc9b75c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/e99d71f8ee002b06a110fdee0f0659cf5cfad55e399fd680c0a4080d8fc9b75c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/e99d71f8ee002b06a110fdee0f0659cf5cfad55e399fd680c0a4080d8fc9b75c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/e99d71f8ee002b06a110fdee0f0659cf5cfad55e399fd680c0a4080d8fc9b75c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/e99d71f8ee002b06a110fdee0f0659cf5cfad55e399fd680c0a4080d8fc9b75c\n    /docker/bb58da56689daf4caac7261ea045cd45a45c383b1091aab8dac92b523b62eccb:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/bb58da56689daf4caac7261ea045cd45a45c383b1091aab8dac92b523b62eccb\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/bb58da56689daf4caac7261ea045cd45a45c383b1091aab8dac92b523b62eccb\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/bb58da56689daf4caac7261ea045cd45a45c383b1091aab8dac92b523b62eccb\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/bb58da56689daf4caac7261ea045cd45a45c383b1091aab8dac92b523b62eccb\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/bb58da56689daf4caac7261ea045cd45a45c383b1091aab8dac92b523b62eccb\n    /docker/948d58743ed589e08f277df0998774ecbf7f62e3df92f5a9cc53948bbfbb56b0:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/948d58743ed589e08f277df0998774ecbf7f62e3df92f5a9cc53948bbfbb56b0\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/948d58743ed589e08f277df0998774ecbf7f62e3df92f5a9cc53948bbfbb56b0\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/948d58743ed589e08f277df0998774ecbf7f62e3df92f5a9cc53948bbfbb56b0\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/948d58743ed589e08f277df0998774ecbf7f62e3df92f5a9cc53948bbfbb56b0\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/948d58743ed589e08f277df0998774ecbf7f62e3df92f5a9cc53948bbfbb56b0\n    /docker/68cfa29a4c7d535e1874bbb3161091d0fd0166b9d4e30c310a2487aeb36fb52c:\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/memory/docker/68cfa29a4c7d535e1874bbb3161091d0fd0166b9d4e30c310a2487aeb36fb52c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuset/docker/68cfa29a4c7d535e1874bbb3161091d0fd0166b9d4e30c310a2487aeb36fb52c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpuacct/docker/68cfa29a4c7d535e1874bbb3161091d0fd0166b9d4e30c310a2487aeb36fb52c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/cpu/docker/68cfa29a4c7d535e1874bbb3161091d0fd0166b9d4e30c310a2487aeb36fb52c\n        /var/lib/docker/aufs/mnt/d810838782dd035ad509294b21da4c99aa3216ca4130d413dd190c66128c05fc/sys/fs/cgroup/blkio/docker/68cfa29a4c7d535e1874bbb3161091d0fd0166b9d4e30c310a2487aeb36fb52c\n```\nDo you mind that I left off the managed containers and aliases?\n. ",
    "alexey-medvedchikov": "Same bug with cadvisor -housekeeping_interval=30s\n. ",
    "brycereynolds": "We are seeing it all over our stacks as well. Just recently this has started happening. Regression maybe?\n. ",
    "rvadim": "We often have same problem on 0.18.0 version and restarting cadivsor as workaround. Any ideas?\n. ",
    "gwleclerc": "I have the same problem using version 0.19.3\n. ",
    "jhjeong-kr": "I signed it!\n2015\ub144 8\uc6d4 7\uc77c (\uae08) \uc624\ud6c4 3:41, cadvisorJenkinsBot notifications@github.com\ub2d8\uc774 \uc791\uc131:\n\nCan one of the admins verify this patch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/844#issuecomment-128613153.\n. @vmarmol Yes, Your suggestion is the simplest way to stop the issue, but I want to distinguish between two cases, 1) system time change(by suspend/resume), 2) actually host machine is very busy(really really busy), so I tried to fix nexthousekeeping() function.\nIf you want, I can update PR.\n. @vmarmol OK. May I update the source as you recommend? If you prefer a simpler way, I can update this PR within a couple of hours.\n. I updated the source code.\n. It was dropped while testing via cli argument.\nAt the moment, it would be better to set as a constant.\nI set 1 minute as a default in housekeeping().\n. \n",
    "jimonreal": "Fixed with PR #849\n. From Docker API v1.19 the Image name field is no longer available, instead there's an array of the Image Tags, this would be usefull to cross information, knowing from what Image is the container comming from, and the data. That way we may correlate the stats correctly.\n. I signed it!\n. I had a conflict while doing the squash, I think it was because I removed the last commit. Do I need to squash again this two commits?\n. Thanks mvdan and everyone who helped me with my first PR :)\n. Done ;)\n. Thank you guys for the help! @vishh @mvdan @rjnagal \n. That will not work. Then I would have to get the HOST hostname from someplace else.\nIt would be needed to pass the HOSTNAME of the HOST as a env var. What do you guys think?\n. From the PR comments I did add the documentation:\nIt needs besides the storage_driver=io, the storage_driver_host=$HOSTNAME that way you may associate the logs, perhaps this could be added in the same code, but it's done this way to make it flexible to the user.\n. Is it bad to have it initialized? or what do you mean by Plumb it?\n. Hahaha I forgot to commit the storagedriver :P My bad\n. Now it's OK\n. Ok, Ill rename it. Yeah IO could be disk or net right? I will change it\n. I got your point. That would be for someone who would want to read it right? but if you think in a logging system like logstash, fluentd, sumologic, splunk, etc. This is better, and you do not have to add extra characters for new line. The other thing on outputing all together is that you will not get mixed up data from other nodes, making this system mix all up.\nLet me know what you think about it \n. probably not\n. I took it from statsd code, let me check\n. I'll change it, thanks for the feedback\n. More than code rutines working at the same time, is the logging system collector that will not know which line correspond to which.\nFor a logging system this is nothing, and its easier to parse strings without the new line in the same log.\nIf you think that I could use chan []string but to improve performance, but still write everything in one line, then that sounds good.\n. If not constant, what could it be? Just right the names down in the arrays on the methods? Or other type may I use?\n. What concat? They are not identically if you see one is Summary the other is Device\n. Ok. Thanks for the help, everyday learning something new :)\n. ",
    "Miciah": "I work for Red Hat, so I should be covered by the corporate CLA.\n. Thanks!\n. ",
    "yujuhong": "Thanks! :)\n. Just FYI, I've decided to not use the cadvisor event stream, and instead adopt docker event stream for my proposal.  The reason is that after receiving a containerStopped event from cadvisor, if you do docker ps immediately, the container would still show up. Since kubelet relies on docker ps to get a list of running containers in syncing, I cannot use this without significantly rewriting kubelet.\n. @yifan-gu, we still need cadvisor to collect containers stats for rkt, and that should be at a higher priority.\n. /cc @dchen1107 \n. > AFAICT this du cheek only runs on aufs backed docker, basically docker running on ubuntu.\nDo we not check disk usage in other cases?\nI think we want more information of disk usage exposed to kubelet. @vishh is probably the one who added this, so he'd defend the choice better than I do.\nI agree that reducing the polling interval is a short-term solution, but it'd help for the time being since we've got quite a few reports from the users. A better implementation is more than welcome :)\n. I am okay with disabling this for now since kubelet hasn't started utilizing it, but I think we still want the disk usage information in the near future. @vishh, WDYT?\n. lgtm with a nit\n. LGTM.\n. > 1m frequency sounds good to me, but might be a problem for containers that have written a lot of data - see kubernetes/kubernetes#10451 (comment).\nThe actual call would be blocked until finished, so it's not worse than today...\n\nBTW, this idea has got me thinking about a new way to collect metrics in cadvisor, each check individually scheduleable & feeding data back over a channel to central collector.\n\nI am fine with the idea, but would like to make sure cadvisor doesn't consume too much resources.\n\nSo does that mean that the length of time for du to run will impact on cpu/memory stats collection? Is that a problem?\n\nThis is not a new problem, but I am also curious. @vishh, would this affect heapster?\n. lgtm\n. Hmm...I just checked the cadvisor code and it seems every container handler uses its own docker client as well. Not only does that waste the resource, cadvisor never gets to close the connection even after the container terminates.\nI think our options are:\n1. Submit a patch to go-dockerclient to allow closing the connection.\n2. Refactor cadvisor to reuse the same docker client for all \"docker\" containers (similar to kubelet's model).\nOption 2 seems reasonable, but it seems go-dockerclient is not particularly thread-safe (e.g., struct field unixHTTPClient is not guarded by any sync primitive). \nThoughts? @jimmidyson @vishh \n. > The unixHTTPClient shouldn't need to guarded - http.Client and & http.Transport are safe for concurrent use by multiple goroutines as documented.\nI meant the initialization of the unixHTTPClient, since you could have both goroutines thinking it's nil and attempt to create a new client...\n. HTTP client uses persistent connection by default, and unless we explicitly request the connection to be closed after the request. In this case, for performance reasons, I think a persistent connection is better for cadvisor anyway :-) \n@vishh @jimmidyson, I am not that familiar with the codebase, could you give an estimate how long would it take to refactor cadvisor? If it's going to take a while, I'd prefer reverting the go-dockerclient version bump since kubernetes HEAD is pretty messed up as of now.\n. sync.Once looks good. Thanks for fixing this!\n. lgtm. Thanks for the fix!\n. Might be useful to add a simple unit test. Would help the reviewers too :)\n. Thanks for the PR. It looks good overall and the test shows the expected output.\nJust a question: I couldn't find the exact output fields of dmsetup status <pool>. Is it not clearly documented?\n. Ah I see. LGTM.\n. @rhvgoyal, I saw the description but the order seemed different than the output @jimmidyson provided in the test. \n. > How about a Prometheus metric to hold metrics for number of specific goroutines? As goroutine starts, increment gauge; decrement on stop. That seems like a more foolproof way to do this?\nThat sounds like a good idea, and will probably be more useful for debugging goroutine leak in a regular cluster.\n. I agree that we need to determine how runtime should integrate with kubelet when it comes to metrics (via cadvisor, provides metrics directly in CRI, or some form of hybrid solution?) Let's move the discussion to https://github.com/kubernetes/kubernetes/issues/27097.\n. Looks good with one question: why do you need golang v1.5.3?\n. LGTM\n. What version of docker do you use and what are the flags?\n. @jeinwag By \"restarting\", do you mean that you called docker restart, or uses docker's restart policy?\nDid those files that cadvisor tried to access existed at the time you check?\n. I am not familiar with the cadvisor codebase, but this is my guess.\nFor some reason (race condition or transient error?), sometimes cadvisor cannot read the docker configuration file (state.json or container.json depending on the versions of docker). cadvisor reports an error but does not retry at all. I can reproduce the error messages periodically by forcing the container to restart every 10s: sudo docker run --restart=always -d busybox /bin/sh -c \"sleep 10\"\nNote that in HEAD (cadvisor 0.23), cadvisor no longer reads the config files from docker, so this problem should be resolved after upgrade. However, It'd also be a good idea to retry getting the spec of the container.\n/cc @timstclair \n. If it helps, the error messages:\nW0413 18:03:30.824396    3398 container.go:341] Failed to create summary reader for \"/docker/f99bd2d7d876cd07743cbec1ef0f3204fc398f112d57442f4cbd7bcebcacd02d\": none of the resources are being tracked.\nI0413 18:03:30.824429    3398 manager.go:802] Added container: \"/docker/f99bd2d7d876cd07743cbec1ef0f3204fc398f112d57442f4cbd7bcebcacd02d\" (aliases: [abc f99bd2d7d876cd07743cbec1ef0f3204fc398f112d57442f4cbd7bcebcacd02d], namespace: \"docker\")\nW0413 18:03:30.824454    3398 manager.go:986] Failed to process watch event: failed to read libcontainer config: open /var/lib/docker/execdriver/native/f99bd2d7d876cd07743cbec1ef0f3204fc398f112d57442f4cbd7bcebcacd02d/container.json: no such file or directory\nI0413 18:03:30.894920    3398 manager.go:855] Destroyed container: \"/docker/f99bd2d7d876cd07743cbec1ef0f3204fc398f112d57442f4cbd7bcebcacd02d\" (aliases: [abc f99bd2d7d876cd07743cbec1ef0f3204fc398f112d57442f4cbd7bcebcacd02d], namespace: \"docker\")\nI0413 18:03:30.895132    3398 handler.go:320] Added event &{/docker/f99bd2d7d876cd07743cbec1ef0f3204fc398f112d57442f4cbd7bcebcacd02d 2016-04-13 18:03:30.89509281 +0000 UTC containerDeletion {<nil>}}\nThe file cadvisor was looking for is in /run/containerd/<container_id>/state.json now that the execdriver abstraction is completely gone.\n. > @yujuhong - could you take a look at this since Vish is out today?\nSure.\n. LGTM with less runtime-specific logic and more reusable code :-) \n. LGTM\n. @k8s-bot ok to test\n. LGTM\n. ok to test\n. Not sure if I am in the admin list :-\\ \n/cc @vishh @timstclair \n. @jimmidyson thanks! Maybe the bot wasn't running at the time.\n. ok to test\n. LGTM.\n. @k8s-bot ok to test\n. OK. I don't think I have the power to summon the bot. While I am figuring this out, anyone else want to reply \"ok to test\"? \n@vishh @timstclair @jimmidyson \n. @k8s-bot ok to test\n. LGTM. Thanks!\n. @k8s-bot test this. @dashpole are you going to review this? Let me know if you're too busy. We can find another reviewer. Thanks!. I think this is the right thing to do since watching those cgroups exhausts node resources easily.\n/lgtm \n/cc @derekwaynecarr . remove the extra line\n. Needs a one line comment to be consistent with all the functions above\n. +1. I don't know if it's useful to set a hard 1-second limit as @jimmidyson mentioned.\n. nit: add a error message explaining the expectation.\n. nit (optional): since here it only checks for a stop signal, you can close the channel at stop(). The code here can then be simplified to check whether a channel is closed. \n. nit: frequency seems to suggest \"rate\". perhaps switching to \"period\" or \"interval\"?\n. nit (optional): use struct{} instead.\n. s/2014/2015\n. Do we not care about the error anymore?...\n. You can get this from Prometheus endpoint /metrics as well.\n```\nHELP go_goroutines Number of goroutines that currently exist.\nTYPE go_goroutines gauge\ngo_goroutines 235\n```\n. Using an arbitrary sleep time seems brittle. Is there any other way to confirm that cadvisor has stabilized?\n. Are we sure that there wouldn't be any short-lived goroutines causing the test to fail, e.g., goroutines serving http requests, etc? I am also not sure whether garbage collector may spawn goroutines, but I think it's possible. \nHow about we start more containers in this test so that we can allow some margin for fluctuations in #goroutines.\n. I don't have any good idea, but I'll throw something out. How about letting the test check periodically for a longer timeout until #goroutines <= initial#goroutines is true? This way we end the test quickly when cadvisor responds faster, but wait for a longer timeout if cadvisor is slow for other reasons.\n. Is there any undesirable side-effect of calling the Cleanup() function twice? If not, it might still be worth adding defer fm.Cleanup() to handle misbehaved tests \n. Now that you moved out the creation of goroutines from the initialization of container handlers, is it necessary to guard the entire function with the lock?\n. What information do we need from the config? Just curious. I can check the code of course :)\nBTW, I think we can check the version of docker to decide where to look for the config, cant' we?\n. can we check what type of error this is? It probably only makes sense to continue if the path doesn't exist...\n. One example of potential issues is that kubelet hits the limits of the inotify watches, but it continues to start up without watching any pods. We can argue that this may happen anytime when kubelet is running, so there is no need to fail the startup. I am ok with that argument, but just want to point out that this is possible and we should try ensuring there is a recovery path. For example, maybe cadvisor should be not ready if errors like this keep happening, and kubelet should report node not ready?\nI don't want to block this PR, but maybe we should open up an issue to discuss propagating the errors better. On the other hand, if cadvisor does not solely rely on inotify watches for notifications (e.g., it lists everything periodically), just ignore what I said :-)\n. minor nit: use t.Errorf to go through all test cases.. cadvisor will log Error trying to work out if we can handle containerName: error, which seems misleading. How about changing this so that CanHandleAndAccept return (true, false, nil) or (false, false, nil)` instead?\nI also think the caller of CanHandleAndAccept logs way too many errors periodically...but that's another issue.. Need a comment explaining why :-). ",
    "yifan-gu": "@rjnagal Thanks for the heading up!\nFor many of the things you listed, such as listing containers, images, getting networking settings. We are planning to provide rkt APIs for them.\nTo get started, I plan to have cadvisor to recognize the cgroups of rkt containers. With machined 216+, containers within one pod created by rkt should have cgroups live under machine.slice/machine-rkt*, https://github.com/coreos/rkt/blob/master/stage1/init/init.go#L620\nFor example: \n machine.slice/machine-rkt\\x2d6813e16e\\x2dd2c3\\x2d45af\\x2db28c\\x2dce8abc2d3488.scope/system.slice/etcd.service\nBut I am not sure what do you mean by Equivalent of labels?\n. /cc @jonboulle @philips as related to https://github.com/coreos/rkt/issues/1208\n. > @yifan-gu By label, I meant if there's a way to add metadata (in image or runtime) for a rkt container, we would want a way to api to discover it.\nCurrently the metadata can be only added into the annotation of image manifest or rkt pod manifest before the pod is launched. But there is no way to read/write them from outside of the rkt pod after it's launched.\n. @rjnagal I'm not very clear of the use case for api to discover labels. Could you give me an example ?\n. FWIW, I talked with @jonboulle offline, and we don't have strong reason against this. It's just we haven't decided the semantics and haven't implemented it yet.\n. Status: I kinda found myself forking/execing rkt cli to get container list, status again just like in kubelet. So I decided to implement coreos/rkt#1208 first to smooth my life :)\n. @yujuhong ok, makes sense. where is your branch you are working on?\n. @f0 Status: rkt have api service already, so we can start integration. But this is not my top priority now since we use individual event stream for individual runtime now.\n. cc @crawford @jonboulle\n. Thanks, probably will close for #966 \n. Thank you @jimmidyson \n. @sjpotter Rebase?\n. @sjpotter It's a little more complicated than normal if the file you moved is modified at master.. It's easy to revert the new changes by accident.\nMaybe just do 2 commits from the current master? e.g.\n- rename.\n- made necessary changes.\nThis should make it easier to review.\n. > so that goes to what cadvisor people prefer. Do you want 2-3 separate patches that make the changes clearer though each individually will not result in a system that can be built. If so, I can do that.\n@sjpotter If you can do 2-3 commits in one PR, the built system will build on the whole instead of each commit.\n. What's the status on this PR? Can we maybe split it into several PRs for easy reviewing? @sjpotter \n. @sjpotter I think you need a rebase :)\n. LGTM after squash. Waiting @vishh @timstclair for more feedback :)\n. ping @timstclair @vishh ?\n. @sjpotter Any ideas why the tests are failing?\n. @sjpotter \nCould you split the Godep changes from this commit big change - rktapi service, pid fs stats, lots of nits?\nSo we can ignore the Godep changes and focus on your actual touch of code?\n. Try to think out loud here. So why should we watch cgroup fs to detect new containers in the first place?\nIs there any way to just talk to rkt api service directly to list containers and get new containers?\n. As @jellonek pointed out here https://github.com/kubernetes/kubernetes/issues/23889#issuecomment-206382577 Relying on cgroups will not work for kvm stage1 anyway. And containers don't just mean cgroup + namespace. \ncAdvisor today does seem to have some abstraction (i.e. the ContainerHandler) I am not sure if that's enough though. If we implement that interface without directly talking with the cgroup fs, will the rkt + cAdvisor work?\n. But back to @sjpotter 's specific problem here, the cause is when running rkt through systemd.service, we gave the nspawn --keep-unit flag, which instead of creating it's own cgroup, it uses the cgroup created by systemd.\n\n--keep-unit\n           Instead of creating a transient scope unit to run the container in, simply register the service or scope unit systemd-nspawn has been\n           invoked in with systemd-machined(8). This has no effect if --register=no is used. This switch should be used if systemd-nspawn is\n           invoked from within a service unit, and the service unit's sole purpose is to run a single systemd-nspawn container. This option is not\n           available if run from a user session.\n\nWonder if removing that flag is a feasible approach? cc @iaguis @alban @jonboulle \nBut again I feel this is not a right direction, because these exposes too much implementation details. Maybe we should spend more time thinking how we can make cadvisor to talk to rkt api service to hide all these details.\n. LGTM overall, with some nits. So this PR splits the watcher from the container handler, right? @sjpotter\n. @timstclair Are we able to get those rkt support PRs in and bump cadvisor in 7 days?\n. Reviewed, most comments is about comments :)\n. @sjpotter \n```\n--- FAIL: TestDockerContainerById (8.08s)\n    assertions.go:150: \nLocation:   docker_test.go:63\n\nError:      No error is expected but got request \"http://e2e-cadvisor-rhel-7-docker111:8080/api/v1.3/docker/69b13738e92dcc233883245aa17abf422a6964ef350ad42bafdab8531898a16c\" failed with error: \"failed to get Docker container \\\"69b13738e92dcc233883245aa17abf422a6964ef350ad42bafdab8531898a16c\\\" with error: unable to find Docker container \\\"69b13738e92dcc233883245aa17abf422a6964ef350ad42bafdab8531898a16c\\\"\"\n\nMessages:   Timed out waiting for container \"69b13738e92dcc233883245aa17abf422a6964ef350ad42bafdab8531898a16c\" to be available in cAdvisor: request \"http://e2e-cadvisor-rhel-7-docker111:8080/api/v1.3/docker/69b13738e92dcc233883245aa17abf422a6964ef350ad42bafdab8531898a16c\" failed with error: \"failed to get Docker container \\\"69b13738e92dcc233883245aa17abf422a6964ef350ad42bafdab8531898a16c\\\" with error: unable to find Docker container \\\"69b13738e92dcc233883245aa17abf422a6964ef350ad42bafdab8531898a16c\\\"\"\n\nFAIL\nFAIL    github.com/google/cadvisor/integration/tests/api    31.716s\nok      github.com/google/cadvisor/integration/tests/healthz    0.011s\ngodep: go exit status 1\n```\n. cc @aaronlevy for similar errors (cgroup path not found instead of permission denied) when running kubelet in rkt fly.\nAlso cc @steveej\n. subscribe\n. More description on the comment?\n. How is the channel size (16)  been decided? @sjpotter \n. Ok, I saw that you copied from the old code.\n. The documentation here is incomplete.\n. @sjpotter I don't understand why we talks about createContainer here?\n. Hmm, not a fan of using a chan for both sending errors and signaling.\n. @sjpotter Got you, can we add some comma or period, e.g. before createContainer just... ? cc @joshix for more suggestions :)\n. You want > 1 or != 1 here?\n. nit, can we break the TODO into multiple lines?\nAlso please add space after //\n. nit: Can we make comments a complete sentence with period and caps?\n. Maybe more documentation here explaining why we don't accpet /system.slice. Or link any issues related to this?\n. Can you give examples how the name will look like, so we can understand why len(splits) >= 3 and len(splits) == 5\n. why shouldn't reach here?\n. Question on Line 122: Do we care about non-running pods?\n. nit, s/can't/Can't\n. nit, period at the end.\n. nit, period at the end.\n. s/overriden/overridden\n. nit, remove this empty line?\n. +1, let's close the channel instead of sending nil to it when stopping the watcher.\n. Why can't we change createContainer() to make it overrides the handler when it exists? \n/cc @timstclair \n. Not sure why it's called \"raw\" watcher?\n. NVM\n. Maybe an example of the cgroup tree will be very helpful\n. Gotcha :)\n. ",
    "f0": "@yifan-gu  any news on this?\n. @vishh i can try it\n. @vishh   should info v1 and v2 supported? i see only v1 imported\n. @vishh  in libcontainer/helper only info from v1 is imported\n@jimmidyson  e.g how many connections in state Established or Listen ..\n. @vishh  ok a working prototype is finished (for the api), maybe you can take a look at  https://github.com/f0/cadvisor  and give some feedback\nI also had some trouble with the api v1, v2 transistion, atm its only working for v1\n. @vishh  ok updated the merge request. Any Idea how to get this working for v2?\n. @vishh  moved to NetworkStats (for v1 and v2)\n. @jimmidyson  i am not a profiling expert, so how can i produce such a graph?\nIf i interpret this correct the regex pattern matching is the biggest CPU user\n. @jimmidyson  thats my plan, but it would be nice to check if it helps .... \n. @jimmidyson OK\n. @jimmidyson   hm also tried a scanf variant, if i see this correct, the problem is gone...\n\n. with the orginal code....\n\n. @jimmidyson  not so much difference\ngo version go1.5.1 linux/amd64\nfk@linux:~/work/src/go/src/github.com/f0/cadvisor> cat /proc/cpuinfo \nprocessor       : 0\nvendor_id       : GenuineIntel\ncpu family      : 6\nmodel           : 58\nmodel name      : Intel(R) Core(TM) i5-3320M CPU @ 2.60GHz\nstepping        : 9\nmicrocode       : 0x1b\ncpu MHz         : 1297.867\ncache size      : 3072 KB\nphysical id     : 0\nsiblings        : 4\ncore id         : 0\ncpu cores       : 2\napicid          : 0\ninitial apicid  : 0\nfpu             : yes\nfpu_exception   : yes\ncpuid level     : 13\nwp              : yes\nflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm ida arat epb pln pts dtherm tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt\nbugs            :\nbogomips        : 5187.68\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 36 bits physical, 48 bits virtual\npower management:\nprocessor       : 1\nvendor_id       : GenuineIntel\ncpu family      : 6\nmodel           : 58\nmodel name      : Intel(R) Core(TM) i5-3320M CPU @ 2.60GHz\nstepping        : 9\nmicrocode       : 0x1b\ncpu MHz         : 1211.437\ncache size      : 3072 KB\nphysical id     : 0\nsiblings        : 4\ncore id         : 0\ncpu cores       : 2\napicid          : 1\ninitial apicid  : 1\nfpu             : yes\nfpu_exception   : yes\ncpuid level     : 13\nwp              : yes\nflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm ida arat epb pln pts dtherm tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt\nbugs            :\nbogomips        : 5187.68\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 36 bits physical, 48 bits virtual\npower management:\nprocessor       : 2\nvendor_id       : GenuineIntel\ncpu family      : 6\nmodel           : 58\nmodel name      : Intel(R) Core(TM) i5-3320M CPU @ 2.60GHz\nstepping        : 9\nmicrocode       : 0x1b\ncpu MHz         : 1242.921\ncache size      : 3072 KB\nphysical id     : 0\nsiblings        : 4\ncore id         : 1\ncpu cores       : 2\napicid          : 2\ninitial apicid  : 2\nfpu             : yes\nfpu_exception   : yes\ncpuid level     : 13\nwp              : yes\nflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm ida arat epb pln pts dtherm tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt\nbugs            :\nbogomips        : 5187.68\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 36 bits physical, 48 bits virtual\npower management:\nprocessor       : 3\nvendor_id       : GenuineIntel\ncpu family      : 6\nmodel           : 58\nmodel name      : Intel(R) Core(TM) i5-3320M CPU @ 2.60GHz\nstepping        : 9\nmicrocode       : 0x1b\ncpu MHz         : 1236.828\ncache size      : 3072 KB\nphysical id     : 0\nsiblings        : 4\ncore id         : 1\ncpu cores       : 2\napicid          : 3\ninitial apicid  : 3\nfpu             : yes\nfpu_exception   : yes\ncpuid level     : 13\nwp              : yes\nflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm ida arat epb pln pts dtherm tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt\nbugs            :\nbogomips        : 5187.68\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 36 bits physical, 48 bits virtual\npower management:\nfk@linux:~/work/src/go/src/github.com/f0/cadvisor> go version\ngo version go1.5.1 linux/amd64\nfk@linux:~/work/src/go/src/github.com/f0/cadvisor> cat /proc/cpuinfo \nprocessor       : 0\nvendor_id       : GenuineIntel\ncpu family      : 6\nmodel           : 58\nmodel name      : Intel(R) Core(TM) i5-3320M CPU @ 2.60GHz\nstepping        : 9\nmicrocode       : 0x1b\ncpu MHz         : 1689.085\ncache size      : 3072 KB\nphysical id     : 0\nsiblings        : 4\ncore id         : 0\ncpu cores       : 2\napicid          : 0\ninitial apicid  : 0\nfpu             : yes\nfpu_exception   : yes\ncpuid level     : 13\nwp              : yes\nflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm ida arat epb pln pts dtherm tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt\nbugs            :\nbogomips        : 5187.68\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 36 bits physical, 48 bits virtual\npower management:\nprocessor       : 1\nvendor_id       : GenuineIntel\ncpu family      : 6\nmodel           : 58\nmodel name      : Intel(R) Core(TM) i5-3320M CPU @ 2.60GHz\nstepping        : 9\nmicrocode       : 0x1b\ncpu MHz         : 2575.929\ncache size      : 3072 KB\nphysical id     : 0\nsiblings        : 4\ncore id         : 0\ncpu cores       : 2\napicid          : 1\ninitial apicid  : 1\nfpu             : yes\nfpu_exception   : yes\ncpuid level     : 13\nwp              : yes\nflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm ida arat epb pln pts dtherm tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt\nbugs            :\nbogomips        : 5187.68\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 36 bits physical, 48 bits virtual\npower management:\nprocessor       : 2\nvendor_id       : GenuineIntel\ncpu family      : 6\nmodel           : 58\nmodel name      : Intel(R) Core(TM) i5-3320M CPU @ 2.60GHz\nstepping        : 9\nmicrocode       : 0x1b\ncpu MHz         : 3244.820\ncache size      : 3072 KB\nphysical id     : 0\nsiblings        : 4\ncore id         : 1\ncpu cores       : 2\napicid          : 2\ninitial apicid  : 2\nfpu             : yes\nfpu_exception   : yes\ncpuid level     : 13\nwp              : yes\nflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm ida arat epb pln pts dtherm tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt\nbugs            :\nbogomips        : 5187.68\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 36 bits physical, 48 bits virtual\npower management:\nprocessor       : 3\nvendor_id       : GenuineIntel\ncpu family      : 6\nmodel           : 58\nmodel name      : Intel(R) Core(TM) i5-3320M CPU @ 2.60GHz\nstepping        : 9\nmicrocode       : 0x1b\ncpu MHz         : 3099.992\ncache size      : 3072 KB\nphysical id     : 0\nsiblings        : 4\ncore id         : 1\ncpu cores       : 2\napicid          : 3\ninitial apicid  : 3\nfpu             : yes\nfpu_exception   : yes\ncpuid level     : 13\nwp              : yes\nflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm ida arat epb pln pts dtherm tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt\nbugs            :\nbogomips        : 5187.68\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 36 bits physical, 48 bits virtual\npower management:\n. @jimmidyson  ok i did the test again, the sscanf code is faster than the regex, on my system its form 750ms (regex) to 510ms (sscanf)\n\n\n. @jimmidyson ok, no Problem\nabout the volatility, if the file is larger(more connections), it takes more time to process the file... \n. @jimmidyson  i am fine with disabling.\n. @jimmidyson   sure i prepare a PR\n. @jimmidyson  done https://github.com/google/cadvisor/pull/969\n. cadvisor builds sucessfully, but in the prometheus output the quota is not exported , but is present in the specific cgroup\n. sure https://gist.github.com/f0/1938007f1d9054f57c56 \n. hm, yes i am sure but i double check ...\n. hm yes its the correct binary, but its not shown\n. @jimmidyson   thx\n. @jimmidyson yep this happens when no Quota is set, so it need to be coverd.... i run build/build.sh\n. @jimmidyson  hm is there a specific go version needed to build ? atm i have 1.4.3\n. @jimmidyson yes\nfk@linux-3zvu:~/work/src/f0/cadvisor> make\n\n\nformatting code\nbuilding binaries\n  cadvisor\ngithub.com/google/cadvisor\nusage: 6l [options] main.6\n  -1    use alternate profiling code\n  -8    assume 64-bit addresses\n  -B info\n        define ELF NT_GNU_BUILD_ID note\n  -C    check Go calls to C code\n  -D addr\n        data address\n  -E sym\n        entry symbol\n  -I interp\n        set ELF interp\n  -L dir\n        add dir to library path\n  -H head\n        header type\n  -K    add stack underflow checks\n  -O    print pc-line tables\n  -Q    debug byte-register code gen\n  -R rnd\n        address rounding\n  -S    check type signatures\n  -T addr\n        text address\n  -V    print version and exit\n  -W    disassemble input\n  -X name value\n        define string data\n  -Z    clear stack frame on entry\n  -a    disassemble output\n  -c    dump call graph\n  -d    disable dynamic executable\n  -extld ld\n        linker to run in external mode\n  -extldflags ldflags\n        flags for external linker\n  -f    ignore version mismatch\n  -g    disable go package data checks\n  -installsuffix suffix\n        pkg directory suffix\n  -k sym\n        set field tracking symbol\n  -linkmode mode\n        set link mode (internal, external, auto)\n  -n    dump symbol table\n  -o outfile\n        set output file\n  -r dir1:dir2:...\n        set ELF dynamic linker search path\n  -race\n        enable race detector\n  -s    disable symbol table\n  -shared\n        generate shared object (implies -linkmode external)\n  -tmpdir dir\n        leave temporary files in this directory\n  -u    reject unsafe packages\n  -v    print link trace\n  -w    disable DWARF generation\ngodep: go exit status 2\nMakefile:32: recipe for target 'build' failed\nmake: *** [build] Error 1\n. ok now i can build, and i got these errors, but why , the type has a field Quota\n\n\nfk@linux-3zvu:~/work/src/f0/cadvisor> make\n formatting code\n building binaries\n   cadvisor\n running tests\nf0/cadvisor/container/raw\ncontainer/raw/handler.go:203: spec.Cpu.Quota undefined (type v1.CpuSpec has no field or method Quota)\nf0/cadvisor/metrics\nmetrics/prometheus.go:523: container.Spec.Cpu.Quota undefined (type v1.CpuSpec has no field or method Quota)\n. @jimmidyson   hm any Idea why spec.Cpu.Quota is not present, its in the files...\n. updated with check for -1 (quota not set), but still not work\n. @jimmidyson   ok have fixed some things, now i get these errors while building, but why, the type has this field? And it does not work in my environment....\nrunning tests\n github.com/f0/cadvisor/container/raw\ncontainer/raw/handler.go:206: spec.Cpu.Quota undefined (type v1.CpuSpec has no field or method Quota)\n github.com/f0/cadvisor/metrics\nmetrics/prometheus.go:523: container.Spec.Cpu.Quota undefined (type v1.CpuSpec has no field or method Quota)\n. @jimmidyson  after fixing gopath and some other orrors it finaly works THX\n. also adding support for the cpu period setting (with this we finally can calculate the percentual cpu usage )\n. @jimmidyson  when will this be merged?\n. yes works perfect  for me, and yes i try to add a test\n. rebased\n. @jimmidyson  ok i  do not found any documentation about the valid values, i tested it, 0 is not allowed, the minimal value is 1s (in ns ) in cpu.cfs_quota_us or 1% with e.g systemd settings, so i am not sure whats the best way, maybe completely disable the output when its set to -1 ?\n. @jimmidyson  i am not sure how to add a test in cadvisor/metrics/prometheus_test.go, can you give me a hint?\n. @jimmidyson  ping\n. @jimmidyson  hm i do not get how to name the testdata in  https://github.com/google/cadvisor/blob/master/metrics/testdata/prometheus_metrics\n. rebased , commits squashed and the first half of testing \n. @jimmidyson  hm i do something wrong.... the testing does not really work, i have no idea why\n. @jimmidyson \nhm i dont get it now i have this error....\n=== RUN TestPrometheusCollector\n--- FAIL: TestPrometheusCollector (0.02s)\n        prometheus_test.go:189: want # HELP container_spec_cpu_quota CPU quota of the container, got # HELP container_fs_io_current Number of I/Os currently in progress\n. @jimmidyson   ah ok so then the  test should be after \ncontainer_start_time_seconds  and before container_tasks_state, but this also gives me the above error(with another error text)\n. @jimmidyson   also does not work\n. hm sorry false positive..... whyever  the tests does not work\n@jimmidyson  i am out of ideas\n. @vishh  @jimmidyson  sorry have not so much spare time atm, try to get this in the next days\n. @jimmidyson  feel free, i have no chance to get this until monday, i am 90% offline at the weekend , sorry.\n. @jimmidyson @vishh finally done (and now i understand the test thing ....)\n. @jimmidyson  @vishh   any outstanding issues with this PR?\n. @jimmidyson  RSS does not include the cache, so its the real stack size for applications, and the correct metric to chek the memory usage /  behaviour\n. @vishh   ok thx, thats easy, but not really obvious\nThe correct flag is --disable_metrics\n. @vishh hm this only happens with such a rkt fly container, not with a normal rkt  container or with a systemd-nspawn container \nand does cadvisor drop privileges after starting?\n. This is from cadvisor when running as user core  , but it only runs some seconds and then exits with the error above\nName:   cadvisor\nState:  S (sleeping)\nTgid:   13260\nNgid:   13260\nPid:    13260\nPPid:   1\nTracerPid:      0\nUid:    0       0       0       0\nGid:    0       0       0       0\nFDSize: 256\nGroups: 0 1 2 3 4 6 10 11 26 27 \nNStgid: 13260\nNSpid:  13260\nNSpgid: 13260\nNSsid:  115025\nVmPeak:  1286292 kB\nVmSize:  1282248 kB\nVmLck:         0 kB\nVmPin:         0 kB\nVmHWM:     83692 kB\nVmRSS:     83692 kB\nVmData:  1252240 kB\nVmStk:       136 kB\nVmExe:     14876 kB\nVmLib:      2092 kB\nVmPTE:       404 kB\nVmPMD:        24 kB\nVmSwap:        0 kB\nHugetlbPages:          0 kB\nThreads:        33\nSigQ:   6/515223\nSigPnd: 0000000000000000\nShdPnd: 0000000000000000\nSigBlk: 0000000000000000\nSigIgn: 0000000000000000\nSigCgt: ffffffffffc1feff\nCapInh: 0000000000000000\nCapPrm: 0000003fffffffff\nCapEff: 0000003fffffffff\nCapBnd: 0000003fffffffff\nCapAmb: 0000000000000000\nSeccomp:        0\nCpus_allowed:   ffffffff,ffffffff,ffffffff,ffffffff\nCpus_allowed_list:      0-127\nMems_allowed:   00000000,00000000,00000000,00000003\nMems_allowed_list:      0-1\nvoluntary_ctxt_switches:        1798\nnonvoluntary_ctxt_switches:     107\n. And if i try to read \"normal\" cgroups, it does work, so maybe wrong permissions on rkt side\ncore@hw1907 ~ $ ls /var/lib/rkt/pods/run/a73fa9b3-3ec4-425b-878d-885f644b6318/stage1/rootfs/opt/stage2/prometheus-container-backend/rootfs/sys/fs/cgroup/cpuset\nls: cannot access /var/lib/rkt/pods/run/a73fa9b3-3ec4-425b-878d-885f644b6318/stage1/rootfs/opt/stage2/prometheus-container-backend/rootfs/sys/fs/cgroup/cpuset: Permission denied\ncore@hw1907 ~ $ ls -l /sys/fs/cgroup/cpuset/\ntotal 0\n-rw-r--r--  1 root root 0 Apr 18 19:04 cgroup.clone_children\n-rw-r--r--  1 root root 0 May 13 16:49 cgroup.procs\n-r--r--r--  1 root root 0 Apr 18 19:04 cgroup.sane_behavior\n-rw-r--r--  1 root root 0 Apr 18 19:04 cpuset.cpu_exclusive\n-rw-r--r--  1 root root 0 Apr 18 19:04 cpuset.cpus\n-r--r--r--  1 root root 0 Apr 18 19:04 cpuset.effective_cpus\n-r--r--r--  1 root root 0 Apr 18 19:04 cpuset.effective_mems\n-rw-r--r--  1 root root 0 Apr 18 19:04 cpuset.mem_exclusive\n-rw-r--r--  1 root root 0 Apr 18 19:04 cpuset.mem_hardwall\n-rw-r--r--  1 root root 0 Apr 18 19:04 cpuset.memory_migrate\n-r--r--r--  1 root root 0 Apr 18 19:04 cpuset.memory_pressure\n-rw-r--r--  1 root root 0 Apr 18 19:04 cpuset.memory_pressure_enabled\n-rw-r--r--  1 root root 0 Apr 18 19:04 cpuset.memory_spread_page\n-rw-r--r--  1 root root 0 Apr 18 19:04 cpuset.memory_spread_slab\n-rw-r--r--  1 root root 0 Apr 18 19:04 cpuset.mems\n-rw-r--r--  1 root root 0 Apr 18 19:04 cpuset.sched_load_balance\n-rw-r--r--  1 root root 0 Apr 18 19:04 cpuset.sched_relax_domain_level\ndrwxr-xr-x 24 root root 0 Mar 31 08:43 machine.slice\n-rw-r--r--  1 root root 0 Apr 18 19:04 notify_on_release\n-rw-r--r--  1 root root 0 Apr 18 19:04 release_agent\ndrwxr-xr-x  4 root root 0 Apr  2 02:40 system.slice\n-rw-r--r--  1 root root 0 Apr 18 19:04 tasks\nhere is the permission denied \n```\nls -l /var/lib/rkt/pods/run/a73fa9b3-3ec4-425b-878d-885f644b6318/stage1/rootfs/opt/stage2\ntotal 8\ndrwxr-x--- 3 root root 4096 May 23 19:02 prometheus-container-backend\n```\nBut rkt and rkt fly containers have the same permissions, why does cadvisor only fail with rkt fly? \n(if i remove the fly container it does work) \nand if cadvisor needs root permissions, cadvisor should drop privs as early as pollible or not?\n. @sjpotter \n```\n[Unit]\nDescription=Prometheus Container Backend Monitor\nWants=network-online.target\nAfter=network-online.target\n[Service]\nSlice=system.slice\nDelegate=true\nTimeoutStartSec=0\nExecStartPre=/usr/bin/mkdir -p /opt/data/prometheus_container_backend\nExecStartPre=/usr/bin/chown -R 5000:5000 /opt/data/prometheus_container_backend\nExecStartPre=/root/rkt-1.6.0 fetch --insecure-options=all http://example.com/rkt/prometheus-container-backend.aci\nExecStart=/root/rkt-1.6.0 --insecure-options=all run --stage1-path=/root/rkt-v1.6.0/stage1-fly.aci --volume=prometheusdata,kind=host,source=/opt/data/prometheus_container_backend dvag.net/prometheus-container-backend:latest --mount volume=prometheusdata,target=/data\nCPUQuota=400%\nMemoryLimit=12G\nRestart=always\n[X-Fleet]\nGlobal=true\n```\n. @sjpotter   cadvisor crashes when a rkt fly container is running. removing the rkt fly container, cadvisor works\nI do not run cadvisor as rkt fly or rkt container, i run the binary directly on coreos\n. @sjpotter  i am running v0.23.2\n. @sjpotter  no, then it does work\n. @sjpotter  so regardless the error cadvisor needs running as root\n. ok then i run as root\n. if i take a look at the documentation https://lwn.net/Articles/432224/  i think the total_{rss,cache,swap} values should be used, but cadvisor uses the not total prefixed once, is there a reason?\n. @vishh  any opinion about the total vs non total statistics?\n. @vishh \n thats true, but the cgroup can have child cgroups or not?\n. ok, switched back to the non total value\n. any chance that this can be merged ?\n. comments fixed, unrelated stuff removed, commits squashed\n. @timstclair  @vishh any outstanding issues?\n. @timstclair  rebased\n. @vishh @timstclair   yes i know this, i use the default build.sh to build the binary and i think this should be work per default, or not?\n. @vishh  ok then the question is why does this not work for me? I use a default go 1.6.2 installation and the default cadvisor build script\n. @vishh  ok this is wired, on my local system (cadvisor was build on this) it does work without problems\nif i try this on CoreOS it breaks with the error above\n. my local system is \nNAME=openSUSE\nVERSION=\"Tumbleweed\"\nVERSION_ID=\"20160520\"\nPRETTY_NAME=\"openSUSE Tumbleweed (20160520) (x86_64)\"\nID=opensuse\nANSI_COLOR=\"0;32\"\nCPE_NAME=\"cpe:/o:opensuse:opensuse:20160520\"\nBUG_REPORT_URL=\"https://bugs.opensuse.org\"\nHOME_URL=\"https://www.opensuse.org/\"\nID_LIKE=\"suse\"\n. @NoumanSaleem  hm why? Cadvisor exports prometheus metrics via /metrics\n. @grobie @timstclair the exposed Cpu Period is the CPU reference time (found e.g in cgroups)\nSee commit https://github.com/google/cadvisor/commit/669bc4abfab349e18bc4efbec0f8d13b13602e29 \nThis PR break this behaviour\n. @grobie  hm you are right, you only break the tests  so yes there is no real problem sorry\n. Hi, i see the same, running the binary direct on CoreOS 1122.2.0, and cadvisor does not start\n```\ncadvisor -logtostderr\nI0930 08:02:45.269108   47423 storagedriver.go:50] Caching stats in memory for 2m0s\nI0930 08:02:45.269314   47423 manager.go:140] cAdvisor running in container: \"/user.slice/user-0.slice/session-c43.scope\"\nfatal error: unexpected signal during runtime execution\n[signal 0xb code=0x1 addr=0x0 pc=0x0]\nruntime stack:\nruntime.throw(0xfdb2e0, 0x2a)\n    /home/stclair/.gvm/gos/go1.6.3/src/runtime/panic.go:547 +0x90\nruntime.sigpanic()\n    /home/stclair/.gvm/gos/go1.6.3/src/runtime/sigpanic_unix.go:12 +0x5a\ngoroutine 59 [syscall, locked to thread]:\nruntime.cgocall(0xaba660, 0xc820051bd8, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/runtime/cgocall.go:123 +0x11b fp=0xc820051b78 sp=0xc820051b48\nnet._C2func_getaddrinfo(0x7fa0400008c0, 0x0, 0xc8202c0450, 0xc820358218, 0x0, 0x0, 0x0)\n    ??:0 +0x55 fp=0xc820051bd8 sp=0xc820051b78\nnet.cgoLookupIPCNAME(0xec0d10, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/cgo_unix.go:111 +0x448 fp=0xc820051d50 sp=0xc820051bd8\nnet.cgoLookupIP(0xec0d10, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/cgo_unix.go:163 +0x56 fp=0xc820051da8 sp=0xc820051d50\nnet.lookupIP(0xec0d10, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/lookup_unix.go:67 +0x94 fp=0xc820051e18 sp=0xc820051da8\nnet.glob.func16(0x1070c30, 0xec0d10, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/hook.go:10 +0x4d fp=0xc820051e58 sp=0xc820051e18\nnet.lookupIPDeadline.func1(0x0, 0x0, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/lookup.go:106 +0x71 fp=0xc820051ed8 sp=0xc820051e58\ninternal/singleflight.(Group).doCall(0x16439c0, 0xc82031c320, 0xec0d10, 0x9, 0xc82039f820)\n    /home/stclair/.gvm/gos/go1.6.3/src/internal/singleflight/singleflight.go:93 +0x2c fp=0xc820051f88 sp=0xc820051ed8\nruntime.goexit()\n    /home/stclair/.gvm/gos/go1.6.3/src/runtime/asm_amd64.s:1998 +0x1 fp=0xc820051f90 sp=0xc820051f88\ncreated by internal/singleflight.(Group).DoChan\n    /home/stclair/.gvm/gos/go1.6.3/src/internal/singleflight/singleflight.go:86 +0x3ee\ngoroutine 1 [select]:\nnet.lookupIPDeadline(0xec0d10, 0x9, 0xecf7ff707, 0x1e461e5e, 0x1644940, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/lookup.go:109 +0x6a6\nnet.internetAddrList(0xea6418, 0x3, 0xec0d10, 0xf, 0xecf7ff707, 0x1e461e5e, 0x1644940, 0x0, 0x0, 0x0, ...)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/ipsock.go:252 +0x6ee\nnet.resolveAddrList(0xea1cb8, 0x4, 0xea6418, 0x3, 0xec0d10, 0xf, 0xecf7ff707, 0xc81e461e5e, 0x1644940, 0x0, ...)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/dial.go:158 +0x466\nnet.(Dialer).Dial(0xc8203055b0, 0xea6418, 0x3, 0xec0d10, 0xf, 0x0, 0x0, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/dial.go:216 +0x124\nnet.DialTimeout(0xea6418, 0x3, 0xec0d10, 0xf, 0x77359400, 0x0, 0x0, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/dial.go:200 +0xa3\ngithub.com/google/cadvisor/container/rkt.Client.func1()\n    /home/stclair/go/src/github.com/google/cadvisor/container/rkt/client.go:44 +0x59\nsync.(Once).Do(0x166a218, 0x106fd68)\n    /home/stclair/.gvm/gos/go1.6.3/src/sync/once.go:44 +0xe4\ngithub.com/google/cadvisor/container/rkt.Client(0x0, 0x0, 0x0, 0x0)\n    /home/stclair/go/src/github.com/google/cadvisor/container/rkt/client.go:79 +0x47\ngithub.com/google/cadvisor/container/rkt.RktPath(0x0, 0x0, 0x0, 0x0)\n    /home/stclair/go/src/github.com/google/cadvisor/container/rkt/client.go:85 +0x48\ngithub.com/google/cadvisor/manager.New(0xc82034f980, 0x7fa04d04c118, 0x166a170, 0xdf8475800, 0x7fa04c008501, 0xc8203b7140, 0xc8203b74a0, 0x0, 0x0, 0x0, ...)\n    /home/stclair/go/src/github.com/google/cadvisor/manager/manager.go:146 +0x341\nmain.main()\n    /home/stclair/go/src/github.com/google/cadvisor/cadvisor.go:127 +0x654\ngoroutine 17 [syscall, locked to thread]:\nruntime.goexit()\n    /home/stclair/.gvm/gos/go1.6.3/src/runtime/asm_amd64.s:1998 +0x1\ngoroutine 3 [syscall]:\nos/signal.signal_recv(0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/runtime/sigqueue.go:116 +0x132\nos/signal.loop()\n    /home/stclair/.gvm/gos/go1.6.3/src/os/signal/signal_unix.go:22 +0x18\ncreated by os/signal.init.1\n    /home/stclair/.gvm/gos/go1.6.3/src/os/signal/signal_unix.go:28 +0x37\ngoroutine 4 [chan receive]:\ngithub.com/google/cadvisor/vendor/github.com/golang/glog.(*loggingT).flushDaemon(0x1644ca0)\n    /home/stclair/go/src/github.com/google/cadvisor/vendor/github.com/golang/glog/glog.go:882 +0x67\ncreated by github.com/google/cadvisor/vendor/github.com/golang/glog.init.1\n    /home/stclair/go/src/github.com/google/cadvisor/vendor/github.com/golang/glog/glog.go:410 +0x297\ngoroutine 56 [IO wait]:\nnet.runtime_pollWait(0x7fa04d04d258, 0x72, 0xc820540000)\n    /home/stclair/.gvm/gos/go1.6.3/src/runtime/netpoll.go:160 +0x60\nnet.(pollDesc).Wait(0xc8203b81b0, 0x72, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/fd_poll_runtime.go:73 +0x3a\nnet.(pollDesc).WaitRead(0xc8203b81b0, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/fd_poll_runtime.go:78 +0x36\nnet.(netFD).Read(0xc8203b8150, 0xc820540000, 0x1000, 0x1000, 0x0, 0x7fa04c004028, 0xc8200880a0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/fd_unix.go:250 +0x23a\nnet.(conn).Read(0xc820358120, 0xc820540000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/net.go:172 +0xe4\nnet/http.noteEOFReader.Read(0x7fa04d04d318, 0xc820358120, 0xc820364a28, 0xc820540000, 0x1000, 0x1000, 0x409fb3, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/http/transport.go:1690 +0x67\nnet/http.(noteEOFReader).Read(0xc82039f660, 0xc820540000, 0x1000, 0x1000, 0xc82004fd1d, 0x0, 0x0)\n    :284 +0xd0\nbufio.(Reader).fill(0xc820016840)\n    /home/stclair/.gvm/gos/go1.6.3/src/bufio/bufio.go:97 +0x1e9\nbufio.(Reader).Peek(0xc820016840, 0x1, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/bufio/bufio.go:132 +0xcc\nnet/http.(persistConn).readLoop(0xc8203649c0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/http/transport.go:1076 +0x177\ncreated by net/http.(*Transport).dialConn\n    /home/stclair/.gvm/gos/go1.6.3/src/net/http/transport.go:860 +0x10a6\ngoroutine 57 [select]:\nnet/http.(persistConn).writeLoop(0xc8203649c0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/http/transport.go:1280 +0x472\ncreated by net/http.(Transport).dialConn\n    /home/stclair/.gvm/gos/go1.6.3/src/net/http/transport.go:861 +0x10cb\n. Version  23.02  does work\n. hm this still happens, any idea?\n. @timstclair  @carlpett   @amcrn building with -tags netgo  fix the problem\n. any news here?. @tiloso  i run k8s 1.8.3 with vsphere volumes, i do not have these metrics..... @tiloso i see, but i do not have metrics with the name kubelet_volume* (not in prometheus, not when curling the kubelet) but i do have pvc's.\nDoes your kubelet run in a Container?  Maybe there is something i need to mount , so the kubelet see the volume.... @piwi91 no, but haven't looked at this to deep . @dashpole ok, so this should be fixed? . makes sense, thx. because if the quota is not set, -1 is returned and internally readInt64 uses an uint and this breaks with -1\n. hm if the quota is not set (-1) i do not set container.Spec.Cpu.Quota and the its empthy and is discarded  by omitifempthy , if i understand his correct\n. yes this would be much cleaner\n. hm ok, what does go assume if a value is not set? nil? i only set container.Spec.Cpu.Quota to a value when the read does not return -1\n. ok in this case 0 would be returned, hm is there a way to discard the output completly?\n.\ncpu.cfs_quota_us\nspecifies the total amount of time in microseconds (\u00b5s, represented here as \"us\") for which all tasks in a cgroup can run during one period (as defined by cpu.cfs_period_us). As soon as tasks in a cgroup use up all the time specified by the quota, they are throttled for the remainder of the time specified by the period and not allowed to run until the next period. If tasks in a cgroup should be able to access a single CPU for 0.2 seconds out of every 1 second, set cpu.cfs_quota_us to 200000 and cpu.cfs_period_us to 1000000. Note that the quota and period parameters operate on a CPU basis. To allow a process to fully utilize two CPUs, for example, set cpu.cfs_quota_us to 200000 and cpu.cfs_period_us to 100000.\nSetting the value in cpu.cfs_quota_us to -1 indicates that the cgroup does not adhere to any CPU time restrictions. This is also the default value for every cgroup (except the root cgroup).\n```\ni think its a valid value\n. @vishh its -1 in the cgroup file\n. @jimmidyson  hm not really , why not but i think this should be done in a different PR \n. ok i take a look\n. @timstclair  hm v2 uses the v1 values\n. ",
    "feiskyer": "@yifan-gu @yujuhong Any updates on this? I'm working on cAdvisor collectors for hyper, though cAdvisor is not so easy to extend to a new container runtime.\n. @vishh Glad to hear this, I think we must simplify the introduction of new container runtimes first for cAdvsior, and then it is much easier to add new Pod level APIs.\nDo you have any plan to decouple the API from docker and make container runtime configurable?\n. @vishh Can you post the idea? I'm working on integration cAdvisor with Hyper.\n. I'm using CentOS 7\n. Finally found it's the problem of selinux. I can start cAdvisor properly when shuting down selinux.\nThanks @jimmidyson \n. I think raw cgroups should be separated from docker/rkt/hyper container runtimes, which means\n- raw manager should be coexist with other container runtime manager\n- container manager could be configurable to select runtime\n- api should be changed not specific to docker\n. Any updates on this?\n. CC @resouer. \n. +1 for splitting.\n. I think those interfaces should also be renamed, so it is not specific to Docker.\n. ",
    "sjpotter": "Hey, in order to try to get rkt working with cadvisor, I've started looking at manager and want to isolate the docker code from the interface.  What I want to do is copy the majority of the implementation code to a new package \"docker\" and have the manager's functions call the docker impelementations functions.  \nWe can then implement a rkt implementation as well and have some smarts in the main manager to select which runtime we care about (or both).  \nthe main niggle is that some of the API calls seems docker specific (but really aren't), I don't see a way to make it generic at this point in time w/o breaking the API so going to leave them the same.\nthoughts?\n. @vishh when I said docker specific APIs I didn't meant that they were specific to docker but that they were named specific to docker.\nala\n```\n// Gets all the Docker containers. Return is a map from full container name to ContainerInfo.\nAllDockerContainers(query *info.ContainerInfoRequest) (map[string]info.ContainerInfo, error)\n// Gets information about a specific Docker container. The specified name is within the Docker namespace.\nDockerContainer(dockerName string, query *info.ContainerInfoRequest) (info.ContainerInfo, error)\n```\nand\n```\n// Get status information about docker.\nDockerInfo() (DockerStatus, error)\n// Get details about interesting docker images.\nDockerImages() ([]DockerImage, error)\n```\ni can imagine these being genericized but not without breaking the API\nAlso in the fs portion (not there yet just noticed it before)\nconst (\n    LabelSystemRoot   = \"root\"\n    LabelDockerImages = \"docker-images\"\n)\nIn general the logic is needed for rkt as well but the APIs makes it appear docker specific when its not.  sorry if I wasn't clear.\n. a first step: https://github.com/google/cadvisor/pull/1049\nthe APIs are still docker named in a docker specific way, should think about a way to rev them?\n. i signed it!\nOn Wed, Jan 13, 2016 at 6:28 PM, googlebot notifications@github.com wrote:\n\nThanks for your pull request. It looks like this may be your first\ncontribution to a Google open source project. Before we can look at your\npull request, you'll need to sign a Contributor License Agreement (CLA).\n[image: :memo:] Please visit https://cla.developers.google.com/\nhttps://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll\nverify. Thanks.\n\nIf you've already signed a CLA, it's possible we don't have your\n  GitHub username or you're using a different email address. Check your\n  existing CLA data https://cla.developers.google.com/clas and verify\n  that your email is set on your git commits\n  https://help.github.com/articles/setting-your-email-in-git/.\nIf you signed the CLA as a corporation, please let us know the\n  company's name.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1049#issuecomment-171511460.\n. ok.  here is another attempt but I think the moving of files around is going to make rebase hard.  or i dont know what im doing?\n. The problem is that its not just a rename.  Its splitting the current\nmanager.go in half.  I.e. just a rename doesnt work.\nOn Jan 14, 2016 5:11 PM, \"Yifan Gu\" notifications@github.com wrote:\n@sjpotter https://github.com/sjpotter It's a little more complicated\nthan normal if the file you moved is modified at master.. It's easy to\nrevert the new changes by accident.\nMaybe just do 2 commits from the current master? e.g.\n- rename.\n- made necessary changes.\nThis should make it easier to review.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1049#issuecomment-171834797.\n. so that goes to what cadvisor people prefer.  Do you want 2-3 separate patches that make the changes clearer though each individually will not result in a system that can be built.  If so, I can do that.\n. @yifan-gu I understand that, I was more concerned about in future if there's a bug and people try to do things like bisect.  I tend to try to make my commits atomic and buildable independently.\n. able to plug in rkt (and other container engines).  Want to proceed in steps.  so this was refactoring docker out to enable other containers to write their own managers.\n. I expect that code will be shared, but wanted to make this simpler at first to be able to experiment and hence figured that will come later.\n\nI expect that the current interface is actually pretty good except for the docker specific names in some of the api calls (and have been digging into fs/ stuff and the labels there)\nI think raw cgroups goes along w/ the docker/rkt co-existing Q.  Right now, that might be difficult and could require a lot more surgery to the code, haven't thought about it enough.\n. ok, also working on figuring out how to fill in the container/ structs for\nrkt.\nOn Thu, Jan 21, 2016 at 4:25 PM, Vish Kannan notifications@github.com\nwrote:\n\nSGTM. This PR LGTM in that case. I'd like to see factoring out of raw\ncgroups driver before adding new code for other runtimes though.\nOn Wed, Jan 20, 2016 at 6:37 PM, feisky notifications@github.com wrote:\n\nI think raw cgroups should be separated from docker/rkt/hyper container\nruntimes, which means\n- raw manager should be coexist with other container runtime manager\n- container manager could be configurable to select runtime\n- api should be changed not specific to docker\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1049#issuecomment-173432967.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1049#issuecomment-173759887.\n. Different approach is going to be used.\n. ok, that was a disaster.  unsure what happened here.\n. ok, all better now.\n. In order to do this nicer I'm thinking \n\n1) a basically nil rkt implementation in containers/ just enough for FS stuff\n1a) this will involve some flags stuff, so will be a bit of dance with kubelet as it embeds it.\n2) a basically nil rkt implementation in manager/ again just enough for FS stuff\n2a) kubelet and cadvisor itself will specify which manager it wants based on flags passed in. \nas some of thse flags are going to be the same as what kubelet has now, there might be a bit of a dance to get them unified without breaking anything?\nthoughts, though thats beyond what we talked about above.  just putting it out there as I find the above ugly :)\n. This is superseded by https://github.com/google/cadvisor/pull/1120\n. same issue as before, trying to fix, will probably be a bad commit then a reset again\n. I should note that this isn't ready there's a lot of interplay between container/ and manager/ which I haven't figured out yet, posted this now so that there can be discussion.\n. @k8s-bot test this\n. @timstclair is what I just pushed along the lines of what you were thinking?\n. is the above jenkins error a failure in the testing infrastructure or in my code?  looks like in the testig infrastructure?\n\"F0210 20:23:56.256849   11661 runner.go:286] Error 0: error reading local log file: open ./log.txt: no such file or directory\"\n. so instead of the the container handler being instantiated in the manager,\nhave it passed to the manager with some more methods to call the specific\nfunctionality needed?  i.e. CreateFSContext() for that stuff?\nOn Wed, Feb 10, 2016 at 12:43 PM, Tim St. Clair notifications@github.com\nwrote:\n\n@timstclair https://github.com/timstclair is what I just pushed along\nthe lines of what you were thinking?\nCloser, but rather than a bunch of switch statements I think it would be\nbetter to encapsulate those branch points in the ContainerHandler, and\nsimply delegate to the container handler at that point. The manager might\nneed a list of handlers (e.g. for Raw containers), but I'm not sure exactly\nhow that would work. Maybe it would be good to schedule a VC meeting to\ndiscuss this in more detail?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1105#issuecomment-182575515.\n. the above commit (and a small fix that will come) pulls out the container configuration into its own factory method \n. though apparently it seems to insist on me assigning copyright to google?\n. and again, need to figure out why a branch off an up to date master causes other things to be pulled in\n. ok, I'll see what I can do.\n\nOn Thu, Feb 25, 2016 at 3:52 PM, Vish Kannan notifications@github.com\nwrote:\n\nIt will help if we can split this PR into several ones - Create a common\npackage, move docker and raw handlers to use that and then add rkt.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1120#issuecomment-189039025.\n. this was supreseded by merged PRs https://github.com/google/cadvisor/pull/1128 and https://github.com/google/cadvisor/pull/1154\n. This goes to a better Q, why is the labels stuff so complicated?  \n\nwhy don't we just say label-x maps to directory y and jus statfs that directory.  I'm sure there's some history behind it, just wondering.\n. notes\n1) container/raw/container_hints.go \u2192 container/common/container_hints.go\nmoves and exported elements as now in a different pacakge\n2) container/raw/container_hints_test.go \u2192 container/common/container_hints_test.go\nmoved and use exported elements\n3) container/docker/fsHandler.go \u2192 container/common/fsHandler.go\nmoved and exported elements\n4) container/common/helpers.go\nmoved common code from raw container and behaves in an abstract class type manner through helper Get functions\n5) container/raw/inotify_watcher.go \u2192 container/common/inotify_watcher.go\nsimple move to new package\n6) container/docker/handler.go\nuses the FsHandler struct that was moved out of the docker package to common\n7) container/raw/factory.go\nuses code / structs that were moved to common package\n8) container/raw/handler.go\nuses code moved to common package and implements the abstract Get function interface to be usable by the common package.\n. @vishh @timstclair @yifan-gu @jonboulle \n. poke @timstclair @vishh \n. @timstclair done\n. what do I need to do?\n. Later as dont undetstand it enough to fix it now.\nOn Mar 7, 2016 10:11 AM, \"Tim St. Clair\" notifications@github.com wrote:\n\nI think @vishh https://github.com/vishh was asking whether you are\ngoing to resolve\nhttps://github.com/google/cadvisor/pull/1128/files#r54950999 now or later.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1128#issuecomment-193375313.\n. @vishh @timstclair @yifan-gu @jonboulle \n. It still basically does that.  The idea is that \"system.slice\" cgroup itself isn't a \"real\" cgroup.\n\ni.e. given a cgroup path\n/sys/fs/cgroup/cpuset//machine.slice/machine-rkt\\\\x2d3c642e7b\\\\x2d7e44\\\\x2d42a1\\\\x2dba5f\\\\x2d2da88df3909c.scope/system.slice/alpine-sh.service\nI want to consider the cgroups under machine.slice to be the \"pod\" (i.e. it will be the only one to report network stats as they are global to the pod) and the cgroups under system.slice to be the \"containers\". And I want those containers to be the direct subcontainers of the pod.\nSo, yea, I could say that the rkt handler doesn't accept or handle \"system.slice\" but it wouldn't have any parent or child from a raw container so would just disappear, seems better to say explicitly it is ignored.\n. making progress, will update later today hopefully\n. @k8s-bot test this\n. @k8s-bot test this\n. @yifan-gu per @vishh its a problem with jenkins not me.\n. @yifan-gu split Godeps into its own commit\n. @vishh need to figure that out, I'm not 100% sure how to do that.  In looking at the docker test cases, there isn't much there to inspire me\ni can probably test individual functions (such as what I use to parse the cgroups), but unsure how to test every part of the e2e process.\nIn practice maybe can do integration tests like you have alrady (but aren't running)\n. I've changed this PR a bit to use new rkt api service support to lookup rkt pods by cgroup path but it has to wait till 1.3 release this week (to be vendored properly).  This is necessary for it to be reliable in the presence of random slice definitions and pod cgroup names.\n. @k8s-bot test this\n. @vishh @timstclair \n. this updates me above table\n| old CanHandleAndAccept |  | new NewContainerHandler |\n| --- | --- | --- |\n| handle | accept | handler |\n| true | false | ignoreHandler |\n| false | true | shouldn't happen |\n| false | false | nil |\n| true | true | !ignoreHandler |\n. any advice on how I can reproduce this to see if I can help?\n. I guess this is just a function of grpc.  i.e. when you try to connect to the docker daemon and cant, its doing the same thing internally.\n. https://github.com/grpc/grpc-go/blob/master/grpclog/logger.go\ndoes it make sense for gprc to use its own logger instead of glog?  worth filing an issue against it?\n. let me try it with a 0s timeout\n. 0, doesn't work, but 1s returns quickly.\n. would you be happier w/ a PR that makes it only take 1s?\n. not sure, etcd people tell me they do something to deal with this in grpc\nhttps://github.com/coreos/etcd/blob/master/clientv3/client.go#L131-L141\nbut having a hard time parsing it.\n. so I'm told its impossible to do a dial and drop in grpc, and the best option is to try to make a raw tcp connection to the service and if it succeeds then to have grpc dial.\n. fixed by https://github.com/google/cadvisor/pull/1218\n. also includes my other PR of the rkt bump\n. @timstclair\n. refreshed PR as one commit was accepted\n. So as of now it can't be easily fixed.  the api service doesn't launch rkt pods, it just monitors the state, so its possible for cgroups to be created (such as when they are created by systemd) before rkt has done anything and hence nothing for hte api-service to return.  \nPeople seem to believe that once coreos ship systemd 229 (and hence will be part of the stage1) that we will be able to handle this better, but healthy skepticism till I see it.\nI'd note that the old code works for cases where a user did a \"rkt run\" on the command line, but doesn't work when rkt is called from a systemd unit at all (ala k8s usage)\n. As of now, think this is going to be deprecated in favor of my other PR, just not closing it yet\n. other approach was used\n. just to note, this support isn't good enough for k8s.  We'll need another release for rktnetes provide stat summary api support\n. tim the last point doesn't bother me as we can easily get the rkt factory to say I could handle this container but I don't accept it and then it wont be handled by any handler.\n. I'd also guess that hyper and all VM based systems will have the same problem as rkt w/ kvm stage1\n. So in thinking about the code.\nCurrently we only have one \"watcher\", which is basically the inotify watcher provided by the raw cgroup handler as well as the ability to recursively list all cgroups to detect any that inotify might have missed\nI don't see a reason why we can't register watchers (all they are are go routines) that would be appropriate for each runtime much like we register multiple container handlers themselves.\neach handler would know how to handle its own watch events as well as return a !accept for cgroups that correspond to its containers (which it will handle via its own mechanism)\nhowever, you still have the race problem of being able to determine if a cgroup belongs to your container or not (i.e. the rkt race described above).  \nthe other thing to think about is that perhaps it doesn't matter if there's a raw handler and a specific handler for the same container, as I'm not sure anything the raw handler does uses much cpu (i.e. the rkt/docker handlers are busier in terms of memory/cpu usage due to how they track disk usage with du but I'm not sure there's really much overhead from the raw handler itself.\nso in this model, each watcher will not just pass a name of the container it sees, but also the \"type\" (which could be raw), handlers will know which types they support and ignore the ones they don't.  so multiple handlers might support raw cgroups names, but we could also have a rkt names, hyper names....\n. > > Wonder if removing that flag is a feasible approach? cc @iaguis @alban @jonboulle\n\nIf we remove that flag, the limits applied to the unit file won't apply to the processes in the pod, which is kind of a fundamental goal for rkt.\n\nso we can't set limits on the command line if we run \"rkt run\" its only settable if run as a systemd unit?\nAnd there's no way to determine what the limits of the cgroup you are running in are?\n. so wouldn't that be a reasonable way to go forward?\n1) enable users to set limits on the rkt command line\n2) enable one to inherit limits from the running context\n3) if inheriting limits, don't allow them to be overridden to be greater.\nedit: also what do --cpu and --memory do now then?\n. I mean manually inherit.  I.e. instead of running in the same unit, which i think is what it does now copy the limits to a new cgroup and run in it.\n. @philips if my understanding of the issue is correct that the race is due to --keep-unit  and and one would only call rkt with --keep-unit within the context of a systemd unit that this might work.\nI can experiment with it\n. So my current approach is to do major surgery on cadvisor and refactor out the watcher to make it easy to plug in a parallel rkt watcher that doesn't really on inotify on the cgroup paths\nfirst part (refactorin cadvisor) is here https://github.com/google/cadvisor/pull/1263\nsecond part that implements it for rkt is not PRd yet but is here https://github.com/sjpotter/cadvisor/tree/new-handler-detection-with-rkt/watcher/rkt\n. pretty sure I solved this issue with my refactor of the cadvisor/rkt integration last spring.. @timstclair @vishh \n. @timstclair \n. it contains 2 parts\n1) pass a \"detection\" type into the CanHandleAndAccept function, i.e. \"raw\" \"rkt\" but can be more in future\n2) break up the createContainer/destroyContainer into unlocked version (along with wrappers that lock them) as well as an updateContainer method that can destroy an old handler while creating a new one.\n. ok, I'll try to work someting up tomorrow, it will basically mirror the existing factory system.\n. so this commit I'm not sure is exactly where you want it, but instead of passing the detectionType all the way through it's just passed to container.NewContainerHandler (i.e. the factory method).\nWhen we register factories we say what type of detection they support (I used a slice so they can technically support more than one, but unsure that's really an issue)\nAnd I expanded the SubcontainerEvent to include the detection type, that way I hope we can just have multiple \"detectors\" writing to the same channel.   Next commit will add a new event called Update which will use the update path from the original commit\n. @timstclair @vishh so lots of surgery, but this moves the raw cgroup container watcher to its own package and enables registering other watchers with the manager to use the same channel to write on.\nI have a polling rkt implementation here https://github.com/sjpotter/cadvisor/tree/new-handler-detection-with-rkt/\n. @k8s-bot test this\n. @yifan-gu correct, this is just the refactor, the rkt support will be a separate drop.  It will be a small rework of the handler along with a new \"watcher\" mechanism that use the rkt api service.\n. @k8s-bot test this\n. Once this pr is merged i have another pr with the rkt implementation ready\nto post.\nTesting it with k8s now.\nOn May 12, 2016 2:15 PM, \"Tim St. Clair\" notifications@github.com wrote:\n\nYes. Please make sure that any remaining issues / PRs are marked with the kubernetes\nv1.3 milestone\nhttps://github.com/google/cadvisor/milestones/Kubernetes%20v1.3\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1263#issuecomment-218888039\n. can do that too. is their a normal way to do that?\n. @timstclair @yifan-gu @euank @jonboulle\n. @f0 how are you running this in rkt fly?\n. so I'm confused, are you running cadvisor in rkt fly or cadvisor crashes when you run a rky fly container?\n. so there are a few things\n\n1) cadvisor does need to run as root\nex on my box without any rkt containers running\n$ ./cadvisor -logtostderr\nI0523 23:33:22.647062   12981 storagedriver.go:50] Caching stats in memory for 2m0s\nI0523 23:33:22.647161   12981 manager.go:144] cAdvisor running in container: \"/user.slice\"\nW0523 23:33:23.081976   12981 manager.go:152] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp 127.0.0.1:15441: getsockopt: connection refused\nI0523 23:33:23.265957   12981 fs.go:139] Filesystem partitions: map[/dev/sda1:{mountpoint:/ major:8 minor:1 fsType:ext4 blockSize:0}]\nI0523 23:33:23.288242   12981 manager.go:198] Machine: {NumCores:4 CpuFrequency:3200000 MemoryCapacity:8040280064 MachineID:aa7fa363ed484b8f8e321f5220335e9d SystemUUID:aa7fa363ed484b8f8e321f5220335e9d BootID:230e101f-46c6-4060-a7b6-5fda10b96897 Filesystems:[{Device:/dev/sda1 Capacity:243780952064 Type:vfs Inodes:15130624}] DiskMap:map[8:0:{Name:sda Major:8 Minor:0 Size:256060514304 Scheduler:deadline}] NetworkDevices:[{Name:enp0s25 MacAddress:54:ee:75:51:0e:b4 Speed:-1 Mtu:1500} {Name:lxcbr0 MacAddress:4e:41:1b:e6:33:74 Speed:0 Mtu:1500} {Name:wlp4s0 MacAddress:34:02:86:43:ed:24 Speed:0 Mtu:1500} {Name:wwp0s20u4 MacAddress:a6:72:6e:38:5c:8f Speed:0 Mtu:1428}] Topology:[{Id:0 Memory:8040280064 Cores:[{Id:0 Threads:[0 1] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[2 3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:4194304 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}\nI0523 23:33:23.288705   12981 manager.go:204] Version: {KernelVersion:4.4.0-22-generic ContainerOsVersion:Ubuntu 16.04 LTS DockerVersion:1.10.3 CadvisorVersion:0.23.2 CadvisorRevision:5f11937}\nI0523 23:33:23.469648   12981 factory.go:217] registering Docker factory\nE0523 23:33:23.469692   12981 manager.go:246] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp 127.0.0.1:15441: getsockopt: connection refused\nI0523 23:33:23.469780   12981 factory.go:54] Registering systemd factory\nI0523 23:33:23.470383   12981 factory.go:86] Registering Raw factory\nI0523 23:33:23.470940   12981 manager.go:1087] Started watching for new ooms in manager\nI0523 23:33:23.472128   12981 oomparser.go:185] oomparser using systemd\nI0523 23:33:23.472450   12981 manager.go:287] Starting recovery of all containers\nI0523 23:33:23.472472   12981 manager.go:292] Recovery completed\nF0523 23:33:23.472485   12981 cadvisor.go:145] Failed to start container manager: inotify_add_watch /run/lxcfs/controllers/memory: permission denied\n$\n2) why does it behave differently when run as root vs when not run as root with and without rky fly.\non that note, what version of cadvisor are you running?\nIf I have time tomorrow I'll try to duplicate your setup.\n. so duplicated with pause container, as of now I think its just root vs. non root as yea, the first failure is on a rkt pod, but without the rkt pod I see the same failure on something else.\n. so dumping some data\nthis is libcontainer.GetCgroupSubsystems() return, notice the lxcfs mounts which don't play friendly with cadvisor\n(libcontainer.CgroupSubsystems) {\n Mounts: ([]cgroups.Mount) (len=10 cap=22) {\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=26) \"/sys/fs/cgroup/cpu,cpuacct\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=2 cap=2) {\n    (string) (len=3) \"cpu\",\n    (string) (len=7) \"cpuacct\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=26) \"/sys/fs/cgroup/cpu,cpuacct\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=2 cap=2) {\n    (string) (len=3) \"cpu\",\n    (string) (len=7) \"cpuacct\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=21) \"/sys/fs/cgroup/memory\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=1 cap=1) {\n    (string) (len=6) \"memory\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=20) \"/sys/fs/cgroup/blkio\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=1 cap=1) {\n    (string) (len=5) \"blkio\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=21) \"/sys/fs/cgroup/cpuset\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=1 cap=1) {\n    (string) (len=6) \"cpuset\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=29) \"/run/lxcfs/controllers/cpuset\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=1 cap=1) {\n    (string) (len=6) \"cpuset\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=28) \"/run/lxcfs/controllers/blkio\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=1 cap=1) {\n    (string) (len=5) \"blkio\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=29) \"/run/lxcfs/controllers/memory\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=1 cap=1) {\n    (string) (len=6) \"memory\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=34) \"/run/lxcfs/controllers/cpu,cpuacct\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=2 cap=2) {\n    (string) (len=3) \"cpu\",\n    (string) (len=7) \"cpuacct\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=34) \"/run/lxcfs/controllers/cpu,cpuacct\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=2 cap=2) {\n    (string) (len=3) \"cpu\",\n    (string) (len=7) \"cpuacct\"\n   }\n  }\n },\n MountPoints: (map[string]string) (len=5) {\n  (string) (len=3) \"cpu\": (string) (len=34) \"/run/lxcfs/controllers/cpu,cpuacct\",\n  (string) (len=7) \"cpuacct\": (string) (len=34) \"/run/lxcfs/controllers/cpu,cpuacct\",\n  (string) (len=5) \"blkio\": (string) (len=28) \"/run/lxcfs/controllers/blkio\",\n  (string) (len=6) \"memory\": (string) (len=29) \"/run/lxcfs/controllers/memory\",\n  (string) (len=6) \"cpuset\": (string) (len=29) \"/run/lxcfs/controllers/cpuset\"\n }\n}\nand when I have a pause container running with fly\n(libcontainer.CgroupSubsystems) {\n Mounts: ([]cgroups.Mount) (len=15 cap=33) {\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=26) \"/sys/fs/cgroup/cpu,cpuacct\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=2 cap=2) {\n    (string) (len=3) \"cpu\",\n    (string) (len=7) \"cpuacct\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=26) \"/sys/fs/cgroup/cpu,cpuacct\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=2 cap=2) {\n    (string) (len=3) \"cpu\",\n    (string) (len=7) \"cpuacct\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=21) \"/sys/fs/cgroup/memory\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=1 cap=1) {\n    (string) (len=6) \"memory\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=20) \"/sys/fs/cgroup/blkio\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=1 cap=1) {\n    (string) (len=5) \"blkio\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=21) \"/sys/fs/cgroup/cpuset\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=1 cap=1) {\n    (string) (len=6) \"cpuset\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=29) \"/run/lxcfs/controllers/cpuset\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=1 cap=1) {\n    (string) (len=6) \"cpuset\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=28) \"/run/lxcfs/controllers/blkio\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=1 cap=1) {\n    (string) (len=5) \"blkio\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=29) \"/run/lxcfs/controllers/memory\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=1 cap=1) {\n    (string) (len=6) \"memory\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=34) \"/run/lxcfs/controllers/cpu,cpuacct\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=2 cap=2) {\n    (string) (len=3) \"cpu\",\n    (string) (len=7) \"cpuacct\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=34) \"/run/lxcfs/controllers/cpu,cpuacct\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=2 cap=2) {\n    (string) (len=3) \"cpu\",\n    (string) (len=7) \"cpuacct\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=122) \"/var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/cgroup/cpu,cpuacct\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=2 cap=2) {\n    (string) (len=3) \"cpu\",\n    (string) (len=7) \"cpuacct\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=122) \"/var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/cgroup/cpu,cpuacct\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=2 cap=2) {\n    (string) (len=3) \"cpu\",\n    (string) (len=7) \"cpuacct\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=117) \"/var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/cgroup/memory\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=1 cap=1) {\n    (string) (len=6) \"memory\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=116) \"/var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/cgroup/blkio\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=1 cap=1) {\n    (string) (len=5) \"blkio\"\n   }\n  },\n  (cgroups.Mount) {\n   Mountpoint: (string) (len=117) \"/var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/cgroup/cpuset\",\n   Root: (string) (len=1) \"/\",\n   Subsystems: ([]string) (len=1 cap=1) {\n    (string) (len=6) \"cpuset\"\n   }\n  }\n },\n MountPoints: (map[string]string) (len=5) {\n  (string) (len=3) \"cpu\": (string) (len=122) \"/var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/cgroup/cpu,cpuacct\",\n  (string) (len=6) \"memory\": (string) (len=117) \"/var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/cgroup/memory\",\n  (string) (len=5) \"blkio\": (string) (len=116) \"/var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/cgroup/blkio\",\n  (string) (len=7) \"cpuacct\": (string) (len=122) \"/var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/cgroup/cpu,cpuacct\",\n  (string) (len=6) \"cpuset\": (string) (len=117) \"/var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/cgroup/cpuset\"\n }\n}\nnow we see the rkt mounts too.  This is because rkt fly is just a chroot so it operates in the same mount namespace as the host which is confusing the libcontainer code (though on my machine lxcfs also does that)\nas one can see from the extra mounts in the host namespace\nudev on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/dev type devtmpfs (rw,nosuid,relatime,size=3905788k,nr_inodes=976447,mode=755)\ndevpts on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000)\ntmpfs on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/dev/shm type tmpfs (rw,nosuid,nodev)\nmqueue on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/dev/mqueue type mqueue (rw,relatime)\nhugetlbfs on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/dev/hugepages type hugetlbfs (rw,relatime)\nproc on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/proc type proc (rw,nosuid,nodev,noexec,relatime)\nsystemd-1 on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/proc/sys/fs/binfmt_misc type autofs (rw,relatime,fd=31,pgrp=1,timeout=0,minproto=5,maxproto=5,direct)\nsysfs on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys type sysfs (rw,nosuid,nodev,noexec,relatime)\nsecurityfs on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/kernel/security type securityfs (rw,nosuid,nodev,noexec,relatime)\ntmpfs on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/cgroup type tmpfs (rw,mode=755)\ncgroup on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd,nsroot=/)\ncgroup on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct,nsroot=/)\ncgroup on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory,nsroot=/)\ncgroup on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio,nsroot=/)\ncgroup on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids,release_agent=/run/cgmanager/agents/cgm-release-agent.pids,nsroot=/)\ncgroup on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio,nsroot=/)\ncgroup on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer,nsroot=/)\ncgroup on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event,release_agent=/run/cgmanager/agents/cgm-release-agent.perf_event,nsroot=/)\ncgroup on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb,release_agent=/run/cgmanager/agents/cgm-release-agent.hugetlb,nsroot=/)\ncgroup on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset,clone_children,nsroot=/)\ncgroup on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices,nsroot=/)\npstore on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/pstore type pstore (rw,nosuid,nodev,noexec,relatime)\ndebugfs on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/kernel/debug type debugfs (rw,relatime)\nfusectl on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/sys/fs/fuse/connections type fusectl (rw,relatime)\ntmpfs on /var/lib/rkt/pods/run/11e9d827-35d9-486c-8915-5d3600c528fe/stage1/rootfs/opt/stage2/pause/rootfs/tmp type tmpfs (rw,relatime)\non the flip side, if I run as normal rkt\nthose cgroup paths aren't there.  I dont think this is a rkt issue, but a libcontainer issue.  @vishh  what do you think?\n.   in containers/libcontainers/helpers.go we have\n```\nfunc GetCgroupSubsystems() (CgroupSubsystems, error) {\n        // Get all cgroup mounts.\n        allCgroups, err := cgroups.GetCgroupMounts()\n        if err != nil {\n                return CgroupSubsystems{}, err\n        }\n        if len(allCgroups) == 0 {\n                return CgroupSubsystems{}, fmt.Errorf(\"failed to find cgroup mounts\")\n        }\n    // Trim the mounts to only the subsystems we care about.\n    supportedCgroups := make([]cgroups.Mount, 0, len(allCgroups))\n    mountPoints := make(map[string]string, len(allCgroups))\n    for _, mount := range allCgroups {\n            for _, subsystem := range mount.Subsystems {\n                    if _, ok := supportedSubsystems[subsystem]; ok {\n                            supportedCgroups = append(supportedCgroups, mount)\n                            mountPoints[subsystem] = mount.Mountpoint\n                    }\n            }\n    }\n\n    return CgroupSubsystems{\n            Mounts:      supportedCgroups,\n            MountPoints: mountPoints,\n    }, nil\n\n}\n```\nperhaps need to do a better job at trimming?\n. @f0 if cadvisor runs first, does it still have problems?\n. So i think my analysis is then correct.\n. It might be possible to run without root but it wasnt designed as such as @vishh said\n. I still think there's a problem in terms of what cgroup paths its getting though\n. i'm no longer working on rkt/cadvisor. @timstclair this can probably be reviewed now, no tests yet, but infrastructure is there\n. @k8s-bot test this\n. So I've made progress on rkt tests, except tht rkt has to run as root.\n. @timstclair the above error is expected as jenkins now fails test as rkt isn't running on the machines, what do we want to do now?\n. I'll see if I can test this later\n. no longer working on rkt/cadvisor stuff.    someone else would have to attack it. . I agree, didn't want to change the API at this point though\n. Im redoing the patch.  I cant seem to rebase it just endless loop even\nafter i fix all merge errors.  Probably do to not using git mv correctly\nfirst time around.\nOn Jan 14, 2016 3:59 PM, \"Tim St. Clair\" notifications@github.com wrote:\n\nIn manager/docker_manager.go\nhttps://github.com/google/cadvisor/pull/1049#discussion_r49804713:\n\n@@ -0,0 +1,1183 @@\n+// Copyright 2014 Google Inc. All Rights Reserved.\n\n(nit) 2016\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1049/files#r49804713.\n. first nit just copied from manager.go (i.e. default listed there)\n\nsecond nit, I'll do that.\n. Rkt seems to refer to it as RktPath so was trying to be consistent with that.\n. oh, I agree that there is a lot of duplicated code.  plan today was to try to refactor out the duplicate code and make a ManagerInternal interface that it would use.\nwishing go had abstract classes though.\n. and I'm thinking basically what you're saying is that the current manager.go is good enough but can stick in some switch statements to control which container handler is loaded?\n. I'm hoping to get this info from the rkt api-service at some point in time,\nbut right now that service is limited.\nOn Wed, Feb 10, 2016 at 3:53 PM, Vish Kannan notifications@github.com\nwrote:\n\nIn container/rkt/factory.go\nhttps://github.com/google/cadvisor/pull/1105#discussion_r52546728:\n\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package rkt\n+\n+import (\n-   \"flag\"\n-   \"fmt\"\n  +\n-   \"github.com/google/cadvisor/container\"\n-   \"github.com/google/cadvisor/fs\"\n-   info \"github.com/google/cadvisor/info/v1\"\n  +)\n  +\n  +// Basepath to all container specific information that libcontainer stores.\n  +var rktPath = flag.String(\"rkt_path\", \"/var/lib/rkt\", \"Absolute path to the Rkt root directory\")\n\nnit: The current code base is complex because flags are configured\nalongside regular code. Since you are adding new (and better) code, can we\nmove the flags to a top level file and inject it into this package?\nSo instead of providing a static method in this package, the path will\nonly be returned from a rkt factory object.\nI intend to perform a similar surgery on the docker factory.\nAlso, is it possible to auto-detect this path?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1105/files#r52546728.\n. See my other pr i do that there.\nOn Feb 10, 2016 3:49 PM, \"Vish Kannan\" notifications@github.com wrote:\nIn fs/fs.go\nhttps://github.com/google/cadvisor/pull/1104#discussion_r52546372:\n\n@@ -165,11 +171,19 @@ func (self *RealFsInfo) addDockerImagesLabel(context Context) {\n        dockerPaths := getDockerImagePaths(context)\nfor src, p := range self.partitions {\n-           self.updateDockerImagesPath(src, p.mountpoint, dockerPaths)\n-           self.updateContainerLabelImagesPath(LabelDockerImages, src, p.mountpoint, dockerPaths)\n\nIt is kind of weird to make addDockerImagesLabel update labels for rkt as\nwell. This logic might require a more thorough refactoring.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1104/files#r52546372.\n. I think the Q is, what api is this trying to provide, I've tried to make heads or tails out of it, and I don't know.  \n\ni.e. why isn't doing a statfs on a given dir good enough at all times?  why do we care about the partition table?\n. It's not really the container handler interface, its a specific interface for just the common helper code.  unsure what a good name is, done to try and be abstract class like\n. so for now perhaps say if mount point is / and its overlay we dont ignore\nit?\nOn Fri, Feb 26, 2016 at 2:43 PM, Vish Kannan notifications@github.com\nwrote:\n\nIn fs/fs.go\nhttps://github.com/google/cadvisor/pull/1125#discussion_r54312699:\n\n@@ -78,9 +78,10 @@ func NewFsInfo(context Context) (FsInfo, error) {\n    }\n    supportedFsType := map[string]bool{\n        // all ext systems are checked through prefix.\n-       \"btrfs\": true,\n-       \"xfs\":   true,\n-       \"zfs\":   true,\n-       \"btrfs\":   true,\n-       \"overlay\": true,\n\nIdeally docker root fs should be detected using docker's APIs (info) and a\nstatfs on that should be good enough.\nThis code just needs some love.\nThis logic here was originally meant for discovering filesystems of\ninterest at a global level.\nLayered filesystems that are ephemeral are not of interest to most users\nin the global context.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1125/files#r54312699.\n. this was perhaps me not understanding the point of it.\n\nI use functionality (perhaps not required) in the rkt handler, when this is comitted and I give a PR for that, can perhaps change this.\n. I think its for testing purposes?  i.e. a test file is passed in by hand during unit testing\n. can we just merge this and figure out what to do with this once I drop the\nrkt handler PR?\nOn Thu, Mar 3, 2016 at 12:28 PM, Tim St. Clair notifications@github.com\nwrote:\n\nIn container/common/container_hints.go\nhttps://github.com/google/cadvisor/pull/1128#discussion_r54943076:\n\n@@ -25,7 +25,7 @@ import (\n    \"os\"\n )\n-var argContainerHints = flag.String(\"container_hints\", \"/etc/cadvisor/container_hints.json\", \"location of the container hints file\")\n+var ArgContainerHints = flag.String(\"container_hints\", \"/etc/cadvisor/container_hints.json\", \"location of the container hints file\")\n\nThe test could set the flag value instead. This is pretty nit-picky though.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1128/files#r54943076.\n. I wasn't saying it was a test flag, I was saying we couldn't make the function not take an argument, but just use the flags value as test code passes a value to the function\n. So this is the same exact code as the raw container, I didn't know if it was recursive or not.  If it is and it just works because of the raw container support, that's great.\n. It will be.  At this point I just wanted to get it in wit ha static path, we've dded support for getting this info out of the api service, just need to figure out how to use its api.\n. will be removed\n. I think it is, need a pid in the cgroup to get networking stats to work.\n. Don't have that done yet, but yes, need to figure out how to role that up (assuming this is the journald stuff we talked about previously?)\n. yes, just haven't quite figured out what goes in there yet.\n. SJP being me.  J is for Joseph\n. not needed.  was learning how it worked.\n. this is straght up copied from raw, goes to other Q if this code is even needed if raw container does all the watching?\n. again straight up copied from raw, so should look there too.\n. so, logs will be pod level storage, not with the containers.  Is that ok?\n. ok, removed and tested and you're right, works just fine.\n. so, made some progress on this but ran into the godeps issue listed in https://github.com/google/cadvisor/issues/1161\n. so ran into an issue here, the logic here of retrieving the pid isn't good enough, i.e. the cgroup is created before there is any data in that file and can be read before its filled in.  Unsure what the apporpriate way to deal with it, can treat it as os.Getpid() until their is something in the file (my current hack) but unsure that is best.\n. using api service for this and the network namespace pid.\n. using api service\n. removed\n. code fixed to not depend on this fragile code.\n. Its been added.\n. no more watcher in rkt handler\n. no more watcher\n. done\n. done\n. done\n. fixed\n. removed\n. It's what the docker handler does\n. fixed\n. fixed\n. fixed\n. So this isn't needed as a comment, the point of the code is that we only need the pid of the \"pod\" container, not a pid of each individual \"app\" container as the pid used only for networking which is at the pod layer.\n. ok, removing hints\n. and the external mounts stuff, still need to be an empty slice to work with common code\n. logs are stored in the journald in the pod's container and are accounted at the pod level.  is that sufficient?\n. a little confused by what is being asked here?  Answering my Q?  so no code change really?\n. you mean my SJP comments?\n. yes.\n. added a first cut at label support, not 100% sure its correct (i.e. we have Pod level labels and app level labels, applying them to the respective containers, but perhaps all pod labels should apply to apps as well)\n. fixed\n. fixed comment\n. out of scope for this PR though?\n. no, if we want to ignore this container then should return a nil handler.  ignore is only if we return a nil handler to tell the factory if it should ignore this container or continue checking the other factories\n\ni.e. ignore is the equivalent of handle = true, accept = false in the previous code.\n. but doesn't this prevent NewContainerHandler from returning other errors that might occur?\n. perhaps a better way is to return an IgnoreHandler instead of nil?\n. see my current update\n. was just mirroring what is done for rkt in k8s\n. is it bad unnecessary?  I was just trying to be clear with the code.\n. so currently we detect containers via raw cgroups (what I'll call the \"raw\" method).  I'm proposing the ability to detect containers via other methods, such as the rkt api service.\nif a raw handler was already created for a cgroup path for a rkt container (via raw detection), we want the rkt path to overwrite/update it to the rkt handler.  Or at least that's what I thought you wanted.  i.e. the raw handler will be destroyed and a rkt handler will take its place.  all in one function to ensure atomic nature.\nif the rkt handler is created first, the raw handler wont be created as createContainer() skips if the cgroup is already a key in the map.\n. done\n// Enables overwriting an existing containerData/Handler object for a given containerName\n// createContainer just returns if a given containerName has a handler already\n// Ex: rkt handler will want to take priority over the raw handler, but the raw handler might be created first\n. I'm trying to minimize the code duplication, so only have them all going through the same event loop which goes to the same factory loop.\nHowever, since we are dealing with the same container names if we don't distinguish where the container name came from, rkt wouldn't be able to determine if it was for it or not and would have to wait to make sure.  it's just labeling which watcher found it.\nI'm not convinced this is right, but I'm trying to keep the code simple and not overly complicated\n. yea, it can be sub manager, but not 100% sure why you want the raw bumped up, looses the symetry with any other watcher that is added\n. I'm not sure what you mean, what is missing?\n. done\n. done, at least in my watcher code, its still in manager.go itself for its internal usages.\n. this is the exact same code as before.  (i.e. re deadlock)  We spoke before about this being sort of pointless, perhaps its ok to not return an error and just signal by closing the quit channel?\n. removed the event for now\n. trying to justify this method as an addition to createContainer()\n. Its now removed in this branch and is part of the rkt addons branch, but the reason is to only allow overriding a raw handler.\ni.e. cgroup gets a raw handler, then we say it should get a rkt handler.  it just to enforce.  If you dont think its neccessary can remove it\n. you did, and I did, and just pushed it.\n. done\n. we could for now, but its needed for identifying handlers later on\n. ok, in progress\n. pretty sure you can, or at least I thought it as such (i.e. err would be for api returning an error, while pod == nil means api didn't return an error, but api couldn't match cgroup to pod)\n. rereading code I factored that away.  need to decide what's appropriate\n. changed it, so that 0 pods isn't an error, just a nil, nil return, only propegate errors if an actual error state occurs\n. agreed\n. not implemented yet :(\n. I removed one of the bools, and duplicate it in the CanHandleAndAccept factory function\nnot sure I have a good reason, can move it into the factory,go if you think its better that way\n. glog.Warningf(\"couldn't find app %v in pod\", parsed.Container)\n. comment isn't accurate anymore, this PR fixes that TODO.  removed, but misinterpreted comment, want a fmt.Errorf() done.\n. don't want to waste time destroying a rkt handler and replacing it with a rkt handler.\ni.e. cadvisor restarts so rkt container handler will work for cgroups that exist, a little later the rkt watcher sends those same cgroups, no need to destroy/recreate\n. I view the err as sort of cross between ok and err here.\ni.e. its ok to not be able to convert a cgroup to a pod, it might not be a pod.  err is only if the code path actually fails\n. done\n. did, but unsure its better.  as it has to be a member function instead of just a utility/library function\n. @euank don't understand your Q.\n. the rkt watcher is only registered if the rkt handler is registered which is if the api service is running\nsee: https://github.com/sjpotter/cadvisor/blob/0dbdd7eca8562e07ab09f543721dbeaadf71dd46/manager/manager.go#L239-L248\n. don't know cost at this point, we can measure, but that seems like an easy tweak for later\n. It's in the comment above, can make it clearer\n. parseName() is only called from a newrktHandler() which is only for cgroup names that passed being accepted by the rocket handler.  If iit reached that point we have a path name that shouldn't have passed that did.  (example a system.slice cgroup)\n. no, the rkt handler will always only handle running pods, it always filters with that.\n. why?  I guess I like the return separated from the go routine\n. So there are 2 things\n1) we can avoid having the Rkt handler run at all for \"Raw\" cgroups, I'm personally in favor of that.\n2) considering listing pods tkaes about .1-.2ms for 100 pods, it should be less for this.\n. I think this is old code, don't see it anymore?\n. One thing I was pondering was adding flags to the event struct, and one would be overridable.  i.e. instead of having manager have to know which watcher sources can override others.\ni.e. we don't want the raw handler overwriting itself (i.e. via housekeeping this will happen a lot).\n. was having a hard time lining up the code from the diff, I see it now.\nbut yes, it's the pod container\n. ok, so all you are asking for is removing the event, I'll try something and\nsee if we're on the same page\nOn Mon, May 16, 2016 at 3:35 PM, Tim St. Clair notifications@github.com\nwrote:\n\nIn manager/manager.go\nhttps://github.com/google/cadvisor/pull/1284#discussion_r63437740:\n\n@@ -783,6 +790,35 @@ func (m manager) registerCollectors(collectorConfigs map[string]string, cont c\n    return nil\n }\n+// Enables overwriting an existing containerData/Handler object for a given containerName.\n+// can't use createContainer as it just returns if a given containerName has a handler already\n+// Ex: rkt handler will want to take priority over the raw handler, but the raw handler might be created first\n+\n+// Only allow raw handler to be overriden\n+func (m *manager) overrideContainer(containerName string, watchSource watcher.ContainerWatchSource) error {\n\nWhat do you mean add flags to the event struct? I'd prefer for the logic\nto be in the manager since it actually knows which watcher sources are\nrunning. The watchers should be independent. Why would the raw handler try\nto overwrite itself?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1284/files/29e0782cedec140df582c9422f4414e4cb3d611d#r63437740\n. I'm trying to keep it the same as the Raw watcher which does want to return an error value and the errors are collected by manager when it tries to stop all watchers.\n. that's just a rename of the DockerRunArgs.  I assumed that args is to the runtime while innerargs are passed to the container?\n. \n",
    "euank": "Are we happy enough with the rkt support we have that we should close this issue and track any remaining problems as new issues (e.g. there's a PR open for improved testing which can track that)?\n. Just to throw in a contrarian note: cAdvisor today works with cgroups, as do docker and rkt (for most stage1s). The rkt integration does not handle the case of a hypervisor stage1 (at least not at an app level) either afaik.\nI imagine hyper integration would just have to look like 1) linking a chain of cadvisor processes together (one running in the virt, one running with the kubelet) 2) simply querying the hypervisor for metrics somehow and taking them as truth, or 3) the hypervisor mocking out a cgroup-like filesystem for cadvisor to consume.\nI'd argue that if it's the 2nd option, which I suspect it is, that it should not be cadvisor's place to integrate with that runtime, but rather kubelet's place to do so.\nI feel like I've run into such discussion of whether cadvisor should actually be responsible for non-cgroup stats ever, but I can't find it now, so if anyone has a link to that or a conclusion from it, that would be helpful.\nAt the very least, I'd like to hear a little more detail about what the planned integration would look like if there's any idea yet.\n. Do you have more information on how these images are setup and so on (sorry, not familiar with the cadvisor test setup)? For reference, node_e2e disable updates now with this.\nIf the cadvisor tests fail on newer CoreOSs post-update, that sounds like something to triage and fix..\n. I'll take a guess that this is related to systemd >= 226's change in cgroup hierarchy, but not sure yet.\nI was able to reproduce on a machine with systemd 226 and docker 1.11 launched with --exec-opt native.cgroupdriver=systemd.\nThe machine in question is gentoo, but I expect it'll reproduce broadly in that configuration. I'll dig further...\n. It finds a cpuset root at both /sys/fs/cgroup/cpuset/system.slice/docker-de6460b5039fa64f505cf383c15dc96d515bbc507c76ec0f7c06a00a5115002f.scope and /sys/fs/cgroup/cpuset/init.scope/system.slice/docker-de6460b5039fa64f505cf383c15dc96d515bbc507c76ec0f7c06a00a5115002f.scope (note the init.scope bit), then tries to use the latter and doesn't find the files it expects and returns \"\". The first one would be correct. I think this is https://github.com/docker/docker/issues/16256#issuecomment-218935000 .. I'm giving a quick go at patching this similarly to as suggested to be sure.\n. Upstream bug to point to as well: https://github.com/opencontainers/runc/issues/931\nOur options are, I think:\n1) Wait for runc + docker-on-coreos to be fixed (we might backport the fix)\n2) Switch the CoreOS+docker test setup to use cgroupfs driver (non-default)\n3) Switch to CoreOS stable where it's still using systemd 225 and thus shouldn't be affected\n4) modify cadvisor to handle runc's bad behaviour and do the \"right thing\" when there are multiple cgroup paths for one container\nMy preference is 3 to put off having to get a better solution, and hope that 1 happens in the meanwhile. Sound reasonable?\n. We should be able to update the CoreOS node if no one has already now that we've switched coreos to use cgroupfs by default.\nI don't have access to the images referenced by ./build/jenkins_e2e.sh to update appropriately, but any version of CoreOS right now should include that change. If it's possible to use unmodified coreos images as I updated the node e2e to do, that might also help.\n. I don't expect we'll need to do more than is done for the node_e2e stuff (user-data of https://github.com/euank/kubernetes/blob/5a5ba51b24c9e62aa775de1f568d365c2761aeb5/test/e2e_node/jenkins/coreos-init.json basically).\nI'm on vacation for the next couple weeks, so I won't be able to verify that's true, and regardless someone with access to the jenkins account where these test instances run will need to start one up, unless we switch to a node_e2e type model where instances are launched and specified as part of code in this repository, not totally out of band. (#1361 could fix that perhaps).\ncc @yifan-gu @crawford to help with or delegate further on this one, thanks!\n. Checking for the binary won't work if it's not in your path.\nIs there a reason not to make rkt integration opt-in and have kubelet instantiate cadvisor with or without rkt support depending on the runtime it's configured with?\n. @timstclair it was passing because the assertion was if reflect.DeepEqual(expected, actual) { error(\"they weren't equal\") }, a missing ! in the conditional.\nThey weren't ever equal because of the two small fixes: the fact that mem3 doesn't appear in the container sample file, but was expected, and the fact that the system error doesn't set victim container but was expected to.\nSo the backwards assertion always passed. Two wrongs make a PASS\n. @timstclair addressed your comments and fixed a bug where OOM events with a process name containing a - in them would break the parser (wouldn't match the end-line for that oom). Lucky me that my test program has a dash :smile:\nHappy to break that one-line fix out into its own PR if you'd like. Rebased and squashed the commit addressing comments from back whenever.. Bump on this, did a rote rebase and afaict there aren't open concerns (other than waiting on whether @vishh feels like reviewing). @k8s-bot test this\n. Rebased for probably the 10th or so time.\nManually tested via the following:\n```sh\n$ sudo ./cadvisor -v 3 -logtostderr\n...\ncgroup OOM\n$ docker run -m 15M euank/gunpowder-memhog:latest 30M\ncadvisor logs:\nI0830 20:24:01.875221    3264 manager.go:1142] Created an OOM event in container \"/docker/6da85dc9675db12149e970c6ef6835240bab712e5499963d13e88a446248e82b\" at 2017-08-30 20:24:01.583571597 +0000 UTC\nsystem OOM\n$ docker run -d euank/gunpowder-memhog 500M\n$ sleep 5\n$ echo f | sudo tee /proc/sysrq-trigger\ncadvisor logs\nI0830 20:59:27.831699    1611 manager.go:1142] Created an OOM event in container \"/\" at 2017-08-30 20:59:27.717686391 +0000 UTC\n```\n@timstclair if this is going to land, can we land it?\nHaving to come by and rebase it once a month is ridiculous.. @vishh yup, it's been ready for review for 9 months now.. \ud83c\udf89 . @k8s-bot ok to test\n(I don't think the bot will listen to me, but I'll try anyways)\nLGTM. Numbers are a good idea, so I gave it a quick go on a few sample directories.\nI wrote a go program (test-get-inode-usage) that does nothing but call the package-level function in fs to compare with.\nMy methodology was to call each command 3 times to warm the cache, and then run it once more and record the output.\nWhen run on cadvisor itself:\nshell\n$ time test-get-inode-usage\n2331                                                  \ntest-get-inode-usage  0.01s user 0.00s system 21% cpu 0.047 total\n$ time sh -c 'find -xdev -printf '.' | wc -c'\n2331\nsh -c 'find -xdev -printf '.' | wc -c'  0.00s user 0.00s system 0% cpu 0.010 total\nWhen run on a 100G directory with quite a few files, recursing, and all that jazz\n```shell\n$ time sh -c 'find -xdev -printf '.' | wc -c'\n1461051\nreal    0m1.713s\nuser    0m0.560s\nsys 0m1.140s\n$ time test-get-inode-usage \n1459087\nreal    0m5.102s\nuser    0m2.480s\nsys 0m2.810s\n```\nOn /var/lib/docker with a modest set of containers lying around\n```shell\n$ time sh -c 'find -xdev -printf '.' | wc -c'\n3764597\nreal    0m3.678s\nuser    0m1.030s\nsys 0m2.620s\n$ time test-get-inode-usage \n1147504\nreal    0m13.063s\nuser    0m6.500s\nsys 0m6.990s\n$ time sh -c 'find . -printf \"%i\\n\" | sort | uniq | wc -l'\n1147504\nreal    0m12.189s\nuser    0m10.180s\nsys 0m2.760s\n```\nIt's interesting to note that for /var/lib/docker, the new code is roughly 3x more correct than the old code!\nI checked what a random set of files with the same inode looked like (via find -links +2 and find -samefile); an example of the sort of duplicates is:\n./overlay/fc737a3304e188bc985d07d7e5896393e1c7c97c712d83317c8107cddc5ff58b/root/tmp/openssl-1.0.2e/doc/ssl/SSL_set_verify_result.pod\n./overlay/9660ad5520bb82e857bcc326fb1bb9469f0cd0b10bf7dada0c3f81bb6af5c944/root/tmp/openssl-1.0.2e/doc/ssl/SSL_set_verify_result.pod\n.... @timstclair Mind explaining what you mean more clearly?\nMy reading of the code is that the fshandlers are created per-container and apply to the read/write upper layer, so the directory is unique per container and can't be so easily deduped.\nMaybe I'm missing something, and there certainly are optimizations that could be made by being smarter, but it seems like a change to make in the future if we're okay with the decrease in performance of this change.. @dashpole if you're willing to do the footwork to get better numbers, that'd be awesome!. @dashpole @timstclair it's also an option to keep the pure go but remove the map (and thus memory allocation) which will make this only slightly slower than the find method I think.\nI'll also point out that on slower disks (e.g. EBS or spinning rust), both find and this will be roughtly equally slower. On my laptop's SSD the difference is more stark than in environments with worse disk io.\nAnd finally, I think it's still worth doing hardlink deduplication if we can in container rootfs too. There aren't too many applications that use hardlinks at runtime, but they're out there, and it would be great to not evict applications that are doing the right thing in terms of saving inodes.\nI have some quite hacky rsync scripts on my local kubernetes cluster that do use hardlinks and would probably benefit from this change.. @dashpole The downside is that hardlinks get double-counted, which is why I'd prefer to leave it if possible.. I did some more fiddling with optimization here. Tim's suggestion to reduce the map's usage was a good one, and that did help some (though as @dashpole points out, it's still slower).\nAt this point, the slowness largely comes from Go's implementation of Walk in the stdlib, not from the code in my walkFn. Exchanging the walkFn for an empty body results in hardly any difference.\nI tried parallelizing via powerwalk (https://github.com/euank/cadvisor/tree/find-inodes-2) and tweaked the parallelism a bit, but that was slower for all the parallelism values I tried (10, 20, 100, 200).\nI didn't really expect it to since sequential access of files (the walk access -> fstat from the fn being sequential) on a (fairly unfragmented) filesystem is usually faster.\nI'm happy to merge inode and dir usage and into one walk operation. I'm not sure if that'll end up quicker than the current C implementations combined though.. @timstclair The primary issue mentioned in the linked go thread is that for find the caller doesn't need the result of fstat.\nHowever, we do use the fstat result to get the inode, so it's not totally wasted for us. If we're going to merge this with the filesystem size code, then we'll actually need the full fstat.\nThe goimports fastwalk function doesn't need the inode, but getdirents does include it so most of the optimizations it does can apply here if we want to get the inode, but don't want to merge in the directory size code.\nIf we do plan to merge inode and size accounting, then we won't want any of the optimizations linked since all of them optimize away the ability to get filesize.\nWhich direction do you think I should head down?\nOptimizing inode counting in isolation, or unifying size/inode counting code to amortize the time that way.. Looks like there's a test for when docker's using the systemd driver, in which case my claim isn't correct.\nMaybe there's a way to determine whether docker's using the default cgroup driver or the systemd one?. /retest\n. Rebased.. If we're planning to switch to directly parsing kmsg instead, I don't see why this change is necessary.. Thanks for that reference. I decided to go with the hotplug safe solution after checking how docker did it and seeing this: https://github.com/moby/moby/blob/8b1adf55c2af329a4334f21d9444d6a169000c81/daemon/stats/collector_unix.go#L73. @dashpole You don't have to run on aws, you can easily use qemu wit h-smp cpus=4,maxcpus=10 or any other virtualization platform that lets you twiddle those knobs.\nI went ahead and manually tested in that environment.\nBefore this patch:\nsh\n$ uname -a\nLinux coreos_production_qemu-1520-0-0 4.13.0-rc7-coreos #1 SMP Tue Aug 29 23:42:52 UTC 2017 x86_64 Intel(R) Core(TM) i7-6600U CPU @ 2.60GHz GenuineIntel GNU/Linux\n$ curl -s http://localhost:8080/api/v1.0/containers/ | jq '.stats[0].cpu.usage.per_cpu_usage'\n[\n  8366979268,\n  8594483963,\n  7864786060,\n  7013534122,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0\n]\nAfter the patch\nsh\n$ curl -s http://localhost:8080/api/v1.0/containers/ | jq '.stats[0].cpu.usage.per_cpu_usage'\n[\n  10150246524,\n  10894847083,\n  9936348997,\n  9566062369\n]. @dada941 How are you running the kubelet? Can you also post the output of cat /sys/fs/cgroup/cpu,cpuacct/cpuacct.usage_percpu (or cpu or cpuacct depending on which exist) from the Kubelet's perspective (e.g. if it's in some sort of container, from within the container)?\nWe should probably treat '0' as a special value and shortcut out early since if len(s.CpuStats.CpuUsage.PercpuUsage) == 0 that could indicate an error condition I think, such as cgroups not mounted or that cgroup feature not being available.\nAfter I understand why this is happening I can make a patch for that.. I was able to reproduce that log message.\nI've so far only been able to observe it once per cgroup creation at most due to a race right around creation.\nIn my reproduction, it doesn't have any ill effect beyond the noisy logline. stats/summary for the kubelet still shows usageCoreNanoSeconds correctly, which I believe is directly calculated from the sum of per cpu usage.\nI created a PR (#1769) to avoid the noisy error log.\nThanks for reporting the issue!. Rebased. \nOnly one commit to this repository has happened since I made my PR and that change didn't touch any related files.\nDoes this repository have the \"Allow merge commits\" setting off? Is there any reason? The commit history does look nice and linear as a result, but this rebase was 100% unnecessary. There's no conflicts, no reason it would conflict, and it's a total waste of developer time.\nIf the worry is that it isn't being tested identically to how it's merged, well, that's what the submit queue solves for k8s proper, and what homu solves for other repositories (like rust)... and those solutions don't require your contributors to do utterly pointless busy work.. It looks like github added an additional setting under the \"Branch protection\" header named \"Require branches to be up to date before merging\".\nI don't think that's the right tradeoff for the sake of developer productivity / happiness.\n\n\n. Note that I have not manually tested that this resolves the issue I mentioned in reality, but I have a strong suspicion this will.. Sure, I added logging for mounts that get skipped.. The best data to use is probably a /var/lib/docker directory using the overlay/overlay2 graphdriver. That's what this code is meant to count in kubernetes after all, and that directory will also often have quite a few hardlinks depending on the container images you have laying there. It could be worth trying to populate it with something approximating a typical k8s workload, either by running the benchmark on a k8s node, or by trying to download similar images and run similar sets of containers.. I'd rather call it Name since I'd expect a little more information from a String method. \ngodoc nit, space missing and should start with the function name\n. we can drop a level of indentation because of the return above.\n. You can't end up with err nil and pod nil, right? Extraneous check I think?\n. I think strings.HasSuffix(name, \"/system.slice\") might be more readable,\n. In cgroupToPod you return an error if the api didn't give you anything, which is that case.\nThe only time you could return a nil error and a nil pod is if the api returned an object of {Pods: [nil]} that it claims matches a name of whatever you passed in. That would be a significant enough bug that a panic would be fine imo (that is, it won't happen)\n. nil, nil sounds like an unexpected return. At the very least a doc-comment about it would be nice, but it would be nice to not have to at all. Since it's not exported, I won't bang too hard on it.\n. In the year 20xx ....\n. You can save a bit of memory by using a chan struct{} since you never return an error on it at the moment.\n. Doesn't have to be exported\n. Is this cgroup used anywhere? I might be misunderstanding, but there was a bit before to filter this one out... is there a reason to return it? Is it needed elsewhere?\n. Lots of indentation; I think there could be use for another helper method around here.\n. I'm not sure if this is just always running, but in the relatively common case of the rkt api server not being reachable, I don't think we should spam the log every 30s. Will the rkt code only be run with user intent? Should there be code before that tries to determine if the rkt api server is up and errors out once early? Is this concern invalid for whatever reason?\n. I meant \"common\" in the case of the user not caring about rkt at all. If this is only registered when the service is initially reachable or with user-intent then my concern isn't valid.\n. We filter out the system.slice parent cgroup elsewhere and only use the pod / application cgroups right? My understanding is that the base cgroup here is that cgroup. Is it used for something? Could we just not include it at all here and have everything still be happy?\n. Before it was info and I thought it should be error, so I picked the middle as a compromise :innocent: . Question: Should we call self.parser.SeekEnd() here or not?\nI think SeekEnd more closely matches the journalctl .. -f behavior of before (start at the end of kmsg), but I think that looking a little into the past makes the kubelet behave more correctly after a restart.\nWill OOM events that happen while the kubelet is down, but are detected on kubelet startup actually do the right thing?\nAre they idempotent so that power-cycling the kubelet having bunches of duplicate events is okay?. Done. This matches the previous behavior because find would exit non-zero on most types of errors you might run into here (notably permission denied), so the err code would be hit before too.\nThere might be some cases this wouldn't work though.. Good call, I'll skip em.. I think the pure go way to implement this, for what it's worth, would be to do one of the following:\n\n\nUse something like docker/pkg/reexec to fork the kubelet into a mode where it just does stats and return the result to the parent kubelet (likely over stdout).\n     This has the benefit that we could also eventually move it into the pod's cgroup, or at least it's own non-kubelet cgroup, and do memory/cpu accounting too for it.\n    This has the downside that it would require some extra setup code in the kubelet to correctly deal with re-exec stuff, and last time I checked kubernetes had no existing code for this.\n\n\nUse runtime.LockOSThread and set nice (using the setpriority syscall) on the one thread doing this stuff.. I think as of go1.10 it's possible to do that safely, though historically it has been fraught with peril.. \n\n",
    "pwittrock": "Thanks everyone.  PTAL. Feedback addressed.\n. Recent push was to squash the commit for the PR review comments\n. Thanks Vish.  LGTM\n. I am good with just using yours.\n. @jthornber FS metrics device mapper support is blocked on this.  Do you have any updates?\n. I just tried it out, and it is pretty slick.  Even works on my mac.\nI got a couple of these types of warnings:\nlink: warning: option -X github.com/google/cadvisor/version.Version 0.20.1 may not work in future releases; use -X github.com/google/cadvisor/version.Version=0.20.1\nAre these expected? / Is it possible to make them go away?\n. fyi, new runs of the Jenkins GCE e2e tests should export the build log (+inline in the browser)\n. @jimmidyson wow that was an amazingly fast turn around.  ran go vet and go fmt on runner.go.\n. @jimmidyson SGMT.  fwiw the test failures were because the internal jenkins is not fully configured, and it seemed to hide the external jenkins test results.  Will sync up with vishh w.r.t to your comment about the challenges with internal jenkins PR builders.\n. retest this please\n. I can manually delete them, or I can disable them entirely.  The spammyness on this PR is in part due to me triggering the build a bunch while I was working on the internal cadvisor Jenkins PR builder.  I believe the kubernetes/kubernetes project has the same behavior of leaving the PR builder comments around instead of deleting them:  https://github.com/kubernetes/kubernetes/pull/17808\n. Thanks for the heads up about the build log.  This is a known problem caused by an issue in the google-storage-plugin not recognizing valid gce scopes.  I am working with our build team on a work around but e.t.a. for a fix is still a couple days.  I have tried to restrict the Experimental PR builder to my own PRs only through the admin + whitelist to minimize annoyance meanwhile.\n. ok to test\n. Yeah I will start looking into how to do this\nOn Fri, Dec 18, 2015 at 10:28 AM Vish Kannan notifications@github.com\nwrote:\n\n@pwittrock https://github.com/pwittrock: Can we update the cAdvisor\ncanary jenkins job to push images whenever a new tag is created?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1025#issuecomment-165864033.\n. @k8s-bot test this please\n. @k8s-bot test this please\n\nflaky failure about not having an authenticated account\n. @wangzhezhe The default check appears to be failing because it is in a bad state.  The \"Jenkins GCE e2e\" check is meant to replace it, so you can ignore default for now.  I have disable the default check for future PRs.\n. Thanks Jimmi.  :)\nI'd love to disable the default Jenkins build now, especially because it seems to be in a bad state.  I'll disable it but keep the job around in case we decide we need it again for whatever reason.\nSeems like the regex isn't properly catching the message, I am going to try to fix it and will comment back when it is resolved.\n. I can separate the configuration from code by moving to a file.  Was hoping the list would remain small enough and change infrequently enough that the indirection would not be justified, but I suppose that is a bit naive.  :P  How does a newline separated list of regexes in a new file called integration/runner/retrywhitelist.txt sound?  I am thinking we probably want the file to be flag configurable?  Whitelist file location / naming suggestions?  Other considerations?\n. PTAL.  Extracted the regexp to a file.\n. Since the recent changes require the whitelist file to be set by flag to enable retries, they are not enabled for the PR builds\n. @jimmidyson Since you added comments after the lgtm / self-merge, would you like to re-review (instead of leaving the lgtm)?  Let me know if you prefer the changes or had something else in mind.  Cheers.\n. Is it normal to take 5+ hours for travis, or do we need to kick something?\n. 'retest this please' should have worked.  We are just using the vanilla jenkins pr builder plugin  Closing and reopening should also work if nothing else is\n. retest this please\n. fwiw: The gcloud auth issue is something I have been trying to track down for the past week+.  It seems to be flaky and unrelated to whether or not there are actual gcloud credentials set up.  I have a PR out to try and get more debug info in our k8s e2e test where we also see this happening.\n. retest this please\n. 'retest this please' does not appear to be working for me either.  I'll have to dig into the Jenkins logs to see if I can debug what the underlying issue is.  As a work around pushing any code change should trigger the test, or closing/reopening the issue should also work.\n. Hm, I thought I remember triggering builds in the past by closing/reopening, but something else could have been going on that was triggering the build.\n. @k8s-bot test this\n. I manually added the trigger phrase \"@k8s-bot test this\" to the ghprb configuration, and that seems to be working now.  Not sure why the built in commands do not work.\n. #1108\n. LGTM\n. Looks good\n. I looked into this a while back for labeling each release, and the plugin we use for the existing jenkins pusher didn't help much.  Might be possible to use the plugin for latest since it is a static label.  I plan to look into this after the k82 1.2 stuff dies down.\n. Yes the e2e seems to get really flaky for a bit, and just when it is painful enough to drop what I am doing and fix it, it seems to clear up.\n. LGTM\n. LGTM pending passing tests\n. @k8s-bot test this please\n. Ideally I'd prefer to grab the output directly from cadvisor, but it is a little hard to do since cadvisor isn't expected to stop.  I think this would be possible after killing it, but would require some updates to structure to capture the \"error\" caused by killing cadvisor.  If you agree, go ahead and add a TODO.\n. LGTM\n. LGTM\n. The tests not properly reporting failure should have been fixed by #1107.  If it hasn't we will need to dig in some more.  Looking at the last few cadvisor pr builder test runs, I don't see any \"Test killed with quit\" messages in the logs.  Perhaps that was a transient issue?\n. @jimmidyson I am going to lgtm this change so that it can be merged into k8s before the 1.2 feature freeze.  I'd prefer to wait for your approval since you have already commented, but doing so would cause this to miss the freeze deadline.  If you still have concerns we can discuss and update/revert if needed (would require a k8s 1.2 cherrypick).\n. LGTM\n. @jimmidyson my pleasure\n. @k8s-bot test this please\n. LGTM\n. lgtm\n. lgtm\n. Looks good.\n. More credentials failures :(\n. I will add a default value for ssh-config just so the pr build passes without updating jenkins\n. Adding a few debug lines for figuring out why e2e tests are failing\n. Working on getting the tests to pass, having some issues with the Jenkins environment\n. I changed this to use generic options to be more flexible.  I think instead of using an ssh_config, we will want to specify the options manually so we don't need to maintain a config file for all of our instances.\n. Failures will be expected because I haven't defaulted the ssh options to the needed credentials\n. Found an issue and repushed, will send a PTAL once the errors are the expected errors\n. PTAL.  Issues should be resolved.\n. @jimmidyson Hopefully this reduces that number of flake failures, as well as makes it simpler to test in non-gce environments.\n. fyi test failure:\n/var/lib/jenkins/workspace/project/src/github.com/google/cadvisor/container/factory_test.go:49: undefined: FactoryForMockContainerHandler\n. yay more credential failures\n. @k8s-bot test this please\n. I am good with this as long as the tests pass\n. #1108 Should make these auth failures go away\n. @k8s-bot test this please\n. LGTM.  Ok to self merge\n. LGTM\n. As stated offline, the test code is getting a bit hard to follow / validate.  We will want to rethink how we remotely execute test code.\nLGTM\n. I would love to just make the images public, but I haven't been able to figure out how to do that.\nI believe I ran this: setup script link\nOn: coreos-stable version: 20160218\nI have also been seeing a lot of coreos failures caused by the ssh connection to run the tests being terminated.\n. @pmorie \n@vishh \n@timstclair \nWould you provide an update on the status + ETA for the documentation for this feature as well as add any PRs as they are created?\nNot Started / In Progress / In Review / Done\nExpected Merge Time:\nThanks\n@pwittrock\n. No that I am aware of.  It sounds like this may not be user-facing and so no docs required?\n. Not obvious why this failed from the logs:\n\nError 1: error running cAdvisor: command \"ssh\" [\"-i\" \"/var/lib/jenkins/gce_keys/google_compute_engine\" \"-o\" \"UserKnownHostsFile=/dev/null\" \"-o\" \"IdentitiesOnly=yes\" \"-o\" \"CheckHostIP=no\" \"-o\" \"StrictHostKeyChecking=no\" \"e2e-cadvisor-coreos-beta-docker19\" \"--\" \"sudo /tmp/cadvisor-3779/cadvisor --port 8080 --logtostderr --docker_env_metadata_whitelist=TEST_VAR  &> /tmp/cadvisor-3779/log.txt\"] failed with error: exit status 2 and output: Warning: Permanently added 'e2e-cadvisor-coreos-beta-docker19' (ECDSA) to the list of known hosts.\n\nI can't tell if this is ssh or cadvisor that is failing.\n. lgtm\n. ok to test\n. @k8s-bot test this\n. @k8s-bot test this\n. @k8s-bot test this\n. Looks like the test instances were deleted.  Will check around with what happened.\n. I am working on fixing the test environment.\n. @k8s-bot test this\n. @k8s-bot test this\n. Got this failure.  Is this a red herring?\n```\n--- FAIL: TestDockerContainerById (12.70s)\n    assertions.go:150: \nLocation:   docker_test.go:63\n\nError:      No error is expected but got request \"http://e2e-cadvisor-rhel-7:8080/api/v1.3/docker/4928ebfc16223b2c44f0366da2b9b22753dbfd70e95d247e164e2bb0ef030265\" failed with error: \"failed to get Docker container \\\"4928ebfc16223b2c44f0366da2b9b22753dbfd70e95d247e164e2bb0ef030265\\\" with error: unable to find Docker container \\\"4928ebfc16223b2c44f0366da2b9b22753dbfd70e95d247e164e2bb0ef030265\\\"\"\n\nMessages:   Timed out waiting for container \"4928ebfc16223b2c44f0366da2b9b22753dbfd70e95d247e164e2bb0ef030265\" to be available in cAdvisor: request \"http://e2e-cadvisor-rhel-7:8080/api/v1.3/docker/4928ebfc16223b2c44f0366da2b9b22753dbfd70e95d247e164e2bb0ef030265\" failed with error: \"failed to get Docker container \\\"4928ebfc16223b2c44f0366da2b9b22753dbfd70e95d247e164e2bb0ef030265\\\" with error: unable to find Docker container \\\"4928ebfc16223b2c44f0366da2b9b22753dbfd70e95d247e164e2bb0ef030265\\\"\"\n\nFAIL\nFAIL    github.com/google/cadvisor/integration/tests/api    87.088s\n```\nlogs\n. @k8s-bot test this\n. Hm, looks like it was probably a flake then.\n. LGTM\n. @timstclair Running against an existing host was always a pain.  I just set the cadvisor port to 80 and opened the vm for web traffic when I needed to run a test.\n. LGTM\n. Done.\n. Done.\n. Yeah would be great to get rid of the canary.  If dockerhub automated builds can do a periodic build from master/head as well, then we can stop building it from jenkins.  If we still need to do jenkins, we will need to make some changes to migrate as the current canary docker file checks out the src from github.com.\n. Since this is run after the copy, docker won't cache any of these commands.  Does it make sense to split out the commands that do not depend on the src code and run them first?\n. Why use the docker ENV directives for these so that they would be available to any images that inherit from this one?\n. Interesting.  I hadn't considered some of the implications of the way docker handles layers.  Does this imply that cleaning up data from previous layers does not reduce the aggregate image size.  How important is image size for cadvisor?  Is there a threshold we want to keep it under?  FWIW, the git src looks like it consumes ~30MB, while the image \"virtual size\" is ~65MB, so copying the source and building cadvisor within the Dockerfile will increase the image size by 30MB.  (not sure if this is considered a lot or trivial)\n$ docker exec -ti e58b09c05e64 du -h -d 1 /gopath/\n29.7M   /gopath/src\n9.8M    /gopath/bin\n764.0K  /gopath/pkg\n40.3M   /gopath/\n$ docker images\nREPOSITORY                                        TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\npwittrock/cadvisor                                latest              763321a5339a        About an hour ago   62.56 MB\n. Yeah if none of the tools are present, it doesn't help much.  I was thinking in the future we could even support running tests through Docker to make cross platform development easier, but this isn't a goal at this point.\n. Image sizes for canary and 0.19.3 for reference (760MB and 35MB respectively)\nbash-3.2$ docker images | grep cadvisor\ngoogle/cadvisor                                canary               ef54655a0485        17 minutes ago      760.5 MB\ngoogle/cadvisor                                0.19.3               548619cdac12        4 weeks ago         34.99 MB\n. yeah, the docker build isn't optimized to support small image sizes.  I really don't have enough context on how the cadvisor image is used to know if the increase in size is an issue for folks using it.  I like your approach because it provides a consistent way of building cadvisor, and having docker hub build the image sounds like it could reduce maintenance cost.\n. Does this need to be updated as well?\n. what is other?\n. Why do we use time.Time in MachineStats, and uint64 here?\n. Why are we using pointers to ints instead of just ints?\n. maybe: ReadDuration *time.Duration\n. leaving a uint64 makes sense to me then.  Prefer changing the name to ReadDuration or ReadDurationMillis\n. Is redirecting the output better than setting the log directory?\n. ~~I believe this will only redirect stdout, and not stderr.  Is that your intent?~~  Could we use --alsologtostderr and then set the log file to <>/log.txt so that the error output still appears in Jenkins?\n. Is there another routine listening to this channel?  If not this will block forever.\n. SGTM\n. nit: is this gofmt'ed?  Usually it lines up the types\n. Since every element in parts is expected to be a string slice of 2 elements, can we unmarshal directly into a map?\n. Can we rely on these being static?  This is a good candidate for a const.\n. Since we are just looking for one element (\"Root Dir\") we don't need to populate the map, and could just return when we find the entry we are looking for.  It is a pretty minor optimization, and only worth doing if it would improve the clarity of the code\n. nit: is it necessary to check if execDriver == \"\", because that doesn't have the prefix \"native\"?\n. Should we change the help documentation to state that this is deprecated and setting it has no effect?  Maybe \"deprecated (no-op): docker root directory is now read from docker info\"\n. You are probably right, I am not that familiar with the string format that docker provides or what the go deserialization libraries are capable of.  I was hoping the deserialization libraries would coerce a slice of slices len=2 into a map.\n. yeah.  Is it still used elsewhere then?\n. as discussed offline, check the major version as well\n. Should we check exactly 1 instead at least 1?\n. golang case statements don't fallthrough, so break shouldn't be necessary\n. Should this be || instead of &&?\nfor docker 2.0 -> [2, 0] -> !(true && false) -> !(false) -> true\n. why not path.Join(self.baseUrl, \"stats\", name)\n. this string seems wrong, since we are checking if it is greater than 2^6\n. nit / optional:\nthere seems to be a widely copied / pasted way of doing this in github.  May want to use the standard approach:\nhttps://github.com/search?q=%2Flayerdb%2Fmounts%2F&type=Code&utf8=%E2%9C%93\nconsider putting this in a function that takes a (sDriver, coantiner id, rwlayerid) and returns the path\n. not in this PR, but add a TODO to give more descriptive names than storageDir and otherStorageDir\n. golang ssh client looks like a better approach.  adding a todo.\n. one would hope, but I wasn't able to find a better solution on SO or the man pages.  Happy to change if anyone with more ssh knowledge could suggest a better way.\n. Fixed this by just requiring the hostname to resolve.  This works for gce and I think is an ok assumption to make.\n. Because I didn't clean up all the of the Gce specific code in the test runner :p .  Do you think that is important enough to do for this PR?\n. Done\n. scp is going to need to take the ssh options in order to properly authenticate\n. Yeah, if the options aren't supported I expect the tests will fail.  :)\n. 1 or 2 dashes (-)?\n. why is this flag initialized differently than storage_duration?\n. I'd vote for 2) if you wouldn't mind doing it.\n. Is this a TODO?  Is this fixed?\n. ",
    "babusatasiya": "Yes, I manage to make it work, We use docker with option bridge=none and\n setup our own bridge and specify part of lxc, once I remove this option it\nworks as you explained.\nIf you got any insights why we could not work would be gr8.\nOn Monday, August 31, 2015, Rohit Jnagal notifications@github.com wrote:\n\nThis is a warning that we will not be able to report some of the stats\nthat we only get from docker using the libcontainer exec driver. You should\nstill be able to see regular cpu, memory stats, but they show under\n\"subcontainers\" rather than docker containers (eg. /lxc container in the\nlogs above).\nYou can ignore these errors if the stats you are interested in are still\nshowing up.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/873#issuecomment-136416990.\n. \n",
    "GeertJohan": "@onlyjob Note: It sais 0.9.x, not x.y.z\n. ",
    "Trane9991": "Thank you for answer @hacpai \nbut it did'nt help, I believe it failing somewhere on the on the line 100 and 101.\naccording to the trace in log:\ngithub.com/google/cadvisor/storage/elasticsearch.New(0xc208102780, 0x24, 0xb66370, 0x8, 0xb45230, 0x5, 0x7ffd14dc5da7, 0x1a, 0x0, 0x0, ...)\n    /home/tpost/go/src/github.com/google/cadvisor/storage/elasticsearch/elasticsearch.go:104 +0x1a6\n. @hacpai \nI have no idea where problem is. Here is very strange behavior.\nRunning:\n```\ncore@8d3c56f4-3468-4c69-9afb-6202099641f0 ~/bin $ curl 127.0.0.1:9200\n{\n  \"status\" : 200,\n  \"name\" : \"Black Tarantula\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"version\" : {\n    \"number\" : \"1.7.1\",\n    \"build_hash\" : \"b88f43fc40b0bcd7f173a1f9ee2e97816de80b19\",\n    \"build_timestamp\" : \"2015-07-29T09:54:16Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"4.10.4\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\ncore@8d3c56f4-3468-4c69-9afb-6202099641f0 ~/bin $ curl http://10.128.129.121:9200/\n{\n  \"status\" : 200,\n  \"name\" : \"Black Tarantula\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"version\" : {\n    \"number\" : \"1.7.1\",\n    \"build_hash\" : \"b88f43fc40b0bcd7f173a1f9ee2e97816de80b19\",\n    \"build_timestamp\" : \"2015-07-29T09:54:16Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"4.10.4\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\ncore@8d3c56f4-3468-4c69-9afb-6202099641f0 ~/bin $ curl http://remote_elk:9200/\n{\n  \"status\" : 200,\n  \"name\" : \"Ecstasy\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"version\" : {\n    \"number\" : \"1.7.1\",\n    \"build_hash\" : \"b88f43fc40b0bcd7f173a1f9ee2e97816de80b19\",\n    \"build_timestamp\" : \"2015-07-29T09:54:16Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"4.10.4\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n```\nAs you can see here is:\n127.0.0.1 its localhost for 10.128.129.121 and remote_elk its some remote elk.\nStarting cadvisor on the 10.128.129.121 CoreOS:\n```\ncore@8d3c56f4-3468-4c69-9afb-6202099641f0 ~/bin $ ./cadvisor -storage_driver=\"elasticsearch\" -port=8099 -alsologtostderr=true -storage_driver_es_host=\"http://remote_elk:9200\"\npanic: no Elasticsearch node available\ngoroutine 1 [running]:\ngithub.com/google/cadvisor/storage/elasticsearch.New(0xc2080a4780, 0x24, 0xb663d0, 0x8, 0xb45290, 0x5, 0x7ffdf2ef8da9, 0x24, 0x0, 0x0, ...)\n    /home/tpost/go/src/github.com/google/cadvisor/storage/elasticsearch/elasticsearch.go:104 +0x1af\nmain.NewMemoryStorage(0x7ffdf2ef8d62, 0xd, 0xbc1890, 0x0, 0x0)\n    /home/tpost/go/src/github.com/google/cadvisor/storagedriver.go:111 +0x716\nmain.main()\n    /home/tpost/go/src/github.com/google/cadvisor/cadvisor.go:63 +0x18a\ngoroutine 5 [chan receive]:\ngithub.com/golang/glog.(*loggingT).flushDaemon(0x11ffe80)\n    /home/tpost/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:839 +0x78\ncreated by github.com/golang/glog.init\u00b71\n    /home/tpost/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:406 +0x2a7\ngoroutine 17 [syscall, locked to thread]:\nruntime.goexit()\n    /usr/local/go/src/runtime/asm_amd64.s:2232 +0x1\ngoroutine 11 [syscall]:\nos/signal.loop()\n    /usr/local/go/src/os/signal/signal_unix.go:21 +0x1f\ncreated by os/signal.init\u00b71\n    /usr/local/go/src/os/signal/signal_unix.go:27 +0x35\ngoroutine 15 [IO wait]:\nnet.(pollDesc).Wait(0xc208011480, 0x72, 0x0, 0x0)\n    /usr/local/go/src/net/fd_poll_runtime.go:84 +0x47\nnet.(pollDesc).WaitRead(0xc208011480, 0x0, 0x0)\n    /usr/local/go/src/net/fd_poll_runtime.go:89 +0x43\nnet.(netFD).Read(0xc208011420, 0xc20800f000, 0x1000, 0x1000, 0x0, 0x7f4ea0dc8fb0, 0xc2080a0ab0)\n    /usr/local/go/src/net/fd_unix.go:242 +0x40f\nnet.(conn).Read(0xc20803a2d8, 0xc20800f000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/net.go:121 +0xdc\nnet/http.noteEOFReader.Read(0x7f4ea0dcb278, 0xc20803a2d8, 0xc2080fd658, 0xc20800f000, 0x1000, 0x1000, 0x9ba820, 0x0, 0x0)\n    /usr/local/go/src/net/http/transport.go:1270 +0x6e\nnet/http.(noteEOFReader).Read(0xc20801fe60, 0xc20800f000, 0x1000, 0x1000, 0xc208012000, 0x0, 0x0)\n    :125 +0xd4\nbufio.(Reader).fill(0xc208038c60)\n    /usr/local/go/src/bufio/bufio.go:97 +0x1ce\nbufio.(Reader).Peek(0xc208038c60, 0x1, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/bufio/bufio.go:132 +0xf0\nnet/http.(persistConn).readLoop(0xc2080fd600)\n    /usr/local/go/src/net/http/transport.go:842 +0xa4\ncreated by net/http.(*Transport).dialConn\n    /usr/local/go/src/net/http/transport.go:660 +0xc9f\ngoroutine 16 [select]:\nnet/http.(persistConn).writeLoop(0xc2080fd600)\n    /usr/local/go/src/net/http/transport.go:945 +0x41d\ncreated by net/http.(Transport).dialConn\n    /usr/local/go/src/net/http/transport.go:661 +0xcbc\n```\nnow trying local elk\ncore@8d3c56f4-3468-4c69-9afb-6202099641f0 ~/bin $ ./cadvisor -storage_driver=\"elasticsearch\" -port=8099 -alsologtostderr=true -storage_driver_es_host=\"http://10.128.129.121:9200\"\nElasticsearch returned with code 200 and version 1.7.1I0910 15:57:04.568553 28608 storagedriver.go:128] Using backend storage type \"elasticsearch\"\nand the same but with 127.0.0.1\ncore@8d3c56f4-3468-4c69-9afb-6202099641f0 ~/bin $ ./cadvisor -storage_driver=\"elasticsearch\" -port=8099 -alsologtostderr=true -storage_driver_es_host=\"http://127.0.0.1:9200\"\nElasticsearch returned with code 200 and version 1.7.1I0910 15:57:58.997353 29003 storagedriver.go:128] Using backend storage type \"elasticsearch\"\nIf I run any of this commands on the other CoreOS machine(where is no local elk installed) all 3 commands will fail.\n. @hacpai \nStill can't write to remote elasticsearch :(\n```\ncore@38a63250-c3e0-4328-b08f-e23fdb6029e4 ~/bin $ ./cadvisor -storage_driver=\"elasticsearch\" -storage_driver_es_host=\"http://10.128.129.121:9200\"\npanic: no Elasticsearch node available\ngoroutine 1 [running]:\ngithub.com/google/cadvisor/storage/elasticsearch.New(0xc2080a4810, 0x24, 0xb66830, 0x8, 0xb456f0, 0x5, 0x7ffdfdf97db3, 0x1a, 0x0, 0x0, ...)\n    /home/tpost/go/src/github.com/google/cadvisor/storage/elasticsearch/elasticsearch.go:104 +0x1af\nmain.NewMemoryStorage(0x7ffdfdf97d8d, 0xd, 0xbc1db0, 0x0, 0x0)\n    /home/tpost/go/src/github.com/hacpai/cadvisor/storagedriver.go:111 +0x716\nmain.main()\n    /home/tpost/go/src/github.com/hacpai/cadvisor/cadvisor.go:63 +0x18a\ngoroutine 5 [chan receive]:\ngithub.com/golang/glog.(*loggingT).flushDaemon(0x1200e80)\n    /home/tpost/go/src/github.com/hacpai/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:839 +0x78\ncreated by github.com/golang/glog.init\u00b71\n    /home/tpost/go/src/github.com/hacpai/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:406 +0x2a7\ngoroutine 17 [syscall, locked to thread]:\nruntime.goexit()\n    /usr/local/go/src/runtime/asm_amd64.s:2232 +0x1\ngoroutine 14 [IO wait]:\nnet.(pollDesc).Wait(0xc2080116b0, 0x72, 0x0, 0x0)\n    /usr/local/go/src/net/fd_poll_runtime.go:84 +0x47\nnet.(pollDesc).WaitRead(0xc2080116b0, 0x0, 0x0)\n    /usr/local/go/src/net/fd_poll_runtime.go:89 +0x43\nnet.(netFD).Read(0xc208011650, 0xc20800f000, 0x1000, 0x1000, 0x0, 0x7f5093909fb0, 0xc2080a0be0)\n    /usr/local/go/src/net/fd_unix.go:242 +0x40f\nnet.(conn).Read(0xc20803a2d0, 0xc20800f000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/net.go:121 +0xdc\nnet/http.noteEOFReader.Read(0x7f509390c208, 0xc20803a2d0, 0xc2080fd0d8, 0xc20800f000, 0x1000, 0x1000, 0x9bac20, 0x0, 0x0)\n    /usr/local/go/src/net/http/transport.go:1270 +0x6e\nnet/http.(noteEOFReader).Read(0xc20801fec0, 0xc20800f000, 0x1000, 0x1000, 0xc208012000, 0x0, 0x0)\n    :125 +0xd4\nbufio.(Reader).fill(0xc208038b40)\n    /usr/local/go/src/bufio/bufio.go:97 +0x1ce\nbufio.(Reader).Peek(0xc208038b40, 0x1, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/bufio/bufio.go:132 +0xf0\nnet/http.(persistConn).readLoop(0xc2080fd080)\n    /usr/local/go/src/net/http/transport.go:842 +0xa4\ncreated by net/http.(*Transport).dialConn\n    /usr/local/go/src/net/http/transport.go:660 +0xc9f\ngoroutine 11 [syscall]:\nos/signal.loop()\n    /usr/local/go/src/os/signal/signal_unix.go:21 +0x1f\ncreated by os/signal.init\u00b71\n    /usr/local/go/src/os/signal/signal_unix.go:27 +0x35\ngoroutine 15 [select]:\nnet/http.(persistConn).writeLoop(0xc2080fd080)\n    /usr/local/go/src/net/http/transport.go:945 +0x41d\ncreated by net/http.(Transport).dialConn\n    /usr/local/go/src/net/http/transport.go:661 +0xcbc\ncore@38a63250-c3e0-4328-b08f-e23fdb6029e4 ~/bin $ curl 10.128.129.121:9200\n{\n  \"status\" : 200,\n  \"name\" : \"Hack\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"version\" : {\n    \"number\" : \"1.7.1\",\n    \"build_hash\" : \"b88f43fc40b0bcd7f173a1f9ee2e97816de80b19\",\n    \"build_timestamp\" : \"2015-07-29T09:54:16Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"4.10.4\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n```\nPS: I'm not using cadvisor from container currently I'm running binary directly from CoreOS.\nDid you try to connect cadvisor to remote elk?\n. @hacpai \nthank you. Looks like Issue are resolved now. Everything works fine with -storage_driver_es_enable_sniffer=false\nBut one more thing which will be nice to have - ability to turn off this kind of logs from elasticsearch driver:\nIndexed tweet AU_byQ_uG986emsPaswE to index cadvisor, type stats\nIndexed tweet AU_byQ_3G986emsPaswG to index cadvisor, type stats\nIndexed tweet AU_byRAFG986emsPaswI to index cadvisor, type stats\nIndexed tweet AU_byRAJG986emsPaswJ to index cadvisor, type stats\nIndexed tweet AU_byRAQG986emsPaswK to index cadvisor, type stats\nIndexed tweet AU_byRAuG986emsPaswP to index cadvisor, type stats\n. I tried to route cadvisor ui to /cadvisor with traefik and couldn't make it work.. ",
    "stonevil": "fixbug-issue-881\ncadvisor trying connect to http://127.0.0.1:9200/ just ignoring provided -storage_driver_es_host=\"http://192.168.0.101:9200\"\nroot@98fde4963846:/go/src/github.com/hacpai/cadvisor# ./cadvisor -storage_driver=\"elasticsearch\" -port=8099 -alsologtostderr=true -storage_driver_es_host=\"http://192.168.0.101:9200\"\npanic: Get http://127.0.0.1:9200/: dial tcp 127.0.0.1:9200: getsockopt: connection refused\ngoroutine 1 [running]:\ngithub.com/google/cadvisor/storage/elasticsearch.New(0xc820121510, 0xc, 0xbb91a0, 0x8, 0xba8f68, 0x5, 0x7ffef34ffe61, 0x19, 0x0, 0x0, ...)\n    /go/src/github.com/google/cadvisor/storage/elasticsearch/elasticsearch.go:111 +0x261\nmain.NewMemoryStorage(0x7ffef34ffe1a, 0xd, 0xc820126580, 0x0, 0x0)\n    /go/src/github.com/hacpai/cadvisor/storagedriver.go:111 +0xe86\nmain.main()\n    /go/src/github.com/hacpai/cadvisor/cadvisor.go:63 +0x193\ngoroutine 17 [syscall, locked to thread]:\nruntime.goexit()\n    /usr/local/go/src/runtime/asm_amd64.s:1696 +0x1\ngoroutine 15 [select]:\nnet/http.(_persistConn).writeLoop(0xc8200b5970)\n    /usr/local/go/src/net/http/transport.go:1009 +0x40c\ncreated by net/http.(_Transport).dialConn\n    /usr/local/go/src/net/http/transport.go:686 +0xc9d\ngoroutine 6 [syscall]:\nos/signal.loop()\n    /usr/local/go/src/os/signal/signal_unix.go:22 +0x18\ncreated by os/signal.init.1\n    /usr/local/go/src/os/signal/signal_unix.go:28 +0x37\ngoroutine 7 [chan receive]:\ngithub.com/golang/glog.(*loggingT).flushDaemon(0x1276220)\n    /go/src/github.com/hacpai/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:839 +0x67\ncreated by github.com/golang/glog.init.1\n    /go/src/github.com/hacpai/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:406 +0x297\ngoroutine 14 [IO wait]:\nnet.runtime_pollWait(0x7fa6b1356790, 0x72, 0xc820010170)\n    /usr/local/go/src/runtime/netpoll.go:157 +0x60\nnet.(_pollDesc).Wait(0xc820123020, 0x72, 0x0, 0x0)\n    /usr/local/go/src/net/fd_poll_runtime.go:73 +0x3a\nnet.(_pollDesc).WaitRead(0xc820123020, 0x0, 0x0)\n    /usr/local/go/src/net/fd_poll_runtime.go:78 +0x36\nnet.(_netFD).Read(0xc820122fc0, 0xc820115000, 0x1000, 0x1000, 0x0, 0x7fa6b1350050, 0xc820010170)\n    /usr/local/go/src/net/fd_unix.go:232 +0x23a\nnet.(_conn).Read(0xc8200263c0, 0xc820115000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/net.go:172 +0xe4\nnet/http.noteEOFReader.Read(0x7fa6b1356850, 0xc8200263c0, 0xc8200b59c8, 0xc820115000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/http/transport.go:1370 +0x67\nnet/http.(_noteEOFReader).Read(0xc82011b740, 0xc820115000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    :126 +0xd0\nbufio.(_Reader).fill(0xc8200158c0)\n    /usr/local/go/src/bufio/bufio.go:97 +0x1e9\nbufio.(_Reader).Peek(0xc8200158c0, 0x1, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/bufio/bufio.go:132 +0xcc\nnet/http.(_persistConn).readLoop(0xc8200b5970)\n    /usr/local/go/src/net/http/transport.go:876 +0xf7\ncreated by net/http.(*Transport).dialConn\n    /usr/local/go/src/net/http/transport.go:685 +0xc78\ngoroutine 24 [select]:\ngopkg.in/olivere/elastic%2ev2.(*Client).sniffer(0xc8200739e0)\n    /go/src/github.com/hacpai/cadvisor/Godeps/_workspace/src/gopkg.in/olivere/elastic.v2/client.go:523 +0x1af\ncreated by gopkg.in/olivere/elastic%2ev2.NewClient\n    /go/src/github.com/hacpai/cadvisor/Godeps/_workspace/src/gopkg.in/olivere/elastic.v2/client.go:223 +0x5bf\ngoroutine 19 [IO wait]:\nnet.runtime_pollWait(0x7fa6b13566d0, 0x72, 0xc820010170)\n    /usr/local/go/src/runtime/netpoll.go:157 +0x60\nnet.(_pollDesc).Wait(0xc8201234f0, 0x72, 0x0, 0x0)\n    /usr/local/go/src/net/fd_poll_runtime.go:73 +0x3a\nnet.(_pollDesc).WaitRead(0xc8201234f0, 0x0, 0x0)\n    /usr/local/go/src/net/fd_poll_runtime.go:78 +0x36\nnet.(_netFD).Read(0xc820123490, 0xc820131000, 0x1000, 0x1000, 0x0, 0x7fa6b1350050, 0xc820010170)\n    /usr/local/go/src/net/fd_unix.go:232 +0x23a\nnet.(_conn).Read(0xc820026408, 0xc820131000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/net.go:172 +0xe4\nnet/http.noteEOFReader.Read(0x7fa6b1356850, 0xc820026408, 0xc8200b5a78, 0xc820131000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/http/transport.go:1370 +0x67\nnet/http.(_noteEOFReader).Read(0xc82011bec0, 0xc820131000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    :126 +0xd0\nbufio.(_Reader).fill(0xc820015e60)\n    /usr/local/go/src/bufio/bufio.go:97 +0x1e9\nbufio.(_Reader).Peek(0xc820015e60, 0x1, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/bufio/bufio.go:132 +0xcc\nnet/http.(_persistConn).readLoop(0xc8200b5a20)\n    /usr/local/go/src/net/http/transport.go:876 +0xf7\ncreated by net/http.(*Transport).dialConn\n    /usr/local/go/src/net/http/transport.go:685 +0xc78\ngoroutine 20 [select]:\nnet/http.(_persistConn).writeLoop(0xc8200b5a20)\n    /usr/local/go/src/net/http/transport.go:1009 +0x40c\ncreated by net/http.(_Transport).dialConn\n    /usr/local/go/src/net/http/transport.go:686 +0xc9d\ngoroutine 22 [IO wait]:\nnet.runtime_pollWait(0x7fa6b1356610, 0x72, 0xc820010170)\n    /usr/local/go/src/runtime/netpoll.go:157 +0x60\nnet.(_pollDesc).Wait(0xc8201236b0, 0x72, 0x0, 0x0)\n    /usr/local/go/src/net/fd_poll_runtime.go:73 +0x3a\nnet.(_pollDesc).WaitRead(0xc8201236b0, 0x0, 0x0)\n    /usr/local/go/src/net/fd_poll_runtime.go:78 +0x36\nnet.(_netFD).Read(0xc820123650, 0xc820147000, 0x1000, 0x1000, 0x0, 0x7fa6b1350050, 0xc820010170)\n    /usr/local/go/src/net/fd_unix.go:232 +0x23a\nnet.(_conn).Read(0xc820026420, 0xc820147000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/net.go:172 +0xe4\nnet/http.noteEOFReader.Read(0x7fa6b1356850, 0xc820026420, 0xc8200b5b28, 0xc820147000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/http/transport.go:1370 +0x67\nnet/http.(_noteEOFReader).Read(0xc82014e000, 0xc820147000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    :126 +0xd0\nbufio.(_Reader).fill(0xc82014c2a0)\n    /usr/local/go/src/bufio/bufio.go:97 +0x1e9\nbufio.(_Reader).Peek(0xc82014c2a0, 0x1, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/bufio/bufio.go:132 +0xcc\nnet/http.(_persistConn).readLoop(0xc8200b5ad0)\n    /usr/local/go/src/net/http/transport.go:876 +0xf7\ncreated by net/http.(*Transport).dialConn\n    /usr/local/go/src/net/http/transport.go:685 +0xc78\ngoroutine 23 [select]:\nnet/http.(_persistConn).writeLoop(0xc8200b5ad0)\n    /usr/local/go/src/net/http/transport.go:1009 +0x40c\ncreated by net/http.(_Transport).dialConn\n    /usr/local/go/src/net/http/transport.go:686 +0xc9d\ngoroutine 25 [select]:\ngopkg.in/olivere/elastic%2ev2.(*Client).healthchecker(0xc8200739e0)\n    /go/src/github.com/hacpai/cadvisor/Godeps/_workspace/src/gopkg.in/olivere/elastic.v2/client.go:689 +0x1b4\ncreated by gopkg.in/olivere/elastic%2ev2.NewClient\n    /go/src/github.com/hacpai/cadvisor/Godeps/_workspace/src/gopkg.in/olivere/elastic.v2/client.go:224 +0x5e1\n. # curl http://localhost:9200/_nodes/http?pretty\n{\n  \"cluster_name\" : \"cluster\",\n  \"nodes\" : {\n    \"82--26PoTTaFcGes4hTatA\" : {\n      \"name\" : \"elastic-3\",\n      \"transport_address\" : \"inet[/192.168.0.103:9300]\",\n      \"host\" : \"e6ca7be52655\",\n      \"ip\" : \"172.17.0.25\",\n      \"version\" : \"1.7.1\",\n      \"build\" : \"b88f43f\",\n      \"http_address\" : \"inet[/192.168.0.103:9200]\",\n      \"http\" : {\n        \"bound_address\" : \"inet[/0:0:0:0:0:0:0:0%0:9200]\",\n        \"publish_address\" : \"inet[/192.168.0.103:9200]\",\n        \"max_content_length_in_bytes\" : 104857600\n      }\n    },\n    \"pIHDWQiIQWGmM9HGRc3YUQ\" : {\n      \"name\" : \"elastic-2\",\n      \"transport_address\" : \"inet[/192.168.0.102:9300]\",\n      \"host\" : \"e660062472bf\",\n      \"ip\" : \"172.17.0.23\",\n      \"version\" : \"1.7.1\",\n      \"build\" : \"b88f43f\",\n      \"http_address\" : \"inet[/192.168.0.102:9200]\",\n      \"http\" : {\n        \"bound_address\" : \"inet[/0:0:0:0:0:0:0:0%0:9200]\",\n        \"publish_address\" : \"inet[/192.168.0.102:9200]\",\n        \"max_content_length_in_bytes\" : 104857600\n      }\n    },\n    \"8fUiRpOvSRawuY2WWkvfoA\" : {\n      \"name\" : \"elastic-1\",\n      \"transport_address\" : \"inet[/192.168.0.101:9300]\",\n      \"host\" : \"04c7b28cac29\",\n      \"ip\" : \"172.17.0.6\",\n      \"version\" : \"1.7.1\",\n      \"build\" : \"b88f43f\",\n      \"http_address\" : \"inet[/192.168.0.101:9200]\",\n      \"http\" : {\n        \"bound_address\" : \"inet[/0:0:0:0:0:0:0:0:9200]\",\n        \"publish_address\" : \"inet[/192.168.0.101:9200]\",\n        \"max_content_length_in_bytes\" : 104857600\n      }\n    }\n  }\n}\n\nOn 12 \u0432\u0435\u0440. 2015 \u0440., at 15:21, JohnHwee notifications@github.com wrote:\nHi @stonevil https://github.com/stonevil\nWhat is the output of curl http://:9200/_nodes/http?pretty when you run it from the server where Elastic is running?\n\u2014\nReply to this email directly or view it on GitHub https://github.com/google/cadvisor/issues/881#issuecomment-139753048.\n. UNICAST turned off.\nES cluster managed by etcd.\nOn 12 \u0432\u0435\u0440. 2015 \u0440., at 15:21, JohnHwee notifications@github.com wrote:\nHi @stonevil https://github.com/stonevil\nWhat is the output of curl http://:9200/_nodes/http?pretty when you run it from the server where Elastic is running?\n\u2014\nReply to this email directly or view it on GitHub https://github.com/google/cadvisor/issues/881#issuecomment-139753048.\n. \n",
    "lborguetti": "I signed it!\n. Merged with master to fix cAdvisor builds! . @dashpole Sorry about the delay. Its done. \nThanks.. ",
    "n0mer": "@vishh here is the output\n```\ncAdvisor version: 0.16.0\nOS version: Buildroot 2014.02\nKernel version: [Supported and recommended]\n    Kernel version is 3.13.0-57-generic. Versions >= 2.6 are supported. 3.0+ are recommended.\nCgroup setup: [Supported and recommended]\n    Available cgroups: map[cpuset:1 cpu:1 memory:1 devices:1 blkio:1 cpuacct:1 freezer:1 perf_event:1 hugetlb:1]\n    Following cgroups are required: [cpu cpuacct]\n    Following other cgroups are recommended: [memory blkio cpuset devices freezer]\n    Hierarchical memory accounting disabled. Memory usage does not include usage from child containers.\nCgroup mount setup: [Supported and recommended]\n    Cgroups are mounted at /sys/fs/cgroup.\n    Cgroup mount directories: blkio cpu cpuacct cpuset devices freezer hugetlb memory perf_event systemd \n    Any cgroup mount point that is detectible and accessible is supported. /sys/fs/cgroup is recommended as a standard location.\n    Cgroup mounts:\n    systemd /sys/fs/cgroup/systemd cgroup ro,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /sys/fs/cgroup/cpuset cgroup ro,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /sys/fs/cgroup/cpu cgroup ro,nosuid,nodev,noexec,relatime,cpu 0 0\n    cgroup /sys/fs/cgroup/cpuacct cgroup ro,nosuid,nodev,noexec,relatime,cpuacct 0 0\n    cgroup /sys/fs/cgroup/memory cgroup ro,nosuid,nodev,noexec,relatime,memory 0 0\n    cgroup /sys/fs/cgroup/devices cgroup ro,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /sys/fs/cgroup/freezer cgroup ro,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /sys/fs/cgroup/blkio cgroup ro,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /sys/fs/cgroup/perf_event cgroup ro,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /sys/fs/cgroup/hugetlb cgroup ro,nosuid,nodev,noexec,relatime,hugetlb 0 0\n    systemd /rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /rootfs/sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /rootfs/sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /rootfs/sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /rootfs/sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /rootfs/sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /rootfs/sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /rootfs/var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/systemd cgroup ro,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpuset cgroup ro,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpu cgroup ro,nosuid,nodev,noexec,relatime,cpu 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpuacct cgroup ro,nosuid,nodev,noexec,relatime,cpuacct 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/memory cgroup ro,nosuid,nodev,noexec,relatime,memory 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/devices cgroup ro,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/freezer cgroup ro,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/blkio cgroup ro,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/perf_event cgroup ro,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/hugetlb cgroup ro,nosuid,nodev,noexec,relatime,hugetlb 0 0\n    systemd /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/systemd cgroup ro,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpuset cgroup ro,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpu cgroup ro,nosuid,nodev,noexec,relatime,cpu 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpuacct cgroup ro,nosuid,nodev,noexec,relatime,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/memory cgroup ro,nosuid,nodev,noexec,relatime,memory 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/devices cgroup ro,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/freezer cgroup ro,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/blkio cgroup ro,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/perf_event cgroup ro,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/hugetlb cgroup ro,nosuid,nodev,noexec,relatime,hugetlb 0 0\n    systemd /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/rootfs/sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/rootfs/sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/rootfs/sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/rootfs/sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/rootfs/sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/rootfs/sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/rootfs/sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/rootfs/sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/rootfs/sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\n    systemd /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/rootfs/var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/systemd cgroup ro,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/rootfs/var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpuset cgroup ro,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/rootfs/var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpu cgroup ro,nosuid,nodev,noexec,relatime,cpu 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/rootfs/var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpuacct cgroup ro,nosuid,nodev,noexec,relatime,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/rootfs/var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/memory cgroup ro,nosuid,nodev,noexec,relatime,memory 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/rootfs/var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/devices cgroup ro,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/rootfs/var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/freezer cgroup ro,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/rootfs/var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/blkio cgroup ro,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/rootfs/var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/perf_event cgroup ro,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/rootfs/var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/hugetlb cgroup ro,nosuid,nodev,noexec,relatime,hugetlb 0 0\n    systemd /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/hugetlb cgroup rw,relatime,hugetlb 0 0\nDocker version: [Supported and recommended]\n    Docker version is 1.8.1. Versions >= 1.0 are supported. 1.2+ are recommended.\nDocker driver setup: [Supported and recommended]\n    Docker exec driver is native-0.2. Storage driver is aufs.\n    Cgroups are being created through cgroup filesystem.\n    Docker container state directory is at \"/var/lib/docker/containers\" and is accessible.\nBlock device setup: [Supported, but not recommended]\n    None of the devices support 'cfq' I/O scheduler. No disk stats can be reported.\n     Disk \"vda\" Scheduler type \"none\".\nInotify watches: \n    /docker:\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/blkio/docker\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpu/docker\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/memory/docker\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpuacct/docker\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpuset/docker\n    /docker/2ea6f1844522175ed4c73a7beac34e2495e1962101bb404a7fce56f84178ffa9:\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/blkio/docker/2ea6f1844522175ed4c73a7beac34e2495e1962101bb404a7fce56f84178ffa9\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpu/docker/2ea6f1844522175ed4c73a7beac34e2495e1962101bb404a7fce56f84178ffa9\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/memory/docker/2ea6f1844522175ed4c73a7beac34e2495e1962101bb404a7fce56f84178ffa9\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpuacct/docker/2ea6f1844522175ed4c73a7beac34e2495e1962101bb404a7fce56f84178ffa9\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpuset/docker/2ea6f1844522175ed4c73a7beac34e2495e1962101bb404a7fce56f84178ffa9\n    /docker/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38:\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/blkio/docker/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpu/docker/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/memory/docker/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpuacct/docker/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpuset/docker/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38\n    /docker/a14be006c16cdc0c888d61cc1180a192757584aa1c652c929ba4c473c40fbe8c:\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpuset/docker/a14be006c16cdc0c888d61cc1180a192757584aa1c652c929ba4c473c40fbe8c\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/blkio/docker/a14be006c16cdc0c888d61cc1180a192757584aa1c652c929ba4c473c40fbe8c\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpu/docker/a14be006c16cdc0c888d61cc1180a192757584aa1c652c929ba4c473c40fbe8c\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/memory/docker/a14be006c16cdc0c888d61cc1180a192757584aa1c652c929ba4c473c40fbe8c\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpuacct/docker/a14be006c16cdc0c888d61cc1180a192757584aa1c652c929ba4c473c40fbe8c\n    /docker/a6ef43604b9bccee9eee8831da3a2ef3d0ecbc53011d03560143bc099c188ed4:\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/blkio/docker/a6ef43604b9bccee9eee8831da3a2ef3d0ecbc53011d03560143bc099c188ed4\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpu/docker/a6ef43604b9bccee9eee8831da3a2ef3d0ecbc53011d03560143bc099c188ed4\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/memory/docker/a6ef43604b9bccee9eee8831da3a2ef3d0ecbc53011d03560143bc099c188ed4\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpuacct/docker/a6ef43604b9bccee9eee8831da3a2ef3d0ecbc53011d03560143bc099c188ed4\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpuset/docker/a6ef43604b9bccee9eee8831da3a2ef3d0ecbc53011d03560143bc099c188ed4\n    /:\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/blkio\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpu\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/memory\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpuacct\n        /var/lib/docker/aufs/mnt/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38/sys/fs/cgroup/cpuset\nManaged containers: \n    /docker/43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38\n        Namespace: docker\n        Aliases:\n            cadvisor\n            43e447b3cf33e22ca61191d126c9cec180554822541aec1ea2a8660595262c38\n    /docker/a6ef43604b9bccee9eee8831da3a2ef3d0ecbc53011d03560143bc099c188ed4\n        Namespace: docker\n        Aliases:\n            scibot_telegram\n            a6ef43604b9bccee9eee8831da3a2ef3d0ecbc53011d03560143bc099c188ed4\n    /docker/2ea6f1844522175ed4c73a7beac34e2495e1962101bb404a7fce56f84178ffa9\n        Namespace: docker\n        Aliases:\n            nrsysmond\n            2ea6f1844522175ed4c73a7beac34e2495e1962101bb404a7fce56f84178ffa9\n    /docker/a14be006c16cdc0c888d61cc1180a192757584aa1c652c929ba4c473c40fbe8c\n        Namespace: docker\n        Aliases:\n            redis_scibot\n            a14be006c16cdc0c888d61cc1180a192757584aa1c652c929ba4c473c40fbe8c\n    /\n    /docker\n```\ncontainer is started with this command\n``` bash\ndocker run -d --name=cadvisor \\\n  -v /:/rootfs:ro \\\n  -v /var/run:/var/run:rw \\\n  -v /sys:/sys:ro \\\n  -v /var/lib/docker/:/var/lib/docker:ro \\\n  -v /opt/cadvisor:/opt/cadvisor:ro \\\n  -p 8089:8080 \\\n  --detach=true \\\n  google/cadvisor:latest --http_auth_realm cadvisor --http_auth_file /opt/cadvisor/cadvisor.htpasswd\n```\nSo what exact privileges are you talking about?\n. When i click on a single entry in that list , then i can see the container name:\n\nBut there is still no way to see container name is this \"subcontainers\" list. \nSo i think this is UI issue, and has nothing with \"privileges\". What do you think @vishh  ?\n. @vishh i finally figured out - you're right, it is possible to see containers if you click on \"Docker Containers\" (/docker) blue link:\n\nContainer names are not visible when i click \"/docker\" (/containers/docker) or \"/user\" (/containers/user) in Subcontainers\n. ",
    "sig-critchie": "Made the same mistake. Thankfully these messages were her.\n. ",
    "Jolly23": "Get it, thanks. ",
    "shahidhs-ibm": "Hi @vishh\nApologies for the delay in incorporating your review comments. Please check the updates and let me know your comments.\n. @vishh \nIt would be great if you can guide me on how to configure Travis for my zSystem specific build.\nThe cAdvisor code to build on s390x (zSystem) needs gccgo on s390x (Refer to link - https://github.com/linux-on-ibm-z/docs/wiki/Building-gccgo). This will need Travis to build gccgo first for s390x and then build cAdvisor.\n. @mvdan Thanks for the info. I am new to Travis, so I will explore the configuration and will update once done.\n. @vishh I'm working for IBM and covered as part of Google Corporate CLA (my id has been added as shahidhs.ibm@gmail.com). Please let me know if there is any additional info required.\n. @vishh \nI tried to reproduce the failure of the 'default' build on my system but unable to reproduce it. Please let me know if the issue is related to my pull request or the environment on which build is running.\n. @vishh If the PR looks good to you, please let me know when it can be merged.\n. Thanks @vishh ! :-)\n. @vishh \nI am not sure if this is right channel to discuss this topic but we would like to know if cAdvisor CI can be extended to support z system. Please let us know what support will be required from IBM side (we are aware on the H/w support).\nLet me know if you want to start this discussion on some other channel.\n. Thanks vish for the information.\nCan you please provide me more details on how the current CI infrastructure is like?\nAlso I need below details in order to setup the CI infrastructure on z Systems:\n1. How frequent the CI run on average?\n2. What is the recommended H/W and configuration requirement\n3. Special S/W requirements (if any)\n. Thanks Vishh.\nWe are in process of procuring the s390x VMs for the CI. We like to extend this CI discussion to include Kubernetes as well. We believe that the above information is also applicable for Kubernetes. Please let us know if this is not the case.\nAlso just curious on if there is any other way than ghprb to hook up the s390x VMs directly to cAdvisor and Kubernetes CI?\n. Could you please let me know how to get in touch with 'sig-testing' group of Kubernetes? I checked their home page but not sure how to start the communication with them. Please guide me here.\nAlso please let me know about the next actions that you do in the CI once the build is success. My query is mainly related to the archiving source, binary etc post build.\n. Here we are getting ASCII code from an array. We are converting this ASCII code into string and appending to form the machine hardware name (s390x for zSystem case). Hence we can't use 'Itoa' for this particular case.\n. Updated the 'utils/sysfs/sysfs.go' with modified if-else statements. Please check and let me know if it looks ok.\n. ",
    "Snorch": "I signed it!\n. Sorry my example were not working, here is an example with real cgroups string I got on my node:\nhttps://play.golang.org/p/NrmYySNVIB\n. Thanks for your comment, I will update pull request.\n. Ok, updated.\n. ",
    "jordanglassman": "I would be curious if there was a ballpark timeline available, particularly for the 2.0 beta API, for planning purposes.\n. I would be curious if there was a ballpark timeline available, particularly for the 2.0 beta API, for planning purposes.\n. ",
    "sandish2222": "which release version of kubernetes contailns 0.18.0\n. ",
    "maarsl": "Thanks Jimmy,\nBelow is the log I get from container\nI1001 20:50:33.354399 00001 storagedriver.go:111] No backend storage selected\nI1001 20:50:33.354440 00001 storagedriver.go:113] Caching stats in memory for 2m0s\nI1001 20:50:33.354501 00001 manager.go:127] cAdvisor running in container: \"/system.slice/docker-6956a609f01b776488f185feced67d5dc13076f888ec5ce15b5d1c4f0d127863.scope\"\nI1001 20:50:33.359378 00001 fs.go:93] Filesystem partitions: map[/dev/mapper/docker-253:1-397726-6956a609f01b776488f185feced67d5dc13076f888ec5ce15b5d1c4f0d127863:{mountpoint:/ major:252 minor:1} /dev/vda1:{mountpoint:/rootfs major:253 minor:1}]\nI1001 20:50:33.367408 00001 machine.go:49] Couldn't collect info from any of the files in \"/etc/machine-id,/var/lib/dbus/machine-id\"\nI1001 20:50:33.367466 00001 manager.go:158] Machine: {NumCores:4 CpuFrequency:2399998 MemoryCapacity:1041285120 MachineID: SystemUUID:7BD1D0F9-1564-46FA-B42B-5A8EFDBEF681 BootID:e1d4b6f0-abd8-4f9b-880f-b5f90e5d6d5c Filesystems:[{Device:/dev/mapper/docker-253:1-397726-6956a609f01b776488f185feced67d5dc13076f888ec5ce15b5d1c4f0d127863 Capacity:105554829312} {Device:/dev/vda1 Capacity:21001654272}] DiskMap:map[252:0:{Name:dm-0 Major:252 Minor:0 Size:107374182400 Scheduler:none} 252:1:{Name:dm-1 Major:252 Minor:1 Size:107374182400 Scheduler:none} 253:0:{Name:vda Major:253 Minor:0 Size:21474836480 Scheduler:none}] NetworkDevices:[{Name:eth0 MacAddress:fa:16:3e:65:1a:c4 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:1073332224 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:1 Memory:0 Cores:[{Id:0 Threads:[1] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:2 Memory:0 Cores:[{Id:0 Threads:[2] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:3 Memory:0 Cores:[{Id:0 Threads:[3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown}\nI1001 20:50:33.368867 00001 manager.go:165] Version: {KernelVersion:3.10.0-229.11.1.el7.x86_64 ContainerOsVersion:Buildroot 2014.02 DockerVersion:1.8.2 CadvisorVersion:0.18.0}\nI1001 20:50:33.501804 00001 factory.go:226] System is using systemd\nI1001 20:50:33.507816 00001 factory.go:234] Registering Docker factory\nI1001 20:50:33.513308 00001 factory.go:89] Registering Raw factory\nI1001 20:50:33.617724 00001 manager.go:998] Started watching for new ooms in manager\nW1001 20:50:33.618610 00001 manager.go:234] Could not configure a source for OOM detection, disabling OOM events: exec: \"journalctl\": executable file not found in $PATH\nI1001 20:50:33.635450 00001 manager.go:247] Starting recovery of all containers\nI1001 20:50:33.728405 00001 manager.go:252] Recovery completed\nI1001 20:50:33.783829 00001 cadvisor.go:94] Starting cAdvisor version: \"0.18.0\" on port 8080\nI see a couple of errors but don't know whether they  are related.\n. yes, it just exits. And if I run docker run manually it works fine. the only difference in running via module is, module creates an init script like \n`````` [Unit]\nDescription=Daemon for cadvisor\nAfter=docker.service\nRequires=docker.service\n[Service]\nRestart=on-failure\nStartLimitInterval=20\nStartLimitBurst=5\nTimeoutStartSec=0\nEnvironment=\"HOME=/root\"\nExecStartPre=-/usr/bin/docker kill cadvisor\nExecStartPre=-/usr/bin/docker rm cadvisor\nExecStart=/usr/bin/docker run \\\n        --net bridge -m 0b --detach=true -p 9090:8080 -v /:/rootfs:ro -v /var/run:/var/run:rw -v /sys:/sys:ro -v /var/lib/docker/:/var/lib/docker:ro --privileged=false \\\n        --name cadvisor \\\n        google/cadvisor:latest --logtostderr \\\nExecStop=-/usr/bin/docker stop cadvisor\n[Install]\nWantedBy=multi-user.target\n``` and start/stop container using this script.\n``````\n. ",
    "MichaelTong": "I think this is a good use since we have some workflows using containers in an come and go manner. It's hard to keep tracking of those data based on names.. @dashpole I mean the Storage Plugins. \nMy set-up is basically Elasticsearch + Kibana. And in the stats sent to Elasticsearch, container_Name is actually the container name. While in my use case where we run some long workflow which will create a number of containers in sequence, it may be a little hard for us to track the stats by container names, and we have already recorded the container ids during the workflow.\nWhile I'm reading the code of elasticsearch plugin, I found that container_Name will be set to cInfo.ContainerReference.Aliases[0] which is the name in most cases, however Aliases[1] or cInfo.ContainerReference.Namewhich is/contains the container id is more useful for our use case.\nSo I'm wondering if it is possible to add an option to set container_Name as at least cInfo.ContainerReference.Name.\nI also noticed that the label in Prometheus seems to contain contain.Name which I believe to be like \"/docker/\\<container id>\", but it is just sad we couldn't switch to prometheus for now.\n. ",
    "nashasha1": "Yes. I got this error when I run godep restore ! Even I use make to build, it still need this package, am I right?\n. Thanks @jimmidyson . I was building it in a new vm. So I had to restore it. The point is this package disappear. I don't know why. You can try go get it, I think you will get the same error.\n. Thanks@jimmidyson  godep go build work well! \n. ",
    "solomatnikov": "I am wondering if current approach to measuring memory footprint in cAdvisor makes sense. Total memory usage reported by Linux or cgroups is generally misleading because it includes cache, e.g.:\nhttp://www.linuxatemyram.com/\nSo called \"hot\" or \"active\" memory metric is also usually useless because it is a rough approximation of LRU used by the kernel to select pages for swapping. However, it the system starts swapping performance decreases by orders of magnitude, so usually people try hard to avoid swapping.\nThis is not a new problem and recently a new more meaningful metric was added to the kernel:\nhttps://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=34e431b0ae398fc54ea69ff85ec700722c9da773\nSo, the suggestion is to use MemTotal - MemAvailable as memory utilization metric for the host/top level.\nUnfortunately, there is no similar metric for cgroups but RSS makes the most sense.\n. @vishh Not sure I understand your comment: are you agreeing with my point that hot/active memory is not useful metric because in general swapping should not happen under normal conditions?\nI am trying to figure out what are the \"actionable\" metrics for containers, particularly memory related. \n. ",
    "alculquicondor": "What sets that hierarchy? is it through the cgroups or some kind of label?. ",
    "a-robinson": "Not as far as I know - I've only observed it the once.\n. ",
    "ncdc": "cc @vbatts @rhvgoyal\n. I'm investigating using docker info to get the pool id, and then either invoking Docker's code to get the info (pending a go 1.5 fix) or using dmsetup status <pool>.\n. I just tested on Kube master (EDIT: which has cadvisor 0.16.0.2). It turns out that capacity is being reported when the Docker storage driver is devicemapper, but I don't believe the reported data is correct.\nI am seeing that cadvisor is reporting usage=17426235392, capacity=29790838784, which lines up with my device for /:\n/dev/mapper/vg_vagrant-lv_root   29092616  17017808   10830668  62% /\nMeanwhile, docker info reports this (yeah yeah, I'm still using loopback on my VM :smile:):\nStorage Driver: devicemapper\n Pool Name: docker-253:1-130513-pool\n Pool Blocksize: 65.54 kB\n Backing Filesystem: extfs\n Data file: /dev/loop0\n Metadata file: /dev/loop1\n Data Space Used: 4.731 GB\n Data Space Total: 107.4 GB\n Data Space Available: 12.36 GB\n Metadata Space Used: 8.663 MB\n Metadata Space Total: 2.147 GB\n Metadata Space Available: 2.139 GB\n. I'm not sure querying for the filesystem that cadvisor has labeled \"docker-images\" is the best way to do this going forward. It might make more sense to move the image storage query into something that isn't filesystem-specific. That way, we can detect the driver and decide the right path (query devicemapper vs filesystem vs ...). WDYT?\n. > I assume we can re-use the dmsetup utility based approach for calculating container's fs usage as well. \nMaybe... I'll defer to @vbatts @rhvgoyal here. I just did a quick test running the busybox image and making a few copies of /bin/busybox inside the container. Here are the results:\n```\nbefore making any copies\nvagrant@f22:~ sudo dmsetup status docker-253:0-33713995-7a20304c351953c720074ff1eda85ee21e11d0bb2bdbe8d358480377ad073ce1\n0 209715200 thin 138240 209715199\nafter making a few copies\nvagrant@f22:~ sudo dmsetup status docker-253:0-33713995-7a20304c351953c720074ff1eda85ee21e11d0bb2bdbe8d358480377ad073ce1\n0 209715200 thin 142336 209715199\n```\nYou can see that one number increased: 138240 to 142336. This seems promising.\n. I think this is a thin-pool LVM, unless I did something wrong when I ran docker-storage-setup (this is a brand new VM with /dev/sdb dedicated to Docker storage).\n. But I agree it sure would be nice just to get this info from somewhere else instead of having to reinvent the wheel.\n. @jimmidyson I was looking at a container; you're looking at the entire pool. I have the thin-pool output as well:\ndocker-docker--pool: 0 53624832 thin-pool 7 16/17408 161/52368 - rw discard_passdown queue_if_no_space\n. I want to state, at least for my own understanding, what I believe to be the problems we are trying to address:\n- determine the used, available, total capacity of the location where Docker is storing images\n  - for e.g. Kubernetes to make a determination when it needs to GC some images\n- determine the used, available, total capacity of a container's file system\n  - is this just the unique COW/overlay delta? or something else?\n  - for ???\nIs this correct?\n. cc @smarterclayton \n. I won't be able to review until tomorrow morning at the earliest.\nOn Tuesday, October 20, 2015, Jimmi Dyson notifications@github.com wrote:\n\n@ncdc https://github.com/ncdc @smarterclayton\nhttps://github.com/smarterclayton Any comments before I merge?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/926#issuecomment-149689248.\n. @jimmidyson sorry, I probably can't get to it this morning :disappointed: If it works for you, I have no objections. I'm assuming that, worst case, this isn't any worse than the current code in master, right? :smile:\n. @vishh @jimmidyson there is still apparently an issue with devicemapper :frowning: \n\n926 added code that sets fsInfo.labels[LabelDockerImages] to the devicemapper device correctly. Unfortunately, after that, we have fsInfo.addLabels(context), which calls updateDockerImagesPath, which ends up replacing the previously set value with the device that corresponds to /, at least in my testing.\nHere is some debugging output:\n```\nI0121 13:15:24.588091    1235 manager.go:131] cAdvisor running in container: \"/\"\nI0121 13:15:24.682259    1235 fs.go:112] Filesystem partitions: map[/dev/sda1:{mountpoint:/boot major:8 minor:1 fsType: blockSize:0} vg_vagrant-docker--pool:{mountpoint: major:253 minor:3 fsType:devicemapper blockSize:1024} /dev/mapper/vg_vagrant-lv_root:{mountpoint:/ major:253 minor:0 fsType: blockSize:0}]\nsource= vg_vagrant-docker--pool\nmountpoint=\nv= /var/lib/docker/devicemapper\nv= /var/lib/docker/btrfs\nv= /var/lib/docker/aufs\nv= /var/lib/docker/overlay\nv= /var/lib/docker/zfs\nv= /var/lib/docker\nv= /var/lib\nv= /var\nv= /\nsource= /dev/mapper/vg_vagrant-lv_root\nmountpoint= /\nv= /var/lib/docker/devicemapper\nv= /var/lib/docker/btrfs\nv= /var/lib/docker/aufs\nv= /var/lib/docker/overlay\nv= /var/lib/docker/zfs\nv= /var/lib/docker\nv= /var/lib\nv= /var\nv= /\ni= vg_vagrant-docker--pool\nmnt=\noverriding with source= /dev/mapper/vg_vagrant-lv_root\nsource= /dev/sda1\nmountpoint= /boot\nv= /var/lib/docker/devicemapper\nv= /var/lib/docker/btrfs\nv= /var/lib/docker/aufs\nv= /var/lib/docker/overlay\nv= /var/lib/docker/zfs\nv= /var/lib/docker\nv= /var/lib\nv= /var\nv= /\n&fs.RealFsInfo{partitions:map[string]fs.partition{\"/dev/mapper/vg_vagrant-lv_root\":fs.partition{mountpoint:\"/\", major:0xfd, minor:0x0, fsType:\"\", blockSize:0x0}, \"/dev/sda1\":fs.partition{mountpoint:\"/boot\", major:0x8, minor:0x1, fsType:\"\", blockSize:0x0}, \"vg_vagrant-docker--pool\":fs.partition{mountpoint:\"\", major:0xfd, minor:0x3, fsType:\"devicemapper\", blockSize:0x400}}, labels:map[string]string{\"docker-images\":\"/dev/mapper/vg_vagrant-lv_root\", \"root\":\"/dev/mapper/vg_vagrant-lv_root\"}}\n```\nIn updateDockerImagesPath, because there is no mountpoint associated with the devicemapper pool, when it sees the source that corresponds to /, it ends up overriding the previously set value. I wonder if we just need to short-circuit if we know it's devicemapper and not call this function.\n. This seems to fix it:\n``` diff\ndiff --git a/fs/fs.go b/fs/fs.go\nindex 32acef5..ed67ff7 100644\n--- a/fs/fs.go\n+++ b/fs/fs.go\n@@ -117,14 +117,21 @@ func NewFsInfo(context Context) (FsInfo, error) {\nfunc (self *RealFsInfo) addLabels(context Context) {\n        dockerPaths := getDockerImagePaths(context)\n+       deviceMapper := false\n+       for , p := range self.partitions {\n+               if p.fsType == \"devicemapper\" {\n+                       deviceMapper = true\n+               }\n+       }\n        for src, p := range self.partitions {\n                if p.mountpoint == \"/\" {\n                        if , ok := self.labels[LabelSystemRoot]; !ok {\n                                self.labels[LabelSystemRoot] = src\n                        }\n                }\n-               self.updateDockerImagesPath(src, p.mountpoint, dockerPaths)\n-               // TODO(rjnagal): Add label for docker devicemapper pool.\n+               if !deviceMapper {\n+                       self.updateDockerImagesPath(src, p.mountpoint, dockerPaths)\n+               }\n        }\n }\n```\n. > When using real volumes instead of loopback, will the size of the pool\n\ndevice correspond to the size of the data volume or the sum of data and\nmetadata volumes?\n\nsudo docker info\nContainers: 2\nImages: 6\nStorage Driver: devicemapper\n Pool Name: vg_vagrant-docker--pool\n Pool Blocksize: 524.3 kB\n Backing Filesystem: xfs\n Data file:\n Metadata file:\n Data Space Used: 529.5 MB\n Data Space Total: 27.58 GB\n Data Space Available: 27.05 GB\n Metadata Space Used: 151.6 kB\n Metadata Space Total: 33.55 MB\n Metadata Space Available: 33.4 MB\nHere's what cadvisor reports (with the patch above applied):\n[]v2.FsInfo{v2.FsInfo{Device:\"vg_vagrant-docker--pool\", Mountpoint:\"\", Capacity:0x66c000000, Available:0x64c700000, Usage:0x1f900000, Labels:[]string{\"docker-images\"}}}\nTranslated, Capacity=27581743104, Available=27052212224, Usage=529530880. Those #s look like they align with the Data values above.\n. @jimmidyson I'll submit, thanks!\n. @jimmidyson do we need to do anything re \"So I would not revert the change but rather extend the check for loopback devicemapper to use the underlying partition info.\" ?\n. I realized my patch blindly sets deviceMapper to true regardless of the partition label - I'm updating it to only do this if the device for LabelDockerImages matches the loop var. It'll be in my PR.\n. @vishh I'll apply my patch to another VM I have that's using loopback and see what it reports. If it's incorrect, would you like the fix in the same PR or a separate one?\n. @vishh yes\n. @jthornber how long does it take to create the metadata snapshot so it can be used with thin_ls?\n. @jthornber oh, so it's super fast? Is it available as soon as we finish calling dmsetup message or is it possible that there's a bit of a delay? I'm trying to make sure we can avoid a race condition where we send the reserve_metadata_snap message and then try to use thin_ls before the snapshot is ready.\n. Sounds like there's no chance for a race. Great, thanks!\n. @vishh @timstclair we can close this now that 1204 is in, right?\n@ravilr I'll check and see if I can get an ETA for when the newer RPMs will be available.\n. cc @DirectXMan12 \n@jszczepkowski @vishh please let us know if this is an area where we could help out\n. FYI @derekwaynecarr \n. I'm working on a fix for this - could you please assign it to me @vishh @timstclair ?\n. @timstclair thanks. Perhaps we could discuss some of my team getting those permissions, similar to what we have in kubernetes/kubernetes?\n. @vishh @jimmidyson PTAL. I think I have all the cases covered...\n. @jimmidyson @vishh @rhvgoyal updated, PTAL. There may be some additional refactoring that could make this cleaner, but I've got to step away for now (impending :snowflake:)\n. It's not that messy :-p\nThanks for the review & merge!\n. :smiley: \n. cc @sjenning\n. cc @timothysc \n. cc @vishh \n. #1158 fixed this - we can close, right?\n. Related issue is #1156\n. Looks like this should work\n. I'm seeing this in the logs:\n```\n--- FAIL: TestDockerContainerSpec (1.35s)\n    assertions.go:150: \n    Location:   docker_test.go:220\n    Error:      Should be true\n    Messages:   Network should be isolated\n--- FAIL: TestDockerContainerNetworkStats (10.89s)\n    assertions.go:150: \n    Location:   docker_test.go:290\n    Error:      Should not be equal\n    Messages:   Network tx and rx bytes should not be equal\nassertions.go:150: \nLocation:   docker_test.go:291\nError:      Should not be equal\nMessages:   Network tx and rx packets should not be equal\n\n```\n. Wasn't that code that he removed essentially duplicating the pid > 0 check?\n. @vishh I didn't test this commit per se, but I did test a similar change in OpenShift's copy of cadvisor that only sets the pid nonzero (i.e. 1) for the root cgroup. And it did fix the CPU issue and I confirmed in pprof too.\n. cc @vishh \n. What CI failed - I'm guessing you have something besides the Jenkins GCE e2e?\n. It appears there's an issue with the way that CI is running the integration tests. It's invoking like this:\n\"godep\" [\"go\" \"test\" \"--timeout\" \"15m0s\" \"\" \"github.com/google/cadvisor/integration/tests/...\" \"--host\" \"e2e-cadvisor-container-vm-v20160127\" \"--port\" \"8080\" \"--ssh-options\" \"-i /home/jenkins/.ssh/google_compute_engine -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o CheckHostIP=no -o StrictHostKeyChecking=no\"]\nNotice the empty string argument in between 15m0s and github.com/google/cadvisor/integration/tests/... . Because of that empty string, CI is basically just running godep go test --host ....\nThe reason that CI is failing is because --host is not a valid flag for the new cadvisor_test.go that was added in Seth's PR in the project root.\n. @vishh it doesn't affect the test framework, per se. What happened is the PR added a new test at the root of the project directory, where one didn't exist before. And because there is no --host flag at the root (it only exists in the integration test code), things fail. Can you all fix how Jenkins invokes godep go test to get rid of that empty string argument?\n. :+1: \n. cc @justinsb \n. @jimmidyson at least when we run the OpenShift node containerized, we bind mount /sys as /sys into the container (see e.g. https://docs.openshift.org/latest/getting_started/administrators.html#running-in-a-docker-container). And the readme in this repo also suggests bind mounting /sys as /sys when running cadvisor as a container. Not sure about the Kubelet, though.\n. SGTM\n. @sjenning fyi there's a contrib PR (https://github.com/kubernetes/contrib/pull/937) that uses hpcloud/tail so I don't think it would be unreasonable if the fsnotify bug were fixed.\n. @sjenning did you ever get a yes/no from @rhvgoyal about the assumption?\n. FYI @vishh @dchen1107 @timstclair the initial \"thin_ls\" PR that we added only works for devicemapper-managed thin pools. This PR should allow support for LVM-managed pools as well. AFAIK this PR helps us pass some node E2E tests that are currently failing on the Red Hat stack.\ncc @rhvgoyal @ingvagabund\n. If this isn't approved for Kube 1.3, we'd love to see it ASAP after.\n. @vishh if this can make 1.3, that would be great\n. @vishh assuming this gets merged, we can do the PR against release-0.23 and then update the kube godep once you guys cut a new tag\n. I think this looks good. @vishh can you kick the e2e?\n. Not sure if I have admin, but let's see:\n@k8s-bot ok to test\n. @vishh @derekwaynecarr any other comments?\n. I'd like to get this merged ASAP once it passes reviews so we can prep the follow-on tasks (release-0.23 cherry-pick, kube godep bump)... any other comments?\n. \ud83d\udc4d \n. @vishh @timstclair we're going to need this for kube 1.3 too - we just found it today :-(\n. cc @dchen1107 @yujuhong \n. Ok, squahsing\n. Squashed\n. Once this gets merged, @sjenning will do the cherry-pick PR to the 0.23 branch. And then once you guys create the next 0.23.x tag, either Seth or I will do the godep bump PR to kube.\n. This is how devicemapper works. I'll get more details later but this is not\na bug.\nOn Mon, Jun 27, 2016 at 1:31 AM DeShuai Ma notifications@github.com wrote:\n\n@ncdc https://github.com/ncdc @sjenning https://github.com/sjenning\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1354#issuecomment-228657366,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AAABYlkG9l0PRFQow0wwKpjuxkNCiaSRks5qP2AygaJpZM4I-xPG\n.\n. @mdshuai correct, it will only ever increase. From what I understand, devicemapper allocates blocks when needed, but it doesn't know about filesystem deletes. @rhvgoyal might be able to provide more concrete details.\n. I think it's better to track the blocks in the thin pool; otherwise, you aren't accounting for the actual space being used and it could look like you have more free space available than you actually do.\n. i.e. I don't think this is a bug\n. @vishh you mean for the devicemapper thin pool that holds all the Docker images? That's covered by the call to fsInfo.addDockerImagesLabel, which supports both devicemapper and non-devicemapper graph storage.\n. With this change, on my VM:\n\nI0629 16:22:30.041129    7662 fs.go:116] Filesystem partitions: map[/dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:ext4 blockSize:0} andydocker-docker--pool:{mountpoint: major:253 minor:3 fsType:devicemapper blockSize:1024} /dev/mapper/vg_vagrant-lv_root:{mountpoint:/ major:253 minor:0 fsType:xfs blockSize:0}]\nSee \"andydocker-docker--pool\"\n. For clarity, nothing about this PR changes how e.g. \"andydocker-docker--pool\" is displayed in the list of filesystem partitions. It just excludes the per-container /var/lib/docker/devicemapper/mnt/... mounts from that list, since those are covered by ThinPoolWatcher.\n. @vishh @timstclair ok to test? \ud83d\ude04 \n. Rebased\n. ok to test\n. @vishh @timstclair PTAL\n. @timstclair updated\n. @timstclair I think what @jimmidyson is saying may be correct. I've seen multiple cadvisor PRs with no conflicts that \"need rebasing\"...\nAnyways, just pushed the rebased copy.\n. Yay thanks @vishh !!!\nAny other comments on this?\n. @vishh @timstclair bump \ud83d\ude04 \n. @pmorie should we just move the cache miss message to glog.V(5)?\n. I can update #1359 to include whatever we decide to do here\n. I'm guessing that changing to pointers probably isn't backwards compatible, so I'm fine with inodesReported bool or something to that effect\n. FYI @timstclair @vishh - lmk if something like this is ok\n. Kernel bugzilla: https://bugzilla.redhat.com/show_bug.cgi?id=1363776\nOpenShift bugzilla: https://bugzilla.redhat.com/show_bug.cgi?id=1362109\n. devicemapper filesystem stats will not be reported: RHEL/Centos 7.x kernel version 3.10.0-366 or later is required to use thin_ls - you have \"3.10.0-327.22.2.el7.x86_64\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\nNeed to fix that \ud83d\ude04 \n. @timstclair any other comments?\n. Done\n. \ud83d\ude04 I was in the middle of squashing when you posted the comment\n. Thanks for the review + merge!\n. GCE failure is unrelated \ud83d\ude22:\n```\nF0809 06:30:42.540891   30944 runner.go:290] Error 0: failed to make remote testing directory: command \"ssh\" [\"-i\" \"/var/lib/jenkins/gce_keys/google_compute_engine\" \"-o\" \"UserKnownHostsFile=/dev/null\" \"-o\" \"IdentitiesOnly=yes\" \"-o\" \"CheckHostIP=no\" \"-o\" \"StrictHostKeyChecking=no\" \"e2e-cadvisor-ubuntu-trusty\" \"--\" \"mkdir\" \"-p\" \"/tmp/cadvisor-30944\"] failed with error: exit status 1 and output: Warning: Permanently added 'e2e-cadvisor-ubuntu-trusty' (ECDSA) to the list of known hosts.\nmkdir: cannot create directory \u2018/tmp/cadvisor-30944\u2019: No space left on device\n```\n. @k8s-bot test this\n. @timstclair @vishh can you guys fix Jenkins out of disk?\n. We're trying to get OpenShift either on a tagged release, or on a branch without having to cherry-pick individual commits into OpenShift's vendored copy of cadvisor.\n. I would probably say it's not super high priority compared to other cherry picks we've done for Kube 1.3.\n. Mine were fixes to devicemapper fs stats and/or thin_ls, so all devicemapper related.\n. @derekwaynecarr @pmorie are you ok w/this?\n. @pmorie @sjenning @derekwaynecarr if you want me to proceed with this, let me know and I'll rebase.\n. I'll take a look in a bit. @derekwaynecarr rebased. @derekwaynecarr rebased. I'm leaning towards option 1.\nHaving had the experience we have being a downstream consumer of Kubernetes in OpenShift, I am really scared of what it would entail to keep forks up to date as cadvisor continues to evolve. If we do end up choosing option 2, we would need to have explicit guarantees about API compatibility, so fork maintainers have as few headaches as possible when it comes time to rebase cadvisor into their fork.\n. Do you want to include any of the changelog data from the other 0.23.x releases after 0.23.2?\n. Do we need to try an array of possible locations?\n. @pmorie chicken & egg?\n. LGTM. FYI the reason thin_ls is so io intensive in this particular case is because we want to determine the exclusive bytes per thin LV. To do that, thin_ls has to read the entire metadata for the thin pool. So if the metadata is 300MB, each invocation of thin_ls has to read 300MB to be able to determine exclusive bytes per thin LV.. @sjenning do we need to add thin-provisioning-tools (I think that's the name?) to https://github.com/google/cadvisor/blob/master/deploy/Dockerfile?. @rhvgoyal is this an appropriate way to see if the Docker devicemapper driver is using loopback?\n. If we do that, we would get \"true\" for loop back too, wouldn't we?\nOn Thursday, January 21, 2016, Jimmi Dyson notifications@github.com wrote:\n\nIn fs/fs.go\nhttps://github.com/google/cadvisor/pull/1070#discussion_r50476917:\n\n@@ -117,14 +117,27 @@ func NewFsInfo(context Context) (FsInfo, error) {\nfunc (self *RealFsInfo) addLabels(context Context) {\n    dockerPaths := getDockerImagePaths(context)\n+\n-   // Determine if Docker is using devicemapper for its storage driver. If so, don't invoke\n-   // self.updateDockerImagesPath below, as that will incorrectly override the device associated with\n-   // LabelDockerImages (set previously up above in NewFsInfo).\n-   dockerUsesDeviceMapper := false\n-   for dev, p := range self.partitions {\n-       if p.fsType == \"devicemapper\" && self.labels[LabelDockerImages] == dev {\n\nWouldn't this be better done using the StorageDriver value from docker\ninfo?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1070/files#r50476917.\n. I'm guessing this is something you put in for debugging that should be removed?\n. Good idea - so move the flag defined in this file to cadvisor.go in the project root and pass it down from there?\n. This needs fixing, right?\n. I'd recommend creating an interface with functions such as reserveMetadataSnap and releaseMetadataSnap so this can be unit tested.\n. Would be nice to guard against multiple simultaneous invocations\n. You mean you don't speak @pmorie? \ud83d\ude1b \n. s/and/an/\n. I'm pretty sure this will break you out of the outer loop. I'd recommend adding some unit tests :)\n. Artificial initial delays are bad. Can we do something else? Run os.Stat() in a loop or something?\n. +1\n. My bad, had it backwards\n. not a hot loop. but i'd rather try immediately and then sleep instead of sleeping up front\n. driver\n. if _, err := os.Stat(metadataDevice); err != nil {\n. I would only do one Errorf in this case - can you combine this with the previous one?\n. I'll take a look at Fields\nOn Thu, Jun 23, 2016 at 2:25 PM Tim St. Clair notifications@github.com\nwrote:\nIn devicemapper/thin_pool_watcher.go\nhttps://github.com/google/cadvisor/pull/1348#discussion_r68288542:\n\n@@ -163,14 +163,14 @@ func (w *ThinPoolWatcher) checkReservation(poolName string) (bool, error) {\n        return false, err\n    }\n-   tokens := strings.Split(string(output), \" \")\n-   // Split returns the input as the last item in the result, adjust the\n-   // number of tokens by one\n-   if len(tokens) != thinPoolDmsetupStatusTokens+1 {\n-       return false, fmt.Errorf(\"unexpected output of dmsetup status command; expected 11 fields, got %v; output: %v\", len(tokens), string(output))\n-   // we care about the field at fields[thinPoolDmsetupStatusHeldMetadataRoot],\n-   // so make sure we get enough fields\n-   fields := strings.SplitN(string(output), \" \", thinPoolDmsetupStatusMinTokens+1)\n\nWould the fields ever be split by tabs or multiple spaces? If so\nstrings.Fields would be a better choice.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1348/files/8faba9efbd4dcd245225157a10cf2b8bcf565c56#r68288542,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AAABYsrTqz03aQYzk1KQqoh8oRxip3jRks5qOs-bgaJpZM4I87rG\n.\n. Re +1, we care about field 7, but we need it to be standalone and not to include the \"leftovers\"\n. K, I'll move it back so it doesn't swallow\nOn Thu, Jul 21, 2016 at 6:52 PM Tim St. Clair notifications@github.com\nwrote:\nIn devicemapper/thin_pool_watcher.go\nhttps://github.com/google/cadvisor/pull/1359#discussion_r71801848:\n\n```\nw.lock.RLock()\ndefer w.lock.RUnlock()\nv, ok := w.cache[deviceId]\nif !ok {\n```\n-       return 0, fmt.Errorf(\"no cached value for usage of device %v\", deviceId)\n-       // TODO: ideally we should keep track of how many times we failed to get the usage for this\n-       // device vs how many refreshes of the cache there have been, and return an error e.g. if we've\n-       // had at least 1 refresh and we still can't find the device.\n-       glog.V(5).Infof(\"no cached value for usage of device %v\", deviceId)\n\nThe problem with consuming the error here is we don't know whether the\nvalue returned is valid. For instance, is it a problem to set the\nbaseUsage to 0 in handler.go when there isn't a cached value?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1359/files/46c41743878b759623c02a508c7c1439aa2b3307#r71801848,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAABYgOLZ8wLpvd5AlH1MEvHDq5Km8fuks5qX_gQgaJpZM4JBiYs\n.\n. Ok cool, thanks!\nOn Thu, Aug 4, 2016 at 1:30 PM Tim St. Clair notifications@github.com\nwrote:\nIn container/docker/factory.go\nhttps://github.com/google/cadvisor/pull/1411#discussion_r73566520:\n\n@@ -197,6 +208,80 @@ func startThinPoolWatcher(dockerInfo _dockertypes.Info) (_devicemapper.ThinPoolW\n    return thinPoolWatcher, nil\n }\n+func getKernelVersion() (string, error) {\n\nThis function already exists here:\nhttps://github.com/google/cadvisor/blob/master/machine/info.go#L142\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1411/files/d1e20410961d80cf2bde3a8a68c6249cf7c06c95#r73566520,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAABYme1PxGA4dN5oKe4pj-zPO7GPq7Eks5qciHAgaJpZM4Jc2Va\n.\n. Make blows up on x86_64. Maybe there's another function that doesn't?\nOn Thu, Aug 4, 2016 at 1:35 PM Tim St. Clair notifications@github.com\nwrote:\nIn container/docker/factory.go\nhttps://github.com/google/cadvisor/pull/1411#discussion_r73567406:\n\n+\n-   b := make([]byte, len(unameOutput.Release))\n-   for i, v := range unameOutput.Release {\n-       b[i] = byte(v)\n-   }\n-   return string(b), nil\n  +}\n  +\n  +func ensureThinLsKernelVersion(\n\nkernelVersion string) error {\n\n\n// kernel 4.4.0 has the proper bug fixes to allow thin_ls to work without corrupting the thin pool\nversion_4_4_0 := semver.MustParse(\"4.4.0\")\n// RHEL kernel 3.10.0 release >= 366 has the proper bug fixes backported from 4.4.0 to allow\n// thin_ls to work without corrupting the thin pool\nversion_3_10_0 := semver.MustParse(\"3.10.0\")\n  +\nmatches := version_re.FindStringSubmatch(kernelVersion)\n\n\nIs this just to check that kernelVersion has sane formatting? I think\nblang/semver already does that with semver.Make\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1411/files/d1e20410961d80cf2bde3a8a68c6249cf7c06c95#r73567406,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAABYtvojP7NzLKBOb5XYLhNiCKb-MLvks5qciL6gaJpZM4Jc2Va\n.\n. I can't speak to if any other vendors backported the bug fixes to something earlier than 4.4.0. (@eparis where did you find that 4.4.0 was the min version with the fix?) My logic flow was this:\n1. Regardless of distro, if >= 4.4.0, good\n2. Otherwise < 4.4.0\n   1. If kernel doesn't contain \".el7.\", bad\n   2. Otherwise it does have \".el7\", so:\n      1. RHEL only backported it to 3.x, so if Major != 3, bad\n      2. Major is 3, so check 3.10.0-366 or newer -> good\n. I just tried Eric's suggestion, and because 4.4.0-abc has a pre-release component (abc), it is considered less than 4.4.0 proper. So that won't work.\n. Yes, except for the aforementioned fact that having x86_64 in the version string won't parse as a semver. so my sem literally only has the major, minor, and patch values.\n. You must have a background in C :-p\n\nOn Thursday, August 25, 2016, Timothy St. Clair notifications@github.com\nwrote:\n\nIn container/docker/factory.go\nhttps://github.com/google/cadvisor/pull/1439#discussion_r76323572:\n\n@@ -284,12 +286,18 @@ func Register(factory info.MachineInfoFactory, fsInfo fs.FsInfo, ignoreMetrics c\n        return fmt.Errorf(\"failed to get cgroup subsystems: %v\", err)\n    }\n-   var thinPoolWatcher *devicemapper.ThinPoolWatcher\n-   var (\n\ncould var-block at the top for readability.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1439/files/27b9eed7388c7627f53143e9be16112488dffa28#r76323572,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAABYvxhPJR4gq_OpLvxLeVhvUXpLQXgks5qjgKGgaJpZM4Jtfne\n.\n. Did you intend to use this on line 93? Also, you can't trap SIGKILL, can you?\n. Do you want whitespace after each version, or not (like 0.23.3 and 4)?\n. SGTM\n. \n",
    "vbatts": "https://github.com/vbatts/docker/commit/f7236a195c84687edb74fec28b6c4cc98e34185c commit was not cherry-picked for 1.8.z, but will be in docker 1.9\n. (and it's docker 1.8 that didn't build with go1.5. docker-1.9 does build with go1.5)\n. reusing docker code will only get you so far here too. And you may get\nstuck. Overlay and btrfs don't give you this detailed of information\neither. Overlay and vfs, you'd have to get df <docker root dir>, and\nbtrfs has different approaches depending on the version of btrfs-progs\navailable.\nOn Fri, Oct 16, 2015 at 3:01 PM, Vivek Goyal notifications@github.com\nwrote:\n\nAs per the thin target documentation first field after \"thin\" depicts, nr\nmapped sectors. So it will tell you how many sectors are actually\nmapped/used for this thin device.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/920#issuecomment-148807563.\n. @ncdc correct. Like I said, while this could be plumbed through docker like @rhvgoyal is saying, it has not been. It is very determined by the underlying CoW setup, and would leave you much more flexible to access the storage directly.\n. \n",
    "rhvgoyal": "I personally think that getting all the information from \"docker info\" makes sense. Converting Data from human readable bytes should be easy. Looks like units package in docker already provide function to convert it back. (FromHumanSize()).\nhttps://github.com/docker/docker/blob/master/pkg/units/size.go#L63\n. Are you referring to truncation which will happen while we converted to human readable sizes? So this will give you a lower bound of free pool space. I think this should be good enough for most use cases. If not, one can always look into changing it and trying using some function from devicemapper package.\n. As per the thin target documentation first field after \"thin\" depicts, nr mapped sectors. So it will tell you how many sectors are actually mapped/used for this thin device. \n. i think we should try to get all the information about images/containers using docker API.\n. @jimmidyson I think this information is dependent on graph driver and some of it is run time information. \nIn general, I think we should export all the needed information using \"docker-inspect\" and let higher layers use it.\nFor example, for devicemapper, you can get some information in \"GraphDriver\" section. For devicemapper it is exporting \"DeviceSize\" which should tell you the total virtual size of container.\nFor space used, I feel you should start container and do \"docker exec df /\".\n. @jimmidyson DeviceSize gives you the virtual size of thin device  in case of devicemapper. That is either 10G or 100G out of box depending on docker version you are running.\n. @jimmidyson If you do \"dmsetup table , it will show you following.\ndocker-253:1-2616453-5ea3100b3d4e0342d6154d68197fa339cf411132e6ab424dfdba9dc289e0253f: 0 209715200 thin 253:4 292\nHere 209715200 is last sector. Multiply it by 512, and you have to device size in bytes. Same as reported by DeviceSize in docker-inspect.\n. @jimmidyson Also DeviceSize is specific to devicemapper. I had introuced it because we were writing some tools so that we could use this information and mount image/container separately and inspect it. atomic mount etc. But some of the use cases were racy w.r.t docker operations (container deletion etc).\nNow people are working on aother tool and try to make it less racy. Basically move COW layer management in a separate tool and let everybody talk to that tool to mount/unmount. Once that tool is\nup and running, there is a possibility that you can call into that one and mount storage and inspect it (including for size).\nBut I guess this will also have some interference w.r.t container deletion as if container is being deleted which you have storage mounted, then deletion will fail. Or storage will be left behind with nobody to clean it up.\n. @jimmidyson So you want to look at \"dmsetup status \" to determine the fs size of container? Give it a try, may be it will work. Otherwise use \"docker exec\". BTW, docker-inspect gives\nyou the name of the device backing container. So it will make life little easy for you to figure out which thin device belongs to container root.\n\"GraphDriver\": {\n        \"Name\": \"devicemapper\",\n        \"Data\": {\n            \"DeviceId\": \"292\",\n            \"DeviceName\": \"docker-253:1-2616453-5ea3100b3d4e0342d6154d68197fa339cf411132e6ab424dfdba9dc289e0253f\",\n            \"DeviceSize\": \"107374182400\"\n        }\n. I just checked with thin pool developers and they think using nr_mapped from thin device status line to figure out how many blocks are mapped is fine. That will tell how many blocks in thin pool are in use.\nThere is a possibility total size calculated here is more than reported by statfs() but this is the real\nspace used.\n. @jimmidyson for total sectors, instead of field 5, it might be better to use field 2. As per man dmsetup section TABLE FORMAT second field is `num_sectors'. Last field is highest mapped sector and not the number of sectors.\n. @vishh If cadvisor is running inside a container, will it even see the docker pid to be able to do nsenter? I am assuming no. I guess if it shares the pid namespace with host, then it should be fine.\n. @jimmidyson can you elaborate a bit more on why sharing pid namespace with host is a problem for cadvisor. If cadvisor is reaping children properly that should continue to work. And if it does not and when cadvior exits, I am assuming all these children will be re-parented to pid 1 of host (systemd) and will be reaped. Is that not the case. Am I missing something?\n. @vishh above dmsetup based magic will include size of both unerlying image as well as writable layer. It is not writable layer only.\nDm developers had mentioned to me that if I really wanted writable layer, I can do following.\n- Take metadata snapshot from thin pool.\n- Do thin_delta\n- Parse output and calculate usage.\nI have not tried it but something along above lines should give us writable layer usage. Its not trivial though.\n. I agree that having an easy way to figure out how much space a thin device is using which is unshared from its parent is probably a useful information. But we don't seem to have that functionality available right now. @cc jthornber\n. CC @jthornber @snitm\n. @sjenning right, currently you can not run thin_ls on live metadata. How did you conclude that reserving and releasing metadata snapshot is heavyweight operation.\n. Here is the link to \"dmsetup status \" command output description.\nhttps://github.com/torvalds/linux/blob/master/Documentation/device-mapper/thin-provisioning.txt#L273\nFollowing fields should be visible.\n<transaction id> <used metadata blocks>/<total metadata blocks>\n<used data blocks>/<total data blocks> <held metadata root>\n[no_]discard_passdown ro|rw\n. Upon file deletion, discards are not sent to thin pool (by default). That means thin pool never knows that block space consumed by file has been freed. That's why you see same space.\nYou have two options.\n- Either enable online discards while mounting file system. That will slow down fs and is not a very good operation.\n- Run fstrim on container rootfs and that should generate discards and free up space.\n. File system knows about the free blocks and hopefully it exhausts free blocks first before it asks for more blocks from thin pool. So I think freed blocks will be reused. Just that these free blocks are sitting with filesystem and have not been returned to thin pool. \n. Can we put both the checks. Extract \"Storage Driver\" from docker info to make sure devicemapper is being used. And then look for \"Data loop file\" to figure out if loop devices are being used or not.\n. ",
    "ramitsurana": "Thanks for the update and Sorry for the trouble.\n. ",
    "osallou": "Same issue (same error message) on a Debian 8.1 on Amazon EC2, and can't access interface.\n. ",
    "prasadnh": "thanks for quick response... tried browser too... shows blank page.\n. i see this\ndocker logs -f cadvisor\nI1020 00:25:55.967297 00001 storagedriver.go:111] No backend storage\nselected\nI1020 00:25:55.967348 00001 storagedriver.go:113] Caching stats in memory\nfor 2m0s\nI1020 00:25:55.967421 00001 manager.go:127] cAdvisor running in container:\n\"/docker/44a8e7c9ac4941579f2afbcdf391cb79c5de81fb9e0317aa10361570fa1c96f9\"\nI1020 00:25:55.971370 00001 fs.go:93] Filesystem partitions:\nmap[/dev/disk/by-uuid/e1d70192-1bb0-461d-b89f-b054e45bfa00:{mountpoint:/rootfs\nmajor:202 minor:1} /dev/xvdb:{mountpoint:/rootfs/mnt major:202 minor:16}]\nI1020 00:25:56.724075 00001 machine.go:49] Couldn't collect info from any\nof the files in \"/etc/machine-id,/var/lib/dbus/machine-id\"\nI1020 00:25:56.724177 00001 manager.go:158] Machine: {NumCores:4\nCpuFrequency:2500042 MemoryCapacity:15770574848 MachineID:\nSystemUUID:EC2C6E40-B180-2157-4650-36A78FC5F6DB\nBootID:08726c62-c50e-48ca-bd6c-851e9842080e\nFilesystems:[{Device:/dev/disk/by-uuid/e1d70192-1bb0-461d-b89f-b054e45bfa00\nCapacity:42127835136} {Device:/dev/xvdb Capacity:39490912256}]\nDiskMap:map[202:0:{Name:xvda Major:202 Minor:0 Size:42949672960\nScheduler:deadline} 202:16:{Name:xvdb Major:202 Minor:16 Size:40256929792\nScheduler:deadline}] NetworkDevices:[{Name:eth0\nMacAddress:02:93:d9:92:29:41 Speed:0 Mtu:9001}] Topology:[{Id:0\nMemory:15770574848 Cores:[{Id:0 Threads:[0 2] Caches:[{Size:32768 Type:Data\nLevel:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified\nLevel:2}]} {Id:1 Threads:[1 3] Caches:[{Size:32768 Type:Data Level:1}\n{Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}]\nCaches:[{Size:26214400 Type:Unified Level:3}]}] CloudProvider:Unknown\nInstanceType:Unknown}\nI1020 00:25:56.725213 00001 manager.go:165] Version:\n{KernelVersion:3.13.0-62-generic ContainerOsVersion:Buildroot 2014.02\nDockerVersion:1.8.1 CadvisorVersion:0.18.0}\nI1020 00:25:56.852462 00001 factory.go:234] Registering Docker factory\nI1020 00:25:56.856547 00001 factory.go:89] Registering Raw factory\nI1020 00:25:57.144390 00001 manager.go:998] Started watching for new ooms\nin manager\nW1020 00:25:57.144673 00001 manager.go:234] Could not configure a source\nfor OOM detection, disabling OOM events: exec: \"journalctl\": executable\nfile not found in $PATH\nI1020 00:25:57.159663 00001 manager.go:247] Starting recovery of all\ncontainers\nI1020 00:25:57.207300 00001 manager.go:252] Recovery completed\nI1020 00:25:57.233508 00001 cadvisor.go:94] Starting cAdvisor version:\n\"0.18.0\" on port 8080\nOn Mon, Oct 19, 2015 at 4:57 PM, Vish Kannan notifications@github.com\nwrote:\n\nCan you run cAdvisor with --logtostderr flag ?\nOn Mon, Oct 19, 2015 at 4:40 PM, prasadnh notifications@github.com\nwrote:\n\nthanks for quick response... tried browser too... shows blank page.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/928#issuecomment-149377736.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/928#issuecomment-149379885.\n\n\nThanks,\nPrasad\n. I think i tried different options like port farwarding to 80 which is open\nin our network and also tried --port option.\nI don't see any issues when i do port farwarding for influxdb or  grafana...\nOn Mon, Oct 19, 2015 at 5:35 PM, Vish Kannan notifications@github.com\nwrote:\n\nI suspect you haven't exposed port 8080 from the cadvisor container on to\nyour host.\nOn Mon, Oct 19, 2015 at 5:33 PM, prasadnh notifications@github.com\nwrote:\n\ni see this\nubuntu@ip-10-201-5-134:~$ docker logs -f cadvisor\nI1020 00:25:55.967297 00001 storagedriver.go:111] No backend storage\nselected\nI1020 00:25:55.967348 00001 storagedriver.go:113] Caching stats in memory\nfor 2m0s\nI1020 00:25:55.967421 00001 manager.go:127] cAdvisor running in\ncontainer:\n\"/docker/44a8e7c9ac4941579f2afbcdf391cb79c5de81fb9e0317aa10361570fa1c96f9\"\nI1020 00:25:55.971370 00001 fs.go:93] Filesystem partitions:\nmap[/dev/disk/by-uuid/e1d70192-1bb0-461d-b89f-b054e45bfa00:{mountpoint:/rootfs\nmajor:202 minor:1} /dev/xvdb:{mountpoint:/rootfs/mnt major:202 minor:16}]\nI1020 00:25:56.724075 00001 machine.go:49] Couldn't collect info from any\nof the files in \"/etc/machine-id,/var/lib/dbus/machine-id\"\nI1020 00:25:56.724177 00001 manager.go:158] Machine: {NumCores:4\nCpuFrequency:2500042 MemoryCapacity:15770574848 MachineID:\nSystemUUID:EC2C6E40-B180-2157-4650-36A78FC5F6DB\nBootID:08726c62-c50e-48ca-bd6c-851e9842080e\nFilesystems:[{Device:/dev/disk/by-uuid/e1d70192-1bb0-461d-b89f-b054e45bfa00\nCapacity:42127835136} {Device:/dev/xvdb Capacity:39490912256}]\nDiskMap:map[202:0:{Name:xvda Major:202 Minor:0 Size:42949672960\nScheduler:deadline} 202:16:{Name:xvdb Major:202 Minor:16 Size:40256929792\nScheduler:deadline}] NetworkDevices:[{Name:eth0\nMacAddress:02:93:d9:92:29:41 Speed:0 Mtu:9001}] Topology:[{Id:0\nMemory:15770574848 Cores:[{Id:0 Threads:[0 2] Caches:[{Size:32768\nType:Data\nLevel:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified\nLevel:2}]} {Id:1 Threads:[1 3] Caches:[{Size:32768 Type:Data Level:1}\n{Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified\nLevel:2}]}]\nCaches:[{Size:26214400 Type:Unified Level:3}]}] CloudProvider:Unknown\nInstanceType:Unknown}\nI1020 00:25:56.725213 00001 manager.go:165] Version:\n{KernelVersion:3.13.0-62-generic ContainerOsVersion:Buildroot 2014.02\nDockerVersion:1.8.1 CadvisorVersion:0.18.0}\nI1020 00:25:56.852462 00001 factory.go:234] Registering Docker factory\nI1020 00:25:56.856547 00001 factory.go:89] Registering Raw factory\nI1020 00:25:57.144390 00001 manager.go:998] Started watching for new ooms\nin manager\nW1020 00:25:57.144673 00001 manager.go:234] Could not configure a source\nfor OOM detection, disabling OOM events: exec: \"journalctl\": executable\nfile not found in $PATH\nI1020 00:25:57.159663 00001 manager.go:247] Starting recovery of all\ncontainers\nI1020 00:25:57.207300 00001 manager.go:252] Recovery completed\nI1020 00:25:57.233508 00001 cadvisor.go:94] Starting cAdvisor version:\n\"0.18.0\" on port 8080\nOn Mon, Oct 19, 2015 at 4:57 PM, Vish Kannan notifications@github.com\nwrote:\n\nCan you run cAdvisor with --logtostderr flag ?\nOn Mon, Oct 19, 2015 at 4:40 PM, prasadnh notifications@github.com\nwrote:\n\nthanks for quick response... tried browser too... shows blank page.\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/google/cadvisor/issues/928#issuecomment-149377736\n.\n\n\u2014\nReply to this email directly or view it on GitHub\n<https://github.com/google/cadvisor/issues/928#issuecomment-149379885\n.\n\n\nThanks,\nPrasad\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/928#issuecomment-149385354.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/928#issuecomment-149385572.\n\n\nThanks,\nPrasad\n. Right...worked now...Thanks for the help\n. ",
    "kuoshin": "I have the same problem....Can you tell me how to fix this problem ?  thanks. ",
    "jekyang": "I just saw that kubelet  will export the metrics by api /stats/container, /stats/pod/containers /stats////, but the data from kubelet&cadvisor is also empty even i get it manually during debugging.  by \"docker inspect\" the pod container, there's empty value in HostConfig.NetworkMode, so there's something should be fixed by kubelet?\n. ",
    "jmartine": "+1\n. ",
    "chrisferry": "Dupe of this: #743\nBut :+1: This needs to happen asap\n. ",
    "berglh": "+0.9\nOn 3 Nov 2015 8:27 am, \"harun yardimci\" notifications@github.com wrote:\n\n+1\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/943#issuecomment-153177617.\n. InfluxDB is not officially supported by cAdvisor yet. As far as I remember, part of this was due to the changes in the line protocol that were due in v0.9.5 of InfluxDB. My understanding is that this has been released and we are waiting for a pull request to be reviewed an accepted.\n- InfluxDB v0.9 support #743\n- How to setup Docker monitoring\n  - This shows how to make it work with InfluxDB 0.8\n. \n",
    "mmaquevice": "+1\n. ",
    "chr0n1x": ":+1: \n. ",
    "andrejvanderzee": "+1\n. ",
    "clemSeveillac": "Hi folks, I didn't find this useful available disk space metrics in the cadvisor /metrics output (but just in /api/v2.0/storage?label=docker-images), does that mean we cannot scrap this via prometheus?. ",
    "andyzheng0831": "cc/ @andyzheng0831\n. It will be great if we can fix this issue soon and make it merged in next k8s release post 1.1\n. @vishh May I ask about the status of bug? As we already cut k8s 1.1.1 release, would it possible for us to speed up fixing this issue, so that this will unblock our effort ASAP.\n. I paste my response to an email thread about this topic below:\nThe path (/sys/class/dmi/id/product_name) exists in GCI. The dir /sys/class/dmi/id is a symlink to /sys/devices/virtual/dmi/id.\n. ",
    "Amey-D": "Regardless of whether systemd is init or not, I think it's desirable for cAdvisor to support the configuration where systemd is not managing cgroups for docker containers.\n. ",
    "graphaelli": "I signed it!\n. ",
    "grobie": "With https://github.com/google/cadvisor/pull/1429 you have full control about the attached labels. This issue should be solved now @timstclair.\n. LGTM. Still a bit too much global state for my taste, but not writing into the global flag state anymore is a huge win already.\n. Fixed with https://github.com/google/cadvisor/pull/1402 and can be closed.\n. @timstclair Can be closed.\n. These are set by kubernetes: https://github.com/kubernetes/kubernetes/blob/b0ea89c2f6b954a27ba0f545da955db4f25009d0/pkg/kubelet/dockertools/labels.go#L37-L50\nAny suggestions how to solve this? None of these labels make sense to export as metric labels. Filtering them in cAdvisor might be a bit awkward as it adds references back to Kubernetes. Changing this in Kubernetes might require quite some changes though.\n. I'm starting to understand what's going on here. Kubernetes attaches all these docker labels as a fallback to retrieve them later again. I guess the easiest would be to filter all labels which start with io.kubernetes.. What do you think @timstclair @vishh?\n. By the way, I believe cAdvisor shouldn't try to parse these labels and create a restart counter from it. This would be the job of Kubernetes, also as Docker containers don't have the concept of restarts.\n. \ud83d\udc4d Also happy to look into this otherwise, just found a similar problem.\n. I'm actually in favor of a consistent prefix, makes relabeling a lot easier. And with relabeling you can also still promote labels to non-prefixed ones. I'd be interested to learn why @fabxc finds this a \"shaky\" behavior though.\nA more descriptive prefix might work better than the generic label_. Something like com_docker_label_?\n. @timstclair Can be closed as https://github.com/google/cadvisor/pull/1402 is merged.\n. We're in high need for prometheus metrics about ceiling enforcement and I created a PR which exposes the information already collected by libcontainer.\n. I updated my commit to use my company email (@soundcloud.com), we have signed the CLA already.\n. @vishh Any feedback? It's a really big issue for us that Kubernetes/cAdvisor doesn't have any metrics on CPU throttling. Happy to fix whatever problem you see (I tried to follow the cAdvisor code style wherever possible).\n. @vishh Anything missing?\n. I signed it!\n. @vishh @timstclair What is missing here?\n. @timstclair I did this as part of my job at SoundCloud, so I changed my commit to use my soundcloud email address. As @matthiasr said, we signed it as a company. Please advise what to do here.\n. Yes, it's even my primary email address.\nOn Tue, Aug 2, 2016 at 2:38 PM, Tim St. Clair notifications@github.com\nwrote:\n\nDo you have your soundcloud email address connected to your github\naccount? (i.e. is it listed here? https://github.com/settings/emails)\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1386#issuecomment-237000100, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAANaIOT760WLCfQuao8Ha5cCl9ekjvYks5qb46MgaJpZM4JPZRD\n.\n. @f0 What behavior gets broken here? I assume you are referring to the period change, it's true that I changed it to the value used by Kubernetes to make the example more clear. Given this is just a test setup, what problem do you see here?\n. @vishh I rebased my change against current master. Do you have any other questions or change requests?\n. Thanks so much @vishh!\n. :+1: Looks great, the namespace makes sense. Thanks a lot!\n. Such labels should be provided via a separate metric and joined. See http://www.robustperception.io/target-labels-are-for-life-not-just-for-christmas/ and http://www.robustperception.io/exposing-the-software-version-to-prometheus/\n. Duplicate of #1404.\n. :+1:\n. Fixes #1312 \n. We don't rely on these internal labels, but use the standard Prometheus relabel configs to build these from the available meta labels https://prometheus.io/docs/operating/configuration/#kubernetes_sd_config. I assumed that is the preferred way, but I can also create a more specific filter if you think that's better. \n\nThe current resulting label names like container_label_io_kubernetes_pod_namespace are a bit too long to be useful anyways.\n. I'd be surprised if the Prometheus Kubernetes discovery client would rely\non labels exported by cAdvisor. As far as I can see, it reads these labels\nfrom the Kubernetes API\nhttps://github.com/prometheus/prometheus/blob/master/retrieval/discovery/kubernetes/pod.go#L294-L304.\nThis PR won't touch that at all, it only reduces the additional label noise\ngenerated by cAdvisor.\nOn Thu, Aug 18, 2016 at 3:29 AM Jimmi Dyson notifications@github.com\nwrote:\n\nI'm pretty sure this will affect pod discovery which uses the container\nlabels from exported metrics for pod name, etc but as I'm out on PTO all\nwell I can't check myself.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1426#issuecomment-240645493, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAANaGm8qQZwvi3dsUsCT2As4LyclZYdks5qhAnfgaJpZM4JnFwR\n.\n. @jimmidyson I understand now what you mean. Given Prometheus scrapes one cadvisor instance per node, it's not possible to use relabeling to attach pod name/namespace labels to the metrics. I'll rework the PR.\n. Here is an example of one metric exported by cadvisor (slightly reformatted to increase visibility).\n\ngoogle/cadvisor@2ed7198 kubelet@v1.3.5\ncontainer_cpu_cfs_periods_total{\n  container_label_io_kubernetes_container_hash=\"5af8c3b4\",\n  container_label_io_kubernetes_container_name=\"sync\",\n  container_label_io_kubernetes_container_restartCount=\"1\",\n  container_label_io_kubernetes_container_terminationMessagePath=\"/dev/termination-log\",\n  container_label_io_kubernetes_pod_name=\"popularsearches-web-3165456836-2bfey\",\n  container_label_io_kubernetes_pod_namespace=\"popularsearches\",\n  container_label_io_kubernetes_pod_terminationGracePeriod=\"30\",\n  container_label_io_kubernetes_pod_uid=\"6a291e48-47c4-11e6-84a4-c81f66bdf8bd\",\n  id=\"/docker/68e1f15353921f4d6d4d998fa7293306c4ac828d04d1284e410ddaa75cf8cf25\",\n  image=\"redacted.com/popularsearches:42-16-ba6bd88\",\n  name=\"k8s_sync.5af8c3b4_popularsearches-web-3165456836-2bfey_popularsearches_6a291e48-47c4-11e6-84a4-c81f66bdf8bd_c02d3775\"\n} 72819\n. Alright, I've reworked the implementation and added a whitelist, allowing io.kubernetes.pod.name and io.kubernetes.pod.namespace still to be applied to all container metrics. There is currently no other way to attach such labels to cadvisor metrics.\ngrobie/cadvisor@12e1d20 kubelet@v1.3.5\ncontainer_cpu_cfs_periods_total{\n  container_label_io_kubernetes_container_name=\"sync\",\n  container_label_io_kubernetes_pod_name=\"popularsearches-web-3165456836-2bfey\",\n  container_label_io_kubernetes_pod_namespace=\"popularsearches\",\n  id=\"/docker/68e1f15353921f4d6d4d998fa7293306c4ac828d04d1284e410ddaa75cf8cf25\",\n  image=\"redacted.com/popularsearches:42-16-ba6bd88\",\n  name=\"k8s_sync.5af8c3b4_popularsearches-web-3165456836-2bfey_popularsearches_6a291e48-47c4-11e6-84a4-c81f66bdf8bd_c02d3775\"\n} 73700\n. @timstclair I completely agree with you on leaking Kubernetes knowledge into cAdvisor. See my very first comment on the issue which started this discussion: https://github.com/google/cadvisor/issues/1312#issuecomment-240599649\nThat being said, without fundamentally changing Kubernetes, I don't see another way to work around this for now. cAdvisor reads these labels directly from the Docker API, so there is no other way to filter them then adding the knowledge to cADvisor. \nThe huge problems with these labels is that they generate new timeseries every time they change. Prometheus is designed under the assumption that a labelset uniquely identifies one entity. These changed labels (e.g. when the restartCount gets increased) result in incorrect aggregation results for example. Some of the now filtered labels are static, but just noise and make it a lot harder to work with the cadvisor metrics.\nBriefly before your comment I changed the implementation similar to your suggestion. I'm using a whitelist instead of a blacklist though. Pod name, namespace as well as the container name remain on the generated metrics as you can see in the example above.\nPTAL again @jimmidyson @fabxc\n. Alright, I didn't read your comment carefully enough. I'll rework it once again so that cadvisor simply accepts a filter function which can be set by kubernetes.\n. I like the approach, any objections to make it even more generic and pass in the container object? There are also labels set based on container.Aliases and container.Spec.Image for example.\nI'd then do something like if labelsFunc == nil { labelsFunc = defaultLabelsFunc } in the constructor.\n. This change did not alter the behavior around metric labels. By default, cAdvisor will continue to export all available Docker labels using a container_label_* prefix. Kubernetes embeds cAdvisor and we pass a custom containerLabelsFunc there to only set a handful of explicitly chosen labels. I'm not aware of how cAdvisor is usually used in combination with docker compose, but something similar could probably be done. I suspect we could alternatively also provide a flag to let users explicitly set the list of labels they want to attach.\n. I guess opening a new issue and discussing this separate topic there makes\nsense in either way.\nOn Mon, Sep 5, 2016 at 7:07 AM Murad notifications@github.com wrote:\n\nLast proposal, with configurable set of labels seems to be most\nreasonable. In addition to the list it could be a map, something like:\n[-container.labels\n ... ]\nwhere label_name_as_should_be_exported is optional, if it is needed to\nchange the name of exported label.\nThat would be great. Shall we open an issue for that?\nThanks in advance\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1426#issuecomment-244721185, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAANaH-zDaDWCK2gExa9JRMBNOcr6BI4ks5qm_fsgaJpZM4JnFwR\n.\n. I can re-open this PR against the release-v0.24 branch if that helps to\navoid cherry-picking?\n\nOn Mon, Aug 22, 2016 at 3:08 PM Tim St. Clair notifications@github.com\nwrote:\n\n@jimmidyson https://github.com/jimmidyson Yes. I just retagged v0.24.0\nto v0.24.0-alpha1 (I know it's bad to change tags, but that was the\noriginal intent). We can cherrypick this into the release-v0.24 branch so\nit will be included in the actual v0.24 release in a couple weeks.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1429#issuecomment-241517232, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAANaGjfqdMO0R6uhHjyJAgEm-q82AnCks5qifOfgaJpZM4JpKV_\n.\n. @timstclair of course. I thought the general pattern for bug fixes is to apply them against the release branch and then merge that release branch into master. but whatever works for you.\n. Cool, thanks. Once this is merged, I'll update my kubernetes PR to use the constants and the real master commit ID in godeps.json.\n. @timstclair I'm happy to tackle this, but just to config: is v2 stable? Any plans when the v1 usage inside v2 will be fixed?\n. Sounds like a plan. I'll close it for now.\n. I'm not worried about the variables, they are contained in the metrics package. The problematic thing is that we register the collector on the global prometheus registry. Until recently that was the only possible way, but client_golang v0.8.0 finally introduced custom registries.\n. The problem is that it makes it impossible to disable cadvisor prometheus metrics from Kubernetes, as the register method gets called even if cadvidor port == 0: https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cadvisor/cadvisor_linux.go#L133\n\nChange is incoming, just wanted to give a heads-up and give room for a discussion first.\n. It also means that all endpoints returning prometheus metrics in a kubelet (there are two: the cadvisor /metrics endpoint as well as the api / metrics endpoint) will both return the same set of metrics.\n. This removes the http_* metrics. The instrument handler is deprecated as it needs to be rewritten. Not sure you would like to keep them?\n. Depending on the outcome of the discussion in kubernetes/kubernetes#32638 we'll need to change the integration of cAdvsior in the kubelet once this PR is merged. We probably could also just stop using RegisterPrometheusHandler in the kubernetes code base to avoid the usage of the global registry.\n. @jimmidyson done\n. Anything left for me to do here? If you're not going to accept this, I can also just close it. @jimmidyson @dashpole . I'm sorry for the noise @dashpole. I updated the dependencies (which will bring several improvements to the metrics exported by Prometheus) and that fixed the build.. @dashpole you sure you want me to squash these two commits together? I'm not sure I see the point, they are independent changes which can be applied in isolation. Given the massive diff caused by vendor updates, I understood it to be best practice to separate them from actual code changes.. @dashpole another week of ping pong ;-). Thanks @Random-Liu. We should fix these metrics indeed.. Sure thing.\n. @vishh Ah, I had looked for that actually already. v2 uses the v1 CPU struct and automatically includes my changes: https://github.com/google/cadvisor/blob/master/info/v2/container.go#L136\n. I'm still a bit unsure how to namespace these stats. They get read from a file called cpu.stats. The implementation is done by using a completely fair scheduler. After reading the kernel documentation https://www.kernel.org/doc/Documentation/scheduler/sched-bwc.txt, maybe Bandwidth would be better?\nWhat do you think @jimmidyson @vishh?\n. I'd apply any naming change also to the Prometheus metric names. Currently it's container_cpu_cfs_*.\n. The configured limits are already available under the spec namespace, see quota and period there. The resulting prometheus metrics can be seen here: https://github.com/google/cadvisor/blob/master/metrics/testdata/prometheus_metrics#L119-L124\nThe number of throttled periods and the total resulting throttled time already get exposed verbatim with the change. What exactly would you like to see?\nI always prefer exporting metrics more or less directly. Most of the documentation users will find will discuss the standard cgroups metrics, I'd find it confusing to first read the code of cadvisor to understand how they were changed and exported instead.\n. If cpu.cfs_quota is set to a value greater -1, the CPU usage of that cgroup will be limited to that amount of nanoseconds per period. The period length is defined by cpu.cfs_period_us. Both of these values get exported already under the spec namespace. The periods here describe how many periods any process in the cgroup requested some cpu cycles. As far as I understand, a completely idle cgroup will not cause this counter to go up.\n. @fabxc done\n. Sure! Any suggestions/pointers where to document that?\nOn Sun, Aug 21, 2016 at 2:33 AM Jimmi Dyson notifications@github.com\nwrote:\n\nIn metrics/prometheus.go\nhttps://github.com/google/cadvisor/pull/1429#discussion_r75589584:\n\n@@ -537,6 +543,26 @@ const (\n    containerEnvPrefix   = \"container_env_\"\n )\n+// DefaultContainerLabels implements ContainerLabelsFunc. It exports the\n+// container name, first alias, image name as well as all its env and label\n+// values.\n+func DefaultContainerLabels(container *info.ContainerInfo) map[string]string {\n-   set := map[string]string{\"id\": container.Name}\n\nSo this now handles all labels, including id name etc. I personally like\nthat control for custom impls but it needs to be documented somewhere as\nwell as release noted.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1429/files/5ad6631b525a0335393344de944427982b26b00e#r75589584,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAANaOkHOd4teDQxJEkc-yPAQXGeSE9zks5qh_FTgaJpZM4JpKV_\n.\n. Done.\n. I extended the documentation a bit, but I'm not completely sure what you'd like to see here. I had hoped the comments on ContainerLabelsFunc, NewPrometheusCollector and DefaultContainerLabels were helpful. Please let me know what else you'd like to see added here.\n. I'm a bit unsure about exposing so many parts. If someone overwrites the function, it applies to their whole setup anyway, why would they need to care about how the labels in other clusters look like?\n\nI could split up the id+name+image part and the labels+env parts into two functions we expose each? I'm also fine with exposing constants for the labels though.\nLastly, actually we're doing it all wrong here by applying so many labels to all metrics. It should be only one identifier (probably id) and export another metric containing the mapping. See http://www.robustperception.io/target-labels-are-for-life-not-just-for-christmas/ and http://www.robustperception.io/exposing-the-software-version-to-prometheus/.\n. Very last part, the env labels have never worked for me so far, they're always empty (I think that's a bug somewhere in cadvisor). When I use docker inspect <container> I see many envs listed for kubernetes managed containers, but I've never seen any env labels on our metrics.\nShall we just remove the env part as we're already working on that part? @jimmidyson \n. Ah, ok. Thanks for the explanation. I still think it's a bad practice to expose such as labels, but I'll not introduce more breaking behavior here and mention that as well in a follow up issue.\n. @timstclair Done. Extracted the id/name/image to be exported constants.\n. This should be removed as well, right?\n. But it will collide with version flags set by other programs, no? I'd move that into main()\n. You're right of course, my bad.\n. ",
    "mrunalp": "@vishh I think the second option might be better so you can have a long running process that is in docker's mount namespace and communicate over a pipe.\n. @smarterclayton Thanks! I will look into it.\n. @smarterclayton Yeah, the system order is different from what we see in the container when we bind mount /sys over /sys. IMO, cadvisor should check which path exists and set the inotify watch accordingly. \n. @vishh This is what libcontainer mounts inside a container (for e.g.):\ndrwxr-xr-x. 2 root root  0 Sep 14 22:18 blkio\nlrwxrwxrwx. 1 root root 11 Sep 14 22:18 cpu -> cpu,cpuacct\ndrwxr-xr-x. 2 root root  0 Sep 14 22:18 cpu,cpuacct\nlrwxrwxrwx. 1 root root 11 Sep 14 22:18 cpuacct -> cpu,cpuacct\ndrwxr-xr-x. 2 root root  0 Sep 14 22:18 cpuset\ndrwxr-xr-x. 2 root root  0 Sep 14 22:18 devices\ndrwxr-xr-x. 2 root root  0 Sep 14 22:18 freezer\ndrwxr-xr-x. 2 root root  0 Sep 14 22:18 hugetlb\ndrwxr-xr-x. 2 root root  0 Sep 14 22:18 memory\nlrwxrwxrwx. 1 root root 16 Sep 14 22:18 net_cls -> net_cls,net_prio\ndrwxr-xr-x. 2 root root  0 Sep 14 22:18 net_cls,net_prio\nlrwxrwxrwx. 1 root root 16 Sep 14 22:18 net_prio -> net_cls,net_prio\ndrwxr-xr-x. 2 root root  0 Sep 14 22:18 perf_event\ndrwxr-xr-x. 2 root root  0 Sep 14 22:18 pids\ndrwxr-xr-x. 2 root root  0 Sep 14 22:18 systemd\nNow, when we bind mount /sys:/sys we are overwriting that with the order on the system which happens to be the opposite on RHEL and hence the issue. \nOne thing that cadvisor could do is not use combined paths as symlinks are provided for each subsystem to a combined path.\nNot sure how we can fix this in libcontainer. \n. @vishh Yes, libcontainer looks at the first mount points that it finds. How is cadvisor expected to be launched in a container? What host paths are expected to be mounted inside? This will help identify how to fix this. \n. @vishh with a -v /sys:/sys:ro ?\n. @vishh I went through the code and I think it will be easier if we discuss this over a call. What you really want is the later mount point from mountinfo but that isn't always the correct assumption that could be made in libcontainer especially when nesting containers (like docker-in-docker). \n. I propose that we enhance the libcontainer API to allow getting all the mounts in order. Then it is up to the caller to use them in whatever way they see fit. See https://github.com/opencontainers/runc/commit/5e91f1cd1b19b091c129d4fb937d5cb54a9b1db0 cadvisor could just pick the later entries and it should all work fine. \n. @derekwaynecarr Sounds good. Let me know if you run into any issues.\n. @derekwaynecarr runc PR is merged now so you can pick it up.\n. @derekwaynecarr Yeah, we are returning the path to container rootfs. So, we need to add \"diff\" here to do the equivalent for cri-o - https://github.com/google/cadvisor/blob/master/container/crio/handler.go#L144. If we want to get to the upper layer of the container in crio, it is as I pointed out earlier https://github.com/google/cadvisor/blob/master/container/crio/handler.go#L144 + /diff.\ne.g. /var/lib/containers/storage/overlay/1462529fa84a113ec84ef5406c6db96812d792e632c66555e1032404c2431486/diff\n. :+1:. ",
    "jthornber": "I'm working on a thin_ls tool for you that should be released in the next couple of weeks.\n. Working on it atm\nOn Fri, 8 Jan 2016 20:48 Vish Kannan notifications@github.com wrote:\n\n@jthornber https://github.com/jthornber: Has your tool been released?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/959#issuecomment-170119999.\n. Yes, release 0.6.0.\n\nhttps://github.com/jthornber/thin-provisioning-tools/releases.  The tool you're after is called 'thin_ls'\n. I've no idea, they're very of of date.\nOn Wed, 3 Feb 2016 20:33 Vish Kannan notifications@github.com wrote:\n\n@jthornber https://github.com/jthornber: When will the alpine package\nfor thin provisioning tools\nhttps://pkgs.alpinelinux.org/package/testing/x86/thin-provisioning-tools\nbe updated to include your new tool?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/959#issuecomment-179448595.\n. To calculate the number of exclusive blocks thin ls has to walk all the\nmeta data. A very slow process, so taking a meta data snap is not going to\nadd much relatively.\n\nOn Mon, 7 Mar 2016 20:59 Vivek Goyal, notifications@github.com wrote:\n\n@sjenning https://github.com/sjenning right, currently you can not run\nthin_ls on live metadata. How did you conclude that reserving and releasing\nmetadata snapshot is heavyweight operation.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/959#issuecomment-193446280.\n. A millisecond?\n\nOn Mon, 7 Mar 2016 21:22 Andy Goldstein, notifications@github.com wrote:\n\n@jthornber https://github.com/jthornber how long does it take to create\nthe metadata snapshot so it can be used with thin_ls?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/959#issuecomment-193456857.\n. It will be available before the message completes.\n\nOn Mon, 7 Mar 2016 21:25 Andy Goldstein, notifications@github.com wrote:\n\n@jthornber https://github.com/jthornber oh, so it's super fast? Is it\navailable as soon as we finish calling dmsetup message or is it possible\nthat there's a bit of a delay? I'm trying to make sure we can avoid a race\ncondition where we send the reserve_metadata_snap message and then try to\nuse thin_ls before the snapshot is ready.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/959#issuecomment-193458087.\n. \n",
    "sjenning": "device-mapper-persistent-data is the name of the package in Fedora.  Currently v0.5.5. for F23, but following upstream close for F24.\n. I can verify that thin_ls is a viable way to do this:\n[root@cadvisor devicemapper]# docker run -d busybox sleep 1000\nfdd91d353daf764c9a9868faeca21a3d5baa12b928e5d16c9cda939d18d72ea5\n[root@cadvisor devicemapper]# docker exec -ti fdd91d353daf764c9a9868faeca21a3d5baa12b928e5d16c9cda939d18d72ea5 /bin/sh\n/ # dd if=/dev/zero of=/zeroes bs=1M count=10\n10+0 records in\n10+0 records out\n10485760 bytes (10.0MB) copied, 0.016788 seconds, 595.7MB/s\n/ # exit\n[root@cadvisor devicemapper]# docker inspect fdd91d353daf764c9a9868faeca21a3d5baa12b928e5d16c9cda939d18d72ea5\n...\n\"GraphDriver\": {\n        \"Name\": \"devicemapper\",\n        \"Data\": {\n            \"DeviceId\": \"768\",\n...\n[root@cadvisor devicemapper]# thin_ls -o DEV,EXCLUSIVE metadata | grep 768\n768     11MiB <-- this is the stat we are looking for\nRHEL is at v0.5.5 for device-mapper-persistent-data so it needs an update to at least v0.6.0.\nNot sure about Alpine.\n. I can verify that thin_ls is a viable way to do this:\n[root@cadvisor devicemapper]# docker run -d busybox sleep 1000\nfdd91d353daf764c9a9868faeca21a3d5baa12b928e5d16c9cda939d18d72ea5\n[root@cadvisor devicemapper]# docker exec -ti fdd91d353daf764c9a9868faeca21a3d5baa12b928e5d16c9cda939d18d72ea5 /bin/sh\n/ # dd if=/dev/zero of=/zeroes bs=1M count=10\n10+0 records in\n10+0 records out\n10485760 bytes (10.0MB) copied, 0.016788 seconds, 595.7MB/s\n/ # exit\n[root@cadvisor devicemapper]# docker inspect fdd91d353daf764c9a9868faeca21a3d5baa12b928e5d16c9cda939d18d72ea5\n...\n\"GraphDriver\": {\n        \"Name\": \"devicemapper\",\n        \"Data\": {\n            \"DeviceId\": \"768\",\n...\n[root@cadvisor devicemapper]# thin_ls -o DEV,EXCLUSIVE metadata | grep 768\n768     11MiB <-- this is the stat we are looking for\nRHEL is at v0.5.5 for device-mapper-persistent-data so it needs an update to at least v0.6.0.\nNot sure about Alpine.\n. This is a little more tricky than I first thought.\nthin_ls does not work against live metadata.  The metadata needs to be snapshotted so it can be examined if it is live.\ndmsetup message /dev/mapper/fedora-docker--pool 0 \"reserve_metadata_snap\"\nThen thin_ls will work.  After thin_ls is run, the snapshot needs to be released\ndmsetup message /dev/mapper/fedora-docker--pool 0 \"release_metadata_snap\"\nThis is a pretty heavyweight operation to do each time cadvisor queries the container size.\n. This is a little more tricky than I first thought.\nthin_ls does not work against live metadata.  The metadata needs to be snapshotted so it can be examined if it is live.\ndmsetup message /dev/mapper/fedora-docker--pool 0 \"reserve_metadata_snap\"\nThen thin_ls will work.  After thin_ls is run, the snapshot needs to be released\ndmsetup message /dev/mapper/fedora-docker--pool 0 \"release_metadata_snap\"\nThis is a pretty heavyweight operation to do each time cadvisor queries the container size.\n. @rhvgoyal true, it doesn't seem to be heavyweight; just multi-step.  That is what I should have said.\n. @rhvgoyal true, it doesn't seem to be heavyweight; just multi-step.  That is what I should have said.\n. Patch to update alpine thin-provisioning-tools package:\nhttp://lists.alpinelinux.org/alpine-devel/5211.html\nWIP for per-container dm thin stats:\nhttps://github.com/sjenning/cadvisor/commit/cb00c4edc46974147fed9ff84b0330da062bc256\n. Patch to update alpine thin-provisioning-tools package:\nhttp://lists.alpinelinux.org/alpine-devel/5211.html\nWIP for per-container dm thin stats:\nhttps://github.com/sjenning/cadvisor/commit/cb00c4edc46974147fed9ff84b0330da062bc256\n. Alpine Linux patch for thin-provisioning-tools package got pushed upstream\nhttps://github.com/alpinelinux/aports/commit/1d40a0754c1eb42b01b8f87938e30723e70f554c\nNot sure when the next rebuild is; monitoring:\nhttps://pkgs.alpinelinux.org/packages?name=thin-provisioning-tools\n. Alpine Linux patch for thin-provisioning-tools package got pushed upstream\nhttps://github.com/alpinelinux/aports/commit/1d40a0754c1eb42b01b8f87938e30723e70f554c\nNot sure when the next rebuild is; monitoring:\nhttps://pkgs.alpinelinux.org/packages?name=thin-provisioning-tools\n. After hacking the test code to prevent removal of the containers after use:\n```\nbuild/integration.sh\n\n\nstarting cAdvisor locally\nrunning integration tests against local cAdvisor\n--- FAIL: TestAttributeInformationIsReturned (0.00s)\n    attributes_test.go:31: unable to post \"attributes\" to \"http://localhost:8080/api/v2.1/attributes\": Get http://localhost:8080/api/v2.1/attributes: dial tcp 127.0.0.1:8080: connection refused\n--- FAIL: TestDockerFilesystemStats (0.91s)\n        Location:       docker_test.go:336\n    Error:      Expected not to be nil.\n    Messages:   got info: {Spec:{CreationTime:2016-02-26 22:54:37.618844915 +0000 UTC Aliases:[distracted_mayer 9e09afa35ffd33a79a51cd4f235f04f65345b5b14f38e889c463a9d384356792] Namespace:docker Labels:map[] Envs:map[] HasCpu:true Cpu:{Limit:1024 MaxLimit:0 Mask:0-3 Quota:0 Period:0} HasMemory:true Memory:{Limit:18446744073709551615 Reservation:0 SwapLimit:18446744073709551615} HasCustomMetrics:false CustomMetrics:[] HasNetwork:true HasFilesystem:false HasDiskIo:true Image:busybox} Stats:[0xc20804f3e0]}\n\n\nFAIL\nFAIL    github.com/google/cadvisor/integration/tests/api    47.968s\n--- FAIL: TestHealthzOk (0.00s)\n    healthz_test.go:32: Get http://localhost:8080/healthz: dial tcp 127.0.0.1:8080: connection refused\nFAIL\nFAIL    github.com/google/cadvisor/integration/tests/healthz    0.012s\ngodep: go exit status 1\nIntegration tests failed\n\n\nstopping cAdvisor\n\n\ndocker logs distracted_mayer\n/bin/sh: dd if=/dev/zero of=/file count=2 bs=8 & sleep 10000: not found\n```\nIt appears to be treating the entire string as a single command.  Fixed with this:\nindex 8d30edf..52f563b 100644\n--- a/integration/tests/api/docker_test.go\n+++ b/integration/tests/api/docker_test.go\n@@ -300,7 +300,7 @@ func TestDockerFilesystemStats(t *testing.T) {\n                sleepDuration = 10 * time.Second\n        )\n        // Wait for the container to show up.\n-       containerId := fm.Docker().RunBusybox(\"/bin/sh\", \"-c\", fmt.Sprintf(\"'dd if=/dev/zero of=/file count=2 bs=%d & sleep 1\n+       containerId := fm.Docker().RunBusybox(\"/bin/sh\", \"-c\", fmt.Sprintf(\"\\\"dd if=/dev/zero of=/file count=2 bs=%d & sleep \n        waitForContainer(containerId, fm)\n        request := &v2.RequestOptions{\n                IdType: v2.TypeDocker,\nbut then fell through to a bunch more errors\n```\nbuild/integration.sh\n\n\nstarting cAdvisor locally\nrunning integration tests against local cAdvisor\n--- FAIL: TestAttributeInformationIsReturned (0.00s)\n    attributes_test.go:31: unable to post \"attributes\" to \"http://localhost:8080/api/v2.1/attributes\": Get http://localhost:8080/api/v2.1/attributes: dial tcp 127.0.0.1:8080: connection refused\n--- FAIL: TestDockerFilesystemStats (100.94s)\n    docker_test.go:320: stats unavailable - request \"http://localhost:8080/api/v2.1/stats/e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a?count=1&recursive=false&type=docker\" failed with error: \"unable to find Docker container \\\"e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a\\\"\"\n    docker_test.go:321: retrying after 10s...\n    docker_test.go:320: stats unavailable - request \"http://localhost:8080/api/v2.1/stats/e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a?count=1&recursive=false&type=docker\" failed with error: \"unable to find Docker container \\\"e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a\\\"\"\n    docker_test.go:321: retrying after 10s...\n    docker_test.go:320: stats unavailable - request \"http://localhost:8080/api/v2.1/stats/e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a?count=1&recursive=false&type=docker\" failed with error: \"unable to find Docker container \\\"e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a\\\"\"\n    docker_test.go:321: retrying after 10s...\n    docker_test.go:320: stats unavailable - request \"http://localhost:8080/api/v2.1/stats/e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a?count=1&recursive=false&type=docker\" failed with error: \"unable to find Docker container \\\"e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a\\\"\"\n    docker_test.go:321: retrying after 10s...\n    docker_test.go:320: stats unavailable - request \"http://localhost:8080/api/v2.1/stats/e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a?count=1&recursive=false&type=docker\" failed with error: \"unable to find Docker container \\\"e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a\\\"\"\n    docker_test.go:321: retrying after 10s...\n    docker_test.go:320: stats unavailable - request \"http://localhost:8080/api/v2.1/stats/e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a?count=1&recursive=false&type=docker\" failed with error: \"unable to find Docker container \\\"e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a\\\"\"\n    docker_test.go:321: retrying after 10s...\n    docker_test.go:320: stats unavailable - request \"http://localhost:8080/api/v2.1/stats/e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a?count=1&recursive=false&type=docker\" failed with error: \"unable to find Docker container \\\"e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a\\\"\"\n    docker_test.go:321: retrying after 10s...\n    docker_test.go:320: stats unavailable - request \"http://localhost:8080/api/v2.1/stats/e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a?count=1&recursive=false&type=docker\" failed with error: \"unable to find Docker container \\\"e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a\\\"\"\n    docker_test.go:321: retrying after 10s...\n    docker_test.go:320: stats unavailable - request \"http://localhost:8080/api/v2.1/stats/e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a?count=1&recursive=false&type=docker\" failed with error: \"unable to find Docker container \\\"e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a\\\"\"\n    docker_test.go:321: retrying after 10s...\n    docker_test.go:320: stats unavailable - request \"http://localhost:8080/api/v2.1/stats/e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a?count=1&recursive=false&type=docker\" failed with error: \"unable to find Docker container \\\"e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a\\\"\"\n    docker_test.go:321: retrying after 10s...\nFAIL\nFAIL    github.com/google/cadvisor/integration/tests/api    148.282s\n--- FAIL: TestHealthzOk (0.00s)\n    healthz_test.go:32: Get http://localhost:8080/healthz: dial tcp 127.0.0.1:8080: connection refused\nFAIL\nFAIL    github.com/google/cadvisor/integration/tests/healthz    0.005s\ngodep: go exit status 1\nIntegration tests failed\nstopping cAdvisor\n```\n\n\nStill looking...\nAlso cadvisor doesn't build on RHEL7 without some changes to build/build.sh\n. After hacking the test code to prevent removal of the containers after use:\n```\nbuild/integration.sh\n\n\nstarting cAdvisor locally\nrunning integration tests against local cAdvisor\n--- FAIL: TestAttributeInformationIsReturned (0.00s)\n    attributes_test.go:31: unable to post \"attributes\" to \"http://localhost:8080/api/v2.1/attributes\": Get http://localhost:8080/api/v2.1/attributes: dial tcp 127.0.0.1:8080: connection refused\n--- FAIL: TestDockerFilesystemStats (0.91s)\n        Location:       docker_test.go:336\n    Error:      Expected not to be nil.\n    Messages:   got info: {Spec:{CreationTime:2016-02-26 22:54:37.618844915 +0000 UTC Aliases:[distracted_mayer 9e09afa35ffd33a79a51cd4f235f04f65345b5b14f38e889c463a9d384356792] Namespace:docker Labels:map[] Envs:map[] HasCpu:true Cpu:{Limit:1024 MaxLimit:0 Mask:0-3 Quota:0 Period:0} HasMemory:true Memory:{Limit:18446744073709551615 Reservation:0 SwapLimit:18446744073709551615} HasCustomMetrics:false CustomMetrics:[] HasNetwork:true HasFilesystem:false HasDiskIo:true Image:busybox} Stats:[0xc20804f3e0]}\n\n\nFAIL\nFAIL    github.com/google/cadvisor/integration/tests/api    47.968s\n--- FAIL: TestHealthzOk (0.00s)\n    healthz_test.go:32: Get http://localhost:8080/healthz: dial tcp 127.0.0.1:8080: connection refused\nFAIL\nFAIL    github.com/google/cadvisor/integration/tests/healthz    0.012s\ngodep: go exit status 1\nIntegration tests failed\n\n\nstopping cAdvisor\n\n\ndocker logs distracted_mayer\n/bin/sh: dd if=/dev/zero of=/file count=2 bs=8 & sleep 10000: not found\n```\nIt appears to be treating the entire string as a single command.  Fixed with this:\nindex 8d30edf..52f563b 100644\n--- a/integration/tests/api/docker_test.go\n+++ b/integration/tests/api/docker_test.go\n@@ -300,7 +300,7 @@ func TestDockerFilesystemStats(t *testing.T) {\n                sleepDuration = 10 * time.Second\n        )\n        // Wait for the container to show up.\n-       containerId := fm.Docker().RunBusybox(\"/bin/sh\", \"-c\", fmt.Sprintf(\"'dd if=/dev/zero of=/file count=2 bs=%d & sleep 1\n+       containerId := fm.Docker().RunBusybox(\"/bin/sh\", \"-c\", fmt.Sprintf(\"\\\"dd if=/dev/zero of=/file count=2 bs=%d & sleep \n        waitForContainer(containerId, fm)\n        request := &v2.RequestOptions{\n                IdType: v2.TypeDocker,\nbut then fell through to a bunch more errors\n```\nbuild/integration.sh\n\n\nstarting cAdvisor locally\nrunning integration tests against local cAdvisor\n--- FAIL: TestAttributeInformationIsReturned (0.00s)\n    attributes_test.go:31: unable to post \"attributes\" to \"http://localhost:8080/api/v2.1/attributes\": Get http://localhost:8080/api/v2.1/attributes: dial tcp 127.0.0.1:8080: connection refused\n--- FAIL: TestDockerFilesystemStats (100.94s)\n    docker_test.go:320: stats unavailable - request \"http://localhost:8080/api/v2.1/stats/e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a?count=1&recursive=false&type=docker\" failed with error: \"unable to find Docker container \\\"e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a\\\"\"\n    docker_test.go:321: retrying after 10s...\n    docker_test.go:320: stats unavailable - request \"http://localhost:8080/api/v2.1/stats/e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a?count=1&recursive=false&type=docker\" failed with error: \"unable to find Docker container \\\"e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a\\\"\"\n    docker_test.go:321: retrying after 10s...\n    docker_test.go:320: stats unavailable - request \"http://localhost:8080/api/v2.1/stats/e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a?count=1&recursive=false&type=docker\" failed with error: \"unable to find Docker container \\\"e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a\\\"\"\n    docker_test.go:321: retrying after 10s...\n    docker_test.go:320: stats unavailable - request \"http://localhost:8080/api/v2.1/stats/e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a?count=1&recursive=false&type=docker\" failed with error: \"unable to find Docker container \\\"e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a\\\"\"\n    docker_test.go:321: retrying after 10s...\n    docker_test.go:320: stats unavailable - request \"http://localhost:8080/api/v2.1/stats/e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a?count=1&recursive=false&type=docker\" failed with error: \"unable to find Docker container \\\"e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a\\\"\"\n    docker_test.go:321: retrying after 10s...\n    docker_test.go:320: stats unavailable - request \"http://localhost:8080/api/v2.1/stats/e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a?count=1&recursive=false&type=docker\" failed with error: \"unable to find Docker container \\\"e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a\\\"\"\n    docker_test.go:321: retrying after 10s...\n    docker_test.go:320: stats unavailable - request \"http://localhost:8080/api/v2.1/stats/e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a?count=1&recursive=false&type=docker\" failed with error: \"unable to find Docker container \\\"e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a\\\"\"\n    docker_test.go:321: retrying after 10s...\n    docker_test.go:320: stats unavailable - request \"http://localhost:8080/api/v2.1/stats/e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a?count=1&recursive=false&type=docker\" failed with error: \"unable to find Docker container \\\"e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a\\\"\"\n    docker_test.go:321: retrying after 10s...\n    docker_test.go:320: stats unavailable - request \"http://localhost:8080/api/v2.1/stats/e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a?count=1&recursive=false&type=docker\" failed with error: \"unable to find Docker container \\\"e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a\\\"\"\n    docker_test.go:321: retrying after 10s...\n    docker_test.go:320: stats unavailable - request \"http://localhost:8080/api/v2.1/stats/e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a?count=1&recursive=false&type=docker\" failed with error: \"unable to find Docker container \\\"e6183f199554d7f044d0270bc382a175d87920746d2efe100ace057a1042275a\\\"\"\n    docker_test.go:321: retrying after 10s...\nFAIL\nFAIL    github.com/google/cadvisor/integration/tests/api    148.282s\n--- FAIL: TestHealthzOk (0.00s)\n    healthz_test.go:32: Get http://localhost:8080/healthz: dial tcp 127.0.0.1:8080: connection refused\nFAIL\nFAIL    github.com/google/cadvisor/integration/tests/healthz    0.005s\ngodep: go exit status 1\nIntegration tests failed\nstopping cAdvisor\n```\n\n\nStill looking...\nAlso cadvisor doesn't build on RHEL7 without some changes to build/build.sh\n. There are a number of issues on RHEL7 so I'll just take them one by one, opening issues and PRs for each.\n. There are a number of issues on RHEL7 so I'll just take them one by one, opening issues and PRs for each.\n. #1136 fixes the \"du: not found\" issue caused by early termination of the container due to bad start command syntax\n1135 is the remaining issue caused by lack of filesystem stats support for containers using devicemapper\n. #1136 fixes the \"du: not found\" issue caused by early termination of the container due to bad start command syntax\n1135 is the remaining issue caused by lack of filesystem stats support for containers using devicemapper\n. @vishh I assume the test case that is failing is the TestDockerFilesystemStats and would be resolved by #1143?\n. This is \"fixed\" by #1143.  I'll open a new issue to track re-enabling this test for RHEL.\n. gah, wrong email in the commit thus cla issues\n. This is related to no devicemapper support for filesystem stats.  Also see #959, #1094.  This test should not be run if the storage driver is devicemapper until such time as the filesystem stat code supports it.\n. @vishh My understanding was that the sleep was there to keep the container alive while the fs stats were gathered, then the container is killed in the test cleanup.  That not the case?\n. @vishh ah nevermind, i see now.  the docker run in not daemonized (-d) or removed (--rm) by docker and the test does proceed until the container returns (stops).  the waitForContainer() made me think that the docker run was asynchronous/daemonized.\nin that case, i'm not sure what the original purpose of the sleep was.  or why it isn't already a problem.\n. @vishh your example works on RHEL from the docker CLI client.  However, I think what the code is producing is something that operates like\n```\ndocker run busybox \"/bin/sh\" \"-c\" \"'dd if=/dev/zero of=/dev/null bs=1M count=10 & sleep 100'\"\n/bin/sh: dd if=/dev/zero of=/dev/null bs=1M count=10 & sleep 100: not found\n```\nwhere removing the single quotes around the internal shell command fixes it:\n```\ndocker run busybox \"/bin/sh\" \"-c\" \"dd if=/dev/zero of=/dev/null bs=1M count=10 & sleep 100\"\n10+0 records in\n10+0 records out\n10485760 bytes (10.0MB) copied, 0.001705 seconds, 5.7GB/s\n``\n. @vishh my comment starting with \"ah nevermind, i see now\" is all wrong!  The test container _is_ daemonzied with -d added to the command infunc (self dockerActions) Run().  So I'm back to not understanding how the sleep hangs the test.  My understanding is that the deferredfm.Cleanup()will mercilessly kill the container withdocker rm -f` at the end of the test.\n. Also worth noting that I don't observe a hang locally on RHEL or Fedora.\nFor reference from the e2e failure:\n```\nError 0: error on host e2e-cadvisor-container-vm-v20151215-docker18: command \"godep\" [\"go\" \"test\" \"--timeout\" \"15m0s\" \"\" \"github.com/google/cadvisor/integration/tests/...\" \"--host\" \"e2e-cadvisor-container-vm-v20151215-docker18\" \"--port\" \"8080\" \"--ssh-options\" \"-i /var/lib/jenkins/.ssh/google_compute_engine -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o CheckHostIP=no -o StrictHostKeyChecking=no\"] failed with error: exit status 1 and output: \"?       github.com/google/cadvisor  [no test files]\npanic: test timed out after 15m0s\ngoroutine 62 [running]:\ntesting.func\u00c2\u00b7008()\n    /usr/local/go/src/testing/testing.go:681 +0x12f\ncreated by time.goFunc\n    /usr/local/go/src/time/sleep.go:129 +0x4b\ngoroutine 1 [chan receive, 14 minutes]:\ntesting.RunTests(0x827288, 0x931ce0, 0xd, 0xd, 0xc20802ab01)\n    /usr/local/go/src/testing/testing.go:556 +0xad6\ntesting.(*M).Run(0xc20802c140, 0x93ca60)\n    /usr/local/go/src/testing/testing.go:485 +0x6c\nmain.main()\n    github.com/google/cadvisor/integration/tests/api/_test/_testmain.go:76 +0x1d5\ngoroutine 5 [runnable]:\ngithub.com/golang/glog.(*loggingT).flushDaemon(0x934dc0)\n    /jenkins-master-data/jobs/cadvisor-pull-build-test-e2e/workspace/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:882 +0x78\ncreated by github.com/golang/glog.init\u00c2\u00b71\n    /jenkins-master-data/jobs/cadvisor-pull-build-test-e2e/workspace/go/src/github.com/google/cadvisor/Godeps/_workspace/src/github.com/golang/glog/glog.go:410 +0x2a7\ngoroutine 17 [syscall, 14 minutes, locked to thread]:\nruntime.goexit()\n    /usr/local/go/src/runtime/asm_amd64.s:2232 +0x1\ngoroutine 59 [syscall, 14 minutes]:\nsyscall.Syscall6(0x3d, 0xf59, 0xc20809d284, 0x0, 0xc20806d290, 0x0, 0x0, 0xc208053a40, 0x402b01, 0xc208053a60)\n    /usr/local/go/src/syscall/asm_linux_amd64.s:46 +0x5\nsyscall.wait4(0xf59, 0xc20809d284, 0x0, 0xc20806d290, 0x90, 0x0, 0x0)\n    /usr/local/go/src/syscall/zsyscall_linux_amd64.go:124 +0x79\nsyscall.Wait4(0xf59, 0xc20809d2cc, 0x0, 0xc20806d290, 0x0, 0x0, 0x0)\n    /usr/local/go/src/syscall/syscall_linux.go:224 +0x60\nos.(Process).wait(0xc20801edc0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/os/exec_unix.go:22 +0x103\nos.(Process).Wait(0xc20801edc0, 0xc2080cb340, 0x0, 0x0)\n    /usr/local/go/src/os/doc.go:45 +0x3a\nos/exec.(Cmd).Wait(0xc208095a40, 0x0, 0x0)\n    /usr/local/go/src/os/exec/exec.go:364 +0x23c\nos/exec.(Cmd).Run(0xc208095a40, 0x0, 0x0)\n    /usr/local/go/src/os/exec/exec.go:246 +0x71\ngithub.com/google/cadvisor/integration/framework.shellActions.Run(0xc208053860, 0x7880d0, 0x4, 0xc208095900, 0x14, 0x14, 0x0, 0x0, 0x0, 0x0)\n    /var/lib/jenkins/jobs/cadvisor-pull-build-test-e2e/workspace/go/src/github.com/google/cadvisor/integration/framework/framework.go:329 +0x23b\ngithub.com/google/cadvisor/integration/framework.dockerActions.Run(0xc208053860, 0x7819b0, 0x7, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xc2080c9920, ...)\n    /var/lib/jenkins/jobs/cadvisor-pull-build-test-e2e/workspace/go/src/github.com/google/cadvisor/integration/framework/framework.go:244 +0x4b6\ngithub.com/google/cadvisor/integration/framework.dockerActions.RunBusybox(0xc208053860, 0xc2080c9920, 0x3, 0x3, 0x0, 0x0)\n    /var/lib/jenkins/jobs/cadvisor-pull-build-test-e2e/workspace/go/src/github.com/google/cadvisor/integration/framework/framework.go:221 +0xa6\ngithub.com/google/cadvisor/integration/tests/api.TestDockerFilesystemStats(0xc20806d0e0)\n    /var/lib/jenkins/jobs/cadvisor-pull-build-test-e2e/workspace/go/src/github.com/google/cadvisor/integration/tests/api/docker_test.go:303 +0x273\ntesting.tRunner(0xc20806d0e0, 0x931db8)\n    /usr/local/go/src/testing/testing.go:447 +0xbf\ncreated by testing.RunTests\n    /usr/local/go/src/testing/testing.go:555 +0xa8b\n...\n```\nIt would seem that the docker run -d busybox ... command is not returning, which it should since -d is passed.  Unless docker is hung or something.\n. @vishh it seems that with the single quotes, running the e2e works but running the integration tests locally with \"make integration\" does not.  This correct? And if you remove the single quotes, it runs locally but the e2e fails?\n. oops, didn't mean to close. update pushed to just skip the TestDockerFilesystemStats if devicemapper is the storage driver.\n. The mechanism for collecting these stats for DM was disabled do to excessive disk IOPs https://github.com/google/cadvisor/pull/1588. @timstclair updated\n. @vishh @timothysc updated\n. @timstclair undetected name collision, my bad :) updated\n. While I'm looking to add log rotation support (#1248) to the path that watches log files, preferring systemd bypasses the issue entirely by reading from journalctl if it is available.  Some systems will have the log files AND systemd and in that case, we want to read from the journal since it hides the log rotation issue.\n. @timstclair updated\n. @timstclair jenkins failure looks like a jenkins issue\n. @timstclair i'm looking into using inotify.  might be less of a hassle than fsnotify was.\n. @timstclair @ncdc updated with inotify instead of polling\n. @timstclair new update that i think captures all the previous comments. PTAL when you can.\n. @timstclair i would need to figure out how to incorperate your feedback.  some of the things you point out, i don't know how to deal with cleanly.\nre: hpcloud/tail it brought in 10k lines of vendor code and the fsnotify implementation is broken:\nhttps://github.com/hpcloud/tail/issues/21\n. I'll fix it up asap. Update within the hour.\nOn May 17, 2016 7:58 PM, \"Tim St. Clair\" notifications@github.com wrote:\n\nFYI, we're hoping to cut a v0.23.2 release tomorrow (2016-05-18) for\nkubernetes 1.3, and I'd like to get this change in. Please ping me if\nthere's anything I can help with to move this along.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1264#issuecomment-219897465\n. @timstclair I'm close to having this done.  Another half hour.\n. @timstclair i've pushed an update.  i reworked watchFile, but now the mutex handling is nasty in there.  recommendations welcome.\n. @timstclair ok, i incorporated your comments and introduced a readerState so that the locking can be cleaner.  the real purpose is to avoid the race between a reader and the log file opening.\n. @timstclair thanks for cleaning up\n. wfm and lgtm, thanks\n. cc @ncdc \n\nThis is related to the thin pool device being DM managed, in which case docker info shows the correct information, or LVM managed, in which case it doesn't.  Haven't looked into the docker DM graphdriver enough to figure out why.\nIn the meantime, I have a tentative fix\nhttps://github.com/sjenning/cadvisor/commit/7702a5e9d9b558ec892917e6a850e613ad63f1b1\nI'm just not sure how solid the assumption is building the dev path from the pool name.\n. @ncdc He kept saying we should use cli tools to determine the metadata device, which is a mess here (cli tools not installed, capturing/parsing output, etc)\n. @aveshagarwal good call :) PR inbound...\n. @aveshagarwal good call :) PR inbound...\n. Just added some new changes\n- os.Stat to check to metadata file existence when using assumed LVM path\n- only fail to gather filesystem stats if metadata device can't be found (other stats still work)\n. Just added some new changes\n- os.Stat to check to metadata file existence when using assumed LVM path\n- only fail to gather filesystem stats if metadata device can't be found (other stats still work)\n. lgtm\n. lgtm\n. @timothysc it is hard to figure out which path is allocating memory in the tail from the information provided.  Could you gather one profile at the end of the pod ramp up (the beginning of the tail) and then another 15-20 afterward.  I'm looking to do a comparison of the two heap profiles.\n. @timothysc it is hard to figure out which path is allocating memory in the tail from the information provided.  Could you gather one profile at the end of the pod ramp up (the beginning of the tail) and then another 15-20 afterward.  I'm looking to do a comparison of the two heap profiles.\n. @mffiedler can I get the heap profile files from you?\n. @mffiedler can I get the heap profile files from you?\n. Thanks, I'll check it out this morning.\nOn Jul 13, 2016 9:35 PM, \"Mike Fiedler\" notifications@github.com wrote:\n\n@sjenning https://github.com/sjenning, ran one more time - identical\ntest....heap profiles attached.\npprof.localhost_10248.inuse_objects.inuse_space.001.pb.gz\nhttps://github.com/google/cadvisor/files/362906/pprof.localhost_10248.inuse_objects.inuse_space.001.pb.gz\npprof.localhost_10248.inuse_objects.inuse_space.002.pb.gz\nhttps://github.com/google/cadvisor/files/362907/pprof.localhost_10248.inuse_objects.inuse_space.002.pb.gz\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1371#issuecomment-232541311,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AAeJQ0Fg1M7_tQHpCJ0ntxqynaGr_ldLks5qVaCEgaJpZM4JJv9F\n.\n. Thanks, I'll check it out this morning.\n\nOn Jul 13, 2016 9:35 PM, \"Mike Fiedler\" notifications@github.com wrote:\n\n@sjenning https://github.com/sjenning, ran one more time - identical\ntest....heap profiles attached.\npprof.localhost_10248.inuse_objects.inuse_space.001.pb.gz\nhttps://github.com/google/cadvisor/files/362906/pprof.localhost_10248.inuse_objects.inuse_space.001.pb.gz\npprof.localhost_10248.inuse_objects.inuse_space.002.pb.gz\nhttps://github.com/google/cadvisor/files/362907/pprof.localhost_10248.inuse_objects.inuse_space.002.pb.gz\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1371#issuecomment-232541311,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AAeJQ0Fg1M7_tQHpCJ0ntxqynaGr_ldLks5qVaCEgaJpZM4JJv9F\n.\n. Let these ungodly machines do all the math!\n\nhttps://paste.fedoraproject.org/391254/53395514/\nOf note:\nflat  flat%   sum%        cum   cum%\n    7.50MB 65.06% 65.06%     7.50MB 65.06%  k8s.io/kubernetes/vendor/github.com/opencontainers/runc/libcontainer/cgroups/fs.getBlkioStat\n    4.50MB 39.06%   100%     4.01MB 34.83%  k8s.io/kubernetes/vendor/github.com/google/cadvisor/container/libcontainer.toContainerStats\n    1.51MB 13.12%   100%     1.51MB 13.12%  k8s.io/kubernetes/vendor/github.com/google/cadvisor/summary.(*StatsSummary).AddSample\n    1.51MB 13.12%   100%     1.51MB 13.12%  k8s.io/kubernetes/vendor/github.com/google/cadvisor/container/libcontainer.DiskStatsCopy1\n         0     0%   100%        5MB 43.39%  k8s.io/kubernetes/vendor/github.com/google/cadvisor/container/docker.(*dockerContainerHandler).GetStats\nThat narrows it down.  I'm not sure if this is a leak or just cadvisor keeping the stat history around for a while.  Still looking.\n. Let these ungodly machines do all the math!\nhttps://paste.fedoraproject.org/391254/53395514/\nOf note:\nflat  flat%   sum%        cum   cum%\n    7.50MB 65.06% 65.06%     7.50MB 65.06%  k8s.io/kubernetes/vendor/github.com/opencontainers/runc/libcontainer/cgroups/fs.getBlkioStat\n    4.50MB 39.06%   100%     4.01MB 34.83%  k8s.io/kubernetes/vendor/github.com/google/cadvisor/container/libcontainer.toContainerStats\n    1.51MB 13.12%   100%     1.51MB 13.12%  k8s.io/kubernetes/vendor/github.com/google/cadvisor/summary.(*StatsSummary).AddSample\n    1.51MB 13.12%   100%     1.51MB 13.12%  k8s.io/kubernetes/vendor/github.com/google/cadvisor/container/libcontainer.DiskStatsCopy1\n         0     0%   100%        5MB 43.39%  k8s.io/kubernetes/vendor/github.com/google/cadvisor/container/docker.(*dockerContainerHandler).GetStats\nThat narrows it down.  I'm not sure if this is a leak or just cadvisor keeping the stat history around for a while.  Still looking.\n. https://github.com/google/cadvisor/blob/master/summary/summary.go#L49\nThere are 1440 (24*60) per-minute samples maintained for 250 pods so I would expect the memory load to grow steadily for a day.\nI personally don't think this is a slow leak, just the stats history building up.\n. https://github.com/google/cadvisor/blob/master/summary/summary.go#L49\nThere are 1440 (24*60) per-minute samples maintained for 250 pods so I would expect the memory load to grow steadily for a day.\nI personally don't think this is a slow leak, just the stats history building up.\n. Well nevermind! I tried this on Fedora but RHEL doesn't have thin_ls yet.  device-mapper-persistent-data update is scheduled to be pushed out to the repos for 7.2 on Aug 2.  I'll reopen then.\n. Well nevermind! I tried this on Fedora but RHEL doesn't have thin_ls yet.  device-mapper-persistent-data update is scheduled to be pushed out to the repos for 7.2 on Aug 2.  I'll reopen then.\n. @dashpole no.  DM per-container fs stats ended up being a YUGE mess, mostly due to the implementation details of the tool we were using to get them, thin_ls.  tl;dr thin_ls would max out IOPs on many cloud instances if the DM metadata got too big. It was disabled in cadvisor PR  https://github.com/google/cadvisor/pull/1588\nThis is dead for the foreseeable future. :disappointed: . @dashpole no.  DM per-container fs stats ended up being a YUGE mess, mostly due to the implementation details of the tool we were using to get them, thin_ls.  tl;dr thin_ls would max out IOPs on many cloud instances if the DM metadata got too big. It was disabled in cadvisor PR  https://github.com/google/cadvisor/pull/1588\nThis is dead for the foreseeable future. :disappointed: . @timstclair thanks! can we get a new v0.23 tag with this?\n. @timstclair thanks! can we get a new v0.23 tag with this?\n. decided to disable thin_ls instead https://github.com/google/cadvisor/pull/1588. decided to disable thin_ls instead https://github.com/google/cadvisor/pull/1588. @ncdc yes. @ncdc yes. cc @jsafrane. cc @jsafrane. @dashpole so it hit the new dependency on systemd-devel I mentioned in the PR description.\nI0801 19:30:09.573] >> building cadvisor\nW0801 19:30:11.299] # github.com/google/cadvisor/vendor/github.com/coreos/go-systemd/sdjournal\nW0801 19:30:11.300] vendor/github.com/coreos/go-systemd/sdjournal/journal.go:27:33: fatal error: systemd/sd-journal.h: No such file or directory\nW0801 19:30:11.300]  // #include <systemd/sd-journal.h>\nW0801 19:30:11.300]                                  ^\nCan you add the distro appropriate package to the builder?. @dashpole so it hit the new dependency on systemd-devel I mentioned in the PR description.\nI0801 19:30:09.573] >> building cadvisor\nW0801 19:30:11.299] # github.com/google/cadvisor/vendor/github.com/coreos/go-systemd/sdjournal\nW0801 19:30:11.300] vendor/github.com/coreos/go-systemd/sdjournal/journal.go:27:33: fatal error: systemd/sd-journal.h: No such file or directory\nW0801 19:30:11.300]  // #include <systemd/sd-journal.h>\nW0801 19:30:11.300]                                  ^\nCan you add the distro appropriate package to the builder?. @euank at the time I submitted this, that PR hadn't moved since mid-March. I did note that this was an alternative to https://github.com/google/cadvisor/pull/1544\nOnly one need be accepted, but one does need to be accepted.. @euank at the time I submitted this, that PR hadn't moved since mid-March. I did note that this was an alternative to https://github.com/google/cadvisor/pull/1544\nOnly one need be accepted, but one does need to be accepted.. It looks like both of these approaches (sdjounal and reading /dev/kmsg) are used in NPD.\nhttps://github.com/kubernetes/node-problem-detector/blob/master/pkg/systemlogmonitor/logwatchers/kmsg/log_watcher.go\nhttps://github.com/kubernetes/node-problem-detector/blob/master/pkg/systemlogmonitor/logwatchers/journald/log_watcher.go\n@dchen1107 just mentioned this as a way during a sig-node meeting, so I wrote it up.. It looks like both of these approaches (sdjounal and reading /dev/kmsg) are used in NPD.\nhttps://github.com/kubernetes/node-problem-detector/blob/master/pkg/systemlogmonitor/logwatchers/kmsg/log_watcher.go\nhttps://github.com/kubernetes/node-problem-detector/blob/master/pkg/systemlogmonitor/logwatchers/journald/log_watcher.go\n@dchen1107 just mentioned this as a way during a sig-node meeting, so I wrote it up.. closed in favor of https://github.com/google/cadvisor/pull/1729. closed in favor of https://github.com/google/cadvisor/pull/1729. cc @tallclair. cc @tallclair. That \"Close and comment\" button should be red! :confounded: . That \"Close and comment\" button should be red! :confounded: . @dashpole @tallclair if I can get a review on this in short order,  I'll update my kube bump PR https://github.com/kubernetes/kubernetes/pull/50629 to include this change.. @dashpole @tallclair if I can get a review on this in short order,  I'll update my kube bump PR https://github.com/kubernetes/kubernetes/pull/50629 to include this change.. Looks similiar to https://github.com/google/cadvisor/issues/1481 again :frowning_face: Maybe my netgo flag PR didn't fix it.. Looks similiar to https://github.com/google/cadvisor/issues/1481 again :frowning_face: Maybe my netgo flag PR didn't fix it.. /retest. /retest. @ravisantoshgudimetla at first I did just decrease the log level.  But then someone running at the higher logging level would have the same complaint; that being continually seeing a message warning me about a persistent condition that I can't do anything about because someone decided what \"too long\" was.. @ravisantoshgudimetla at first I did just decrease the log level.  But then someone running at the higher logging level would have the same complaint; that being continually seeing a message warning me about a persistent condition that I can't do anything about because someone decided what \"too long\" was.. fyi @derekwaynecarr @RobertKrawitz\nThis breaks stats, and downstream imageGC in kube, when using devicemapper as the docker storage.\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1570577\nWe need to backport this into kube 1.10 and make sure we bump the dep in kube for 1.11 to include this.\n. fyi @derekwaynecarr @RobertKrawitz\nThis breaks stats, and downstream imageGC in kube, when using devicemapper as the docker storage.\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1570577\nWe need to backport this into kube 1.10 and make sure we bump the dep in kube for 1.11 to include this.\n. @dashpole Ah yes, godeps removed these, but I didn't want to change more than was needed and forgot to do a git add interactive on Godeps.json like I normally do.  I can add a commit to this that does the cleanup if you like?. @dashpole Ah yes, godeps removed these, but I didn't want to change more than was needed and forgot to do a git add interactive on Godeps.json like I normally do.  I can add a commit to this that does the cleanup if you like?. ah, i didn't do the ./... at the end.  when i do that, the godep save on master is a no-op :-/  oh godeps.... ah, i didn't do the ./... at the end.  when i do that, the godep save on master is a no-op :-/  oh godeps.... ok should be good now. ok should be good now. @dashpole are we looking to bump the kube dep in 1.11?  is there anything else you'd like to get in before a bump?. @dashpole are we looking to bump the kube dep in 1.11?  is there anything else you'd like to get in before a bump?. no rush, just wanted to make sure we bumped for 1.11 at some point. thanks!. no rush, just wanted to make sure we bumped for 1.11 at some point. thanks!. The ThinPoolWatcher is currently globally disabled.\nhttps://github.com/google/cadvisor/blob/f7576313bd1df261f90bca93ab91ba019eb823d5/container/docker/factory.go#L70\nThis was done because it turns out that thin_ls, the program that can get the thin LV usage, does an uncached read of the pool metadata on each run.  This causes a LOT of disk activity that negatively effects the performance of the node, especially in low IOPS limited environments like small and medium cloud instances.\nGetting per-container thin LV usage is currently not possible.. @lw8008 I recommend moving to overlay for container storage.  devicemapper container storage won't see any future development.. You are saying that we should only not gather stats for the rootfsStorageDir?\n. Nit: this will be the third instance of if self.name == \"/\" { check.  Maybe create a isRootCgroup() to help make this uniform and clear?\n. that is true. i figured it was bug before, sending oom events for things that may have happened days ago in the logs.\n. it breaks me out of the for loop starting line 90 which is what i want.  i've done tests so i know it works.  i'll have to think about how to unit test it.\n. in a hot loop?  while the normal case is that the file will be recreated in a short amount of time, it also might not be.  how long would we do this loop before timing out?  seems like it's just an idle wait vs a busy one.\n. @timstclair  The desired information is never in the start line so no problem.   Good you brought it up though.\n. i counted from one here since the message \"attempt 0 of 3\" isn't human friendly\n. how do i avoid these?\n. i'm not following here\n. To block readers before the file is open\nOn May 17, 2016 10:25 PM, \"Tim St. Clair\" notifications@github.com wrote:\n\nIn utils/tail/tail.go\nhttps://github.com/google/cadvisor/pull/1264#discussion_r63640227:\n\n+\n+const retryOpenInterval = time.Second\n+const maxOpenAttempts = 3\n+\n+// NewTail starts opens the given file and watches it for deletion/rotation\n+func NewTail(filename string) (*Tail, error) {\n-   t := &Tail{\n-       filename: filename,\n-   }\n-   var err error\n-   t.stop = make(chan bool)\n-   t.watcher, err = inotify.NewWatcher()\n-   if err != nil {\n-       return nil, fmt.Errorf(\"inotify init failed on %s: %v\", t.filename, err)\n-   }\n-   t.Lock()\n\nWhy are you locking here?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1264/files/8d80aaca81434fbf0804c6923f451df1869cb9c3#r63640227\n. if i don't capture the error, i can't return it in the block\n. nit: s/Tokens/Fields/ since we changed it below\n. any reason you use fmt.Sscanf here and strconv.ParseUint on 63?. this block can never be run; shorted by the previous block. We didn't before so I just maintained what the old code did before i.e. returning \"Unknown\" if it couldn't be parsed.\n\nMy opinion is that we shouldn't log.  The \"Unknown\" in the kubectl get nodes output is evidence enough that the parsing failed.. ",
    "pmorie": "PR: #1204 \n. I need to do a follow-up to check for thin_ls, that should be it.\n. K, this LGTM\n. After talking with @vishh, I'm going to mostly recut this PR.  I'll comment when something ready to review is available.\n. @vishh This implements the factoring we discussed now, PTAL.\n. @timstclair, it's cool -- what I am most interested in for now is whether the broad strokes of the thin pool watcher and how it should fit together with the docker fs handler are good.  I've held off moving forward on the rest of the PR because it's easier to refactor a rough WIP than it is a polished PR w/ unit tests.  If you had an opinion on that subject (broad strokes of thin pool watcher / docker fs handler), I'd be interested to know it.\n. @vishh, thanks, np about latency, quality work takes time :)\n. @vishh PTAL - if you think this is generally correct, i'll refactor the docker stuff to follow @timstclair's PR and wrap up comments.\n. @vishh @timstclair PTAL, integration tests pass.\n. that's what i thought, @k8s-bot !\n. that's what i thought, @k8s-bot !\n. @timstclair updated, PTAL\n. @timstclair updated, PTAL\n. Updated, PTAL\n. Updated, PTAL\n. @k8s-bot test this\n. @k8s-bot test this\n. @vishh light is green, trap is clean :ghost: \n. @vishh light is green, trap is clean :ghost: \n. Yep, thin_ls data is cached, but my WIP hasn't established at what interval it is refreshed\n. Yep, thin_ls data is cached, but my WIP hasn't established at what interval it is refreshed\n. I'm on PTO until Thurs.  Will look at it then.\nOn Mon, May 23, 2016 at 6:23 PM, Tim St. Clair notifications@github.com\nwrote:\n\nActually, the actual flake looks like #1228\nhttps://github.com/google/cadvisor/issues/1228:\nF0523 15:08:38.158517   10157 runner.go:290] Error 0: error on host e2e-cadvisor-rhel-7: command \"godep\" [\"go\" \"test\" \"--timeout\" \"15m0s\" \"github.com/google/cadvisor/integration/tests/...\" \"--host\" \"e2e-cadvisor-rhel-7\" \"--port\" \"8080\" \"--ssh-options\" \"-i /home/jenkins/.ssh/google_compute_engine -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o CheckHostIP=no -o StrictHostKeyChecking=no\"] failed with error: exit status 1 and output: godep: WARNING: Go version (go1.6) & $GO15VENDOREXPERIMENT= wants to enable the vendor experiment, but disabling because a Godep workspace (Godeps/_workspace) exists\n--- FAIL: TestDockerContainerCpuStats (1.87s)\n        framework.go:338: Failed to run \"sudo\" [docker rm -f 3348d8eca4c0b22dd53f626ec3387662581f865f1ace484b99792f55811e741c] in \"e2e-cadvisor-rhel-7\" with error: \"exit status 255\". Stdout: \"\", Stderr: Warning: Permanently added 'e2e-cadvisor-rhel-7' (ECDSA) to the list of known hosts.\n                Permission denied (publickey,gssapi-keyex,gssapi-with-mic).\nFAIL\nFAIL    github.com/google/cadvisor/integration/tests/api        90.315s\nok      github.com/google/cadvisor/integration/tests/healthz    0.011s\ngodep: go exit status 1\nI think the errors reported in the logs should still be investigated, even\nif it just means turning down the log spam.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1306#issuecomment-221113903\n. I'm on PTO until Thurs.  Will look at it then.\n\nOn Mon, May 23, 2016 at 6:23 PM, Tim St. Clair notifications@github.com\nwrote:\n\nActually, the actual flake looks like #1228\nhttps://github.com/google/cadvisor/issues/1228:\nF0523 15:08:38.158517   10157 runner.go:290] Error 0: error on host e2e-cadvisor-rhel-7: command \"godep\" [\"go\" \"test\" \"--timeout\" \"15m0s\" \"github.com/google/cadvisor/integration/tests/...\" \"--host\" \"e2e-cadvisor-rhel-7\" \"--port\" \"8080\" \"--ssh-options\" \"-i /home/jenkins/.ssh/google_compute_engine -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o CheckHostIP=no -o StrictHostKeyChecking=no\"] failed with error: exit status 1 and output: godep: WARNING: Go version (go1.6) & $GO15VENDOREXPERIMENT= wants to enable the vendor experiment, but disabling because a Godep workspace (Godeps/_workspace) exists\n--- FAIL: TestDockerContainerCpuStats (1.87s)\n        framework.go:338: Failed to run \"sudo\" [docker rm -f 3348d8eca4c0b22dd53f626ec3387662581f865f1ace484b99792f55811e741c] in \"e2e-cadvisor-rhel-7\" with error: \"exit status 255\". Stdout: \"\", Stderr: Warning: Permanently added 'e2e-cadvisor-rhel-7' (ECDSA) to the list of known hosts.\n                Permission denied (publickey,gssapi-keyex,gssapi-with-mic).\nFAIL\nFAIL    github.com/google/cadvisor/integration/tests/api        90.315s\nok      github.com/google/cadvisor/integration/tests/healthz    0.011s\ngodep: go exit status 1\nI think the errors reported in the logs should still be investigated, even\nif it just means turning down the log spam.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1306#issuecomment-221113903\n. @timstclair you're right, we should have a check that verifies that thin_ls is on the path and avoid error spew.\n. @timstclair you're right, we should have a check that verifies that thin_ls is on the path and avoid error spew.\n. @godliness the package is called devicemapper-persistent-data\n. @godliness the package is called devicemapper-persistent-data\n. @ae6rt Sorry!  It's device-mapper-persistent-data\n. @ae6rt Sorry!  It's device-mapper-persistent-data\n. @ae6rt so in my case, I'm on F23, and i installed using:\n\n$ dnf install device-mapper-persistent-data --enablerepo=updates-testing --best\n. @ae6rt so in my case, I'm on F23, and i installed using:\n$ dnf install device-mapper-persistent-data --enablerepo=updates-testing --best\n. @ae6rt I don't think thin_ls is in the centos/RHEL package yet :-/  That's actually part of the motivation for this issue.\n. @ae6rt I don't think thin_ls is in the centos/RHEL package yet :-/  That's actually part of the motivation for this issue.\n. @ae6rt thin_ls lets you gather information about filesystems backed by device mapper thin pools.\n. @ae6rt thin_ls lets you gather information about filesystems backed by device mapper thin pools.\n. @ae6rt think of it as du for thin pools.\n. @ae6rt think of it as du for thin pools.\n. @timstclair @vishh \n. @timstclair @vishh \n. @dchen1107 :( i will TAL\n. @dchen1107 :( i will TAL\n. @pwittrock @fejta @dchen1107 Looks like the builder had an infra-related problem -- what's the process to trigger a retest?  (I'm assuming the bot won't listen to me)\n. @pwittrock @fejta @dchen1107 Looks like the builder had an infra-related problem -- what's the process to trigger a retest?  (I'm assuming the bot won't listen to me)\n. same issue.\n. same issue.\n. LGTM\n. LGTM\n. I'll move it to V(5), yeah.\n. I'll move it to V(5), yeah.\n. Ah, nevermind, I see that you have a PR open.  Go ahead and move to V(5)\n. Ah, nevermind, I see that you have a PR open.  Go ahead and move to V(5)\n. I am also wondering what can be done to test this -- we could write some very simple unit tests for stuff like fsToFsStats but I'm really uncertain about integration or e2es\n. I am also wondering what can be done to test this -- we could write some very simple unit tests for stuff like fsToFsStats but I'm really uncertain about integration or e2es\n. protobuf is going to be harder to support/debug than json.  suggest we allow both so that implementers of plugins can use json during development and switch over to protobuf once their plugins are mature\n. protobuf is going to be harder to support/debug than json.  suggest we allow both so that implementers of plugins can use json during development and switch over to protobuf once their plugins are mature\n. we also need to version this API to support adding new features while maintaining backward compatibility with older plugin versions, probably. If we externalize storage drivers, driver implementations will always evolve at different paces. \n. we also need to version this API to support adding new features while maintaining backward compatibility with older plugin versions, probably. If we externalize storage drivers, driver implementations will always evolve at different paces. \n. sorry, I see that you have addressed versioning already :-)\n. sorry, I see that you have addressed versioning already :-)\n. seems like it might be good to have 'opencontainers/runc#1049' in the commit message for the carry\n. seems like it might be good to have 'opencontainers/runc#1049' in the commit message for the carry\n. It's too bad docker doesn't report this in docker info\n. It's too bad docker doesn't report this in docker info\n. @ncdc :chicken: :egg: \n. @ncdc :chicken: :egg: \n. @ncdc you're right -- but it would be nice to get some kind of hint\n. @ncdc you're right -- but it would be nice to get some kind of hint\n. I realize this is following the pattern that's in the code already, but why log the error instead of returning it, here?\n. Agree\n. @vishh I do not understand the role of the FsInfo abstraction vs. The FsHandler one -- could you clarify, perhaps?\n. I made a PR for it -- I'll make a PR to bump the godep\n. @vishh also -- is there a reason to keep devicemapper as a docker-only notion?\n. @ncdc, yeah, this isn't intended to be the final factoring -- just trying\nto get the rough factoring w/in cadvisor correct before getting the little\ndetails right\nOn Thu, Apr 21, 2016 at 1:53 PM, Andy Goldstein notifications@github.com\nwrote:\n\nIn volume/thin_pool_watcher.go\nhttps://github.com/google/cadvisor/pull/1204#discussion_r60626204:\n\n+\n-   w.lock.Lock()\n-   defer w.lock.Unlock()\n  +\n-   w.cache = parseThinLsOutput(output)\n  +}\n  +\n  +// doThinLs handles obtaining the output of thin_ls for the given pool name; it:\n  +//\n  +// 1. Reserves a metadata snapshot for the pool\n  +// 2. Runs thin_ls against that snapshot\n  +// 3. Releases the snapshot\n  +func doThinLs(poolName string) ([]byte, error) {\n-   // (1)\n-   // NOTE: \"0\" in the call below is for the 'sector' argument to 'dmsetup message'.  It's not needed for thin pools.\n-   if _, err := exec.Command(\"dmsetup\", \"message\", poolName, \"0\", \"reserve_metadata_snap\").Output(); err != nil {\n\nI'd recommend creating an interface with functions such as\nreserveMetadataSnap and releaseMetadataSnap so this can be unit tested.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1204/files/f984134c77bde6df61048b98d9dce966aa60b513#r60626204\n. it's protected by a mutex, I will add comments\n. @vishh \nWhy are you adding the fs logic here, instead of abstracting all of it in FsHandler?\n\nI added it here because I had thought the substance of our agreement was to isolate knowledge of device mapper to the docker driver.  The Fs handler we use is common code, and I thought we discussed 1) keeping device mapper out of the common fs handler 2) avoiding creating a new fs handler that was similar to the common one, but also knew about device mapper.\nIf you would prefer, I can do one of the following:\n1.  Create a distinct docker fs handler that took a thin pool watcher and a common fs handler and composed together the usage\n2.  Add a thin pool watcher to the common fs handler and just handle all of it in the common fs handler\n3.  Something else\n. Yeah, I was going to try to find a way to copy the code, not vendor kube.  I refuse to vendor kube, so I will probably just implement something new.\n. Yeah, I think this needs attention still.\n. Cool, that SGTM, will refactor\n. It was a todo of some kind :)\n. My bad.\n. We can't, because it introduces import cycles.  I think @vishh may have thought this was an unaddressed comment -- we discussed it in slack, recording here for posterity.\n. It does.\n. Fixed\n. Added this.\n. I'm happy to do this in a follow up, but for the sake of getting this in, I would prefer to punt on this for now.\n. Well, I added logging the duration of all Refresh() calls -- we can make it only log across a certain threshold in a follow-up, if you like.\n. for some reason this is showing in the diff, but it is not code that I touched (that I remember), I'll fix anyway.\n. It's necessary unless you want to wait 15 seconds to start getting accurate FS usage for containers backed by a dm thin pool.\n. I've fixed this in my local copy, but let me note that this tells me there is a need to add an error to the signature of this interface method.\n. There's an issue to track dependencies of kubelet, and I've called it out there.  we neeed to figure out the scheme in general for tracking and verifying stuff like that.\n. RPM\n. I'll update in a follow-up\n. lol.\n. I'm not really thrilled about doing any of those, let me think about it.  In general I wish this was something log frameworks were a little smarter about.. totes. ",
    "ravilr": "@sjenning @ncdc couldn't  find latest version of device-mapper-persistent-data-0.6* in RHEL7/CentOS7 repos. Do you know who can help disting this? Thanks.\n. ",
    "yghannam": "CLA signed through Linaro.\n@jimmidyson \nThis fixes a runtime error so I'm not sure that a build constraint will be easy to do in this case. I think there may be some other parts that expect machineInfo.CpuFrequency to be defined.\n. ",
    "guoshimin": "@jimmidyson Page cache uses otherwise unused regions of memory. When the system is under memory pressure, page cache entries can be evicted without killing the process to which the page cache is attributed to.\n. Yeah, happy to do it.\nOn Tue, Dec 1, 2015 at 11:16 AM, Jimmi Dyson notifications@github.com\nwrote:\n\n@guoshimin https://github.com/guoshimin Thanks for the explanation -\nnow I understand :) Certainly very useful - any chance you could submit a\nPR for it?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/975#issuecomment-161067154.\n. I prepared two branches, one (https://github.com/guoshimin/cadvisor/tree/icanhazrss) exposes only RSS, whereas the other (https://github.com/guoshimin/cadvisor/tree/exposeall) exposes all the metrics in memory.stat, of which I'm counting 31 on my system. I think exposing them all is better, but it does increase the size of /metrics page by about 50%. Please advise what's the best path forward.\n. Opened another PR #1055. Closing this one.\n. All this is just plumbing to get to the information in memory.stat, which is officially documented at https://www.kernel.org/doc/Documentation/cgroups/memory.txt. I don't find it tasteful to manually enumerate the entries here and duplicate the official documentation.\n\nAs a compromise, I can limit the scope of the PR to only expose RSS, which is the original request in #975.\n. The v2 API references v1: https://github.com/google/cadvisor/blob/e4262b91b17cc132a35a023865f9ef97a048aa21/info/v2/container.go#L102\n. ",
    "psychoche": "+\nI have the same problem.\nAfter container rename, cAdvisor just lost it and only option is to restart cAdvisor. \n. +\nI have the same problem.\nAfter container rename, cAdvisor just lost it and only option is to restart cAdvisor. \n. ",
    "xautjzd": "@vishh I'm very happy to get your answer, Thank you!  But I can't still understand how to use. Can you give me an POST example?\nI have try to post like this, but failed:\n```\nPOST /api/v1.0/containers/{containerId}\nBody:\n{\"start\": \"xxxx\", \"end\": \"xxxx\"}\n```\n. ",
    "SidneyAn": "@rjnagal Thanks for your explain of \"The custom_metrics\".\nThe cadvisor logs is follows:\nI1202 19:25:14.020715 32388 container.go:430] Failed to update stats for container \"/docker/0d3867aa94d32bf2e0cb1fb19186c77df4e8f3cbab85e7a76599f5771470302b\": Error 0: Error 0: No match found for regexp: Active connections: ([0-9]+) for metric 'activeConnections' in config,Error 1: No match found for regexp: Reading: ([0-9]+) .* for metric 'reading' in config, continuing to push custom stats\nAnd the output for api/v1.3/subcontainer is \n[\n\"name\":\"/docker/...71470302b\",\n \"aliases\":[\"k8s_nginx.b483cfba_nginx-y8wov_default_...\",\"0d386...2b\" ],\n \"namespace\":\"docker\",\n \"spec\":{\n    \"creation_time\":\"2015-11-30T10:29:22.244952008Z\",\n    \"labels\":{\n        \"io.cadvisor.metric.nginx\":\"/var/cadvisor/nginx_test.json\",\n        \"io.kubernetes.pod.name\":\"default/nginx-y8wov\"\n    },\n    \"has_cpu\":true,\n    \"cpu\":{  \"limit\":2,\"max_limit\":0,\"mask\":\"0-7\"},\n    ...\n    \"has_custom_metrics\":true,\n    \"custom_metrics\":[\n        {\"name\":\"activeConnections\",\"type\":\"gauge\",\"format\":\"int\",\"units\":\"number of active connections\"},\n       {\"name\":\"reading\",\"type\":\"gauge\",\"format\":\"int\",\"units\":\"number of reading connections\" }    ],\n    \"image\":\"192.168.111.98:5000/artest:v1\"\n },\n \"stats\":[\n    {\n        \"timestamp\":\"2015-12-01T17:52:05.139747749+08:00\",\n...\n]\n. @rjnagal  the endpoints in nginx_config.json is \"http://localhost:2520/nginx_status\"\nI start a simple HTTP server at port 2520 in the host machine.\npython -m SimpleHTTPServer 2520\n. ",
    "zqfan": "I come across same issue with a different reason, all 2.0 api cannot return custom metrics and has_custom_metrics is always false, while 1.3 api can return the custom metric properly!\nthe custom metric endpoint is heapster v1.2.0 api /metrics, the kubelet/cAdvisor log shows \"Error 0: Get http://localhost:8082/metrics: EOF, continuing to push custom stats\"\nI cannot get help from google search and github issues, any hint? Thanks. I find that if I specify to only fetch limited metrics such as add field: \"metrics_config\":[\"go_gc_duration_seconds\"], then this issue will not happen, but if the specified metric is in the middle of the returned metrics (401 entries in total), then this issue will raise again\nso I guess the http lib cadvisor used to fetch custom metric endpoint has problem when the response is a bit large (33K in my case), if I don't specify the metrics_config field, then only 41/401 metrics will be collected, with error message get ... EOF in log. further investigation shows it seems related to the returned result, but I'm still confused\nfor example\n```\nHELP go_gc_duration_seconds A summary of the GC invocation durations.\nTYPE go_gc_duration_seconds summary\ngo_gc_duration_seconds{quantile=\"0\"} 0.000390958\ngo_gc_duration_seconds{quantile=\"0.25\"} 0.0005863660000000001\ngo_gc_duration_seconds{quantile=\"0.5\"} 0.000699541\ngo_gc_duration_seconds{quantile=\"0.75\"} 0.097823592\ngo_gc_duration_seconds{quantile=\"1\"} 0.199161746\ngo_gc_duration_seconds_sum 0.904787788\ngo_gc_duration_seconds_count 27\n```\nif I specify go_gc_duration_seconds then it works fine, without error message. But go_gc_duration_seconds_sum and go_gc_duration_seconds_count will not be collected\nin another case\n```\nHELP heapster_scraper_duration_microseconds Time spent scraping sources in microseconds.\nTYPE heapster_scraper_duration_microseconds summary\nheapster_scraper_duration_microseconds{source=\"kubelet_summary:10.245.1.3:10255\",quantile=\"0.5\"} 17.336\nheapster_scraper_duration_microseconds{source=\"kubelet_summary:10.245.1.3:10255\",quantile=\"0.9\"} 64.359\nheapster_scraper_duration_microseconds{source=\"kubelet_summary:10.245.1.3:10255\",quantile=\"0.99\"} 64.359\nheapster_scraper_duration_microseconds_sum{source=\"kubelet_summary:10.245.1.3:10255\"} 241.91500000000002\nheapster_scraper_duration_microseconds_count{source=\"kubelet_summary:10.245.1.3:10255\"} 6\n```\nif I specify heapster_scraper_duration_microseconds, then it will raise error message Get ... EOF in log, however API v2.0 works fine, but again, the heapster_scraper_duration_microseconds_sum and heapster_scraper_duration_microseconds_count will not be collected\nIt seems  go_gc_duration_seconds and heapster_scraper_duration_microseconds are both valid here but they are treated differently, and why sum and count metrics are not collected?\nthanks. ",
    "abushoeb": "Hello, I have the same problem and don't see any values for custom metrics. When I check cAdvisor at http://:4194/api/v2.1/spec, it shows me this:\n{\n/: {\ncreation_time: \"2017-06-21T21:38:08.53Z\",\nhas_cpu: true,\ncpu: {\nlimit: 1024,\nmax_limit: 0,\nmask: \"0-5\"\n},\nhas_memory: true,\nmemory: {\nlimit: 10316423168,\nreservation: 9223372036854772000\n},\nhas_custom_metrics: false,\nhas_network: true,\nhas_filesystem: true,\nhas_diskio: true\n}\n}\nCould somebody tell me how can I enable has_custom_metrics to true? Also how can I find cAdvisor logs and version?. ",
    "jszczepkowski": "According to https://github.com/google/cadvisor/blob/master/docs/application_metrics.md, if you provide the following config:\n{\n  \"endpoint\" : \"http://localhost:8000/metrics\",\n  \"metrics_config\" : [\n    {\n      \"scheduler_binding_latency\",\n      \"scheduler_e2e_scheduling_latency\",\n      \"scheduling_algorithm_latency\"\n    }\n  ]\n}\nonly scheduler_binding_latency, scheduler_e2e_scheduling_latency and scheduling_algorithm_latency metrics should be exposed. However, in practice, all metrics from http://localhost:8000/metrics are exposed.\nBTW: the documentation is also broken. The sample config file should be:\n{\n  \"endpoint\" : \"http://localhost:8080/metrics\",\n  \"metrics_config\" : [\n      \"scheduler_binding_latency\",\n      \"scheduler_e2e_scheduling_latency\",\n      \"scheduling_algorithm_latency\"\n  ]\n}\nand the docker label pointing to the file for Prometheus collector should be named io.cadvisor.metric.prometheus.\n. @DirectXMan12 \nGreat thanks for fixing this!\n@rjnagal @vishh @jimmidyson \nCan one of you take a look? This PR is badly needed to handle custom metrics in Kubernetes.\n. @jimmidyson Can you please merge it?\n. CC @vishh \n. > Also, at the very least, I think it would make sense to allow just dropping the config currently stored in the file into a container label.\n+1\nI really like this idea.\n. ",
    "DirectXMan12": "I can take a look at fixing this.\n. I can take a look at fixing this.\n. +1\nAlso, at the very least, I think it would make sense to allow just dropping the config currently stored in the file into a container label.  For example:\n$ docker run ... -l io.cadvisor.metric.prometheus=\"{\\\"endpoint\\\": \\\"http://localhost:8080/metrics\\\"}\" mycontainer\nI can write up something like this, if you'd like\n. +1\nAlso, at the very least, I think it would make sense to allow just dropping the config currently stored in the file into a container label.  For example:\n$ docker run ... -l io.cadvisor.metric.prometheus=\"{\\\"endpoint\\\": \\\"http://localhost:8080/metrics\\\"}\" mycontainer\nI can write up something like this, if you'd like\n. It should be fairly quick to implement.  I'll see what I can whip up.\n. It should be fairly quick to implement.  I'll see what I can whip up.\n. > To be clear, for the kubelet use case, what we need is an internal API that can communicate to the cadvisor core what custom metrics are supposed to be collected.\nSure.  The labels-based idea is a quick nice-to-have.\n\nIdeally the config should be part of the Docker images.\n\nWhy?  It seems like it might not be uncommon to have a metrics endpoint that exposes a lot of metrics, but you might only want to use a few.  It seems silly to have to entirely rebuild your Docker image to change which metrics are exposed, when you're not really making a change that affects the rest of the image itself.  It also seems odd to leak cAdvisor details into the filesystem of the container itself.\n. > To be clear, for the kubelet use case, what we need is an internal API that can communicate to the cadvisor core what custom metrics are supposed to be collected.\nSure.  The labels-based idea is a quick nice-to-have.\n\nIdeally the config should be part of the Docker images.\n\nWhy?  It seems like it might not be uncommon to have a metrics endpoint that exposes a lot of metrics, but you might only want to use a few.  It seems silly to have to entirely rebuild your Docker image to change which metrics are exposed, when you're not really making a change that affects the rest of the image itself.  It also seems odd to leak cAdvisor details into the filesystem of the container itself.\n. > I'd say that passing the whole config is label is a bigger hack than what we have now. It abuses the labels use and also makes it hard to verify the runtime argument.\nHow so?  Docker suggests in their documentation that labels can be used for storing JSON data.\n\nKubelet shouldn't be trying to create config files anyway. The runtime blob is neither extensible or shareable. \n\nI'm unclear as to what you are saying here.  Are you suggesting that Kubelet controlling which metrics are collected is a bad idea in general?\n. > I'd say that passing the whole config is label is a bigger hack than what we have now. It abuses the labels use and also makes it hard to verify the runtime argument.\nHow so?  Docker suggests in their documentation that labels can be used for storing JSON data.\n\nKubelet shouldn't be trying to create config files anyway. The runtime blob is neither extensible or shareable. \n\nI'm unclear as to what you are saying here.  Are you suggesting that Kubelet controlling which metrics are collected is a bad idea in general?\n. > The user, even if its kube components, should be deciding what metrics we need to collect. Shouldn't we try to keep this a passthrough operation for kubelet?\nSo, in this case, kubelet would have to receive all the metrics from cAdvisor, and then filter before it presents those metrics out as JSON?\nWhat if an application was written to export Prometheus metrics, but wasn't written for cAdvisor?  It seems, to me, silly to say \"if you want to use cAdvisor to collect your custom metrics, you must build your image with cAdvisor specific configuration data built in to the filesystem\" (you could mount it in, but we discussed why that can be a pain in the kube custom metrics thread).\nI get the desire to provide \"sharable\" configurations, but I also think the user should have some flexibility -- if you decide to build the config in, you can use that.  If you want to control exposed metrics at runtime, or you want to use an \"off-the-shelf\" image that exposes metrics but doesn't have built-in cadvisor configuration, then you can use a label.  I don't see the harm in allowing this, and it seems like a nice feature to have.\n. > The user, even if its kube components, should be deciding what metrics we need to collect. Shouldn't we try to keep this a passthrough operation for kubelet?\nSo, in this case, kubelet would have to receive all the metrics from cAdvisor, and then filter before it presents those metrics out as JSON?\nWhat if an application was written to export Prometheus metrics, but wasn't written for cAdvisor?  It seems, to me, silly to say \"if you want to use cAdvisor to collect your custom metrics, you must build your image with cAdvisor specific configuration data built in to the filesystem\" (you could mount it in, but we discussed why that can be a pain in the kube custom metrics thread).\nI get the desire to provide \"sharable\" configurations, but I also think the user should have some flexibility -- if you decide to build the config in, you can use that.  If you want to control exposed metrics at runtime, or you want to use an \"off-the-shelf\" image that exposes metrics but doesn't have built-in cadvisor configuration, then you can use a label.  I don't see the harm in allowing this, and it seems like a nice feature to have.\n. It was argued in the kube discussion that option 2 is clunky (which is what spawned this issue), and I'm inclined to agree.\nOn a somewhat more ideological basis:\nFurthermore, I'd argue that configuration for how an external service should treat the container should not be embedded in the container's filesystem -- the filesystem should be used for information that the container processes themselves need to access (whether that's configuration for the container processes, or information about where they're running), while labels/annotations/external APIs/etc should be used to hold metadata that needs to be accessed by services outside the container.\n. It was argued in the kube discussion that option 2 is clunky (which is what spawned this issue), and I'm inclined to agree.\nOn a somewhat more ideological basis:\nFurthermore, I'd argue that configuration for how an external service should treat the container should not be embedded in the container's filesystem -- the filesystem should be used for information that the container processes themselves need to access (whether that's configuration for the container processes, or information about where they're running), while labels/annotations/external APIs/etc should be used to hold metadata that needs to be accessed by services outside the container.\n. > The image author is the one who knows about the metrics available in their\n\nimage. As a user I'd expect cAdvisor based monitoring to just work, instead\nof probing the image, figuring out how metrics are exposed and then setting\nup a runtime config.\n\nI'm not arguing that we should do some sort of auto-detection of Prometheus endpoints, or something like that; I'm simply arguing that it makes sense to allow storing configuration in labels, instead the current system, where we store a file path in labels, and then making cAdvisor load that file path from the container's filesystem.  It's possible that the person running the image might also know how the metrics are exposed.\nStoring the info in labels still allows the image author to set the configuration 1, but also allows control at runtime, if need be (e.g. the image doesn't originally have the config on it, it is desired to have the configuration generated from elsewhere, which metrics are actually exposed by the image is dynamically determined and thus cannot be hard-coded, etc).\n\n1 with the exception of Docker 1.8, which doesn't propagate image labels to the container to become container labels (but in that case currently you have to set the label with points to the configuration file path at runtime anyway)\n. > The image author is the one who knows about the metrics available in their\n\nimage. As a user I'd expect cAdvisor based monitoring to just work, instead\nof probing the image, figuring out how metrics are exposed and then setting\nup a runtime config.\n\nI'm not arguing that we should do some sort of auto-detection of Prometheus endpoints, or something like that; I'm simply arguing that it makes sense to allow storing configuration in labels, instead the current system, where we store a file path in labels, and then making cAdvisor load that file path from the container's filesystem.  It's possible that the person running the image might also know how the metrics are exposed.\nStoring the info in labels still allows the image author to set the configuration 1, but also allows control at runtime, if need be (e.g. the image doesn't originally have the config on it, it is desired to have the configuration generated from elsewhere, which metrics are actually exposed by the image is dynamically determined and thus cannot be hard-coded, etc).\n\n1 with the exception of Docker 1.8, which doesn't propagate image labels to the container to become container labels (but in that case currently you have to set the label with points to the configuration file path at runtime anyway)\n. > Ok. Let's try to collect the requirements.\n\n\nImage authors should be able to expose what ever metrics they deem to be necessary.\nImage users should be able to add metrics configuration if one doesn't exist already.\nImage users need ability to query only a subset of all the available metrics if required.\n\n1 is supported.\n2 can be achieved today using volumes. If volumes doesn't seem appealing, how about creating a \nglobal config directory on the host? We can have cAdvisor pick up configs either based on image > name, or specific labels.\n\nIMO, this seems poor, since you now no longer have the configuration somehow directly attached to the container.\n\n3 is already supported. cAdvisor exposes an endpoint that lists all the available metrics.\nDid I miss any other requirement?\n\nThose seem like the requirements.\n\nUsing labels for passing configuration data doesn't sound right. For example, in Kube that is the reason why Annotations were introduced. \n\nI suspect we're going to continue to disagree on this, but Docker doesn't have annotations, so labels could be used similarly to either kubernetes labels or kubernetes annotations.  The current cAdvisor custom metrics setup even uses them as such -- the path pointing the the configuration is itself configuration.\nAt this point, as long as you are fine for the Kubernetes custom metrics work on using the \"kubelet creates a configuration file and mounts it as volume\" method, I feel like we're just going back and forth here and not getting anywhere, so I'm fine just dropping this.\n. > Ok. Let's try to collect the requirements.\n\n\nImage authors should be able to expose what ever metrics they deem to be necessary.\nImage users should be able to add metrics configuration if one doesn't exist already.\nImage users need ability to query only a subset of all the available metrics if required.\n\n1 is supported.\n2 can be achieved today using volumes. If volumes doesn't seem appealing, how about creating a \nglobal config directory on the host? We can have cAdvisor pick up configs either based on image > name, or specific labels.\n\nIMO, this seems poor, since you now no longer have the configuration somehow directly attached to the container.\n\n3 is already supported. cAdvisor exposes an endpoint that lists all the available metrics.\nDid I miss any other requirement?\n\nThose seem like the requirements.\n\nUsing labels for passing configuration data doesn't sound right. For example, in Kube that is the reason why Annotations were introduced. \n\nI suspect we're going to continue to disagree on this, but Docker doesn't have annotations, so labels could be used similarly to either kubernetes labels or kubernetes annotations.  The current cAdvisor custom metrics setup even uses them as such -- the path pointing the the configuration is itself configuration.\nAt this point, as long as you are fine for the Kubernetes custom metrics work on using the \"kubelet creates a configuration file and mounts it as volume\" method, I feel like we're just going back and forth here and not getting anywhere, so I'm fine just dropping this.\n. At this point, I'm also fine dropping the \"label\" angle, and just working on a more direct internal API, but I'm unsure based on your comments if you want that, either.\n. At this point, I'm also fine dropping the \"label\" angle, and just working on a more direct internal API, but I'm unsure based on your comments if you want that, either.\n. @vishh what, specifically, do you mean?  Are you asking what happens if you have two configurations for the same endpoint, one file-based and one direct-label-based?  In that case, it would be exactly as if you had two file-based labels pointing to the same endpoint.  The collectors with file-based-configs get registered before collectors with direct-label-based-configs, but AFAICT the way it would work would be like the combination of the two, since all collectors get processed and add their metrics to the set.\n. @vishh what, specifically, do you mean?  Are you asking what happens if you have two configurations for the same endpoint, one file-based and one direct-label-based?  In that case, it would be exactly as if you had two file-based labels pointing to the same endpoint.  The collectors with file-based-configs get registered before collectors with direct-label-based-configs, but AFAICT the way it would work would be like the combination of the two, since all collectors get processed and add their metrics to the set.\n. (I'm willing to work on this -- the fix shouldn't be too hard)\n. (I'm willing to work on this -- the fix shouldn't be too hard)\n. cc @mwielgus @piosz @fgrzadkowski @dchen1107 this should help make custom metrics in Kube more usable\n. cc @mwielgus @piosz @fgrzadkowski @dchen1107 this should help make custom metrics in Kube more usable\n. @vishh you could put in on the pod IP, but then you'd have to know the pod IP when writing the configuration, so you'd basically be back to have kube write the configuration.  This is suboptimal for that reason, but also because it means that you'd have to expose the metrics externally.  If you just expose them on localhost in the pod's network namespace, it means that others can't easily get at the metrics (which is desirable in many cases).\n. @vishh you could put in on the pod IP, but then you'd have to know the pod IP when writing the configuration, so you'd basically be back to have kube write the configuration.  This is suboptimal for that reason, but also because it means that you'd have to expose the metrics externally.  If you just expose them on localhost in the pod's network namespace, it means that others can't easily get at the metrics (which is desirable in many cases).\n. > Depends on when the configuration is added. It is binded at the last minute, the kubelet can generate configs dynamically for example.\nWith the exception of the \"exposing metrics to anyone with the pod IP\" issue mention above, this seems pretty reasonable to me. However, I will note that this was originally part of how custom metrics worked, and I recall that there were objections to having Kubelet autogenerate custom metrics config on the fly.\nFor example, you could have annotations like custom-metrics/port=8080, custom-metrics/path=/metrics, custom-metrics/metrics=foo,bar (or something similar as one object), with the path defaulting to /metrics and /metrics defaulting to \"*\" if those weren't specified.\n\nWe could also compose the configuration and have provisions for overriding specific fields in the config.\n\nI prefer the \"generate on the fly\" idea from above.  It would probably be easier/fewer moving parts for users to work with anyway.\n\nTo be clear, we have tried switching namespaces and it has caused bugs in the past. Otherwise your idea of exposing stats only on localhost does make a lot of sense.\n\nAh, ok.  I'd be curious to hear what they were -- I was thinking something very simply like shelling out to nsenter --target $PID --net -- curl $URL.\n. > Depends on when the configuration is added. It is binded at the last minute, the kubelet can generate configs dynamically for example.\nWith the exception of the \"exposing metrics to anyone with the pod IP\" issue mention above, this seems pretty reasonable to me. However, I will note that this was originally part of how custom metrics worked, and I recall that there were objections to having Kubelet autogenerate custom metrics config on the fly.\nFor example, you could have annotations like custom-metrics/port=8080, custom-metrics/path=/metrics, custom-metrics/metrics=foo,bar (or something similar as one object), with the path defaulting to /metrics and /metrics defaulting to \"*\" if those weren't specified.\n\nWe could also compose the configuration and have provisions for overriding specific fields in the config.\n\nI prefer the \"generate on the fly\" idea from above.  It would probably be easier/fewer moving parts for users to work with anyway.\n\nTo be clear, we have tried switching namespaces and it has caused bugs in the past. Otherwise your idea of exposing stats only on localhost does make a lot of sense.\n\nAh, ok.  I'd be curious to hear what they were -- I was thinking something very simply like shelling out to nsenter --target $PID --net -- curl $URL.\n. > Go threads were in random network namespaces.\nYeah, that's mainly why I wasn't thinking of just using the system calls, etc.\n\nOn top of that exec'ing is also not cheap and adds latency.\n\nFair.  It seems like the \"expose metrics on the pod IP\" is the way we should go for the moment, then.\n\nFor example, a new feature called \"init containers\" can be used to dynamically generate configuration for cAdvisor.  We could support variable substitution and get pod IP from env vars.\n\nSo, I think I'd prefer just having the kubelet generate the config.  Somewhat separate from this issue, I've a suspicion that the whole configmap-volume-manually-writing-json workflow will end up being confusing and complicated for user.  If we switched to just having Kubelet generate the config, we could kill two birds with one stone here, so to speak.\nMy worry with the \"init containers\" idea is that a) it appears to just be at the proposal phase, and b) unless the init container is generated automatically (at which point why not just have kubelet generate the config directly), it makes things more complicated, not less (see above).  Thus, if we can't just have kubelet generate the config, I'd probably lean towards env vars.\n. > Go threads were in random network namespaces.\nYeah, that's mainly why I wasn't thinking of just using the system calls, etc.\n\nOn top of that exec'ing is also not cheap and adds latency.\n\nFair.  It seems like the \"expose metrics on the pod IP\" is the way we should go for the moment, then.\n\nFor example, a new feature called \"init containers\" can be used to dynamically generate configuration for cAdvisor.  We could support variable substitution and get pod IP from env vars.\n\nSo, I think I'd prefer just having the kubelet generate the config.  Somewhat separate from this issue, I've a suspicion that the whole configmap-volume-manually-writing-json workflow will end up being confusing and complicated for user.  If we switched to just having Kubelet generate the config, we could kill two birds with one stone here, so to speak.\nMy worry with the \"init containers\" idea is that a) it appears to just be at the proposal phase, and b) unless the init container is generated automatically (at which point why not just have kubelet generate the config directly), it makes things more complicated, not less (see above).  Thus, if we can't just have kubelet generate the config, I'd probably lean towards env vars.\n. IIRC, yes, cAdvisor now supports pod IP:port now.. IIRC, yes, cAdvisor now supports pod IP:port now.. I'm torn (no pun intended), although I'm leaning towards agreeing with @fabxc and going for the split.\nOn the one hand, cAdvisor's support for application metrics is fairly clunky, and doesn't really work for Kubernetes' use case (for instance, it needs a Kubernetes HostPort to work, which is not really feasible to do at scale).  Splitting out the application metrics would give us a chance to start fresh and build something that worked well for Kubernetes.\nOn the other hand, collecting application metrics may need awareness of the container system to properly collect (e.g. you may want to allow exposing metrics on localhost inside the container's net namespace, you may want to have the container runtime or a daemon collect all the metrics together and present them in one call, etc), so being inside cAdvisor makes sense, because it already has that knowledge.\n. I'm torn (no pun intended), although I'm leaning towards agreeing with @fabxc and going for the split.\nOn the one hand, cAdvisor's support for application metrics is fairly clunky, and doesn't really work for Kubernetes' use case (for instance, it needs a Kubernetes HostPort to work, which is not really feasible to do at scale).  Splitting out the application metrics would give us a chance to start fresh and build something that worked well for Kubernetes.\nOn the other hand, collecting application metrics may need awareness of the container system to properly collect (e.g. you may want to allow exposing metrics on localhost inside the container's net namespace, you may want to have the container runtime or a daemon collect all the metrics together and present them in one call, etc), so being inside cAdvisor makes sense, because it already has that knowledge.\n. > not using the App/Kubelet/Heapster/Google Cloud Monitoring pipeline and being wary of carrying too much of that around in environments where it doesn't translate.\nRemember, when we talk application metrics here, we're not just talking about metrics used solely for monitoring purposes.  The horizontal pod autoscaler in Kubernetes (and possibly other components in the future) can use custom metrics to scale on.  While it might be desirable to only expose a small subset of metrics for such purposes, we still need to be able to send that subset through in some way that it is consumable by Kubernetes components (currently, that means exposing it through Heapster, which means Heapster needs to be able to collect it).\n\nWhy would I want that? Why would I want it on a once-per-node level, not a once-per-pod or a once-per-cluster (which is basically Prometheus with the default Kubernetes config)?\n\nWhatever the collector is only has to make one HTTP call per node, greatly reducing the number of calls that the central collection agent needs to make.  Additionally, having the collection agent one the node opens up more collection possibilities (e.g. I don't want to expose my metrics endpoint publicly to the cluster, just to the collection agent).\nI think whatever we decide here, we have to keep in mind that fundamentally, as @fabxc has mentioned in the past, there are two concerns at work here: metrics for cluster operation (e.g. autoscaling in its various forms), and metrics for monitoring.  We need a path for both concerns.  It doesn't have to be the same path, but in the end, we need to make sure that we can still collect a few metrics for use by autoscaling, while not requiring all the metrics to go through that path.\n. > not using the App/Kubelet/Heapster/Google Cloud Monitoring pipeline and being wary of carrying too much of that around in environments where it doesn't translate.\nRemember, when we talk application metrics here, we're not just talking about metrics used solely for monitoring purposes.  The horizontal pod autoscaler in Kubernetes (and possibly other components in the future) can use custom metrics to scale on.  While it might be desirable to only expose a small subset of metrics for such purposes, we still need to be able to send that subset through in some way that it is consumable by Kubernetes components (currently, that means exposing it through Heapster, which means Heapster needs to be able to collect it).\n\nWhy would I want that? Why would I want it on a once-per-node level, not a once-per-pod or a once-per-cluster (which is basically Prometheus with the default Kubernetes config)?\n\nWhatever the collector is only has to make one HTTP call per node, greatly reducing the number of calls that the central collection agent needs to make.  Additionally, having the collection agent one the node opens up more collection possibilities (e.g. I don't want to expose my metrics endpoint publicly to the cluster, just to the collection agent).\nI think whatever we decide here, we have to keep in mind that fundamentally, as @fabxc has mentioned in the past, there are two concerns at work here: metrics for cluster operation (e.g. autoscaling in its various forms), and metrics for monitoring.  We need a path for both concerns.  It doesn't have to be the same path, but in the end, we need to make sure that we can still collect a few metrics for use by autoscaling, while not requiring all the metrics to go through that path.\n. > cAdvisor is better positioned for collecting node level container metrics. For example, it can discover containers dynamically & also acquire metadata from corresponding runtimes. This makes it an attractive solution for handling application level metrics as well.\nAgreed :+1:\n\nAs for the issues that @DirectXMan12 mentioned, I find them to be bugs and not fundamental issues.\n\nI agree -- they're not fundamental issues with cAdvisor, but they do need to be solved.  My argument there was mainly that a separate repo could take the burden of review/development off of the main cAdvisor maintainers, making experimenting with a new solution/fixes quicker, and freeing up the main cAdvisor team to work on the \"core\" cAdvisor metrics.  However, if we can work out a way to fix the issues while having application metrics remain inside cAdvisor, I'd by happy with that.\n\nAs for what Kubernetes should do by default, it is beyond cAdvisor. Metrics in kubernetes is a larger, more complex problem. So let's restrict this discussion to just cAdvisor as a standalone entity.\nThere are several discussions and proposals (e.g. https://github.com/kubernetes/heapster/blob/master/docs/proposals/push-metrics.md) for a separate application metrics pipeline in Kubernetes. I think the more relevant discussion here is how much value this feature adds for non-Kubernetes (a.k.a. standalone) users.\n\nI'm certainly aware of push metrics, but my (original) intent there was to supplement Kubelet's (and therefore cAdvisor's) container-level application metrics gathering, not necessarily supplant.\nFor the reasons that @vishh outlines above, I still think on-node container-level application metrics gathering could be advantageous.\nAny decision made here is going to have an effect on the future of container-level custom metrics in the metrics pipeline in Kubernetes, so I think we should bear that in mind while making decisions about application metrics in cAdvisor.\n. > cAdvisor is better positioned for collecting node level container metrics. For example, it can discover containers dynamically & also acquire metadata from corresponding runtimes. This makes it an attractive solution for handling application level metrics as well.\nAgreed :+1:\n\nAs for the issues that @DirectXMan12 mentioned, I find them to be bugs and not fundamental issues.\n\nI agree -- they're not fundamental issues with cAdvisor, but they do need to be solved.  My argument there was mainly that a separate repo could take the burden of review/development off of the main cAdvisor maintainers, making experimenting with a new solution/fixes quicker, and freeing up the main cAdvisor team to work on the \"core\" cAdvisor metrics.  However, if we can work out a way to fix the issues while having application metrics remain inside cAdvisor, I'd by happy with that.\n\nAs for what Kubernetes should do by default, it is beyond cAdvisor. Metrics in kubernetes is a larger, more complex problem. So let's restrict this discussion to just cAdvisor as a standalone entity.\nThere are several discussions and proposals (e.g. https://github.com/kubernetes/heapster/blob/master/docs/proposals/push-metrics.md) for a separate application metrics pipeline in Kubernetes. I think the more relevant discussion here is how much value this feature adds for non-Kubernetes (a.k.a. standalone) users.\n\nI'm certainly aware of push metrics, but my (original) intent there was to supplement Kubelet's (and therefore cAdvisor's) container-level application metrics gathering, not necessarily supplant.\nFor the reasons that @vishh outlines above, I still think on-node container-level application metrics gathering could be advantageous.\nAny decision made here is going to have an effect on the future of container-level custom metrics in the metrics pipeline in Kubernetes, so I think we should bear that in mind while making decisions about application metrics in cAdvisor.\n. > I understood that Heapster was serving that purpose.\nHeapster has to get the metrics from somewhere.  For the same reasons that it's nice to have a single collection agent per node, it's nice to not require Heapster to go and poll each pod, or even to have to know how to find which pods to talk to (the more things Heapster has to poll, the more time issues that we have).\n. > I understood that Heapster was serving that purpose.\nHeapster has to get the metrics from somewhere.  For the same reasons that it's nice to have a single collection agent per node, it's nice to not require Heapster to go and poll each pod, or even to have to know how to find which pods to talk to (the more things Heapster has to poll, the more time issues that we have).\n. uhh..., if people are expecting this to be only one of these set values, this is going to throw them for a loop.\nv1.MetricType  seems to be an implicit enumeration (since Go doesn't have explicit enumerations :-/), so I really don't think it's a good idea to arbitrarily add extra values to the enum at runtime.\n. this could really use a comment to indicate why it's checking this (e.g. // skip any metrics not on our list to collect).  I realize that the old code doesn't have a comment, but we should fix that (I blame myself :-P).\n. you should probably add a comment noting that it's just a guess at a useful initial capacity\n. >  A concatenation of all Prometheus labels?\nYeah, seems like it.  That seems... poor.  It works ok for one label, but multiple labels could come in any order.  If we wanted to make sure this was comparable, we could sort them first.  Doesn't look like the label is actually carried through to the Kube summary API. \n. none-the-less, something more expressive might be in order (perhaps rawmodel or rawtypes?)\n. Please file a follow-up issue about issue about this, because people probably won't expect to see any values  beyond what's defined in v1 (whether that means defining more types, or an \"other\" type, or something else)\n. ",
    "zSys-bot": "Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. Build finished. \n. ",
    "miguel250": "@vishh Yeah this set as well.\n. Just created #1021\n. Hi @vishh,\nis there anything else that needs to be done for this pull request?\n. ",
    "DanielDent": "I'm unclear if zfs support is supposed to be working right now. For me, it does not appear to be.. On systems with ZFS, I get complaints about /dev/zfs being missing. If I provide the /dev/zfs device by bind mounting the host's device, then I get new complaints about zfs libraries being missing.\n. ",
    "laurikimmel": "+1 - it would be nice\nVersion 0.20.5 was released 3 days ago. Latest release available in Docker Hub is still 0.20.4.\n. +1 - it would be nice\nVersion 0.20.5 was released 3 days ago. Latest release available in Docker Hub is still 0.20.4.\n. ",
    "dan-lind": "It looks like it works if I start WITHOUT running as sudo. I tried this but got a bit thrown off by the fact that the container log was completely empty. But when accessing my docker-machine on 8080, the dashboard is indeed there.\n. ",
    "wangzhezhe": "Thanks for your concern , but why some checks were still not successful ?  I hope that I could contribute more effective codes in future.\n. @pwittrock Thank you for the reply\uff01\n. Thanks ! \n. ",
    "arichardet": "I signed it!\n. @timstclair I've addressed your PR feedback. \n. Travis-CI is not liking the Kafka.go header going from 2015 -> 2016\n. @timstclair I tried rebasing and it this is what I got :-1: I can close this PR and resubmit as a fresh one that is cleanly in one commit if thats ok.\n. @timstclair I was able to clean it up. \n. @timstclair I've separated them.\n. @timstclair Is there anything else you need me to do?\n. @timstclair should close this and resubmit a clean PR? Thanks\n. @timstclair comments addressed. Thanks\n. @timstclair comment addressed. Thanks\n. Thanks @krallin.\n. @krallin I am getting zero values for load_average and some task stats for some containers.\n. I was aiming to be consistent with the convention set by this refactor.\n. ",
    "achille-roussel": "I'm looking at the Prometheus interface and I'm seeing these stats:\n```\nHELP container_memory_usage_bytes Current memory usage in bytes.\nTYPE container_memory_usage_bytes gauge\ncontainer_memory_usage_bytes{id=\"/\"} 0\n...\n```\nAre these disk metrics? How can I figure out what they are actually reporting?\n. Do you guys have an idea what may be causing this? I really just used the default setup and there's nothing fancy about the docker install either (using Docker 1.8 on a Debian on AWS).\n. Sorry I forgot to mention that, there's about nothing in the logs:\ndocker logs --tail=10 -f container-exporter\nI0108 21:21:58.873729 00001 manager.go:158] Machine: {NumCores:4 CpuFrequency:2400072 MemoryCapacity:16863678464 MachineID: SystemUUID:*** BootID:6a8d7a36-e177-44ec-a42a-7562c3677d15 Filesystems:[{Device:/dev/xvda1 Capacity:8316706816} {Device:/dev/xvdb Capacity:270565117952}] DiskMap:map[202:0:{Name:xvda Major:202 Minor:0 Size:8589934592 Scheduler:cfq} 202:16:{Name:xvdb Major:202 Minor:16 Size:274877906944 Scheduler:cfq}] NetworkDevices:[{Name:eth0 MacAddress:*** Speed:10000 Mtu:9001}] Topology:[{Id:0 Memory:16863678464 Cores:[{Id:0 Threads:[0 2] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[1 3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:31457280 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown}\nI0108 21:21:58.874528 00001 manager.go:164] Version: {KernelVersion:3.16.0-4-amd64 ContainerOsVersion:Buildroot 2014.02 DockerVersion:1.8.3 CadvisorVersion:0.18.0}\nI0108 21:21:58.968268 00001 factory.go:227] System is using systemd\nI0108 21:21:58.971804 00001 factory.go:235] Registering Docker factory\nI0108 21:21:58.975366 00001 factory.go:93] Registering Raw factory\nI0108 21:21:59.122014 00001 manager.go:1001] Started watching for new ooms in manager\nW0108 21:21:59.122174 00001 manager.go:232] Could not configure a source for OOM detection, disabling OOM events: exec: \"journalctl\": executable file not found in $PATH\nI0108 21:21:59.141765 00001 manager.go:245] Starting recovery of all containers\nI0108 21:21:59.215566 00001 manager.go:250] Recovery completed\nI0108 21:21:59.249463 00001 cadvisor.go:94] Starting cAdvisor version: \"0.18.0\" on port 8080\nNothing more than this is shown, Prometheus is pulling every 15 seconds and I forced generating a few HTTP requests as well but the logs stay silent.\nI'm also checking the command line options, the -stderrthreshold and -v are at 0 which as I understand means all logs will be printed... I'm not sure how to get more data on what's going on.\n. Alright, that was confusing, -stderrthreshold=0 sending all stats to stderr but -v=0 is actually turning off stats, thanks for your advise.\nI've got more data now, but there's nothing explicit about memory stats not being read, a bunch of stuff that look like warnings show up tho but I'm too inexperienced with cadvisor to tell what would be giving hints on what's going on.\nThings that look weird to me, I'm gonna investigate more:\nI0111 20:20:05.495421 00001 machine.go:49] Couldn't collect info from any of the files in \"/etc/machine-id,/var/lib/dbus/machine-id\"\nI0111 20:20:05.749432 00001 factory.go:71] Error trying to work out if we can handle /: error inspecting container: unexpected end of JSON input\nHere are all the logs if someone has some spare cycles to take a look:\nI0111 20:20:05.487631 00001 manager.go:127] cAdvisor running in container: \"/system.slice/docker-1833c88c617a80f2182b7dd588f119db49175ec75e303f42ba9b715703ddd75f.scope\"\nI0111 20:20:05.491407 00001 fs.go:93] Filesystem partitions: map[/dev/xvda1:{mountpoint:/rootfs major:202 minor:1} /dev/xvdb:{mountpoint:/rootfs/mnt/prometheus major:202 minor:16}]\nI0111 20:20:05.495421 00001 machine.go:49] Couldn't collect info from any of the files in \"/etc/machine-id,/var/lib/dbus/machine-id\"\nI0111 20:20:05.495466 00001 manager.go:158] Machine: {NumCores:4 CpuFrequency:2400072 MemoryCapacity:16863678464 MachineID: SystemUUID:EC2A5423-30E0-02E9-E596-2F0EA2365FBB BootID:6a8d7a36-e177-44ec-a42a-7562c3677d15 Filesystems:[{Device:/dev/xvda1 Capacity:8316706816} {Device:/dev/xvdb Capacity:270565117952}] DiskMap:map[202:0:{Name:xvda Major:202 Minor:0 Size:8589934592 Scheduler:cfq} 202:16:{Name:xvdb Major:202 Minor:16 Size:274877906944 Scheduler:cfq}] NetworkDevices:[{Name:eth0 MacAddress:12:38:96:44:90:13 Speed:10000 Mtu:9001}] Topology:[{Id:0 Memory:16863678464 Cores:[{Id:0 Threads:[0 2] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[1 3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:31457280 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown}\nI0111 20:20:05.496289 00001 manager.go:164] Version: {KernelVersion:3.16.0-4-amd64 ContainerOsVersion:Buildroot 2014.02 DockerVersion:1.8.3 CadvisorVersion:0.18.0}\nI0111 20:20:05.583212 00001 factory.go:227] System is using systemd\nI0111 20:20:05.586936 00001 factory.go:235] Registering Docker factory\nI0111 20:20:05.590411 00001 factory.go:93] Registering Raw factory\nI0111 20:20:05.746294 00001 manager.go:1001] Started watching for new ooms in manager\nW0111 20:20:05.746392 00001 manager.go:232] Could not configure a source for OOM detection, disabling OOM events: exec: \"journalctl\": executable file not found in $PATH\nI0111 20:20:05.749432 00001 factory.go:71] Error trying to work out if we can handle /: error inspecting container: unexpected end of JSON input\nI0111 20:20:05.749449 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/\"\nI0111 20:20:05.749462 00001 factory.go:78] Using factory \"raw\" for container \"/\"\nI0111 20:20:05.750068 00001 manager.go:798] Added container: \"/\" (aliases: [], namespace: \"\")\nI0111 20:20:05.750363 00001 handler.go:322] Added event &{/ 2016-01-05 01:11:41.221597572 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.750445 00001 manager.go:245] Starting recovery of all containers\nI0111 20:20:05.750466 00001 container.go:368] Start housekeeping for container \"/\"\nI0111 20:20:05.766835 00001 factory.go:71] Error trying to work out if we can handle /system.slice: error inspecting container: No such container: system.slice\nI0111 20:20:05.766902 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice\"\nI0111 20:20:05.766914 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice\"\nI0111 20:20:05.767039 00001 manager.go:798] Added container: \"/system.slice\" (aliases: [], namespace: \"\")\nI0111 20:20:05.767139 00001 handler.go:322] Added event &{/system.slice 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.767209 00001 container.go:368] Start housekeeping for container \"/system.slice\"\nI0111 20:20:05.767818 00001 factory.go:71] Error trying to work out if we can handle /system.slice/mnt-prometheus.mount: error inspecting container: No such container: mnt-prometheus.mount\nI0111 20:20:05.767850 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/mnt-prometheus.mount\"\nI0111 20:20:05.767864 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/mnt-prometheus.mount\"\nI0111 20:20:05.767997 00001 manager.go:798] Added container: \"/system.slice/mnt-prometheus.mount\" (aliases: [], namespace: \"\")\nI0111 20:20:05.768127 00001 handler.go:322] Added event &{/system.slice/mnt-prometheus.mount 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.768163 00001 container.go:368] Start housekeeping for container \"/system.slice/mnt-prometheus.mount\"\nI0111 20:20:05.768832 00001 factory.go:71] Error trying to work out if we can handle /system.slice/systemd-fsck-root.service: error inspecting container: No such container: systemd-fsck-root.service\nI0111 20:20:05.768851 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/systemd-fsck-root.service\"\nI0111 20:20:05.768863 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/systemd-fsck-root.service\"\nI0111 20:20:05.768988 00001 manager.go:798] Added container: \"/system.slice/systemd-fsck-root.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.769087 00001 handler.go:322] Added event &{/system.slice/systemd-fsck-root.service 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.769143 00001 container.go:368] Start housekeeping for container \"/system.slice/systemd-fsck-root.service\"\nI0111 20:20:05.769887 00001 factory.go:71] Error trying to work out if we can handle /system.slice/systemd-update-utmp.service: error inspecting container: No such container: systemd-update-utmp.service\nI0111 20:20:05.769916 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/systemd-update-utmp.service\"\nI0111 20:20:05.769932 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/systemd-update-utmp.service\"\nI0111 20:20:05.770071 00001 manager.go:798] Added container: \"/system.slice/systemd-update-utmp.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.770163 00001 handler.go:322] Added event &{/system.slice/systemd-update-utmp.service 2016-01-05 01:35:21.037221544 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.770244 00001 container.go:368] Start housekeeping for container \"/system.slice/systemd-update-utmp.service\"\nI0111 20:20:05.770971 00001 factory.go:71] Error trying to work out if we can handle /system.slice/udev-finish.service: error inspecting container: No such container: udev-finish.service\nI0111 20:20:05.770986 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/udev-finish.service\"\nI0111 20:20:05.770997 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/udev-finish.service\"\nI0111 20:20:05.771148 00001 manager.go:798] Added container: \"/system.slice/udev-finish.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.771249 00001 handler.go:322] Added event &{/system.slice/udev-finish.service 2016-01-05 01:35:21.037221544 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.771295 00001 container.go:368] Start housekeeping for container \"/system.slice/udev-finish.service\"\nI0111 20:20:05.771736 00001 factory.go:71] Error trying to work out if we can handle /system.slice/dev-hugepages.mount: error inspecting container: No such container: dev-hugepages.mount\nI0111 20:20:05.771756 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/dev-hugepages.mount\"\nI0111 20:20:05.771768 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/dev-hugepages.mount\"\nI0111 20:20:05.771894 00001 manager.go:798] Added container: \"/system.slice/dev-hugepages.mount\" (aliases: [], namespace: \"\")\nI0111 20:20:05.772006 00001 handler.go:322] Added event &{/system.slice/dev-hugepages.mount 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.772054 00001 container.go:368] Start housekeeping for container \"/system.slice/dev-hugepages.mount\"\nI0111 20:20:05.773055 00001 factory.go:71] Error trying to work out if we can handle /system.slice/rsyslog.service: error inspecting container: No such container: rsyslog.service\nI0111 20:20:05.773076 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/rsyslog.service\"\nI0111 20:20:05.773088 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/rsyslog.service\"\nI0111 20:20:05.773233 00001 manager.go:798] Added container: \"/system.slice/rsyslog.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.773346 00001 handler.go:322] Added event &{/system.slice/rsyslog.service 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.773387 00001 container.go:368] Start housekeeping for container \"/system.slice/rsyslog.service\"\nI0111 20:20:05.774404 00001 factory.go:71] Error trying to work out if we can handle /system.slice/-.mount: error inspecting container: No such container: -.mount\nI0111 20:20:05.774427 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/-.mount\"\nI0111 20:20:05.774439 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/-.mount\"\nI0111 20:20:05.774543 00001 manager.go:798] Added container: \"/system.slice/-.mount\" (aliases: [], namespace: \"\")\nI0111 20:20:05.774642 00001 handler.go:322] Added event &{/system.slice/-.mount 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.774691 00001 container.go:368] Start housekeeping for container \"/system.slice/-.mount\"\nI0111 20:20:05.775412 00001 factory.go:71] Error trying to work out if we can handle /system.slice/cgroupfs-mount.service: error inspecting container: No such container: cgroupfs-mount.service\nI0111 20:20:05.775425 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/cgroupfs-mount.service\"\nI0111 20:20:05.775436 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/cgroupfs-mount.service\"\nI0111 20:20:05.775580 00001 manager.go:798] Added container: \"/system.slice/cgroupfs-mount.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.775684 00001 handler.go:322] Added event &{/system.slice/cgroupfs-mount.service 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.775769 00001 container.go:368] Start housekeeping for container \"/system.slice/cgroupfs-mount.service\"\nI0111 20:20:05.777341 00001 factory.go:71] Error trying to work out if we can handle /system.slice/system-getty.slice: error inspecting container: No such container: system-getty.slice\nI0111 20:20:05.777353 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/system-getty.slice\"\nI0111 20:20:05.777360 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/system-getty.slice\"\nI0111 20:20:05.777462 00001 manager.go:798] Added container: \"/system.slice/system-getty.slice\" (aliases: [], namespace: \"\")\nI0111 20:20:05.777533 00001 handler.go:322] Added event &{/system.slice/system-getty.slice 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.777583 00001 container.go:368] Start housekeeping for container \"/system.slice/system-getty.slice\"\nI0111 20:20:05.778414 00001 factory.go:71] Error trying to work out if we can handle /system.slice/systemd-modules-load.service: error inspecting container: No such container: systemd-modules-load.service\nI0111 20:20:05.778429 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/systemd-modules-load.service\"\nI0111 20:20:05.778441 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/systemd-modules-load.service\"\nI0111 20:20:05.778548 00001 manager.go:798] Added container: \"/system.slice/systemd-modules-load.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.778641 00001 handler.go:322] Added event &{/system.slice/systemd-modules-load.service 2016-01-05 01:35:21.037221544 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.778686 00001 container.go:368] Start housekeeping for container \"/system.slice/systemd-modules-load.service\"\nI0111 20:20:05.779238 00001 factory.go:71] Error trying to work out if we can handle /user.slice: error inspecting container: No such container: user.slice\nI0111 20:20:05.779250 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/user.slice\"\nI0111 20:20:05.779260 00001 factory.go:78] Using factory \"raw\" for container \"/user.slice\"\nI0111 20:20:05.779431 00001 manager.go:798] Added container: \"/user.slice\" (aliases: [], namespace: \"\")\nI0111 20:20:05.779570 00001 handler.go:322] Added event &{/user.slice 2016-01-05 01:35:21.037221544 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.779640 00001 container.go:368] Start housekeeping for container \"/user.slice\"\nI0111 20:20:05.780430 00001 factory.go:71] Error trying to work out if we can handle /system.slice/dev-mqueue.mount: error inspecting container: No such container: dev-mqueue.mount\nI0111 20:20:05.780445 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/dev-mqueue.mount\"\nI0111 20:20:05.780456 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/dev-mqueue.mount\"\nI0111 20:20:05.780589 00001 manager.go:798] Added container: \"/system.slice/dev-mqueue.mount\" (aliases: [], namespace: \"\")\nI0111 20:20:05.780695 00001 handler.go:322] Added event &{/system.slice/dev-mqueue.mount 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.780801 00001 container.go:368] Start housekeeping for container \"/system.slice/dev-mqueue.mount\"\nI0111 20:20:05.782080 00001 factory.go:78] Using factory \"docker\" for container \"/system.slice/docker-5422c86bb0116f43be90b82b76942bc3cb2a0edcb1779641751ab7af35e81950.scope\"\nI0111 20:20:05.783713 00001 manager.go:798] Added container: \"/system.slice/docker-5422c86bb0116f43be90b82b76942bc3cb2a0edcb1779641751ab7af35e81950.scope\" (aliases: [prometheus 5422c86bb0116f43be90b82b76942bc3cb2a0edcb1779641751ab7af35e81950], namespace: \"docker\")\nI0111 20:20:05.784149 00001 handler.go:322] Added event &{/system.slice/docker-5422c86bb0116f43be90b82b76942bc3cb2a0edcb1779641751ab7af35e81950.scope 2016-01-05 02:47:36.063077392 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.784194 00001 container.go:368] Start housekeeping for container \"/system.slice/docker-5422c86bb0116f43be90b82b76942bc3cb2a0edcb1779641751ab7af35e81950.scope\"\nI0111 20:20:05.784879 00001 factory.go:71] Error trying to work out if we can handle /system.slice/systemd-logind.service: error inspecting container: No such container: systemd-logind.service\nI0111 20:20:05.784893 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/systemd-logind.service\"\nI0111 20:20:05.784904 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/systemd-logind.service\"\nI0111 20:20:05.785424 00001 manager.go:798] Added container: \"/system.slice/systemd-logind.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.785525 00001 handler.go:322] Added event &{/system.slice/systemd-logind.service 2016-01-05 01:35:21.037221544 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.785647 00001 container.go:368] Start housekeeping for container \"/system.slice/systemd-logind.service\"\nI0111 20:20:05.786148 00001 factory.go:71] Error trying to work out if we can handle /system.slice/systemd-random-seed.service: error inspecting container: No such container: systemd-random-seed.service\nI0111 20:20:05.786171 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/systemd-random-seed.service\"\nI0111 20:20:05.786182 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/systemd-random-seed.service\"\nI0111 20:20:05.786333 00001 manager.go:798] Added container: \"/system.slice/systemd-random-seed.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.786452 00001 handler.go:322] Added event &{/system.slice/systemd-random-seed.service 2016-01-05 01:35:21.037221544 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.786525 00001 container.go:368] Start housekeeping for container \"/system.slice/systemd-random-seed.service\"\nI0111 20:20:05.787321 00001 factory.go:71] Error trying to work out if we can handle /system.slice/systemd-remount-fs.service: error inspecting container: No such container: systemd-remount-fs.service\nI0111 20:20:05.787335 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/systemd-remount-fs.service\"\nI0111 20:20:05.787346 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/systemd-remount-fs.service\"\nI0111 20:20:05.787475 00001 manager.go:798] Added container: \"/system.slice/systemd-remount-fs.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.787580 00001 handler.go:322] Added event &{/system.slice/systemd-remount-fs.service 2016-01-05 01:35:21.037221544 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.787631 00001 container.go:368] Start housekeeping for container \"/system.slice/systemd-remount-fs.service\"\nI0111 20:20:05.788730 00001 factory.go:71] Error trying to work out if we can handle /system.slice/systemd-udevd.service: error inspecting container: No such container: systemd-udevd.service\nI0111 20:20:05.788752 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/systemd-udevd.service\"\nI0111 20:20:05.788765 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/systemd-udevd.service\"\nI0111 20:20:05.788904 00001 manager.go:798] Added container: \"/system.slice/systemd-udevd.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.789010 00001 handler.go:322] Added event &{/system.slice/systemd-udevd.service 2016-01-05 01:35:21.037221544 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.789115 00001 container.go:368] Start housekeeping for container \"/system.slice/systemd-udevd.service\"\nI0111 20:20:05.789820 00001 factory.go:71] Error trying to work out if we can handle /system.slice/cloud-config.service: error inspecting container: No such container: cloud-config.service\nI0111 20:20:05.789836 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/cloud-config.service\"\nI0111 20:20:05.789847 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/cloud-config.service\"\nI0111 20:20:05.789969 00001 manager.go:798] Added container: \"/system.slice/cloud-config.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.790103 00001 handler.go:322] Added event &{/system.slice/cloud-config.service 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.790204 00001 container.go:368] Start housekeeping for container \"/system.slice/cloud-config.service\"\nI0111 20:20:05.790925 00001 factory.go:71] Error trying to work out if we can handle /system.slice/cloud-final.service: error inspecting container: No such container: cloud-final.service\nI0111 20:20:05.790940 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/cloud-final.service\"\nI0111 20:20:05.790951 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/cloud-final.service\"\nI0111 20:20:05.791041 00001 manager.go:798] Added container: \"/system.slice/cloud-final.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.791110 00001 handler.go:322] Added event &{/system.slice/cloud-final.service 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.791184 00001 container.go:368] Start housekeeping for container \"/system.slice/cloud-final.service\"\nI0111 20:20:05.791977 00001 factory.go:71] Error trying to work out if we can handle /system.slice/cloud-init.service: error inspecting container: No such container: cloud-init.service\nI0111 20:20:05.792034 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/cloud-init.service\"\nI0111 20:20:05.792059 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/cloud-init.service\"\nI0111 20:20:05.792222 00001 manager.go:798] Added container: \"/system.slice/cloud-init.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.792358 00001 handler.go:322] Added event &{/system.slice/cloud-init.service 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.792471 00001 container.go:368] Start housekeeping for container \"/system.slice/cloud-init.service\"\nI0111 20:20:05.793570 00001 factory.go:78] Using factory \"docker\" for container \"/system.slice/docker-0be3a0479f7265d299a28e607d0deb01195116a21f611260deb8f27ff57f42f9.scope\"\nI0111 20:20:05.795002 00001 manager.go:798] Added container: \"/system.slice/docker-0be3a0479f7265d299a28e607d0deb01195116a21f611260deb8f27ff57f42f9.scope\" (aliases: [consul 0be3a0479f7265d299a28e607d0deb01195116a21f611260deb8f27ff57f42f9], namespace: \"docker\")\nI0111 20:20:05.795395 00001 handler.go:322] Added event &{/system.slice/docker-0be3a0479f7265d299a28e607d0deb01195116a21f611260deb8f27ff57f42f9.scope 2016-01-05 21:55:04.770951619 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.795490 00001 container.go:368] Start housekeeping for container \"/system.slice/docker-0be3a0479f7265d299a28e607d0deb01195116a21f611260deb8f27ff57f42f9.scope\"\nI0111 20:20:05.796702 00001 factory.go:78] Using factory \"docker\" for container \"/system.slice/docker-76bbd245a77ce85d0697235348a556828160d134900987a8d3c4b417b602d0cb.scope\"\nI0111 20:20:05.798091 00001 manager.go:798] Added container: \"/system.slice/docker-76bbd245a77ce85d0697235348a556828160d134900987a8d3c4b417b602d0cb.scope\" (aliases: [alertmanager 76bbd245a77ce85d0697235348a556828160d134900987a8d3c4b417b602d0cb], namespace: \"docker\")\nI0111 20:20:05.798469 00001 handler.go:322] Added event &{/system.slice/docker-76bbd245a77ce85d0697235348a556828160d134900987a8d3c4b417b602d0cb.scope 2016-01-05 02:46:32.70334153 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.798518 00001 container.go:368] Start housekeeping for container \"/system.slice/docker-76bbd245a77ce85d0697235348a556828160d134900987a8d3c4b417b602d0cb.scope\"\nI0111 20:20:05.799279 00001 factory.go:71] Error trying to work out if we can handle /system.slice/ssh.service: error inspecting container: No such container: ssh.service\nI0111 20:20:05.799351 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/ssh.service\"\nI0111 20:20:05.799375 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/ssh.service\"\nI0111 20:20:05.799799 00001 manager.go:798] Added container: \"/system.slice/ssh.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.799907 00001 handler.go:322] Added event &{/system.slice/ssh.service 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.800021 00001 container.go:368] Start housekeeping for container \"/system.slice/ssh.service\"\nI0111 20:20:05.801985 00001 factory.go:71] Error trying to work out if we can handle /system.slice/sys-kernel-debug.mount: error inspecting container: No such container: sys-kernel-debug.mount\nI0111 20:20:05.802035 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/sys-kernel-debug.mount\"\nI0111 20:20:05.802083 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/sys-kernel-debug.mount\"\nI0111 20:20:05.802247 00001 manager.go:798] Added container: \"/system.slice/sys-kernel-debug.mount\" (aliases: [], namespace: \"\")\nI0111 20:20:05.802387 00001 handler.go:322] Added event &{/system.slice/sys-kernel-debug.mount 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.802451 00001 container.go:368] Start housekeeping for container \"/system.slice/sys-kernel-debug.mount\"\nI0111 20:20:05.803421 00001 factory.go:71] Error trying to work out if we can handle /system.slice/sysinfo.service: error inspecting container: No such container: sysinfo.service\nI0111 20:20:05.803466 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/sysinfo.service\"\nI0111 20:20:05.803489 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/sysinfo.service\"\nI0111 20:20:05.803648 00001 manager.go:798] Added container: \"/system.slice/sysinfo.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.803807 00001 handler.go:322] Added event &{/system.slice/sysinfo.service 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.803865 00001 container.go:368] Start housekeeping for container \"/system.slice/sysinfo.service\"\nI0111 20:20:05.804508 00001 factory.go:71] Error trying to work out if we can handle /system.slice/systemd-journald.service: error inspecting container: No such container: systemd-journald.service\nI0111 20:20:05.804540 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/systemd-journald.service\"\nI0111 20:20:05.804593 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/systemd-journald.service\"\nI0111 20:20:05.804737 00001 manager.go:798] Added container: \"/system.slice/systemd-journald.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.804850 00001 handler.go:322] Added event &{/system.slice/systemd-journald.service 2016-01-05 01:35:21.037221544 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.804954 00001 container.go:368] Start housekeeping for container \"/system.slice/systemd-journald.service\"\nI0111 20:20:05.805704 00001 factory.go:71] Error trying to work out if we can handle /system.slice/systemd-setup-dgram-qlen.service: error inspecting container: No such container: systemd-setup-dgram-qlen.service\nI0111 20:20:05.805740 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/systemd-setup-dgram-qlen.service\"\nI0111 20:20:05.805753 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/systemd-setup-dgram-qlen.service\"\nI0111 20:20:05.805930 00001 manager.go:798] Added container: \"/system.slice/systemd-setup-dgram-qlen.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.806094 00001 handler.go:322] Added event &{/system.slice/systemd-setup-dgram-qlen.service 2016-01-05 01:35:21.037221544 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.806143 00001 container.go:368] Start housekeeping for container \"/system.slice/systemd-setup-dgram-qlen.service\"\nI0111 20:20:05.806936 00001 factory.go:71] Error trying to work out if we can handle /system.slice/dbus.service: error inspecting container: No such container: dbus.service\nI0111 20:20:05.806968 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/dbus.service\"\nI0111 20:20:05.806990 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/dbus.service\"\nI0111 20:20:05.807153 00001 manager.go:798] Added container: \"/system.slice/dbus.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.807285 00001 handler.go:322] Added event &{/system.slice/dbus.service 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.807341 00001 container.go:368] Start housekeeping for container \"/system.slice/dbus.service\"\nI0111 20:20:05.808130 00001 factory.go:71] Error trying to work out if we can handle /system.slice/docker.service: error inspecting container: No such container: docker.service\nI0111 20:20:05.808144 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/docker.service\"\nI0111 20:20:05.808155 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/docker.service\"\nI0111 20:20:05.808322 00001 manager.go:798] Added container: \"/system.slice/docker.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.808417 00001 handler.go:322] Added event &{/system.slice/docker.service 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.808469 00001 container.go:368] Start housekeeping for container \"/system.slice/docker.service\"\nI0111 20:20:05.809226 00001 factory.go:71] Error trying to work out if we can handle /system.slice/networking.service: error inspecting container: No such container: networking.service\nI0111 20:20:05.809242 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/networking.service\"\nI0111 20:20:05.809253 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/networking.service\"\nI0111 20:20:05.809354 00001 manager.go:798] Added container: \"/system.slice/networking.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.809417 00001 handler.go:322] Added event &{/system.slice/networking.service 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.809468 00001 container.go:368] Start housekeeping for container \"/system.slice/networking.service\"\nI0111 20:20:05.809951 00001 factory.go:71] Error trying to work out if we can handle /system.slice/systemd-tmpfiles-setup-dev.service: error inspecting container: No such container: systemd-tmpfiles-setup-dev.service\nI0111 20:20:05.810075 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/systemd-tmpfiles-setup-dev.service\"\nI0111 20:20:05.810120 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/systemd-tmpfiles-setup-dev.service\"\nI0111 20:20:05.810277 00001 manager.go:798] Added container: \"/system.slice/systemd-tmpfiles-setup-dev.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.810383 00001 handler.go:322] Added event &{/system.slice/systemd-tmpfiles-setup-dev.service 2016-01-05 01:35:21.037221544 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.810431 00001 container.go:368] Start housekeeping for container \"/system.slice/systemd-tmpfiles-setup-dev.service\"\nI0111 20:20:05.811092 00001 factory.go:71] Error trying to work out if we can handle /system.slice/systemd-tmpfiles-setup.service: error inspecting container: No such container: systemd-tmpfiles-setup.service\nI0111 20:20:05.811107 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/systemd-tmpfiles-setup.service\"\nI0111 20:20:05.811120 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/systemd-tmpfiles-setup.service\"\nI0111 20:20:05.811210 00001 manager.go:798] Added container: \"/system.slice/systemd-tmpfiles-setup.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.811290 00001 handler.go:322] Added event &{/system.slice/systemd-tmpfiles-setup.service 2016-01-05 01:35:21.037221544 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.811365 00001 container.go:368] Start housekeeping for container \"/system.slice/systemd-tmpfiles-setup.service\"\nI0111 20:20:05.811992 00001 factory.go:71] Error trying to work out if we can handle /system.slice/systemd-user-sessions.service: error inspecting container: No such container: systemd-user-sessions.service\nI0111 20:20:05.812009 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/systemd-user-sessions.service\"\nI0111 20:20:05.812020 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/systemd-user-sessions.service\"\nI0111 20:20:05.812126 00001 manager.go:798] Added container: \"/system.slice/systemd-user-sessions.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.812189 00001 handler.go:322] Added event &{/system.slice/systemd-user-sessions.service 2016-01-05 01:35:21.037221544 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.812266 00001 container.go:368] Start housekeeping for container \"/system.slice/systemd-user-sessions.service\"\nI0111 20:20:05.813371 00001 factory.go:71] Error trying to work out if we can handle /system.slice/cloud-init-local.service: error inspecting container: No such container: cloud-init-local.service\nI0111 20:20:05.813398 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/cloud-init-local.service\"\nI0111 20:20:05.813414 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/cloud-init-local.service\"\nI0111 20:20:05.813512 00001 manager.go:798] Added container: \"/system.slice/cloud-init-local.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.813591 00001 handler.go:322] Added event &{/system.slice/cloud-init-local.service 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.813642 00001 container.go:368] Start housekeeping for container \"/system.slice/cloud-init-local.service\"\nI0111 20:20:05.814450 00001 factory.go:71] Error trying to work out if we can handle /system.slice/cron.service: error inspecting container: No such container: cron.service\nI0111 20:20:05.814474 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/cron.service\"\nI0111 20:20:05.814494 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/cron.service\"\nI0111 20:20:05.814604 00001 manager.go:798] Added container: \"/system.slice/cron.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.814703 00001 handler.go:322] Added event &{/system.slice/cron.service 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.814769 00001 container.go:368] Start housekeeping for container \"/system.slice/cron.service\"\nI0111 20:20:05.816147 00001 factory.go:78] Using factory \"docker\" for container \"/system.slice/docker-1833c88c617a80f2182b7dd588f119db49175ec75e303f42ba9b715703ddd75f.scope\"\nI0111 20:20:05.817461 00001 manager.go:798] Added container: \"/system.slice/docker-1833c88c617a80f2182b7dd588f119db49175ec75e303f42ba9b715703ddd75f.scope\" (aliases: [container-exporter 1833c88c617a80f2182b7dd588f119db49175ec75e303f42ba9b715703ddd75f], namespace: \"docker\")\nI0111 20:20:05.817893 00001 handler.go:322] Added event &{/system.slice/docker-1833c88c617a80f2182b7dd588f119db49175ec75e303f42ba9b715703ddd75f.scope 2016-01-11 20:16:15.794547335 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.817942 00001 container.go:368] Start housekeeping for container \"/system.slice/docker-1833c88c617a80f2182b7dd588f119db49175ec75e303f42ba9b715703ddd75f.scope\"\nI0111 20:20:05.819276 00001 factory.go:78] Using factory \"docker\" for container \"/system.slice/docker-4dfcb5a1439c7c93fb3ce1ad9a0b35b27ef6b0bf8c916dbd95374d9ae21f4ded.scope\"\nI0111 20:20:05.820637 00001 manager.go:798] Added container: \"/system.slice/docker-4dfcb5a1439c7c93fb3ce1ad9a0b35b27ef6b0bf8c916dbd95374d9ae21f4ded.scope\" (aliases: [grafana 4dfcb5a1439c7c93fb3ce1ad9a0b35b27ef6b0bf8c916dbd95374d9ae21f4ded], namespace: \"docker\")\nI0111 20:20:05.821048 00001 handler.go:322] Added event &{/system.slice/docker-4dfcb5a1439c7c93fb3ce1ad9a0b35b27ef6b0bf8c916dbd95374d9ae21f4ded.scope 2016-01-05 21:55:47.398292817 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.821096 00001 container.go:368] Start housekeeping for container \"/system.slice/docker-4dfcb5a1439c7c93fb3ce1ad9a0b35b27ef6b0bf8c916dbd95374d9ae21f4ded.scope\"\nI0111 20:20:05.822090 00001 factory.go:78] Using factory \"docker\" for container \"/system.slice/docker-ac25b22d15d4097f93b3f7ca2349cb1c0dfe90537d336193f08aeb1d42f71a68.scope\"\nI0111 20:20:05.823574 00001 manager.go:798] Added container: \"/system.slice/docker-ac25b22d15d4097f93b3f7ca2349cb1c0dfe90537d336193f08aeb1d42f71a68.scope\" (aliases: [consul-exporter ac25b22d15d4097f93b3f7ca2349cb1c0dfe90537d336193f08aeb1d42f71a68], namespace: \"docker\")\nI0111 20:20:05.823967 00001 handler.go:322] Added event &{/system.slice/docker-ac25b22d15d4097f93b3f7ca2349cb1c0dfe90537d336193f08aeb1d42f71a68.scope 2015-12-04 08:29:34.425481508 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.824023 00001 container.go:368] Start housekeeping for container \"/system.slice/docker-ac25b22d15d4097f93b3f7ca2349cb1c0dfe90537d336193f08aeb1d42f71a68.scope\"\nI0111 20:20:05.824944 00001 factory.go:71] Error trying to work out if we can handle /system.slice/kmod-static-nodes.service: error inspecting container: No such container: kmod-static-nodes.service\nI0111 20:20:05.824983 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/kmod-static-nodes.service\"\nI0111 20:20:05.824996 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/kmod-static-nodes.service\"\nI0111 20:20:05.825143 00001 manager.go:798] Added container: \"/system.slice/kmod-static-nodes.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.825228 00001 handler.go:322] Added event &{/system.slice/kmod-static-nodes.service 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.825341 00001 container.go:368] Start housekeeping for container \"/system.slice/kmod-static-nodes.service\"\nI0111 20:20:05.826009 00001 factory.go:71] Error trying to work out if we can handle /system.slice/ntp.service: error inspecting container: No such container: ntp.service\nI0111 20:20:05.826029 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/ntp.service\"\nI0111 20:20:05.826039 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/ntp.service\"\nI0111 20:20:05.826185 00001 manager.go:798] Added container: \"/system.slice/ntp.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.826292 00001 handler.go:322] Added event &{/system.slice/ntp.service 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.826389 00001 container.go:368] Start housekeeping for container \"/system.slice/ntp.service\"\nI0111 20:20:05.827069 00001 factory.go:71] Error trying to work out if we can handle /system.slice/rc-local.service: error inspecting container: No such container: rc-local.service\nI0111 20:20:05.827104 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/rc-local.service\"\nI0111 20:20:05.827204 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/rc-local.service\"\nI0111 20:20:05.827427 00001 manager.go:798] Added container: \"/system.slice/rc-local.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.827503 00001 handler.go:322] Added event &{/system.slice/rc-local.service 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.827567 00001 container.go:368] Start housekeeping for container \"/system.slice/rc-local.service\"\nI0111 20:20:05.828282 00001 factory.go:71] Error trying to work out if we can handle /system.slice/system-serial\\x2dgetty.slice: error inspecting container: No such container: system-serial\\x2dgetty.slice\nI0111 20:20:05.828296 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/system-serial\\\\x2dgetty.slice\"\nI0111 20:20:05.828303 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/system-serial\\\\x2dgetty.slice\"\nI0111 20:20:05.828425 00001 manager.go:798] Added container: \"/system.slice/system-serial\\\\x2dgetty.slice\" (aliases: [], namespace: \"\")\nI0111 20:20:05.828502 00001 handler.go:322] Added event &{/system.slice/system-serial\\x2dgetty.slice 2016-01-05 01:35:21.033221591 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.828593 00001 container.go:368] Start housekeeping for container \"/system.slice/system-serial\\\\x2dgetty.slice\"\nI0111 20:20:05.829680 00001 factory.go:71] Error trying to work out if we can handle /system.slice/systemd-sysctl.service: error inspecting container: No such container: systemd-sysctl.service\nI0111 20:20:05.829723 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/systemd-sysctl.service\"\nI0111 20:20:05.829771 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/systemd-sysctl.service\"\nI0111 20:20:05.829999 00001 manager.go:798] Added container: \"/system.slice/systemd-sysctl.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.830141 00001 handler.go:322] Added event &{/system.slice/systemd-sysctl.service 2016-01-05 01:35:21.037221544 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.830191 00001 container.go:368] Start housekeeping for container \"/system.slice/systemd-sysctl.service\"\nI0111 20:20:05.831006 00001 factory.go:71] Error trying to work out if we can handle /system.slice/systemd-udev-trigger.service: error inspecting container: No such container: systemd-udev-trigger.service\nI0111 20:20:05.831024 00001 factory.go:82] Factory \"docker\" was unable to handle container \"/system.slice/systemd-udev-trigger.service\"\nI0111 20:20:05.831035 00001 factory.go:78] Using factory \"raw\" for container \"/system.slice/systemd-udev-trigger.service\"\nI0111 20:20:05.831212 00001 manager.go:798] Added container: \"/system.slice/systemd-udev-trigger.service\" (aliases: [], namespace: \"\")\nI0111 20:20:05.831328 00001 handler.go:322] Added event &{/system.slice/systemd-udev-trigger.service 2016-01-05 01:35:21.037221544 +0000 UTC containerCreation {<nil>}}\nI0111 20:20:05.831403 00001 manager.go:250] Recovery completed\nI0111 20:20:05.831426 00001 container.go:368] Start housekeeping for container \"/system.slice/systemd-udev-trigger.service\"\nI0111 20:20:05.859808 00001 cadvisor.go:94] Starting cAdvisor version: \"0.18.0\" on port 8080\n. Nothing is being logged after starting (which I find strange), but metrics are definitely getting updated, it's being fetched into prometheus and graphed via grafana, all working well except for memory, tasks and IO which are all at zero.\nAnd yes, cAdvisor is running as root (in the container):\n$ docker exec container-exporter ps ax | grep cadvisor\n    1 root     /usr/bin/cadvisor -enable_load_reader=true -logtostderr=true -log_cadvisor_usage=true -v=4\n. So as far as I understand the memory stats are pulled from this https://github.com/opencontainers/runc/blob/master/libcontainer/cgroups/fs/memory.go#L101\nI've scanned the host and container file systems and couldn't find any trace of a memory.stat file, do you know if that could be the cause of the problem?\nIt's weird that no error is reported by cAdvisor tho.\n. I'm wondering if it may be related to this:\n- https://docs.docker.com/engine/articles/runmetrics/\n- https://github.com/docker/docker/issues/4250\n- https://github.com/docker/docker/issues/396\nI'm gonna set these config options and see if that fixed the issue with memory stats.\n. Yep that worked! \nAdding GRUB_CMDLINE_LINUX_DEFAULT=\"cgroup_enable=memory swapaccount=1\" then running update-grub && reboot, then I've got this in the logs:\nI0111 23:21:15.384420 00001 container.go:410] [/system.slice/docker-574001368f978a8e11caa8b471cebda7d741b010eb0b3543b34f3c17837563fd.scope] 0.046 cores (average: 0.068 cores), 33.37 MB of memory\nAnd the memory metrics are being reported properly now.\nThanks a lot for your help, hopefully this will be useful to someone else as well.\n. ",
    "nuxman": "Thanks a Lot @marcellodesales\n. ",
    "jdanielcano": "Thanks for your quick answer. Now it is clear for me that I can not run cAdvisor without being a privileged container.\nActually the need of access to cAdvisor directly comes from a management decision, not for any technical reason, it just a matter of show something.\nThanks for all! I will close the issue!\n. ",
    "aclisp": "0.19.2\n. ",
    "timothysc": "Awesome, thx @jimmidyson for the quick fix. \n. @vishh the linked issue ( kubernetes/kubernetes#19633) contains the complete details.  \ncadvisor can leak goroutines. \n. PTAL comments addressed. \nFor simplicity I'm keeping to the easiest mod and if we need to iterate, then I will follow up after we run another performance evaluation.\n. PTAL.  @vishh \n/cc @jeremyeder\n. :-( \n. Our memory profiles also show what appears to be a slow leak, the tail of the graph below should be flat, but there is a slope. \n\n. So mike has run some experiments(overlay vs. devicemapper) and here is a summary of the findings.\n1. Our experiment for base level overhead of system components needed to shift to using pause containers vs. hello-openshift b/c it skewed the data. \n2. Sum of kubelet + docker 1.10.3 @ 250 pods/node was ~400MB (overlay) and ~500 MB (devicemapper).  All things considered this is not that much memory so I'll close this ticket. \n3. Most of the memory overhead we observed to be device-mapper directly, and @ 250 pods/node, the total in memory difference  ~ 1.2GB .\n. This is breaking nodes in our cluster atm.\n. @vikaschoudhary16 If you have a POC in place.  Recommended to make a [WIP/POC] PR here to get feedback.\n. I think this is cherry-pick worthy fwiw. . @dashpole Can we get this all the way onto 1.8.2?  \n/cc @jpbetz. sgtm \n. @vishh  would prefer 1.1 or 1.2 at minimum. \n. I think your initial starting point should also have jitter in the case of initial burst of containers. \n. k, will converge on 1 till we have more data. \n. I'm ok with just having it here on the 1st iteration, we can revise if needed. \n. I think we should keep the function parameter at minimum, and if we want to remove the constant and simply default the argument that's fine.  But at this point we're splitting hairs.\n. Done. \n. I've left the function signature, but defaulted on the call.\nThis should address the comments... I would like to leave the signature so I can rebuild and exercise during testing.  If at some point if it's deemed unnecessary we can easily collapse as it's a trivial change.\n. could var-block at the top for readability. \n. ",
    "mwielgus": "Ok, if the jitter is not too big (lets say less 1 - 2 sec)\n. fixed\n. @vishh Build infra is failing :). \nError 0: failed to make remote testing directory: command \"gcloud\" [\"compute\" \"ssh\" \"--zone\" \"us-central1-f\" \"e2e-cadvisor-container-vm-v20160127-docker18\" \"--\" \"mkdir\" \"-p\" \"/tmp/cadvisor-31876\"] failed with error: exit status 1 and output: \"ERROR: (gcloud.compute.ssh) Your current active account [211744435552-compute@developer.gserviceaccount.com] does not have any valid credentials\\nPlease run:\\n\\n  $ gcloud auth login\\n\\nto obtain new credentials, or if you have already logged in with a\\ndifferent account:\\n\\n  $ gcloud config set account ACCOUNT\\n\\nto select an already authenticated account to use. \\n\"\nError 1: failed to make remote testing directory: command \"gcloud\" [\"compute\" \"ssh\" \"--zone\" \"us-central1-f\" \"e2e-cadvisor-rhel-7-docker19\" \"--\" \"mkdir\" \"-p\" \"/tmp/cadvisor-31876\"] failed with error: exit status 1 and output: \"ERROR: (gcloud.compute.ssh) Your current active account [211744435552-compute@developer.gserviceaccount.com] does not have any valid credentials\\nPlease run:\\n\\n  $ gcloud auth login\\n\\nto obtain new credentials, or if you have already logged in with a\\ndifferent account:\\n\\n  $ gcloud config set account ACCOUNT\\n\\nto select an already authenticated account to use. \\n\"\nBuild step 'Execute shell' marked build as failure\n. @vishh Can you rerun the tests?\n. Still the same problems with gcloud. Can we merge this PR based on the unit test resuts?\n. This is the number of custom metrics per scrape. It has nothing to do with 2 minutes.\n. done\n. Just picked a relatively sane number that won't hurt most of the existing users.\n. This is a collector for user defined metrics. There are two of them \n- generic - that can consume arbitrary metric via a regexp\n- prometheus - that requires that the metrics are in prometheus format\n. Probably no, but I placed this check to be consistent with the prometheus code. Can be removed if you wish.\n. CustomMetrics in Kubernetes will have it set to much lower value (5 or so).\n. flag renamed.\n. Removed.\n. My impression was that it would still calculate regular metrics even if something was broken with the application/custom metrics. But apparently the whole monitoring collapse if the application metrics are bad and collector is not created. I can either remove this:\nhttps://github.com/google/cadvisor/blob/master/manager/manager.go#L787\nor remove checks during collector creation and leave only these in housekeeps. \n. Removed.\n. ",
    "g-dury": "Hi, thanks for the fast answer. No root I'm running it as a privileged container (run --volume=/:/rootfs:ro   --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro --publish=8080:8080 --detach=true --name=cadvisor --privileged=true  google/cadvisor:latest)\nI am wondering if this is not an issue with my docker configuration...as I was trying also sysdig cloud for monitoring, I had the same problem: no container names... their support suggested that I added to my docker OPTS (in /etc/default/docker to start as a service) \"-H tcp://0.0.0.0:4243 -H unix:///var/run/docker.sock\" and it is working... even with only \"-H unix:///var/run/docker.sock\" working but as I want super tight security wondering if this is not a security flaw ?\nRight now I am running my docker with only opts \"-H=0.0.0.0:2376\" as recommended here:\nhttps://docs.docker.com/engine/articles/https/\nSorry I know this a bit out of Cadvisor scope..\nThanks again for the help.\nEDIT: I think I find my answer here https://docs.docker.com/engine/reference/commandline/daemon/\nI didn't start docker with the unix socket that is why Cadvisor cannot read it I guess...\n. Just in case, indeed Cadvisor need the unix socket to scrape the names of containers. Thanks jimmydyson, I'll close the issue !\n. ",
    "krhubert": "I can create PR in next 2 days.\n. I signed it!\n. I see, but don't know how this test is running. I have run it localy and everthing was fine (log below).\nAs i can see from details:\nI0120 19:43:51.128928   26521 runner.go:123] Running integration tests targeting \"e2e-cadvisor-ubuntu-trusty-docker19\"...\nI0120 19:44:49.865380   26521 runner.go:153] Execution time 1m54.149321223s\nF0120 19:44:49.865469   26521 runner.go:246] Error 0: timed out waiting for cAdvisor to come up at host \"e2e-cadvisor-rhel-7-docker19\"\nBuild step 'Execute shell' marked build as failure\nThe integrantion tests are falling, cAdvisor didn't come up, but there is no logs about why ... :/\n```\n[root@dev /usr/src/go/src/github.com/google/cadvisor (master)] git log -1 \ncommit 7d053578d455b71cc4dc68642ebe00dd99e15262\nAuthor: Hubert Krauze krhubert@gmail.com\nDate:   Wed Jan 20 08:46:42 2016 +0100\nUse uint64 for Memory Stats\n\n[root@dev /usr/src/go/src/github.com/google/cadvisor (master)] make test; echo $?\n\n\nrunning tests\n?       github.com/google/cadvisor  [no test files]\nok      github.com/google/cadvisor/api  1.031s\n?       github.com/google/cadvisor/cache    [no test files]\nok      github.com/google/cadvisor/cache/memory 1.021s\nok      github.com/google/cadvisor/client   1.063s\n?       github.com/google/cadvisor/client/clientexample [no test files]\nok      github.com/google/cadvisor/client/v2    1.042s\nok      github.com/google/cadvisor/collector    1.033s\nok      github.com/google/cadvisor/container    1.018s\n?       github.com/google/cadvisor/container/docker [no test files]\nok      github.com/google/cadvisor/container/libcontainer   1.044s\nok      github.com/google/cadvisor/container/raw    1.024s\nok      github.com/google/cadvisor/events   1.034s\nok      github.com/google/cadvisor/fs   1.042s\n?       github.com/google/cadvisor/healthz  [no test files]\n?       github.com/google/cadvisor/http [no test files]\n?       github.com/google/cadvisor/http/mux [no test files]\nok      github.com/google/cadvisor/info/v1  1.014s\n?       github.com/google/cadvisor/info/v1/test [no test files]\nok      github.com/google/cadvisor/info/v2  1.018s\n?       github.com/google/cadvisor/integration/common   [no test files]\n?       github.com/google/cadvisor/integration/framework    [no test files]\n?       github.com/google/cadvisor/integration/runner   [no test files]\nok      github.com/google/cadvisor/integration/tests/api    1.024s\nok      github.com/google/cadvisor/integration/tests/healthz    1.018s\nok      github.com/google/cadvisor/manager  1.157s\nok      github.com/google/cadvisor/metrics  1.116s\n?       github.com/google/cadvisor/pages    [no test files]\n?       github.com/google/cadvisor/pages/static [no test files]\n?       github.com/google/cadvisor/storage  [no test files]\n?       github.com/google/cadvisor/storage/bigquery [no test files]\n?       github.com/google/cadvisor/storage/bigquery/client  [no test files]\n?       github.com/google/cadvisor/storage/bigquery/client/example  [no test files]\n?       github.com/google/cadvisor/storage/elasticsearch    [no test files]\n?       github.com/google/cadvisor/storage/influxdb [no test files]\n?       github.com/google/cadvisor/storage/redis    [no test files]\n?       github.com/google/cadvisor/storage/statsd   [no test files]\n?       github.com/google/cadvisor/storage/statsd/client    [no test files]\n?       github.com/google/cadvisor/storage/stdout   [no test files]\n?       github.com/google/cadvisor/storage/test [no test files]\nok      github.com/google/cadvisor/summary  1.046s\nok      github.com/google/cadvisor/utils    1.017s\n?       github.com/google/cadvisor/utils/cloudinfo  [no test files]\n?       github.com/google/cadvisor/utils/cpuload    [no test files]\n?       github.com/google/cadvisor/utils/cpuload/netlink    [no test files]\n?       github.com/google/cadvisor/utils/cpuload/netlink/example    [no test files]\nok      github.com/google/cadvisor/utils/machine    1.023s\nok      github.com/google/cadvisor/utils/oomparser  1.095s\n?       github.com/google/cadvisor/utils/oomparser/oomexample   [no test files]\n?       github.com/google/cadvisor/utils/procfs [no test files]\n?       github.com/google/cadvisor/utils/sysfs  [no test files]\n?       github.com/google/cadvisor/utils/sysfs/fakesysfs    [no test files]\nok      github.com/google/cadvisor/utils/sysinfo    1.014s\nok      github.com/google/cadvisor/validate 1.041s\n?       github.com/google/cadvisor/version  [no test files]\n\n\n0\n```\n. @vishh Can you help me with that?\n. I know it breaks API, but it can't be done in any other way :( and also int64 is too small for current memory size...\n. ",
    "dmehra": "Correct, when I set -storage_driver_buffer_duration=2s the actual delay is 2 minutes, not seconds. \n. When I tried running again with -storage_driver_buffer_duration=2s on a newer canary build, it worked as expected, doing writes to influx every 2 seconds. I cannot explain the earlier behavior; closing this issue as not reproducible.\n. ",
    "damianopezzotti": "To reproduce the scenario you have to use Docker 1.9.1.\nCreate a new network\ndocker network create test_cAdvisor\nCreate a container attached to the network\ndocker run -it --rm --name=myContainer --net=test_cAdvisor debian:latest bash\nAnd perform a lot of network traffic using curl or wget\nIf you inspect the network (docker network inspect test_cAdvisor) or inspect the container (docker inspect myContainer)\nYou can see that container is attached to the network.\nThanks\nDamy\n. It works!\n. When will you release the fix on the docker hub?\n2016-01-27 10:01 GMT+01:00 Jimmi Dyson notifications@github.com:\n\nThanks for reporting back @damianopezzotti\nhttps://github.com/damianopezzotti!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1076#issuecomment-175494856.\n\n\nDamiano Pezzotti\n...................................... . .\nSoftware Architect | Alambitco\nmobile +39 333 2116258\nfax +39 02 700432209\nemail damy@alambitco.com\nweb www.alambitco.com\nflickr www.flickr.com/photos/damianopezzotti\nlinkedin www.linkedin.com/in/damianopezzotti\npgp id 0x12DFD989\n. Great.\nThanks\n2016-01-27 10:07 GMT+01:00 Jimmi Dyson notifications@github.com:\n\njust waiting for another runc tag & update godeps with that & i will\nrelease ASAP after that - not too long i hope.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1076#issuecomment-175497358.\n\n\nDamiano Pezzotti\n...................................... . .\nSoftware Architect | Alambitco\nmobile +39 333 2116258\nfax +39 02 700432209\nemail damy@alambitco.com\nweb www.alambitco.com\nflickr www.flickr.com/photos/damianopezzotti\nlinkedin www.linkedin.com/in/damianopezzotti\npgp id 0x12DFD989\n. ",
    "Scorpiion": "Hi @timstclair, if you refer to the merging of #969, yes it is. #969 does not implement this.\nSee this discussion: \nhttps://github.com/google/cadvisor/pull/969#discussion-diff-46494813\n. Any updates on this? Is it complicated to add support for this in the web UI?. Okay, I understand, thanks for the quick reply @vishh.. ",
    "zaa": "Could you please add a comment or note why filesystem usage should not be collected for devicemapper storage?\n. ",
    "enoodle": "what is the Authorized Contributors Group for the CLA ?\n. @simon3z \n. @deads2k can you review this? Thanks\n. @deads2k Nothing more to implement. It was wip for the dependencies.\n. @deads2k I will rebase to change the dependencies commit name in a moment.\n. @vishh ping? you actually lgtm'ed it before, I only added a tiny fix to remove \\n char when reading the azure instance ID from file.\n. @vishh I didn't find any clear way to differentiate openstack from AWS with the REST API. I think a naive user could easily set an openstack availability-zone to the same name as the ones from AWS or give a public hostname that includes the string \"amazon\" or \"aws\".\n. @k8s-bot test again\n. @timstclair I fixed your comments.\n. rebased\n. I think detectCloudProvider could take some time. The others will be almost immediate as we are accessing the correct API server. In short this function could block for some time and its users should be aware of this as it was already blocking before this patch.\n. No. I couldn't find any api service comparable to aws/gce 's for Azure, nor any other official documentation on ways to detect if an instance is running on Azure. I did find [1] which explains how to use dmidecode to get the instance ID (hence /sys/class/dmi/id/produect_uuid) and I noticed that on all my instances I had that string for sys_vendor so I used this.\n[1] https://azure.microsoft.com/en-us/blog/accessing-and-using-azure-vm-unique-id/\n. ",
    "deads2k": "@googlebot @enoodle is covered under the Red Hat CLA.\n. @enoodle When you add the godep packages, can you name the commit bump(github.com/aws/aws-sdk-go) 72440a9c5a884936f1c679591fcf04e31a9b772a (that looks like what you were trying to get).  If the rest are just transitive that's fine, but that name will help track back to the target.\n. Is this still a WIP?  If so, what is remaining?\n. @vishh Implementation is done.  This enables fallback behavior for ProviderID determination in the kubelet (see https://github.com/kubernetes/kubernetes/pull/21373).  Take another look?\n. @jimmidyson ?\n. ",
    "lucevers": "Problem Found:\nI see that the webtool makes connections to the Google website and this was blocked by the local PC.\n. ",
    "hridyeshpant": "yes @vishh i am referring the UI page. I tried removing \"/var/lib/docker/:/var/lib/docker:ro\" in my command line, but still Cadvisor Ui is not showing subContainers section . Same is working fine with docker 1.7.1. Please see the image attached, missing subContainers when running with 1.9.1 in UI.\n. @vishh is there docker version for  v0.22.0 . i am using google/cadvisor image \n. ",
    "raykrueger": "On amazon linux I added --volume=/cgroup:/sys/fs/cgroup:ro to get this working again.\n. ",
    "upccup": "Use latest cadvistor the error  Couldn't collect info from any of the files in \"/etc/machine-id,/var/lib/dbus/machine-id\" * disappeared.  But access */api/v1.3/docker/ Still no data\n. cadvisor log \n[root@localhost ~]# docker logs cadvisor\nI0217 05:51:13.600956       1 storagedriver.go:45] Caching stats in memory for 2m0s\nI0217 05:51:13.601152       1 manager.go:131] cAdvisor running in container: \"/docker/4c488b997944a44fafcb84e9aa062212f1dc29fb9c34be7efa578575ca834205\"\nI0217 05:51:13.797401       1 fs.go:107] Filesystem partitions: map[/dev/vda1:{mountpoint:/rootfs major:253 minor:1 fsType: blockSize:0} /dev/vdb:{mountpoint:/rootfs/data major:253 minor:16 fsType: blockSize:0} /dev/mapper/docker-253:1-67218380-6d36af56e65ad2786e039c527a413e0052783c4a3797f2d4c06b80edab9d1cbb:{mountpoint:/ major:252 minor:27 fsType: blockSize:0}]\nI0217 05:51:13.810076       1 manager.go:166] Machine: {NumCores:2 CpuFrequency:2593748 MemoryCapacity:3976708096 MachineID:8d1fcbed219ea3264af0402615579c5a SystemUUID:08940C1D-6E60-4C18-9E9F-F1E152985AEB BootID:e1c32c8e-11fa-40ef-ac4f-021eb6a5018e Filesystems:[{Device:/dev/mapper/docker-253:1-67218380-6d36af56e65ad2786e039c527a413e0052783c4a3797f2d4c06b80edab9d1cbb Capacity:105554829312} {Device:/dev/vda1 Capacity:20961034240} {Device:/dev/vdb Capacity:107321753600}] DiskMap:map[252:25:{Name:dm-25 Major:252 Minor:25 Size:107374182400 Scheduler:none} 252:5:{Name:dm-5 Major:252 Minor:5 Size:107374182400 Scheduler:none} 252:8:{Name:dm-8 Major:252 Minor:8 Size:107374182400 Scheduler:none} 252:11:{Name:dm-11 Major:252 Minor:11 Size:107374182400 Scheduler:none} 252:14:{Name:dm-14 Major:252 Minor:14 Size:107374182400 Scheduler:none} 252:17:{Name:dm-17 Major:252 Minor:17 Size:107374182400 Scheduler:none} 252:24:{Name:dm-24 Major:252 Minor:24 Size:107374182400 Scheduler:none} 252:30:{Name:dm-30 Major:252 Minor:30 Size:107374182400 Scheduler:none} 252:32:{Name:dm-32 Major:252 Minor:32 Size:107374182400 Scheduler:none} 252:16:{Name:dm-16 Major:252 Minor:16 Size:107374182400 Scheduler:none} 252:22:{Name:dm-22 Major:252 Minor:22 Size:107374182400 Scheduler:none} 252:10:{Name:dm-10 Major:252 Minor:10 Size:107374182400 Scheduler:none} 252:13:{Name:dm-13 Major:252 Minor:13 Size:107374182400 Scheduler:none} 252:15:{Name:dm-15 Major:252 Minor:15 Size:107374182400 Scheduler:none} 252:4:{Name:dm-4 Major:252 Minor:4 Size:107374182400 Scheduler:none} 252:7:{Name:dm-7 Major:252 Minor:7 Size:107374182400 Scheduler:none} 253:0:{Name:vda Major:253 Minor:0 Size:21474836480 Scheduler:deadline} 252:18:{Name:dm-18 Major:252 Minor:18 Size:107374182400 Scheduler:none} 252:21:{Name:dm-21 Major:252 Minor:21 Size:107374182400 Scheduler:none} 252:23:{Name:dm-23 Major:252 Minor:23 Size:107374182400 Scheduler:none} 252:9:{Name:dm-9 Major:252 Minor:9 Size:107374182400 Scheduler:none} 252:0:{Name:dm-0 Major:252 Minor:0 Size:107374182400 Scheduler:none} 252:2:{Name:dm-2 Major:252 Minor:2 Size:107374182400 Scheduler:none} 252:29:{Name:dm-29 Major:252 Minor:29 Size:107374182400 Scheduler:none} 253:16:{Name:vdb Major:253 Minor:16 Size:107374182400 Scheduler:deadline} 252:12:{Name:dm-12 Major:252 Minor:12 Size:107374182400 Scheduler:none} 252:19:{Name:dm-19 Major:252 Minor:19 Size:107374182400 Scheduler:none} 252:28:{Name:dm-28 Major:252 Minor:28 Size:107374182400 Scheduler:none} 252:27:{Name:dm-27 Major:252 Minor:27 Size:107374182400 Scheduler:none} 252:3:{Name:dm-3 Major:252 Minor:3 Size:107374182400 Scheduler:none} 252:33:{Name:dm-33 Major:252 Minor:33 Size:107374182400 Scheduler:none} 252:6:{Name:dm-6 Major:252 Minor:6 Size:107374182400 Scheduler:none} 252:1:{Name:dm-1 Major:252 Minor:1 Size:107374182400 Scheduler:none} 252:20:{Name:dm-20 Major:252 Minor:20 Size:107374182400 Scheduler:none} 252:26:{Name:dm-26 Major:252 Minor:26 Size:107374182400 Scheduler:none}] NetworkDevices:[{Name:eth0 MacAddress:52:54:00:97:34:b8 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:4294537216 Cores:[{Id:0 Threads:[0] Caches:[]}] Caches:[]} {Id:1 Memory:0 Cores:[{Id:0 Threads:[1] Caches:[]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown}\nI0217 05:51:13.811270       1 manager.go:172] Version: {KernelVersion:3.10.0-123.4.4.el7.x86_64 ContainerOsVersion:Alpine Linux v3.2 DockerVersion:1.10.1 CadvisorVersion:0.21.0 CadvisorRevision:ed56482}\nI0217 05:51:13.811943       1 factory.go:252] System is using systemd\nI0217 05:51:14.016195       1 factory.go:304] Registering Docker factory\nI0217 05:51:14.020538       1 factory.go:94] Registering Raw factory\nI0217 05:51:14.278675       1 manager.go:1000] Started watching for new ooms in manager\nW0217 05:51:14.279220       1 manager.go:239] Could not configure a source for OOM detection, disabling OOM events: exec: \"journalctl\": executable file not found in $PATH\nI0217 05:51:14.280512       1 manager.go:252] Starting recovery of all containers\nI0217 05:51:14.302366       1 manager.go:257] Recovery completed\nI0217 05:51:14.329203       1 cadvisor.go:106] Starting cAdvisor version: 0.21.0-ed56482 on port 8080\n. @vishh @adimania with v0.22 it works good\n. @timstclair  thx for reply. i ask about a command-line option to specify which containers cAdvisor should monito\n. i mean the filter condition is like docker label \"AAAA=bbbb\",  is the possible?\n. Because now  if cadvisor get one of container monitor data failed, it did nor continue so i can not get other container info. But  the error container maybe i did not care, so i want to ignore it.\n. ",
    "adimania": "I am facing the same problem. Is there any workaround?\n. ",
    "aaronlevy": "I have a CLA signed by employer (CoreOS). cla bot on kubernetes repo picks it up (or at least it has in the past).\n. I'm pretty unfamiliar with this code, so apologies for the PR that supposedly would break everything :)\nWhat I'm seeing is that cadvisor (in kubelet) was not adding a \"docker-images\" label, because the rootfs (from the kubelet's perspective) was overlayfs. The kubelet would then keep logging non-existent label \"docker-images\".\nA quick test of the change in this PR resolved that issue, and the cadvisor output of /api/2.0/storage seemed reasonable (not adding a bunch of overlay filesystems). But again, really unfamiliar with what to look for.\nAny thoughts on a way to move forward?\n. ps: may not have been clear, but I also added a new commit to this PR that takes a new approach / fixes the issues I was seeing running kubelet in a container. /cc @sjpotter @vishh \n. ",
    "philips": "lgtm\n. The wait is only annoying because we can't tell if a cgroup should wait on rkt metadata. In the case of rkt it needs to wait; but in other cases it doesn't need to wait.\nTo fix this we should just put explicitly in a unit file that the type is rkt then we can have cAdvisor parse it.\nFor example: \n```\n[Service]\nExecStart=/bin/sleep 3000\n[X-cAdvisor]\nType=rkt\n```\nWorking code to talk to systemd and grab this metadata:\nhttps://gist.github.com/philips/07aae9eea7ffddd75cf0c1d6ecf839b1\n. Do you have a concrete example and what the prefixing solution would look like? Maybe a prometheus bug report URL?\n. @vishh fixed and removed the IRC channel @timstclair \n. @timstclair rebased.\n. @timstclair should we really let the robots have this much control?!\n. @mrunalp What do you mean by \"Now, when we bind mount /sys:/sys we are overwriting that with the order on the system which happens to be the opposite on RHEL and hence the issue?\"\n. ",
    "TeChn4K": "Same problem here. Any update ?\n. ",
    "studna": "Same here. I've upgraded Docker 1.9 -> 1.11 and now I can't see container names in influxdb. Any update? @vishh @marcellodesales @TeChn4K \n. ",
    "mrgleam": "Me too. i've upgraded to 1.11.1\nI can't see container name in influxdb and i can't see network interface in cadvisor.\nPlease help me.\nThank you.\n. I used cadvisor tag :lastest\nNot work for me :(\n. Ok this work. Tahnk you very much.\n. ",
    "biswars": "Having the same issue in kubernetes setup. As cadvisor is embedded in kubelet binary, how can the cadvisor be upgraded in the kubernetes setup ?\n. Thanks @timstclair - very useful.\n. ",
    "itoffshore": "you can also install GNU wget & ca-certificates to get support for https (in Alpine Linux containers with busybox)\n. ",
    "ljha-CS": "I got this error while downloading gradle using wget, fixed this by installing openssl package\n. ",
    "tvalasek": "apk update && apk add ca-certificates && update-ca-certificates && apk add openssl\n. ",
    "listx": "@tvalasek Can this be just apk add --update ca-certificates openssl && update-ca-certificates ?\n. ",
    "johnwesley": "apk add --update openssl\n. ",
    "AndreLouisCaron": "I don't necessarily want the containers to be sorted.\nI would mostly like a timeline graph showing CPU usage by container.  The home page shows this type of graph, but the usage is by CPU core (which only helps determine whether the docker daemon is distributing containers equally across all cores).\nThere is a detailed view by container where you can see the CPU usage for a single container, so I guess the information is already available and that we only need to combine the different \"feeds\" into a single graph on the home page.\nI might consider putting some time into this, but I don't know anything about Go...\n. Did a quick look into this today.  If I understand this correctly, pages/containers_html.go contains the page markup and pages/static/containers_js.go contains the JS code that powers it.  I don't see any tests for these files, are they stored elsewhere?\nAlso, sorry if this is a newbie question (I'm more fluent in Python), but why is the all the HTML and JS code in strings in Go files and how do you get syntax highlighting in your editor for this?\n. ",
    "rochdev": "I am getting the same issue.\nDocker Version 1.10.2\nKernel Version 4.1.18-boot2docker\nOS Version Boot2Docker 1.10.2 (TCL 6.4.1); master : 611be10 - Mon Feb 22 22:47:06 UTC 2016\n. @vishh I am using the latest version using the latest tag. I have just pulled the image again and recreated the container and I am still getting the issue.\n. I no longer get the error with the latest version. Thanks @vishh !\n. ",
    "lucacri": "Same error for me running on Docker version 1.10.3, build 20f81dd\n. ",
    "AKA-bingo": "I am getting the same issue with follow message on centos:\nW1031 15:50:40.441065       1 container.go:507] Failed to update stats for container \"/system.slice/docker-88af1bd465a9597c3e531ff1f5f1a75adffe65313e83f6556c43446a47f4ff11.scope\": failed to get load stat for \"/system.slice/docker-88af1bd465a9597c3e531ff1f5f1a75adffe65313e83f6556c43446a47f4ff11.scope\" - path \"/sys/fs/cgroup/cpuacct,cpu/system.slice/docker-88af1bd465a9597c3e531ff1f5f1a75adffe65313e83f6556c43446a47f4ff11.scope\", error failed to open cgroup path /sys/fs/cgroup/cpuacct,cpu/system.slice/docker-88af1bd465a9597c3e531ff1f5f1a75adffe65313e83f6556c43446a47f4ff11.scope: \"open /sys/fs/cgroup/cpuacct,cpu/system.slice/docker-88af1bd465a9597c3e531ff1f5f1a75adffe65313e83f6556c43446a47f4ff11.scope: no such file or directory\"\ncAdvisor version: v0.31.0-fc17731a\ndocker Version: 1.13.1\nBut when i check the folder in contains, it show the file already exist.\n/sys/fs/cgroup/cpu,cpuacct/system.slice/docker-88af1bd465a9597c3e531ff1f5f1a75adffe65313e83f6556c43446a47f4ff11.scope # cd ..\n/sys/fs/cgroup/cpu,cpuacct/system.slice # stat docker-88af1bd465a9597c3e531ff1f5f1a75adffe65313e83f6556c43446a47f4ff11.scope\n  File: docker-88af1bd465a9597c3e531ff1f5f1a75adffe65313e83f6556c43446a47f4ff11.scope\n  Size: 0               Blocks: 0          IO Block: 4096   directory\nDevice: 1bh/27d Inode: 388002743   Links: 2\nAccess: (0755/drwxr-xr-x)  Uid: (    0/    root)   Gid: (    0/    root)\nAccess: 2018-10-31 15:51:04.000000000\nModify: 2018-10-31 15:50:40.000000000\nChange: 2018-10-31 15:50:40.000000000\nMy docker run command :\ndocker run -itd --restart=always \\  \n    --volume=/:/rootfs:ro \\  \n    --volume=/var/run:/var/run:ro \\  \n    --volume=/sys:/sys:ro \\  \n    --volume=/var/lib/docker/:/var/lib/docker:ro \\  \n    --volume=/cgroup:/cgroup:ro \\  \n    --volume=/etc/localtime:/etc/localtime:ro  \\  \n    --detach=true \\  \n    --name=cadvisor \\  \n    --net=host \\  \n    --privileged=true \\  \n    google/cadvisor:v0.31.0 \\  \n    --disable_metrics=udp \\  \n    --enable_load_reader=true  \\  \n    --port=8803. ",
    "fahimeh2010": "thanks, but where in the its interface?\n. I downloaded the latest image on docker hub.(google/cadvisor:latest)\n. Same here. we use promethous and cadvisor to monitor containers . We can not view container_fs_io_current per containers.\n. same here, use overlay2 as docker filesystem. @jaylinski it works for you?. ",
    "derekwaynecarr": "I can update the comment in morning.\nIn general, I would love a solution that does not block for even 5s like\nthis so we can visit the alternate approaches post 1.2.\nThanks!\nOn Tuesday, March 8, 2016, Justin Santa Barbara notifications@github.com\nwrote:\n\nLGTM, except I don't think there's any DNS involved, as on EC2 the\nmetadata service is on 169.254.169.254. If I'm correct the comment is a bit\nmisleading...\nThere is also this alternative approach, though I haven't tried it in\nproduction (it seems to be accurate though):\nhttp://serverfault.com/a/700771\nI'm fine with PR as is. I don't think I've ever seen the metadata service\nfail to respond... I'm pretty sure it is implemented on the host machine,\nso it seems the best option, particularly if we want to get it into 1.2\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1152#issuecomment-194067573.\n. @vishh updated the comment to try to improve clarity.\n. @vishh updated to 2s timeout.\n\nsudo ./cadvisor -logtostderr\nI0309 16:26:12.040451   23094 storagedriver.go:45] Caching stats in memory for 2m0s\nI0309 16:26:12.040596   23094 manager.go:168] cAdvisor running in container: \"/\"\nI0309 16:26:12.181361   23094 fs.go:109] Filesystem partitions: map[/dev/mapper/fedora-root:{mountpoint:/ major:253 minor:2 fsType: blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType: blockSize:0} /dev/mapper/fedora-home:{mountpoint:/home major:253 minor:3 fsType: blockSize:0}]\nI0309 16:26:14.241805   23094 manager.go:205] Machine: { ...\n. LGTM, thanks!\n. @ncdc :+1: \n. LGTM\n. @vishh - I think this should be cherry-picked into kube 1.2.  You agree?\n. @vishh - I will pull and test now.\n. @ncdc - i have it built locally, should take a few more secs for me to verify in the interest of safety.\n. LGTM, confirmed we are getting stats 1 time at the expected default 1s interval.\n. LGTM\n. :+1:  - this also removes a timeout delay at start-up...\n. I was getting errors like the following from here:\nhttps://github.com/kubernetes/kubernetes/blob/master/Godeps/_workspace/src/github.com/google/cadvisor/cache/memory/memory.go#L104\nunable to find data for container /system.slice/var-lib-docker-devicemapper-mnt-6b4fa4a130bbfa32390d93562453da45350787058d449b15ee1f75c1a56a1c1f\\x2dinit.mount\n. @vishh - that works, is there a particular interface you have in mind that we could implement?\n. I need to look into this some more, but isn't the issue that the raw container handler is what would need to be updated?\n. So the factories are registered in-order, if I have a systemd factory registered before the raw factory, are you saying I could say that the systemd can handle .mount cgroups, but it doesn't accept them and the raw factory would just ignore them as a consequence? \n. I guess that is what the code currently is doing... ok, I can buy that this may work.\n. Yeah, I want to add more to it as time permits\nOn Friday, April 22, 2016, Vish Kannan notifications@github.com wrote:\n\nYeah. The raw drive is the fallback option. I'd like to see more systemd\nspecific code go into the systemd handler.\nOn Wed, Apr 20, 2016 at 11:27 AM, Derek Carr notifications@github.com\n<javascript:_e(%7B%7D,'cvml','notifications@github.com');>\nwrote:\n\nI guess that is what the code currently is doing... ok, I can buy that\nthis may work.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1211#issuecomment-212546039\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/issues/1211#issuecomment-213585150\n. From what I can gather, fsInfo is computed on demand, so no separate housekeeping interval is needed there, but @pmorie has informed me that thin_ls data per container is cached.  Either way for out of resource killing, we care more about rootfs available bytes and imagefs available bytes.\n. @vishh @timstclair\nMy understanding is that machine stats are derived by the root container stats:\n\nhttps://github.com/google/cadvisor/blob/master/api/versions.go#L483\nI will throw another wrinkle in here, and broaden the request.\nI suspect when we get a little further into the future, we will want to get the stats for certain special containers at a higher frequency.  For example, the kubelet container, the docker container, and the container that parents end-user pods are all special, and should reasonably be able to ask for higher fidelity housekeeping intervals.\nThe container that parents end-user pods is probably the container that we will want to drive eviction on when we move to pod-level cgroups world.\nSo I want to come back and re-phrase my request, I want to be able to tell cadvisor a set of special containers that have a shorter housekeeping interval.  I am fine not exposing it as a flag to the binary, but I would like to be able to specify it for how Kubernetes starts its internal cadvisor.\nThoughts?\n. @timstclair - I am happy to defer to what you and @vishh think is best, you have more expertise in this area then me, just wanted to state where my confusion came from as all things looked derived from the cached root container.  If the desire is to support on-demand scraping instead, that works for me because I get the same net result as the caller.  Any suggestions on how you would want to see this implemented?  I am volunteering my time because I think this is needed to make evictions actually useful in Kubernetes to end users without having to sacrifice large amounts of reserved memory on the node.\n. The bigger issue I see is not being able to report file system usage for a\ncontainer should not cause all other stats to be inaccessible by cAdvisor\nconsumers for that container.\nOn Monday, June 20, 2016, Seth Jennings notifications@github.com wrote:\n\n@ncdc https://github.com/ncdc He kept saying we should use cli tools to\ndetermine the metadata device, which is a mess here (cli tools not\ninstalled, capturing/parsing output, etc)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1338#issuecomment-227251697,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AF8dbF2HQBOKD5ak_slQOUwnzVp-CuWDks5qNvDqgaJpZM4I6BzT\n.\n. I think this needs to make 1.3 as without it pod stats are not reported at all.\n. LGTM\n. Will rebase this to https://github.com/google/cadvisor/pull/1368\n. This is now rebased to https://github.com/google/cadvisor/pull/1368 , PTAL\n. /cc @timstclair \n. Nag :-).  I want to consume this in Kubelet this week if possible to\nsupport inode based eviction.\n\nOn Tuesday, July 26, 2016, Kubernetes Bot notifications@github.com wrote:\n\nJenkins GCE e2e\nBuild/test passed for commit 7708bb5\nhttps://github.com/google/cadvisor/commit/7708bb5f9ef419aab7dd71b1c940c66eae5d21f1\n.\n- Build Log\n  https://storage.cloud.google.com/kubernetes-jenkins/pr-logs/pull/1365/cadvisor-pull-build-test-e2e/766/build-log.txt\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1365#issuecomment-235345021, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AF8dbGb7IkymASeAohFAMMaqM6hzyVH5ks5qZkWvgaJpZM4JFfuD\n.\n. @ronnielai - what kind of test do you recommend?  I see no obvious pattern to integrate with existing tests...\n. ok, will add unit test tomorrow in fsToFsStats\n. @ronnielai - added a unit test.\n. To summarize:\n- inodes and inodes_free in v2.FsInfo will be pointers\n- inodes and inodes_free will switch to use pointers in fs.Fs\n- we will not change v1.FsStats.InodesFree (which means it will be wrong for devicemapper)\n- we will not change v1.FsInfo.Inodes (which means it will be wrong for devicemapper)\n- kube will use the v2 API to handle inodes related info\n\nI will send a PR today or tomorrow.\n. In trying to fix this:\nhttps://github.com/google/cadvisor/blob/master/manager/manager.go#L690\nInternally, cadvisor InMemoryCache is working against the v1 API.\nAn option I thought about is the following:\n- inodes and inodes_free in v2.FsInfo will be pointers\n- inodes and inodes_free will switch to use pointers in fs.Fs\n- add HasInodes bool to v1.FsStats\n- add HasInodes bool to v1.FsInfo\nThe above would allow us to change the code here:\nhttps://github.com/google/cadvisor/blob/master/manager/manager.go#L690\nIt has advantages:\n- we can convert v1 to v2 format object correctly and vice-versa while limit code churn\n- we can return the proper value for devicemapper in v1 api\n@vishh - please ack\n. @timstclair - this should be good to go now.  verified all API endpoints on a host with and without devicemapper.  note I also cleaned up some other iterators to ensure we don't get a repeat of the earlier bug we saw on referencing values.  PTAL\n. Thanks... it took most of my afternoon to track down ;-)  I need :beers:\n. /cc @pmorie @ncdc @sjenning \n. Looks like the same error that is fixed by this pending PR when running\ncAdvisor in a container.\nhttps://github.com/google/cadvisor/pull/1474\nOn Wednesday, September 21, 2016, Ripunjay Godhani notifications@github.com\nwrote:\n\nI am also getting same error\nI0921 10:51:49.077889 1 storagedriver.go:50] Caching stats in memory for\n2m0s\nI0921 10:51:49.079061 1 manager.go:140] cAdvisor running in container:\n\"/docker/f388d0af1e7af95910b978cf5952457ce33f75aa8338613f1db680be5d6c7c1c\"\nW0921 10:51:49.088118 1 manager.go:148] unable to connect to Rkt api\nservice: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441:\ngetsockopt: connection refused\nI0921 10:51:49.095053 1 fs.go:116] Filesystem partitions:\nmap[/dev/mapper/docker-253:1-9144532-367762f1736f18be685e3b77748876\nf38ab6b65355943fadb060be71c8676c70:{mountpoint:/ major:252 minor:2\nfsType:xfs blockSize:0} /dev/vda1:{mountpoint:/var/lib/docker/devicemapper\nmajor:253 minor:1 fsType:xfs blockSize:0} /dev/mapper/docker-253:1-\n9144532-c92bccf7a2ec59be6d94486cf7e58e346e305fe5f986ba89ae85e54b8443\na2c9:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/\nc92bccf7a2ec59be6d94486cf7e58e346e305fe5f986ba89ae85e54b8443a2c9\nmajor:252 minor:1 fsType:xfs blockSize:0}]\nI0921 10:51:49.097158 1 manager.go:195] Machine: {NumCores:2\nCpuFrequency:2600000 MemoryCapacity:8004255744 MachineID:\n1d188a370466406fbb093d606a0478eb SystemUUID:5271A133-2134-45B0-96FB-0C234A2D4F43\nBootID:3f999247-cedc-4483-988a-68113136f62e Filesystems:[{Device:/dev/\nmapper/docker-253:1-9144532-367762f1736f18be685e3b77748876\nf38ab6b65355943fadb060be71c8676c70 Capacity:10725883904 Type:vfs\nInodes:10484736 HasInodes:true} {Device:/dev/vda1 Capacity:12873142272\nType:vfs Inodes:12581664 HasInodes:true} {Device:/dev/mapper/docker-\n253:1-9144532-c92bccf7a2ec59be6d94486cf7e58e346e305fe5f986ba89ae85e54b8443a2c9\nCapacity:10725883904 Type:vfs Inodes:10484736 HasInodes:true}]\nDiskMap:map[252:0:{Name:dm-0 Major:252 Minor:0 Size:107374182400\nScheduler:none} 252:1:{Name:dm-1 Major:252 Minor:1 Size:10737418240\nScheduler:none} 252:2:{Name:dm-2 Major:252 Minor:2 Size:10737418240\nScheduler:none} 253:0:{Name:vda Major:253 Minor:0 Size:12884901888\nScheduler:none}] NetworkDevices:[{Name:eth0 MacAddress:fa:16:3e:51:21:a3\nSpeed:0 Mtu:1454}] Topology:[{Id:0 Memory:8388198400 Cores:[{Id:0\nThreads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768\nType:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]}\n{Id:1 Memory:0 Cores:[{Id:0 Threads:[1] Caches:[{Size:32768 Type:Data\nLevel:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified\nLevel:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown\nInstanceID:None}\nI0921 10:51:49.097712 1 manager.go:201] Version:\n{KernelVersion:3.10.0-327.10.1.el7.x86_64 ContainerOsVersion:Alpine Linux\nv3.4 DockerVersion:1.12.1 CadvisorVersion:v0.24.0 CadvisorRevision:0cdf491}\nE0921 10:51:49.102474 1 factory.go:291] devicemapper filesystem stats will\nnot be reported: unable to find thin_ls binary\nI0921 10:51:49.102491 1 factory.go:295] Registering Docker factory\nW0921 10:51:49.102518 1 manager.go:244] Registration of the rkt container\nfactory failed: unable to communicate with Rkt api service: rkt: cannot tcp\nDial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused\nI0921 10:51:49.102525 1 factory.go:54] Registering systemd factory\nI0921 10:51:49.102657 1 factory.go:86] Registering Raw factory\nI0921 10:51:49.102786 1 manager.go:1082] Started watching for new ooms in\nmanager\nW0921 10:51:49.103264 1 manager.go:272] Could not configure a source for\nOOM detection, disabling OOM events: unable to find any kernel log file\navailable from our set: [/var/log/kern.log /var/log/messages\n/var/log/syslog]\nI0921 10:51:49.103910 1 manager.go:285] Starting recovery of all containers\nI0921 10:51:49.164319 1 manager.go:290] Recovery completed\nF0921 10:51:49.164348 1 cadvisor.go:151] Failed to start container\nmanager: inotify_add_watch /sys/fs/cgroup/cpuacct,cpu: no such file or\ndirectory\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1401#issuecomment-248580919,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AF8dbFW0cD0PXOz5YTX_ZYDHL2i-X2j8ks5qsRD8gaJpZM4JYadx\n.\n. My understanding is that kernel provides no guarantee on ordering of the\nname of combined cgroup.  There should be symlinks for each one.\nLibcontainer has some code to handle this I believe.\n\nOn Thursday, August 4, 2016, Tim St. Clair notifications@github.com wrote:\n\nI spun up a RHEL7 instance on GCE, and I'm trying to run the integration\ntests against this image, but hitting this error:\nF0804 18:48:48.415384       1 cadvisor.go:151] Failed to start container manager: inotify_add_watch /sys/fs/cgroup/cpuacct,cpu: no such file or directory\nThe directory name is slightly different:\n[stclair@cadvisor-rhel cadvisor]$ ls /sys/fs/cgroup\nblkio  cpu  cpuacct  cpu,cpuacct  cpuset  devices  freezer  hugetlb  memory  net_cls  perf_event  systemd\nI'm running with:\nsudo docker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  --privileged=true \\\n  google/cadvisor:test\n@derekwaynecarr https://github.com/derekwaynecarr @jimmidyson\nhttps://github.com/jimmidyson any suggestions?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1407#issuecomment-237650809, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AF8dbBYY8Lbd5yOpsHtu4WdQNaRch2Qzks5qcjcagaJpZM4JbHrD\n.\n. /cc @eparis \n. /cc @vishh @ronnielai - for future tracking.\n. /cc @ncdc @pmorie -- this impacts us more if/when we adopt overlay.\n. I added support to optionally include inode information for devices that supported them.\n. Is cadvisor running in a container in your setup?\n. This should be fixed in\n\nhttps://github.com/google/cadvisor/releases/tag/v0.24.1\nCan you verify?\nOn Saturday, October 29, 2016, QQYES notifications@github.com wrote:\n\n@derekwaynecarr https://github.com/derekwaynecarr yes\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1438#issuecomment-257086146,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AF8dbDjeZ_6-k7qAFXs9wEncM-lBsGeOks5q4yzNgaJpZM4Jry9F\n.\n. I can investigate, but I had previously verified the fix on RHEL family.  I\nwill not be able to look at this again for a few days.\n\nOn Tuesday, November 1, 2016, QQYES notifications@github.com wrote:\n\n@derekwaynecarr https://github.com/derekwaynecarr hello,the result of\nthe is the same to v0.24.0, I downloaded the image from the\nhttps://github.com/google/cadvisor/releases/tag/v0.24.1,and it does't\nwork too.the API returns:\n/system.slice/cbss-app-apps-docker-containers-\n5a2735a98e0bce4f62cdfe676d891813d25e530b2ccca14722e7f4e3e186\n924e-shm.mount\":{\"spec\":{\"creation_time\":\"2016-10-11T10:\n28:21.501636778Z\",\"aliases\":[\"mesos-75185ff8-8829-4af7-b4b8-\n0684e99ff143-S24.ff5873d3-58aa-4049-87db-b7fb0612fbde\",\"\n5a2735a98e0bce4f62cdfe676d891813d25e530b2ccca14722e7f4e3e186\n924e\"],\"namespace\":\"docker\",\"has_cpu\":true,\"cpu\":{\"limit\":\n1024,\"max_limit\":0},\"has_memory\":true,\"memory\":{\"limit\"\n:9223372036854775807,\"reservation\":9223372036854775807,\"swap_\nlimit\":9223372036854775807},\"has_custom_metrics\":false,\"\nhas_network\":true,\"has_filesystem\":true,\"has_diskio\":true,\"image\":\"\n10.161.24.239/iap_docker/marathon-lb-source:1.3.3\"},\"stats\":[{\"timestamp\":\n\"2016-11-01T08:14:20.781558968Z\",\"cpu\":{\"usage\":{\"\ntotal\":0,\"per_cpu_usage\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0],\"user\":0,\"system\":0},\"cfs\":{\"periods\":0,\n\"throttled_periods\":0,\"throttled_time\":0},\"load_average\":0},\"diskio\":{},\"\nmemory\":{\"usage\":0,\"cache\":0,\"rss\":0,\"swap\":0,\"working_set\":\n0,\"failcnt\":0,\"container_data\":{\"pgfault\":0,\"pgmajfault\":0},\n\"hierarchical_data\":{\"pgfault\":0,\"pgmajfault\":0}},\"network\":\n{\"interfaces\":[{\"name\":\"enp2s0f0\",\"rx_bytes\":846680017736,\"rx_packets\":\n935771674,\"rx_errors\":0,\"rx_dropped\":935593445,\"tx_bytes\":\n222,\"tx_packets\":3,\"tx_errors\":0,\"tx_dropped\":0},{\"name\":\"\nenp2s0f1\",\"rx_bytes\":61754184715629,\"rx_packets\":\n171557733135,\"rx_errors\":0,\"rx_dropped\":11631,\"tx_bytes\":\n32119400044549,\"tx_packets\":151598538902,\"tx_errors\":0,\"\ntx_dropped\":0},{\"name\":\"enp129s0f0\",\"rx_bytes\":0,\"rx_\npackets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_\npackets\":0,\"tx_errors\":0,\"tx_dropped\":0},{\"name\":\"\nenp129s0f1\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_\ndropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_\ndropped\":0},{\"name\":\"bond0\",\"rx_bytes\":62600018285193,\"rx_\npackets\":172491226093,\"rx_errors\":0,\"rx_dropped\":935980297,\"tx_bytes\":\n32118859739947,\"tx_packets\":151595794698,\"tx_errors\":0,\"\ntx_dropped\":0},{\"name\":\"bond0.102\",\"rx_bytes\":58239182270100,\"rx_packets\":\n150156562431,\"rx_errors\":0,\"rx_dropped\":452,\"tx_bytes\":\n31595298620878,\"tx_packets\":143661676229,\"tx_errors\":0,\"\ntx_dropped\":0}],\"tcp\":{\"Established\":0,\"SynSent\":0,\"\nSynRecv\":0,\"FinWait1\":0,\"FinWait2\":0,\"TimeWait\":0,\"\nClose\":0,\"CloseWait\":0,\"LastAck\":0,\"Listen\":0,\"Closing\":0},\"tcp6\":{\"\nEstablished\":0,\"SynSent\":0,\"SynRecv\":0,\"FinWait1\":0,\"\nFinWait2\":0,\"TimeWait\":0,\"Close\":0,\"CloseWait\":0,\"LastAck\":0,\"Listen\":0,\"\nClosing\":0}},\"filesystem\":{\"totalUsageBytes\":40960,\"baseUsageBytes\":0}},{\"\ntimestamp\":\"2016-11-01T08:14:22.278044494Z\",\"cpu\":{\"usage\":\n{\"total\":0,\"per_cpu_usage\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0],\"user\":0,\"system\":0},\"cfs\":{\"periods\":0,\n\"throttled_periods\":0,\"throttled_time\":0},\"load_average\":0},\"cpu_inst\":{\"\nusage\":{\"total\":0,\"per_cpu_usage\":[0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0],\"user\":0,\"system\":0}},\"diskio\":\n{},\"memory\":{\"usage\":0,\"cache\":0,\"rss\":0,\"swap\":0,\"working_\nset\":0,\"failcnt\":0,\"container_data\":{\"pgfault\":0,\"\npgmajfault\":0},\"hierarchical_data\":{\"pgfault\":0,\"\npgmajfault\":0}},\"network\":{\"interfaces\":[{\"name\":\"enp2s0f0\",\"rx_bytes\":\n846680019560,\"rx_packets\":935771702,\"rx_errors\":0,\"rx_\ndropped\":935593473,\"tx_bytes\":222,\"tx_packets\":3,\"tx_errors\"\n:0,\"tx_dropped\":0},{\"name\":\"enp2s0f1\",\"rx_bytes\":\n61754191264289,\"rx_packets\":171557754678,\"rx_errors\":0,\"\nrx_dropped\":11631,\"tx_bytes\":32119402850871,\"tx_packets\":\n151598555967,\"tx_errors\":0,\"tx_dropped\":0},{\"name\":\"\nenp129s0f0\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_\ndropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_\ndropped\":0},{\"name\":\"enp129s0f1\",\"rx_bytes\":0,\"rx_\npackets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_\npackets\":0,\"tx_errors\":0,\"tx_dropped\":0},{\"name\":\"bond0\",\"\nrx_bytes\":62600024835677,\"rx_packets\":172491247664,\"rx_\nerrors\":0,\"rx_dropped\":935980325,\"tx_bytes\":32118862546269,\"tx_packets\":\n151595811763,\"tx_errors\":0,\"tx_dropped\":0},{\"name\":\"bond0.102\",\"rx_bytes\":\n58239188392980,\"rx_packets\":150156581587,\"rx_errors\":0,\"\nrx_dropped\":452,\"tx_bytes\":31595301387666,\"tx_packets\":\n143661692655,\"tx_errors\":0,\"tx_dropped\":0}],\"tcp\":{\"\nEstablished\":0,\"SynSent\":0,\"SynRecv\":0,\"FinWait1\":0,\"\nFinWait2\":0,\"TimeWait\":0,\"Close\":0,\"CloseWait\":0,\"LastAck\":0,\"Listen\":0,\"\nClosing\":0},\"tcp6\":{\"Established\":0,\"SynSent\":0,\"SynRecv\":0,\"FinWait1\":0,\"\nFinWait2\":0,\"TimeWait\":0,\"Close\":0,\"CloseWait\":0,\"LastAck\":0,\"Listen\":0,\"\nClosing\":0}},\"filesystem\":{\"totalUsageBytes\":40960,\"baseUsageBytes\":0}}]}}\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1438#issuecomment-257510945,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AF8dbIghSzNkCxgrch0n56l_J75lhBOSks5q5vUjgaJpZM4Jry9F\n.\n. This never hit my radar until just now when assigned.  @ncdc mind rebasing this?. @ncdc - sorry i missed this, but it looks like we need a rebase again.  the changes lgtm.. LGTM. This should be fixed by https://github.com/google/cadvisor/pull/1476\n. @workhardcc -- i cant tell if you resolved your own issue?\n. I spoke w/ @mrunalp and we want to proceed as follows:\n- get this merged into opencontainers/runc https://github.com/opencontainers/runc/pull/1049\n- bump the cadvisor dependency to include that commit\n- update the call here: https://github.com/google/cadvisor/blob/master/container/libcontainer/helpers.go#L48 to pass true to get all mount points.\n\nGiven the ordering of the mount points, cAdvisor when iterating them will get the proper items in its map by virtue of its iteration order in the slice as desired.\n. I didn't think this needed to gate 1.4 as it only impacts running cAdvisor or Kubelet in a container... /cc @eparis @dawnchen \n. this is fixed now, take a look at the PR and should be clear.  If your host\nhad cpu,cpuacct but inside the container it was always looking for\ncpuacct,cpu independent of what the host had since it didn't look at all\nmountpoints.  So if your OS had sys/fs/cgroup/ setup in the opposite order,\nyou would hit this.\nOn Wednesday, September 21, 2016, Brandon Philips notifications@github.com\n<javascript:_e(%7B%7D,'cvml','notifications@github.com');> wrote:\n\n@mrunalp https://github.com/mrunalp What do you mean by \"Now, when we\nbind mount /sys:/sys we are overwriting that with the order on the system\nwhich happens to be the opposite on RHEL and hence the issue?\"\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1461#issuecomment-248755770,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AF8dbIcWLsE-AqPrKVOdeZtYEDc0B8scks5qsakEgaJpZM4J9SiV\n.\n. FYI: @smarterclayton @ncdc @pmorie @mrunalp\n\nThe first commit is temporary carry for https://github.com/opencontainers/runc/pull/1049.  Once that merges, I can just drop the carry and update with a vendor dependency being updated.\nI am testing this now on RHEL in container to validate it works ;-)\n. OK -- confirmed on RHEL7 in container that this works for me.\nWill update the PR when @mrunalp code merges.\n. @pmorie -- the carry is intended to be replaced before this PR merges.  I was just holding it there for testing purposes.\n. requisite runc pr has merged, will send a pr to update vendor library and then drop the first commit here.\n. Closing this in favor of https://github.com/google/cadvisor/pull/1476\n. well, it appears i did something wrong to stop tests building.\n. @timstclair @pmorie @vishh @ncdc -- PTAL, needed to fix running cAdvisor in container.\n. I want to do one last test on this before merging.\n. Ok, tested and confirmed.  All is good on RHEL flavored systems with containers.\n. FYI @pmorie @ncdc @timstclair \n. This is fixed by https://github.com/google/cadvisor/pull/1476\n. /cc @ncdc @pmorie \n. How did you install docker on this machine?  Where did you source your RPMs?\n. This should have been fixed.  I will try to reproduce.\nOn Friday, October 7, 2016, jbh6 notifications@github.com wrote:\n\nHi\nI have a 3 master, 5 data node Elastic search test cluster. On a separate\nmachine, I have a docker container running. Now, I want to send the host\nmetrics (on which docker application is running) and the docker metrics to\nthe EL cluster. I have Kibana set up over the elastic search cluster, so I\ndon't want to/need to monitor the data from cadvisor UI, hence I omitted\nthe publish flag in the following command:\nsudo docker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw\n--volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro\n--privileged=true --volume=/cgroup:/cgroup:ro --detach=true\ngoogle/cadvisor:latest -storage_driver=\"elasticsearch\"\n-alsologtostderr=true -storage_driver_es_host=\"http://10.25.11.133:9200\"\nHowever, the docker container immediately exists with the following logs:\nI1007 09:24:23.736924       1 storagedriver.go:48] Using backend storage type \"elasticsearch\"\nI1007 09:24:23.737060       1 storagedriver.go:50] Caching stats in memory for 2m0s\nI1007 09:24:23.737135       1 manager.go:140] cAdvisor running in container: \"/docker/66ae6e432f7201494e09607184f950450f50d87ee4689c0b50bf31e293eada30\"\nW1007 09:24:23.746538       1 manager.go:148] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused\nI1007 09:24:23.756258       1 fs.go:116] Filesystem partitions: map[/dev/mapper/docker-202:1-129297-635ed0921ff2f7ecbc6a0b169d329e7addd07f54da7586ffbfdf178ea9a8b4ef:{mountpoint:/ major:253 minor:2 fsType:xfs blockSize:0} /dev/xvda1:{mountpoint:/var/lib/docker/devicemapper major:202 minor:1 fsType:xfs blockSize:0} /dev/mapper/docker-202:1-129297-b674f7a1f8238f9e89f3f45f45c3103cd2c1c294b8824733c331f32180f33a86:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/b674f7a1f8238f9e89f3f45f45c3103cd2c1c294b8824733c331f32180f33a86 major:253 minor:1 fsType:xfs blockSize:0}]\nI1007 09:24:23.759935       1 manager.go:195] Machine: {NumCores:1 CpuFrequency:2400066 MemoryCapacity:1039843328 MachineID:f32e0af35637b5dfcbedcb0a1de8dca1 SystemUUID:EC290DDB-B3C7-116E-83F5-27C0DDF0E749 BootID:0e8dd135-3de3-453c-bafb-2d4b9ec04af9 Filesystems:[{Device:/dev/mapper/docker-202:1-129297-635ed0921ff2f7ecbc6a0b169d329e7addd07f54da7586ffbfdf178ea9a8b4ef Capacity:10725883904 Type:vfs Inodes:10484736 HasInodes:true} {Device:/dev/xvda1 Capacity:8578400256 Type:vfs Inodes:8387584 HasInodes:true} {Device:/dev/mapper/docker-202:1-129297-b674f7a1f8238f9e89f3f45f45c3103cd2c1c294b8824733c331f32180f33a86 Capacity:10725883904 Type:vfs Inodes:10484736 HasInodes:true}] DiskMap:map[253:1:{Name:dm-1 Major:253 Minor:1 Size:10737418240 Scheduler:none} 253:2:{Name:dm-2 Major:253 Minor:2 Size:10737418240 Scheduler:none} 202:0:{Name:xvda Major:202 Minor:0 Size:8589934592 Scheduler:deadline} 253:0:{Name:dm-0 Major:253 Minor:0 Size:107374182400 Scheduler:none}] NetworkDevices:[{Name:eth0 MacAddress:0e:64:2d:c2:01:3d Speed:0 Mtu:9001}] Topology:[{Id:0 Memory:1073336320 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:31457280 Type:Unified Level:3}]}] CloudProvider:AWS InstanceType:t2.micro InstanceID:i-0a6963ca9b9364a80}\nI1007 09:24:23.760469       1 manager.go:201] Version: {KernelVersion:3.10.0-327.13.1.el7.x86_64 ContainerOsVersion:Alpine Linux v3.4 DockerVersion:1.12.1 CadvisorVersion:v0.24.0 CadvisorRevision:0cdf491}\nE1007 09:24:23.766147       1 factory.go:291] devicemapper filesystem stats will not be reported: unable to find thin_ls binary\nI1007 09:24:23.766157       1 factory.go:295] Registering Docker factory\nW1007 09:24:23.766168       1 manager.go:244] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused\nI1007 09:24:23.766172       1 factory.go:54] Registering systemd factory\nI1007 09:24:23.766285       1 factory.go:86] Registering Raw factory\nI1007 09:24:23.766380       1 manager.go:1082] Started watching for new ooms in manager\nW1007 09:24:23.766664       1 manager.go:272] Could not configure a source for OOM detection, disabling OOM events: unable to find any kernel log file available from our set: [/var/log/kern.log /var/log/messages /var/log/syslog]\nI1007 09:24:23.766983       1 manager.go:285] Starting recovery of all containers\nI1007 09:24:23.819583       1 manager.go:290] Recovery completed\nF1007 09:24:23.826940       1 cadvisor.go:151] Failed to start container manager: inotify_add_watch /sys/fs/cgroup/cpuacct,cpu: no such file or directory\nElasticsearch returned with code 200 and version 2.4.1\nAm I missing something here? The -storage_driver_es_host=\"http:\n//10.25.11.133:9200\" specifies the URL of the machine running the Elastic\nsearch data node.\nAlso, I am not clear how can I send the host machine metrics to the\ncluster?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1494, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AF8dbC7x464NUPFxJpAjxEPlOoo5Dz_4ks5qxhIQgaJpZM4KQ16_\n.\n. This was fixed, but a new release has not yet been pushed to docker hub that contains the fix.\n\nFor more detail, see here:\nhttps://github.com/google/cadvisor/pull/1476#issuecomment-252070142\n. LGTM\n. LGTM\n. I think this should fix https://github.com/google/cadvisor/issues/1573. So the topology with cells and cpus listed?\nWould you do this on MachineInfo?\nGot a response format in mind?\nOn Saturday, October 29, 2016, Connor Doyle notifications@github.com\nwrote:\n\ncc @balajismaniam\n\nOn Oct 28, 2016, at 20:44, Vikas Choudhary notifications@github.com\n<javascript:_e(%7B%7D,'cvml','notifications@github.com');> wrote:\ncc @jeremyeder @timstclair @ConnorDoyle @derekwaynecarr\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1521#issuecomment-257075536,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AF8dbPAOBINKmlRdln7Kg_bT5zKIekaDks5q4u6QgaJpZM4KkA6p\n.\n. LGTM, but be aware that we have found performance issues with thin_ls. can you rebase?. @ncdc @pmorie @smarterclayton -- i think this will impact us.  need to invetigate further.. i sent a patch to ignore any .mount cgroup in docker (as it should just care about the .scope cgroups). @yujuhong -- updated pr so for .mount cgroup, docker will not handle or accept.  in addition, just updated the handler to no longer return an error when a container is not a valid docker name since it caused excessive noise.\n\nat log-level 5, you now see:\nI0116 11:04:28.508348   29276 factory.go:115] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-overlay-9f086b233ab7c786bf8b40b164680b658a8f00e94323868e288d6ce20bc92193-merged.mount\"\nI0116 11:04:28.508359   29276 factory.go:108] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-overlay-9f086b233ab7c786bf8b40b164680b658a8f00e94323868e288d6ce20bc92193-merged.mount\", but ignoring.\nI0116 11:04:28.508373   29276 manager.go:867] ignoring container \"/system.slice/var-lib-docker-overlay-9f086b233ab7c786bf8b40b164680b658a8f00e94323868e288d6ce20bc92193-merged.mount\"\nthis should be good to go, PTAL. @k8s-bot test this. merging.... poke. The dynamic period is an improvement.  I would like to verify that this also fixes unintended consequences of thin_ls impacting cpu usage stat collection prior to merging.. @k8s-bot test this. per our discussion, once we have some measurement times, we may do a follow-on that requires opt-in to this functionality via an env var of some kind.. in at least one environment on one cloud provider, we have seen thin_ls never return so period alone is not a sufficient fix for everyone.  for the environments where execution has proven slow, or iops heavy, the period helps.  we need to potentially get a follow-on pr that disables the usage.. To handle the \"command never returns case\", what about the following:\n\nadd a RunUntil(period) command in exec.go\nif command fails to complete in time, log a message, and do not call again for life of process\n\ni guess in the case of iops exhaustion, some folks may complain, but i think for now that seems like a relatively safe solution rather than an env var, etc.\nthoughts? /cc @vishh @timstclair \n. In the future, we need to do any of the following:\n\ncontrol thin_ls via blkio\nincrease the period between thin_ls invocations (https://github.com/google/cadvisor/pull/1583)\ncontrol the execution time of thin_ls via an upper bound\npossibly only invoke thin_ls based on rate of change reported by lvs\n\n/cc @ncdc @sjenning @smarterclayton . cc @pmorie . /cc @sjenning . @dashpole - the customer reported this was putting their root partition at risk, but in theory the should be able to monitor logs separately.  i want to avoid similar reports in the future, so just knowing this is lgtm from you is fine.  i would like the version of cadvisor we have w/ kube 1.8 has this fix.. rebasing.... @dashpole -- rebased, simple review.. /cc @sjenning @vishh @ConnorDoyle @PiotrProkop. @dashpole -- thanks for updating the ci pieces... rebased, and it appears to pass.. @dashpole - updated.  i would like us to return a value for each supported page size on the machine.  so in your case, a value of 0 for 2048kb at least lets me know your host supports 2Mi pages.. our queue of cadvisor updates is growing, need to track down e2e problems.. LGTM, will let @dashpole review before merging.\nAwesome investigation.  Seeing the profile before and after is really helpful.. I need to test this in a kube context to make sure it works as expected for our summary api.. ok, i think this is fine.. @dashpole @dchen1107 -- this is the follow-up from our discussion in this week's sig-node.  i can handle primary review on this.. fyi @sjenning . /test pull-cadvisor-e2e. @runcom -- looks like a gofmt error on handler_test.go i think.. @runcom - just the one question on timeout looking rather high.  once addressed, this is LGTM\n. LGTM. LGTM. /retest. /test pull-cadvisor-e2e\n. fyi @runcom @mrunalp @sjenning -- input here?\nalso do we have the same or similar issue w/ crio?. @mrunalp @runcom can you do same exercise as @dashpole on crio?. @dashpole I will review . well, i hit the wrong button... @runcom -- can you drop my merge branch and rebase on master?. fyi @sjenning . /test pull-cadvisor-e2e. /test-/test pull-cadvisor-e2e\n. @dashpole -- any idea why test results are not reporting?. to support a smooth transition, can we check for one, and then the other?. @dashpole -- this can correspond to 1.9, @runcom - can you confirm the 1.9 cri-o packaging is in the new location?. @dashpole -- this should only go in kube 1.9 tracking releases and not kube 1.8 prior streams.. @dashpole --- this is really awesome.. fyi @sjenning xmas came early!. @sjenning ack.\n@dashpole thanks for the fix.. I am going to clarify the comment.\nThe default aws behavior is to retry 4 times, each with a 5 second timeout.  If you run cadvisor outside of gce or aws, you end up waiting 20s because the cloud detection checks gce first, aws second, and then azure third, and then finally it says I give up.\n. I caught it running e2e not GCE or AWS.  It was very obvious.\nOn Wed, Mar 9, 2016 at 2:23 PM, Vish Kannan notifications@github.com\nwrote:\n\nIn utils/cloudinfo/aws.go\nhttps://github.com/google/cadvisor/pull/1152#discussion_r55573268:\n\n@@ -23,7 +23,10 @@ import (\n )\nfunc onAWS() bool {\n-   client := ec2metadata.New(session.New(&aws.Config{}))\n-   // by default, we set retries to 0 for this operation.\n-   // when not running on AWS, not setting retries can cause the operation to block\n\nI wonder why the e2e tests did not catch this.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/cadvisor/pull/1152/files#r55573268.\n. yes, but I was apprehensive to doing that since I have no idea if the 5s was based on actual observation in aws.  that said, i would be happy to go to 2s or 1s if someone could vouch for that risk.\n\nclient.Config.HTTPClient.Timeout can be lowered after this line to something smaller...\n. @justinsb - do you think its safe to decrease the default timeout to 2s?\n. assume kubelet will not have to check this thing?  wondering if we should also list it as a soft/hard dependency there\n. so when i run dmsetup status  , i can get values like:\n0 37740544 thin-pool 247 1065/262144 66667/294848 - rw no_discard_passdown queue_if_no_space\ncan we have an example with a larger number of fields?\n. wait, nm, i see we have that below...\n. we need to potentially treat this and inodes_free to be optional fields so we can distinguish the field as being set versus unset in cases where its not knowable.\n. this is problematic with devicemapper where inodes_free is not knowable since it operates at a block level.  I think we should make this a pointer.\n. I think we need to have a way on FsStats to know if inodes information is knowable in order to support block level things like devicemapper properly.  We would then only set InodesFree if fs.InodesKnowable was true for example.  I am not sure how much backwards compatibility concerns there are around FsStats struct.\n. look at https://github.com/google/cadvisor/pull/1368\n. ack, will update\n. hmm, can you model out what you would like to see to save some time?\nThe main intention in adding this was to keep this code from getting messy here:\nhttps://github.com/google/cadvisor/blob/master/manager/manager.go#L671\nI am not sure how to pull back the ContainerSpec cleanly from that context, but am happy to make the update with a pointer.\n. So I have tried this change in my local workspace, but I am having trouble figuring out the best way to set it on ContainerSpec since best I can tell wherever we call GetSpec https://github.com/google/cadvisor/blob/master/container/common/helpers.go#L48, we don't yet appear to actually know if the associated filesystem supports inodes or not yet.\n. ok, i will leave machine as is.  see my other question on how to properly initialize for Container FsSpec\n. trying to think how to handle this for container spec:\n- zfs, devicemapper do not support inodes at all\n- btrfs, xfs have inode limits that can dynamically grow\nnot sure where in code we want to populate the FsSpec \n. per our discussion, moving to Container FsSpec in v1 API proved problematic as you have to basically fetch all fs stats, and treat it as a []FsSpec since a container has multiple filesystems each of which may or may not support inodes.  the outcome was to stick with keeping a has_inodes throughout the v1 API.\n. per earlier comment, moved away from the FsSpec proposal in v1 API.\n. ok, will do that now.\n. i know its not new to this pr, but its ashame we are not consistent with camel casing.\n. rather than firstCall, maybe just say log bool on whether it should log anything or not?  there are two log calls in this existing operation. or an alternative is to update this to return []*ContainerStats, []warnings, and then we can optionally log warnings or not from the caller?. could this be a lru cache so we don't grow forever?. ParseUint got tripped up by the \\n in the file.. is this the same literal behavior just a different name?. ok, went back in runc history and see its the same thing just a rename.. Can you make this public?  I would want to reference it in kubelet . Nit from my port.  s/docker/cri-o. Cleanup logging. s/docker/crio. I know I wrote this comment.  I would prefer we do not do this for perf. Same here. I think we should have this be a struct similar to docker. can this be shorter?  rkt is 2s. ok, thanks for the detail.. log something like the following:\n\"du and find on following dirs took %v: %v; will log in future if exceeds %v\". ",
    "ghodss": "Yes that's right - I was looking for it in the wrong place.\nOne thing that may want to be fixed is that the comment says \"Default is unlimited (-1)\", but the field is uint64 so \"default\" is actually MaxUint64 (i.e. 18446744073709551615). You may want to either make it an actual int or change the documentation.\n. ",
    "ericchiang": "cc @vishh @timstclair \n. What's a good way to add go get golang.org/x/tools/cmd/vet to the Jenkins build? :)\n. $ godep version\ngodep v57 (linux/amd64/go1.5.3)\nShould we record this somewhere?\n. ",
    "wjimenez5271": "I'm observing something similar.\nDocker version 1.9.1, build a34a1d5/1.9.1 on Amazon Linux AMI release 2016.03\ndocker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --volume=/cgroup:/cgroup:ro \\\n  --publish=9070:8080 \\\n  --detach=true \\\n  --privileged=true \\\n  --name=cadvisor \\\n  --restart=unless-stopped \\\n  google/cadvisor:latest\n. ",
    "binman-docker": "We're seeing the same thing, not necessarily linked to restarts, though it does seem to always be the same containers. Running cadvisor 0.22 on docker engine 1.9.1.\nuser@host:/var/lib/docker# docker logs cadvisor 2>&1 | grep 8d97417dfabd\nW0424 05:15:16.991521       1 container.go:341] Failed to create summary reader for \"/docker/8d97417dfabd2c58edc9a67137260a16d022c44f8aad4a7abbf224f2a087123a\": none of the resources are being tracked.\nW0424 05:15:16.992561       1 manager.go:1022] Failed to process watch event: failed to read libcontainer config: open /var/lib/docker/execdriver/native/8d97417dfabd2c58edc9a67137260a16d022c44f8aad4a7abbf224f2a087123a/container.json: no such file or directory\nW0424 06:22:32.339289       1 container.go:341] Failed to create summary reader for \"/docker/8d97417dfabd2c58edc9a67137260a16d022c44f8aad4a7abbf224f2a087123a\": none of the resources are being tracked.\nW0424 06:22:32.339352       1 manager.go:1022] Failed to process watch event: failed to read libcontainer config: open /var/lib/docker/execdriver/native/8d97417dfabd2c58edc9a67137260a16d022c44f8aad4a7abbf224f2a087123a/container.json: no such file or directory\nW0424 06:50:25.392315       1 container.go:341] Failed to create summary reader for \"/docker/8d97417dfabd2c58edc9a67137260a16d022c44f8aad4a7abbf224f2a087123a\": none of the resources are being tracked.\nW0424 06:50:25.392378       1 manager.go:1022] Failed to process watch event: failed to read libcontainer config: open /var/lib/docker/execdriver/native/8d97417dfabd2c58edc9a67137260a16d022c44f8aad4a7abbf224f2a087123a/container.json: no such file or directory\nThe directory \"/var/lib/docker/execdriver\" does not exist at all (/var/lib/docker does, of course).\nAccessing the container in the webui gets this:\nfailed to get container \"/docker/8d97417dfabd2c58edc9a67137260a16d022c44f8aad4a7abbf224f2a087123a\" with error: unable to find data for container /docker/8d97417dfabd2c58edc9a67137260a16d022c44f8aad4a7abbf224f2a087123a\nRestarting the cadvisor container fixes the issue. I can include the /validate if that would help.\n. ",
    "joshbambrick": "I have since identified that the read values do update as you would expect. I have written a test program showing a repeated copy of data from one file to another, and the read values are as expected, but the write values remain at 0.\n. ",
    "Thiht": "I'd be glad to help with the clients refactoring and improvements since I'm using them right now.\n. Great!\nHere are a few expectations I have from the client:\n- Every REST endpoint should be exposed in the client\n  - List of the available endpoints\n  - The methods should take a nilable/optional v2.RequestOptions whenever it makes sense\n    - It would be nice to have a DefaultRequestOptions somewhere with the default values\n  - The method names should match the endpoint name (machine -> Machine() instead of MachineInfo() for example)\n  - Obviously add tests for every method\n    - Currently Stats is broken, it should return a map[string][]v2.DeprecatedContainerStats instead of a map[string]v2.ContainerInfo\n      - And maybe it'd be better for the resource to be a map[string][]v2.ContainerInfo, this \"Deprecated\" is scary :)\n- When creating a client, the version shouldn't be hardcoded but parameterizable\n  - Maybe this would result in a single client able to handle both v1 and v2? I don't know if it's doable or a good idea\n- At least httpGetJsonData should be publicly exposed to allow users to access non-exposed endpoints (in case the development of the API moves faster than the client)\n  - This method should be more specialized and handle baseUrl internally, it would allow to get rid of the *Url helper methods. Thus it would just need an endpoint string parameter instead of url string\nThese modifications would have helped me a lot when using it!\nAs for moving client to client/v1, won't it result in breaking the existing builds? Or is it not an issue since cAdvisor is still 0.x.x?\n. Vendoring seems to be the way to go with Go so you're right, these changes should not be a problem. I'm not completely familiar with this way of doing things yet! \nThe api-cleanup branch seems like a good compromise to me.\n\n\n\n\nAt least httpGetJsonData should be publicly exposed to allow users to access non-exposed endpoints (in case the development of the API moves faster than the client)\n\n\nI think this would be better solved with policy / pre-submit checks: require additional endpoints to be handled in the clients.\n\nIt would be ideal if you think that's feasible. Exposing the method is a nice workaround for the users when not everything is available in the client, but I agree it would be better to keep the client and the API synced. It's a bit more work but I'm confident it can be handled.\n\n\n\n\nWhen creating a client, the version shouldn't be hardcoded but parameterizable\nMaybe this would result in a single client able to handle both v1 and v2? I don't know if it's doable or a good idea\n\n\nThe V1 and V2 clients may expose different methods, and those methods are going to have different input and output parameter types. I don't think there is a good way of accomplishing this point without using interface{} types to circumvent the type system, which is a bad idea. I think it's better to require users to explicitly import the client type they want, even if the constructors for both look the same.\n\nI quick-checked and I see what you mean, let's keep v1 and v2 separate then. Is there the same issue between the v2 minor versions? Because it would be really handy for the user to be able to specify at least the minor version when communicating with servers using an older cAdvisor version\n. I started working with:\n\n\nThis method should be more specialized and handle baseUrl internally, it would allow to get rid of the *Url helper methods. Thus it would just need an endpoint string parameter instead of url string\n\n\non my fork. I noticed the httpGet* methods take a postData argument, which doesn't really make sense semantically in a REST context. Moreover, it doesn't seem to be used anywhere (yet?) as far as I can tell. Do you mind if I remove it completely, or at the very least split the \"httpGet* methods with postData argument\" in \"httpGet* methods and httpPost* methods\", which would be nicer on the eyes?\n. Hello,\nI've got a little problem with the v2 API I don't really know how to solve properly. The problem is /api/v2.0/stats and /api/v2.1/stats respectively return map[string][]v2.DeprecatedContainerStats and map[string]v2.ContainerInfo, two different types.\nSo I don't really know how to represent this in the client. I was thinking maybe making two methods would be an acceptable solution:\n``` go\n// Named Stats2_0 to highlight the fact it's an old version (or \"StatsLegacy\" or whatever)\nfunc (*Client) Stats2_0(...) (map[string][]v2.DeprecatedContainerStats, error) {\n  // ...\n}\n// Normal name for the 2.1 method\nfunc (*Client) Stats(...) (map[string]v2.ContainerInfo, error) {\n  // ...\n}\n```\nDo you have another solution? The only other idea I have in mind would be for Stats to return an interface{} for the user to assert with the correct type but it does seem a bit ugly.\n. ",
    "flqw": "Sadly no. Since I was flexible I switched out influx for prometheus and scrape the cadvisor and application-metrics separately.\n. ",
    "cirocosta": "Hey @vishh , \ncould you provide more details into what needs to get done? I'd be willing to tackle this one. \nThx!. Hey, sorry if I'm getting a little off the main track of the issue but I have two questions: \n\nIs this configuration restricted to docker bridge networks and kubernetes pods? \n~~Is clone()ing and then performing the request the only way of getting into the network ns? Would it be possible to use setns to get to the namespace, perform the request and then switch to another ns?~~ - update: oh, now I see: https://www.weave.works/blog/linux-namespaces-and-go-don-t-mix\n\nAs I see  there's no way to get a response from <ContainerIp>:<Port> without a bridge / port binding / kubernetes pod networking without entering the namespace (or am I wrong? \ud83e\udd14 ).\nFrom the PR that adds support there's the addition of the following:\nhttps://github.com/google/cadvisor/blob/6ef612f21e042dcbec189ab253741f94e47804d7/collector/util.go#L21-L25\nbut this is just gathering of the ip address and no collection of netns. \nPlease let me know if I'm missing something!\nThx! \n\nUpdate: looks like now there's more room to entering the network namespace reliably: https://golang.org/doc/go1.10#runtime\n. Hey @tallclair, I just ran into the same as @liftedkilt reported in many of the machines we're running ( all very well packed for their sizes). By default they're all running with 8K watches set in syctl.conf. \nNaturally one can fix that by raising that limit but I was wondering whether we could reduce that number by removing unnecessary watches that are placed in raw.go - do we really need to keep track of each cgroup subsystem in order to discover if a container has been created/removed/.. ? \nDo you think there's room for making this more light-weight? I'd be willing to give a shot implementing it.\nThx!. Oh, I totally missed restartcount, nice catch!\nAbout the naming (label_whitelist, ...), I like the suggestion! \nHowever, I think it only makes sense if we include rkt in the PR :thinking: Going to add the check for restartcount and then check the rkt code :+1:  \nThx!\n\n~~(PR not ready yet)~~. Hey @dashpole, sorry for the delay, only found time today; could you please take a look once again?\nI also updated the 'docs'  document related to runtime configs, do you think the format is appropriate?\nI'm having some problems manually testing the rkt part, it's my first time using it so I'm not sure if I'm doing something completely wrong :thinking:  \nI ran a container using\nrkt run --insecure-options=image docker://nginx:alpine --user-label=foo=bar\nand also initiated api-service:\nrkt api-service --listen=localhost:15441\napi-service: API service starting...\napi-service: Listening on localhost:15441\napi-service: API service running\nthen when I start cadvisor, api-service shows:\n2017/09/08 15:47:08 grpc: Server.Serve failed to create ServerTransport:  connection error: desc = \"transport: write tcp 127.0.0.1:15441->127.0.0.1:49326: write: broken pipe\"\nwhich leads to an error in cadvisor logs:\nI0908 15:47:09.007274   17800 manager.go:320] Recovery completed\nI0908 15:47:09.007400   17800 rkt.go:56] starting detectRktContainers thread\nI0908 15:47:09.076635   17800 cadvisor.go:159] Starting cAdvisor version: v0.27.0.19+33f84c4934366a-dirty-33f84c4 on port 8080\nW0908 15:47:19.014873   17800 handler.go:105] couldn't find app machine-rkt\\x2d2d9c1b0f\\x2d5004\\x2d44f8\\x2d9716\\x2d6938106bd2c4.scope in pod\nE0908 15:47:19.014928   17800 helpers.go:131] ReadFile failed, couldn't read /var/lib/rkt/pods/run/2d9c1b0f-5004-44f8-9716-6938106bd2c4/appsinfo/machine-rkt\\x2d2d9c1b0f\\x2d5004\\x2d44f8\\x2d9716\\x2d6938106bd2c4.scope/treeStoreID to get upper dir: open /var/lib/rkt/pods/run/2d9c1b0f-5004-44f8-9716-6938106bd2c4/appsinfo/machine-rkt\\x2d2d9c1b0f\\x2d5004\\x2d44f8\\x2d9716\\x2d6938106bd2c4.scope/treeStoreID: no such file or directory\nW0908 15:47:19.016649   17800 manager.go:1117] Failed to process watch event {EventType:0 Name:/user.slice/user-0.slice/session-1.scope/machine-rkt\\x2d2d9c1b0f\\x2d5004\\x2d44f8\\x2d9716\\x2d6938106bd2c4.scope/system.slice/nginx.service WatchSource:1}: this should be impossible!, new handler failing, but factory allowed, name = /user.slice/user-0.slice/session-1.scope/machine-rkt\\x2d2d9c1b0f\\x2d5004\\x2d44f8\\x2d9716\\x2d6938106bd2c4.scope/system.slice/nginx.service\nI0908 15:47:19.016666   17800 container.go:471] Failed to update stats for container \"/user.slice/user-0.slice/session-1.scope/machine-rkt\\x2d2d9c1b0f\\x2d5004\\x2d44f8\\x2d9716\\x2d6938106bd2c4.scope\": stat failed on  with error: no such file or directory, continuing to push stats\nam I doing something wrong here?\nThx!. Note.: this can probably be expanded to other storage drivers - I just looked at influxdb for now. . hmmm in the collection you mean? Under libcontainer/helpers.go it fetches all of them from /proc/<pid>/net/dev:\nhttps://github.com/google/cadvisor/blob/c2f8191ba9550e302379473faf41d98759d1eff1/container/libcontainer/helpers.go#L146-L155\nI was willing to then do something like how it's done for cpu:\nhttps://github.com/google/cadvisor/blob/2835c35bde4983eee1cc76aefc74d778776cca80/storage/influxdb/influxdb.go#L187-L194\n. Got it, at first I thought about just adding new 4 fields:\n```go\n    serRxBytes string = \"rx_bytes\"\n    // Cumulative count of receive errors encountered.\n    serRxErrors string = \"rx_errors\"\n    // Cumulative count of bytes transmitted.\n    serTxBytes string = \"tx_bytes\"\n    // Cumulative count of transmit errors encountered.\n    serTxErrors string = \"tx_errors\"\nserRxBytesPerIface string = \"rx_bytes_per_iface\"\n// Cumulative count of receive errors encountered.\nserRxErrorsPerIface string = \"rx_errors_per_iface\"\n// Cumulative count of bytes transmitted.\nserTxBytesPerIface string = \"tx_bytes_per_iface\"\n// Cumulative count of transmit errors encountered.\nserTxErrorsPerIface string = \"tx_errors_per_iface\"\n\n```\nbut that caries the problem of having 4 fields (rx_bytes, rx_errors, tx_bytes, tx_errors) being transmitted only for backwards compatibility (someone using *_per_iface would certainly not be using) - this is something that already goes on for those that are using straight serialization of containerStats (as the fields are included there. \nA second option that I see is introducing a flag that enables the new behavior (-collect_network_stats_per_interface?) but ... yet another flag that would be affecting only some storage drivers - those that don't serialize and are not prometheus: redis, statsd, stdout and influxDb.\nFor my use case the second option (flag) would be fine even though it's yet another one.\nWdyt?\nthx!. Hey,\nanother idea is to add a flag that triggers the behavior of aggregating the values reported for each stat:\n--influx-db-aggregate-network-stats\nwhich would imply on (having an if aggregateNetworkStats...)\ndiff\n    if len(stats.Network.Interfaces) > 0 {\n-       stats.Network.InterfaceStats = stats.Network.Interfaces[0]\n+       for _, iface := range stats.Network.Interfaces {\n+           stats.Network.Name = \"all\"\n+\n+           stats.Network.TxBytes += iface.TxBytes\n+           stats.Network.TxErrors += iface.TxErrors\n+           stats.Network.TxDropped += iface.TxDropped\n+           stats.Network.TxPackets += iface.TxPackets\n+\n+           stats.Network.RxBytes += iface.RxBytes\n+           stats.Network.RxErrors += iface.RxErrors\n+           stats.Network.RxDropped += iface.RxDropped\n+           stats.Network.RxPackets += iface.RxPackets\n+       }\n    }\nWdyt? Those relying on the old behavior (stats from the first iface found) would still have what they expect and we can move forward with a value that includes all interfaces. \nMaybe an extra flag would emit those _per_iface metrics to influx. \nps.: for my  use-case the first solution is enough as we don't segregate the interfaces and counting loopback is valid but maybe 2 is also useful for others.\nThx!. ",
    "mokshpooja": "Found the solution myself- \nwhen using the dockerfiles mentioned above the docker-compose.yml need to be configured as-\ncadvisor:\n  build: ./cadvisor\n  ports:\n    - \"8080:8080\"\n  volumes:\n    - \"/:/rootfs:ro\"\n    - \"/var/run:/var/run:rw\"\n    - \"/sys:/sys:ro\"\n    - \"/var/lib/docker/:/var/lib/docker:ro\"\n  expose:\n    - \"8080\"\n. ",
    "Farkya": "try : \n--publish=\"desired-port\":8080 \\\n. ",
    "shujamughal": "complete command if you want to run it on 8484 port is like\ndocker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8484:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  google/cadvisor:latest \n. ",
    "blakebarnett": "FYI --disable_metrics=\"\" doesn't work but --disable_metrics= does.. Possibly related to this? https://github.com/google/cadvisor/pull/2124. Correct, that's what we were running before (minus the cgroups whitelist). Just to verify it had nothing to do with cluster version I tested it on both 1.10.9-gke.5 and 1.11.7-gke.6.  Here's the entire DaemonSet yaml that I'm trying this with: https://gist.github.com/blakebarnett/68e3b35754a683173e43cc3e4975aeda\nInitially I thought this was happening on all replicas, but I've just verified it only happens on 1 node. I'll debug further and find the culprit.\n@dashpole is correct, this is running as a separate DaemonSet in kubernetes, because we need some of the additional metrics that are not available in the kubelet version.. I moved a few pods off the node, and the problem went away. There was nothing special about any of them, and now I can't reproduce it by running similar pods on that same node again.  I'll close this until I can track down more specifics.. I was able to track this down to the annotations on kubernetes pods. We had a these two annotations on 2 different pods:\n\"annotation.config-checksum\" and \"annotation.config_checksum\"\nAfter sanitizeLabelName() https://github.com/google/cadvisor/blob/master/metrics/prometheus.go#L1243 runs they conflict. I imagine this case is fairly easy to bump into given how arbitrary annotations can be. \nShould I submit a PR to prevent the clash after sanitization?. Our work-around for now is to use --store_container_labels=false with --whitelisted_container_labels= of the few we know we need, maybe this should just be recommended as a default when running in kubernetes.. going to work on a better approach while I work out the CLA. I signed it!. We noticed a something similar to this: https://github.com/google/cadvisor/issues/2183 with --store_container_labels=false, the first container in the cgroup has all the kubernetes-provided label values, the rest don't. I can't figure if this is a regression of #1704 or something new... \nThis is without any changes and without this PR applied, I discovered it when validating this change.. oops, fixing. Yeah, confused myself by doing this a different way before submitting the PR. Thanks :). I was wondering that after the test failure, but this shouldn't change that behavior right?  It will only exclude a label if a duplicate label already exists and the value will still get set for that label.. Hmm, yeah I see. It could happen if someone provides multiple permutations of a label that all normalize to the same thing. Should we just throw them out in that case? I can't think of a great default behavior there.. ^ on the same container, it should hopefully be a rare edge-case.... No #2181 was node-wide. 2 separate containers (separate pods) with annotations that normalize to the same thing caused the panic.. That's true, in fact I noticed when running without --store_container_labels=false all labels that were present on any of the containers on the host showed up with empty values for all container metrics in prometheus, that was what made me look into the cgroup whitelisting initially and then I noticed this crash behavior.. ",
    "elisehuard": "I signed it!  or rather, our company, Mastodon C (mastodonc.com) did. I changed my primary email to reflect my corporate email, hope that works.\n. ok to test\n. for following docker parameters:\n--label cluster=staging --label application=witan-app\nwe get\n``` sh\nInfluxDB shell 0.11.0\n\nuse cadvisor_staging\nUsing database cadvisor_staging\nselect * from cpu_usage_total where time > now() - 30s and application = 'witan-app';\nname: cpu_usage_total\n\n\ntime            application cluster container_name                                      machine                     value\n1460214463278854367 witan-app   staging mesos-8e886703-8611-42fe-b3f4-481a795b131c-S0.e6956c8a-5e9e-427c-a7a7-a4006da4a394  ip-172-20-0-64.eu-central-1.compute.internal    137506480061\n1460214464431717864 witan-app   staging mesos-8e886703-8611-42fe-b3f4-481a795b131c-S0.e6956c8a-5e9e-427c-a7a7-a4006da4a394  ip-172-20-0-64.eu-central-1.compute.internal    137507561223\n1460214465903021964 witan-app   staging mesos-8e886703-8611-42fe-b3f4-481a795b131c-S0.e6956c8a-5e9e-427c-a7a7-a4006da4a394  ip-172-20-0-64.eu-central-1.compute.internal    137508589961\n1460214467859716933 witan-app   staging mesos-8e886703-8611-42fe-b3f4-481a795b131c-S0.e6956c8a-5e9e-427c-a7a7-a4006da4a394  ip-172-20-0-64.eu-central-1.compute.internal    137509826912\n1460214469838721624 witan-app   staging mesos-8e886703-8611-42fe-b3f4-481a795b131c-S0.e6956c8a-5e9e-427c-a7a7-a4006da4a394  ip-172-20-0-64.eu-central-1.compute.internal    137511352916\n1460214471833120573 witan-app   staging mesos-8e886703-8611-42fe-b3f4-481a795b131c-S0.e6956c8a-5e9e-427c-a7a7-a4006da4a394  ip-172-20-0-64.eu-central-1.compute.internal    137512723311\n1460214473276874791 witan-app   staging mesos-8e886703-8611-42fe-b3f4-481a795b131c-S0.e6956c8a-5e9e-427c-a7a7-a4006da4a394  ip-172-20-0-64.eu-central-1.compute.internal    137513603633\n1460214474447067112 witan-app   staging mesos-8e886703-8611-42fe-b3f4-481a795b131c-S0.e6956c8a-5e9e-427c-a7a7-a4006da4a394  ip-172-20-0-64.eu-central-1.compute.internal    137515287091\n1460214475634303966 witan-app   staging mesos-8e886703-8611-42fe-b3f4-481a795b131c-S0.e6956c8a-5e9e-427c-a7a7-a4006da4a394  ip-172-20-0-64.eu-central-1.compute.internal    137516137399\n1460214476924362206 witan-app   staging mesos-8e886703-8611-42fe-b3f4-481a795b131c-S0.e6956c8a-5e9e-427c-a7a7-a4006da4a394  ip-172-20-0-64.eu-central-1.compute.internal    137516995708\n1460214478356428984 witan-app   staging mesos-8e886703-8611-42fe-b3f4-481a795b131c-S0.e6956c8a-5e9e-427c-a7a7-a4006da4a394  ip-172-20-0-64.eu-central-1.compute.internal    137518651390\n1460214479653384517 witan-app   staging mesos-8e886703-8611-42fe-b3f4-481a795b131c-S0.e6956c8a-5e9e-427c-a7a7-a4006da4a394  ip-172-20-0-64.eu-central-1.compute.internal    137519870110\n1460214480866061208 witan-app   staging mesos-8e886703-8611-42fe-b3f4-481a795b131c-S0.e6956c8a-5e9e-427c-a7a7-a4006da4a394  ip-172-20-0-64.eu-central-1.compute.internal    137520682225\n1460214482591797716 witan-app   staging mesos-8e886703-8611-42fe-b3f4-481a795b131c-S0.e6956c8a-5e9e-427c-a7a7-a4006da4a394  ip-172-20-0-64.eu-central-1.compute.internal    137521927571\n1460214483792832030 witan-app   staging mesos-8e886703-8611-42fe-b3f4-481a795b131c-S0.e6956c8a-5e9e-427c-a7a7-a4006da4a394  ip-172-20-0-64.eu-central-1.compute.internal    137522774202\n1460214485361916967 witan-app   staging mesos-8e886703-8611-42fe-b3f4-481a795b131c-S0.e6956c8a-5e9e-427c-a7a7-a4006da4a394  ip-172-20-0-64.eu-central-1.compute.internal    137523996053\n1460214486894360935 witan-app   staging mesos-8e886703-8611-42fe-b3f4-481a795b131c-S0.e6956c8a-5e9e-427c-a7a7-a4006da4a394  ip-172-20-0-64.eu-central-1.compute.internal    137524968688\n1460214488414000343 witan-app   staging mesos-8e886703-8611-42fe-b3f4-481a795b131c-S0.e6956c8a-5e9e-427c-a7a7-a4006da4a394  ip-172-20-0-64.eu-central-1.compute.internal    137526854234\n```\n. fixed\n. ",
    "muradm": "Same with firefox\n. Will this fix the same behavior when using Docker Composer? Or we need another PR to remove container_label_com_docker_composer_* ?\n. Last proposal, with configurable set of labels seems to be most reasonable. In addition to the list it could be a map, something like:\n[-container.labels <label_name_as_set_on_container[:label_name_as_should_be_exported]> ... ]\nwhere label_name_as_should_be_exported is optional, if it is needed to change the name of exported label.\nThat would be great. Shall we open an issue for that?\nThanks in advance\n. same here. ",
    "Random-Liu": "@timstclair Rebased. :)\n. @timstclair Addressed comments.\n. @krallin @timstclair I think engine-api also has the problem, see https://github.com/docker/engine-api/blob/master/types/types.go#L224\n. @timstclair Test failed. :)\n. LGTM\n. @dchen1107 Just did a rebase just now. The code base is changing so fast~ :)\n. Update the PR to address https://github.com/google/cadvisor/pull/1381#discussion_r71241593 only continue when the error is \"file not exist\".\n@timstclair @yujuhong PTAL~\n. vendor/github.com/docker/docker/pkg/system/chtimes.go:47: undefined: setCTime\n. LGTM.\n. @vishh \nThis pull request can be automatically merged by project collaborators\n. @timstclair Cool thanks! Hope we can clean up this someday. :)\n. Verified that this PR causes https://github.com/google/cadvisor/issues/1671 and https://github.com/kubernetes/kubernetes/issues/47744.\nIt seems that Prometheus v0.8.0 added some consistency check https://github.com/prometheus/client_golang/pull/214, which breaks the metrics.\nFiled https://github.com/google/cadvisor/pull/1679 to work around this for now. We should figure out and fix the inconsistent metrics in the future.. @AdoHe Yeah, but even you add sleep:\n1) In theory, it can not guarantee that Tail is initialized after sleep.\n2) During log rotation, it could still happen.\n. > Is it possible to add a unit test for this failure case?\nI can write a simple one. :)\n. @timstclair Addressed comments. :)\n. @timstclair Are you ok with this PR?\n. Without this PR:\n* collected metric container_tasks_state label:<name:\"container_label_annotation_io_kubernetes_container_hash\" value:\"228e8d0c\" > label:<name:\"container_label_annotation_io_kubernetes_container_ports\" value:\"[{\\\"name\\\":\\\"dns-local\\\",\\\"containerPort\\\":10053,\\\"protocol\\\":\\\"UDP\\\"},{\\\"name\\\":\\\"dns-tcp-local\\\",\\\"containerPort\\\":10053,\\\"protocol\\\":\\\"TCP\\\"},{\\\"name\\\":\\\"metrics\\\",\\\"containerPort\\\":10055,\\\"protocol\\\":\\\"TCP\\\"}]\" > label:<name:\"container_label_annotation_io_kubernetes_container_restartCount\" value:\"0\" > label:<name:\"container_label_annotation_io_kubernetes_container_terminationMessagePath\" value:\"/dev/termination-log\" > label:<name:\"container_label_annotation_io_kubernetes_container_terminationMessagePolicy\" value:\"File\" > label:<name:\"container_label_annotation_io_kubernetes_pod_terminationGracePeriod\" value:\"30\" > label:<name:\"container_label_io_kubernetes_container_logpath\" value:\"/var/log/pods/432b46fd-55e0-11e7-b764-42010af00002/kubedns_0.log\" > label:<name:\"container_label_io_kubernetes_container_name\" value:\"kubedns\" > label:<name:\"container_label_io_kubernetes_docker_type\" value:\"container\" > label:<name:\"container_label_io_kubernetes_pod_name\" value:\"kube-dns-2673147055-32wcs\" > label:<name:\"container_label_io_kubernetes_pod_namespace\" value:\"kube-system\" > label:<name:\"container_label_io_kubernetes_pod_uid\" value:\"432b46fd-55e0-11e7-b764-42010af00002\" > label:<name:\"container_label_io_kubernetes_sandbox_id\" value:\"20c8e397df74c045ed8297bc53b91271dd84e9fbd23ba85d9007e9c21abba526\" > label:<name:\"id\" value:\"/kubepods/burstable/pod432b46fd-55e0-11e7-b764-42010af00002/893eb7e95242dd196284f7e10d5b04277c423ac2ff69c26284494b4706a92c21\" > label:<name:\"image\" value:\"sha256:ca8759c215c9c2377bee9425bb3ba547ebf85e511759652bdb5d2d980e4f4a21\" > label:<name:\"name\" value:\"k8s_kubedns_kube-dns-2673147055-32wcs_kube-system_432b46fd-55e0-11e7-b764-42010af00002_0\" > label:<name:\"state\" value:\"stopped\" > gauge:<value:0 >  has label dimensions inconsistent with previously collected metrics in the same metric family\nWith this PR:\n```\ncontainer_spec_cpu_quota{container_label_annotation_io_kubernetes_container_hash=\"d7c8bd4a\",container_label_annotation_io_kubernetes_container_restartCount=\"0\",container_label_annotation_io_kubernetes_container_terminationMessagePath=\"/dev/termination-log\",container_label_annotation_io_kubernetes_container_terminationMessagePolicy=\"File\",container_label_annotation_io_kubernetes_pod_terminationGracePeriod=\"30\",container_label_io_kubernetes_container_logpath=\"/var/log/pods/51605191-55e0-11e7-b764-42010af00002/heapster-nanny_0.log\",container_label_io_kubernetes_container_name=\"heapster-nanny\",container_label_io_kubernetes_docker_type=\"container\",container_label_io_kubernetes_pod_name=\"heapster-v1.3.0-1514755676-z24f5\",container_label_io_kubernetes_pod_namespace=\"kube-system\",container_label_io_kubernetes_pod_uid=\"51605191-55e0-11e7-b764-42010af00002\",container_label_io_kubernetes_sandbox_id=\"89d4ce90493e28474622dc057c5132262198868b789e233e5a983632db580248\",id=\"/kubepods/pod51605191-55e0-11e7-b764-42010af00002/457a9ce101c77b6bfa3a7f03c5072375853e11c7613a6f5bf1952abbcba2e64f\",image=\"sha256:9b0815c8711889802a3081d1a609cc4251357e6ec0a28ac5963aac72bec67691\",name=\"k8s_heapster-nanny_heapster-v1.3.0-1514755676-z24f5_kube-system_51605191-55e0-11e7-b764-42010af00002_0\"} 5000\n. @dchen1107 . Apply LGTM based on https://github.com/google/cadvisor/pull/1685#pullrequestreview-45517760. /lgtm. @timstclair Test failed.. /lgtm. LGTM. LGTM. /ok-to-test. @dashpole Could you help take a look when you have time? Thanks!. /test pull-cadvisor-e2e. @abhi\nW1108 17:12:00.427] vendor/github.com/sirupsen/logrus/text_formatter.go:13:2: cannot find package \"golang.org/x/crypto/ssh/terminal\" in any of:\nW1108 17:12:00.428]     /go/src/github.com/google/cadvisor/vendor/golang.org/x/crypto/ssh/terminal (vendor tree)\nW1108 17:12:00.428]     /usr/local/go/src/golang.org/x/crypto/ssh/terminal (from $GOROOT)\nW1108 17:12:00.428]     /go/src/golang.org/x/crypto/ssh/terminal (from $GOPATH)\n```\nNot all dependencies are included.. /test pull-cadvisor-e2e. @abhi The godep change is merged. Please rebase your PR.\nThe test is passing. I think ready for a review now. :). @abhi LGTM other than hasFilesystem. Because I'm not quite sure what that means. :). > It doesnt need to happen now, but eventually it would be nice to have disk stats for containerd through cAdvisor. k8s would disable disk stats, but it would mean that standalone cAdvisor could still be used to monitor docker after docker rebases on top of containerd.\n@dashpole When docker move to containerd 1.0 image management, I guess the image fs stats in the current docker integration won't work. We need to either change cadvisor to understand the new disk layout, or let cadvisor get fs stats from underlying containerd.\n@abhi I see that we hard code containerd unix socket path now. In the future, after docker moves to the new containerd, if it is using the same socket path, current cadvisor code might get confused.\nThe current containerd integration is mainly for k8s.io containerd namespace, so the containerd namespace may help us avoid trouble. However, we should keep an eye on that.\nPR LGTM other than the last comment.. /lgtm. @dashpole @abhi has removed the rkt comments.. /lgtm. > Just curious, why keep the feature? Just in-case we need it later?\n@dashpole For backward compatibility. There might be users using it, right? It is part of cadvisor api.\nAnd also, maybe we'll also need it someday.\nIf you think it is fine to remove the feature, and it won't break users. I can do that :). @dashpole See this. https://github.com/google/cadvisor/pull/1724/files#diff-b833d5a6501040e03694b19aa7e8ae7bR424. @miaoyq What is the data after this optimization?. /lgtm. @dashpole Can we get this in?. Thanks! @dashpole @miaoyq . @delskev Does docker 18.02.0 has a socket at /var/run/containerd/containerd.sock?\nIf it doesn't, the error message is expected I think.. /lgtm. The api proto was changed after the version we vendor 27d450a01bb533d7ebc5701eb52792565396b084 https://github.com/containerd/containerd/commit/c5022ad92d1cfbf0c0e3cf13875ada48c3000473#diff-57e6caf1fbfa881a24736c37c9ce62d7.\nHowever, because the binary protocol is still compatible, so this shouldn't be a problem.\nAs for this issue, if you want to update containerd version, you need to update the client code correspondingly.. Actually the log is pretty normal...\nI0329 23:48:58.375924   29858 factory.go:356] Registering Docker factory\nI0329 23:48:58.375967   29858 manager.go:302] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp 127.0.0.1:15441: connect: connection refused\nI0329 23:48:58.376129   29858 manager.go:313] Registration of the containerd container factory failed: failed to fetch containerd client version: grpc: the connection is unavailable: unavailable\nI0329 23:48:58.376327   29858 manager.go:318] Registration of the crio container factory failed: Get http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info: dial unix /var/run/crio/crio.sock: connect: no such file or directory\nIt's similar with rkt and cri-o. And the log level is already v 5. https://github.com/google/cadvisor/blob/master/manager/manager.go#L313. @dashpole Hm, ok. I can build locally, but I guess it is using my local GOPATH.. @dashpole Done.. /test pull-cadvisor-e2e. Fixed the test.. ACK, :)\n. Here, it is checking all sub directories, even though one directory returns error, we can still check other directories?\n. > One example of potential issues is that kubelet hits the limits of the inotify watches, but it continues to start up without watching any pods. We can argue that this may happen anytime when kubelet is running, so there is no need to fail the startup. I am ok with that argument, but just want to point out that this is possible and we should try ensuring there is a recovery path. For example, maybe cadvisor should be not ready if errors like this keep happening, and kubelet should report node not ready?\nWe don't have a good way to expose errors now. Failing cadvisor makes dubug easier, because the system will fail early when there is unexpected error.\nHowever, failing cadvisor will make the system less robust, especially for temporary errors.\nFor now, the only temporary error we know is the file not exist error. Maybe we can log the error only if the path doesn't exist to fix current issue, and fail cadvisor otherwise. In the future, if we find that there are a lot more temporary errors, we can consider how to handle the errors better.\n@yujuhong @timstclair WDYT?\n. Will do!\n. Can you use go1.8?. Why do we need to update golang.org/x/net?. Do we need this? Why only convert error for TaskPid?. nit: s/%s/%q. nit: Use k8sContainerdNamespace?. nit: unnecessary empty line.. Why canHandle=false and canAccept=true? Don't quite understand. :p \nAnd should we load task instead, and check whether task is running?. nit: Separate group for cadvisor.. I still a bit concern. Have you tried without containerd running? How long does it take to try connecting containerd?. Do we need this change? It seems that we are not using it.. This doesn't seem right to me. What rootfs do we need here? It seems that we are using this rootfs to get network stats.\nHowever,\n1) Why docker/cri-o are using /rootfs?\n2) There may not be proc inside container, right? e.g. an image built from scratch.. This is for custom metrics. We need to figure out whether it's ok to not support it.\n@dashpole Are we still using custom metrics?. What does this mean? I saw both cri-o and docker have this.\nHowever, because we don't collect image filesystem stats, so just make sure.. @dashpole I don't think we could get docker container ip in cadvisor, when we are using kubenet. So do we really support this?. TBH, I don't know.\n1) The pid we get is host pid, right? Why don't we just access /proc/pid/net/ns?\n2)  Container's /proc mount is in container's mount namespace, right? Can we access it on the host?\n3) What does /rootfs mean in docker/crio? Does that mean container root filesystem?. I believe the inHostNameSpace here, means whether cadvisor is running in host namespace or inside container.\nWhen cadvisor is running inside a container, we mount host root filesystem to /rootfs.. So we should have same code with docker, and remove the container rootfs related logic.. See here.. Still. Don't understand this one.. Remove brackets.. Check running?. Never used?. Thanks @dashpole @DirectXMan12!\nConfirmed that this doesn't work anymore. We don't need to support it.. Why do you add this back?. I believe there is a race condition.\nLet's use sync.Once. Hm. Good question.. It seems that grpc has some reconnect logic, can you try it?. ... my bad. Done. No, we check err == nil before this, right?. ",
    "ainsleyc": "Just signed it.\n. Sorry for multiple requests, was trying to get the CLA check to work correctly.\n. Sure, thanks for the feedback! I'll make the suggested changes and resubmit.\n. I think that covers everything. I tried to squash some of the commits together, but it causes git to forget the containers.js history.\nFeel free to let me know if there are any other changes you'd like. Thanks!\n. Cool, I didn't know you could manipulate git history to this degree. Thanks for the tip.\n. Yup, I've clicked through and believe that everything is loading correctly without errors in the browser console.\n. Ah. So how would you like to handle the boilerplate header for the generated file? I guess the options are to prepend it in the asset script, or omit that file from the boilerplate check script.\n. Addressed comments from review.\n. Haha... seems the disk on the machine Jenkins is running on is full?\n. Addressed comments. Thanks!\n. 1. Yeah, this is probably caused by the sorting based on the simplistic average cpu/memory. Let me try re-sorting the sliced top N containers by name before displaying them, that will definitely reduce the churn at least in the likely common case where N > number of containers.\n2. As you say, I think this one is caused by time offset between the subcontainers. I could try to use some heuristic to detect how much to trim, but might be discarding information relevant to the user. I'm inclined to leave it as-is, unless you have a suggestion how it might be done?\n3. I think this is true of all the charts. Happy to take it on, though let's do it in a separate commit.\n. Added the sorting of the slices by name, let me know if that's not working for your use case.\n. Maybe I am misunderstanding how \"go get\" works, or the build environment is different somehow?\nWhen I delete both the go-bindata src and executable in the bin from my $GOPATH and run the \"go get\" command, the script is working for me. \n. Ah, my bad. I thought \"mktemp\" was an arbitrary name, didn't realize it was a utility.\n. I had actually tweaked the function inputs so that it would accept a trailing list of directories like go-bindata does, so I believe it should be working as intended.\nNo harm in adding the quotes though, so I've done so.\n. For the subcontainers, the timestamps of the metrics are not aligned. If I understand the DataTables API correctly, this means I need to generate a series of sparse arrays of equal length, one for each unique timestamp. Ex:\n[timestamp1, null, null, 23],\n[timestamp2, null, 322, null],\n[timestamp3, 664, null, null],\n...\n. I don't believe the order matters, the library must handle it for you.\n. ",
    "xiangpengzhao": "@vishh Does cAdvisor support pod IP:port now? If so, how?\nAnother question, what if the node can't access the pod IP?\nThanks!. @DirectXMan12 Thanks for your information! \nAny idea about the question what if the node can't access the pod IP? In Kubernetes environments, the node network and the pod network may be isolated (depends on the third-party CNI network plugin). Then kubelet (cAdvisor) can't access the pod IP. In this case, how to collect custom metrics since we can neither access the pod IP nor use container's network namespace ? Thanks!. @alex88  Seems like config like this file sample_config_endpoint_config, then cAdvisor will access pod IP:port. Not so sure about this.. It seems like this issue is due to code GetStats. It only return stats of the first interface.\nref: https://github.com/google/cadvisor/issues/686 https://github.com/google/cadvisor/pull/751\n/cc @vishh @rjnagal @vmarmol . /ok-to-test\nnote: this command is a test of https://github.com/kubernetes/test-infra/issues/5793. ",
    "alex88": "@DirectXMan12 @xiangpengzhao it supports pod IP:port, how? with the init containers?. ",
    "mtaufen": "A few initial thoughts :)\n- I am a big fan of this kind of cleanup work. Thank you.\n- I second previous comments wrt global state. But this is definitely a step in the right direction.\n- It looks like there's some initialization of components going on in main, are these things that will have to move to a separate e.g. Init function to make this a library? If you have an Init code path, you'll be able to pass config into that and from there to sub-components and can probably get rid of the global state. \n- It might be worth adding an explicit validation stage for the config before any sub-components start running.\n. > We already use cAdvisor in \"library form\" in the kubelet, mostly accessed through the manager package. Generally, the main package is doing similar things to what the kubelet sets up with the manager, plus a few extras that aren't relevant to us. I think this is working ok.\nAck.\n\nI think you mean component constructors, as opposed to an actual golang init function? The golang init function adds more global state, and should be avoided... To reduce the global state, I would pass the config through to the various component constructors.\n\nYup. Constructors. Definitely not golang's init. Sorry for the confusion.\n\nI like this idea, but from the library perspective, it's not clear where this validation would go. It could be a separate function that is the responsibility of the library-user to call before starting any cAdvisor components. I think a better option is to just make each sub-component validate the relevant part of the config (I think it already works this way to some extent).\n\nYeah, I suppose that's the easiest path to having validation and people being able to pick-and-choose what components they use. It might be nice to expose a validation function for each component that a library user could independently call to validate config without actually constructing the component. That way people can still front-load their validation if they want to. \n. @k8s-bot test this issue: #IGNORE\n. @k8s-bot test this issue: #IGNORE\n. Nice.\n. Chicken vs. egg problem: Jenkins GCE e2e won't pass until this goes in. :)\n. Sure.\n. cc @timstclair @vishh \n. We will just cut a tag for this one, and will include in the release notes when we cut a release from master.\n. Don't merge this yet, I think I did something wrong.. That looks better. Something smelled funny about +200k loc.... Aaaaaand godep is still failing to put all of the necessary dependencies in vendor.. Or maybe I forgot to add some untracked files... let's hope it was that.. Nope. Godep is still cruelly omitting these:\nW0824 21:13:12.192] vendor/github.com/docker/docker/pkg/symlink/fs.go:16:2: cannot find package \"github.com/docker/docker/pkg/system\" in any of:\nW0824 21:13:12.192]     /go/src/github.com/google/cadvisor/vendor/github.com/docker/docker/pkg/system (vendor tree)\nW0824 21:13:12.192]     /usr/local/go/src/github.com/docker/docker/pkg/system (from $GOROOT)\nW0824 21:13:12.192]     /go/src/github.com/docker/docker/pkg/system (from $GOPATH)\nW0824 21:13:12.193] vendor/github.com/sirupsen/logrus/text_formatter.go:13:2: cannot find package \"golang.org/x/crypto/ssh/terminal\" in any of:\nW0824 21:13:12.193]     /go/src/github.com/google/cadvisor/vendor/golang.org/x/crypto/ssh/terminal (vendor tree)\nW0824 21:13:12.193]     /usr/local/go/src/golang.org/x/crypto/ssh/terminal (from $GOROOT)\nW0824 21:13:12.193]     /go/src/golang.org/x/crypto/ssh/terminal (from $GOPATH). Yes, this looks like what I was trying to do - essentially get runc to head so both cadvisor's and kubelet's runc dependency use the same integer types (runc waffled between int64 and uint64 for an API that could accept -1 as a parameter, and eventually settled back on int64).. ",
    "luxas": "/cc @vishh \n. It seems like it's these files that are using cgo: \n- utils/cpuload/netlink/defs.go\n- utils/procfs/jiffy.go\nOf course, the best way of fixing it would be to workaround/not have to depend on these C libraries, but I guess it's just fine with static CGO linking.\nI'm sending a patch so we'll see if it passes.\n. Please squash to one commit, then LGTM for me. \n@vishh \n. You have to gofmt -s -w utils/machine/machine.go\n. I would just add GOARCH=arm to go build command in build/build.sh.\nIt should just work.\nHowever, if we want to cross-build a docker image, we probably have to base it on debian for other arches than amd64. See the hyperkube image how to do this: https://github.com/kubernetes/kubernetes/blob/master/cluster/images/hyperkube/Makefile\nIf we decide on a naming convention for non-amd64 cAdvisor and think we should do it, I may send a PR that builds cAdvisor images for arm, arm64, ppc64le.\n. I may do it if you want it\n. @mboussaa I'm not really sure what you mean.\nOn an ARM host, cAdvisor built for ARM may monitor ARM containers without problems.\nThe current cadvisor image on docker hub is built for linux/amd64 (like the most other things), so it won't run on an ARM host at all\n. Ahh, doesn't it recognize me? :)\nAlso cc @timstclair \n. @vishh Tests passed. Are you confident enough to merge it :smile:?\nThere will be time to evalute it before v0.24.0\n. Argh, I see.\nWill you set up an other CI or should I update the line you linked to?\n. Thanks @djtm, will try it tomorrow\n. It seems like a flake @vishh \n. @k8s-bot test this\n. Seems like a ssh issue\n. I don't think this is related to my PR:\nError 1: failed to make remote testing directory:\n. Seems like you forgot #1237, which was opened ages ago :( and failed due to internal flakes\nHowever, I'm glad this is in, and I've been experimenting with linking kubelet statically...\n. No no. This is good as-is :)\nInstead, please review kubernetes/kubernetes#26028 :)\n. (Just a small note, I don't think the case was inside a container, it was in the localkube vm)\nBut it might be a bug nonetheless. At least it's confusing in case it's not required\n. Any clue how to solve?\n. @vishh @timstclair Will this be fixed before v1.3?\n. @vishh Do you have any clue what to write here to make it work?\nI tried a lot of different combinations, but the linker just didn't parse it.\n. ",
    "djtm": "Static compile works fine, haven't gone through the tests though:\n``` bash\nexport GOPATH=~/src/go\nexport PATH=\"$PATH:$GOPATH/bin\"\napt-get install golang-go # build does not work in alpine\nhttps://github.com/kubernetes/kubernetes/issues/19464\ngit clone --depth 1 https://github.com/google/cadvisor.git\ngo get github.com/tools/godep\ngo get -d github.com/google/cadvisor\nbuild statically for scratch image\ngodep go build --ldflags '-extldflags \"-static\"' github.com/google/cadvisor\n```\nThe important part is --ldflags '-extldflags \"-static\"'\nStatic scratch image at https://hub.docker.com/r/djtm/cadvisor-386/\nYou can use the same run command as usual.\n. > flag provided but not defined: -extldflags \"-static\"\nMaybe it doesn't eat the =? ->\nerr := RunCommand(\"godep\", \"go\", \"build\", \"--ldflags\", \" '-extldflags \\\"-static\\\" ' \", \"github.com/google/cadvisor\")\nMaybe we need not escape here.\nerr := RunCommand(\"godep\", \"go\", \"build\", \"--ldflags\", \" '-extldflags -static ' \", \"github.com/google/cadvisor\")\n. Ha, great. :+1: . ",
    "sprohaska": "I tried a few changes on the es storage driver a while ago: See 3 commits up to\nhttps://github.com/sprohaska/cadvisor/commit/bc1e500b05da8e33df5fe9b9d3f01f5d8a8a5ac5.\nFeel free to use my commits as a basis.\nI won't continue my work, since I've decided in the meantime to collect cAdvisor stats via fluentd instead of sending stats directly to Elasticsearch.\n. I think this might cause a panic if --disable_metrics is not empy:\ngoroutine 1 [running]:\n    panic(0xb0c8c0, 0xc82020cdd0)\n        /usr/local/go/src/runtime/panic.go:481 +0x3e6\n    main.(*metricSetValue).Set(0x15ac528, 0x7fffe9354f34, 0x4, 0x0, 0x0)\n        /go/src/github.com/google/cadvisor/cadvisor.go:85 +0x1da\n    flag.(*FlagSet).parseOne(0xc820066060, 0xc820118801, 0x0, 0x0)\n        /usr/local/go/src/flag/flag.go:881 +0xdd9\n    flag.(*FlagSet).Parse(0xc820066060, 0xc820064110, 0x3, 0x3, 0x0, 0x0)\n        /usr/local/go/src/flag/flag.go:900 +0x6e\n    flag.Parse()\n        /usr/local/go/src/flag/flag.go:928 +0x6f\n    main.main()\n        /go/src/github.com/google/cadvisor/cadvisor.go:99 +0x68\n    panic: assignment to entry in nil map\nI'm unsure about the details. I'm not very familiar with Go.\n. ",
    "lkzcgfvf": "@krallin \nthanks a lot. It works this way. The only concern about this - I need to know how many cores there are on each host.\nif I have hosts with different number of cores in my cloud I need to hardcode this number for each host during visualization. \nDo you know is there a way to avoid it? May be to get this info from Docker daemon and store into InfluxDB? Or may be calculate percentage and store it to InfluxDB together with raw data?\n. ",
    "outrunthewolf": "@krallin @lkzcgfvf\nDid you ever figure out how to programatically check the number of cores on a server? i have the same issue, as all my boxes have different CPU's. ",
    "RoyceShen1": "@krallin \ni just did the exact same query,but the results show some negative figures and zeros \nSELECT derivative(\"value\", 1s)/1000000000  FROM \"cpu_usage_total\" WHERE (\"container_name\" = 'apinfogather') AND time >= now() - 30m\n\ni am using cadvisor v0.24 and influxdb v1.0\n. have the same problem. I  am wondering how can I display cpu usage in percentage, seems that grafana is not able to deal with raw cpu data in percentage. . @dashpole \nok, I'll show you a complete screen shot\n\nthis is my mongo container.what confuses me is that the RSS doesn't match the Total Usage that is shown below in the graph. The number of Total Usage is way higher.  Doesn't this \"Total Usage\" mean the memory usage which this container takes up?\nhere is the mongo container memory usage i got by top\n\nthank you\n. ",
    "Dean-Christian-Armada": "I am also encountering the negative \n```\n\nSELECT DERIVATIVE(value) FROM cpu_usage_total ORDER BY time DESC LIMIT 3;\nname: cpu_usage_total\ntime                derivative\n----                ----------\n1513575234810842321 -13920085468.139048\n1513575234646245689 940772015055.5693\n1513575233854110295 -198917284788.05984\n```. \n",
    "ausil": "Thanks for the comments, pushed up updated patch without the extra i and a fix for the feedback\n. I signed it!\n. commits squashed into 1\n. ",
    "solidnerd": "@luxas \nI tried to cross compile with Docker 4 Mac and a golang:1.6.3 container.\n1. docker run -it --rm  golang:1.6.3\n2. apt-get update && apt-get install vim\n2. go get -d github.com/google/cadvisor\n3. cd $GOPATH/src/github.com/google/cadvisor\n4.  vim build/build.sh and add env GOARCH=arm GOBIN=$PWD go \"$GO_CMD\" ${GO_FLAGS} -ldflags \"${ldflags}\" \"${repo_path}\"\n5. make build\nI get the following error:\nbash\nvendor/github.com/opencontainers/runc/libcontainer/cgroups/systemd/apply_systemd.go:17:2: no buildable Go source files in /go/src/github.com/google/cadvisor/vendor/github.com/coreos/go-systemd/util\nMakefile:35: recipe for target 'build' failed\nDo you know what I'm doing wrong ?\n. ",
    "Brain-Gamer": "I compiled cadvisor and managed to edit the dockerfile to work on my raspberry pi.\nI uploaded the result here.\nI decided to compile it my self because any other versions weren't up to date :D. @rjlee sure! It takes maybe a few days but I will do it ;). @rjlee I forked this repo and added the information you asked for in the readme. If you need anything else just ask ;). ",
    "rjlee": "@Brain-Gamer could you provide the dockerfile/instructions so others can do the same ?. @Brain-Gamer many thanks for sharing.. ",
    "vaibhavsood": "Hi, want to check if the cadvisor docker image can be considered for being extended to be multi-arch on docker hub? (as per above comments right now its for intel/amd64 arch)\nThe process for doing so would be along the lines as per here:  https://github.com/docker-library/official-images#multiple-architectures\nMy interest here is to ultimately see a cadvisor docker image under https://hub.docker.com/u/ppc64le/\nHowever since cadvisor is not a official Docker image, the first step i guess would be to make it official https://docs.docker.com/docker-hub/official_repos/#how-do-i-create-a-new-official-repository and then move to making the dockerfiles multi-arch (this process is as per Docker)\nWant to check if this makes sense/any comments?. ",
    "whatever4711": "Hi,\ncurrently we tried to port cAdvisor to a multiarchitecture Docker Image. The source repository is unibaktr/docker-cadvisor.\nTherefore, we restructured the Dockerfile to use a slim Debian based image, since all binaries, cAdvisor uses, are rarely available for Alpine based multiarchitecture images.\nFeel free to try it out\ndocker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro --volume=/dev/disk/:/dev/disk:ro --publish=8080:8080 --detach=true --name=cadvisor unibaktr/cadvisor\nPerhaps we can find a way to integrate our work it into the official cAdvisor Repo.\nBest regards\n. ",
    "devx": "I tested manually, it seems like an SSH issue for me.\n. I have made the suggested changes.\n-Victor\n. Documentation updated.  Thank you for the guidance, and patience.\n. ",
    "nickvanw": "The latest version in the Docker registry does not have Docker 1.11 support. If you build from source and use the Dockerfile in build/ you can get something that works. \n. ",
    "yuvipanda": "Did this switch happen?\n. ",
    "zhangjianleaves": "thanks it is working perfectly!  I didn't find this flag on document...\n. ",
    "nik-kor": "I signed it!\n. done. rebased it\n. done\n. ",
    "aaskey": "Full id doesn't work either:\n$ curl http://localhost:4194/api/v1.0/containers/docker/3ff966ba487fe2bb9f775913cff2f16e09ac34934b7f6c3c23ef0a9e593da671\nfailed to get container \"/docker/3ff966ba487fe2bb9f775913cff2f16e09ac34934b7f6c3c23ef0a9e593da671\" with error: unknown container \"/docker/3ff966ba487fe2bb9f775913cff2f16e09ac34934b7f6c3c23ef0a9e593da671\"\nKubelet /stats/summary is great. I can use it in some scenario. Is there any documents talking about Kubelet API? In addition, does Kubernetes provides similar stats in its API? If not, is there any consideration to do so?\n. @vishh , thanks for the clarification. Hope Kubernetes metrics api can be available as soon as possible. \nCan you or someone help with the cAdvisor api question? Is API changed but document not updated? \n. Bingo. The v1.2 API works for both docker container ID and name. Thank you so much. \n. Ok, the container name is shown correctly in newer Docker version. Thanks for clarifying the Docker version supported in testing. I might have missed it, but I didn't see this mentioned anywhere, can we add the info to the build page or somewhere?\n. ",
    "chenchun": "Happened to do the same thing. Maybe adding another make target docker-build instead of depending on DOCKER env makes makefile cleaner?\n. Sorry, I missed accept := name == \"/\" || !*dockerOnly. > @dashpole I don't think we could achieve what @chenchun needs with the current approach\n@WanLinghao Yes, I'm quite confused about the current approach. How does it stop kubelet from collecting /system.slice/sshd.service?\n\nWe could set docker_only=true&& rawPrefixWhiteList = \"/system.slice/kubelet.service, /system.slice/docker.service..\"\n\ndocker_only flag will be removed, but we can use this for now. I wonder if there is any replaceable flag?\n\nWe could set docker_only=true&& rawPrefixWhiteList = \"/system.slice/kubelet.service, /system.slice/docker.service..\"\n\nLike I said, one problem is that container runtime may not starts before kubelet and runtime cgroup path can be created after kubelet started, so it would be hard to add a static white list path prefix on creating cadvisor interface.\nSo I'm suggesting the two solutions, but it seems the second one of adding a blacklist to cadvisor raw factory will work unless you change your kubelet and runtime cgroup path out of system.slice and changing cgroup path is painful for existing clusters.\n\nchange cadvisor to make it watch the input cgroup paths dynamically when invoking GetContainerInfoV2() or GetContainerInfo() if it doesn't watch them before\n\nI prefer this approach, is it acceptable?\n. ~~ref https://github.com/kubernetes/kubernetes/pull/21337 , kubelet does checking runtime cgroups periodically rather than passing those in when we start cAdvisor to avoids race with runtime uptime. So I don't think this is an acceptable approach.~~\nkubelet ensures runtime cgroup is exactly the same as runtime-cgroups, https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/dockershim/cm/container_manager_linux.go#L94-L99. ",
    "iaguis": "\nWonder if removing that flag is a feasible approach? cc @iaguis @alban @jonboulle\n\nIf we remove that flag, the limits applied to the unit file won't apply to the processes in the pod, which is kind of a fundamental goal for rkt.\n. > so we can't set limits on the command line if we run \"rkt run\" its only settable if run as a systemd unit?\nNot now, but it can be implemented. Note that if you pass a bigger value on the command line than the one in the \"outer\" service file, the value on the service file will be respected.\n\nAnd there's no way to determine what the limits of the cgroup you are running in are?\n\nYou can read the values of the controllers of your own cgroup hierarchy (check /proc/self/cgroup).\n. You mean not inheriting by default (that is, don't pass --keep-unit)?\n\nedit: also what do --cpu and --memory do now then?\n\nThey are per-app limits, not pod-level limits.\n. ",
    "minhdanh": "I upgraded influxdb to version 0.12 and the errors were gone. Closing now.\n. ",
    "leonardinius": "I've just experienced the same behavior.\n. ",
    "kamaltherocky": "i have the same query. Any help would be great?. ",
    "jcmcken": "I also have this issue, and don't think it's possible given the data that cAdvisor pushes into Influx. If it pushed the number of CPUs with the CPU metrics, that would solve the issue.\nA related problem is that cAdvisor seems to create one measurement table per metric, rather than putting related metrics in the same table (e.g. tx_bytes and rx_bytes are in separate tables, rather than being separate columns in the same table). Since Influx doesn't support table joins, you can't do any sort of math between metrics.. Any updates on this?. @VariantMonkey @dashpole Where does cAdvisor get the number of CPUs in that JS snippet? That's not pushed into Influx AFAIK. So something like Grafana doesn't have access to that info to do the percentage calculations.. Same issues here. The workaround won't work for me without doing something custom like writing a wrapper. We have some hosts that mount it one way and some that mount it the other (not sure if there's really any guarantee which way it will go). We're running cAdvisor in Mesos, so the task configuration can't be different per host. I would imagine there would be a similar problem in Kubernetes.\nIf there isn't, I think there needs to be some treatment of the cgroup names when cgroups are comounted. I don't think there's any guarantee of the ordering when e.g. cpu and cpuacct or net_cls and net_prio are comounted under the same hierarchy. The comounted names need to be considered identical if the members of the comount (regardless of order) are the same.. ",
    "roberthbailey": "/cc @andyzheng0831 \n. ",
    "shilpamayanna": "Thanks @vishh. With this PR, we are trying to add support to obtain filesystem stats for all device files used by container. \nWe can leverage the support from PR #1204 for thin-provisioned devices.\nHowever, the implementation doesn\u2019t seem to handle the block devices shown in the example below.\nnvme1n1                     259:0    0    10G  0 disk /var/lib/kubelet/pods/d3162804-1222-11e6-9aea-2c600c82eccc/volumes/datawise.io~volume/data\nnvme2n1                     259:1    0    10G  0 disk /var/lib/kubelet/pods/d30a9040-1222-11e6-9aea-2c600c82eccc/volumes/datawise.io~volume/data\nCould you please let me know if there is any active work around this area to obtain stats for these devices?\n. @vishh Thanks for the reply. There is a check to ensure that the statistics are retrieved for block devices only and other mounts that are not backed by devices are skipped. If the mounts are shared between containers, the stats are collected for all the containers sharing the device.\nSummary API provides metrics for the filesystem usage and capacity only. \nIs there a plan to add support for disk metrics?\n. @vishh We use disk stats to calculate IOPs used by containers. Is there any other way to compute IOPs?\n. @vishh Thanks for providing the pointers. I observed that DiskIO stats reported contains device major and minor number. In a cluster with multiple nodes, these numbers could overlap across nodes. We need a way to map these stats to the volume. May be container\u2019s mountpoint would help. \nAlso it would be helpful to export these DiskIO stats to heapster.\n. @vishh Thanks. Will open issue against heapster. \nHow can I get the machine stats?\n. ",
    "rkand4": "@mboussaa Did you resolve this issue? I am facing exactly the same issue.. ",
    "godliness": "Hi pmorie, would you please tell us what package should be going to install to avoid this error?\n. ",
    "ae6rt": "@pmorie Sorry, I don't see this package called devicemapper-persistent-data in a naive\nyum provides devicemapper-persistent-data\nWhere can CentOS users get thin_ls?\n. @pmorie Thanks.  I installed this package, but find no thin_ls anywhere:\n$ sudo find / -name thin_ls\n<no output>\nBased on the comments in this issue, I would have expected to find thin_ls somewhere under /.  Was I wrong?\n. On CentOS7, I see this package info\n$ yum info device-mapper-persistent-data \nLoaded plugins: fastestmirror\nLoading mirror speeds from cached hostfile\n * base: mirror.tocici.com\n * extras: centos.mirror.ndchost.com\n * updates: mirrors.usc.edu\nInstalled Packages\nName        : device-mapper-persistent-data\nArch        : x86_64\nVersion     : 0.5.5\nRelease     : 1.el7\nSize        : 1.2 M\nRepo        : installed\nFrom repo   : base\nSummary     : Device-mapper Persistent Data Tools\nURL         : https://github.com/jthornber/thin-provisioning-tools\nLicense     : GPLv3+\nDescription : thin-provisioning-tools contains check,dump,restore,repair,rmap\n            : and metadata_size tools to manage device-mapper thin provisioning\n            : target metadata devices; cache check,dump,metadata_size,restore\n            : and repair tools to manage device-mapper cache metadata devices\n            : are included and era check, dump, restore and invalidate to manage\n            : snapshot eras\nwould you mind posting your corresponding output?  \nThanks much.\n. No problem.  I'll just build it from source.  For those of us new to thin_ls, though, what does it do for us in Kubernetes?\n. ",
    "rikatz": "Actually, this isn't a Kubernetes Label, but a Docker Label also (when using --restart=always directive):\ndocker inspect nginx1 |grep -i restartcount\n        \"RestartCount\": 2,\nThis is a pure Docker 17.03 running, no Kubernetes. I'm going to check what's the impact to change cAdvisor code to also set a metric containing the RestartCount of the containers.. I signed it!!\n. This PR solves the issue #1490 :)\n. @dashpole Did as requested. It's a pretty old PR, so I didn't tested anymore but it's not a really huge change, so it might work :). Since the drivers from 2 and 5 aren't compatible, we need to have 2 drivers :(\nI've faced this very same issue in Heapster.\nThere are some prettier solutions here (maybe using an interface, configuring the version in storage URI when starting cAdvisor), but the point here is that we still need 2 drivers :)\nHaven't tested this implementation by itself. I've missed an _test file.\nAbout the refactor of cAdvisor storage (as in #1458), can't say something about this (as I'm no expert in developing) but sounds better to me anyway to have plugable storage instead of maintaining each solution market. By the way, would prometheus be a 'storage' implementation, or should we keep this separate as a '/metric' implementation? :)\n. Hi @dashpole.\nActually it exposes through API and not through cgroups. I was trying to search were in the code cAdvisor gets its data.\nI don't think it's limited to cgroups, because of the 'Labels' and Image retrieval, but I couldn't figure out how to add also the restartCount metrics :)\nSomething probably specified here: https://github.com/google/cadvisor/blob/d7a44cb1a2c66e1688ccdc5d09e56069eecb659a/info/v1/container.go#L128\nThanks!\n. @dashpole FIY, made a PR here using DockerHandler and the API :)\nHope this is right!\nThanks!. This was closed in PR #1649. @dashpole Made the comments for you. Only have to check the v2 API.\nAbout Kubernetes engine, it uses an internal counter and you can export those metrics to prometheus using kube-state-metrics.\nAbout rkt, its API  doesn't support a restart count (as in CoreOS, as an example this is controlled through systemd) and maybe this can be removed.\nThis is also why this 'metric' is handled only by the Docker Handler, and shows a value of '0' in other systems (like raw and RKT).\nMaybe using some conditional in Prometheus metrics would make it less noisy, like if the restart count equals to 0 there is no need to export/show it anyway.\nAbout using this as a label, I don't think this is a good idea when exporting this info to Prometheus and Kafka, as this is much a 'counter' then a static identifier.\nThanks for your time and your review :)\n. @dashpole @timstclair OK, understood :)\nI though that creating this metric would make easier to monitor some container problems, but I did understand what's the main focus of cAdvisor now :)\nNo problems, I can even create a standalone golang program to expose only this, or maybe using metricbeat or zabbix to expose this info also would be fine :)\nAnyway, I'm going to make the suggested changes (between today and tomorrow), and if You have any suggestion in how (and if we should) to achieve this in cAdvisor, I'm here available for :)\nThanks for your time guys.. @dashpole @timstclair ok, kubernetes exposes this as a label:\ncontainer_cpu_system_seconds_total{[...]container_label_io_kubernetes_container_restartCount=\"0\"[...]} 1.29\nSo, if I change the handler from Kubernetes to create this as a label (inside GetStats or something like that), but only for Docker, would this be fine?\nThe same happens in API v1, but v2 doesn't exposes the Labels, so it have to be checked later as something to be migrated, regardless of being a restart count or not.\nThis is not the perfect solution for my case (as this is a Label and not a Counter for Prometheus), but as Kubernetes also does this way, I think this is the way to be followed.\nMay I change some parts of the code to accomplish this way?\nThanks!. @dashpole The changes I've made here insert the restartcount as a label instead of creating a new stats.\nThis mades easier to implement (also, I don't need to change the API code, and this will be available in APIv2 as someone ports also the labels handler). This is the same methodology kubernetes uses to expose to outside systems how many restarts a POD has suffered (together with some other labels, as namespace, container name, etc).\nPlease take a look. I don't think cAdvisor's main feature is to transform some internal Docker Counters into Labels, but I do agree this is probably the best way we can do this instead of changing the stats core.\nIf, anyway this is not ok, then I'm going to write something else (or maybe fork cAdvisor), as I need this info to Prometheus to check if there is a container in a crash loop or something else (Out of memory, etc), to trigger an alarm to me.\nThanks anyway guys :smile: . @dashpole Anything about this? Thanks :D. @dashpole friendly ping :). @dashpole Sure, will do this.. Did as asked :) Anything else you need I'm here available to help! And thanks for your time :D. Thanks :). @derekwaynecarr any sugestion on replacing the variable name, or shall we keep this?\n. @dashpole I've tested this and works fine. The counter is increases each time a container restarts :) This is only a handler of 'how to gather this information', and it's used here. Is that needed? Didn't know about that. Will check and post a new commit wheter this is necessary.. The idea here is to make available the times a countainer has been restarted to Prometheus. This is why I've created this metric (and also this PR), so we can check and alert here when a container have a big restart count.\nIs this really necessary to be removed?. Yes, actually the new handler defines how the info is going to be gathered by GetStats function. So I assume this point of review is solved :). My bad, removed this :). Yeah, this is vscode fault. Corrected this :). ",
    "Magikon": "kubectl describe po nginx1 | grep \"Restart Count\"\n      Restart Count:  0. ",
    "goettl79": "Testet it with cadvios 0.24.1 by mounting the journalctl binary into the container like this:\n--volume=/usr/bin/journalctl:/usr/bin/journalctl:ro \nThis change makes the oom parser at least start: \nmanager.go:1082] Started watching for new ooms in manager\nHowever, the shared libraries for journalctl as well as the journalctl data is missing inside the container. I'd suggest to install journalctl in the cadvisor image and actually change the logic that determines if journalctl or syslog is used. My proposal is to change it based on program arguments, not on existence of the journalctl binary.\n. I think this would not be a dependency to cadvisor, rather a change in the dockerfile. . This fix  would also make the packaging issue of cadvisor / journalctl inside a docker container obsolete.  https://github.com/google/cadvisor/issues/1313. If I'm right, the cadvisor bugfix version 0.26.1 release and branch do not include the commit \"8aed6e9\", which contained the prometheus metrics. Might be a release / merge bug. Building and running the binary from the current master works. A rerelease / cherry pick to the branch 0.26.1 is necessary.. ",
    "ZimboBoyd": "I signed it!\n. ",
    "shikhovmy": "I signed it!\n. ",
    "thebillkidy": "I have this problem as well, I need the CreationTime on my Kafka queue. Is this on the roadmap or should I create my own PR?\n. ",
    "NoumanSaleem": "@jimmidyson thanks, totally agree with you. My actual issue (which I guess wouldn't be solved by this GH issue) is that I have some containers which execute in <1 min  and I'd like to use prometheus + cadvisor to alert on container_tasks_state{com_amazonaws_ecs_container_name=\"...\", state=\"running\"}[; however It seems like cadvisor doesn't record these containers sometimes resulting in false alerts.\n. Still dealing with this, so I decided to run a trial.\nLocally, I ran the following docker-compose file:\n```\ncadvisor:\n  image: google/cadvisor\n  command: --log_cadvisor_usage=true\n  ports:\n    - \"8080:8080\"\n  volumes:\n    - /:/rootfs:ro\n    - /var/run:/var/run:rw\n    - /sys:/sys:ro\n    - /var/lib/docker/:/var/lib/docker:ro\nprometheus:\n  image: prom/prometheus\n  ports:\n    - \"9090:9090\"\n  volumes:\n    - ./prometheus.yml:/etc/prometheus/prometheus.yml\n  links:\n    - cadvisor\n```\nWith the following prometheus.yml\n```\nglobal:\n  scrape_interval:     1m\n  evaluation_interval: 1m\nscrape_configs:\n  - job_name: 'prom'\n    static_configs:\n      - targets: ['127.0.0.1:9090']\n\njob_name: 'cadvisor'\n    static_configs:\ntargets: ['cadvisor:8080']\n```\n\n\n\nOnce prometheus had completed a few scrapes, I ran the following docker-compose file three times consecutively, waiting about a minute between runs.\n```\ntest60:\n  image: alpine\n  command: /bin/sh -c \"sleep 60; exit 0;\"\ntest30:\n  image: alpine\n  command: /bin/sh -c \"sleep 30; exit 0;\"\ntest10:\n  image: alpine\n  command: /bin/sh -c \"sleep 10; exit 0;\"\n```\nLastly, I queried prometheus using:\ncount_over_time(container_tasks_state{image=\"alpine\",state=\"running\"}[1h])\nThe result showed:\nElement,Value\n{container_label_com_docker_compose_config_hash=\"b223f362152debbce7399b955ba04ff16e579ab593e9a5e05b7d4c0cc3e983ce\",container_label_com_docker_compose_container_number=\"1\",container_label_com_docker_compose_oneoff=\"False\",container_label_com_docker_compose_project=\"cadvisortest\",container_label_com_docker_compose_service=\"test30\",container_label_com_docker_compose_version=\"1.8.0\",id=\"/docker/82ab6b070a618111e30a8b69ced2022fb31af43fe79eb361140d0efe2acd807f\",image=\"alpine\",instance=\"cadvisor:8080\",job=\"cadvisor\",name=\"cadvisortest_test30_1\",state=\"running\"}    1\n{container_label_com_docker_compose_config_hash=\"632f9aabd0b5f54f481338f31ac871ace8d5c5f8f20a21dcd9b3bdeda13378f3\",container_label_com_docker_compose_container_number=\"1\",container_label_com_docker_compose_oneoff=\"False\",container_label_com_docker_compose_project=\"cadvisortest\",container_label_com_docker_compose_service=\"test60\",container_label_com_docker_compose_version=\"1.8.0\",id=\"/docker/14a08442c0848a08549e9c04a652c1611997203f2c41836e8cea047ea8019c7d\",image=\"alpine\",instance=\"cadvisor:8080\",job=\"cadvisor\",name=\"cadvisortest_test60_1\",state=\"running\"}    3\n{container_label_com_docker_compose_config_hash=\"89806db0cd00c051cdc4d55f3a8d28e3c11723b994d6c7dc3404fd57ecb90419\",container_label_com_docker_compose_container_number=\"1\",container_label_com_docker_compose_oneoff=\"False\",container_label_com_docker_compose_project=\"cadvisortest\",container_label_com_docker_compose_service=\"test10\",container_label_com_docker_compose_version=\"1.8.0\",id=\"/docker/97d46566140dd6161f36350d47d6e54a63e5b7fe4425584e1f3b2da7d530a247\",image=\"alpine\",instance=\"cadvisor:8080\",job=\"cadvisor\",name=\"cadvisortest_test10_1\",state=\"running\"}    1\nOnly the container which ran for 60 seconds was captured all three times. \nIf this is not a cadvisor use case, please let me know. I was hoping to use prometheus + cadvisor container_tasks_state to alert for short lived jobs not running, instead of adopting the prometheus push gateway.\nOther than this, I've had a fantastic experience with cAdvisor. Thank you!\nSystem: OSX\nDocker version: 1.12.0\n. @timstclair thank you, that was my confusion. It seems more appropriate to use the pushgateway instead then.\n. @timstclair thanks. Does cAdvisor have a configurable retention duration?\n. ",
    "kvadevack": "+1. What about this PR?. I see.. ",
    "iter-io": "@googlebot - Any chance this will be merged soon?. ",
    "fejta": "ok to test\n. @pwittrock if that doesn't work try directing the comment at k8s-bot....\n. Looks like it is testing now! http://kubekins.dls.corp.google.com:8081/job/cadvisor-pull-build-test-e2e/646/\n. @pwittrock or another admin needs to make the test this comment he did above\n. Should we close out this PR? . Yes I've of the repo admins can do that. You need the --modify flag. Are you a repo admin? You need to mark the Jenkins GCE e2e as not require and instead mark pull-cadvisor-e2e as required. . In the repo settings. . ",
    "mwringe": "Anything I can do to get this looked at?\n. @jimmidyson Interesting, that can be a useful option for some people. Not eveyone is going to want to add in an extra Prometheus endpoints, especially if they are already using Jolokia endpoints.\nThe system already support HTTP based endpoints and Prometheus ones. Is there some policy against supporting another endpoint type?\n. This PR also fixes a few issues with the current implementation which makes using custom metrics cumbersome, regardless of which endpoint type you are using. \n- we can now gather the ip address directly from the container instead of having to specify the exact url. This means we don't need to know before hand which ip address or hostname the container will be running under. It also means we don't need to use use hostports when dealing with kubernetes pods.\n- ability to pass certificates to be used by cadvisor when fetching metrics from endpoints. This allows the endpoint to be secured with certificate based authentication.  We probably still would want to add in more security here (eg basic auth) and we would probably also want to add this to the non-Jolokia endpoints as well\n. OK, I have created a new PR here https://github.com/google/cadvisor/pull/1349 which adds in support for gathering the ip address of docker containers and being able to read files within docker containers.\nOnce we get consensus and approval for that PR, I will update this one with only the Jolokia parts\n. Hmm, anyone know what is going on with the e2e tests? \n. Bump, anything I can do to get this pushed through any faster?\n. Ok, I have updated this PR to bring it into line with the latest from master and to take into affect the comments made.\nA few changes:\n- I removed the ability to read from container directly. Originally I had this in there to allow the ability to pull out a ca cert from a container to verify self signed https endpoints, but I think it makes a lot more sense to just trust any endpoint, regardless if its certificates are necessarily valid or not.\n- I also updated the Prometheus and generic collectors to be able to read from a container's ip address directly instead of requiring the ip address to be hardcoded. This will make things a lot more useful when dealing with things such as containers running in Kubernetes\n. Additional commit to make the http client used by the collectors to be configurable. This will allow clients to use certificate based authentication.\n. @timstclair Can you please be more specific? This PR is does a few things, all of them are improvements for the collectors. Do you just mean the second commit I applied to this issue? Or do you want the first commit also broken down more?\nHow receptive is cAdvisor to fixes and updates? I am wondering how difficult its going to be to get custom metrics functioning properly. Its been a bit frustrating to have pr succumb to bit rot.\n. @timstclair Ok, I have updated my PR again, this time all the collector stuff has been removed. Can you please take a look?\nNote that I also have another 2-3 PRs to submit which are all dependent on each other. I am really hoping this is not going to take over a month each to get reviewed and merged\n. Rebased to master\n. Ok, the requested change has been applied.\n. @timstclair any opinion on this?\n. Essentially the need for InsecureSkipVerify is because not having it set it doesn't actually improve security and would otherwise require a bunch of hoops for developers to jump through.\n- cAdvisor can read metrics from a http endpoints, it doesn't verify the authenticity of the container exposing the metrics. If a developer were to switch on or off https, it doesn't change the security of how metrics are stored within cAdvisor.\n- it is common for containers to self-sign their own certificates. In this case, the container would have to pass to cAdvisor the CA certificate to trust. But since both are coming from the container, this doesn't make anything more secure (this was the reason why originally I had exposed the option to read a file from a container, to pass the CA cert).\n- if we are accessing the endpoint via its ip address, in order for the endpoint to be acceptable, it has to have the ip address listed under its certificate. Since the ip address of the container is most likely not known before the container is started, this can make it difficult to configure.\nIn the end I couldn't figure out a reason why we shouldn't enabled InsecureSkipVerify. We are not actually verifying the authenticity of the container which expose metric endpoints.\nIn terms of why a container would want to expose an https endpoint instead of an http one. There are a few reasons:\n- they may already have an https endpoint already configured for some other process and they may not want to expose another http one just for cAdvisor.\n- security. If the endpoint wanted to expose metrics in a secure fashion, they would need to do so over an https connection. This could mean either by only trusting connections from clients with the cAdvisor certificate, or requiring the client to verify themselves (in which they would have to provide the username/password in the endpoint configuration file). With either of these, having InsecureSkipVerify shouldn't have an affect on the security of the system. \n. I made the change to rename the httpClients to collectorHttpClient to make it more apparent its only meant for the collectors. And also rebased to master.\nIn terms of the InsecureSkipVerify concern, would making this a configurable option for each individual endpoint be a more palatable option? Eg we add a 'InsecureSkipVerify' option to https://github.com/google/cadvisor/blob/master/collector/config.go in which the configuration can either set it to true/false (with a default of false)\n. Rebased.\n. @vishh have you had a chance to look this over yet?\n. Sorry for the long delay. I have updated this PR with the requested changes.\nI did not change what happens when a metric collection fails (eg retries) as this is how all the other collectors work. An issue has been opened for changing how this works (https://github.com/google/cadvisor/issues/1440)\n. @vishh would it be possible for you to take another look at this? Thanks\n. @vishh @timstclair anything else which needs to be modified with this?\n. @vishh @timstclair anything else I can do here to get this moving along?\n. @vishh for a version field, if one is not specified would you expect a failure right away? or an attempt to read it?\n. @timstclair any comments on this?\n. I think with https://groups.google.com/forum/#!topic/kubernetes-sig-autoscaling/MnPUaMY8aDM a lot of things are being reconsidered. \n. The main support I added was for custom metrics. This includes being able to directly use endpoints from the container itself (instead of having full hardcoded URLs), allowing SSL endpoint access, and being able to provide a certificate which is exposed to custom endpoints (so the endpoints can verify that it is cAdvisor asking for the metrics).\nIf there is anything in particular you would like for me to elaborate on, please let me know.\n. Ah, I can add quotes around the 'used' to make it seem more clear that the name of the attribute is 'used' (the used amount of HeapMemoryUsage)\n. Its a java component which can be used to expose metrics (along with other management features). https://en.wikipedia.org/wiki/Java_Management_Extensions\n. Name is a user generated name for the metric. This is what would show up to the user from the cadvisor results.\nAttribute is something specific to the mbean.\n. All of the collectors currently act this way, its not something specific to the Jolokia endpoint. I have created an issue to track better handling of the collectors when an error condition occurs when trying to fetch a metric endpoint (https://github.com/google/cadvisor/issues/1440).\n. ",
    "LudicrousSpeed": "I am seeing this same issue with both go 1.6 and 1.5\n. Running from the latest release branch(v0.23) has fixed the issues.\n. ",
    "waleedsamy": "I signed it!\n. @thomaso-mirodin @vishh I rebased it again.\n. ",
    "ronnielai": "I fixed it hopefully :). \n. Hmm.. I've done a rebase. Hope it's happy this time.\n. @derekwaynecarr could you add some tests?\n. I was referring to adding simple unit tests. :)\nLGTM overall.\n. @timstclair \n. Do you mean something like:\ndockerStatus := info.DockerStatus{}\n    tempStatus, err := docker.Status()\n    if err != nil {\n            glog.Warningf(\"Unable to connect to Docker: %v\", err)\n    } else {\n           dockerStatus = tempStatus\n    }\n. Done\n. Fixed.\n. Since this logic is repeated in 2 places, maybe put them in a utility method?\n. ",
    "aveshagarwal": "@pmorie @sjenning @derekwaynecarr\n. Just to clarify that it happens with docker-1.9 too.\n. @sjenning functionality wise your patch seem to work on rhel7, though not sure about its correctness/assumption.\n. @sjenning A part of your patch is doing exactly what is being done in DockerThinPoolName, so you could call this I think.\n. ",
    "imikushin": "I've signed it!\n. ",
    "nitinmulchandani": "Thanks for you reply.. No.. i can't \ni have just setup a ubuntu server (with docker and cadvisor setup) on cloud and then from my windows machine i want to access the the API's.. so how to make this possible ?\n. ",
    "crawford": "Minor nit, the correct extension for that file should be .ign or .ignition per the IANA MIME registration.\n. ",
    "krousey": "cc @dchen1107 \n. @timstclair What is the cherrypick process for this repo?\n. @timstclair \n. ",
    "ixdy": "@k8s-bot test this please (reconfigured Jenkins job)\n. let's try that again.\n@k8s-bot test this please\n. Yeah, just saw that. https://github.com/kubernetes/test-infra/pull/243\n. again!\n@k8s-bot test this please\n. yay!\n. @k8s-bot ok to test\n. It looks like the jenkins@agent-pr-21 ssh key is missing from the project metadata for kubernetes-jenkins-pull, same for agent-pr-80.\nWe may have exceeded the ssh key limit for the project and started pushing some keys off. The cadvisor tests are more vulnerable to this since they use vanilla ssh instead of gcloud ssh, which would ensure the public keys were added.\nBut really we should make sure the jenkins master ssh key is added to metadata for the project (it appears to be missing, too!) and make the cadvisor jobs use the master ssh key, which is in Jenkins secrets, rather than each host's own key. This would match the behavior of the e2e and node e2e jobs.. @krzyzacy no, we should fix the cadvisor test so it uses the Jenkins secret instead.. my best guess is that changing golang versions caused this (Jenkins went 1.6 -> 1.8).. oops. I thought something looked odd about that.\n/lgtm. I believe our test infra merges the PR into the branch at HEAD when running tests, so even if the PR is out of date, it'll test as if it were to merge at that point. (This is different from Travis, I believe, which just tests the PR as-is.)\nIf a PR was created, tests were run, and then it was merged many days later (without explicitly running tests again), it's possible that the old test results would be stale and invalid. This is why we periodically re-run tests on LG'd PRs in k/k and always re-run before merging, though I also don't think the \"require branches to be up-to-date\" option existed when we first implemented the SQ, either.. you probably wanted the apt-get clean after the apt-get install to remove the .debs downloaded.. ",
    "yogihardi": "Using Mac.\nand yes, you're right, it's running well on linux.\nthanks.\n. ",
    "Paritosh-Anand": "Getting the same error. Is there no way to run this project on Mac(amd64) ? \nmake build\n```\n\n\nbuilding binaries\nbuilding cadvisor\ncontainer/common/inotify_watcher.go:20:2: no buildable Go source files in $GOPATH/src/github.com/google/cadvisor/vendor/golang.org/x/exp/inotify\nmake: *** [build] Error 1\n```\n. \n\n",
    "mffiedler": "@sjenning I'm on it.   Should be able to have that for you in the next day or so.\n. @sjenning  Here are two heap profiles:  https://paste.fedoraproject.org/390678/\nFirst is 3 minutes after completion of 250 pods starting on the node.\nSecond is 20 minutes later with no activity on the cluster\nGraph of kbmemused identical to the one above\n. @sjenning, ran one more time - identical test....heap profiles attached.\npprof.localhost_10248.inuse_objects.inuse_space.001.pb.gz\npprof.localhost_10248.inuse_objects.inuse_space.002.pb.gz\n. We can let it soak for 26-ish hours and collect memory stats over the last bit to see if it flattens out.\n. ",
    "ujovlado": "Any chance to review this? (cc: @vishh @timstclair )\n. ok, great.\n. Hi, is there something I can help with?. Oh, I didn't notice that. Thanks!. Hi, I can see this issue had almost no activity in past year. What's currently the best way to add custom driver?. ",
    "outcoldman": "I signed it!\n. Our company Splunk Inc. has signed Contributor License Agreement\n. ping @timstclair \nseems like bot got stuck, as it took me too long to reply \"I signed it!\"\n. @timstclair did not know it was a requirement, changed to my working email \n. I signed it!\n. @timstclair  yes, it is associated. I am a little bit confused by the sign process, our company signed \"Google Software Grant and Corporate Contributor License Agreement\" in January, but without specifying any accounts (at least not mine). Does the bot just tracks the company domain in emails?\nBtw about the failure in the unit tests: in one of the unit tests I verify that storage works as expected when Splunk is not available and will retry to send stats when it will be available. Also because I specify the limit for the buffer (and retry logic) - this is why this test generates error logs. How should I handle this case?\n. @timstclair awesome, thanks!\n. @timstclair Hey Tim, any updates on this?\n. @vishh sure, we can take a look on that as well, when do you plan to switch to 1.8, right after release?\n. ",
    "gmarek": "@vishh @timstclair - this is causing startup errors in our test jobs, can one of you review this?\n. ",
    "matthiasr": "We supposedly signed a corporate CLA, however, I have no idea how to figure out whether that is actually in place. The CLA pages show nothing.\n. It seems the signing stalled on our end. Hold on while I follow up \u2026\n. The CLA is really in place now, sorry about that.\n. I'm also for keeping application metrics separate, coming from a) the Prometheus tradition of small, focussed components and b) not using the App/Kubelet/Heapster/Google Cloud Monitoring pipeline and being wary of carrying too much of that around in environments where it doesn't translate.\n\nyou may want to have the container runtime or a daemon collect all the metrics together and present them in one call\n\nWhy would I want that? Why would I want it on a once-per-node level, not a once-per-pod or a once-per-cluster (which is basically Prometheus with the default Kubernetes config)?\n. No, 8aed6e9 introduces a different set of metrics. I suspect this issue is another instance of #1704.. cc @grobie . According to https://github.com/google/cadvisor/issues/1690#issuecomment-313597011 the fix in 0.26.1 isn't working / incomplete, maybe this is the same problem?. Does this problem happen on a cAdvisor built from master, which includes #1679?. @dashpole this also needs to be fixed in the Kubernetes 1.7.x series, or it will be impossible to collect useful container metrics for anyone who relies on the Prometheus format.\nIf the real fix is too complex for a cherry-pick, there is an option that can be passed when initialising the Prometheus client that turns off these validations and restores the previous behaviour. Of course, not producing invalid metrics in the first place is preferable.\nShouldn't these errors have shown up in log files all over the place?. Related discussions are ongoing in kubernetes/kube-state-metrics#194 \u2013 I thought there is a knob but in reality it's just using a hacked up Prometheus client. I think @bboreham's fix is the right way to go and should be cherry-picked both onto the cAdvisor and Kubernetes release branches.. Aha, another knob. Looks like #1679 is not sufficient, since the issue still persists?. But would that whitelist be mandatory?\nEven for an unbounded set of labels, I think the following would work:\n\ncollect all label names\ninitialize a map with these names as keys, all values as empty strings\nfor each metric\nmake a copy of the map\nin the copy, set the values for each label that has one\ngenerate the metrics with all labels, empty or not\n\nThis way, the consistency condition will be fulfilled, but because the\nPrometheus server actually treats empty values the same as the label not\nbeing present, maintains current behavior at query time.\nThis is a bit inefficient (multiple passes over the data, lots of copies\nand allocations), maybe someone can come up with a better solution?\nOn Tue, Aug 29, 2017, 00:48 David Ashpole notifications@github.com wrote:\n\nonce we have the label whitelist #1730\nhttps://github.com/google/cadvisor/issues/1730, we can ensure all\nwhitelisted labels are present as metric labels, or set them as \"\"\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1704#issuecomment-325504774,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAICBg21L0iGbsqT6Bfg3b-CtcIKLllsks5sc0O8gaJpZM4Oixos\n.\n. Only at each point in time.\n\nOn Wed, Aug 30, 2017, 23:46 David Ashpole notifications@github.com wrote:\n\nDoes the set of labels need to be consistent across time, or just at a\nsingle point in time? That would only work if we don't need consistency\nacross time.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1704#issuecomment-326128466,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAICBlgyW0sZuqfsBYCgmZkdWFefPU61ks5sddgwgaJpZM4Oixos\n.\n. In principle, yes, but there's a limit to how far into the future you can predict which labels there will be. Things get wonky if this changes all the time, and depending on the queries potentially at the point of change, but at some point things do need to change.\n\nAn approach that is pretty common (and that kube-state-metrics uses) is to contain the variable label sets in a separate \"foo_labels\" metric. This metric would need to deal with this variability but all the other metrics would have a fixed set of labels. This pushes the responsibility for getting labels and actual metrics together to query time, with the hope that at that point you know which labels you want. This kind of joining is possible in Prometheus, but I don't know if there are other systems that consume this endpoint; and if you want to do this kind of fundamental change at all.. I'm for the LabelFixingRegistry in the library. We already have two\nconcrete examples at hand that need this, and if we don't solve it\ngenerally we'll just end up with several badly copy-pasted versions of the\nsame thing.\nOn Thu, Aug 31, 2017, 17:35 Bj\u00f6rn Rabenstein notifications@github.com\nwrote:\n\nOK, nobody likes approach 1. Fair enough. @matthiasr\nhttps://github.com/matthiasr also told me, that was not what he meant.\nMy bad for not reading carefully enough.\nHowever, when it receives the metrics, Prometheus checks that all metrics\nin the same family have the same label set, and rejects those that do not.\nThis sounds like a bug on the Prometheus side. This should cause the whole\nscrape to fail, not silently drop metrics. Partial data is to be avoided.\nI guess, \u201cPrometheus\u201d above means \u201cthe Prometheus client library\u201d. The\ndefault behavior is indeed to fail the whole scrape, but you can explicitly\nset a continue on error behavior to still serve as many metrics as\npossible, see\nhttps://godoc.org/github.com/prometheus/client_golang/prometheus/promhttp#HandlerErrorHandling\n.\nI like the approach of a \u2026_info metric with all those container labels\ninstead of assigning them everywhere. That's also how Kubernetes labels are\nhandled. However, this approach is orthogonal to solving the consistency\nproblem (it just reduces it to only one metric family).\nLet's figure out if we want support for this in client_golang at all. If\nnot, you know what to do in cAdvisor, and the same has to happen in\nkube-state-metrics. I'll document that approach in client_golang then.\nIf we feel we should have support in client_golang, the question would be\nbetween LenientRegistry (easy to implement, five lines of code or\nsomething) or the LabelFixingRegistry (slightly more complicated). In any\ncase, it would be opt-in with a lot of warning signs attached to it.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1704#issuecomment-326333626,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAICBi8dhW_mlC2OSpkig4TpAkR46-Luks5sdtKrgaJpZM4Oixos\n.\n. \n",
    "zouyee": "you could tag label in json file when you create a app\n. https://github.com/google/cadvisor/issues/1641\nFROM google/cadvisor\nRUN apk add --update findutils && rm -rf /var/cache/apk/*. ",
    "atombender": "@timstclair: Is that merger's job or mine?\n. @timstclair: Done.\n. ",
    "bah2830": "Is there anything else I need to do to get k8s-bot testing approved?\n. I don't have a problem with that. I can get it updated when I get home from work.\nThanks\n. ",
    "asultan001": "Hey guys thanks for the feedback and comments. @vishh the original driver for wanting to do this was to enable windows support in k8s. Having said that, if cadvisor is the wrong abstraction layer then we'd be happy to consider something else, however without some alternative, we'll have a big gap on the Windows side of things. \nThere are a few other options we can move forward with but we wanted to keep the implementation as symmetric as possible between Windows and Linux (as long as they make sense). A couple alternatives:\n- Implement some rudimentary monitoring using straight Windows Perf Monitors to provide sufficient capability for k8s needs\n- Leverage another monitoring solution like snap since it's been proven it can already work with k8s\n- Wait for the container runtime interface to provide a stats interface but it seems like this would take too long.\nAny suggestions?\nThanks in advanced,\nAbe\ncc: @alexbrand \n. @vishh thanks for the update, a few more thoughts on this:\n\nOk. I was wondering if providing monitoring for windows containers is also a goal. For example, cAdvisor is already being used widely for monitoring docker containers on Linux, even outside of k8s.\n\nIn part that is why we originally thought that supporting cAdvisor on windows would make sense as well even beyond the value for k8s. \n\nI'm optimistic about this interface. There are quite a few people working towards implementing this interface.\n\nDo you have an idea of the ETA for this? This may be the easiest route to implement if we are simply trying to unblock the requirements for k8s at this point. Also, is the goal to ship this as part of 1.4? from the conversation on #27097 and the comment on that issue by @timstclair it seems like this is not currently prioritized.\nCheers\nAbe\n. ",
    "alexbrand": "Hey @vishh, as @asultan001 mentioned, we are working on supporting the kubelet on windows. From @timstclair's and your comments, it seems like the Container Runtime API will have a stats interface that we could then implement on the Windows side using perf counters. This sounds like a reasonable approach. On my end, I have to do a bit more research into what stats are collected by cAdvisor, and what the Kubelet is interested in.\n. ",
    "ichekrygin": "@derekwaynecarr: do you know by any chance if this fix made its way into v1.3.5?\n. ",
    "wallnerryan": "same\n. centos 7, docker 1.12.6,  google/cadvisor:latest. ",
    "otakup0pe": "seeing similar on a host with a lot of short lived containers... Ubuntu 16.04.2 LTS, Docker 17.04.0-ce, google/cadvisor:latest.. ",
    "steinbrueckri": "same ...\n\nCentOS Linux release 7.3.1611 (Core)\nDocker version 1.12.6, build 3a094bd/1.12.6\ngoogle/cadvisor:latest (cAdvisor version v0.25.0 (17543be)). In my case it's a \"long running\" container... but yes i have also some \"short lived\" containers.. +1. \n",
    "narcoticfresh": "Incredible that this issue it still open.. it is quite prominent.. AFAIK, everybody in a swarm cluster and their nodes using overlayfs, has this issue.. obviously, the proc structures/folders in those folders are pseudo files and du is having problems with that..\n--> if only we could configure some exclude pattern (--exclude= option from du) on cadvisor.. . ",
    "cajturner": "I am getting the same error as @marksullivancrowd \n. I am getting the same error as @marksullivancrowd \n. ",
    "ripun": "I am also getting same error \nI0921 10:51:49.077889       1 storagedriver.go:50] Caching stats in memory for 2m0s\nI0921 10:51:49.079061       1 manager.go:140] cAdvisor running in container: \"/docker/f388d0af1e7af95910b978cf5952457ce33f75aa8338613f1db680be5d6c7c1c\"\nW0921 10:51:49.088118       1 manager.go:148] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused\nI0921 10:51:49.095053       1 fs.go:116] Filesystem partitions: map[/dev/mapper/docker-253:1-9144532-367762f1736f18be685e3b77748876f38ab6b65355943fadb060be71c8676c70:{mountpoint:/ major:252 minor:2 fsType:xfs blockSize:0} /dev/vda1:{mountpoint:/var/lib/docker/devicemapper major:253 minor:1 fsType:xfs blockSize:0} /dev/mapper/docker-253:1-9144532-c92bccf7a2ec59be6d94486cf7e58e346e305fe5f986ba89ae85e54b8443a2c9:{mountpoint:/rootfs/var/lib/docker/devicemapper/mnt/c92bccf7a2ec59be6d94486cf7e58e346e305fe5f986ba89ae85e54b8443a2c9 major:252 minor:1 fsType:xfs blockSize:0}]\nI0921 10:51:49.097158       1 manager.go:195] Machine: {NumCores:2 CpuFrequency:2600000 MemoryCapacity:8004255744 MachineID:1d188a370466406fbb093d606a0478eb SystemUUID:5271A133-2134-45B0-96FB-0C234A2D4F43 BootID:3f999247-cedc-4483-988a-68113136f62e Filesystems:[{Device:/dev/mapper/docker-253:1-9144532-367762f1736f18be685e3b77748876f38ab6b65355943fadb060be71c8676c70 Capacity:10725883904 Type:vfs Inodes:10484736 HasInodes:true} {Device:/dev/vda1 Capacity:12873142272 Type:vfs Inodes:12581664 HasInodes:true} {Device:/dev/mapper/docker-253:1-9144532-c92bccf7a2ec59be6d94486cf7e58e346e305fe5f986ba89ae85e54b8443a2c9 Capacity:10725883904 Type:vfs Inodes:10484736 HasInodes:true}] DiskMap:map[252:0:{Name:dm-0 Major:252 Minor:0 Size:107374182400 Scheduler:none} 252:1:{Name:dm-1 Major:252 Minor:1 Size:10737418240 Scheduler:none} 252:2:{Name:dm-2 Major:252 Minor:2 Size:10737418240 Scheduler:none} 253:0:{Name:vda Major:253 Minor:0 Size:12884901888 Scheduler:none}] NetworkDevices:[{Name:eth0 MacAddress:fa:16:3e:51:21:a3 Speed:0 Mtu:1454}] Topology:[{Id:0 Memory:8388198400 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]} {Id:1 Memory:0 Cores:[{Id:0 Threads:[1] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:4194304 Type:Unified Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}\nI0921 10:51:49.097712       1 manager.go:201] Version: {KernelVersion:3.10.0-327.10.1.el7.x86_64 ContainerOsVersion:Alpine Linux v3.4 DockerVersion:1.12.1 CadvisorVersion:v0.24.0 CadvisorRevision:0cdf491}\nE0921 10:51:49.102474       1 factory.go:291] devicemapper filesystem stats will not be reported: unable to find thin_ls binary\nI0921 10:51:49.102491       1 factory.go:295] Registering Docker factory\nW0921 10:51:49.102518       1 manager.go:244] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused\nI0921 10:51:49.102525       1 factory.go:54] Registering systemd factory\nI0921 10:51:49.102657       1 factory.go:86] Registering Raw factory\nI0921 10:51:49.102786       1 manager.go:1082] Started watching for new ooms in manager\nW0921 10:51:49.103264       1 manager.go:272] Could not configure a source for OOM detection, disabling OOM events: unable to find any kernel log file available from our set: [/var/log/kern.log /var/log/messages /var/log/syslog]\nI0921 10:51:49.103910       1 manager.go:285] Starting recovery of all containers\nI0921 10:51:49.164319       1 manager.go:290] Recovery completed\nF0921 10:51:49.164348       1 cadvisor.go:151] Failed to start container manager: inotify_add_watch /sys/fs/cgroup/cpuacct,cpu: no such file or directory\n. ",
    "bmouthrob": "Hi,\nI don't think the above issue is fixed, I have tried both cadvisor and cadvisor-canary and am getting similar symptoms on an Amazon Linux instance...\nI0208 22:08:06.528378       1 storagedriver.go:50] Caching stats in memory for 2m0s\nI0208 22:08:06.528672       1 manager.go:140] cAdvisor running in container: \"/docker/aeca51a748adbe6c12dc08550ed15024194544b4235fcb66144bee3dbac08334\"\nW0208 22:08:06.542050       1 manager.go:148] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused\nI0208 22:08:06.566575       1 fs.go:116] Filesystem partitions: map[/dev/mapper/docker-202:1-395786-98d5d321dc375464f3fb4cdc61dfc09e97c813ce39fe7bde32c1cdef15a7f7b7:{mountpoint:/ major:253 minor:13 fsType:xfs blockSize:0} /dev/xvda1:{mountpoint:/var/lib/docker/devicemapper major:202 minor:1 fsType:ext4 blockSize:0}]\nI0208 22:08:06.570021       1 info.go:47] Couldn't collect info from any of the files in \"/etc/machine-id,/var/lib/dbus/machine-id\"\nI0208 22:08:06.570107       1 manager.go:195] Machine: {NumCores:2 CpuFrequency:3000000 MemoryCapacity:16037875712 MachineID: SystemUUID:EC2FE94F-E16E-29BC-DC54-897C42293EDC BootID:e22f450d-be25-4472-a042-b290c9e76394 Filesystems:[{Device:/dev/mapper/docker-202:1-395786-98d5d321dc375464f3fb4cdc61dfc09e97c813ce39fe7bde32c1cdef15a7f7b7 Capacity:10725883904 Type:vfs Inodes:10484736 HasInodes:true} {Device:/dev/xvda1 Capacity:8318783488 Type:vfs Inodes:524288 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:107374182400 Scheduler:none} 253:3:{Name:dm-3 Major:253 Minor:3 Size:10737418240 Scheduler:none} 253:7:{Name:dm-7 Major:253 Minor:7 Size:10737418240 Scheduler:none} 253:8:{Name:dm-8 Major:253 Minor:8 Size:10737418240 Scheduler:none} 253:9:{Name:dm-9 Major:253 Minor:9 Size:10737418240 Scheduler:none} 202:0:{Name:xvda Major:202 Minor:0 Size:8589934592 Scheduler:noop} 253:10:{Name:dm-10 Major:253 Minor:10 Size:10737418240 Scheduler:none} 253:6:{Name:dm-6 Major:253 Minor:6 Size:10737418240 Scheduler:none} 253:12:{Name:dm-12 Major:253 Minor:12 Size:10737418240 Scheduler:none} 253:4:{Name:dm-4 Major:253 Minor:4 Size:10737418240 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:10737418240 Scheduler:none} 253:11:{Name:dm-11 Major:253 Minor:11 Size:10737418240 Scheduler:none} 253:13:{Name:dm-13 Major:253 Minor:13 Size:10737418240 Scheduler:none} 253:2:{Name:dm-2 Major:253 Minor:2 Size:10737418240 Scheduler:none} 253:5:{Name:dm-5 Major:253 Minor:5 Size:10737418240 Scheduler:none}] NetworkDevices:[{Name:eth0 MacAddress:12:18:27:50:94:b8 Speed:0 Mtu:9001}] Topology:[{Id:0 Memory:16037875712 Cores:[{Id:0 Threads:[0 1] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:47185920 Type:Unified Level:3}]}] CloudProvider:AWS InstanceType:r4.large InstanceID:i-0176c03a9ed8c4cb3}\nI0208 22:08:06.570741       1 manager.go:201] Version: {KernelVersion:4.4.41-36.55.amzn1.x86_64 ContainerOsVersion:Alpine Linux v3.4 DockerVersion:1.12.6 CadvisorVersion:v0.24.1 CadvisorRevision:ae6934c}\nE0208 22:08:06.580310       1 factory.go:291] devicemapper filesystem stats will not be reported: unable to find thin_ls binary\nI0208 22:08:06.580328       1 factory.go:295] Registering Docker factory\nW0208 22:08:06.580342       1 manager.go:244] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused\nI0208 22:08:06.580349       1 factory.go:54] Registering systemd factory\nI0208 22:08:06.580781       1 factory.go:86] Registering Raw factory\nI0208 22:08:06.581134       1 manager.go:1082] Started watching for new ooms in manager\nW0208 22:08:06.581430       1 manager.go:272] Could not configure a source for OOM detection, disabling OOM events: unable to find any kernel log file available from our set: [/var/log/kern.log /var/log/messages /var/log/syslog]\nI0208 22:08:06.581769       1 manager.go:285] Starting recovery of all containers\nI0208 22:08:06.581837       1 manager.go:290] Recovery completed\nF0208 22:08:06.581865       1 cadvisor.go:151] Failed to start container manager: inotify_add_watch /var/lib/docker/devicemapper/mnt/98d5d321dc375464f3fb4cdc61dfc09e97c813ce39fe7bde32c1cdef15a7f7b7/rootfs/sys/fs/cgroup/cpu: no such file or directory\n. Hi,\nI don't think the above issue is fixed, I have tried both cadvisor and cadvisor-canary and am getting similar symptoms on an Amazon Linux instance...\nI0208 22:08:06.528378 1 storagedriver.go:50] Caching stats in memory for 2m0s\nI0208 22:08:06.528672 1 manager.go:140] cAdvisor running in container: \"/docker/aeca51a748adbe6c12dc08550ed15024194544b4235fcb66144bee3dbac08334\"\nW0208 22:08:06.542050 1 manager.go:148] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused\nI0208 22:08:06.566575 1 fs.go:116] Filesystem partitions: map[/dev/mapper/docker-202:1-395786-98d5d321dc375464f3fb4cdc61dfc09e97c813ce39fe7bde32c1cdef15a7f7b7:{mountpoint:/ major:253 minor:13 fsType:xfs blockSize:0} /dev/xvda1:{mountpoint:/var/lib/docker/devicemapper major:202 minor:1 fsType:ext4 blockSize:0}]\nI0208 22:08:06.570021 1 info.go:47] Couldn't collect info from any of the files in \"/etc/machine-id,/var/lib/dbus/machine-id\"\nI0208 22:08:06.570107 1 manager.go:195] Machine: {NumCores:2 CpuFrequency:3000000 MemoryCapacity:16037875712 MachineID: SystemUUID:EC2FE94F-E16E-29BC-DC54-897C42293EDC BootID:e22f450d-be25-4472-a042-b290c9e76394 Filesystems:[{Device:/dev/mapper/docker-202:1-395786-98d5d321dc375464f3fb4cdc61dfc09e97c813ce39fe7bde32c1cdef15a7f7b7 Capacity:10725883904 Type:vfs Inodes:10484736 HasInodes:true} {Device:/dev/xvda1 Capacity:8318783488 Type:vfs Inodes:524288 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:107374182400 Scheduler:none} 253:3:{Name:dm-3 Major:253 Minor:3 Size:10737418240 Scheduler:none} 253:7:{Name:dm-7 Major:253 Minor:7 Size:10737418240 Scheduler:none} 253:8:{Name:dm-8 Major:253 Minor:8 Size:10737418240 Scheduler:none} 253:9:{Name:dm-9 Major:253 Minor:9 Size:10737418240 Scheduler:none} 202:0:{Name:xvda Major:202 Minor:0 Size:8589934592 Scheduler:noop} 253:10:{Name:dm-10 Major:253 Minor:10 Size:10737418240 Scheduler:none} 253:6:{Name:dm-6 Major:253 Minor:6 Size:10737418240 Scheduler:none} 253:12:{Name:dm-12 Major:253 Minor:12 Size:10737418240 Scheduler:none} 253:4:{Name:dm-4 Major:253 Minor:4 Size:10737418240 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:10737418240 Scheduler:none} 253:11:{Name:dm-11 Major:253 Minor:11 Size:10737418240 Scheduler:none} 253:13:{Name:dm-13 Major:253 Minor:13 Size:10737418240 Scheduler:none} 253:2:{Name:dm-2 Major:253 Minor:2 Size:10737418240 Scheduler:none} 253:5:{Name:dm-5 Major:253 Minor:5 Size:10737418240 Scheduler:none}] NetworkDevices:[{Name:eth0 MacAddress:12:18:27:50:94:b8 Speed:0 Mtu:9001}] Topology:[{Id:0 Memory:16037875712 Cores:[{Id:0 Threads:[0 1] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:47185920 Type:Unified Level:3}]}] CloudProvider:AWS InstanceType:r4.large InstanceID:i-0176c03a9ed8c4cb3}\nI0208 22:08:06.570741 1 manager.go:201] Version: {KernelVersion:4.4.41-36.55.amzn1.x86_64 ContainerOsVersion:Alpine Linux v3.4 DockerVersion:1.12.6 CadvisorVersion:v0.24.1 CadvisorRevision:ae6934c}\nE0208 22:08:06.580310 1 factory.go:291] devicemapper filesystem stats will not be reported: unable to find thin_ls binary\nI0208 22:08:06.580328 1 factory.go:295] Registering Docker factory\nW0208 22:08:06.580342 1 manager.go:244] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused\nI0208 22:08:06.580349 1 factory.go:54] Registering systemd factory\nI0208 22:08:06.580781 1 factory.go:86] Registering Raw factory\nI0208 22:08:06.581134 1 manager.go:1082] Started watching for new ooms in manager\nW0208 22:08:06.581430 1 manager.go:272] Could not configure a source for OOM detection, disabling OOM events: unable to find any kernel log file available from our set: [/var/log/kern.log /var/log/messages /var/log/syslog]\nI0208 22:08:06.581769 1 manager.go:285] Starting recovery of all containers\nI0208 22:08:06.581837 1 manager.go:290] Recovery completed\nF0208 22:08:06.581865 1 cadvisor.go:151] Failed to start container manager: inotify_add_watch /var/lib/docker/devicemapper/mnt/98d5d321dc375464f3fb4cdc61dfc09e97c813ce39fe7bde32c1cdef15a7f7b7/rootfs/sys/fs/cgroup/cpu: no such file or directory. Are you sure? I also tried the same with cadvisor-canary:latest image and got the same outcome. Should canary not have picked up this fix yet?. I see! Thanks for your time.. ",
    "dene14": "For those who trapped into that issue... \"latest\" is kinda outdated currently, tag is 5 month old. v0.25.0 works fine for me with Amazon ECS optimized AMI.. ",
    "damienmarshall": "Hi!\nHitting the same issue. I've tried with v0.25.0 to no avail! Anyone had any joy in getting this work on ECS optimized AMI?\nCouldn't collect info from any of the files in \"/rootfs/etc/machine-id,/var/lib/dbus/machine-id\"\nDamien. Whoops, sorry, meant to tag @timstclair - does v0.25.0 work on latest ECS optimized AMI for you?\nThanks!\nDamien. ",
    "ronmessana-concerto": "\u2753 has there been a resolution to this issue?. ",
    "TwitchChen": "me toooooo\nwhen i check the CAvisor, the \"fs_**_io\" is not 0,see the\npicture:\n\nbut\uff0cwhen it export to the prometheus/metrics ,it become 0:\nlike this \n\nand the prometheus Graph get 0\nwhy?\n. ",
    "ZhenyangZhao": "@saiwl @Chentongxuedefanhua \n+1 \nme too.\nthe same situation.\ncontainer fs io always 0 in metrics/\n. @dashpole just disk io\n. ",
    "barnettZQG": "I also got the info about io is  always 0 from cadvisor and prometheus.\n. Can you explain what time can solve this issue? thank you\n. @skebo\nyou could use docker api about cgroup.it could get blkio stats\n. Can you explain what time can solve this issue? thank you \n. ",
    "skebo": "yes, all my blkio stats for containers are all 0 as well.  I've tested on ubuntu 16.04  (docker v1.6 through 1.12 w/aufs) and RHEL 7.2 (docker 1.11 and 1.12 w/devicemapper) with my I/O scheduler set to cfq and I still get all zeroes. \nHere is the /validate from my ubuntu box:\n```\ncAdvisor version: 0.23.8\nOS version: Alpine Linux v3.2\nKernel version: [Supported and recommended]\n    Kernel version is 4.4.0-34-generic. Versions >= 2.6 are supported. 3.0+ are recommended.\nCgroup setup: [Supported and recommended]\n    Available cgroups: map[freezer:1 net_cls:1 perf_event:1 hugetlb:1 pids:1 cpuset:1 cpuacct:1 memory:1 devices:1 net_prio:1 cpu:1 blkio:1]\n    Following cgroups are required: [cpu cpuacct]\n    Following other cgroups are recommended: [memory blkio cpuset devices freezer]\n    Hierarchical memory accounting enabled. Reported memory usage includes memory used by child containers.\nCgroup mount setup: [Supported and recommended]\n    Cgroups are mounted at /sys/fs/cgroup.\n    Cgroup mount directories: blkio cpu cpu,cpuacct cpuacct cpuset devices freezer hugetlb memory net_cls net_cls,net_prio net_prio perf_event pids systemd \n    Any cgroup mount point that is detectible and accessible is supported. /sys/fs/cgroup is recommended as a standard location.\n    Cgroup mounts:\n    cgroup /sys/fs/cgroup/systemd cgroup ro,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /sys/fs/cgroup/hugetlb cgroup ro,nosuid,nodev,noexec,relatime,hugetlb 0 0\n    cgroup /sys/fs/cgroup/net_cls,net_prio cgroup ro,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /sys/fs/cgroup/cpu,cpuacct cgroup ro,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /sys/fs/cgroup/pids cgroup ro,nosuid,nodev,noexec,relatime,pids 0 0\n    cgroup /sys/fs/cgroup/memory cgroup ro,nosuid,nodev,noexec,relatime,memory 0 0\n    cgroup /sys/fs/cgroup/freezer cgroup ro,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /sys/fs/cgroup/blkio cgroup ro,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /sys/fs/cgroup/cpuset cgroup ro,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /sys/fs/cgroup/perf_event cgroup ro,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /sys/fs/cgroup/devices cgroup ro,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /rootfs/sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0\n    cgroup /rootfs/sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /rootfs/sys/fs/cgroup/pids cgroup rw,nosuid,nodev,noexec,relatime,pids 0 0\n    cgroup /rootfs/sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0\n    cgroup /rootfs/sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /rootfs/sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /rootfs/sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /rootfs/sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/systemd cgroup ro,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/hugetlb cgroup ro,nosuid,nodev,noexec,relatime,hugetlb 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/net_cls,net_prio cgroup ro,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/cpu,cpuacct cgroup ro,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/pids cgroup ro,nosuid,nodev,noexec,relatime,pids 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/memory cgroup ro,nosuid,nodev,noexec,relatime,memory 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/freezer cgroup ro,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/blkio cgroup ro,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/cpuset cgroup ro,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/perf_event cgroup ro,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/devices cgroup ro,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0\n    cgroup /sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /sys/fs/cgroup/pids cgroup rw,nosuid,nodev,noexec,relatime,pids 0 0\n    cgroup /sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0\n    cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/systemd cgroup ro,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/hugetlb cgroup ro,nosuid,nodev,noexec,relatime,hugetlb 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/net_cls,net_prio cgroup ro,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/cpu,cpuacct cgroup ro,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/pids cgroup ro,nosuid,nodev,noexec,relatime,pids 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/memory cgroup ro,nosuid,nodev,noexec,relatime,memory 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/freezer cgroup ro,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/blkio cgroup ro,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/cpuset cgroup ro,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/perf_event cgroup ro,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/devices cgroup ro,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/rootfs/sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/rootfs/sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/rootfs/sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/rootfs/sys/fs/cgroup/pids cgroup rw,nosuid,nodev,noexec,relatime,pids 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/rootfs/sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/rootfs/sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/rootfs/sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/rootfs/sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/rootfs/sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/rootfs/sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/rootfs/var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/systemd cgroup ro,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/rootfs/var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/hugetlb cgroup ro,nosuid,nodev,noexec,relatime,hugetlb 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/rootfs/var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/net_cls,net_prio cgroup ro,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/rootfs/var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/cpu,cpuacct cgroup ro,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/rootfs/var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/pids cgroup ro,nosuid,nodev,noexec,relatime,pids 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/rootfs/var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/memory cgroup ro,nosuid,nodev,noexec,relatime,memory 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/rootfs/var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/freezer cgroup ro,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/rootfs/var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/blkio cgroup ro,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/rootfs/var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/cpuset cgroup ro,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/rootfs/var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/perf_event cgroup ro,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/rootfs/var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/devices cgroup ro,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/pids cgroup rw,nosuid,nodev,noexec,relatime,pids 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/78d3ceae2050a76d71673a00daaa68b976786079563f2d6197c7bd752126154f/sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\nDocker version: [Supported and recommended]\n    Docker version is 1.12.1. Versions >= 1.0 are supported. 1.2+ are recommended.\nDocker driver setup: [Supported and recommended]\n    Docker exec driver is . Storage driver is aufs.\nBlock device setup: [Supported and recommended]\n    At least one device supports 'cfq' I/O scheduler. Some disk stats can be reported.\n     Disk \"sda\" Scheduler type \"cfq\".\nInotify watches: \nManaged containers: \n    /system.slice/systemd-timesyncd.service\n    /system.slice/ssh.service\n    /system.slice/dev-disk-by\\x2dpath-pci\\x2d0000:00:1f.2\\x2data\\x2d1\\x2dpart5.swap\n    /system.slice/ModemManager.service\n    /system.slice/apport.service\n    /system.slice/systemd-logind.service\n    /system.slice/networking.service\n    /system.slice/var-lib-docker-containers-a98c6e4edc74737b42ea030a8e01c242ad060926dd4888224ee8988680a43f02-shm.mount\n        Namespace: docker\n        Aliases:\n            cadvisor\n            a98c6e4edc74737b42ea030a8e01c242ad060926dd4888224ee8988680a43f02\n    /system.slice/accounts-daemon.service\n    /system.slice/systemd-journal-flush.service\n    /system.slice/irqbalance.service\n    /system.slice/rsyslog.service\n    /system.slice/ondemand.service\n    /system.slice/thermald.service\n    /system.slice/NetworkManager.service\n    /system.slice/keyboard-setup.service\n    /system.slice/dev-disk-by\\x2did-wwn\\x2d0x5000c5005daac616\\x2dpart5.swap\n    /system.slice/alsa-restore.service\n    /system.slice/dev-disk-by\\x2duuid-d653327d\\x2d500d\\x2d48e8\\x2da104\\x2d067a77778211.swap\n    /system.slice/systemd-journald.service\n    /system.slice/resolvconf.service\n    /system.slice/acpid.service\n    /system.slice/lightdm.service\n    /system.slice/dev-disk-by\\x2did-ata\\x2dST250DM000\\x2d1BD141_W2AMDZXN\\x2dpart5.swap\n    /system.slice/systemd-tmpfiles-setup-dev.service\n    /init.scope\n    /system.slice/speech-dispatcher.service\n    /docker\n    /system.slice/rtkit-daemon.service\n    /system.slice/upower.service\n    /system.slice/systemd-user-sessions.service\n    /system.slice/udisks2.service\n    /system.slice/apparmor.service\n    /system.slice/snapd.service\n    /system.slice/systemd-tmpfiles-setup.service\n    /\n    /system.slice/setvtrgb.service\n    /system.slice/systemd-udev-trigger.service\n    /system.slice/systemd-sysctl.service\n    /system.slice/systemd-random-seed.service\n    /system.slice/systemd-udevd.service\n    /system.slice/dev-sda5.swap\n    /system.slice/systemd-remount-fs.service\n    /system.slice/rc-local.service\n    /system.slice/kmod-static-nodes.service\n    /system.slice/docker.service\n    /system.slice/console-setup.service\n    /system.slice/cgroupfs-mount.service\n    /system.slice\n    /system.slice/ufw.service\n    /system.slice/polkitd.service\n    /system.slice/cron.service\n    /user.slice\n    /system.slice/systemd-update-utmp.service\n    /system.slice/system-getty.slice\n    /docker/a98c6e4edc74737b42ea030a8e01c242ad060926dd4888224ee8988680a43f02\n        Namespace: docker\n        Aliases:\n            cadvisor\n            a98c6e4edc74737b42ea030a8e01c242ad060926dd4888224ee8988680a43f02\n    /system.slice/dbus.service\n    /system.slice/whoopsie.service\n    /docker/5ae6127048c41324f1b9b06573a00d1d2dcd030b572d9d4e997f6494a1404706\n        Namespace: docker\n        Aliases:\n            kickass_goldstine\n            5ae6127048c41324f1b9b06573a00d1d2dcd030b572d9d4e997f6494a1404706\n    /system.slice/colord.service\n    /system.slice/grub-common.service\n    /system.slice/systemd-modules-load.service\n```\n. no @renatosis, i did not solve this problem and didn't see any progress, so we've moved away from cadvisor. . @k-s-dean we moved to Datadog.  I'd really prefer a non-commercial solution, so I was also very disappointed by the flaw.  However, the lack of response on this issue was really what sealed the deal.  There was no indication that this problem would ever be addressed.. ",
    "renatosis": "Hi @skebo , did u solve this issue? how?\nThanks. ",
    "k-s-dean": "@skebo Out of interest what did you move to ? I am surprised that none of the cadvisor maintainers or developers are willing to let us know why 0 is being returned.. ",
    "tehlers320": "this is sending via UDP how does this happen?\n. Nevermind automation started this is many places\n. ",
    "joeyasperger": "I signed it!\n. @magnusbaeck Yeah I though about giving it a more general name and you're right I probably should've done that. I'll update it later today.\n. ",
    "magnusbaeck": "Why call this storage driver Logstash? It just serializes the data to JSON and pushes it over UDP. That could easily be used with other tools than Logstash.\n. ",
    "GabKlein": "Any update on this PR ?. +1. ",
    "j00p34": "Is this going to be merged. Really need this functionality. @timstclair ?. ",
    "grokbot": "@joeyasperger are you available to make the requested changes?. I am not 100% sure but you might be able to use http://username:password@elasticsearch:9200 as the host.. I am considering making a temporary docker image until the JSON storage adapter gets merged in. I am in a similar boat as you it seems.. I signed it!. failed tests indicate that I should include storage in the import, this looks in line with how it is done in the redis.go file and others.. @k8s-bot test this. I will wait to do anything more until you've looked at this again, looks like when building it fails on the integration steps, from reading the log looks like it had to do with a network error:\nW0118 03:34:51.184] W0118 11:32:29.594719   24504 manager.go:151] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service:. Ah so it did retest. New to the Kubernetes, sorry!. @k8s-bot test this. @dashpole I squashed mine, but not @joeyasperger commits, is that cool?. @timstclair All good?. @timstclair I will work on these changes. Writing a test sounds nice, any advice on that? The only other storage driver with a test is the influxdb one. I am willing to give it a shot though.. Yeah I am working on it, sorry for the delay.. hold on I apparently messed up my rebase.\nEdit: yeah not rebasing. merging.. Okay thanks.. ",
    "bingohuang": "I signed it!\n. ",
    "cmluciano": "@drnic are you still working on this?. @jianzhangbjz What specific metrics are you looking for? Is this just a count?. Yes I can submit a PR @pineking . Would this require the use of a nvidia specific tool?. @andyxning I think the beta code for Kube only handles Nvidia for now, so initially I was only thinking of supporting that.. I submitted a proposal for NVML kubernetes/community#414 . I want to use things like nvidia-smi to grab metrics. I would like some more feedback over there from @vishh and @therc before I submit a PR.. cc @tianshapjq\nI read through your PR on the K8s, repo and I think that you definitely have captured a few key specs.\nI think that we should consider leveraging the C NVML bindings for metrics collection. This would eliminate the need for dumping/parsing XML from nvidia-smi.. cc @RenaudWasTaken @3XX0 could use your input here.\nMy first thought was to use the NVML bindings in nvidia-docker for gathering metrics. . Yes. The NVML library from Nvidia is preferred. Wrapping the command line tool could be a performance drain. Nvidia-docker currently has nvml bindings, but we should push to get them in libnvidia-container. . I believe nvidia-smi is just wrapping nvml.\nhttps://github.com/NVIDIA/libnvidia-container\nhttps://github.com/NVIDIA/nvidia-docker. I've been holding off on a PR until the nvidia-container project exposes Go bindings. However, the library would still need to be available on the host as noted. \nI can start something for option 2 if we want to leverage cgo instead.. A quick look-through shows that several vendored libs are using cgo, but it doesn't seem to be within cadvisors pkgs.. @flx42 @renaudwastaken Are you interested in owning this?. Is this awaiting maintainer review?. Is this awaiting a maintainer review?. Might be related https://github.com/prometheus/client_golang/commit/f0c45acc506afde8b240d7d377f61845ba3089e3 ?. Is the test failure a known flake?. Is this technically a duplicate of #1436 ? . This you add information about the resource usage of each of these containers and the general stability of the node in terms of CPU/MEM load?\nOfficially I do not believe that Kube supports this great of a workload per node.. @donghwicha Which additional metrics are you interested in ?.  I'm under the impression that \"modern Linux distros\" will almost always have this directory, but I didn't know if that is 100% correct or not.\nI was originally worried that we would be creating log entries unnecessary if we landed on a node that did not have NUMA support.. Are any of these other metrics useful for determine topology for NUMA scheduling?. Is it possible for this to be returned as a real number instead of an integer? \nOn a 64bit system this should return a max uint64 number.\nCan you use strconv.ParseUint with a base of 10 and bit size of 64? This will return a uint64 and eliminate the conversion on line 266 as well.. Nit: Wondering if you could increase clarity by adding numa in these types of messages. Nits\n-the above command will emit the following error ...\n-move to avoid it... to a new sentence. Wondering if there should be a TODO: about removing the VersionSting() in a future release. . Looks like Kubernetes is using blang/semver. ",
    "orientzc": "Sorry. i use docker_env_metadata_whitelist parameter to get envs in influxdb. but it is working in cadvisor, not working in influxdb. the detail is the issue:#1559. could anyone help me? thanks.\nps: i try google/cadvisor:latest and google/cadvisor:canary. . @erSitzt Thanks very very much. \ni am sorry to be late. and using your Github Repository, will i only see envs in cpu_usage_per_cpu? is this because that you only edit the code of cpu_usage_per_cpu? and i will focus on the issue: #1427. and i expect good news. If  you send a message to me when the merge request handled, i will be lucky. good luck to you and me.\nThanks very very much.. oh,  i close the issue: #1559. and should i send \"ok to test\" ? . use cadvisor API, like this: api/v1.2/docker/,  i can see the env by cadvisor, but it may not write to influxdb. \n\n\n\nwhat do i miss?\nps: i try google/cadvisor:latest and goole/cadvisor:canary.\n. thanks very much. i think we did hit the same problem, and i also see the issue:#1427. so i try google/cadvisor:latest and google/cadvisor:canary. because docker_env_metadata_whitelist parameter is not used by influxdb storage plugin in the code of google/cadvisor:latest, and it is used in the code of google/cadvisor:canary.(#1427: https://github.com/erSitzt/cadvisor). see this,\n\nbut it is not working. but i donnt known the reason, or what do i miss?. Thanks very very much. i am sorry to be late. and using your Github Repository, will i only see envs in cpu_usage_per_cpu? is this because that you only edit the code of cpu_usage_per_cpu? and i will focus on the issue: #1427. and i expect good news. If  you send a message to me when the merge request handled, i will be lucky. good luck to you and me.\nThanks very very much.. ",
    "erSitzt": "@orientzc its not working because the pull-request was never merged.... @timstclair could someone review and merge this ? do i have to do anything else ?. Hmm i just tested it with my cadvisor version..\nnpm_config_loglevel is an env var only present in our node containers.\nAnd this is my option for cAdvisor:\n-docker_env_metadata_whitelist\nHOST,NPM_CONFIG_LOGLEVEL\n\n. I use a single dash for docker_env_metadata_whitelist btw. And here cAdvisor /metrics endpoint\ncontainer_cpu_system_seconds_total{container_env_host=\"Mesos-Slave2.neo-test.domain.com\",container_env_npm_config_loglevel=\"info\",container_label_collectd_docker_app=\"neo/artikel/demo\",id=\"/system.slice/docker-75a83ec849cf3512e31d5676d1de4d7ac00afaa11209223698c22825c8e58807.scope\",image=\"srv-nexus-docker-registry.domain.com/neo/artikel-neo:demo-3262\",name=\"mesos-b7626133-ecc3-415e-9d38-cdeabaed1c7c-S3.45de2ba5-8a64-4033-85a9-9b325e889256\"} 72.35. I just saw my merge request was still open ?... so this issue could be closed i think. If you want you can build your own cAdvisor docker image like this:\nDockerfile for binary\n```\nFROM golang:latest\nRUN apt-get install -y git dmsetup\nRUN go get github.com/tools/godep\nRUN git clone https://github.com/erSitzt/cadvisor.git /go/src/github.com/google/cadvisor\nRUN cd /go/src/github.com/google/cadvisor && git checkout release-v0.23 && make\nupload binary to some location you can use\nRUN curl -v -k --user 'raw:raw' --upload-file /go/src/github.com/google/cadvisor/cadvisor https://somefilehosting-url.domain.com/cadvisor\n```\nDockerfile for container\n```\nFROM alpine:3.4\nMAINTAINER dengnan@google.com vmarmol@google.com vishnuk@google.com jimmidyson@gmail.com\nENV GLIBC_VERSION \"2.23-r1\"\nRUN apk --no-cache add ca-certificates wget curl device-mapper && \\\n    apk --no-cache add zfs --repository http://dl-3.alpinelinux.org/alpine/edge/main/ && \\\n    wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://raw.githubusercontent.com/sgerrand/alpine-pkg-glibc/master/sgerrand.rsa.pub && \\\n    wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/${GLIBC_VERSION}/glibc-${GLIBC_VERSION}.apk && \\\n    wget https://github.com/andyshinn/alpine-pkg-glibc/releases/download/${GLIBC_VERSION}/glibc-bin-${GLIBC_VERSION}.apk && \\\n    apk add glibc-${GLIBC_VERSION}.apk glibc-bin-${GLIBC_VERSION}.apk && \\\n    /usr/glibc-compat/sbin/ldconfig /lib /usr/glibc-compat/lib && \\\n    echo 'hosts: files mdns4_minimal [NOTFOUND=return] dns mdns4' >> /etc/nsswitch.conf && \\\n    rm -rf /var/cache/apk/*\nGrab cadvisor from the staging directory.\nRUN curl -k -o /usr/bin/cadvisor  https://somefilehosting-url.domain.com/cadvisor\nEXPOSE 8080\nENTRYPOINT [\"/usr/bin/cadvisor\", \"-logtostderr\"]\n```\nThis is using my Github Repository, so use at your own risk.. @orientzc Nope, not only for cpu_usage_per_cpu, for all metrics.. ",
    "Bart-Z": "Anybody going to merge this ?. ",
    "jianzhangbjz": "@andyxning I'm so sorry, I have been busy with other work this time. If you want to get this feature ASAP, I think you can start to do this. :). @andyxning :+1: Any problems about GPU, please feel free to contact me if you need. :). ",
    "pineking": "I think it is just a count,  #GPUs are used by each pods, #GPUs available/total on each nodes, do you have plan to implement this? @cmluciano . @therc @vishh @cmluciano any progress or update?. @mindprince @Colstuwjx could you tell me where is the document for using cadvisor to monitor GPU?. @donghwicha do you have the update or pr?. @dims when this is merged, which version of k8s can be fixed, v1.9.1 or master branch of k8s?. ",
    "liyubobj": "@cmluciano @pineking  No, we want to expose GPU live metrics, such as GPU core and GPU memory utility, to cAdvisor, as what they did for CPU and memory today.. @andyxning Sorry, actually I want to say idle/used GPU memory.. @cmluciano  Yes, you need either libnvidia-ml.so lib or nvidia-smi binary to get live metrics. Both of them are in nvidia GPU driver. But if you want to get GPU count only, a hack is to check /dev/nvidiaX. . if you want to get the metric I mentioned, libnvidia-ml lib is enough. @vishh . In my side, I aimed at nvidia GPU only. Would you have requirement to support other GPU brands? @andyxning . @hangliu could you prepare the proposal for this?. I prefer to using NVML lib than nvidia-smi because it would be terrible if we call a binary, query all GPU metrics, parse useful GPU info from xml in every 1 second for example.... ",
    "therc": "@vishh there's the nvml library, but it's native code. The alternative is calling nvidia-smi, a nvml client, which I think is safe to assume will be installed along with the drivers. There's no need to scrape its human-readable output, because it also has an XML output mode.\nKubernetes right now only allocates whole GPU devices, but I agree that it would be nice to have some additional metrics, such as product name, driver version (because it affects which CUDA version is supported), RAM available and in use, number of processes talking to the device. Stuff like temperatures, fan speeds, performance state are less of a priority.. ",
    "Hui-Zhi": "@therc I think so far we only need the GPU details which could be used for volume injection, I do not think share GPUs to multiple containers is a good idea now, like tensorflow always take the whole memory for its own, and once the application in an out of GPU memory situation, some times they just panic.. NVML is the professional way to handle Nvidia GPU, nvidia-smi also uses NVML. just dynamic load and unload NVML is what we need.. @WIZARD-CXY Could you write a proposal about it? I would like to give some comments about the detail.\nAnd regarding the NVML in cadvisor, so far I am working some other projects, but this definitely in my schedule.. ",
    "seelam": "Irrespective of the GPU sharing between containers, developers need to know how much GPU memory is used by their application, total memory, gpu utilization, temperature, pcie link bandwidth etc. Nvidia-smi provides a rich collection of these metrics, I suggest we capture as many metrics as possible. . ",
    "3XX0": "This would be one way to do it, but this shouldn't be static IMHO, this should be pluggable. For example, you might want to leverage DCGM instead for Tesla GPUs.. Yeah sorry I realized afterwards and edited my comment :)\nAlso it's worth mentioning that you won't get much metrics per container with NVML.\nThe only thing you can get is the memory used per PID (be careful about the PID namespace).\nnvidia-smi pmon uses internal functions to get more metrics, I mentioned it a while ago to the team in charge, I will try to see if they can fix it.. Unfortunately, we can't do a native implementation because NVML is closed source.\nThe best we can do is to leverage cgo either in cAdvisor or in a separate gRPC service.\nEither way, we will probably publish a Go package (akin to the nvidia-docker nvml package) to abstract this away.. ",
    "WIZARD-CXY": "@Hui-Zhi @vishh any updates or pr here? thx. @vishh I plan to implement a simple version of gpu monitor using cadvisor+prometheus.I want to monitor each container's gpu sm util and mem util by using nvidia-smi pmonaccording to @3XX0 while I noticed many people prefer to use nvml to do the job.. @Hui-Zhi I googled about nvml today and don't  find much about how to get metrics per container with NVML as @3XX0 said while I found it really easy to use nvidia-smi pmon, so I would give it a naive try. I think I would use the same approach as other metrics like cpu usage of a container like https://github.com/google/cadvisor/blob/master/metrics/prometheus.go. I think we should not use nvml directly, we'd better use plugin mechanism just like cni solution, we can actually use weave, calico as different provider. We just need to define the correct interface, that's enough IMHO.. I like option 3 plugin mechanism. Actually there is already a REST API  available https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker-plugin#rest-api. glad to see this thing move forward. +1 fixed by adding \"-tags netgo\" on ubuntu  16.04 with go1.8.1. thx @dashpole so i need to loop through all the handlers and there is no cache exist\uff1f. oh my need is that I have some pids, and I want to find their relevant containers, So dockerContainerHandler may not be enough. @dashpole PTAL. @dashpole PTAL. @dashpole  I think gpu-related pr and this one are actually different things, this pr is intended to fullfill the functionality of ListProcesses as it is. After all, a seemingly good function doesn't work like it claim to be is very confusing for the user IMHO.. I think this function is very basic and could be use elsewhere other than gpu, I think we don't want user use this function and says \"ah, it does not work, why???\". @dashpole the issue #1652 gives a detail of this. \"Can you shed some more light on how this impacts users?\"\nAs the issue said, the user can't use ListProcesses method of dockerContainerHandler. When I want to get a container's pid, ListProcesses method always return empty slice. After some digging,I found it uses fsCgroup to get the pids as the code below said https://github.com/google/cadvisor/blob/master/vendor/github.com/opencontainers/runc/libcontainer/cgroups/fs/apply_raw.go#L234. It uses devices subsytem to get pid. But in cadvisor the devices subsytem is not supported https://github.com/google/cadvisor/blob/master/container/libcontainer/helpers.go#L76. So ListProcesses will never work. Adding devices substem works fine, tested ok on my machine. \nThis function is never used elsewhere in cadvisor so it explains why nobody has noticed it would never work. Other place like api uses other method (exec a ps command and analyze its output as result) as func (c *containerData) GetProcessList(cadvisorContainer string, inHostNamespace bool) ([]v2.ProcessInfo, error) defined.\n. @dashpole I understand your concern, my intension is just to make the function work, if it is an intented omission now then I will withdraw this pr. Reproducing procedure:\njust call ListProcess method somewhere like in updateStats https://github.com/google/cadvisor/blob/master/manager/container.go#L509 . I want to get process list within a container and use it as a part of my gpu monitoring @dashpole . please somebody review this issue and my fix @vishh @dashpole @vmarmol It already took too long. ",
    "flx42": "@vishh I would like to know what's your opinion on how we should proceed here for adding GPU support to cAdvisor. The two main options already discussed are:\n1. Call nvidia-smi repeatedly like suggested in https://github.com/kubernetes/kubernetes/pull/43934\nnvidia-smi might not be installed, and it's probably a waste of CPU cycles (many calls to exec + repeated initialization of the context).\n2. Use the NVML library. This will require using cgo, header nvml.h at build time and dlopen/dlsym at runtime. It will be more complex to do, but will provide better performance (a single NVML initialization).\nI see a third option though:\n\nUse a plugin approach for custom metrics where the NVIDIA metrics will be served by a different process over gRPC, for instance. 2 years ago, it seems it was considered out of scope: https://github.com/google/cadvisor/issues/483\n\ncc @3XX0 . @vishh How do you want to proceed? Is cgo in cAdvisor an option?\nIf not, is cgo in an external golang package imported by cAdvisor an option? (this would just hide the cgo in the different codebase).. Yes, but not immediately, I hope we can start working on this soon.. I don't think we can simply remove this flag like that. We can't use RTLD_GLOBAL like our nvidia-docker code does, you will need to dlsym each symbol manually. Tedious, but I don't believe we really have a choice.. @yongtang @vishh @mindprince @3XX0 @cmluciano: before considering this PR, we should probably sync on what we want to achieve, how, and what the roadmap should be. What do you think?. @mindprince September 7 works for me!. @tianshapjq this shouldn't be needed if the container runtime does it for you (e.g. with nvidia-docker or a device plugin).. You don't \"install\" GPU memory like you do for CPU memory.. This comment slightly confused me, a container can have access to all GPUs on the machine but not have 195:*, for instance if it enumerates each device:\nc 195:3 rwm\nc 195:2 rwm\nc 195:1 rwm\nc 195:0 rwm\nIt depends on what the container runtime is doing.\nWhat is the rationale behind skipping this use case?. This assumes that one GPU is used by only one container, right?\nDoes that mean you won't be able to see the status of GPUs that are not being used by any container? It might still be useful (e.g., they might be overheating, or be in a unhealthy state).\nMy feeling is that the current NvidiaGpuStats struct should be per-node (for all GPUs) and not per-container.\nPer-container, you could still try to detect the assigned GPUs, or even better: determine what each container is really using. This could be done by using the NVML API to list the PIDs that have an active context on the GPU and then finding which container these PIDs belong to. This would be similar to what you see when you do nvidia-smi.\n. Nevermind then, let's be consistent.. It's not an API, it's an implementation detail that happens to work with Docker.\nThere are other ways to block GPUs besides groups, for instance you can bind mount the devices and then block mknod, for instance when using user namespaces.\nThat being said, for the Docker use case, I'm not aware of any upcoming breaking change, but my vision doesn't go up to one year forward.\n. ",
    "tianshapjq": "@3XX0 is any schedule on the release of the Go package?. @yongtang I am working on this issue as well, but erasing \"-extldflags '-static'\" in build.sh still makes me confused: would it just use dynamic link for compile and then we have to mount more volumes into the container? Because there exists lots of cgo in cadvisor, is it ok to use dynamic link?. @mindprince @flx42 This issue is WIP in my work, would you please inform me if you have made the schedule of the meeting? :). @mindprince @yongtang @flx42 hi, do we have a summary after the meeting?. HI, any news about this feature? still waiting for the DCGM coming up? @mindprince . @derekwaynecarr @vishh I'm not sure who can handle about this, any suggestions for me? thx.. Sorry, careless mistake for the vendor code. I will close this.. @mindprince @vishh PTAL at your convenience ~. Because I can't get these info from the /spec api. I would like to obtain all the gpu info attached to the node, and I think gpu info is just similar to cpu and memory, we should be able to get these hardware info of the node through /spec.\nAnother thing, the nvidia gpu has some kind of bug in the production environment, that is, some cards just disappeared for some reasons we didn't know, and I have to figure out which one got disappeared through comparing the gpus in spec and the gpus in stats. So, in fact, what I need most in spec is the ID and memory of the gpu.. /assign @mindprince . Also, we need to aggregate the stats of gpu in heapster, which requires the spec to tell if there are any accelerators. Ref: heapster metrics. To my understanding, the containerName == \"/\" means the info of the node, which is collected by cAdvisor at present, so it does collect the info of node. Right?\nThe metric \"MetricAcceleratorMemoryTotal \" in hepaster is just used for summary api, and will not show if I want the stats data, such as the total_memory or average_memory of gpus  the container used.. I got that it's not listed in the devices cgroup, so I coded like \nif containerName == \"/\" {\n    cont.nvidiaCollector, err = m.nvidiaManager.GetAllDevicesCollector()\n } else {\n    cont.nvidiaCollector, err = m.nvidiaManager.GetCollector(devicesCgroupPath)\n }\nto get the gpu spec of containers and node.\nThis is useful in our production, and I would like to know if this does for others.. @dashpole yes, this will collect the metrics twice. I could refactor to collect just once with a cache or something like that, but I need to be sure this feature is acceptable. \nThe reason for why I did this is mentioned above:\n\nBecause I can't get these info from the /spec api. I would like to obtain all the gpu info attached to the node, and I think gpu info is just similar to cpu and memory, we should be able to get these hardware info of the node through /spec.\nAnother thing, the nvidia gpu has some kind of bug in the production environment, that is, some cards just disappeared for some reasons we didn't know, and I have to figure out which one got disappeared through comparing the gpus in spec and the gpus in stats. So, in fact, what I need most in spec is the ID and memory of the gpu.\n\nI will deploy by DaemonSet If cAdvisor is removed from in-tree Kubernetes.. @mindprince I found these mismatches in these two api, could you please verify this?. @derekwaynecarr is this acceptable?. if error occurs, any chance to try again?. a question not relevant here: do we have to mount the folder in which libnvidia-ml.so.1 located into the cAdvisor container? If so, we have to add this to ReadMe. @flx42 that means I have to install nvidia-docker in prior if we want to use this functionality? That we may have to note in the ReadMe as well IIUC.. the nvidiaDevices attribute is updated in the initializeNVML(), since it's also called somewhere else, do we need to sync lock it up when updating the nvidiaDevices?. as your new commit, the nvidiaDevices will be updated every 5 minutes. should we sync to lock it up here as well?. SGTM. ",
    "yongtang": "I have a very preliminary implementation for this issue:\nhttps://github.com/google/cadvisor/compare/master...yongtang:1436-GPU-Info\nAt the moment the implementation only collects model, path (/dev/), total memory, and used memory. The implementation dlopen' nvml (if it is available) so it relies on cgo.\nIf this is OK, I could create a pull request and work on further implementations like additional GPU info, etc.\n. @flx42 I will update the implementation to use dlsym. \nThough I am not sure we could get around the dynamic issue. For example, the following snippet will generate a warning\n```\nroot@4f51cb55a0db:/go/src/github.com/google.com/cadvisor# cat main.go \npackage main\n/*\ncgo linux LDFLAGS: -ldl\ninclude \ninclude \nvoid nvml = NULL;\n/\nimport \"C\"\nimport (\n    \"fmt\"\n)\nfunc main() {\n    C.nvml = C.dlopen(C.CString(\"libnvidia-ml.so.1\"), C.RTLD_LAZY)\n    if C.nvml == nil {\n        fmt.Println(\"Couldn't load nvml library\")\n    }\n    fmt.Println(\"Hello, playground\")\n}\nroot@4f51cb55a0db:/go/src/github.com/google.com/cadvisor# go build -ldflags=\"-extldflags '-static'\"\ngithub.com/google.com/cadvisor\n/tmp/go-link-816969679/000000.o: In function _cgo_56e60412441f_Cfunc_dlopen':\n/tmp/go-build/github.com/google.com/cadvisor/_obj/cgo-gcc-prolog:42: warning: Using 'dlopen' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking\n```\n. @flx42 Updated and created the PR #1739 to usedlsym` for resolving functions. Please take a look.. @flx42 @mindprince A sync up would be great \ud83d\udc4d . I am available next week but other times might work as well.. /cc @flx42. ",
    "mindprince": "Agreed. Let's sync up in a meeting. This week is kubernetes code freeze deadline, so we can't meet this week. Probably sometime next week, maybe September 7?. Anyone who wants to join:\nMeeting date: September 7, 11am to 11:50am PDT.\nMeeting URL: https://talkgadget.google.com/hangouts/_/google.com/gpu-cadvisor\n. I will try to share a doc by EOD today.. Here are the meeting notes: https://docs.google.com/document/d/1cNo88Z1b5gb3svHNlOQLWCLGevF5GIOpfVbJFKDg5iY/edit\nYou will have to be a member of kubernetes-dev@googlegroups.com to access.. @tianshapjq I have a WIP PR here: https://github.com/google/cadvisor/pull/1762. @rapatel0 https://github.com/google/cadvisor/issues/1796 tracks adding documentation about how to use this.. /cc @vishh @jiayingz @dashpole \nLooking for early feedback.. @vishh @dashpole This is ready for review. It maybe easier to review it commit by commit.. @vishh @dashpole Abstracted out the nvidia metrics collection. Please take a look. If it looks good now, let's get this merged.. /cc @dashpole . Hi @Colstuwjx, Thanks for trying this out.\n\nNVML initialized. Number of nvidia devices: 4\n\nThis means everything was initialized correctly and metrics should show up.\nCouple of things to note:\n- There are no machine level metrics. So, metrics won't show up if no container with accelerator attached is running.\n- Metrics will only show up if accelerators are explicitly attached to the container. For example, by passing --device /dev/nvidia0:/dev/nvidia0 flag to docker. If nothing is explicitly attached to the container, metrics will not show up.\nI will use this issue to track adding documentation about GPU metrics.. Yeah, cAdvisor running inside the container will not find NVML unless you mount the path containing the library and update LD_LIBRARY_PATH. I will add this to the documentation as well.. It's not on the web UI, only in the API.\nPRs welcome!. The GPU metrics should be available both under /metrics and through the cAdvisor REST API.. Without this bazel tests were failing in https://github.com/kubernetes/kubernetes/pull/55188.\n/cc @dashpole . I think they are good to be merged. But cc'ing @vishh and @jiayingz just in case.. @choury You are right. I sent a quick PR to fix this: https://github.com/google/cadvisor/pull/1814\nThanks for finding this out.\n@dashpole Do you think this fix can make the vendoring cut for 1.9?. /cc @dashpole @choury. This PR is just fixing the case where we could have a concurrent map access.\nThe time taken by initializeNVML() on  my desktop is in ms but on some cloud environments it can take up to 5s. That's why I didn't put it in the critical path. We can see how big of an issue this is and change that in the future. My guess is that it's unlikely that there are containers with GPUs attached when the kubelet starts up but I maybe wrong and we can change that in the future.. > We should not require to restart all containers if we just want to restart or upgrade kubelet.\nAgreed. Good point.\n\nMay be call GetCollector after NVML initialized or wait it in GetCollector?\n\nNot sure what you mean exactly. Can you elaborate?. Discussed with @vishh and made changes to work better in case of kubelet restarts.. /assign @dashpole @vishh . /retest. I had added power usage in the first iteration of my initial PR. But then we decided to add only the bare minimum metrics in the first version and wait for user feedback. NVML exposes a lot of metrics (power usage, temperature, fan speed etc.) but it's not clear how helpful these metrics are to users running GPU workloads.\nWhile testing, I also noticed that power usage graph was exactly the same as the duty_cycle graph.. @dfredell If you only want machine level GPU metrics, the \"correct\" way is to write a GPU prometheus exporter. It should be pretty easy to build using https://github.com/mindprince/gonvml but I haven't gotten around to doing it yet.. Not sure what docker_env_metadata_whitelist does or why is it needed.\nDid you check https://github.com/google/cadvisor/blob/master/docs/running.md#hardware-accelerator-monitoring. If you are running cadvisor outside the container as root, you don't need to give it explicit device access. root already has that. To make sure it has access to NVML libraries, you either update your ldconfig or do something like:\nexport LD_LIBRARY_PATH=<path-to-nvml>\nsudo ./cadvisor. Are there any containers running that are using GPUs? You will only see metrics for containers using GPUs. There are no machine level GPU metrics.. Also, as the documentation mentions the containers should be explicitly using GPU devices.. Is this still an issue? Can this be closed?. Does cAdvisor has access to NVML? From https://github.com/google/cadvisor/blob/v0.29.1/docs/running.md#hardware-accelerator-monitoring\n\nIf you are running cAdvisor inside a container, you will need to do the following to give the container access to NVML library:\n-e LD_LIBRARY_PATH=<path-where-nvml-is-present>\n--volume <above-path>:<above-path>\n\nAnswers to your questions:\n1. GPU metrics don't show up in the UI.\n2. You can look at the /api/v1.3/subcontainers path.\n3. Yes. But I think nvidia-docker does it for you. Note that this integration is not tested with nvidia-docker, only with Kubernetes.\n4. Any image based on https://hub.docker.com/r/nvidia/cuda/tags/ should work.\n5. Yes, this is tested in Kubernetes. If you request containers using Kubernetes API (resource nvidia.com/gpu, things should work automatically). See OSS docs or GKE docs.\n6. Please look at Godeps.json in https://github.com/kubernetes/kubernetes for the version of you Kubernetes you want to check.\n7. The location of cAdvisor logs depends on how you run cAdvisor. It does tell whether it's able to collect GPU stats.. Yes, GPU monitoring support in Kubernetes was added in 1.9.. /close. @dashpole This can be closed.. Why do you want to duplicate this information? It's available through the stats struct.. The reason spec doesn't have accelerator info is because cAdvisor only supports container level accelerator metrics, i.e., metrics are only shown when there are containers running that have GPUs attached. We don't support machine level accelerator metrics and so spec was not needed. I am not sure if we want to add support for machine level accelerator metrics.\nHeapster already supports accelerator metrics: https://github.com/kubernetes/heapster/blob/v1.5.2/metrics/core/metrics.go#L795-L801\n. > To my understanding, the containerName == \"/\" means the info of the node, which is collected by cAdvisor at present, so it does collect the info of node. Right?\nNo, at /, the NVIDIA GPU devices are not listed individually in the devices cgroup and so they are not collected (https://github.com/google/cadvisor/blob/v0.29.1/accelerators/nvidia.go#L210).. cc @derekwaynecarr . /assign @dashpole @vishh . LGTM. https://github.com/google/cadvisor/blob/master/docs/running.md#hardware-accelerator-monitoring has all the details. You need to make sure that however you are running cAdvisor, it satisfies the two conditions mentioned there: access to NVML library and access to GPU devices.\nIf you are running cAdvisor embedded into the kubelet, then kubelet should have access to NVML library (i.e. its LD_LIBRARY_PATH should contain the location where nvml is present.) Similarly, it should have access to GPU devices.\nIf you are running cAdvisor/kubelet inside a container itself, things are complicated but the two requirements are the same. The link above explains how to satisfy these two requirements when running cAdvisor inside the container.\nIt's hard to debug individual cases without access to the environment.. > Yes, for embedded cAdvisor, I have set the right LD_LIBRARY_PATH, I am not sure what you mean for have access to GPU devices but it should be since tensorflow training for GPU can run successfully on my GPU node. Additionally, may you double confirm what is the metrics representing for GPU?\nBy access to GPU devices, I meant the process running cAdvisor, should have access to the GPU devices in /dev/ (/dev/nvidiactl, /dev/nvidia0, and so on).\nYou should see the log like \"NVML initialized, number of nvidia devices: N\" for the process running cAdvisor.. I am not sure this file can be included in the repo.. I prefixed this with nvidia because I wasn't really able to find what metrics other GPU vendors expose (AMD/Intel etc.) and what I did find seemed very non-standard.\nNot sure what the best way here is. I would prefer to not add a generic API without knowing that it's really generic enough to fit all (most) use cases.. Good point. It depends on the device plugin implementation (which is outside the kubernetes/kubernetes tree). In the current implementation, when you ask for nvidia.com/gpu resource, the GPU IDs are nvidia0, nvidia1 etc. So, just the minor numbers. I think there are plans to request different models of GPUs through annotations possibly but nothing concrete has happened in that regard yet.\ncc - @jiayingz. It's constant for a particular GPU, but there can be multiple GPUs with different total memory each. If I move it to spec I would have to have a way to link the total memory of a GPU with its usage. I felt using two different time series while plotting would be inconvenient for the user.. Unfortunately, nvml doesn't expose cumulative usage. It only exposes this point-in-time gauge. The sample period is dependent upon the GPU type as you found out.. The unit is milliwatts. The  documentation doesn't say how long they sample the energy consumption to calculate the power usage. I would assume it's another point-in-time gauge.. I copied it from the help text of nvidia-smi:\n$ nvidia-smi --help-query-gpu\n...\n\"memory.total\"\nTotal installed GPU memory.\n...\nWhat would be a better comment here?. I mentioned this in the method comment.\nIn cases where the container has access to all devices or all NVIDIA devices but the devices are not enumerated separately in the devices.list file, we return an empty list.\nWe will handle the case where the container has access to all devices and they are enumerated in the devices.list file. We are skipping the case where the devices are not enumerated in the devices.list file.\nI will make the comment clearer.. > This assumes that one GPU is used by only one container, right?\nNot necessarily. If the same GPU is found in the devices.list file of multiple containers, then all those containers would have stats for that GPU. The stats would be the same (since NVML is not cgroup aware). Since Kubernetes doesn't assign one GPU to more than one container, it should be fine.\n\nDoes that mean you won't be able to see the status of GPUs that are not being used by any container? It might still be useful (e.g., they might be overheating, or be in a unhealthy state).\n\nYes.\nWe definitely need to get the usage per container. I will *try* to make NvidiaGpuStats both a machine level and container level field. The machine level will show stats for all GPUs. The container level will show stats just for the GPUs attached to the container. I will still use the devices.list file to find which GPU is used by which container, we can move to PID based mapping later on, if needed.. I tested this by directly running it using:\nsudo ./cadvisor -alsologtostderr\nKubelet also runs directly on the host, so it would work there as well.\nYou are right, we need to document that to run inside a container and still get GPU metrics, you will need to either use nvidia-docker or mount the libraries and setup some environment variables like what device-plugins do.. This method is called only after NVML is initialized and we know the device count, I don't see how this can be a transient error after those two conditions are satisfied. The only failure scenario after that is if the user doesn't have permission to access the device (documentation). In that case, there's no point in retrying.\nCurrently, the code requires the GPUs to be already attached and NVML installed before cadvisor is started. I am thinking of making it so that it rechecks for NVML/GPU presense every so often.. I don't think a lock is needed. nvidiaDevices is only ever written by this goroutine. Once we return from here, it's never updated. Everywhere else we are only reading the value. In the worst case, if the reads happen while initializeNVML() is in progress, the read will see incomplete devices. That will be corrected in the next housekeeping.. Yeah, duty cycle is commonly used to refer percent of time in which a system was active.. Updated. Thanks for noticing this! (facepalm). Done.. Actually this made me realize that I can simplify the taking locks multiple times to taking lock just once. PTAL.. I would skip the comment here.. Maybe link to https://github.com/google/cadvisor/blob/master/docs/running.md#hardware-accelerator-monitoring from here?. ",
    "rapatel0": "Hello, \nI was just wondering what the status is? There doesn'i seem to be any documentation on how to enable gpu tracking in cadvisor.\n. ",
    "guptaNswati": "@tianshapjq We have published DCGM go bindings if you want to try. . No errors and I don't think nvml is getting initialized either. I am passing all these,\nsudo ./cadvisor -docker_env_metadata_whitelist=\u201c--volume=/usr/lib/nvidia-384:/usr/lib/nvidia-384, LD_LIBRARY_PATH=/usr/lib/nvidia-384, --privileged, --device-cgroup-rule=\u2018c 195:* mrw\u2019, --device=/dev/nvidiactl:/dev/nvidiactl, --device=/dev/nvidia0:/dev/nvidia0, --device=/dev/nvidia1:/dev/nvidia1, --publish=8080:8080\u201d -logtostderr\n  . I am using this flag to pass access to nvidia devices. I did follow the instructions from above link but it only documents for containers. How do I pass nvml-path and device access to the binary?. Okay, but I had my LD_LIBRARY_PATH set but since sudo wont have the usr env, this does not work. It should be sudo LD_LIBRARY_PATH=\"nvml-path\" ./cadvisor - logtostderr\nI see nvml initialized but still no Gpu metrics on /metrics endpoint.\n  . Oh right! missed that. Let me try running a Gpu container.\nThanks. Ran a Gpu container and getting this error, \nNVML initialized. Number of nvidia devices: 1\nI0105 22:22:13.010226   17819 manager.go:329] Starting recovery of all containers\nW0105 22:22:13.085032   17819 container.go:507] Failed to update stats for container \"%s\": %s/docker/ad33e310628098e54ec8af4feec25afb59b38299e1eacec069ff7206b8e292baerror while getting gpu utilization: nvml: Not Found\n. Yes, got it working. Closing, thanks. . ",
    "micahhausler": "Can someone \"ok to test\" this?\n. @timothysc Updated\n. @timstclair  I think that the test container can't get the inodes of the device. Do you want me to do a check for if fs.HasInodes {} or just zero-out the test data?\n. We're observing the same behavior, in the kubernetes 1.7.0 kubelet (port 4194), and the docker image for v0.26.1\nVersions:\ndocker: 1.12.6\nKubelet: v1.7.0+coreos.0\nOS: CoreOS Linux 1409.7.0\nKernel Version: 4.11.11-coreos\nI ran cadvisor on kubernetes using the following DaemonSet\nyaml\napiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  name: cadvisor\n  namespace: default\n  labels:\n    app: \"cadvisor\"\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: \"cadvisor\"\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: 4194\n        prometheus.io/path: '/metrics'\n    spec:\n      containers:\n      - name: \"cadvisor\"\n        image: \"google/cadvisor:v0.26.1\"\n        args:\n        - \"-port=4194\"\n        - \"-logtostderr\"\n        livenessProbe:\n          httpGet:\n            path: /api\n            port: 4194\n        volumeMounts:\n        - name: root\n          mountPath: /rootfs\n          readOnly: true\n        - name: var-run\n          mountPath: /var/run\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: var-lib-docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n        resources:\n          limits:\n            cpu: 500.0m\n            memory: 256Mi\n          requests:\n            cpu: 250.0m\n            memory: 128Mi\n      restartPolicy: Always\n      volumes:\n      - name: \"root\"\n        hostPath:\n          path: /\n      - name: \"var-run\"\n        hostPath:\n          path: /var/run\n      - name: \"sys\"\n        hostPath:\n          path: /sys\n      - name: \"var-lib-docker\"\n        hostPath:\n          path: /var/lib/docker\n      - name: \"docker-socket\"\n        hostPath:\n          path: /var/run/docker.sock\nAnd this is what it looked like in Prometheus:\n\n. ",
    "cloverstd": "I got the same problem.\nI also have another problem, that I could not get the correct spec.memory.limit value, the value from cadivsor is 9223372036854776000, but my host machine total memory is 8GB\nsystem\ncentos 7.2.1511\nLinux node52 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\ndocker version\ndocker version 1.12.1, build 23cf638\ncadvisor version\n0.23.8\n. I got the same problem.\nI also have another problem, that I could not get the correct spec.memory.limit value, the value from cadivsor is 9223372036854776000, but my host machine total memory is 8GB\nsystem\ncentos 7.2.1511\nLinux node52 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\ndocker version\ndocker version 1.12.1, build 23cf638\ncadvisor version\n0.23.8\n. ",
    "QQYES": "@derekwaynecarr yes\n. @derekwaynecarr hello,the result of the v0.24.1 is the same to v0.24.0, I downloaded the image from the https://github.com/google/cadvisor/releases/tag/v0.24.1,and it does't work too.\nthe API returns:\n/system.slice/cbss-app-apps-docker-containers-5a2735a98e0bce4f62cdfe676d891813d25e530b2ccca14722e7f4e3e186924e-shm.mount\":{\"spec\":{\"creation_time\":\"2016-10-11T10:28:21.501636778Z\",\"aliases\":[\"mesos-75185ff8-8829-4af7-b4b8-0684e99ff143-S24.ff5873d3-58aa-4049-87db-b7fb0612fbde\",\"5a2735a98e0bce4f62cdfe676d891813d25e530b2ccca14722e7f4e3e186924e\"],\"namespace\":\"docker\",\"has_cpu\":true,\"cpu\":{\"limit\":1024,\"max_limit\":0},\"has_memory\":true,\"memory\":{\"limit\":9223372036854775807,\"reservation\":9223372036854775807,\"swap_limit\":9223372036854775807},\"has_custom_metrics\":false,\"has_network\":true,\"has_filesystem\":true,\"has_diskio\":true,\"image\":\"10.161.24.239/iap_docker/marathon-lb-source:1.3.3\"},\"stats\":[{\"timestamp\":\"2016-11-01T08:14:20.781558968Z\",\"cpu\":{\"usage\":{\"total\":0,\"per_cpu_usage\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"user\":0,\"system\":0},\"cfs\":{\"periods\":0,\"throttled_periods\":0,\"throttled_time\":0},\"load_average\":0},\"diskio\":{},\"memory\":{\"usage\":0,\"cache\":0,\"rss\":0,\"swap\":0,\"working_set\":0,\"failcnt\":0,\"container_data\":{\"pgfault\":0,\"pgmajfault\":0},\"hierarchical_data\":{\"pgfault\":0,\"pgmajfault\":0}},\"network\":{\"interfaces\":[{\"name\":\"enp2s0f0\",\"rx_bytes\":846680017736,\"rx_packets\":935771674,\"rx_errors\":0,\"rx_dropped\":935593445,\"tx_bytes\":222,\"tx_packets\":3,\"tx_errors\":0,\"tx_dropped\":0},{\"name\":\"enp2s0f1\",\"rx_bytes\":61754184715629,\"rx_packets\":171557733135,\"rx_errors\":0,\"rx_dropped\":11631,\"tx_bytes\":32119400044549,\"tx_packets\":151598538902,\"tx_errors\":0,\"tx_dropped\":0},{\"name\":\"enp129s0f0\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},{\"name\":\"enp129s0f1\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},{\"name\":\"bond0\",\"rx_bytes\":62600018285193,\"rx_packets\":172491226093,\"rx_errors\":0,\"rx_dropped\":935980297,\"tx_bytes\":32118859739947,\"tx_packets\":151595794698,\"tx_errors\":0,\"tx_dropped\":0},{\"name\":\"bond0.102\",\"rx_bytes\":58239182270100,\"rx_packets\":150156562431,\"rx_errors\":0,\"rx_dropped\":452,\"tx_bytes\":31595298620878,\"tx_packets\":143661676229,\"tx_errors\":0,\"tx_dropped\":0}],\"tcp\":{\"Established\":0,\"SynSent\":0,\"SynRecv\":0,\"FinWait1\":0,\"FinWait2\":0,\"TimeWait\":0,\"Close\":0,\"CloseWait\":0,\"LastAck\":0,\"Listen\":0,\"Closing\":0},\"tcp6\":{\"Established\":0,\"SynSent\":0,\"SynRecv\":0,\"FinWait1\":0,\"FinWait2\":0,\"TimeWait\":0,\"Close\":0,\"CloseWait\":0,\"LastAck\":0,\"Listen\":0,\"Closing\":0}},\"filesystem\":{\"totalUsageBytes\":40960,\"baseUsageBytes\":0}},{\"timestamp\":\"2016-11-01T08:14:22.278044494Z\",\"cpu\":{\"usage\":{\"total\":0,\"per_cpu_usage\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"user\":0,\"system\":0},\"cfs\":{\"periods\":0,\"throttled_periods\":0,\"throttled_time\":0},\"load_average\":0},\"cpu_inst\":{\"usage\":{\"total\":0,\"per_cpu_usage\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"user\":0,\"system\":0}},\"diskio\":{},\"memory\":{\"usage\":0,\"cache\":0,\"rss\":0,\"swap\":0,\"working_set\":0,\"failcnt\":0,\"container_data\":{\"pgfault\":0,\"pgmajfault\":0},\"hierarchical_data\":{\"pgfault\":0,\"pgmajfault\":0}},\"network\":{\"interfaces\":[{\"name\":\"enp2s0f0\",\"rx_bytes\":846680019560,\"rx_packets\":935771702,\"rx_errors\":0,\"rx_dropped\":935593473,\"tx_bytes\":222,\"tx_packets\":3,\"tx_errors\":0,\"tx_dropped\":0},{\"name\":\"enp2s0f1\",\"rx_bytes\":61754191264289,\"rx_packets\":171557754678,\"rx_errors\":0,\"rx_dropped\":11631,\"tx_bytes\":32119402850871,\"tx_packets\":151598555967,\"tx_errors\":0,\"tx_dropped\":0},{\"name\":\"enp129s0f0\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},{\"name\":\"enp129s0f1\",\"rx_bytes\":0,\"rx_packets\":0,\"rx_errors\":0,\"rx_dropped\":0,\"tx_bytes\":0,\"tx_packets\":0,\"tx_errors\":0,\"tx_dropped\":0},{\"name\":\"bond0\",\"rx_bytes\":62600024835677,\"rx_packets\":172491247664,\"rx_errors\":0,\"rx_dropped\":935980325,\"tx_bytes\":32118862546269,\"tx_packets\":151595811763,\"tx_errors\":0,\"tx_dropped\":0},{\"name\":\"bond0.102\",\"rx_bytes\":58239188392980,\"rx_packets\":150156581587,\"rx_errors\":0,\"rx_dropped\":452,\"tx_bytes\":31595301387666,\"tx_packets\":143661692655,\"tx_errors\":0,\"tx_dropped\":0}],\"tcp\":{\"Established\":0,\"SynSent\":0,\"SynRecv\":0,\"FinWait1\":0,\"FinWait2\":0,\"TimeWait\":0,\"Close\":0,\"CloseWait\":0,\"LastAck\":0,\"Listen\":0,\"Closing\":0},\"tcp6\":{\"Established\":0,\"SynSent\":0,\"SynRecv\":0,\"FinWait1\":0,\"FinWait2\":0,\"TimeWait\":0,\"Close\":0,\"CloseWait\":0,\"LastAck\":0,\"Listen\":0,\"Closing\":0}},\"filesystem\":{\"totalUsageBytes\":40960,\"baseUsageBytes\":0}}]}}\n. ",
    "dneray": "Experiencing the same issue on coreOS 1235.1.0 with docker 1.12.3.\nWhen I run cadvisor outside of a docker container, I can only gather stats for all containers in system.slice, the rest report 0. When I run cadvisor inside a docker container, I can't gather stats fir system.slice, those ones all report 0.\nHave tried with v0.24.1. ",
    "roasties": "Hi, I'm having exactly the same porblem. Docker 1.12.3 & cAdvisor v0.24.1. Is there any update?. ",
    "F30": "I can confirm this for cAdvisor 0.28.3 running natively (not in a Docker container, but as a systemd unit) on Debian Stretch. This happens both when running cAdvisor as root and unprivileged user.. ",
    "Arlenmbx": "I'm having the same problem now with docker Version 17.12.0-ce and cadvisor version is release0.27,release0.28 and master branch.\nI run it both linux ubuntu host and in a container.\nhost info is  4.13.0-37-generic #42~16.04.1-Ubuntu SMP Wed Mar 7 16:03:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ",
    "mgdsxhy": "Run outside of a container, I want run the Cadvisor with nohup or systemctl.\nSuch as   \"nohup cAdvisor &\"\n              \"systemctl start cAdvisor\"\nCan realize?\n. Thanks, I'll try it.\n. ",
    "danwinship": "Well... I guess ideally cadvisor shouldn't assume that the set of network devices is static (since it isn't), and it should watch for notifications of devices coming and going (eg via netlink) and update NetworkDevices accordingly.\nOr if no other users of cadvisor use the NetworkDevices field either, it could just be dropped.\nA smaller fix would be to just change the code to be aware that devices can go away while it's trying to iterate them, and not consider it an error. I could write a patch for that if you wanted.\n. ",
    "stefanprodan": "On Ubuntu 16.04 the docker log is full of these errors, I'm getting hundreds of them per minute.\n. ",
    "jonathanperret": "Could this be #1572 ?. ",
    "shah-zobair": "I was able to solve it by:\nfor name in $(ls | grep -vls /var/lib/docker/containers/ | paste -sd \u201c|\u201d -); do rm -f /var/lib/dockershim/$name; done\n. ",
    "jjduhamel": "I'm seeing this as well.\n. +1. Any update on this?  Would be good to add support for this asap.. ",
    "taimir": ":+1: just stumbled upon the same problem\n. ",
    "alexellis": "Also getting this issue with latest Docker image as per Github instructions page on SLES. \n. ",
    "qband": "Here is the solution that works https://github.com/kubernetes/kubernetes/issues/32728#issuecomment-252469277\n. Here is the solution that works https://github.com/kubernetes/kubernetes/issues/32728#issuecomment-252469277\n. ",
    "oszi": "This issue has not been fixed still. :( The workaround recommended by @qband did work.\nREPOSITORY                  TAG                 IMAGE ID            CREATED             SIZE\ndocker.io/google/cadvisor   latest              75f88e3ec333        6 weeks ago         62.21 MB. ",
    "abelgana": "Experiencing the same issue. The workaround did work for me too. . ",
    "wongoo": "use the latest released version instead of the version latest.. ",
    "axiaoxin": "I run cadvisor in my centos7 is still failed\ndocker run   --volume=/:/rootfs:ro   --volume=/var/run:/var/run:rw   --volume=/sys:/sys:ro   --volume=/var/lib/docker/:/var/lib/docker:ro   --volume=/dev/disk/:/dev/disk:ro   --publish=8080:8080   --detach=false   --name=cadvisor   cadvisor:20180330\nI0330 08:06:28.976330       1 storagedriver.go:50] Caching stats in memory for 2m0s\nI0330 08:06:28.976684       1 manager.go:151] cAdvisor running in container: \"/sys/fs/cgroup/cpuacct,cpu\"\nI0330 08:06:28.999279       1 fs.go:139] Filesystem UUIDs: map[2ecd79ee-5c25-45b5-bb2a-f7d7d4908a42:/dev/sda4 7031eb24-d2f7-497b-a0d3-119ffab6f8ef:/dev/sda1 8806a345-6dc8-4149-93b0-fd0ea6ea372f:/dev/sda3 a9724b4d-4dbb-4a1e-88fb-47fa69f0467a:/dev/sda2]\nI0330 08:06:28.999695       1 fs.go:140] Filesystem partitions: map[shm:{mountpoint:/dev/shm major:0 minor:101 fsType:tmpfs blockSize:0} none:{mountpoint:/ major:0 minor:100 fsType:aufs blockSize:0} tmpfs:{mountpoint:/dev major:0minor:104 fsType:tmpfs blockSize:0} /dev/root:{mountpoint:/rootfs major:8 minor:1 fsType:ext4 blockSize:0} /dev/sda3:{mountpoint:/rootfs/usr/local major:8 minor:3 fsType:ext4 blockSize:0} /dev/sda4:{mountpoint:/rootfs/data major:8 minor:4 fsType:ext4 blockSize:0}]\nI0330 08:06:29.006887       1 manager.go:225] Machine: {NumCores:24 CpuFrequency:2600102 MemoryCapacity:134880329728 HugePages:[{PageSize:2048 NumPages:4096}] MachineID:bf59f7060e9742e18a5153b62aee491c SystemUUID:58783500-0A81-11E5-ADD1-403EE33EA27C BootID:48492dba-828e-44c1-bbfa-0ef93a55a14e Filesystems:[{Device:/dev/sda4 DeviceMajor:8 DeviceMinor:4 Capacity:1427020939264 Type:vfs Inodes:88498176 HasInodes:true} {Device:shm DeviceMajor:0 DeviceMinor:101 Capacity:67108864 Type:vfs Inodes:16464884 HasInodes:true} {Device:none DeviceMajor:0 DeviceMinor:100 Capacity:1427020939264 Type:vfs Inodes:88498176 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:104 Capacity:67440164864Type:vfs Inodes:16464884 HasInodes:true} {Device:/dev/root DeviceMajor:8 DeviceMinor:1 Capacity:21003628544 Type:vfs Inodes:1310720 HasInodes:true} {Device:/dev/sda3 DeviceMajor:8 DeviceMinor:3 Capacity:21003628544 Type:vfs Inodes:1310720 HasInodes:true}] DiskMap:map[43:1:{Name:nbd1 Major:43 Minor:1 Size:0 Scheduler:cfq} 43:12:{Name:nbd12 Major:43 Minor:12 Size:0 Scheduler:cfq} 43:6:{Name:nbd6 Major:43 Minor:6 Size:0 Scheduler:cfq} 43:8:{Name:nbd8 Major:43 Minor:8 Size:0 Scheduler:cfq} 8:0:{Name:sda Major:8 Minor:0 Size:1494996746240 Scheduler:cfq} 9:0:{Name:md0 Major:9 Minor:0 Size:0 Scheduler:none} 43:10:{Name:nbd10 Major:43 Minor:10 Size:0 Scheduler:cfq} 43:13:{Name:nbd13 Major:43 Minor:13 Size:0 Scheduler:cfq} 43:14:{Name:nbd14 Major:43 Minor:14 Size:0 Scheduler:cfq} 43:15:{Name:nbd15 Major:43 Minor:15 Size:0 Scheduler:cfq} 43:2:{Name:nbd2 Major:43 Minor:2 Size:0 Scheduler:cfq} 43:5:{Name:nbd5 Major:43Minor:5 Size:0 Scheduler:cfq} 43:7:{Name:nbd7 Major:43 Minor:7 Size:0 Scheduler:cfq} 43:9:{Name:nbd9 Major:43 Minor:9 Size:0 Scheduler:cfq} 43:0:{Name:nbd0 Major:43 Minor:0 Size:0 Scheduler:cfq} 43:11:{Name:nbd11 Major:43 Minor:11 Size:0 Scheduler:cfq} 43:3:{Name:nbd3 Major:43 Minor:3 Size:0 Scheduler:cfq} 43:4:{Name:nbd4 Major:43 Minor:4 Size:0 Scheduler:cfq}] NetworkDevices:[{Name:br-3f0d18e897c0 MacAddress:02:42:4e:60:5f:12 Speed:0 Mtu:1500} {Name:eth0MacAddress:7c:a2:3e:8c:f0:3c Speed:0 Mtu:1500} {Name:eth1 MacAddress:7c:a2:3e:8c:f0:3d Speed:1000 Mtu:1500} {Name:tunl0 MacAddress:00:00:00:00 Speed:0 Mtu:0}] Topology:[{Id:0 Memory:68689707008 Cores:[{Id:0 Threads:[0 12] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[1 13] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:UnifiedLevel:2}]} {Id:2 Threads:[2 14] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:3 Threads:[3 15] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:4 Threads:[4 16] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:5 Threads:[5 17] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:15728640Type:Unified Level:3}]} {Id:1 Memory:68719476736 Cores:[{Id:0 Threads:[6 18] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[7 19] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:2 Threads:[8 20] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:3 Threads:[9 21] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:4 Threads:[10 22] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:5 Threads:[11 23] Caches:[{Size:32768 Type:Data Level:1} {Size:32768Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:15728640 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}\nI0330 08:06:29.007899       1 manager.go:231] Version: {KernelVersion:3.10.107-1-tlinux2-0046 ContainerOsVersion:Alpine Linux v3.4 DockerVersion:1.12.6 DockerAPIVersion:1.24 CadvisorVersion:v0.28.3 CadvisorRevision:1e567c2}\nI0330 08:06:29.017279       1 factory.go:356] Registering Docker factory\nI0330 08:06:31.017629       1 factory.go:54] Registering systemd factory\nI0330 08:06:31.018829       1 factory.go:86] Registering Raw factory\nI0330 08:06:31.019825       1 manager.go:1178] Started watching for new ooms in manager\nW0330 08:06:31.019855       1 manager.go:313] Could not configure a source for OOM detection, disabling OOM events: open /dev/kmsg: no such file or directory\nI0330 08:06:31.026114       1 manager.go:329] Starting recovery of all containers\nI0330 08:06:31.107502       1 manager.go:334] Recovery completed\nF0330 08:06:31.124525       1 cadvisor.go:156] Failed to start container manager: inotify_add_watch /sys/fs/cgroup/cpuacct,cpu: no such file or directory\nll -d /sys/fs/cgroup/cpu*\nlrwxrwxrwx 1 root root 11 Jan 24 10:19 /sys/fs/cgroup/cpu -> cpu,cpuacct\ndrwxr-xr-x 4 root root  0 Feb  7 11:38 /sys/fs/cgroup/cpu,cpuacct\nlrwxrwxrwx 1 root root 11 Jan 24 10:19 /sys/fs/cgroup/cpuacct -> cpu,cpuacct\ndrwxr-xr-x 3 root root  0 Jan 24 10:19 /sys/fs/cgroup/cpuset\n\nFixed it by:\nmount -o remount,rw '/sys/fs/cgroup'\nln -s /sys/fs/cgroup/cpu,cpuacct /sys/fs/cgroup/cpuacct,cpu.\n",
    "kartikjena3": "@axiaoxin nice work, Working perfectly. ",
    "Cherishty": "@axiaoxin It works, but will reproduce whenever I reboot the machine, any workaround?. Hi @WanLinghao  what you mean for /cc ? \nSince we are actively working on exposing GPU to Prometheus, any suggestion or guide?\nBest Regards!. cc @mindprince. Quite appreciate for your attention and help!\n@dashpole  I have a try on your solution, checking the log of pod I believe it has detected the GPU, then where it exposes the metrics to and how can I check it?\n@mindprince  Yes, for embedded cAdvisor, I have set the right LD_LIBRARY_PATH, I am not sure what you mean for  have access to GPU devices but it should be since tensorflow training for GPU can run successfully on my GPU node. Additionally, may you double confirm what is the metrics representing for GPU? \nFollowing the context in issue 1912, seems it should be container_accelerator_xxx, but I didn't find clu like this on my ::10255/metrics/cadvisor\nThanks !!!\n. @dashpole  Thanks for your guidance.\nWhile it returns nothing when I running wget localhost:8080/metrics , I check the log which reports that:\n\n[root@stcal-102 ~]# kubectl log nvidia-gpu-monitoring-daemonset-4fgp6 -n monitoring\nW1116 10:40:48.336472  105667 cmd.go:353] log is DEPRECATED and will be removed in a future version. Use logs instead.\nI1113 15:14:59.103954       1 main.go:41] Starting example-gpu-monitor\nI1113 15:14:59.798777       1 stats.go:56] NVML initialized. Number of nvidia devices: 1\nW1113 15:14:59.804360       1 util_unix.go:75] Using \"/podresources/kubelet.sock\" as endpoint is deprecated, please consider using full url format \"unix:///podresources/kubelet.sock\".\nE1116 01:59:52.948493       1 metrics.go:71] error getting devices from the kubelet: &{0xc0002101e0}.Get() = , rpc error: code = Unavailable desc = grpc: the connection is unavailable\n\nAs I mentioned before, I am using --feature-aget=Accelerators=true in kubelet instead of device-plugin and I do NOT install nvidia-docker, is that a necessary?\nDocker version: 1.12.6\nKubernetes version: 1.9.5\nAny clues or ideas?. @dashpole I can not appreciate more for your patient clarification and guidance!\nFollowing your words I try Kustomize with your patch and generate below cadvisor.yaml:\n```\napiVersion: v1\nkind: Namespace\nmetadata:\n  labels:\n    app: cadvisor\n  name: cadvisor\n\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  annotations:\n    seccomp.security.alpha.kubernetes.io/pod: docker/default\n  labels:\n    app: cadvisor\n  name: cadvisor\n  namespace: cadvisor\nspec:\n  selector:\n    matchLabels:\n      app: cadvisor\n      name: cadvisor\n  template:\n    metadata:\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: \"\"\n      labels:\n        app: cadvisor\n        name: cadvisor\n    spec:\n      automountServiceAccountToken: false\n      containers:\n      - args:\n        - --housekeeping_interval=10s\n        - --max_housekeeping_interval=15s\n        - --event_storage_event_limit=default=0\n        - --event_storage_age_limit=default=0\n        - --disable_metrics=percpu,disk,network,tcp,udp\n        - --docker_only\n        env:\n        - name: LD_LIBRARY_PATH\n          value: /bin/nvidia/lib64/\n        image: k8s.gcr.io/cadvisor:v0.30.2\n        name: cadvisor\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 300m\n          requests:\n            cpu: 150m\n            memory: 200Mi\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /dev\n          name: dev\n        - mountPath: /bin/nvidia/lib64/\n          name: libnvidia\n        - mountPath: /rootfs\n          name: rootfs\n          readOnly: true\n        - mountPath: /var/run\n          name: var-run\n          readOnly: true\n        - mountPath: /sys\n          name: sys\n          readOnly: true\n        - mountPath: /var/lib/docker\n          name: docker\n          readOnly: true\n        - mountPath: /dev/disk\n          name: disk\n          readOnly: true\n      - command:\n        - /monitor\n        - --stackdriver-prefix=custom.googleapis.com\n        - --source=cadvisor:http://localhost:8080\n        - --pod-id=$(POD_NAME)\n        - --namespace-id=$(POD_NAMESPACE)\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: gcr.io/google-containers/prometheus-to-sd:v0.2.6\n        name: prometheus-to-sd\n        ports:\n        - containerPort: 6061\n          name: profiler\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n      priorityClassName: system-node-critical\n      terminationGracePeriodSeconds: 30\n      tolerations:\n      - key: CriticalAddonsOnly\n        operator: Exists\n      volumes:\n      - hostPath:\n          path: /dev\n        name: dev\n      - hostPath:\n          path: /usr/lib64/nvidia\n        name: libnvidia\n      - hostPath:\n          path: /\n        name: rootfs\n      - hostPath:\n          path: /var/run\n        name: var-run\n      - hostPath:\n          path: /sys\n        name: sys\n      - hostPath:\n          path: /var/lib/docker\n        name: docker\n      - hostPath:\n          path: /dev/disk\n        name: disk\n```\nWhile it occurs CrashLoopBackOff and I collect the error log as below:\nkubectl log -p cadvisor-9pl2f -n cadvisor\nI1119 07:49:56.632813       1 manager.go:233] Version: {KernelVersion:3.10.0-862.14.4.el7.x86_64 ContainerOsVersion:Alpine Linux v3.7 DockerVersion:1.12.6 DockerAPIVersion:1.24 CadvisorVersion:v0.30.2 CadvisorRevision:de723a09}\nE1119 07:49:56.674977       1 factory.go:340] devicemapper filesystem stats will not be reported: usage of thin_ls is disabled to preserve iops\nI1119 07:49:56.675983       1 factory.go:356] Registering Docker factory\nI1119 07:49:56.676199       1 factory.go:54] Registering systemd factory\nI1119 07:49:56.678237       1 factory.go:86] Registering Raw factory\nI1119 07:49:56.680433       1 manager.go:1205] Started watching for new ooms in manager\nI1119 07:49:56.820255       1 nvidia.go:100] NVML initialized. Number of nvidia devices: 1\nI1119 07:49:56.827148       1 manager.go:356] Starting recovery of all containers\nI1119 07:49:58.522108       1 manager.go:361] Recovery completed\nF1119 07:49:58.727346       1 cadvisor.go:159] Failed to start container manager: inotify_add_watch /sys/fs/cgroup/cpuacct,cpu: no such file or directory\nand\n[root@stcal-102 ~]# kubectl log -p cadvisor-9pl2f -n cadvisor  prometheus-to-sd\nW1119 15:50:15.588580   69321 cmd.go:353] log is DEPRECATED and will be removed in a future version. Use logs instead.\nF1119 07:49:57.880255       1 main.go:82] Failed to get GCE config: Not running on GCE.\nSeems securityContext: privileged: true is lost in the generated yaml?\nAny clues or suggestion?. @mindprince Sorry to disturb you, but still want to make it clear that how can the cadvisor embedded in kubelet access to expose gpu metrics? \nSince I can't see any metrics like container_accelerator under the default configuration\nBest Regards!. @dashpole  After fixing the issue you mentioned, The only error it occurs is that prometheus-to-sd container said:\n\nFailed to get GCE config: Not running on GCE.\n\nSo I remove it and re-create cadvisor, now it can export container_accelerator_ metrics as you said when invoking GPU in container !\nAgain, thanks a lot for your contribution and kindness \ud83d\udcaf . @mindprince  Sorry to say that I am not quite familiar with how cadvisor embedded in k8s, so I assume cadvisor running in kubeletand checking its log I find nothing about NVML :\n\njournalctl -r -u kubelet\n\n```\nkubelet[25630]: I1120 16:55:05.979606   25630 reconciler.go:217] operationExecutor.VerifyControllerAttachedVolume started for volume \"libnvidia\" (UniqueName: \"kubernetes.io/host-path/2a711dea-ec91-11e8-85b6-000d3af9a998-libnvidia\") pod \"cadvisor-5j7mh\" (UID: \"2a711dea-ec91-11e8-85b6-000d3af9a998\")\nkubelet[25630]: E1120 16:55:04.678992   25630 container_manager_linux.go:583] [ContainerManager]: Fail to get rootfs information unable to find data for container /\nkubelet[25630]: I1120 16:55:04.180135   25630 kubelet.go:1789] skipping pod synchronization - [container runtime is down]\nkubelet[25630]: I1120 16:55:04.113884   25630 kubelet_node_status.go:792] Node became not ready: {Type:Ready Status:False LastHeartbeatTime:2018-11-20 16:55:04.113844472 +0800 CST m=+1.748154141 LastTransitionTime:2018-11-20 16:55:04.113844472 +0800 CST m=+1.748154141 Reason:KubeletNotReady Message:container runtime is down}\nkubelet[25630]: I1120 16:55:04.078260   25630 kubelet_network.go:196] Setting Pod CIDR:  -> 10.244.1.0/24\nkubelet[25630]: I1120 16:55:04.077938   25630 docker_service.go:343] docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.1.0/24,},}\nkubelet[25630]: I1120 16:55:04.077617   25630 kuberuntime_manager.go:918] updating runtime config through cri with podcidr 10.244.1.0/24\nkubelet[25630]: I1120 16:55:03.891016   25630 kubelet_node_status.go:85] Successfully registered node suzlab1080-001\nkubelet[25630]: I1120 16:55:03.890976   25630 kubelet_node_status.go:127] Node suzlab1080-001 was previously registered\nkubelet[25630]: E1120 16:55:03.678373   25630 container_manager_linux.go:583] [ContainerManager]: Fail to get rootfs information unable to find data for container /\nkubelet[25630]: I1120 16:55:03.379983   25630 kubelet.go:1789] skipping pod synchronization - [container runtime is down]\nkubelet[25630]: I1120 16:55:02.979777   25630 kubelet.go:1789] skipping pod synchronization - [container runtime is down]\nkubelet[25630]: I1120 16:55:02.817838   25630 kubelet_node_status.go:82] Attempting to register node suzlab1080-001\nkubelet[25630]: I1120 16:55:02.779384   25630 kubelet.go:1789] skipping pod synchronization - [container runtime is down]\nkubelet[25630]: I1120 16:55:02.779257   25630 kubelet_node_status.go:273] Setting node annotation to enable volume controller attach/detach\nkubelet[25630]: E1120 16:55:02.707977   25630 factory.go:340] devicemapper filesystem stats will not be reported: usage of thin_ls is disabled to preserve iops\nkubelet[25630]: I1120 16:55:02.679263   25630 kubelet.go:1789] skipping pod synchronization - [container runtime is down PLEG is not healthy: pleg was last seen active 2562047h47m16.854775807s ago; threshold is 3m0s]\nkubelet[25630]: I1120 16:55:02.679204   25630 volume_manager.go:247] Starting Kubelet Volume Manager\nkubelet[25630]: I1120 16:55:02.679128   25630 kubelet.go:1772] Starting kubelet main sync loop.\nkubelet[25630]: I1120 16:55:02.679077   25630 status_manager.go:140] Starting to sync pod status with apiserver\nkubelet[25630]: I1120 16:55:02.679015   25630 fs_resource_analyzer.go:66] Starting FS ResourceAnalyzer\nkubelet[25630]: E1120 16:55:02.678152   25630 container_manager_linux.go:583] [ContainerManager]: Fail to get rootfs information unable to find data for container /\nkubelet[25630]: I1120 16:55:02.637304   25630 kubelet_node_status.go:273] Setting node annotation to enable volume controller attach/detach\nkubelet[25630]: I1120 16:55:02.635290   25630 server.go:299] Adding debug handlers to kubelet server.\nkubelet[25630]: I1120 16:55:02.633370   25630 server.go:129] Starting to listen on 0.0.0.0:10250\nkubelet[25630]: E1120 16:55:02.633281   25630 kubelet.go:1281] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data for container /\nkubelet[25630]: I1120 16:55:02.633158   25630 server.go:755] Started kubelet\nkubelet[25630]: I1120 16:55:02.632266   25630 client.go:109] Start docker client with request timeout=2m0s\nkubelet[25630]: I1120 16:55:02.632239   25630 client.go:80] Connecting to docker on unix:///var/run/docker.sock\nkubelet[25630]: I1120 16:55:02.630339   25630 kuberuntime_manager.go:186] Container runtime docker initialized, version: 1.12.6, apiVersion: 1.24.0\nkubelet[25630]: I1120 16:55:02.628326   25630 remote_runtime.go:43] Connecting to runtime service unix:///var/run/dockershim.sock\nkubelet[25630]: I1120 16:55:02.606446   25630 docker_service.go:250] Setting cgroupDriver to systemd\nkubelet[25630]: I1120 16:55:02.606282   25630 docker_service.go:237] Docker Info: &{ID:E2YI:PBFW:JOI7:TW3T:XIEA:GWCS:BCHK:RCTR:MREP:6TPW:MMUJ:S2SO Containers:40 ContainersRunning:40 ContainersPaused:0 ContainersStopped:0 Images:21 Driver:devicemapper DriverStatus:[[Pool Name docker-thinpool] [Pool Blocksize 524.3 kB] [Base Device Size 21.47 GB] [Backing Filesystem xfs] [Data file ] [Metadata file ] [Data Space Used 7.034 GB] [Data Space Total 2.089 TB] [Data Space Available 2.082 TB] [Metadata Space Used 3.588 MB] [Metadata Space Total 16.98 GB] [Metadata Space Available 16.97 GB] [Thin Pool Minimum Free Space 208.9 GB] [Udev Sync Supported true] [Deferred Removal Enabled true] [Deferred Deletion Enabled true] [Deferred Deleted Device Count 0] [Library Version 1.02.146-RHEL7 (2018-01-22)]] SystemStatus:[] Plugins:{Volume:[local] Network:[null host bridge overlay] Authorization:[] Log:[]} MemoryLimit:true SwapLimit:true KernelMemory:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:238 OomKillDisable:true NGoroutines:190 SystemTime:2018-11-20T16:55:02.605262641+08:00 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:3.10.0-862.14.4.el7.x86_64 OperatingSystem:CentOS Linux 7 (Core) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc42052a5b0 NCPU:8 MemTotal:67038867456 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:suzlab1080-001 Labels:[] ExperimentalBuild:false ServerVersion:1.12.6 ClusterStore: ClusterAdvertise: Runtimes:map[runc:{Path:docker-runc Args:[--systemd-cgroup=true]}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:0xc420648500} LiveRestoreEnabled:false Isolation: InitBinary: ContainerdCommit:{ID: Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[seccomp]}\nkubelet[25630]: I1120 16:55:02.587108   25630 docker_service.go:232] Docker cri networking managed by cni\nkubelet[25630]: I1120 16:55:02.580430   25630 client.go:109] Start docker client with request timeout=2m0s\nkubelet[25630]: I1120 16:55:02.580408   25630 client.go:80] Connecting to docker on unix:///var/run/docker.sock\nkubelet[25630]: I1120 16:55:02.580081   25630 kubelet.go:577] Hairpin mode set to \"hairpin-veth\"\nkubelet[25630]: W1120 16:55:02.580040   25630 kubelet_network.go:139] Hairpin mode set to \"promiscuous-bridge\" but kubenet is not enabled, falling back to \"hairpin-veth\"\nkubelet[25630]: I1120 16:55:02.575019   25630 kubelet.go:316] Watching apiserver\nkubelet[25630]: I1120 16:55:02.574964   25630 kubelet.go:291] Adding manifest path: /etc/kubernetes/manifests\nkubelet[25630]: I1120 16:55:02.574767   25630 container_manager_linux.go:266] Creating device plugin manager: false\nkubelet[25630]: I1120 16:55:02.574536   25630 container_manager_linux.go:247] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: ContainerRuntime:docker CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[memory:{i:{value:1073741824 scale:0} d:{Dec:} s:1Gi Format:BinarySI}] HardEvictionThresholds:[{Signal:memory.available Operator:LessThan Value:{Quantity:200Mi Percentage:0} GracePeriod:0s MinReclaim:} {Signal:nodefs.available Operator:LessThan Value:{Quantity: Percentage:0.1} GracePeriod:0s MinReclaim:} {Signal:imagefs.available Operator:LessThan Value:{Quantity: Percentage:0.2} GracePeriod:0s MinReclaim:}]} ExperimentalQOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalCPUManagerReconcilePeriod:10s}\nkubelet[25630]: I1120 16:55:02.574506   25630 container_manager_linux.go:242] container manager verified user specified cgroup-root exists: /\nkubelet[25630]: I1120 16:55:02.573190   25630 server.go:428] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /\nkubelet[25630]: I1120 16:55:02.497428   25630 certificate_store.go:130] Loading cert/key pair from (\"/var/lib/kubelet/pki/kubelet-client.crt\", \"/var/lib/kubelet/pki/kubelet-client.key\").\nkubelet[25630]: I1120 16:55:02.467619   25630 plugins.go:101] No cloud provider specified.\nkubelet[25630]: I1120 16:55:02.467484   25630 feature_gate.go:226] feature gates: &{{} map[Accelerators:true]}\nkubelet[25630]: I1120 16:55:02.467414   25630 server.go:182] Version: v1.9.5\nkubelet[25630]: I1120 16:55:02.456538   25630 controller.go:118] kubelet config controller: validating combination of defaults and flags\nkubelet[25630]: I1120 16:55:02.456533   25630 controller.go:114] kubelet config controller: starting controller\nkubelet[25630]: I1120 16:55:02.456452   25630 feature_gate.go:226] feature gates: &{{} map[Accelerators:true]}\nsystemd[1]: Starting kubelet: The Kubernetes Node Agent...\nsystemd[1]: Started kubelet: The Kubernetes Node Agent.\nsystemd[1]: kubelet.service failed.\nsystemd[1]: Unit kubelet.service entered failed state.\nsystemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE\nsystemd[1]: Stopping kubelet: The Kubernetes Node Agent...\n```. Sorry but it return nothing :(. Sure,  here is my environemnt:\n\nOS kernel : CentOS 7.5 3.10.0-862.14.4.el7.x86_64\nkubernetes version :1.9.5\ndocker version : 1.12.6\nnvidia Driver Version: 390.59 \nGPU :  GTX 1080\n\nI have two VM with the same configuration and the same behavior\nAnd below is the log  provided by  journalctl -u kubelet --no-pager > log after running systemctl restart kubelet\nNov 22 23:43:10 suzlab1080-001 systemd[1]: Stopping kubelet: The Kubernetes Node Agent...\nNov 22 23:43:10 suzlab1080-001 systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE\nNov 22 23:43:10 suzlab1080-001 systemd[1]: Unit kubelet.service entered failed state.\nNov 22 23:43:10 suzlab1080-001 systemd[1]: kubelet.service failed.\nNov 22 23:43:10 suzlab1080-001 systemd[1]: Started kubelet: The Kubernetes Node Agent.\nNov 22 23:43:10 suzlab1080-001 systemd[1]: Starting kubelet: The Kubernetes Node Agent...\nNov 22 23:43:10 suzlab1080-001 kubelet[102275]: I1122 23:43:10.862651  102275 feature_gate.go:226] feature gates: &{{} map[Accelerators:true]}\nNov 22 23:43:10 suzlab1080-001 kubelet[102275]: I1122 23:43:10.862749  102275 controller.go:114] kubelet config controller: starting controller\nNov 22 23:43:10 suzlab1080-001 kubelet[102275]: I1122 23:43:10.862769  102275 controller.go:118] kubelet config controller: validating combination of defaults and flags\nNov 22 23:43:10 suzlab1080-001 kubelet[102275]: I1122 23:43:10.874132  102275 server.go:182] Version: v1.9.5\nNov 22 23:43:10 suzlab1080-001 kubelet[102275]: I1122 23:43:10.874201  102275 feature_gate.go:226] feature gates: &{{} map[Accelerators:true]}\nNov 22 23:43:10 suzlab1080-001 kubelet[102275]: I1122 23:43:10.874325  102275 plugins.go:101] No cloud provider specified.\nNov 22 23:43:10 suzlab1080-001 kubelet[102275]: I1122 23:43:10.901221  102275 certificate_store.go:130] Loading cert/key pair from (\"/var/lib/kubelet/pki/kubelet-client.crt\", \"/var/lib/kubelet/pki/kubelet-client.key\").\nNov 22 23:43:10 suzlab1080-001 kubelet[102275]: I1122 23:43:10.971050  102275 server.go:428] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /\nNov 22 23:43:10 suzlab1080-001 kubelet[102275]: I1122 23:43:10.972203  102275 container_manager_linux.go:242] container manager verified user specified cgroup-root exists: /\nNov 22 23:43:10 suzlab1080-001 kubelet[102275]: I1122 23:43:10.972230  102275 container_manager_linux.go:247] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: ContainerRuntime:docker CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[memory:{i:{value:1073741824 scale:0} d:{Dec:<nil>} s:1Gi Format:BinarySI}] HardEvictionThresholds:[{Signal:memory.available Operator:LessThan Value:{Quantity:200Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.1} GracePeriod:0s MinReclaim:<nil>} {Signal:imagefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.2} GracePeriod:0s MinReclaim:<nil>}]} ExperimentalQOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalCPUManagerReconcilePeriod:10s}\nNov 22 23:43:10 suzlab1080-001 kubelet[102275]: I1122 23:43:10.972419  102275 container_manager_linux.go:266] Creating device plugin manager: false\nNov 22 23:43:10 suzlab1080-001 kubelet[102275]: I1122 23:43:10.972590  102275 kubelet.go:291] Adding manifest path: /etc/kubernetes/manifests\nNov 22 23:43:10 suzlab1080-001 kubelet[102275]: I1122 23:43:10.972634  102275 kubelet.go:316] Watching apiserver\nNov 22 23:43:10 suzlab1080-001 kubelet[102275]: W1122 23:43:10.977287  102275 kubelet_network.go:139] Hairpin mode set to \"promiscuous-bridge\" but kubenet is not enabled, falling back to \"hairpin-veth\"\nNov 22 23:43:10 suzlab1080-001 kubelet[102275]: I1122 23:43:10.977319  102275 kubelet.go:577] Hairpin mode set to \"hairpin-veth\"\nNov 22 23:43:10 suzlab1080-001 kubelet[102275]: I1122 23:43:10.977617  102275 client.go:80] Connecting to docker on unix:///var/run/docker.sock\nNov 22 23:43:10 suzlab1080-001 kubelet[102275]: I1122 23:43:10.977635  102275 client.go:109] Start docker client with request timeout=2m0s\nNov 22 23:43:10 suzlab1080-001 kubelet[102275]: I1122 23:43:10.984214  102275 docker_service.go:232] Docker cri networking managed by cni\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: I1122 23:43:11.005308  102275 docker_service.go:237] Docker Info: &{ID:O7OI:MPIJ:N47K:LJFI:3PAO:POKH:O67Z:RPLI:D4CH:PIFX:6WVF:GWHU Containers:39 ContainersRunning:38 ContainersPaused:0 ContainersStopped:1 Images:17 Driver:devicemapper DriverStatus:[[Pool Name docker-thinpool] [Pool Blocksize 524.3 kB] [Base Device Size 21.47 GB] [Backing Filesystem xfs] [Data file ] [Metadata file ] [Data Space Used 5.741 GB] [Data Space Total 2.089 TB] [Data Space Available 2.083 TB] [Metadata Space Used 3.457 MB] [Metadata Space Total 16.98 GB] [Metadata Space Available 16.98 GB] [Thin Pool Minimum Free Space 208.9 GB] [Udev Sync Supported true] [Deferred Removal Enabled true] [Deferred Deletion Enabled true] [Deferred Deleted Device Count 1] [Library Version 1.02.146-RHEL7 (2018-01-22)]] SystemStatus:[] Plugins:{Volume:[local] Network:[host bridge overlay null] Authorization:[] Log:[]} MemoryLimit:true SwapLimit:true KernelMemory:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:223 OomKillDisable:true NGoroutines:180 SystemTime:2018-11-22T23:43:11.004295354+08:00 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:3.10.0-862.14.4.el7.x86_64 OperatingSystem:CentOS Linux 7 (Core) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc42016a000 NCPU:8 MemTotal:67038867456 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:suzlab1080-001 Labels:[] ExperimentalBuild:false ServerVersion:1.12.6 ClusterStore: ClusterAdvertise: Runtimes:map[runc:{Path:docker-runc Args:[--systemd-cgroup=true]}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:0xc42098c000} LiveRestoreEnabled:false Isolation: InitBinary: ContainerdCommit:{ID: Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[seccomp]}\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: I1122 23:43:11.005499  102275 docker_service.go:250] Setting cgroupDriver to systemd\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: I1122 23:43:11.027212  102275 remote_runtime.go:43] Connecting to runtime service unix:///var/run/dockershim.sock\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: I1122 23:43:11.029500  102275 kuberuntime_manager.go:186] Container runtime docker initialized, version: 1.12.6, apiVersion: 1.24.0\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: I1122 23:43:11.031778  102275 client.go:80] Connecting to docker on unix:///var/run/docker.sock\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: I1122 23:43:11.031814  102275 client.go:109] Start docker client with request timeout=2m0s\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: I1122 23:43:11.032730  102275 server.go:755] Started kubelet\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: E1122 23:43:11.032908  102275 kubelet.go:1281] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data for container /\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: I1122 23:43:11.032976  102275 server.go:129] Starting to listen on 0.0.0.0:10250\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: I1122 23:43:11.034928  102275 server.go:299] Adding debug handlers to kubelet server.\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: I1122 23:43:11.036642  102275 kubelet_node_status.go:273] Setting node annotation to enable volume controller attach/detach\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: E1122 23:43:11.081316  102275 container_manager_linux.go:583] [ContainerManager]: Fail to get rootfs information unable to find data for container /\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: I1122 23:43:11.082329  102275 fs_resource_analyzer.go:66] Starting FS ResourceAnalyzer\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: I1122 23:43:11.082416  102275 status_manager.go:140] Starting to sync pod status with apiserver\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: I1122 23:43:11.082486  102275 volume_manager.go:247] Starting Kubelet Volume Manager\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: I1122 23:43:11.082500  102275 kubelet.go:1772] Starting kubelet main sync loop.\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: I1122 23:43:11.082572  102275 kubelet.go:1789] skipping pod synchronization - [container runtime is down PLEG is not healthy: pleg was last seen active 2562047h47m16.854775807s ago; threshold is 3m0s]\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: E1122 23:43:11.106867  102275 factory.go:340] devicemapper filesystem stats will not be reported: usage of thin_ls is disabled to preserve iops\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: I1122 23:43:11.182648  102275 kubelet.go:1789] skipping pod synchronization - [container runtime is down]\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: I1122 23:43:11.182671  102275 kubelet_node_status.go:273] Setting node annotation to enable volume controller attach/detach\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: I1122 23:43:11.224339  102275 kubelet_node_status.go:82] Attempting to register node suzlab1080-001\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: I1122 23:43:11.382967  102275 kubelet.go:1789] skipping pod synchronization - [container runtime is down]\nNov 22 23:43:11 suzlab1080-001 kubelet[102275]: I1122 23:43:11.783123  102275 kubelet.go:1789] skipping pod synchronization - [container runtime is down]\nNov 22 23:43:12 suzlab1080-001 kubelet[102275]: E1122 23:43:12.081579  102275 container_manager_linux.go:583] [ContainerManager]: Fail to get rootfs information unable to find data for container /\nNov 22 23:43:12 suzlab1080-001 kubelet[102275]: I1122 23:43:12.315118  102275 kubelet_node_status.go:127] Node suzlab1080-001 was previously registered\nNov 22 23:43:12 suzlab1080-001 kubelet[102275]: I1122 23:43:12.315156  102275 kubelet_node_status.go:85] Successfully registered node suzlab1080-001\nNov 22 23:43:12 suzlab1080-001 kubelet[102275]: I1122 23:43:12.505908  102275 kuberuntime_manager.go:918] updating runtime config through cri with podcidr 10.244.1.0/24\nNov 22 23:43:12 suzlab1080-001 kubelet[102275]: I1122 23:43:12.506236  102275 docker_service.go:343] docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.1.0/24,},}\nNov 22 23:43:12 suzlab1080-001 kubelet[102275]: I1122 23:43:12.506607  102275 kubelet_network.go:196] Setting Pod CIDR:  -> 10.244.1.0/24\nNov 22 23:43:12 suzlab1080-001 kubelet[102275]: I1122 23:43:12.547624  102275 kubelet_node_status.go:792] Node became not ready: {Type:Ready Status:False LastHeartbeatTime:2018-11-22 23:43:12.54758975 +0800 CST m=+1.771800734 LastTransitionTime:2018-11-22 23:43:12.54758975 +0800 CST m=+1.771800734 Reason:KubeletNotReady Message:container runtime is down}\nNov 22 23:43:12 suzlab1080-001 kubelet[102275]: I1122 23:43:12.583274  102275 kubelet.go:1789] skipping pod synchronization - [container runtime is down]\nNov 22 23:43:13 suzlab1080-001 kubelet[102275]: E1122 23:43:13.082231  102275 container_manager_linux.go:583] [ContainerManager]: Fail to get rootfs information unable to find data for container /\nNov 22 23:43:14 suzlab1080-001 kubelet[102275]: I1122 23:43:14.382937  102275 reconciler.go:217] operationExecutor.VerifyControllerAttachedVolume started for volume \"proc\" (UniqueName: \"kubernetes.io/host-path/92568c5d-e4b3-11e8-85b6-000d3af9a998-proc\") pod \"node-exporter-r6bvv\" (UID: \"92568c5d-e4b3-11e8-85b6-000d3af9a998\"). Additionally, below is my configuration for 10-kubeadm.conf and docker daemon.json\n\n10-kubeadm.conf\n\n[Service]\nEnvironment=\"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\"\nEnvironment=\"KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true\"\nEnvironment=\"KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin\"\nEnvironment=\"KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local\"\nEnvironment=\"KUBELET_AUTHZ_ARGS=--authentication-token-webhook=true --authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt\"\nEnvironment=\"KUBELET_CGROUP_ARGS=--cgroup-driver=systemd\"\nEnvironment=\"KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki\"\nEnvironment=\"KUBELET_EXTRA_ARGS=--feature-gates=Accelerators=true --eviction-hard=memory.available<200Mi,nodefs.available<10%,imagefs.available<20% --eviction-minimum-reclaim=memory.available=100Mi,nodefs.available=5%,imagefs.available=5% --system-reserved=memory=1Gi --root-dir=/var/lib/kubelet\"\nExecStart=\nExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_SYSTEM_PODS_ARGS $KUBELET_NETWORK_ARGS $KUBELET_DNS_ARGS $KUBELET_AUTHZ_ARGS $KUBELET_CADVISOR_ARGS $KUBELET_CGROUP_ARGS $KUBELET_CERTIFICATE_ARGS $KUBELET_EXTRA_ARGS\n\ndaemon.json\n```\n{\n    \"insecure-registries\": [],\n    \"storage-driver\": \"devicemapper\",\n    \"storage-opts\": [\n      \"dm.thinpooldev=/dev/mapper/docker-thinpool\",\n      \"dm.basesize=20G\",\n      \"dm.use_deferred_removal=true\",\n      \"dm.use_deferred_deletion=true\"\n    ],\n    \"exec-opts\": [\"native.cgroupdriver=systemd\"]\n}\n\n```\nBest Regards!. That's right. \nAfter doing this and reload daemon,restart docker and kubelet , it shows as below:\nNov 29 18:44:35 suzlab1080-001 kubelet[17830]: I1129 18:44:35.783570   17830 nvidia.go:98] Could not initialize NVML: could not load NVML library\nNov 29 18:44:35 suzlab1080-001 kubelet[17830]: I1129 18:44:35.783773   17830 nvidia.go:59] Starting goroutine to initialize NVML\nI have set $LD_LIBRARY_PATH=/usr/lib64/nvidia, where does locate the NVML library mentioned before.\nAny clues or suggestion?\n. BTW, for the cadivsor running in container, which has been proved to run, I can only find container_accelerator_memory_total_bytes and container_accelerator_memory_used_bytes metrics\nshould any other metrics which reflect gpu-utilization be provided?. ",
    "tn-osimis": "\n@axiaoxin It works, but will reproduce whenever I reboot the machine, any workaround?\n\nThis is what we do:\n```\n$ tree cadvisor-1843-workaround/\ncadvisor-1843-workaround/\n\u251c\u2500\u2500 add-symlink.service\n\u251c\u2500\u2500 add-symlink.sh\n\u2514\u2500\u2500 install.sh\n$ cat cadvisor-1843-workaround/add-symlink.service \n[Unit]\nDescription=cAdvisor #1843 workaround\n[Service]\nType=oneshot\nExecStart=/usr/local/bin/cadvisor-1843-add-symlink\n[Install]\nWantedBy=multi-user.target\n$ cat cadvisor-1843-workaround/add-symlink.sh \n!/usr/bin/env bash\nhttps://github.com/google/cadvisor/issues/1843\nset -o errexit\nset -o xtrace\nmount --options=remount,rw /sys/fs/cgroup\nln --symbolic /sys/fs/cgroup/{\"cpu,cpuacct\",\"cpuacct,cpu\"}\n$ cat cadvisor-1843-workaround/install.sh \n!/usr/bin/env bash\nset -o errexit\nset -o xtrace\nln add-symlink.sh /usr/local/bin/cadvisor-1843-add-symlink\nln add-symlink.service /etc/systemd/system/cadvisor-1843-add-symlink.service\nsystemctl enable cadvisor-1843-add-symlink\nsystemctl start cadvisor-1843-add-symlink\nsystemctl status cadvisor-1843-add-symlink\n```. Confirming:\n```\n(. /etc/os-release && echo $PRETTY_NAME)\nCentOS Linux 7 (Core)\ndocker-compose run --rm cadvisor --version\ncAdvisor version v0.28.3 (1e567c2)\ndocker-compose run --rm cadvisor\nI0202 14:32:55.337212       1 storagedriver.go:50] Caching stats in memory for 2m0s\nI0202 14:32:55.338005       1 manager.go:151] cAdvisor running in container: \"/sys/fs/cgroup/cpuacct,cpu\"\nI0202 14:32:55.341946       1 fs.go:139] Filesystem UUIDs: map[203575e2-7c86-408b-b2e0-59df18bba2fb:/dev/sda1 2fbf3bea-7fde-4f32-8ac9-e9402a07da5d:/dev/sdc 390dbbea-ae34-44e6-a0fe-1b0c8a061827:/dev/sdb1 bf383770-7408-48df-b204-d408f67e439b:/dev/sda2]\nI0202 14:32:55.341976       1 fs.go:140] Filesystem partitions: map[/dev/sdb1:{mountpoint:/rootfs/mnt/resource major:8 minor:17 fsType:ext4 blockSize:0} shm:{mountpoint:/dev/shm major:0 minor:123 fsType:tmpfs blockSize:0} tmpfs:{mountpoint:/dev major:0 minor:127 fsType:tmpfs blockSize:0} /dev/sda2:{mountpoint:/var/lib/docker major:8 minor:2 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/rootfs/boot major:8 minor:1 fsType:xfs blockSize:0}]\nI0202 14:32:55.346862       1 manager.go:225] Machine: {NumCores:4 CpuFrequency:2394447 MemoryCapacity:16809521152 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] MachineID:d1085630399a48c6b29cf2e1de0eb5f4 SystemUUID:9EE4C12D-E3F9-114B-A81E-1D9E29FFBFAB BootID:bce6d330-b70d-4534-9ef8-6c9702c66abe Filesystems:[{Device:/dev/sdb1 DeviceMajor:8 DeviceMinor:17 Capacity:33685192704 Type:vfs Inodes:2097152 HasInodes:true} {Device:shm DeviceMajor:0 DeviceMinor:123 Capacity:67108864 Type:vfs Inodes:2051943 HasInodes:true} {Device:overlay DeviceMajor:0 DeviceMinor:118 Capacity:31671447552 Type:vfs Inodes:15472128 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:127 Capacity:8404758528 Type:vfs Inodes:2051943 HasInodes:true} {Device:/dev/sda2 DeviceMajor:8 DeviceMinor:2 Capacity:31671447552 Type:vfs Inodes:15472128 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520785920 Type:vfs Inodes:256000 HasInodes:true}] DiskMap:map[8:16:{Name:sdb Major:8 Minor:16 Size:34359738368 Scheduler:deadline} 8:32:{Name:sdc Major:8 Minor:32 Size:139586437120 Scheduler:deadline} 2:0:{Name:fd0 Major:2 Minor:0 Size:4096 Scheduler:deadline} 8:0:{Name:sda Major:8 Minor:0 Size:32212254720 Scheduler:deadline}] NetworkDevices:[{Name:br-bbac878bbbfb MacAddress:02:42:ec:de:0b:71 Speed:0 Mtu:1500} {Name:eth0 MacAddress:00:0d:3a:b3:75:01 Speed:40000 Mtu:1500}] Topology:[{Id:0 Memory:17179402240 Cores:[{Id:0 Threads:[0 1] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2} {Size:31457280 Type:Unified Level:3}]} {Id:1 Threads:[2 3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2} {Size:31457280 Type:Unified Level:3}]}] Caches:[]}] CloudProvider:Azure InstanceType:Unknown InstanceID:9EE4C12D-E3F9-114B-A81E-1D9E29FFBFAB}\nI0202 14:32:55.347416       1 manager.go:231] Version: {KernelVersion:3.10.0-693.17.1.el7.x86_64 ContainerOsVersion:Alpine Linux v3.4 DockerVersion:Unknown DockerAPIVersion:Unknown CadvisorVersion:v0.28.3 CadvisorRevision:1e567c2}\nI0202 14:32:57.349385       1 factory.go:54] Registering systemd factory\nI0202 14:32:57.350048       1 factory.go:86] Registering Raw factory\nI0202 14:32:57.350405       1 manager.go:1178] Started watching for new ooms in manager\nW0202 14:32:57.350432       1 manager.go:313] Could not configure a source for OOM detection, disabling OOM events: open /dev/kmsg: no such file or directory\nI0202 14:32:57.353222       1 manager.go:329] Starting recovery of all containers\nI0202 14:32:57.424192       1 manager.go:334] Recovery completed\nF0202 14:32:57.466754       1 cadvisor.go:156] Failed to start container manager: inotify_add_watch /sys/fs/cgroup/cpuacct,cpu: no such file or directory\ngrep --after-context=8 cadvisor:$ docker-compose.yml\ncadvisor:\n    image: google/cadvisor:v0.28.3\n    volumes:\n        - /:/rootfs:ro\n        - /var/run:/var/run:ro\n        - /sys:/sys:ro\n        - /var/lib/docker/:/var/lib/docker:ro\n        - /dev/disk:/dev/disk:ro\n    restart: unless-stopped\n\ndate\nFri Feb  2 14:35:31 UTC 2018\nuname --kernel-release\n3.10.0-693.17.1.el7.x86_64\n```\nAfter removing /sys bindmount:\ncadvisor_1             | I0202 14:50:32.674041       1 storagedriver.go:50] Caching stats in memory for 2m0s\ncadvisor_1             | I0202 14:50:32.674871       1 manager.go:151] cAdvisor running in container: \"/sys/fs/cgroup/cpuacct,cpu\"\ncadvisor_1             | I0202 14:50:32.679600       1 fs.go:139] Filesystem UUIDs: map[bf383770-7408-48df-b204-d408f67e439b:/dev/sda2 203575e2-7c86-408b-b2e0-59df18bba2fb:/dev/sda1 2fbf3bea-7fde-4f32-8ac9-e9402a07da5d:/dev/sdc 390dbbea-ae34-44e6-a0fe-1b0c8a061827:/dev/sdb1]\ncadvisor_1             | I0202 14:50:32.679652       1 fs.go:140] Filesystem partitions: map[/dev/sdb1:{mountpoint:/rootfs/mnt/resource major:8 minor:17 fsType:ext4 blockSize:0} shm:{mountpoint:/dev/shm major:0 minor:123 fsType:tmpfs blockSize:0} tmpfs:{mountpoint:/dev major:0 minor:127 fsType:tmpfs blockSize:0} /dev/sda2:{mountpoint:/var/lib/docker major:8 minor:2 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/rootfs/boot major:8 minor:1 fsType:xfs blockSize:0}]\ncadvisor_1             | I0202 14:50:32.683995       1 manager.go:225] Machine: {NumCores:4 CpuFrequency:2394447 MemoryCapacity:16809521152 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] MachineID:d1085630399a48c6b29cf2e1de0eb5f4 SystemUUID:9EE4C12D-E3F9-114B-A81E-1D9E29FFBFAB BootID:bce6d330-b70d-4534-9ef8-6c9702c66abe Filesystems:[{Device:overlay DeviceMajor:0 DeviceMinor:118 Capacity:31671447552 Type:vfs Inodes:15472128 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:127 Capacity:8404758528 Type:vfs Inodes:2051943 HasInodes:true} {Device:/dev/sda2 DeviceMajor:8 DeviceMinor:2 Capacity:31671447552 Type:vfs Inodes:15472128 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520785920 Type:vfs Inodes:256000 HasInodes:true} {Device:/dev/sdb1 DeviceMajor:8 DeviceMinor:17 Capacity:33685192704 Type:vfs Inodes:2097152 HasInodes:true} {Device:shm DeviceMajor:0 DeviceMinor:123 Capacity:67108864 Type:vfs Inodes:2051943 HasInodes:true}] DiskMap:map[2:0:{Name:fd0 Major:2 Minor:0 Size:4096 Scheduler:deadline} 8:0:{Name:sda Major:8 Minor:0 Size:32212254720 Scheduler:deadline} 8:16:{Name:sdb Major:8 Minor:16 Size:34359738368 Scheduler:deadline} 8:32:{Name:sdc Major:8 Minor:32 Size:139586437120 Scheduler:deadline}] NetworkDevices:[{Name:eth0 MacAddress:02:42:ac:12:00:0b Speed:10000 Mtu:1500}] Topology:[{Id:0 Memory:17179402240 Cores:[{Id:0 Threads:[0 1] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2} {Size:31457280 Type:Unified Level:3}]} {Id:1 Threads:[2 3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2} {Size:31457280 Type:Unified Level:3}]}] Caches:[]}] CloudProvider:Azure InstanceType:Unknown InstanceID:9EE4C12D-E3F9-114B-A81E-1D9E29FFBFAB}\ncadvisor_1             | I0202 14:50:32.684572       1 manager.go:231] Version: {KernelVersion:3.10.0-693.17.1.el7.x86_64 ContainerOsVersion:Alpine Linux v3.4 DockerVersion:Unknown DockerAPIVersion:Unknown CadvisorVersion:v0.28.3 CadvisorRevision:1e567c2}\ncadvisor_1             | I0202 14:50:34.685916       1 factory.go:54] Registering systemd factory\ncadvisor_1             | I0202 14:50:34.686479       1 factory.go:86] Registering Raw factory\ncadvisor_1             | I0202 14:50:34.687028       1 manager.go:1178] Started watching for new ooms in manager\ncadvisor_1             | W0202 14:50:34.687068       1 manager.go:313] Could not configure a source for OOM detection, disabling OOM events: open /dev/kmsg: no such file or directory\ncadvisor_1             | I0202 14:50:34.689742       1 manager.go:329] Starting recovery of all containers\ncadvisor_1             | I0202 14:50:34.691993       1 manager.go:334] Recovery completed\ncadvisor_1             | I0202 14:50:34.694875       1 cadvisor.go:162] Starting cAdvisor version: v0.28.3-1e567c2 on port 8080\nThe message \"cAdvisor running in container: \"/sys/fs/cgroup/cpuacct,cpu\"\" may or may not suggest the kernel is still exposing the necessary data within the container, and that bindmounting sys may or may not be necessary to get all features.. I could be wrong but this might mean that if the engine never replies for some reason cadvisor will hang indefinitely without giving a clue as to what it's trying to do in the logs.\nIt's a very welcome fix for #1866 and should probably be merged; though as a long-term approach I suggest a higher default timeout at startup (e.g. 30s or 60s) as well as the addition of a setting for configuring a non-default value.\n(Otherwise, maybe at least add log output before attempting to connect so that ops can more easily figure out what's blocking the startup process in this scenario.)\nWDYT?. If there is a generic library implementing an exponential backoff algorithm (or whatever else) somewhere in your stack already, that's probably what you want (and the logical conclusion to your thought). Otherwise indeed a basic fixed-interval retry approach usually works better than just waiting indefinitely for cases where the state machine of the peer (i.e. Docker engine) is a little messed up.\nFWIW (opinion of a random guy on the Internet) I agree with retrying indefinitely (as long as attempts are logged).. ",
    "govint": "Suggest a /volume endpoint, persistent volume metrics for a given container name is returned. Am making changes to support this and will post a PR soon.\n. Addresses #1445 \n. @k8s-bot ok to test\n. PR #1545 supersedes this one and is on top of the current code level.. Let me push an update patch if its out of synch with the master.\nOn Sun, Jan 8, 2017 at 5:13 AM, Gediminas \u0160edbaras <notifications@github.com\n\nwrote:\nany progress on this?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1545#issuecomment-271118308, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/APHseBW4AHDG4I0xdNHKmm2RX1aDuOKbks5rQCM0gaJpZM4K4Mh7\n.\n. \n",
    "FlyingShit-XinHuang": "I signed it!\n. ",
    "SamSaffron": "Note @dashpole on one of my servers I am seeing a payload of 27.3M of payload mostly due to metrics looking like this:\ncontainer_cpu_load_average_10s{container_label_build_date=\"\",container_label_license=\"\",container_label_maintainer=\"\",container_label_name=\"\",container_label_org_discourse_puppet_command_hash=\"\",container_label_org_discourse_service_1514=\"\",container_label_org_discourse_service_5044=\"\",container_label_org_discourse_service_5150=\"\",container_label_org_discourse_service_5151=\"\",container_label_org_discourse_service_5432=\"\",container_label_org_discourse_service_6432=\"\",container_label_org_discourse_service_8053=\"\",container_label_org_discourse_service_8080=\"\",container_label_org_discourse_service_8080_tags=\"\",container_label_org_discourse_service_9113=\"\",container_label_org_discourse_service_9113_tags=\"\",container_label_org_discourse_service_9127=\"\",container_label_org_discourse_service_9127_tags=\"\",container_label_org_discourse_service_9304=\"\",container_label_org_discourse_service_9304_tags=\"\",container_label_org_discourse_service_9600=\"\",container_label_org_discourse_service_9696=\"\",container_label_org_discourse_service_9696_tags=\"\",container_label_org_discourse_service_9797=\"\",container_label_org_discourse_service_9797_tags=\"\",container_label_org_discourse_service__logstash_beats_instance=\"\",container_label_org_discourse_service__logstash_beats_port=\"\",container_label_org_discourse_service__logstash_stats_instance=\"\",container_label_org_discourse_service__logstash_stats_port=\"\",container_label_org_discourse_service__prom_exp_instance=\"\",container_label_org_discourse_service__prom_exp_port=\"\",container_label_restartcount=\"\",container_label_vendor=\"\",id=\"/\",image=\"\",name=\"\"} 0\nProfiling shows that about 27% of our CPU time is in labelPairsToText ... our cadvisor is pegged at 60% of one core ... if we disable collection of metrics from prometheus it goes down to 6% of one core. This machine has 180 containers . I think a very very important note to anyone coming across this ticket @dashpole is that now we have:\n-store_container_labels=false \nThis is the graphical impact of what kind of a change this made to one of our servers. \n\nIt also halved CPU usage on prometheus. \nI think it is probably correct to make a breaking change here for the greater good and defaulting store_container_labels to false. . @bmerry curious if you have made any progress debugging this since July? . I did realize something... should we not be de-duplicating this string? \nhttps://github.com//blob/b36e6fb63ac7099a420139a53a411d70c9cc2553/container/libcontainer/helpers.go#L133-L136. ",
    "vladgolubev": "I signed it!\n. ",
    "fzu-huang": "use dd to make some disk IO.Then check it again.    I met this situation before..\n. @googlebot I signed it!. @derekwaynecarr @dashpole Do you have any idea about why it happened? I saw so much log but can not find any process match the Pids.. I run a goroutine in kubelet after it call cadvisor's manager.Start(), It will timevally init a new fsInfo by calling NewFsInfo ,then compare this fsInfo and old one, if changed, stop the manager,clear old global factories(ClearContainerHandlerFactories()),finally, init a new manager and Start it .\nI think it is not a good way to solve this problem but easy enough. And I think that  this problem should be solved by cadvisor, but not the upper componement(such as kubernetes).. @dashpole  it blocked here, what should I do for this pr?. /cc @dashpole . @dashpole  3q~  have rebased.. @dashpole  I create a new PR for this one . Refresh my fork project and re-commit. Shall I do anything else?. @dashpole  Can I know what the test-case is?  I tried to run with\nGORACE=\"halt_on_error=1\" ./cadvisor --docker_env_metadata_whitelist=TEST_VAR --v=4 --logtostderr &> \"$log_file\"\nand go test integration/tests/api/* ; go test integeration/test/healthz/* \nEverything looks ok:\n````\nroot@huangdev:~/mygo/src/github.com/google/cadvisor# go test integration/tests/api/\nok      command-line-arguments  25.424s\nroot@huangdev:~/mygo/src/github.com/google/cadvisor# go test integration/tests/healthz/\nok      command-line-arguments  0.007s\n`. I have used `-race` when I built. Follow your ways, I tried `GO_FLAGS=\"-race\" ./build/prow_e2e.sh`  and it exit at this part of `build/assets.sh` :\nif [[ ! -z \"$(git diff --name-only pages)\" ]]; then\n  echo \"Found changes to UI assets:\"\n  git diff --name-only pages\n  echo \"Run: make assets FORCE=true\"\n  exit 1\nfi\nI still did the rest steps:  \n`make all`:\n\n\nvetting code\nchecking go formatting\nchecking file boilerplate\nbuilding assets\n++ dirname ./build/assets.sh\n+ GIT_ROOT=./build/..\n+ ASSETS_INPUT_DIRS='./build/../pages/assets/js/... ./build/../pages/assets/styles/...'\n+ ASSETS_OUTPUT_PATH=./build/../pages/static/assets.go\n+ ASSETS_PACKAGE=static\n+ TEMPLATES_INPUT_DIRS=./build/../pages/assets/html/...\n+ TEMPLATES_OUTPUT_PATH=./build/../pages/templates.go\n+ TEMPLATES_PACKAGE=pages\n+ FORCE=\n+ go get -u github.com/jteeuwen/go-bindata/...\n+ for f in '$GIT_ROOT/pages/assets/js/' '$GIT_ROOT/pages/assets/styles/'\n+ '[' '' == true ']'\n+ '[' ./build/../pages/assets/js/bootstrap-4.0.0-beta.2.min.js -nt ./build/../pages/static/assets.go -o '!' -e ./build/../pages/static/assets.go ']'\n+ for f in '$GIT_ROOT/pages/assets/js/' '$GIT_ROOT/pages/assets/styles/'\n+ '[' '' == true ']'\n+ '[' ./build/../pages/assets/js/containers.js -nt ./build/../pages/static/assets.go -o '!' -e ./build/../pages/static/assets.go ']'\n+ for f in '$GIT_ROOT/pages/assets/js/' '$GIT_ROOT/pages/assets/styles/'\n+ '[' '' == true ']'\n+ '[' ./build/../pages/assets/js/gcharts.js -nt ./build/../pages/static/assets.go -o '!' -e ./build/../pages/static/assets.go ']'\n+ for f in '$GIT_ROOT/pages/assets/js/' '$GIT_ROOT/pages/assets/styles/'\n+ '[' '' == true ']'\n+ '[' ./build/../pages/assets/js/google-jsapi.js -nt ./build/../pages/static/assets.go -o '!' -e ./build/../pages/static/assets.go ']'\n+ for f in '$GIT_ROOT/pages/assets/js/' '$GIT_ROOT/pages/assets/styles/'\n+ '[' '' == true ']'\n+ '[' ./build/../pages/assets/js/jquery-3.0.0.min.js -nt ./build/../pages/static/assets.go -o '!' -e ./build/../pages/static/assets.go ']'\n+ for f in '$GIT_ROOT/pages/assets/js/' '$GIT_ROOT/pages/assets/styles/'\n+ '[' '' == true ']'\n+ '[' ./build/../pages/assets/js/popper.min.js -nt ./build/../pages/static/assets.go -o '!' -e ./build/../pages/static/assets.go ']'\n+ for f in '$GIT_ROOT/pages/assets/js/' '$GIT_ROOT/pages/assets/styles/'\n+ '[' '' == true ']'\n+ '[' ./build/../pages/assets/styles/bootstrap-4.0.0-beta.2.min.css -nt ./build/../pages/static/assets.go -o '!' -e ./build/../pages/static/assets.go ']'\n+ for f in '$GIT_ROOT/pages/assets/js/' '$GIT_ROOT/pages/assets/styles/'\n+ '[' '' == true ']'\n+ '[' ./build/../pages/assets/styles/bootstrap-theme-3.1.1.min.css -nt ./build/../pages/static/assets.go -o '!' -e ./build/../pages/static/assets.go ']'\n+ for f in '$GIT_ROOT/pages/assets/js/' '$GIT_ROOT/pages/assets/styles/'\n+ '[' '' == true ']'\n+ '[' ./build/../pages/assets/styles/containers.css -nt ./build/../pages/static/assets.go -o '!' -e ./build/../pages/static/assets.go ']'\n+ for f in '$GIT_ROOT/pages/assets/html/*'\n+ '[' '' == true ']'\n+ '[' ./build/../pages/assets/html/containers.html -nt ./build/../pages/templates.go -o '!' -e ./build/../pages/templates.go ']'\n+ exit 0\nbuilding binaries\nbuilding cadvisor\nrunning tests\nok      github.com/google/cadvisor  1.664s\nok      github.com/google/cadvisor/accelerators 1.026s\nok      github.com/google/cadvisor/api  1.064s\n?       github.com/google/cadvisor/cache    [no test files]\nok      github.com/google/cadvisor/cache/memory 1.022s\nok      github.com/google/cadvisor/client   1.095s\n?       github.com/google/cadvisor/client/clientexample [no test files]\nok      github.com/google/cadvisor/client/v2    1.053s\nok      github.com/google/cadvisor/collector    1.053s\nok      github.com/google/cadvisor/container    1.029s\nok      github.com/google/cadvisor/container/common 1.016s\nok      github.com/google/cadvisor/container/containerd 1.039s\nok      github.com/google/cadvisor/container/crio   1.025s\nok      github.com/google/cadvisor/container/docker 1.051s\nok      github.com/google/cadvisor/container/libcontainer   1.018s\nok      github.com/google/cadvisor/container/raw    1.042s\nok      github.com/google/cadvisor/container/rkt    1.037s\n?       github.com/google/cadvisor/container/systemd    [no test files]\n?       github.com/google/cadvisor/container/testing    [no test files]\nok      github.com/google/cadvisor/devicemapper 1.013s\n?       github.com/google/cadvisor/devicemapper/fake    [no test files]\nok      github.com/google/cadvisor/events   1.018s\nok      github.com/google/cadvisor/fs   1.195s\n?       github.com/google/cadvisor/healthz  [no test files]\n?       github.com/google/cadvisor/http [no test files]\n?       github.com/google/cadvisor/http/mux [no test files]\nok      github.com/google/cadvisor/info/v1  1.020s\n?       github.com/google/cadvisor/info/v1/test [no test files]\nok      github.com/google/cadvisor/info/v2  1.016s\n?       github.com/google/cadvisor/integration/framework    [no test files]\n?       github.com/google/cadvisor/integration/runner   [no test files]\nok      github.com/google/cadvisor/integration/tests/api    1.052s\nok      github.com/google/cadvisor/integration/tests/healthz    1.041s\nok      github.com/google/cadvisor/machine  1.032s\nok      github.com/google/cadvisor/manager  1.091s\n?       github.com/google/cadvisor/manager/watcher  [no test files]\n?       github.com/google/cadvisor/manager/watcher/raw  [no test files]\n?       github.com/google/cadvisor/manager/watcher/rkt  [no test files]\nok      github.com/google/cadvisor/metrics  1.097s\n?       github.com/google/cadvisor/pages    [no test files]\n?       github.com/google/cadvisor/pages/static [no test files]\n?       github.com/google/cadvisor/storage  [no test files]\n?       github.com/google/cadvisor/storage/bigquery [no test files]\n?       github.com/google/cadvisor/storage/bigquery/client  [no test files]\n?       github.com/google/cadvisor/storage/bigquery/client/example  [no test files]\n?       github.com/google/cadvisor/storage/elasticsearch    [no test files]\n?       github.com/google/cadvisor/storage/influxdb [no test files]\n?       github.com/google/cadvisor/storage/kafka    [no test files]\n?       github.com/google/cadvisor/storage/redis    [no test files]\n?       github.com/google/cadvisor/storage/statsd   [no test files]\n?       github.com/google/cadvisor/storage/statsd/client    [no test files]\n?       github.com/google/cadvisor/storage/stdout   [no test files]\n?       github.com/google/cadvisor/storage/test [no test files]\nok      github.com/google/cadvisor/summary  1.025s\nok      github.com/google/cadvisor/utils    1.028s\n?       github.com/google/cadvisor/utils/cloudinfo  [no test files]\n?       github.com/google/cadvisor/utils/container  [no test files]\n?       github.com/google/cadvisor/utils/cpuload    [no test files]\n?       github.com/google/cadvisor/utils/cpuload/netlink    [no test files]\n?       github.com/google/cadvisor/utils/cpuload/netlink/example    [no test files]\n?       github.com/google/cadvisor/utils/docker [no test files]\nok      github.com/google/cadvisor/utils/oomparser  1.029s\n?       github.com/google/cadvisor/utils/oomparser/oomexample   [no test files]\n?       github.com/google/cadvisor/utils/procfs [no test files]\n?       github.com/google/cadvisor/utils/sysfs  [no test files]\n?       github.com/google/cadvisor/utils/sysfs/fakesysfs    [no test files]\nok      github.com/google/cadvisor/utils/sysinfo    1.017s\nok      github.com/google/cadvisor/utils/tail   1.017s\nok      github.com/google/cadvisor/validate 1.054s\n?       github.com/google/cadvisor/version  [no test files]\n?       github.com/google/cadvisor/zfs  [no test files]\n```\n\n\nand ./build/integration.sh : \n````\n\n\nstarting cAdvisor locally\nWaiting for cAdvisor to start ...\nWaiting for cAdvisor to start ...\nWaiting for cAdvisor to start ...\nrunning integration tests against local cAdvisor\nPASS\nPASS\nstopping cAdvisor\n````\n\n\nCannot find anything wrong.  Did I miss something?. @dashpole  \n\nI would either just use the failure from the test framework we have here, or try and get prow_e2e.sh to work locally for you \n\nSo what could I do for this PR ? . @dashpole   Sorry for bothering you again, I have tried a successful commit and it passed on e2e-test   , but when I rebase my code and tried again, It failed again at node-test. Do you know why ?.  @dashpole  cc . /retest. /retest. /retest. I read the code roughly and I found that we init  fs data by fs/fs.go: NewFsInfo().And we put this fsInfo into a new factory when we called manager.Start(). If we run two manager with a global factories array, the handlers of new manager's factory won't be called(container/facotry.go)\n```\n// Register a ContainerHandlerFactory. These should be registered from least general to most general\n// as they will be asked in order whether they can handle a particular container.\nfunc RegisterContainerHandlerFactory(factory ContainerHandlerFactory, watchTypes []watcher.ContainerWatchSource) {\n    factoriesLock.Lock()\n    defer factoriesLock.Unlock()\nfor _, watchType := range watchTypes {\n    factories[watchType] = append(factories[watchType], factory)\n}\n\n}\n```\n// Create the ContainerHandler with the first factory that supports it.\n    for _, factory := range factories[watchType] {\n        canHandle, canAccept, err := factory.CanHandleAndAccept(name)\n        if err != nil {\n            glog.V(4).Infof(\"Error trying to work out if we can handle %s: %v\", name, err)\n        }\n        if canHandle {\n            if !canAccept {\n                glog.V(3).Infof(\"Factory %q can handle container %q, but ignoring.\", factory, name)\n                return nil, false, nil\n            }\n            glog.V(3).Infof(\"Using factory %q for container %q\", factory, name)\n            handle, err := factory.NewContainerHandler(name, inHostNamespace)\n            return handle, canAccept, err\n        } else {\n            glog.V(4).Infof(\"Factory %q was unable to handle container %q\", factory, name)\n        }\n    }\nI think maybe we should provide each manager a factories array. However, leave it up to the caller to call container.ClearContainerHandlerFactories() is also a solution. So ignore this fix. : ). OK, sorry for that ,  should I PR again? . At first I used inotify to watch /proc/self/mountinfo but when I attach a disk into my machine and mount to a directory, Nothing was watched, even though the file is quitely changed. \nSame result with watching /proc/mounts and /proc/self/mounts by inotify\nAfter that I found two topics in SOF: \nhttps://stackoverflow.com/questions/1113176/how-could-i-detect-when-a-directory-is-mounted-with-inotify/24671215#24671215\nhttps://stackoverflow.com/questions/5070801/monitoring-mount-point-changes-via-proc-mounts/5072579#5072579   , and tried it with golang.\nOr do you know some better ways to do capture mount/umount event?\n. I have some questions:\n1.If  reconstructing machineInfo each time , what's the interval of reconstructing machineInfo? It cann't be too soon, or it will waste cpu and memory.  What about  30s or 1 minute? \n2. I would move  inHostNamespace  into the struct manager , is that reasonable? How do you think?. Still doesn't work. \nI init a FSWatcher by:\nfiles:=\"/proc/self/mountinfo\"\n        files1 := \"/proc/mounts\"\n        files3 := \"/proc/self/mounts\"\n        watcher , _ := newFSWatcher(files, files1, files3, file2)\nand capture all Events and Errors:\nfor {\n        select {\n                case event := <-watcher.Events:\n                        fmt.Println(event.Name,  event.Op)\n                case err := <-watcher.Errors:\n                        fmt.Println(\"inotify: %s\", err)\n        }\n}\nStill print nothing when i mount /dev/vda1 /test or umount /test. ",
    "workhardcc": "@fzu-huang You're right. dd if=/dev/zero of=/home/cc bs=1M count=3000 makes it work. \nI have a another question: why I dd some data in container's disk  but there is no change in blkio.throttle.io_service_bytes \n. @derekwaynecarr sorry for late reply. I didn't resolve this problem, now I'm almost give up get container's diskio metrics.\n. ",
    "dims": "@timstclair no worries, let me drop this :)\n. LGTM Thanks @timstclair \n. Hmm, v1.12.3 does not have the stuff needed (just the master branch) am looking at https://github.com/docker/docker/tree/v1.12.3/api/types\n. See https://github.com/kubernetes/kubernetes/issues/44059 for a bit more context. @dashpole - tested by hand - http://paste.openstack.org/show/611784/ as i don't have access to that centos based environment mentioned in that kubernetes issue. @pineking neither one ... we will need this to get in first, then propose a PR to pull in a fresh cadvisor (to fix master) and then backport it to 1.9 branch. @dashpole yep, i can do that\n. /test pull-cadvisor-e2e. @dashpole something wrong with the build? local make looks file (http://paste.openstack.org/show/636661/). Thanks @dashpole . :). /test pull-cadvisor-e2e. /test pull-cadvisor-e2e. ack @dashpole . @dashpole rebased! cross my fingers. hmm, this may not be needed.. /assign @dashpole . @dashpole i tried a few public latest images and they all seemed to have ionice \ndocker run -it ubuntu bash\ndocker run -it busybox sh\ndocker run -it fedora sh\ndocker run -it debian sh\nAlso tried our hyperkube\ndocker run -it k8s.gcr.io/hyperkube-amd64:v1.10.2 sh\n. while looking at the various issues filed, i did not see find causing issue, so may be we should wait to do that?. aha! so we could do at least \"nice\" on the find command\n. we recently added \"ionice\" to the \"du\" command, but deliberately leaving that out here. when we have evidence of io wait problems with find, we can add that. cool. Done! (added ionice). cc @sashankreddya @dashpole . @dashpole yep, just filing it for next round of updates or if someone complains :). related to https://github.com/kubernetes/kubernetes/issues/70264. cc @dashpole @sigma . +1 yes please let's do this.. /lgtm. /ok-to-test. ",
    "wmturner": "Can we please see some traction on this PR?  It is currently very frustrating using the non-official repo especially when some infrastructure is not in WF.. ",
    "ezeev": "Another thought that hasn't been discussed yet. I'm wondering if the new storage driver model should have some mechanism for controlling the interval at which AddStats is called that is decoupled from the --housekeeping_interval flag. As a user, is there a scenario where I may want cAdvisor to use dynamic housekeeping but only flush stats to my storage backend every minute for example? \nI achieved this in our storage driver by calculating the number of seconds between intervals and only flushing when a specified amount of time has passed. \nIs this something that should be baked into the design or should we assume users will use the --housekeeping_interval flag. If we were to add something like this as an option it could be a new flag. Something like --storage_flush_interval.\nWDYT?\n. We are seeing this issue occur for random users of cAdvisor out in the wild. Even on very common linux distros like Ubuntu 14.04 LTS. I can also confirm that adding -tags netgo to the build command fixes the problem.. ",
    "oopschen": "option 1 +1,\nThe message queue may be a good candidate, like rabbitmq. They are all language independent and do  have a good performance with push model. Some light weight network library zeromq, also referred as message queue may be an alternative. What's more, by using mq, add some more functions like publish-consumer, filter, etc. We could take advantage of them.\nWe also could implement it by customizing a protocol over tcp, it takes time and make the user spend more time implementing the receiver.. +1\nIt will be great to have es 5 support. Check pull request 1597, hope will merge soon.. @dashpole  There is two major release of elasticsearch: 2.x and 5.x, and there are not compatible. I think it is better to support two of them to offer a better user experience for developers.\nIt definitely possible to merge two major release ES storage, if it is required, should i open a new pull request or merge them in this pull request?. ",
    "eparis": "@timstclair I think yes. Which means maybe you get the cadvisor rebase after all   :)\n. I'm guessing he was worried about 2.x kernels?\n. I suggested search/replace  _ with -...\n. How about we turn all of these into:\nif !strings.Contains(kernelVersion, \".el7.\") || sem.EQ(version_3_10_0) {\n return ERROR\n}\n[check more RHEL version madness]\n(admittedly that might make it harder to extend for others, but how many people do the long term support like RHEL kernels and are likely to find/fix this?)\n. git tag -l --contains ed8b45a3679eb49069b094c0711b30833f27c734 v\\*\n. ",
    "nnordrum": "Assuming you're running in docker, you can do this too:\nsudo docker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --volume=/sys/fs/cgroup/cpu,cpuacct:/sys/fs/cgroup/cpuacct,cpu:rw \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  google/cadvisor:latest\nI just took the example from the front page and added this:\n--volume=/sys/fs/cgroup/cpu,cpuacct:/sys/fs/cgroup/cpuacct,cpu:rw \\\nWorks like a champ!\n. ",
    "mohamedhaleem": "@timstclair - how can i get the cAdvisor output? are you looking for the UI or the api output?\nhere is the api output\nhttps://gist.github.com/mohamedhaleem/4dbdc012b1c21507f5c8e36cc6a7d762\n. \"aliases\": [\n                    \"mesos-f3b1b03c-c900-43ac-b07c-697c555dc65e-S12.fc74c1b1-23c1-4c3c-a26f-4b74e2da6ded\",\n                    \"41606b8d7a54235d9886057c398a9b4ccba3e9360d41816b89595e88b2bc392f\"\n                ],\n. `\ndocker version\nClient:\n Version:      1.11.2\n API version:  1.23\n Go version:   go1.5.4\n Git commit:   b9f10c9\n Built:        Wed Jun  1 21:23:11 2016\n OS/Arch:      linux/amd64\nServer:\n Version:      1.11.2\n API version:  1.23\n Go version:   go1.5.4\n Git commit:   b9f10c9\n Built:        Wed Jun  1 21:23:11 2016\n OS/Arch:      linux/amd64\ndocker exec -i 59830455d9ad /usr/bin/cadvisor -version\ncAdvisor version 0.23.8 (5c5b2b8)\n`\n. @timstclair any more ideas?\n. thank you for looking into it @timothysc -those names are correct, we wanted a way to specify human readable name that we can then group the applications together.\n. Well - we are no longer using cadvisor and instead defer to using https://github.com/bobrik/collectd-docker\n\nMohamed\n\nFrom: mimmus notifications@github.com<mailto:notifications@github.com>\nReply-To: google/cadvisor reply@reply.github.com<mailto:reply@reply.github.com>\nDate: Thursday, February 16, 2017 at 9:44 AM\nTo: google/cadvisor cadvisor@noreply.github.com<mailto:cadvisor@noreply.github.com>\nCc: Local Administrator Mohamed_Haleem@Comcast.com<mailto:Mohamed_Haleem@Comcast.com>, Mention mention@noreply.github.com<mailto:mention@noreply.github.com>\nSubject: Re: [google/cadvisor] cAdvisor container names not captured when running cAdvisor in mesos (#1468)\nI am hitting the same thing and it would be really nice to use the image name or something like that.\n-\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://github.com/google/cadvisor/issues/1468#issuecomment-280348914, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AGx7om-OlWFHX88bbvs8dPpGAwyZKKR_ks5rdGC7gaJpZM4KA7pI.\n. ",
    "franklsm1": "@mohamedhaleem Did you figure out a work around to get a human readable name as the alias for mesos containers? I am hitting the same thing and it would be really nice to use the image name or something like that.. @mimmus I ended up adding a label field to the docker parameter section of marathon like so:\n\"parameters\": [\n      {\n        \"key\": \"label\",\n        \"value\": \"application=cadvisor\"\n      }\n    ]\nYou will have to add that block to each container you want to track and of course change the name. Sample query I use to get both the new label (application) and the container name:\nSELECT mean(\"value\") FROM \"cpu_usage_system\" WHERE  $timeFilter GROUP BY time($interval), \"container_name\",\"application\" fill(null)\nThat allowed me to query based on the label. The label will be the same for multiple instances which can be bad if you wanted each one separate (would be nice if there was a way to append an instance identifier), but that allowed me to at lease be able to tell which container I was looking at.. ",
    "mimmus": "I am hitting the same thing and it would be really nice to use the image name or something like that.. ",
    "giuseppecossu": "Thank you @franklsm1 it works!. ",
    "tangjiaxing669": "@derekwaynecarr I'm sorry\uff0c How can I solve this problem\uff1f\n- update the libcontainer dependency.\n- look at all cgroup mounts\nmaybe i am very stupid\nplease help\nI tried it, but the exception still exists.\n. @timstclair That is my question.\n. ",
    "fabMrc": "Also have an error . the docker image is not reachable although port 8080 is exposed. When I log into the container I can get the content http://localhost:8080 but not from the host\ngetsockopt: connection refused in docker logs\n. ",
    "blancoh": "Also seeing this using 10acre-ranch on Core Linux.\n2/13/2017 7:54:06 PMFlag --api-servers has been deprecated, Use --kubeconfig instead. Will be removed in a future version.\n2/13/2017 7:54:06 PMI0214 00:54:06.018650   32308 feature_gate.go:181] feature gates: map[]\n2/13/2017 7:54:06 PMI0214 00:54:06.140527   32308 docker.go:356] Connecting to docker on unix:///var/run/docker.sock\n2/13/2017 7:54:06 PMI0214 00:54:06.141512   32308 docker.go:376] Start docker client with request timeout=2m0s\n2/13/2017 7:54:06 PMI0214 00:54:06.155548   32308 manager.go:143] cAdvisor running in container: \"/docker/09b613c68222a76b216f45f8dbec055f50caa723a136df7c6cd15e616808ca1d\"\n2/13/2017 7:54:06 PMW0214 00:54:06.182879   32308 manager.go:151] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp 127.0.0.1:15441: getsockopt: connection refused\n2/13/2017 7:54:06 PMI0214 00:54:06.199632   32308 fs.go:117] Filesystem partitions: map[/dev/sda1:{mountpoint:/etc/cni major:8 minor:1 fsType:ext4 blockSize:0} none:{mountpoint:/ major:0 minor:52 fsType:aufs blockSize:0}]\n2/13/2017 7:54:06 PMI0214 00:54:06.203559   32308 manager.go:198] Machine: {NumCores:1 CpuFrequency:1098977 MemoryCapacity:1044209664 MachineID:b07a180a2c8547f7956e9a6f93a452a4 SystemUUID:C4D0495A-0000-0000-A526-D0E450717A8D BootID:0ec50f1a-9043-4758-910e-0862a64f7453 Filesystems:[{Device:/dev/sda1 Capacity:19195224064 Type:vfs Inodes:2436448 HasInodes:true} {Device:none Capacity:19195224064 Type:vfs Inodes:2436448 HasInodes:true}] DiskMap:map[8:0:{Name:sda Major:8 Minor:0 Size:20971520000 Scheduler:deadline} 251:0:{Name:zram0 Major:251 Minor:0 Size:203997184 Scheduler:none}] NetworkDevices:[{Name:dummy0 MacAddress:36:89:29:c2:d3:28 Speed:0 Mtu:1500} {Name:eth0 MacAddress:72:01:e7:8d:13:bd Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:0 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:4194304 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}\n2/13/2017 7:54:06 PMI0214 00:54:06.208938   32308 manager.go:204] Version: {KernelVersion:4.4.27-boot2docker ContainerOsVersion:Debian GNU/Linux 8 (jessie) DockerVersion:1.12.3 CadvisorVersion: CadvisorRevision:}\n2/13/2017 7:54:06 PMW0214 00:54:06.215900   32308 container_manager_linux.go:205] Running with swap on is not supported, please disable swap! This will be a fatal error by default starting in K8s v1.6! In the meantime, you can opt-in to making this a fatal error by enabling --experimental-fail-swap-on.\n2/13/2017 7:54:06 PMI0214 00:54:06.216636   32308 kubelet.go:252] Watching apiserver\n2/13/2017 7:54:06 PMW0214 00:54:06.224945   32308 kubelet_network.go:69] Hairpin mode set to \"promiscuous-bridge\" but kubenet is not enabled, falling back to \"hairpin-veth\"\n2/13/2017 7:54:06 PMI0214 00:54:06.225843   32308 kubelet.go:477] Hairpin mode set to \"hairpin-veth\"\n2/13/2017 7:54:06 PMI0214 00:54:06.272543   32308 docker_manager.go:257] Setting dockerRoot to /mnt/sda1/var/lib/docker\n2/13/2017 7:54:06 PMI0214 00:54:06.272879   32308 docker_manager.go:260] Setting cgroupDriver to cgroupfs\n2/13/2017 7:54:06 PMI0214 00:54:06.286342   32308 server.go:770] Started kubelet v1.5.0-115+611cbb22703182\n2/13/2017 7:54:06 PME0214 00:54:06.311630   32308 kubelet.go:1145] Image garbage collection failed: unable to find data for container /\n2/13/2017 7:54:06 PMI0214 00:54:06.312608   32308 kubelet_node_status.go:204] Setting node annotation to enable volume controller attach/detach\n2/13/2017 7:54:06 PMI0214 00:54:06.312851   32308 rancher.go:641] ExternalID [rs-host2]\n2/13/2017 7:54:06 PMI0214 00:54:06.313182   32308 rancher.go:648] InstanceID [rs-host2]\n2/13/2017 7:54:06 PMI0214 00:54:06.314155   32308 server.go:123] Starting to listen on 0.0.0.0:10250\n2/13/2017 7:54:06 PMI0214 00:54:06.415908   32308 rancher.go:648] InstanceID [rs-host2]\n2/13/2017 7:54:06 PMI0214 00:54:06.440233   32308 rancher.go:648] InstanceID [rs-host2]\n2/13/2017 7:54:06 PMI0214 00:54:06.475077   32308 kubelet_node_status.go:246] Adding node label from cloud provider: beta.kubernetes.io/instance-type=rancher\n2/13/2017 7:54:06 PMI0214 00:54:06.476373   32308 kubelet_node_status.go:257] Adding node label from cloud provider: failure-domain.beta.kubernetes.io/zone=FailureDomain1\n2/13/2017 7:54:06 PMI0214 00:54:06.476708   32308 kubelet_node_status.go:261] Adding node label from cloud provider: failure-domain.beta.kubernetes.io/region=Region1\n2/13/2017 7:54:06 PME0214 00:54:06.500240   32308 kubelet.go:1634] Failed to check if disk space is available for the runtime: failed to get fs info for \"runtime\": unable to find data for container /\n2/13/2017 7:54:06 PME0214 00:54:06.500883   32308 kubelet.go:1642] Failed to check if disk space is available on the root partition: failed to get fs info for \"root\": error trying to get filesystem Device for dir /var/lib/kubelet: err: could not find device with major: 0, minor: 15 in cached partitions map\n2/13/2017 7:54:06 PMI0214 00:54:06.506987   32308 fs_resource_analyzer.go:66] Starting FS ResourceAnalyzer\n2/13/2017 7:54:06 PMI0214 00:54:06.507882   32308 status_manager.go:129] Starting to sync pod status with apiserver\n2/13/2017 7:54:06 PMI0214 00:54:06.510982   32308 kubelet.go:1714] Starting kubelet main sync loop.\n2/13/2017 7:54:06 PMI0214 00:54:06.511715   32308 kubelet.go:1725] skipping pod synchronization - [container runtime is down]\n2/13/2017 7:54:06 PMI0214 00:54:06.508294   32308 volume_manager.go:242] Starting Kubelet Volume Manager\n2/13/2017 7:54:06 PMI0214 00:54:06.609259   32308 kubelet_node_status.go:204] Setting node annotation to enable volume controller attach/detach\n2/13/2017 7:54:06 PMI0214 00:54:06.610455   32308 rancher.go:641] ExternalID [rs-host2]\n2/13/2017 7:54:06 PMI0214 00:54:06.610775   32308 rancher.go:648] InstanceID [rs-host2]\n2/13/2017 7:54:06 PMI0214 00:54:06.637423   32308 rancher.go:648] InstanceID [rs-host2]\n2/13/2017 7:54:06 PMI0214 00:54:06.658984   32308 rancher.go:648] InstanceID [rs-host2]\n2/13/2017 7:54:06 PMI0214 00:54:06.676996   32308 factory.go:295] Registering Docker factory\nUnknown Date\n2/13/2017 7:54:06 PMW0214 00:54:06.677261   32308 manager.go:247] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp 127.0.0.1:15441: getsockopt: connection refused\n2/13/2017 7:54:06 PMI0214 00:54:06.677353   32308 factory.go:54] Registering systemd factory\n2/13/2017 7:54:06 PMI0214 00:54:06.679758   32308 factory.go:86] Registering Raw factory\n2/13/2017 7:54:06 PMI0214 00:54:06.682258   32308 manager.go:1106] Started watching for new ooms in manager\n2/13/2017 7:54:06 PMI0214 00:54:06.686403   32308 kubelet_node_status.go:246] Adding node label from cloud provider: beta.kubernetes.io/instance-type=rancher\n2/13/2017 7:54:06 PMI0214 00:54:06.686785   32308 kubelet_node_status.go:257] Adding node label from cloud provider: failure-domain.beta.kubernetes.io/zone=FailureDomain1\n2/13/2017 7:54:06 PMI0214 00:54:06.687101   32308 kubelet_node_status.go:261] Adding node label from cloud provider: failure-domain.beta.kubernetes.io/region=Region1\n2/13/2017 7:54:06 PMI0214 00:54:06.688448   32308 oomparser.go:185] oomparser using systemd\n2/13/2017 7:54:06 PMI0214 00:54:06.689606   32308 manager.go:288] Starting recovery of all containers\n2/13/2017 7:54:06 PMI0214 00:54:06.689919   32308 manager.go:293] Recovery completed\n2/13/2017 7:54:06 PMF0214 00:54:06.690084   32308 kubelet.go:1210] Failed to start cAdvisor inotify_add_watch /var/lib/docker/aufs/mnt/fcb38bdb43ffec21de47040b856bfeed3952be694fd9dffc7f4cfe360700eec0/sys/fs/cgroup/cpu: no such file or directory\nDisconnected. ",
    "jralmaraz": "Hi,\nI've seen the same kubelet issue when starting a kubernetes stack on Rancher within an AWS AMI.\nWas this fixed?\nCheers!\nW0324 03:14:36.151049   24540 manager.go:151] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp 127.0.0.1:15441: getsockopt: connection refused\nI0324 03:14:36.160518   24540 fs.go:117] Filesystem partitions: map[/dev/mapper/docker-202:1-395314-24d5625b839a1f50be37a765b3f443a8c1092cccbbeaaeea7a658081e49bcf68:{mountpoint:/ major:253 minor:20 fsType:xfs blockSize:0} /dev/xvda1:{mountpoint:/var/lib/docker major:202 minor:1 fsType:ext4 blockSize:0}]\nI0324 03:14:36.164720   24540 manager.go:198] Machine: {NumCores:1 CpuFrequency:2400072 MemoryCapacity:1043574784 MachineID:efee03ac51c6418889650dfa2a40350d SystemUUID:EC2EFC94-EC2C-5A77-F9C6-17B06C214A5B BootID:281d292b-9308-4d1c-83e0-51d58e7b4f79 Filesystems:[{Device:/dev/xvda1 Capacity:8318783488 Type:vfs Inodes:524288 HasInodes:true} {Device:/dev/mapper/docker-202:1-395314-24d5625b839a1f50be37a765b3f443a8c1092cccbbeaaeea7a658081e49bcf68 Capacity:10725883904 Type:vfs Inodes:10484736 HasInodes:true}] DiskMap:map[253:18:{Name:dm-18 Major:253 Minor:18 Size:10737418240 Scheduler:none} 253:20:{Name:dm-20 Major:253 Minor:20 Size:10737418240 Scheduler:none} 253:7:{Name:dm-7 Major:253 Minor:7 Size:10737418240 Scheduler:none} 253:9:{Name:dm-9 Major:253 Minor:9 Size:10737418240 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:10737418240 Scheduler:none} 253:11:{Name:dm-11 Major:253 Minor:11 Size:10737418240 Scheduler:none} 253:15:{Name:dm-15 Major:253 Minor:15 Size:10737418240 Scheduler:none} 253:16:{Name:dm-16 Major:253 Minor:16 Size:10737418240 Scheduler:none} 253:4:{Name:dm-4 Major:253 Minor:4 Size:10737418240 Scheduler:none} 253:5:{Name:dm-5 Major:253 Minor:5 Size:10737418240 Scheduler:none} 253:8:{Name:dm-8 Major:253 Minor:8 Size:10737418240 Scheduler:none} 202:0:{Name:xvda Major:202 Minor:0 Size:8589934592 Scheduler:noop} 253:0:{Name:dm-0 Major:253 Minor:0 Size:107374182400 Scheduler:none} 253:10:{Name:dm-10 Major:253 Minor:10 Size:10737418240 Scheduler:none} 253:12:{Name:dm-12 Major:253 Minor:12 Size:10737418240 Scheduler:none} 253:14:{Name:dm-14 Major:253 Minor:14 Size:10737418240 Scheduler:none} 253:17:{Name:dm-17 Major:253 Minor:17 Size:10737418240 Scheduler:none} 253:2:{Name:dm-2 Major:253 Minor:2 Size:10737418240 Scheduler:none} 253:6:{Name:dm-6 Major:253 Minor:6 Size:10737418240 Scheduler:none} 253:13:{Name:dm-13 Major:253 Minor:13 Size:10737418240 Scheduler:none} 253:19:{Name:dm-19 Major:253 Minor:19 Size:10737418240 Scheduler:none} 253:3:{Name:dm-3 Major:253 Minor:3 Size:10737418240 Scheduler:none} 202:16:{Name:xvdb Major:202 Minor:16 Size:21474836480 Scheduler:noop}] NetworkDevices:[{Name:eth0 MacAddress:0a:6e:7e:f7:5b:f7 Speed:0 Mtu:9001}] Topology:[{Id:0 Memory:1043574784 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:31457280 Type:Unified Level:3}]}] CloudProvider:AWS InstanceType:t2.micro InstanceID:i-040e5d8fb87e06c91}\nI0324 03:14:36.165299   24540 manager.go:204] Version: {KernelVersion:4.4.51-40.58.amzn1.x86_64 ContainerOsVersion:Debian GNU/Linux 8 (jessie) DockerVersion:1.12.6 CadvisorVersion: CadvisorRevision:}\nI0324 03:14:36.166856   24540 kubelet.go:252] Watching apiserver\nW0324 03:14:36.169796   24540 kubelet_network.go:69] Hairpin mode set to \"promiscuous-bridge\" but kubenet is not enabled, falling back to \"hairpin-veth\"\nI0324 03:14:36.172036   24540 kubelet.go:477] Hairpin mode set to \"hairpin-veth\"\nI0324 03:14:36.197273   24540 docker_manager.go:256] Setting dockerRoot to /var/lib/docker\nI0324 03:14:36.197312   24540 docker_manager.go:259] Setting cgroupDriver to cgroupfs\nI0324 03:14:36.224984   24540 server.go:770] Started kubelet v1.5.4-rancher1\nE0324 03:14:36.226924   24540 kubelet.go:1145] Image garbage collection failed: unable to find data for container /\nI0324 03:14:36.227117   24540 kubelet_node_status.go:204] Setting node annotation to enable volume controller attach/detach\nI0324 03:14:36.227140   24540 rancher.go:641] ExternalID [ip-172-31-23-241.ap-southeast-2.compute.internal]\nI0324 03:14:36.227151   24540 rancher.go:648] InstanceID [ip-172-31-23-241.ap-southeast-2.compute.internal]\nI0324 03:14:36.229489   24540 server.go:123] Starting to listen on 0.0.0.0:10250\nI0324 03:14:36.278986   24540 rancher.go:648] InstanceID [ip-172-31-23-241.ap-southeast-2.compute.internal]\nI0324 03:14:36.324146   24540 rancher.go:648] InstanceID [ip-172-31-23-241.ap-southeast-2.compute.internal]\nI0324 03:14:36.360564   24540 kubelet_node_status.go:246] Adding node label from cloud provider: beta.kubernetes.io/instance-type=rancher\nI0324 03:14:36.360618   24540 kubelet_node_status.go:257] Adding node label from cloud provider: failure-domain.beta.kubernetes.io/zone=FailureDomain1\nI0324 03:14:36.360637   24540 kubelet_node_status.go:261] Adding node label from cloud provider: failure-domain.beta.kubernetes.io/region=Region1\nE0324 03:14:36.398949   24540 kubelet.go:1634] Failed to check if disk space is available for the runtime: failed to get fs info for \"runtime\": unable to find data for container /\nE0324 03:14:36.398998   24540 kubelet.go:1642] Failed to check if disk space is available on the root partition: failed to get fs info for \"root\": unable to find data for container /\nI0324 03:14:36.400057   24540 fs_resource_analyzer.go:66] Starting FS ResourceAnalyzer\nI0324 03:14:36.400113   24540 status_manager.go:129] Starting to sync pod status with apiserver\nI0324 03:14:36.400132   24540 kubelet.go:1714] Starting kubelet main sync loop.\nI0324 03:14:36.400154   24540 kubelet.go:1725] skipping pod synchronization - [container runtime is down]\nI0324 03:14:36.401554   24540 volume_manager.go:242] Starting Kubelet Volume Manager\nE0324 03:14:36.416297   24540 factory.go:291] devicemapper filesystem stats will not be reported: unable to find thin_ls binary\nI0324 03:14:36.416338   24540 factory.go:295] Registering Docker factory\nW0324 03:14:36.416377   24540 manager.go:247] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp 127.0.0.1:15441: getsockopt: connection refused\nI0324 03:14:36.416415   24540 factory.go:54] Registering systemd factory\nI0324 03:14:36.416574   24540 factory.go:86] Registering Raw factory\nI0324 03:14:36.416727   24540 manager.go:1106] Started watching for new ooms in manager\nI0324 03:14:36.417897   24540 oomparser.go:185] oomparser using systemd\nI0324 03:14:36.418256   24540 manager.go:288] Starting recovery of all containers\nI0324 03:14:36.418329   24540 manager.go:293] Recovery completed\nF0324 03:14:36.418351   24540 kubelet.go:1210] Failed to start cAdvisor inotify_add_watch /sys/fs/cgroup/cpuacct: no such file or directory\n. ",
    "cyrossignol": "I encountered this issue when running a cAdvisor container on a host machine with a non-standard ps executable (BusyBox) that doesn't support an option passed by cAdvisor (in my case, stime). \nThis issue occurs because cAdvisor chroots into the rootfs of the host machine, that we mounted when running the container, before executing ps, so the output of ps depends on the result of the command available in the chroot environment. \nI worked around this issue by installing the procps package of utilities in the host OS, but this issue may be better solved by finding a more portable way for cAdvisor to pull process information, or at least by adding some way to configure the options/columns passed to ps.. ",
    "Gallaecio": "What configuration are you using that everything else works? Me, I get redirected to /containers as described in #1587.. ",
    "renanbr": "I'm facing the same problem. I've tryed latest, canary, v0.26.1, v0.25.0 and v0.24.1 using this configuration:\n```\nserver {\n    ...\nlocation ^~ /cadvisor/ {\n    proxy_pass http://cadvisor:8080/;\n    proxy_redirect off;\n    proxy_set_header Host $host;\n}\n\n...\n\n}\n```\nI'm not familiar with this project to make a PR, but I take this opportunity to share my experience as a end-user. I achieved make it works with these containers:\n\nphpmyadmin has the same problem, but it can be fixed through PMA_ABSOLUTE_URI\nredis-commander had the same problem, it was fixed since https://github.com/joeferner/redis-commander/commit/71b28043fca0cb54039dce12546e9615ba54de27\n. \n",
    "JamesKyburz": "@derekwaynecarr @timstclair  Great thanks!\n. ",
    "carlpett": "This also happens while running 0.24.1 standalone on Centos 7.1.1503, kernel 3.10.0-229.11.1.el7.x86_64. Works with 0.23.8.\n. Also 0.24.0-alpha1 and 0.24.0 have the same issue.\n. Interestingly, it works on 0.24.1 if I run cadvisor as a systemd service, but not if I run it from the commandline... In both cases, it is running as root, same cmdline (-logtostderr).\n. Ok, correction - it seems pretty random if it works or not. 30% of our (identical) machines it works, the rest fail with fatal error: unexpected signal during runtime execution. \nPossibly there is some race condition?\n. @timstclair You mean building with netgo? \n. I think I may be seeing this? The issue I'm having is that we seemingly at random do not get all the containers, or at least not all labels, on the /metric endpoint. As an example, here I'm grepping for the container_cpu_usage_seconds_total metric, and seeing if it has an image label:\n```\ncurl -s localhost:9190/metrics | grep -E 'container_cpu_usage_seconds_total.+image' | wc -l\n580\ncurl -s localhost:9190/metrics | grep -E 'container_cpu_usage_seconds_total.+image' | wc -l\n20\ncurl -s localhost:9190/metrics | grep -E 'container_cpu_usage_seconds_total.+image' | wc -l\n159\ncurl -s localhost:9190/metrics | grep -E 'container_cpu_usage_seconds_total.+image' | wc -l\n0\ncurl -s localhost:9190/metrics | grep -E 'container_cpu_usage_seconds_total.+image' | wc -l\n159\ncurl -s localhost:9190/metrics | grep -E 'container_cpu_usage_seconds_total.+image' | wc -l\n580\n```\n(These lines were in quick succession.)\nWe're running cadvisor as a systemd service (not in a container). Tried upgrading from 0.23.8 to 0.26.1, no difference. \nOS is CentOS Linux release 7.2.1511 (Core), docker 17.05.0-ce. We are not using Kubernetes.\n(I'm also having random SIGSEGVs on startup, but that seems unrelated). ",
    "rodrigomalara": "I'm having a similar issue for 2 days now. \ncAdvisor worked at first but after the second docker-compose down and then docker-compose up it stopped working.\nBelow some info about the environment\nroot@vlslpbstc74qat/opt/data #>lsb_release -a\nLSB Version:    :base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch\nDistributor ID: OracleServer\nDescription:    Oracle Linux Server release 6.8\nRelease:    6.8\nCodename:   n/a\nroot@vlslpbstc74qat/opt/data #>sestatus \nSELinux status:                 disabled\nroot@vlslpbstc74qat/opt/data #>docker --version\nDocker version 1.12.6, build 1512168\nroot@vlslpbstc74qat/opt/data #>uname -a\nLinux vlslpbstc74qat 4.1.12-37.6.3.el6uek.x86_64 #2 SMP Fri Aug 12 15:36:40 PDT 2016 x86_64 x86_64 x86_64 GNU/Linux\n\nBelow the docker-compose file\n# Basic version obtained from https://github.com/vegasbrianc/prometheus/blob/version-2/docker-compose.yml\nversion: '2'\n\nvolumes:\n    prometheus_data: {}\n    grafana_data: {}\n\nnetworks:\n  front-tier:\n    driver: bridge\n  back-tier:\n    driver: bridge\n\nservices:\n  prometheus:\n    image: prom/prometheus\n    container_name: prometheus\n    volumes:\n      - ./prometheus/:/etc/prometheus/\n      - prometheus_data:/prometheus\n    command:\n      - '-config.file=/etc/prometheus/prometheus.yml'\n      - '-storage.local.path=/prometheus'\n      - '-alertmanager.url=http://alertmanager:9093'\n    expose:\n      - 9090\n    ports:\n      - 9090:9090\n    links:\n      - cadvisor:cadvisor\n      - alertmanager:alertmanager\n    depends_on:\n      - cadvisor\n    networks:\n      - back-tier\n\n  node-exporter:\n    image: prom/node-exporter\n    expose:\n      - 9100\n    networks:\n      - back-tier\n  alertmanager:\n    image: prom/alertmanager\n    ports:\n      - 9093:9093\n    volumes:\n      - ./alertmanager/:/etc/alertmanager/\n    networks:\n      - back-tier\n    command:\n      - '-config.file=/etc/alertmanager/config.yml'\n      - '-storage.path=/alertmanager'\n\n  cadvisor:\n    image: google/cadvisor\n    volumes:\n      - /:/rootfs:ro\n      - /var/run:/var/run:rw\n      - /sys:/sys:ro\n      - /var/lib/docker/:/var/lib/docker:ro\n    expose:\n      - 8080\n    networks:\n      - back-tier\n\n  grafana:\n    image: grafana/grafana\n    depends_on:\n      - prometheus\n    ports:\n      - 3000:3000\n    volumes:\n      - grafana_data:/var/lib/grafana\n    env_file:\n      - config.monitoring\n    networks:\n      - back-tier\n      - front-tier\n\nBelow logs being generated\nI0209 16:23:45.332220       1 storagedriver.go:50] Caching stats in memory for 2m0s\nI0209 16:23:45.332831       1 manager.go:140] cAdvisor running in container: \"/docker/2a45e5c517393b3e52e9949cf5b5101f959d5ba1e147146df0ff6abf9044ab71\"\nfatal error: unexpected signal during runtime execution\n[signal 0xb code=0x1 addr=0x63 pc=0x7f78d0ccf1ed]\n\nruntime stack:\nruntime.throw(0xfdc760, 0x2a)\n    /home/stclair/.gvm/gos/go1.6.3/src/runtime/panic.go:547 +0x90\nruntime.sigpanic()\n    /home/stclair/.gvm/gos/go1.6.3/src/runtime/sigpanic_unix.go:12 +0x5a\n\ngoroutine 28 [syscall, locked to thread]:\nruntime.cgocall(0xabb6c0, 0xc820030bd8, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/runtime/cgocall.go:123 +0x11b fp=0xc820030b78 sp=0xc820030b48\nnet._C2func_getaddrinfo(0x7f78c40008c0, 0x0, 0xc8202ca810, 0xc82010c430, 0x0, 0x0, 0x0)\n    ??:0 +0x55 fp=0xc820030bd8 sp=0xc820030b78\nnet.cgoLookupIPCNAME(0xec20a0, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x1072200)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/cgo_unix.go:111 +0x448 fp=0xc820030d50 sp=0xc820030bd8\nnet.cgoLookupIP(0xec20a0, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0, 0x2c307c7c722b7c7c)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/cgo_unix.go:163 +0x56 fp=0xc820030da8 sp=0xc820030d50\nnet.lookupIP(0xec20a0, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/lookup_unix.go:67 +0x94 fp=0xc820030e18 sp=0xc820030da8\nnet.glob.func16(0x1072270, 0xec20a0, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/hook.go:10 +0x4d fp=0xc820030e58 sp=0xc820030e18\nnet.lookupIPDeadline.func1(0x0, 0x0, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/lookup.go:106 +0x71 fp=0xc820030ed8 sp=0xc820030e58\ninternal/singleflight.(*Group).doCall(0x16459c0, 0xc82000e960, 0xec20a0, 0x9, 0xc8201bf6e0)\n    /home/stclair/.gvm/gos/go1.6.3/src/internal/singleflight/singleflight.go:93 +0x2c fp=0xc820030f88 sp=0xc820030ed8\nruntime.goexit()\n    /home/stclair/.gvm/gos/go1.6.3/src/runtime/asm_amd64.s:1998 +0x1 fp=0xc820030f90 sp=0xc820030f88\ncreated by internal/singleflight.(*Group).DoChan\n    /home/stclair/.gvm/gos/go1.6.3/src/internal/singleflight/singleflight.go:86 +0x3ee\n\ngoroutine 1 [select]:\nnet.lookupIPDeadline(0xec20a0, 0x9, 0xed02e8e93, 0x14c4c7f6, 0x1646980, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/lookup.go:109 +0x6a6\nnet.internetAddrList(0xea77a8, 0x3, 0xec20a0, 0xf, 0xed02e8e93, 0x14c4c7f6, 0x1646980, 0x0, 0x0, 0x0, ...)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/ipsock.go:252 +0x6ee\nnet.resolveAddrList(0xea3050, 0x4, 0xea77a8, 0x3, 0xec20a0, 0xf, 0xed02e8e93, 0x14c4c7f6, 0x1646980, 0x0, ...)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/dial.go:158 +0x466\nnet.(*Dialer).Dial(0xc8203815b0, 0xea77a8, 0x3, 0xec20a0, 0xf, 0x0, 0x0, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/dial.go:216 +0x124\nnet.DialTimeout(0xea77a8, 0x3, 0xec20a0, 0xf, 0x77359400, 0x0, 0x0, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/dial.go:200 +0xa3\ngithub.com/google/cadvisor/container/rkt.Client.func1()\n    /home/stclair/go/src/github.com/google/cadvisor/container/rkt/client.go:44 +0x59\nsync.(*Once).Do(0x166c260, 0x10713a8)\n    /home/stclair/.gvm/gos/go1.6.3/src/sync/once.go:44 +0xe4\ngithub.com/google/cadvisor/container/rkt.Client(0x0, 0x0, 0x0, 0x0)\n    /home/stclair/go/src/github.com/google/cadvisor/container/rkt/client.go:79 +0x47\ngithub.com/google/cadvisor/container/rkt.RktPath(0x0, 0x0, 0x0, 0x0)\n    /home/stclair/go/src/github.com/google/cadvisor/container/rkt/client.go:85 +0x48\ngithub.com/google/cadvisor/manager.New(0xc8202e1c40, 0x7f78d0491230, 0x166c1b8, 0xdf8475800, 0x7f78d33d2501, 0xc820303620, 0xc8203037a0, 0x0, 0x0, 0x0, ...)\n    /home/stclair/go/src/github.com/google/cadvisor/manager/manager.go:146 +0x341\nmain.main()\n    /home/stclair/go/src/github.com/google/cadvisor/cadvisor.go:127 +0x654\n\ngoroutine 17 [syscall, locked to thread]:\nruntime.goexit()\n    /home/stclair/.gvm/gos/go1.6.3/src/runtime/asm_amd64.s:1998 +0x1\n\ngoroutine 6 [syscall]:\nos/signal.signal_recv(0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/runtime/sigqueue.go:116 +0x132\nos/signal.loop()\n    /home/stclair/.gvm/gos/go1.6.3/src/os/signal/signal_unix.go:22 +0x18\ncreated by os/signal.init.1\n    /home/stclair/.gvm/gos/go1.6.3/src/os/signal/signal_unix.go:28 +0x37\n\ngoroutine 7 [chan receive]:\ngithub.com/google/cadvisor/vendor/github.com/golang/glog.(*loggingT).flushDaemon(0x1646ce0)\n    /home/stclair/go/src/github.com/google/cadvisor/vendor/github.com/golang/glog/glog.go:882 +0x67\ncreated by github.com/google/cadvisor/vendor/github.com/golang/glog.init.1\n    /home/stclair/go/src/github.com/google/cadvisor/vendor/github.com/golang/glog/glog.go:410 +0x297\n\ngoroutine 25 [IO wait]:\nnet.runtime_pollWait(0x7f78d0492370, 0x72, 0xc820296000)\n    /home/stclair/.gvm/gos/go1.6.3/src/runtime/netpoll.go:160 +0x60\nnet.(*pollDesc).Wait(0xc8203485a0, 0x72, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/fd_poll_runtime.go:73 +0x3a\nnet.(*pollDesc).WaitRead(0xc8203485a0, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/fd_poll_runtime.go:78 +0x36\nnet.(*netFD).Read(0xc820348540, 0xc820296000, 0x1000, 0x1000, 0x0, 0x7f78d33ce050, 0xc820010120)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/fd_unix.go:250 +0x23a\nnet.(*conn).Read(0xc82010c408, 0xc820296000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/net.go:172 +0xe4\nnet/http.noteEOFReader.Read(0x7f78d0492430, 0xc82010c408, 0xc820058618, 0xc820296000, 0x1000, 0x1000, 0x409fb3, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/http/transport.go:1690 +0x67\nnet/http.(*noteEOFReader).Read(0xc8201bf540, 0xc820296000, 0x1000, 0x1000, 0xc82002dd1d, 0x0, 0x0)\n    <autogenerated>:284 +0xd0\nbufio.(*Reader).fill(0xc820054a80)\n    /home/stclair/.gvm/gos/go1.6.3/src/bufio/bufio.go:97 +0x1e9\nbufio.(*Reader).Peek(0xc820054a80, 0x1, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /home/stclair/.gvm/gos/go1.6.3/src/bufio/bufio.go:132 +0xcc\nnet/http.(*persistConn).readLoop(0xc8200585b0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/http/transport.go:1076 +0x177\ncreated by net/http.(*Transport).dialConn\n    /home/stclair/.gvm/gos/go1.6.3/src/net/http/transport.go:860 +0x10a6\n\ngoroutine 26 [select]:\nnet/http.(*persistConn).writeLoop(0xc8200585b0)\n    /home/stclair/.gvm/gos/go1.6.3/src/net/http/transport.go:1280 +0x472\ncreated by net/http.(*Transport).dialConn\n    /home/stclair/.gvm/gos/go1.6.3/src/net/http/transport.go:861 +0x10cb\n\nBelow the output of docker inspect\n[\n    {\n        \"Id\": \"2a45e5c517393b3e52e9949cf5b5101f959d5ba1e147146df0ff6abf9044ab71\",\n        \"Created\": \"2017-02-09T16:23:44.377391743Z\",\n        \"Path\": \"/usr/bin/cadvisor\",\n        \"Args\": [\n            \"-logtostderr\"\n        ],\n        \"State\": {\n            \"Status\": \"exited\",\n            \"Running\": false,\n            \"Paused\": false,\n            \"Restarting\": false,\n            \"OOMKilled\": false,\n            \"Dead\": false,\n            \"Pid\": 0,\n            \"ExitCode\": 2,\n            \"Error\": \"\",\n            \"StartedAt\": \"2017-02-09T16:23:45.14984008Z\",\n            \"FinishedAt\": \"2017-02-09T16:23:45.395502216Z\"\n        },\n        \"Image\": \"sha256:cc8254dd08c6ff3e6521fab4fdd9cc361c704f2fa51007ce230035f0b391f118\",\n        \"ResolvConfPath\": \"/opt/data/docker/containers/2a45e5c517393b3e52e9949cf5b5101f959d5ba1e147146df0ff6abf9044ab71/resolv.conf\",\n        \"HostnamePath\": \"/opt/data/docker/containers/2a45e5c517393b3e52e9949cf5b5101f959d5ba1e147146df0ff6abf9044ab71/hostname\",\n        \"HostsPath\": \"/opt/data/docker/containers/2a45e5c517393b3e52e9949cf5b5101f959d5ba1e147146df0ff6abf9044ab71/hosts\",\n        \"LogPath\": \"/opt/data/docker/containers/2a45e5c517393b3e52e9949cf5b5101f959d5ba1e147146df0ff6abf9044ab71/2a45e5c517393b3e52e9949cf5b5101f959d5ba1e147146df0ff6abf9044ab71-json.log\",\n        \"Name\": \"/prometheusgrafanacadvisor_cadvisor_1\",\n        \"RestartCount\": 0,\n        \"Driver\": \"overlay\",\n        \"MountLabel\": \"\",\n        \"ProcessLabel\": \"\",\n        \"AppArmorProfile\": \"\",\n        \"ExecIDs\": null,\n        \"HostConfig\": {\n            \"Binds\": [\n                \"/var/lib/docker:/var/lib/docker:ro\",\n                \"/var/run:/var/run:rw\",\n                \"/:/rootfs:ro\",\n                \"/sys:/sys:ro\"\n            ],\n            \"ContainerIDFile\": \"\",\n            \"LogConfig\": {\n                \"Type\": \"json-file\",\n                \"Config\": {}\n            },\n            \"NetworkMode\": \"prometheusgrafanacadvisor_back-tier\",\n            \"PortBindings\": {},\n            \"RestartPolicy\": {\n                \"Name\": \"\",\n                \"MaximumRetryCount\": 0\n            },\n            \"AutoRemove\": false,\n            \"VolumeDriver\": \"\",\n            \"VolumesFrom\": [],\n            \"CapAdd\": null,\n            \"CapDrop\": null,\n            \"Dns\": null,\n            \"DnsOptions\": null,\n            \"DnsSearch\": null,\n            \"ExtraHosts\": null,\n            \"GroupAdd\": null,\n            \"IpcMode\": \"\",\n            \"Cgroup\": \"\",\n            \"Links\": null,\n            \"OomScoreAdj\": 0,\n            \"PidMode\": \"\",\n            \"Privileged\": false,\n            \"PublishAllPorts\": false,\n            \"ReadonlyRootfs\": false,\n            \"SecurityOpt\": null,\n            \"UTSMode\": \"\",\n            \"UsernsMode\": \"\",\n            \"ShmSize\": 67108864,\n            \"Runtime\": \"runc\",\n            \"ConsoleSize\": [\n                0,\n                0\n            ],\n            \"Isolation\": \"\",\n            \"CpuShares\": 0,\n            \"Memory\": 0,\n            \"CgroupParent\": \"\",\n            \"BlkioWeight\": 0,\n            \"BlkioWeightDevice\": null,\n            \"BlkioDeviceReadBps\": null,\n            \"BlkioDeviceWriteBps\": null,\n            \"BlkioDeviceReadIOps\": null,\n            \"BlkioDeviceWriteIOps\": null,\n            \"CpuPeriod\": 0,\n            \"CpuQuota\": 0,\n            \"CpusetCpus\": \"\",\n            \"CpusetMems\": \"\",\n            \"Devices\": null,\n            \"DiskQuota\": 0,\n            \"KernelMemory\": 0,\n            \"MemoryReservation\": 0,\n            \"MemorySwap\": 0,\n            \"MemorySwappiness\": -1,\n            \"OomKillDisable\": false,\n            \"PidsLimit\": 0,\n            \"Ulimits\": null,\n            \"CpuCount\": 0,\n            \"CpuPercent\": 0,\n            \"IOMaximumIOps\": 0,\n            \"IOMaximumBandwidth\": 0\n        },\n        \"GraphDriver\": {\n            \"Name\": \"overlay\",\n            \"Data\": {\n                \"LowerDir\": \"/opt/data/docker/overlay/ca76231f9ea9e8cc08b09af8384ad1f6cc08b47862a8f356ec5c975d7a6f4d8d/root\",\n                \"MergedDir\": \"/opt/data/docker/overlay/4da26ef7ae1677f874beadad70109cfb3c99c9ec7d70013cb42d0441309006c3/merged\",\n                \"UpperDir\": \"/opt/data/docker/overlay/4da26ef7ae1677f874beadad70109cfb3c99c9ec7d70013cb42d0441309006c3/upper\",\n                \"WorkDir\": \"/opt/data/docker/overlay/4da26ef7ae1677f874beadad70109cfb3c99c9ec7d70013cb42d0441309006c3/work\"\n            }\n        },\n        \"Mounts\": [\n            {\n                \"Source\": \"/var/run\",\n                \"Destination\": \"/var/run\",\n                \"Mode\": \"rw\",\n                \"RW\": true,\n                \"Propagation\": \"rprivate\"\n            },\n            {\n                \"Source\": \"/\",\n                \"Destination\": \"/rootfs\",\n                \"Mode\": \"ro\",\n                \"RW\": false,\n                \"Propagation\": \"rprivate\"\n            },\n            {\n                \"Source\": \"/sys\",\n                \"Destination\": \"/sys\",\n                \"Mode\": \"ro\",\n                \"RW\": false,\n                \"Propagation\": \"rprivate\"\n            },\n            {\n                \"Source\": \"/var/lib/docker\",\n                \"Destination\": \"/var/lib/docker\",\n                \"Mode\": \"ro\",\n                \"RW\": false,\n                \"Propagation\": \"rprivate\"\n            }\n        ],\n        \"Config\": {\n            \"Hostname\": \"2a45e5c51739\",\n            \"Domainname\": \"\",\n            \"User\": \"\",\n            \"AttachStdin\": false,\n            \"AttachStdout\": false,\n            \"AttachStderr\": false,\n            \"ExposedPorts\": {\n                \"8080/tcp\": {}\n            },\n            \"Tty\": false,\n            \"OpenStdin\": false,\n            \"StdinOnce\": false,\n            \"Env\": [\n                \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n                \"GLIBC_VERSION=2.23-r3\"\n            ],\n            \"Cmd\": null,\n            \"Image\": \"google/cadvisor\",\n            \"Volumes\": {\n                \"/rootfs\": {},\n                \"/sys\": {},\n                \"/var/lib/docker\": {},\n                \"/var/run\": {}\n            },\n            \"WorkingDir\": \"\",\n            \"Entrypoint\": [\n                \"/usr/bin/cadvisor\",\n                \"-logtostderr\"\n            ],\n            \"OnBuild\": null,\n            \"Labels\": {\n                \"com.docker.compose.config-hash\": \"3cf5b2f074b7bfade6be11280fee739ca25eea79ce857e7a44df90b4afc16db4\",\n                \"com.docker.compose.container-number\": \"1\",\n                \"com.docker.compose.oneoff\": \"False\",\n                \"com.docker.compose.project\": \"prometheusgrafanacadvisor\",\n                \"com.docker.compose.service\": \"cadvisor\",\n                \"com.docker.compose.version\": \"1.10.0\"\n            }\n        },\n        \"NetworkSettings\": {\n            \"Bridge\": \"\",\n            \"SandboxID\": \"aff03cb51ad4391084c4a2969d0900b8392a100754621f08f85a31520b8ff7fc\",\n            \"HairpinMode\": false,\n            \"LinkLocalIPv6Address\": \"\",\n            \"LinkLocalIPv6PrefixLen\": 0,\n            \"Ports\": null,\n            \"SandboxKey\": \"/var/run/docker/netns/aff03cb51ad4\",\n            \"SecondaryIPAddresses\": null,\n            \"SecondaryIPv6Addresses\": null,\n            \"EndpointID\": \"\",\n            \"Gateway\": \"\",\n            \"GlobalIPv6Address\": \"\",\n            \"GlobalIPv6PrefixLen\": 0,\n            \"IPAddress\": \"\",\n            \"IPPrefixLen\": 0,\n            \"IPv6Gateway\": \"\",\n            \"MacAddress\": \"\",\n            \"Networks\": {\n                \"prometheusgrafanacadvisor_back-tier\": {\n                    \"IPAMConfig\": null,\n                    \"Links\": null,\n                    \"Aliases\": [\n                        \"2a45e5c51739\",\n                        \"cadvisor\"\n                    ],\n                    \"NetworkID\": \"ab4e4b94a2abb0c37e55ab9603d2b69c2349d02df6e431c27acc728b457ecb9f\",\n                    \"EndpointID\": \"\",\n                    \"Gateway\": \"\",\n                    \"IPAddress\": \"\",\n                    \"IPPrefixLen\": 0,\n                    \"IPv6Gateway\": \"\",\n                    \"GlobalIPv6Address\": \"\",\n                    \"GlobalIPv6PrefixLen\": 0,\n                    \"MacAddress\": \"\"\n                }\n            }\n        }\n    }\n]\n\n. ",
    "bkc1": "same cadvisor issue happening on CentOS Linux release 7.3.1611 vagrant host.. ",
    "avdv": "I hit the same problem, Red Hat Enterprise Linux Server release 7.3.\nI have tried the pre-compiled 0.24.1 binary and also build version 0.24.2 myself (on Arch Linux with go1.8 amd64).\nInterestingly, when running cadvisor inside gdb, it does not crash. Also, when running the official docker image it works too.\n```\ncadvisor -port 9870 -docker_only\nfatal error: unexpected signal during runtime execution\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x63 pc=0x7f1512b8660d]\nruntime stack:\nruntime.throw(0xc92d96, 0x2a)\n    /usr/lib/go/src/runtime/panic.go:596 +0x95\nruntime.sigpanic()\n    /usr/lib/go/src/runtime/signal_unix.go:274 +0x2db\ngoroutine 11 [syscall, locked to thread]:\nruntime.cgocall(0xa17dc0, 0xc4200225d8, 0xc91fa6)\n    /usr/lib/go/src/runtime/cgocall.go:131 +0xe2 fp=0xc420022598 sp=0xc420022558\nnet._C2func_getaddrinfo(0x7f150c0008c0, 0x0, 0xc420344ba0, 0xc42000e040, 0x0, 0x0, 0x0)\n    net/_obj/_cgo_gotypes.go:86 +0x68 fp=0xc4200225d8 sp=0xc420022598\nnet.cgoLookupIPCNAME.func2(0x7f150c0008c0, 0x0, 0xc420344ba0, 0xc42000e040, 0xc7dec9, 0x9, 0xc420300590)\n    /usr/lib/go/src/net/cgo_unix.go:151 +0x171 fp=0xc420022638 sp=0xc4200225d8\nnet.cgoLookupIPCNAME(0xc7dec9, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/lib/go/src/net/cgo_unix.go:151 +0x1bd fp=0xc420022738 sp=0xc420022638\nnet.cgoIPLookup(0xc42013a2a0, 0xc7dec9, 0x9)\n    /usr/lib/go/src/net/cgo_unix.go:203 +0x4d fp=0xc4200227c8 sp=0xc420022738\nruntime.goexit()\n    /usr/lib/go/src/runtime/asm_amd64.s:2197 +0x1 fp=0xc4200227d0 sp=0xc4200227c8\ncreated by net.cgoLookupIP\n    /usr/lib/go/src/net/cgo_unix.go:213 +0xb4\ngoroutine 1 [select]:\nnet.(Resolver).LookupIPAddr(0x12eb6e5, 0x1235e20, 0xc42013a1e0, 0xc7dec9, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/lib/go/src/net/lookup.go:165 +0x9b1\nnet.(Resolver).internetAddrList(0x12eb6e5, 0x1235e20, 0xc42013a1e0, 0xc737c5, 0x3, 0xc7dec9, 0xf, 0x0, 0x0, 0xc42040d590, ...)\n    /usr/lib/go/src/net/ipsock.go:245 +0x4cc\nnet.(*Resolver).resolveAddrList(0x12eb6e5, 0x1235e20, 0xc42013a1e0, 0xc73d2a, 0x4, 0xc737c5, 0x3, 0xc7dec9, 0xf, 0x0, ...)\n    /usr/lib/go/src/net/dial.go:188 +0x6b3\n...\n```. > +1 fixed by adding \"-tags netgo\" on ubuntu 16.04 with go1.8.1\nConfirmed. Setting GODEBUG=netdns=go in the envirnoment to force using the Go resolver at runtime (without recompiliation) has the same effect:\n```\n./cadvisor -version\ncAdvisor version v0.25.0 (17543be)\nGODEBUG=netdns=cgo ./cadvisor -docker_only -port 9999\nfatal error: unexpected signal during runtime execution\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x63 pc=0x7f56933a760d]\nruntime stack:\nruntime.throw(0xcdd37b, 0x2a)\n    /usr/local/go/src/runtime/panic.go:566 +0x95\nruntime.sigpanic()\n    /usr/local/go/src/runtime/sigpanic_unix.go:12 +0x2cc\ngoroutine 30 [syscall, locked to thread]:\n[...]\nGODEBUG=netdns=go ./cadvisor -docker_only -port 9999\nE0510 08:33:17.417989    3391 factory.go:305] devicemapper filesystem stats will not be reported: usage of thin_ls is disabled to preserve iops\n```\n. \\followup: I have just upgrade to cadvisor 0.26.1 and are seeing this issue now also when running cadvisor on the host machine from systemd.\nmanager.go:143] cAdvisor running in container: \"/system.slice/cadvisor.service\"\nfatal error: unexpected signal during runtime execution\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x63 pc=0x7fefcca4260d]\n...\nMitigated by using Environment='GODEBUG=netdns=go' in the Service section of the systemd unit... ",
    "pangzheng": "Use the official image and install findutils to run normally\ndockerfiles\n```\ngoogle-cadvisor:v0.25.0\nMAINTAINER pro zpang\ntime\nRUN apk add --update tzdata && rm -rf /var/cache/apk/*\nRUN cp /usr/share/zoneinfo/Hongkong /etc/localtime\nadd gnu find\nRUN apk add --update findutils && rm -rf /var/cache/apk/*\n```. After starting, run under non-docker, the same error, is the other building method?. other test, install gdb\nENV CADVISOR_VERSION=\"v0.25.0\"\nRUN yum install glibc gdb -y\nRUN wget -O /cadvisor https://github.com/google/cadvisor/releases/download/$CADVISOR_VERSION/cadvisor && \\\n    chmod 755 /cadvisor\nENTRYPOINT [\"/cadvisor\"]\nsame erro\n```\nfatal error: unexpected signal during runtime execution\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x47 pc=0x7f822a06c60d]\nruntime stack:\nruntime.throw(0xcdd37b, 0x2a)\n    /usr/local/go/src/runtime/panic.go:566 +0x95\nruntime.sigpanic()\n    /usr/local/go/src/runtime/sigpanic_unix.go:12 +0x2cc\ngoroutine 36 [syscall, locked to thread]:\nruntime.cgocall(0xa551f0, 0xc4203fcdf8, 0xc400000000)\n    /usr/local/go/src/runtime/cgocall.go:131 +0x110 fp=0xc4203fcdb0 sp=0xc4203fcd70\nnet._C2func_getaddrinfo(0x7f82240008c0, 0x0, 0xc42022c1b0, 0xc42011e048, 0x0, 0x0, 0x0)\n    ??:0 +0x68 fp=0xc4203fcdf8 sp=0xc4203fcdb0\nnet.cgoLookupIPCNAME(0xcc7b5a, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/cgo_unix.go:146 +0x37c fp=0xc4203fcf18 sp=0xc4203fcdf8\nnet.cgoIPLookup(0xc42027e1e0, 0xcc7b5a, 0x9)\n    /usr/local/go/src/net/cgo_unix.go:198 +0x4d fp=0xc4203fcfa8 sp=0xc4203fcf18\nruntime.goexit()\n    /usr/local/go/src/runtime/asm_amd64.s:2086 +0x1 fp=0xc4203fcfb0 sp=0xc4203fcfa8\ncreated by net.cgoLookupIP\n    /usr/local/go/src/net/cgo_unix.go:208 +0xb4\ngoroutine 1 [select]:\nnet.lookupIPContext(0x12c2500, 0xc42027e120, 0xcc7b5a, 0x9, 0x0, 0x0, 0x0, 0x0, 0xc42047d390)\n    /usr/local/go/src/net/lookup.go:122 +0x7bc\nnet.internetAddrList(0x12c2500, 0xc42027e120, 0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x0, 0x0, 0x0, 0xed087827f, ...)\n    /usr/local/go/src/net/ipsock.go:241 +0x5e0\nnet.resolveAddrList(0x12c2500, 0xc42027e120, 0xcbd267, 0x4, 0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x0, 0x0, ...)\n    /usr/local/go/src/net/dial.go:179 +0x106\nnet.(Dialer).DialContext(0xc42047d790, 0x12c24c0, 0xc4200143d8, 0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x0, 0x0, 0x0, ...)\n    /usr/local/go/src/net/dial.go:329 +0x238\nnet.(Dialer).Dial(0xc42047d790, 0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x2, 0xc4202a8640, 0x48, 0xc42047d850)\n    /usr/local/go/src/net/dial.go:282 +0x75\nnet.DialTimeout(0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x77359400, 0xc, 0xc420330812, 0x7, 0xc420329080)\n    /usr/local/go/src/net/dial.go:268 +0x9d\ngithub.com/google/cadvisor/container/rkt.Client.func1()\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/container/rkt/client.go:44 +0x69\nsync.(*Once).Do(0x1369b30, 0xd3aac0)\n    /usr/local/go/src/sync/once.go:44 +0xdb\ngithub.com/google/cadvisor/container/rkt.Client(0xc420330bd0, 0x9, 0x0, 0x0)\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/container/rkt/client.go:79 +0x39\ngithub.com/google/cadvisor/container/rkt.RktPath(0xc4202a6170, 0xa, 0xc42021c160, 0x15)\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/container/rkt/client.go:85 +0x29\ngithub.com/google/cadvisor/manager.New(0xc42036a140, 0x12c6520, 0x1369a90, 0xdf8475800, 0x12b9c01, 0xc4203a7740, 0xc4203a7a40, 0x0, 0x0, 0x0, ...)\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/manager/manager.go:149 +0x18b\nmain.main()\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/cadvisor.go:127 +0x1e0\ngoroutine 17 [syscall, locked to thread]:\nruntime.goexit()\n    /usr/local/go/src/runtime/asm_amd64.s:2086 +0x1\ngoroutine 18 [syscall]:\nos/signal.signal_recv(0x0)\n    /usr/local/go/src/runtime/sigqueue.go:116 +0x157\nos/signal.loop()\n    /usr/local/go/src/os/signal/signal_unix.go:22 +0x22\ncreated by os/signal.init.1\n    /usr/local/go/src/os/signal/signal_unix.go:28 +0x41\ngoroutine 19 [chan receive]:\ngithub.com/google/cadvisor/vendor/github.com/golang/glog.(*loggingT).flushDaemon(0x13459c0)\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/vendor/github.com/golang/glog/glog.go:882 +0x7a\ncreated by github.com/google/cadvisor/vendor/github.com/golang/glog.init.1\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/vendor/github.com/golang/glog/glog.go:410 +0x21d\ngoroutine 10 [IO wait]:\nnet.runtime_pollWait(0x7f822a2c58f0, 0x72, 0x4)\n    /usr/local/go/src/runtime/netpoll.go:160 +0x59\nnet.(pollDesc).wait(0xc4203d4d10, 0x72, 0xc4200359d0, 0xc420014140)\n    /usr/local/go/src/net/fd_poll_runtime.go:73 +0x38\nnet.(pollDesc).waitRead(0xc4203d4d10, 0x12bc800, 0xc420014140)\n    /usr/local/go/src/net/fd_poll_runtime.go:78 +0x34\nnet.(netFD).Read(0xc4203d4cb0, 0xc4202b6000, 0x1000, 0x1000, 0x0, 0x12bc800, 0xc420014140)\n    /usr/local/go/src/net/fd_unix.go:243 +0x1a1\nnet.(conn).Read(0xc420382090, 0xc4202b6000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/net.go:173 +0x70\nnet/http.(persistConn).Read(0xc420068d00, 0xc4202b6000, 0x1000, 0x1000, 0x4d4f70, 0xc420035b58, 0x4081fd)\n    /usr/local/go/src/net/http/transport.go:1261 +0x154\nbufio.(Reader).fill(0xc420124cc0)\n    /usr/local/go/src/bufio/bufio.go:97 +0x10c\nbufio.(Reader).Peek(0xc420124cc0, 0x1, 0xc420035bbd, 0x1, 0x0, 0xc420125080, 0x0)\n    /usr/local/go/src/bufio/bufio.go:129 +0x62\nnet/http.(persistConn).readLoop(0xc420068d00)\n    /usr/local/go/src/net/http/transport.go:1418 +0x1a1\ncreated by net/http.(*Transport).dialConn\n    /usr/local/go/src/net/http/transport.go:1062 +0x4e9\ngoroutine 11 [select]:\nnet/http.(persistConn).writeLoop(0xc420068d00)\n    /usr/local/go/src/net/http/transport.go:1646 +0x3bd\ncreated by net/http.(Transport).dialConn\n    /usr/local/go/src/net/http/transport.go:1063 +0x50e\ngoroutine 35 [select]:\nnet.cgoLookupIP(0x12c2500, 0xc42027e120, 0xcc7b5a, 0x9, 0x207261767b296528, 0x2e65282626653d74, 0x636f4472656e776f, 0x657c7c746e656d75, 0x656d75636f642e29, 0x6e656d656c45746e)\n    /usr/local/go/src/net/cgo_unix.go:209 +0x2f5\nnet.lookupIP(0x12c2500, 0xc42027e120, 0xcc7b5a, 0x9, 0x2e74613d702c7d7b, 0x6d75636f44746573, 0x636e75663d746e65, 0x7b2965286e6f6974, 0x3f653d6e20726176)\n    /usr/local/go/src/net/lookup_unix.go:70 +0xf9\nnet.glob..func11(0x12c2500, 0xc42027e120, 0xd3ba38, 0xcc7b5a, 0x9, 0x70795465646f6e2e, 0x636f642e6e262665, 0x656c45746e656d75, 0x3d66283f746e656d, 0x6f642e6e3d642c6e)\n    /usr/local/go/src/net/hook.go:19 +0x52\nnet.lookupIPContext.func1(0x746e657645686361, 0x6f6665626e6f2228, 0x64616f6c6e756572, 0x6974636e75662c22)\n    /usr/local/go/src/net/lookup.go:119 +0x5c\ninternal/singleflight.(Group).doCall(0x13443d0, 0xc420116280, 0xcc7b5a, 0x9, 0xc420329140)\n    /usr/local/go/src/internal/singleflight/singleflight.go:93 +0x3c\ncreated by internal/singleflight.(Group).DoChan\n    /usr/local/go/src/internal/singleflight/singleflight.go:86 +0x339\nfatal error: unexpected signal during runtime execution\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x47 pc=0x7f66bc15b60d]\nruntime stack:\nruntime.throw(0xcdd37b, 0x2a)\n    /usr/local/go/src/runtime/panic.go:566 +0x95\nruntime.sigpanic()\n    /usr/local/go/src/runtime/sigpanic_unix.go:12 +0x2cc\ngoroutine 46 [syscall, locked to thread]:\nruntime.cgocall(0xa551f0, 0xc4200485f8, 0xc400000000)\n    /usr/local/go/src/runtime/cgocall.go:131 +0x110 fp=0xc4200485b0 sp=0xc420048570\nnet._C2func_getaddrinfo(0x7f66a80008c0, 0x0, 0xc4202036e0, 0xc420026020, 0x0, 0x0, 0x0)\n    ??:0 +0x68 fp=0xc4200485f8 sp=0xc4200485b0\nnet.cgoLookupIPCNAME(0xcc7b5a, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/cgo_unix.go:146 +0x37c fp=0xc420048718 sp=0xc4200485f8\nnet.cgoIPLookup(0xc4203b4300, 0xcc7b5a, 0x9)\n    /usr/local/go/src/net/cgo_unix.go:198 +0x4d fp=0xc4200487a8 sp=0xc420048718\nruntime.goexit()\n    /usr/local/go/src/runtime/asm_amd64.s:2086 +0x1 fp=0xc4200487b0 sp=0xc4200487a8\ncreated by net.cgoLookupIP\n    /usr/local/go/src/net/cgo_unix.go:208 +0xb4\ngoroutine 1 [select]:\nnet.lookupIPContext(0x12c2500, 0xc4203b4240, 0xcc7b5a, 0x9, 0x0, 0x0, 0x0, 0x0, 0xc4202a9390)\n    /usr/local/go/src/net/lookup.go:122 +0x7bc\nnet.internetAddrList(0x12c2500, 0xc4203b4240, 0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x0, 0x0, 0x0, 0xed087827f, ...)\n    /usr/local/go/src/net/ipsock.go:241 +0x5e0\nnet.resolveAddrList(0x12c2500, 0xc4203b4240, 0xcbd267, 0x4, 0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x0, 0x0, ...)\n    /usr/local/go/src/net/dial.go:179 +0x106\nnet.(Dialer).DialContext(0xc4202a9790, 0x12c24c0, 0xc420070338, 0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x0, 0x0, 0x0, ...)\n    /usr/local/go/src/net/dial.go:329 +0x238\nnet.(Dialer).Dial(0xc4202a9790, 0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x2, 0xc4200844c0, 0x48, 0xc4202a9850)\n    /usr/local/go/src/net/dial.go:282 +0x75\nnet.DialTimeout(0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x77359400, 0xc, 0xc420394de2, 0x7, 0xc420203200)\n    /usr/local/go/src/net/dial.go:268 +0x9d\ngithub.com/google/cadvisor/container/rkt.Client.func1()\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/container/rkt/client.go:44 +0x69\nsync.(*Once).Do(0x1369b30, 0xd3aac0)\n    /usr/local/go/src/sync/once.go:44 +0xdb\ngithub.com/google/cadvisor/container/rkt.Client(0xc420395630, 0x9, 0x0, 0x0)\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/container/rkt/client.go:79 +0x39\ngithub.com/google/cadvisor/container/rkt.RktPath(0xc420244170, 0xa, 0xc420206460, 0x15)\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/container/rkt/client.go:85 +0x29\ngithub.com/google/cadvisor/manager.New(0xc42023f540, 0x12c6520, 0x1369a90, 0xdf8475800, 0x12b9c01, 0xc4203b9e60, 0xc42044ec90, 0x0, 0x0, 0x0, ...)\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/manager/manager.go:149 +0x18b\nmain.main()\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/cadvisor.go:127 +0x1e0\ngoroutine 17 [syscall, locked to thread]:\nruntime.goexit()\n    /usr/local/go/src/runtime/asm_amd64.s:2086 +0x1\ngoroutine 3 [syscall]:\nos/signal.signal_recv(0x0)\n    /usr/local/go/src/runtime/sigqueue.go:116 +0x157\nos/signal.loop()\n    /usr/local/go/src/os/signal/signal_unix.go:22 +0x22\ncreated by os/signal.init.1\n    /usr/local/go/src/os/signal/signal_unix.go:28 +0x41\ngoroutine 4 [chan receive]:\ngithub.com/google/cadvisor/vendor/github.com/golang/glog.(*loggingT).flushDaemon(0x13459c0)\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/vendor/github.com/golang/glog/glog.go:882 +0x7a\ncreated by github.com/google/cadvisor/vendor/github.com/golang/glog.init.1\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/vendor/github.com/golang/glog/glog.go:410 +0x21d\ngoroutine 8 [IO wait]:\nnet.runtime_pollWait(0x7f66bc37a230, 0x72, 0x4)\n    /usr/local/go/src/runtime/netpoll.go:160 +0x59\nnet.(pollDesc).wait(0xc42036df00, 0x72, 0xc4200359d0, 0xc4200700a0)\n    /usr/local/go/src/net/fd_poll_runtime.go:73 +0x38\nnet.(pollDesc).waitRead(0xc42036df00, 0x12bc800, 0xc4200700a0)\n    /usr/local/go/src/net/fd_poll_runtime.go:78 +0x34\nnet.(netFD).Read(0xc42036dea0, 0xc4202bc000, 0x1000, 0x1000, 0x0, 0x12bc800, 0xc4200700a0)\n    /usr/local/go/src/net/fd_unix.go:243 +0x1a1\nnet.(conn).Read(0xc420076458, 0xc4202bc000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/net.go:173 +0x70\nnet/http.(persistConn).Read(0xc420067300, 0xc4202bc000, 0x1000, 0x1000, 0x4d4f70, 0xc420035b58, 0x4081fd)\n    /usr/local/go/src/net/http/transport.go:1261 +0x154\nbufio.(Reader).fill(0xc420136c60)\n    /usr/local/go/src/bufio/bufio.go:97 +0x10c\nbufio.(Reader).Peek(0xc420136c60, 0x1, 0xc420035bbd, 0x1, 0x0, 0xc42006c420, 0x0)\n    /usr/local/go/src/bufio/bufio.go:129 +0x62\nnet/http.(persistConn).readLoop(0xc420067300)\n    /usr/local/go/src/net/http/transport.go:1418 +0x1a1\ncreated by net/http.(*Transport).dialConn\n    /usr/local/go/src/net/http/transport.go:1062 +0x4e9\ngoroutine 9 [select]:\nnet/http.(persistConn).writeLoop(0xc420067300)\n    /usr/local/go/src/net/http/transport.go:1646 +0x3bd\ncreated by net/http.(Transport).dialConn\n    /usr/local/go/src/net/http/transport.go:1063 +0x50e\ngoroutine 45 [select]:\nnet.cgoLookupIP(0x12c2500, 0xc4203b4240, 0xcc7b5a, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/cgo_unix.go:209 +0x2f5\nnet.lookupIP(0x12c2500, 0xc4203b4240, 0xcc7b5a, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/lookup_unix.go:70 +0xf9\nnet.glob..func11(0x12c2500, 0xc4203b4240, 0xd3ba38, 0xcc7b5a, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/hook.go:19 +0x52\nnet.lookupIPContext.func1(0x0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/lookup.go:119 +0x5c\ninternal/singleflight.(Group).doCall(0x13443d0, 0xc42044cdc0, 0xcc7b5a, 0x9, 0xc420203230)\n    /usr/local/go/src/internal/singleflight/singleflight.go:93 +0x3c\ncreated by internal/singleflight.(Group).DoChan\n    /usr/local/go/src/internal/singleflight/singleflight.go:86 +0x339\nfatal error: unexpected signal during runtime execution\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x47 pc=0x7fd58170460d]\nruntime stack:\nruntime.throw(0xcdd37b, 0x2a)\n    /usr/local/go/src/runtime/panic.go:566 +0x95\nruntime.sigpanic()\n    /usr/local/go/src/runtime/sigpanic_unix.go:12 +0x2cc\ngoroutine 40 [syscall, locked to thread]:\nruntime.cgocall(0xa551f0, 0xc42001edf8, 0xc400000000)\n    /usr/local/go/src/runtime/cgocall.go:131 +0x110 fp=0xc42001edb0 sp=0xc42001ed70\nnet._C2func_getaddrinfo(0x7fd57c0008c0, 0x0, 0xc4201307b0, 0xc4200261e0, 0x0, 0x0, 0x0)\n    ??:0 +0x68 fp=0xc42001edf8 sp=0xc42001edb0\nnet.cgoLookupIPCNAME(0xcc7b5a, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/cgo_unix.go:146 +0x37c fp=0xc42001ef18 sp=0xc42001edf8\nnet.cgoIPLookup(0xc42006a480, 0xcc7b5a, 0x9)\n    /usr/local/go/src/net/cgo_unix.go:198 +0x4d fp=0xc42001efa8 sp=0xc42001ef18\nruntime.goexit()\n    /usr/local/go/src/runtime/asm_amd64.s:2086 +0x1 fp=0xc42001efb0 sp=0xc42001efa8\ncreated by net.cgoLookupIP\n    /usr/local/go/src/net/cgo_unix.go:208 +0xb4\ngoroutine 1 [select]:\nnet.lookupIPContext(0x12c2500, 0xc42006a360, 0xcc7b5a, 0x9, 0x0, 0x0, 0x0, 0x0, 0xc420177390)\n    /usr/local/go/src/net/lookup.go:122 +0x7bc\nnet.internetAddrList(0x12c2500, 0xc42006a360, 0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x0, 0x0, 0x0, 0xed0878280, ...)\n    /usr/local/go/src/net/ipsock.go:241 +0x5e0\nnet.resolveAddrList(0x12c2500, 0xc42006a360, 0xcbd267, 0x4, 0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x0, 0x0, ...)\n    /usr/local/go/src/net/dial.go:179 +0x106\nnet.(Dialer).DialContext(0xc420177790, 0x12c24c0, 0xc42006e338, 0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x0, 0x0, 0x0, ...)\n    /usr/local/go/src/net/dial.go:329 +0x238\nnet.(Dialer).Dial(0xc420177790, 0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x2, 0xc42036af40, 0x48, 0xc420177850)\n    /usr/local/go/src/net/dial.go:282 +0x75\nnet.DialTimeout(0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x77359400, 0xc, 0xc42035ef62, 0x7, 0xc420130240)\n    /usr/local/go/src/net/dial.go:268 +0x9d\ngithub.com/google/cadvisor/container/rkt.Client.func1()\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/container/rkt/client.go:44 +0x69\nsync.(*Once).Do(0x1369b30, 0xd3aac0)\n    /usr/local/go/src/sync/once.go:44 +0xdb\ngithub.com/google/cadvisor/container/rkt.Client(0xc42035f2c0, 0x9, 0x0, 0x0)\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/container/rkt/client.go:79 +0x39\ngithub.com/google/cadvisor/container/rkt.RktPath(0xc42025c170, 0xa, 0xc42017e160, 0x15)\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/container/rkt/client.go:85 +0x29\ngithub.com/google/cadvisor/manager.New(0xc4201851c0, 0x12c6520, 0x1369a90, 0xdf8475800, 0x12b9c01, 0xc420360630, 0xc420360c30, 0x0, 0x0, 0x0, ...)\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/manager/manager.go:149 +0x18b\nmain.main()\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/cadvisor.go:127 +0x1e0\ngoroutine 17 [syscall, locked to thread]:\nruntime.goexit()\n    /usr/local/go/src/runtime/asm_amd64.s:2086 +0x1\ngoroutine 39 [select]:\nnet.cgoLookupIP(0x12c2500, 0xc42006a360, 0xcc7b5a, 0x9, 0xcc283b, 0x9, 0xc4200182e0, 0xc420017500, 0xc42001eea0, 0x40848e)\n    /usr/local/go/src/net/cgo_unix.go:209 +0x2f5\nnet.lookupIP(0x12c2500, 0xc42006a360, 0xcc7b5a, 0x9, 0x0, 0x0, 0xc42009a5a0, 0xc420079040, 0xc42006a238)\n    /usr/local/go/src/net/lookup_unix.go:70 +0xf9\nnet.glob..func11(0x12c2500, 0xc42006a360, 0xd3ba38, 0xcc7b5a, 0x9, 0x4081fd, 0xb892a0, 0xc42006a1e0, 0x4125bc, 0xc42006a238)\n    /usr/local/go/src/net/hook.go:19 +0x52\nnet.lookupIPContext.func1(0x408cbb, 0xc42006a238, 0x0, 0x6)\n    /usr/local/go/src/net/lookup.go:119 +0x5c\ninternal/singleflight.(Group).doCall(0x13443d0, 0xc42009a000, 0xcc7b5a, 0x9, 0xc4201302a0)\n    /usr/local/go/src/internal/singleflight/singleflight.go:93 +0x3c\ncreated by internal/singleflight.(Group).DoChan\n    /usr/local/go/src/internal/singleflight/singleflight.go:86 +0x339\ngoroutine 20 [syscall]:\nos/signal.signal_recv(0x0)\n    /usr/local/go/src/runtime/sigqueue.go:116 +0x157\nos/signal.loop()\n    /usr/local/go/src/os/signal/signal_unix.go:22 +0x22\ncreated by os/signal.init.1\n    /usr/local/go/src/os/signal/signal_unix.go:28 +0x41\ngoroutine 21 [chan receive]:\ngithub.com/google/cadvisor/vendor/github.com/golang/glog.(*loggingT).flushDaemon(0x13459c0)\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/vendor/github.com/golang/glog/glog.go:882 +0x7a\ncreated by github.com/google/cadvisor/vendor/github.com/golang/glog.init.1\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/vendor/github.com/golang/glog/glog.go:410 +0x21d\ngoroutine 52 [IO wait]:\nnet.runtime_pollWait(0x7fd5737308a0, 0x72, 0x4)\n    /usr/local/go/src/runtime/netpoll.go:160 +0x59\nnet.(pollDesc).wait(0xc4203d1fe0, 0x72, 0xc4204939d0, 0xc42006e0a0)\n    /usr/local/go/src/net/fd_poll_runtime.go:73 +0x38\nnet.(pollDesc).waitRead(0xc4203d1fe0, 0x12bc800, 0xc42006e0a0)\n    /usr/local/go/src/net/fd_poll_runtime.go:78 +0x34\nnet.(netFD).Read(0xc4203d1f80, 0xc42021f000, 0x1000, 0x1000, 0x0, 0x12bc800, 0xc42006e0a0)\n    /usr/local/go/src/net/fd_unix.go:243 +0x1a1\nnet.(conn).Read(0xc4200740b0, 0xc42021f000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/net.go:173 +0x70\nnet/http.(persistConn).Read(0xc420069200, 0xc42021f000, 0x1000, 0x1000, 0x4d4f70, 0xc420493b58, 0x4081fd)\n    /usr/local/go/src/net/http/transport.go:1261 +0x154\nbufio.(Reader).fill(0xc420220c60)\n    /usr/local/go/src/bufio/bufio.go:97 +0x10c\nbufio.(Reader).Peek(0xc420220c60, 0x1, 0xc420493bbd, 0x1, 0x0, 0xc4203b8240, 0x0)\n    /usr/local/go/src/bufio/bufio.go:129 +0x62\nnet/http.(persistConn).readLoop(0xc420069200)\n    /usr/local/go/src/net/http/transport.go:1418 +0x1a1\ncreated by net/http.(*Transport).dialConn\n    /usr/local/go/src/net/http/transport.go:1062 +0x4e9\ngoroutine 53 [select]:\nnet/http.(persistConn).writeLoop(0xc420069200)\n    /usr/local/go/src/net/http/transport.go:1646 +0x3bd\ncreated by net/http.(Transport).dialConn\n    /usr/local/go/src/net/http/transport.go:1063 +0x50e\nfatal error: unexpected signal during runtime execution\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x47 pc=0x7fdf294ce60d]\nruntime stack:\nruntime.throw(0xcdd37b, 0x2a)\n    /usr/local/go/src/runtime/panic.go:566 +0x95\nruntime.sigpanic()\n    /usr/local/go/src/runtime/sigpanic_unix.go:12 +0x2cc\ngoroutine 38 [syscall, locked to thread]:\nruntime.cgocall(0xa551f0, 0xc420023df8, 0xc400000000)\n    /usr/local/go/src/runtime/cgocall.go:131 +0x110 fp=0xc420023db0 sp=0xc420023d70\nnet._C2func_getaddrinfo(0x7fdf240008c0, 0x0, 0xc4201c09c0, 0xc420026058, 0x0, 0x0, 0x0)\n    ??:0 +0x68 fp=0xc420023df8 sp=0xc420023db0\nnet.cgoLookupIPCNAME(0xcc7b5a, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/cgo_unix.go:146 +0x37c fp=0xc420023f18 sp=0xc420023df8\nnet.cgoIPLookup(0xc42020e1e0, 0xcc7b5a, 0x9)\n    /usr/local/go/src/net/cgo_unix.go:198 +0x4d fp=0xc420023fa8 sp=0xc420023f18\nruntime.goexit()\n    /usr/local/go/src/runtime/asm_amd64.s:2086 +0x1 fp=0xc420023fb0 sp=0xc420023fa8\ncreated by net.cgoLookupIP\n    /usr/local/go/src/net/cgo_unix.go:208 +0xb4\ngoroutine 1 [select]:\nnet.lookupIPContext(0x12c2500, 0xc42020e120, 0xcc7b5a, 0x9, 0x0, 0x0, 0x0, 0x0, 0xc42025f390)\n    /usr/local/go/src/net/lookup.go:122 +0x7bc\nnet.internetAddrList(0x12c2500, 0xc42020e120, 0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x0, 0x0, 0x0, 0xed0878281, ...)\n    /usr/local/go/src/net/ipsock.go:241 +0x5e0\nnet.resolveAddrList(0x12c2500, 0xc42020e120, 0xcbd267, 0x4, 0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x0, 0x0, ...)\n    /usr/local/go/src/net/dial.go:179 +0x106\nnet.(Dialer).DialContext(0xc42025f790, 0x12c24c0, 0xc4200143d8, 0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x0, 0x0, 0x0, ...)\n    /usr/local/go/src/net/dial.go:329 +0x238\nnet.(Dialer).Dial(0xc42025f790, 0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x2, 0xc42036a040, 0x48, 0xc42025f850)\n    /usr/local/go/src/net/dial.go:282 +0x75\nnet.DialTimeout(0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x77359400, 0xc, 0xc42029c5f2, 0x7, 0xc4202d1f80)\n    /usr/local/go/src/net/dial.go:268 +0x9d\ngithub.com/google/cadvisor/container/rkt.Client.func1()\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/container/rkt/client.go:44 +0x69\nsync.(*Once).Do(0x1369b30, 0xd3aac0)\n    /usr/local/go/src/sync/once.go:44 +0xdb\ngithub.com/google/cadvisor/container/rkt.Client(0xc42029c910, 0x9, 0x0, 0x0)\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/container/rkt/client.go:79 +0x39\ngithub.com/google/cadvisor/container/rkt.RktPath(0xc42024e170, 0xa, 0xc4201de160, 0x15)\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/container/rkt/client.go:85 +0x29\ngithub.com/google/cadvisor/manager.New(0xc4201e2100, 0x12c6520, 0x1369a90, 0xdf8475800, 0x12b9c01, 0xc42039f290, 0xc42039f3b0, 0x0, 0x0, 0x0, ...)\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/manager/manager.go:149 +0x18b\nmain.main()\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/cadvisor.go:127 +0x1e0\ngoroutine 17 [syscall, locked to thread]:\nruntime.goexit()\n    /usr/local/go/src/runtime/asm_amd64.s:2086 +0x1\ngoroutine 37 [select]:\nnet.cgoLookupIP(0x12c2500, 0xc42020e120, 0xcc7b5a, 0x9, 0xcc283b, 0x9, 0xc4200197e0, 0xc420018a00, 0xc420023ea0, 0x40848e)\n    /usr/local/go/src/net/cgo_unix.go:209 +0x2f5\nnet.lookupIP(0x12c2500, 0xc42020e120, 0xcc7b5a, 0x9, 0x0, 0x0, 0xc4200105f0, 0xc4200ce680, 0xc42006c238)\n    /usr/local/go/src/net/lookup_unix.go:70 +0xf9\nnet.glob..func11(0x12c2500, 0xc42020e120, 0xd3ba38, 0xcc7b5a, 0x9, 0x4081fd, 0xb892a0, 0xc42006c1e0, 0x4125bc, 0xc42006c238)\n    /usr/local/go/src/net/hook.go:19 +0x52\nnet.lookupIPContext.func1(0x408cbb, 0xc42006c238, 0x0, 0x6)\n    /usr/local/go/src/net/lookup.go:119 +0x5c\ninternal/singleflight.(Group).doCall(0x13443d0, 0xc4203da000, 0xcc7b5a, 0x9, 0xc4201c04e0)\n    /usr/local/go/src/internal/singleflight/singleflight.go:93 +0x3c\ncreated by internal/singleflight.(Group).DoChan\n    /usr/local/go/src/internal/singleflight/singleflight.go:86 +0x339\ngoroutine 6 [syscall]:\nos/signal.signal_recv(0x0)\n    /usr/local/go/src/runtime/sigqueue.go:116 +0x157\nos/signal.loop()\n    /usr/local/go/src/os/signal/signal_unix.go:22 +0x22\ncreated by os/signal.init.1\n    /usr/local/go/src/os/signal/signal_unix.go:28 +0x41\ngoroutine 7 [chan receive]:\ngithub.com/google/cadvisor/vendor/github.com/golang/glog.(*loggingT).flushDaemon(0x13459c0)\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/vendor/github.com/golang/glog/glog.go:882 +0x7a\ncreated by github.com/google/cadvisor/vendor/github.com/golang/glog.init.1\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/vendor/github.com/golang/glog/glog.go:410 +0x21d\ngoroutine 26 [IO wait]:\nnet.runtime_pollWait(0x7fdf285b27b8, 0x72, 0x4)\n    /usr/local/go/src/runtime/netpoll.go:160 +0x59\nnet.(pollDesc).wait(0xc4203d8370, 0x72, 0xc4200359d0, 0xc420014140)\n    /usr/local/go/src/net/fd_poll_runtime.go:73 +0x38\nnet.(pollDesc).waitRead(0xc4203d8370, 0x12bc800, 0xc420014140)\n    /usr/local/go/src/net/fd_poll_runtime.go:78 +0x34\nnet.(netFD).Read(0xc4203d8310, 0xc420278000, 0x1000, 0x1000, 0x0, 0x12bc800, 0xc420014140)\n    /usr/local/go/src/net/fd_unix.go:243 +0x1a1\nnet.(conn).Read(0xc420362130, 0xc420278000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/net.go:173 +0x70\nnet/http.(persistConn).Read(0xc42000b400, 0xc420278000, 0x1000, 0x1000, 0x4d4f70, 0xc420035b58, 0x4081fd)\n    /usr/local/go/src/net/http/transport.go:1261 +0x154\nbufio.(Reader).fill(0xc42006cde0)\n    /usr/local/go/src/bufio/bufio.go:97 +0x10c\nbufio.(Reader).Peek(0xc42006cde0, 0x1, 0xc420035bbd, 0x1, 0x0, 0xc42027a3c0, 0x0)\n    /usr/local/go/src/bufio/bufio.go:129 +0x62\nnet/http.(persistConn).readLoop(0xc42000b400)\n    /usr/local/go/src/net/http/transport.go:1418 +0x1a1\ncreated by net/http.(*Transport).dialConn\n    /usr/local/go/src/net/http/transport.go:1062 +0x4e9\ngoroutine 27 [select]:\nnet/http.(persistConn).writeLoop(0xc42000b400)\n    /usr/local/go/src/net/http/transport.go:1646 +0x3bd\ncreated by net/http.(Transport).dialConn\n    /usr/local/go/src/net/http/transport.go:1063 +0x50e\nfatal error: unexpected signal during runtime execution\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x47 pc=0x7f5bf304960d]\nruntime stack:\nruntime.throw(0xcdd37b, 0x2a)\n    /usr/local/go/src/runtime/panic.go:566 +0x95\nruntime.sigpanic()\n    /usr/local/go/src/runtime/sigpanic_unix.go:12 +0x2cc\ngoroutine 41 [syscall, locked to thread]:\nruntime.cgocall(0xa551f0, 0xc42041edf8, 0xc400000000)\n    /usr/local/go/src/runtime/cgocall.go:131 +0x110 fp=0xc42041edb0 sp=0xc42041ed70\nnet._C2func_getaddrinfo(0x7f5be80008c0, 0x0, 0xc420348c30, 0xc420026150, 0x0, 0x0, 0x0)\n    ??:0 +0x68 fp=0xc42041edf8 sp=0xc42041edb0\nnet.cgoLookupIPCNAME(0xcc7b5a, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/cgo_unix.go:146 +0x37c fp=0xc42041ef18 sp=0xc42041edf8\nnet.cgoIPLookup(0xc4200643c0, 0xcc7b5a, 0x9)\n    /usr/local/go/src/net/cgo_unix.go:198 +0x4d fp=0xc42041efa8 sp=0xc42041ef18\nruntime.goexit()\n    /usr/local/go/src/runtime/asm_amd64.s:2086 +0x1 fp=0xc42041efb0 sp=0xc42041efa8\ncreated by net.cgoLookupIP\n    /usr/local/go/src/net/cgo_unix.go:208 +0xb4\ngoroutine 1 [select]:\nnet.lookupIPContext(0x12c2500, 0xc4200642a0, 0xcc7b5a, 0x9, 0x0, 0x0, 0x0, 0x0, 0xc420187390)\n    /usr/local/go/src/net/lookup.go:122 +0x7bc\nnet.internetAddrList(0x12c2500, 0xc4200642a0, 0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x0, 0x0, 0x0, 0xed0878282, ...)\n    /usr/local/go/src/net/ipsock.go:241 +0x5e0\nnet.resolveAddrList(0x12c2500, 0xc4200642a0, 0xcbd267, 0x4, 0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x0, 0x0, ...)\n    /usr/local/go/src/net/dial.go:179 +0x106\nnet.(Dialer).DialContext(0xc420187790, 0x12c24c0, 0xc420068338, 0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x0, 0x0, 0x0, ...)\n    /usr/local/go/src/net/dial.go:329 +0x238\nnet.(Dialer).Dial(0xc420187790, 0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x2, 0xc4202de040, 0x48, 0xc420187850)\n    /usr/local/go/src/net/dial.go:282 +0x75\nnet.DialTimeout(0xcbcb90, 0x3, 0xcc7b5a, 0xf, 0x77359400, 0xc, 0xc4202c05f2, 0x7, 0xc4203b4ea0)\n    /usr/local/go/src/net/dial.go:268 +0x9d\ngithub.com/google/cadvisor/container/rkt.Client.func1()\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/container/rkt/client.go:44 +0x69\nsync.(*Once).Do(0x1369b30, 0xd3aac0)\n    /usr/local/go/src/sync/once.go:44 +0xdb\ngithub.com/google/cadvisor/container/rkt.Client(0xc4202c0860, 0x9, 0x0, 0x0)\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/container/rkt/client.go:79 +0x39\ngithub.com/google/cadvisor/container/rkt.RktPath(0xc42024e170, 0xa, 0xc420210460, 0x15)\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/container/rkt/client.go:85 +0x29\ngithub.com/google/cadvisor/manager.New(0xc420246080, 0x12c6520, 0x1369a90, 0xdf8475800, 0x12b9c01, 0xc42020bd40, 0xc42020be60, 0x0, 0x0, 0x0, ...)\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/manager/manager.go:149 +0x18b\nmain.main()\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/cadvisor.go:127 +0x1e0\ngoroutine 17 [syscall, locked to thread]:\nruntime.goexit()\n    /usr/local/go/src/runtime/asm_amd64.s:2086 +0x1\ngoroutine 34 [syscall]:\nos/signal.signal_recv(0x0)\n    /usr/local/go/src/runtime/sigqueue.go:116 +0x157\nos/signal.loop()\n    /usr/local/go/src/os/signal/signal_unix.go:22 +0x22\ncreated by os/signal.init.1\n    /usr/local/go/src/os/signal/signal_unix.go:28 +0x41\ngoroutine 35 [chan receive]:\ngithub.com/google/cadvisor/vendor/github.com/golang/glog.(*loggingT).flushDaemon(0x13459c0)\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/vendor/github.com/golang/glog/glog.go:882 +0x7a\ncreated by github.com/google/cadvisor/vendor/github.com/golang/glog.init.1\n    /usr/local/google/home/dashpole/go/src/github.com/google/cadvisor/vendor/github.com/golang/glog/glog.go:410 +0x21d\ngoroutine 30 [IO wait]:\nnet.runtime_pollWait(0x7f5bf32a43e8, 0x72, 0x4)\n    /usr/local/go/src/runtime/netpoll.go:160 +0x59\nnet.(pollDesc).wait(0xc420376450, 0x72, 0xc4200359d0, 0xc4200680a0)\n    /usr/local/go/src/net/fd_poll_runtime.go:73 +0x38\nnet.(pollDesc).waitRead(0xc420376450, 0x12bc800, 0xc4200680a0)\n    /usr/local/go/src/net/fd_poll_runtime.go:78 +0x34\nnet.(netFD).Read(0xc4203763f0, 0xc42035a000, 0x1000, 0x1000, 0x0, 0x12bc800, 0xc4200680a0)\n    /usr/local/go/src/net/fd_unix.go:243 +0x1a1\nnet.(conn).Read(0xc42012c098, 0xc42035a000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/net.go:173 +0x70\nnet/http.(persistConn).Read(0xc4200d9300, 0xc42035a000, 0x1000, 0x1000, 0x4d4f70, 0xc420035b58, 0x4081fd)\n    /usr/local/go/src/net/http/transport.go:1261 +0x154\nbufio.(Reader).fill(0xc420132c60)\n    /usr/local/go/src/bufio/bufio.go:97 +0x10c\nbufio.(Reader).Peek(0xc420132c60, 0x1, 0xc420035bbd, 0x1, 0x0, 0xc4202a41e0, 0x0)\n    /usr/local/go/src/bufio/bufio.go:129 +0x62\nnet/http.(persistConn).readLoop(0xc4200d9300)\n    /usr/local/go/src/net/http/transport.go:1418 +0x1a1\ncreated by net/http.(*Transport).dialConn\n    /usr/local/go/src/net/http/transport.go:1062 +0x4e9\ngoroutine 31 [select]:\nnet/http.(persistConn).writeLoop(0xc4200d9300)\n    /usr/local/go/src/net/http/transport.go:1646 +0x3bd\ncreated by net/http.(Transport).dialConn\n    /usr/local/go/src/net/http/transport.go:1063 +0x50e\ngoroutine 40 [select]:\nnet.cgoLookupIP(0x12c2500, 0xc4200642a0, 0xcc7b5a, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/cgo_unix.go:209 +0x2f5\nnet.lookupIP(0x12c2500, 0xc4200642a0, 0xcc7b5a, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/lookup_unix.go:70 +0xf9\nnet.glob..func11(0x12c2500, 0xc4200642a0, 0xd3ba38, 0xcc7b5a, 0x9, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/hook.go:19 +0x52\nnet.lookupIPContext.func1(0x0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/lookup.go:119 +0x5c\ninternal/singleflight.(Group).doCall(0x13443d0, 0xc4202c20f0, 0xcc7b5a, 0x9, 0xc4203b50b0)\n    /usr/local/go/src/internal/singleflight/singleflight.go:93 +0x3c\ncreated by internal/singleflight.(Group).DoChan\n    /usr/local/go/src/internal/singleflight/singleflight.go:86 +0x339\n.\ndocker info\nContainers: 104\n Running: 4\n Paused: 0\n Stopped: 100\nImages: 29\nServer Version: 17.03.1-ce\nStorage Driver: overlay\n Backing Filesystem: xfs\n Supports d_type: false\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: bridge host macvlan null overlay\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nInit Binary: docker-init\ncontainerd version: 4ab9917febca54791c5f071a9d1f404867857fcc\nrunc version: 54296cf40ad8143b62dbcaa1d90e520a2136ddfe\ninit version: 949e6fa\nSecurity Options:\n seccomp\n  Profile: default\nKernel Version: 3.10.0-327.el7.x86_64\nOperating System: CentOS Linux 7 (Core)\nOSType: linux\nArchitecture: x86_64\nCPUs: 4\nTotal Memory: 7.64 GiB\nName: sryslave1\nID: XGTG:GSYB:ESVE:CMV7:PLKX:5OTL:SRRK:CX4H:TMUJ:NKDX:APW3:OO53\nDocker Root Dir: /data/docker\nDebug Mode (client): false\nDebug Mode (server): false\nRegistry: https://index.docker.io/v1/\nWARNING: bridge-nf-call-iptables is disabled\nWARNING: bridge-nf-call-ip6tables is disabled\nExperimental: false\nInsecure Registries:\n 192.168.1.213\n 192.168.1.213:5001\n 192.168.1.213:5002\n 127.0.0.0/8\nLive Restore Enabled: true\n```. Use the official image and install findutils to run normally\ndockerfiles\n```\ngoogle-cadvisor:v0.25.0\nMAINTAINER pro zpang\ntime\nRUN apk add --update tzdata && rm -rf /var/cache/apk/*\nRUN cp /usr/share/zoneinfo/Hongkong /etc/localtime\nadd gnu find\nRUN apk add --update findutils && rm -rf /var/cache/apk/*\n```. cadvisor api has data\ndiskio: {\nio_service_bytes: [\n{\nmajor: 253,\nminor: 0,\nstats: {\nAsync: 6170981376,\nRead: 6170981376,\nSync: 122608486912,\nTotal: 128779468288,\nWrite: 122608486912\n}\n},\n{\nmajor: 253,\nminor: 1,\nstats: {\nAsync: 348160,\nRead: 0,\nSync: 0,\nTotal: 348160,\nWrite: 348160\n}\n},\n{\nmajor: 8,\nminor: 0,\nstats: {\nAsync: 6171354112,\nRead: 6170981376,\nSync: 122608486912,\nTotal: 128779841024,\nWrite: 122608859648\n}\n}\n],\nio_serviced: [\n{\nmajor: 253,\nminor: 1,\nstats: {\nAsync: 85,\nRead: 0,\nSync: 0,\nTotal: 85,\nWrite: 85\n}\n},\n{\nmajor: 8,\nminor: 0,\nstats: {\nAsync: 1404026,\nRead: 1403935,\nSync: 4360625,\nTotal: 5764651,\nWrite: 4360716\n}\n},\n{\nmajor: 253,\nminor: 0,\nstats: {\nAsync: 1403935,\nRead: 1403935,\nSync: 4360625,\nTotal: 5764560,\nWrite: 4360625\n}\n}\n]\n},. Please help answer\uff0cthx. docker run \ndocker run -d \\\n                --privileged=true \\\n                --log-driver=journald \\\n                --name $CONTAINER_NAME --net host --restart always \\\n                -v /:/rootfs:ro \\\n                -v /var/run:/var/run:rw \\\n                -v /sys:/sys:ro \\\n                -v /data/docker/:/var/lib/docker:ro \\\n                $CADVISOR_IMAGE \\\n                -logtostderr \\\n                --listen_ip=$LOCAL_IP \\\n                --port=5014. Docker Image: google/cadvisor:v0.26.1. ",
    "nenadalm": "I have the same issue on Fedora 25 with running\nshell\n$ sudo docker run   --volume=/:/rootfs:ro   --volume=/var/run:/var/run:rw   --volume=/sys:/sys:ro   --volume=/var/lib/docker/:/var/lib/docker:ro   --publish=8080:8080    --name=cadvisor --rm  --privileged=true  google/cadvisor:latest\ncontainer is able to start without --volume=/var/run:/var/run:rw as mentioned here: https://github.com/google/cadvisor/issues/1646#issuecomment-298817234\n. I have the same issue on Fedora 25 with running\nshell\n$ sudo docker run   --volume=/:/rootfs:ro   --volume=/var/run:/var/run:rw   --volume=/sys:/sys:ro   --volume=/var/lib/docker/:/var/lib/docker:ro   --publish=8080:8080    --name=cadvisor --rm  --privileged=true  google/cadvisor:latest\ncontainer is able to start without --volume=/var/run:/var/run:rw as mentioned here: https://github.com/google/cadvisor/issues/1646#issuecomment-298817234\n. ",
    "mikolatero": "Same problem on \"CentOS Linux release 7.3.1611 (Core) 3.10.0-514.21.2.el7.x86_64\" with cAdvisor v0.26.1 binary.\nConfirmed, with \"GODEBUG=netdns=go ./cadvisor\" run without any issue.. ",
    "Arezki1995": "I have the same issue on Ubuntu 18.0 \n-> solved by updating cadvisor. ",
    "pkrolikowski": "So any suggestion what can I do with it ?\n. ",
    "tcolgate": "Attempted in #1488 \n. /proc/net/stat may be a better source of a single counter\n. Please don't merge, I'll add the rx_queue and tx_queue, and think it could do with a bit ore error checking.\n. Fixed dropped to actuall do the right thing.\nAdd RxQueued and TxQueued\nAdded a test.. @dashpole anything else I need to do for this? Is it just a case of waiting for someone to merge it?. ",
    "rajivgandhi1": "Nope - doesnt work. same error. Tried both 0.24.1 and 0.25.0. The fix doesnt work for me tried both 0.24.1 and 0.25.0. Was this fixed for you? I am facing the same problem. ",
    "k8s-ci-robot": "Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line.\nRegular contributors should join the org to skip this step.\n. Jenkins GCE e2e failed for commit acb2426b117a6be535dfafa05e1fdfba7c4219a7. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line.\nRegular contributors should join the org to skip this step.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line.\nRegular contributors should join the org to skip this step.\n. Jenkins GCE e2e failed for commit c2948ba70824cce119baf2c881777b8b71cf1280. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.\n. Jenkins GCE e2e failed for commit 6b9266d88fa9a1822e60344060ebeeca1478ed22. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.\n. Jenkins GCE e2e failed for commit 3ce02d66908240995ae839cf5fa5a1c38622998d. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the kubernetes/test-infra repository.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the kubernetes/test-infra repository.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the kubernetes/test-infra repository.\n. Jenkins GCE e2e failed for commit 2d97e6f10bb8a92712ed00874b8048631474a6f9. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the kubernetes/test-infra repository.\n. Can a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. \nCan a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. \nCan a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @fzu-huang: you can't request testing unless you are a kubernetes member.\n\n\nIn response to [this comment](https://github.com/google/cadvisor/pull/1547#issuecomment-262423086):\n\n>@k8s-bot ok to test\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Jenkins GCE e2e failed for commit 8ff59f00a07c01d03db7b12f9fbb7842ded56ade. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.. \nCan a kubernetes member verify that this patch is reasonable to test? If so, please reply with \"@k8s-bot ok to test\" on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands will still work. Regular contributors should join the org to skip this step.\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @bakins. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @sykesm. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Jenkins GCE e2e failed for commit c7bd164f6dac093e35299f5dcbcb1b6b635c7195. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.\n\n\nIf you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @bamarni. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @andyxning. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nIf you have questions or suggestions related to this bot's behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Jenkins GCE e2e failed for commit 122d0276fdb611c5a54212ff85731738ee7203ce. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.\n\n\nIf you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Jenkins GCE e2e failed for commit 2426237fc81870c853662a2869239b63db848c08. Full PR test history.\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.\n\n\nIf you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Jenkins GCE e2e failed for commit 14fdb8935780ca53f207a51a40f059c37d585c87. Full PR test history. cc @derekwaynecarr\nThe magic incantation to run this job again is @k8s-bot test this. Please help us cut down flakes by linking to an open flake issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @dmrub. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @grokbot. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. @euank: The following test(s) failed:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\nJenkins GCE e2e | 6a36b4c43dca79785733eb51114be452e00a598d | link | @k8s-bot test this\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n test report \n. @pmorie: The following tests failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\nJenkins GCE e2e | 5ab941d4aec36a970c5aaacbfb5ec91af46cdfec | link | @k8s-bot test this\npull-cadvisor-e2e | 5ab941d4aec36a970c5aaacbfb5ec91af46cdfec | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. Hi @NickrenREN. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. @dashpole: The following test(s) failed:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-cadvisor-e2e | bc7d89fee4560dc41eb32cef7ad3a03f2cd9e675 | link | @k8s-bot pull-cadvisor-e2e test this\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. Hi @QianJin2013. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @bamarni. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. @bamarni: The following test(s) failed:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\nJenkins GCE e2e | ecd6536a57366ccc1230fbb2b8a584bdb6eb6bdf | link | @k8s-bot test this\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. Hi @cvlc. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @matcornic. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @matcornic. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @oopschen. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @timchenxiaoyu. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @harsheetbhatia. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/prow/commands.md).\n\n. Hi @vincentdaniel. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. @vincentdaniel: The following test(s) failed:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\nJenkins GCE e2e | 5bc8287a48c492620a5abe6ca13ef269d6c86f2f | link | @k8s-bot test this\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. Hi @mkumatag. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @mkumatag. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @edevil. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. @edevil: The following tests failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\nJenkins GCE e2e | d51cbd47aca379f2bb158a4bf39e13accb5c70a3 | link | @k8s-bot test this\npull-cadvisor-e2e | 72fbbaae9c3e5ddf8796bc55fb50db95eeedf7b2 | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. Hi @kitt1987. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @basvdlei. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @rikatz. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @WIZARD-CXY. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-request-commands.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @Christopher-Bui. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @Christopher-Bui. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @Leibniz137. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @flavio. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @Colstuwjx. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @Colstuwjx. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @apilloud. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @stelzner. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @CaoShuFeng. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with @k8s-bot ok to test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @luanna97. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @jianzi123. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @jianzi123. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @lborguetti. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @naveenkandakur. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. @sjenning: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-cadvisor-e2e | b06faf01b4b6b52528429d714cb55dd3f707e0dd | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. Hi @tianshapjq. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @jharshman. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @ZhiqinYang. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @allencloud. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. @mtaufen: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-cadvisor-e2e | 2e6ced5701b77302043f4a3353643d86f178a1e9 | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. Hi @AlexJakeGreen. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @cirocosta. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @yongtang. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @bsingr. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @bakins. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @majst01. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @majst01. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @majst01. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. Hi @tklauser. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n. @dashpole: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-cadvisor-e2e | 8aea7ea60c647145329165e32887cf78e9fe54d1 | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. @dashpole: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-cadvisor-e2e | bf0fbec5de81081f944cefb1f4bb8817d008244c | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. Hi @majst01. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @majst01: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-cadvisor-e2e | af8cc54701f81f96bde7ab80c353812dce480bd7 | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. Hi @kant. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @abhi. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @abhi. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @sentinelt. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @mRoca. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @jsravn. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @brian-brazil. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @jsravn. Thanks for your PR.\nI'm waiting for a kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @jsravn. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @fzu-huang. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @fzu-huang: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-cadvisor-e2e | 0e797eae5d2b5a53fccdb08c6d270177546c1361 | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. @dashpole: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-cadvisor-e2e | 6842d387aae8f02d42a694f21214a3e771712655 | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. @vikaschoudhary16: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-cadvisor-e2e | 0804052eff8b192e4d473175e8216944ab6fad68 | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. Hi @donghwicha. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @donghwicha: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-cadvisor-e2e | 36e49fc7badb3ec8e24129615fc0985c49538ea5 | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://github.com/kubernetes/test-infra/blob/master/commands.md).\n\n test report \n. Hi @fzu-huang. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @fzu-huang: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-cadvisor-e2e | 3650be28fb8720e452c21779e458c6d435389ea1 | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. Hi @jsravn. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @nielsole. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @gogeof. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @bhuvanchandra. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @fzu-huang. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @fzu-huang: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-cadvisor-e2e | f1d0c9553c1380731b60cea7592a9f728b34963f | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. Hi @adrien-f. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @alicek106. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @alicek106: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-cadvisor-e2e | d620d74c801b3fcb03ac781271803f586c40bf84 | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. Hi @IvanVlasic. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @tianshapjq. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @tianshapjq. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @mavidser. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @mariusbld. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @rsevilla87. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/devel/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @dashpole: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-cadvisor-e2e | da75abf80abba10bd90c303e0ec8f16f830dc6a3 | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. Hi @niedbalski. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @niedbalski: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-cadvisor-e2e | a36df4a88f61eaa8598cb5d824afe7231c64e605 | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. Hi @ElfoLiNk. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @ElfoLiNk. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @gvessere. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @sashankreddya. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @MaximilianMeister. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @dashpole: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-cadvisor-e2e | 9ab811db3585351e9f91ad5ab6c87735ecccae9e | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. @dashpole: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-cadvisor-e2e | 6bcf74bf5dd4926df3bc45caaea9786b63021e68 | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. Hi @moooofly. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @jaloren. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @moooofly. Thanks for your PR.\nI'm waiting for a kubernetes or google member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @ringtail. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @lubinszARM. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @mannychang. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @tghosgor. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @ringtail. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @hangongithub. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @hangongithub. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @chenchun. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @lubinszARM. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @lubinsz. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @lubinsz: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-cadvisor-e2e | 9cf1d60eaaa262a31b7ff9d12e3e8869908e16ab | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. Hi @tiagoapimenta. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @mikkeloscar. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @lubinszARM. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @tghosgor. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @tghosgor. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @ElfoLiNk. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @changlan. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @Betula-L. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @igorljubuncic. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @lubinszARM. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @RobertKrawitz. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @Betula-L. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @mikkeloscar. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @namreg. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @dtrejod. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @gaorong. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @gaorong. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @viberan. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @CodeLingoBot. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @CodeLingoBot: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-cadvisor-e2e | 85fa6b678798486f879b3172ce0794189621ab07 | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. Hi @nekop. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @namreg. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @nzoueidi. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @nzoueidi: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-cadvisor-e2e | 7600e10ef6077e42258bd7feb92cb20f1e803073 | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. Hi @NolanChan. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @ddtmachado. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @ddtmachado: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-cadvisor-e2e | cf8dfa34039353d4a9bd75d26aba7be393efe522 | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. Hi @blakebarnett. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @blakebarnett. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. Hi @ivenk. Thanks for your PR.\nI'm waiting for a google or kubernetes member to verify that this patch is reasonable to test. If it is, they should reply with /ok-to-test on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step.\nOnce the patch is verified, the new status will be reflected by the ok-to-test label.\nI understand the commands that are listed here.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n\n. @ivenk: The following test failed, say /retest to rerun them all:\nTest name | Commit | Details | Rerun command\n--- | --- | --- | ---\npull-cadvisor-e2e | 1cd03ce56997291db7e81644e73114a77ce0c482 | link | /test pull-cadvisor-e2e\nFull PR test history. Your PR dashboard. Please help us cut down on flakes by linking to an open issue when you hit one in your PR.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. I understand the commands that are listed [here](https://go.k8s.io/bot-commands).\n\n test report \n. ",
    "spxtr": "@k8s-bot test this\nIt didn't have permissions to set the status, but the test ran :P\n. @k8s-bot test this\n. ",
    "jmat6": "@derekwaynecarr , any updates on this? Do you think it might be a problem because I am missing some flags while running the docker container?\n. ",
    "l15k4": "I've got the same problem on : \nDistributor ID: Ubuntu\nDescription:    Ubuntu 16.04.2 LTS\nRelease:    16.04\nCodename:   xenial\nIdentical log messages as in this issue. I'm using the latest google/cadvisor:latest, privileged mode doesn't help. I run it as root.... ",
    "LeJav": "OS is CentOS 7.2\ncadvisor runs in docker container\nHonestly, I have given up with cadvisor due to this problem.\n. I thank you for your feedback.\nBut as this ticket is now old and I do not use anymore cadvisor, I close now this issue.\nAnyway, thanks for your good work and your support.. ",
    "hanikesn": "This generates about 18 000 msg per hour on a cluster with 125 pods.. Doing some further investigation it looks like https://github.com/google/cadvisor/blob/master/info/v1/container.go#L256 could give us cpu saturation information, though it won't be as accurate as blocked time is not accumulated like with time spent waiting on a runqueue.\nAnd this is the removal commit: https://github.com/google/cadvisor/commit/14844dbedddabbd24aa96b35fd2051cbf788a919#diff-4a183213ad22db20d703e523b9c4077e. @nielsole I don't think those graphs are comparable, when summing up those container stats there's some double accounting going on. I'd recommend doing it for a single container.. ",
    "shkrid": "same problem. ",
    "jaylinski": "Can confirm, happens for me on a DigitalOcean Droplet with Ubuntu 16.04. I'm using docker-compose:\nDec 21 09:24:55 xxx dockerd[20414]: time=\"2016-12-21T09:24:55.980184307-05:00\" level=error msg=\"Handler for GET /containers/50fbd7cddccf663af19cf6704df5937817107643124c517f9c07974f8a5de852/json returned error: No such container: 50fbd7cddccf663af19cf6704df5937817107643124c517f9c07974f8a5de852\"\nDec 21 09:24:55 xxx dockerd[20414]: time=\"2016-12-21T09:24:55.981949345-05:00\" level=error msg=\"Handler for GET /containers/e86431e3968ad371e0ef2bad007cbfa84e529d67dcb72d0b0c7f2bd0c6ec52c1/json returned error: No such container: e86431e3968ad371e0ef2bad007cbfa84e529d67dcb72d0b0c7f2bd0c6ec52c1\"\nDec 21 09:24:55 xxx dockerd[20414]: time=\"2016-12-21T09:24:55.982386727-05:00\" level=error msg=\"Handler for GET /containers/62784a73ffe9295953ddc22dd695c1de2d84cae4b464350408bd97c7daf88446/json returned error: No such container: 62784a73ffe9295953ddc22dd695c1de2d84cae4b464350408bd97c7daf88446\"\nDec 21 09:24:55 xxx dockerd[20414]: time=\"2016-12-21T09:24:55.982768326-05:00\" level=error msg=\"Handler for GET /containers/063db19abeed6c3a4f10d99ab5718405ac6a976ac854805528ec6258971e2b40/json returned error: No such container: 063db19abeed6c3a4f10d99ab5718405ac6a976ac854805528ec6258971e2b40\". @fahimeh2010 I can't tell yet. I'm waiting for the fix to be released in a stable image (https://hub.docker.com/r/google/cadvisor/tags/).. ",
    "maikotz": "same problem. any updates on how to prevent this? . ",
    "x3rus": "Hi folks , \nI found this thread and I had the same issue : \nlevel=error msg=\"Handler for GET /containers/7586102c883e87f63588bf45a761edaf39f3338e63c1646fbc4e39a8534bccaa/json returned error: No such container: 7586102c883e87f63588bf45a761edaf39f3338e63c1646fbc4e39a8534bccaa\"\nFor 24 hours I had 28,800 error message , of course not with the same container ID :P , I use cadvisor as a container cadvisor:latest , I switch for  cadvisor:canary (3 Mars 2017) and it fix my problem .\nNo more error log :) \nI'm very happy, so THANKS a lot for your work !\nI know the issue is close , but for other people like me who found this thread this FIX WORK !\nDo you have any idea when this fix will be include in the stable release ?. ",
    "magicrobotmonkey": "I'm having the same issue. Might I suggest also/instead allowing to collapse to one total cpu usage per container, rather than one per core?\n. Actually, I realized that container_cpu_system_seconds_total gives the per-system usage. This means a workaround for this is to just drop the per cpu metrics like so:\nyaml\n- job_name: 'mesos-compute-cadvisor'\n  metric_relabel_configs:\n    - source_labels: [cpu]\n      regex: (cpu.*)\n      action: drop\n. Yes, you're absolutely correct. This workaround only saves you storage space/io on your prometheus host. \n. Actually I think I'm also wrong about container_cpu_system_seconds_total. It appears it just gives system time. You'd have to add container_cpu_user_seconds_total to it to replace summing container_cpu_user_seconds_total\n. This works for me and is much more performant than 40 cores * number of containers. \nsum(rate(container_cpu_system_seconds_total{pypeline_name=~'test'}[1m]) + rate(container_cpu_user_seconds_total{pypeline_name=~'test'}[1m])) by (pypeline_task)\n. ",
    "piontec": "OK, but this filtering is outside the cAdvisor, right (prometheus in this case)? So the data is there and it still needs to be downloaded - and gives me over 10MB of unnecessary data per each poll. I think it also makes cAdvisor to use considerable amount of CPU power on the host.\n. ",
    "bboreham": "I have the same problem.  I have looked at the replacement via container_cpu_system_seconds_total and container_cpu_user_seconds_total and find they do not add up to the same number.  Close, but a bit less.  Possibly due to rounding?\nI would like to propose that the CpuStats.CpuUsage.TotalUsage figure from libcontainer be exposed as a Prometheus metric by cAdvisor, and that an option be added to disable the per-cpu metrics.\nThe new metric could be named container_cpu_seconds_total ?  Any better suggestions?. For the sake of anyone coming across this, I'll note this resulted in the output being reduced to some random subset of all metrics: it \"continues\" but it still drops the ones it doesn't like.  See #1704 for more discussion.. Seeing the same high-level symptoms: for me it's the labels that are missing, not the containers.  And when the labels are missing I get a lot more lines for other cgroups.  \nI'm running kuberntes 1.7.3 on Ubuntu Linux ip-172-20-3-76 4.4.0-92-generic #115-Ubuntu SMP Thu Aug 10 09:04:33 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\nTwo examples from the same kubelet on the same machine, a few seconds apart:\nExample 1:\n```\ncurl -s 127.0.0.1:10255/metrics/cadvisor | grep container_cpu_user_seconds_total\nHELP container_cpu_user_seconds_total Cumulative user cpu time consumed in seconds.\nTYPE container_cpu_user_seconds_total counter\ncontainer_cpu_user_seconds_total{id=\"/\"} 3.6788206e+06\ncontainer_cpu_user_seconds_total{id=\"/init.scope\"} 69.43\ncontainer_cpu_user_seconds_total{id=\"/kubepods\"} 3.49797001e+06\ncontainer_cpu_user_seconds_total{id=\"/kubepods/besteffort\"} 162742.99\ncontainer_cpu_user_seconds_total{id=\"/kubepods/besteffort/pod13eacef1-8342-11e7-9534-0a97ed59c75e\"} 69.47\ncontainer_cpu_user_seconds_total{id=\"/kubepods/besteffort/pod5f43c843-7db5-11e7-9534-0a97ed59c75e\"} 703.82\ncontainer_cpu_user_seconds_total{id=\"/kubepods/besteffort/pod6b2e45d7-7db5-11e7-9534-0a97ed59c75e\"} 70.04\ncontainer_cpu_user_seconds_total{id=\"/kubepods/besteffort/pod94ad7fd4-8351-11e7-9534-0a97ed59c75e\"} 363.18\ncontainer_cpu_user_seconds_total{id=\"/kubepods/besteffort/pod965b711b-8262-11e7-9534-0a97ed59c75e\"} 5.9\ncontainer_cpu_user_seconds_total{id=\"/kubepods/besteffort/podd2b82b9c-8355-11e7-9534-0a97ed59c75e\"} 35733.13\ncontainer_cpu_user_seconds_total{id=\"/kubepods/besteffort/pode4c7eace-8352-11e7-9534-0a97ed59c75e\"} 150.78\ncontainer_cpu_user_seconds_total{id=\"/kubepods/burstable\"} 3.33525364e+06\ncontainer_cpu_user_seconds_total{id=\"/kubepods/burstable/pod53559243-7db5-11e7-9534-0a97ed59c75e\"} 276743.3\ncontainer_cpu_user_seconds_total{id=\"/kubepods/burstable/pod55af46fe-834c-11e7-9534-0a97ed59c75e\"} 105958.75\ncontainer_cpu_user_seconds_total{id=\"/kubepods/burstable/pod7964f3e653196edee64f6bad72589dee\"} 366.77\ncontainer_cpu_user_seconds_total{id=\"/kubepods/burstable/pod7964f3e653196edee64f6bad72589dee/8d2eb34023eab40d08ba6e4be149e315c3844749f8321f44be2dcda024534757/\\\"\\\"\"} 366.65\ncontainer_cpu_user_seconds_total{id=\"/kubepods/burstable/podc7af9dff-8364-11e7-9534-0a97ed59c75e\"} 434974.97\ncontainer_cpu_user_seconds_total{id=\"/kubepods/burstable/podcb5d3cc0-8364-11e7-9534-0a97ed59c75e\"} 891563\ncontainer_cpu_user_seconds_total{id=\"/kubepods/burstable/podcf18531c-8365-11e7-9534-0a97ed59c75e\"} 17225.18\ncontainer_cpu_user_seconds_total{id=\"/system.slice\"} 151482.27\ncontainer_cpu_user_seconds_total{id=\"/system.slice/acpid.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/apparmor.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/apport.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/atd.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/cgroupfs-mount.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/cloud-config.service\"} 0.32\ncontainer_cpu_user_seconds_total{id=\"/system.slice/cloud-final.service\"} 0.37\ncontainer_cpu_user_seconds_total{id=\"/system.slice/cloud-init-local.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/cloud-init.service\"} 0.63\ncontainer_cpu_user_seconds_total{id=\"/system.slice/console-setup.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/cron.service\"} 25.49\ncontainer_cpu_user_seconds_total{id=\"/system.slice/dbus.service\"} 14.82\ncontainer_cpu_user_seconds_total{id=\"/system.slice/docker.service\"} 94117.92\ncontainer_cpu_user_seconds_total{id=\"/system.slice/ebtables.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/grub-common.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/ifup@cbr0.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/ifup@ens3.service\"} 0.79\ncontainer_cpu_user_seconds_total{id=\"/system.slice/irqbalance.service\"} 40.56\ncontainer_cpu_user_seconds_total{id=\"/system.slice/iscsid.service\"} 1.69\ncontainer_cpu_user_seconds_total{id=\"/system.slice/keyboard-setup.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/kmod-static-nodes.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/kubelet.service\"} 21323.06\ncontainer_cpu_user_seconds_total{id=\"/system.slice/lvm2-lvmetad.service\"} 8.94\ncontainer_cpu_user_seconds_total{id=\"/system.slice/lvm2-monitor.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/lxcfs.service\"} 0.37\ncontainer_cpu_user_seconds_total{id=\"/system.slice/lxd-containers.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/mdadm.service\"} 0.02\ncontainer_cpu_user_seconds_total{id=\"/system.slice/networking.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/ondemand.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/open-iscsi.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/polkitd.service\"} 3.63\ncontainer_cpu_user_seconds_total{id=\"/system.slice/rc-local.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/resolvconf.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/rsyslog.service\"} 100.82\ncontainer_cpu_user_seconds_total{id=\"/system.slice/setvtrgb.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/snapd.firstboot.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/snapd.service\"} 0.04\ncontainer_cpu_user_seconds_total{id=\"/system.slice/ssh.service\"} 51.39\ncontainer_cpu_user_seconds_total{id=\"/system.slice/system-getty.slice\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/system-serial\\x2dgetty.slice\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/systemd-journal-flush.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/systemd-journald.service\"} 489.31\ncontainer_cpu_user_seconds_total{id=\"/system.slice/systemd-logind.service\"} 3.02\ncontainer_cpu_user_seconds_total{id=\"/system.slice/systemd-modules-load.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/systemd-random-seed.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/systemd-remount-fs.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/systemd-sysctl.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/systemd-timesyncd.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/systemd-tmpfiles-setup-dev.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/systemd-tmpfiles-setup.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/systemd-udev-trigger.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/systemd-udevd.service\"} 0.52\ncontainer_cpu_user_seconds_total{id=\"/system.slice/systemd-update-utmp.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/systemd-user-sessions.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/system.slice/ufw.service\"} 0\ncontainer_cpu_user_seconds_total{id=\"/user.slice\"} 29270.98\n```\nExample 2:\n```\ncurl -s 127.0.0.1:10255/metrics/cadvisor | grep container_cpu_user_seconds_total\nHELP container_cpu_user_seconds_total Cumulative user cpu time consumed in seconds.\nTYPE container_cpu_user_seconds_total counter\ncontainer_cpu_user_seconds_total{container_name=\"POD\",id=\"/kubepods/besteffort/pod5f43c843-7db5-11e7-9534-0a97ed59c75e/e49ec1309ec25475a7edd8c4dd6d7003fef3f7debd053b234716649d920ac15f\",image=\"gcr.io/google_containers/pause-amd64:3.0\",name=\"k8s_POD_prom-node-exporter-w4nvq_monitoring_5f43c843-7db5-11e7-9534-0a97ed59c75e_1\",namespace=\"monitoring\",pod_name=\"prom-node-exporter-w4nvq\"} 0\ncontainer_cpu_user_seconds_total{container_name=\"POD\",id=\"/kubepods/besteffort/pod6b2e45d7-7db5-11e7-9534-0a97ed59c75e/2bf50e4b99aaf24eb05a61b9808d9e60d4fd78ba47ac7669ce29bb3f8c862501\",image=\"gcr.io/google_containers/pause-amd64:3.0\",name=\"k8s_POD_reboot-required-rn9h4_monitoring_6b2e45d7-7db5-11e7-9534-0a97ed59c75e_1\",namespace=\"monitoring\",pod_name=\"reboot-required-rn9h4\"} 0\ncontainer_cpu_user_seconds_total{container_name=\"POD\",id=\"/kubepods/besteffort/pod94ad7fd4-8351-11e7-9534-0a97ed59c75e/f3a1a656eabae83bb3a50206d7278b154fe1ddf2521e6a0bfd31667642867968\",image=\"gcr.io/google_containers/pause-amd64:3.0\",name=\"k8s_POD_memcached-296817331-t3q5v_kube-system_94ad7fd4-8351-11e7-9534-0a97ed59c75e_0\",namespace=\"kube-system\",pod_name=\"memcached-296817331-t3q5v\"} 0\ncontainer_cpu_user_seconds_total{container_name=\"POD\",id=\"/kubepods/besteffort/pod965b711b-8262-11e7-9534-0a97ed59c75e/c6e3b1012a1e607e4d164233f96a4c2ef83f377fc9dfb82e0dab7fc218e4e72a\",image=\"gcr.io/google_containers/pause-amd64:3.0\",name=\"k8s_POD_kured-wp23j_kube-system_965b711b-8262-11e7-9534-0a97ed59c75e_1\",namespace=\"kube-system\",pod_name=\"kured-wp23j\"} 0\ncontainer_cpu_user_seconds_total{container_name=\"POD\",id=\"/kubepods/besteffort/podd2b82b9c-8355-11e7-9534-0a97ed59c75e/e62bf79dd1981e285df9138a057b481357a5be6e464b43235e1335ac33bcf00b\",image=\"gcr.io/google_containers/pause-amd64:3.0\",name=\"k8s_POD_fluxd-3608285890-x4bz7_kube-system_d2b82b9c-8355-11e7-9534-0a97ed59c75e_0\",namespace=\"kube-system\",pod_name=\"fluxd-3608285890-x4bz7\"} 0\ncontainer_cpu_user_seconds_total{container_name=\"POD\",id=\"/kubepods/besteffort/pode4c7eace-8352-11e7-9534-0a97ed59c75e/e618f7cb1f3ec97f463ed9f97143890b80c730f53075a127d9f59714aab35163\",image=\"gcr.io/google_containers/pause-amd64:3.0\",name=\"k8s_POD_nats-651776541-6vrk3_scope_e4c7eace-8352-11e7-9534-0a97ed59c75e_0\",namespace=\"scope\",pod_name=\"nats-651776541-6vrk3\"} 0\ncontainer_cpu_user_seconds_total{container_name=\"POD\",id=\"/kubepods/burstable/pod53559243-7db5-11e7-9534-0a97ed59c75e/44f9f0113185f75f827eca36a42a7d2f91e166594c63eb2efecc7155eda03a70\",image=\"gcr.io/google_containers/pause-amd64:3.0\",name=\"k8s_POD_scope-probe-master-3cktj_kube-system_53559243-7db5-11e7-9534-0a97ed59c75e_1\",namespace=\"kube-system\",pod_name=\"scope-probe-master-3cktj\"} 0\ncontainer_cpu_user_seconds_total{container_name=\"POD\",id=\"/kubepods/burstable/pod55af46fe-834c-11e7-9534-0a97ed59c75e/9f9696a06e93a617a4e606731a474966c139681eef1a66344f0d06c965c68e47\",image=\"gcr.io/google_containers/pause-amd64:3.0\",name=\"k8s_POD_authfe-1607895901-bjnd9_default_55af46fe-834c-11e7-9534-0a97ed59c75e_0\",namespace=\"default\",pod_name=\"authfe-1607895901-bjnd9\"} 0\ncontainer_cpu_user_seconds_total{container_name=\"POD\",id=\"/kubepods/burstable/pod7964f3e653196edee64f6bad72589dee/7c3dc6bb8bb540224ca1f6d121d5fe2c5df0606ce5d45e7a0c802c29765c6625\",image=\"gcr.io/google_containers/pause-amd64:3.0\",name=\"k8s_POD_kube-proxy-ip-172-20-3-76.ec2.internal_kube-system_7964f3e653196edee64f6bad72589dee_1\",namespace=\"kube-system\",pod_name=\"kube-proxy-ip-172-20-3-76.ec2.internal\"} 0\ncontainer_cpu_user_seconds_total{container_name=\"POD\",id=\"/kubepods/burstable/podc7af9dff-8364-11e7-9534-0a97ed59c75e/3f5329bc7772496d70821ea9c9bc80045af6c29299c42d74e2d27baf8c3cc72a\",image=\"gcr.io/google_containers/pause-amd64:3.0\",name=\"k8s_POD_prometheus-2177618048-kgczb_monitoring_c7af9dff-8364-11e7-9534-0a97ed59c75e_0\",namespace=\"monitoring\",pod_name=\"prometheus-2177618048-kgczb\"} 0\ncontainer_cpu_user_seconds_total{container_name=\"POD\",id=\"/kubepods/burstable/podcb5d3cc0-8364-11e7-9534-0a97ed59c75e/decb876fb0dad43964deed741609ee45d3cf9049ae9f3ac934aefd596695302c\",image=\"gcr.io/google_containers/pause-amd64:3.0\",name=\"k8s_POD_fluxsvc-438909710-2jtz8_fluxy_cb5d3cc0-8364-11e7-9534-0a97ed59c75e_0\",namespace=\"fluxy\",pod_name=\"fluxsvc-438909710-2jtz8\"} 0\ncontainer_cpu_user_seconds_total{container_name=\"POD\",id=\"/kubepods/burstable/podcf18531c-8365-11e7-9534-0a97ed59c75e/133181c676d51606d4fa3d7d5c7e7455535636d30c5629526f0ba0cac5fcb522\",image=\"gcr.io/google_containers/pause-amd64:3.0\",name=\"k8s_POD_fluentd-loggly-z9jp4_monitoring_cf18531c-8365-11e7-9534-0a97ed59c75e_0\",namespace=\"monitoring\",pod_name=\"fluentd-loggly-z9jp4\"} 0\ncontainer_cpu_user_seconds_total{container_name=\"POD\",id=\"/kubepods/burstable/pode84e93da-865d-11e7-940d-12467a080e24/dabbd2c12e2d2666dd818b0c44be54760a701bdaf850ee4804b32efd36c42754\",image=\"gcr.io/google_containers/pause-amd64:3.0\",name=\"k8s_POD_collection-3392593966-7nxpw_scope_e84e93da-865d-11e7-940d-12467a080e24_0\",namespace=\"scope\",pod_name=\"collection-3392593966-7nxpw\"} 0\ncontainer_cpu_user_seconds_total{container_name=\"authfe\",id=\"/kubepods/burstable/pod55af46fe-834c-11e7-9534-0a97ed59c75e/c796e0b2c3afc41e1ed6750c9dc9f5550e19efe25f0aa717fe4f9b2578c16c67\",image=\"quay.io/weaveworks/authfe@sha256:c82cb113d15e20f65690aa3ca7f3374ae7ed2257dee2bc131bd61b1ac2bf180a\",name=\"k8s_authfe_authfe-1607895901-bjnd9_default_55af46fe-834c-11e7-9534-0a97ed59c75e_0\",namespace=\"default\",pod_name=\"authfe-1607895901-bjnd9\"} 64953.6\ncontainer_cpu_user_seconds_total{container_name=\"billing-ingester\",id=\"/kubepods/burstable/pode84e93da-865d-11e7-940d-12467a080e24/50c82895bd84971bc6b8b9f5873512710ab06f754a0e0d3261bc20a2fddd4533\",image=\"quay.io/weaveworks/billing-ingester@sha256:5fd857a96cac13e9f96678e63a07633af45de0e83a34e8ef28f627cf0589a042\",name=\"k8s_billing-ingester_collection-3392593966-7nxpw_scope_e84e93da-865d-11e7-940d-12467a080e24_0\",namespace=\"scope\",pod_name=\"collection-3392593966-7nxpw\"} 187.34\ncontainer_cpu_user_seconds_total{container_name=\"collection\",id=\"/kubepods/burstable/pode84e93da-865d-11e7-940d-12467a080e24/f33f520aa2ed2c6f2277064fed34c4797ddf76a0a0bef25309348517cb1c4030\",image=\"quay.io/weaveworks/scope@sha256:45be0490dba82f68a20faba8994cde307e9ace863a310196ba91401122bda4f8\",name=\"k8s_collection_collection-3392593966-7nxpw_scope_e84e93da-865d-11e7-940d-12467a080e24_0\",namespace=\"scope\",pod_name=\"collection-3392593966-7nxpw\"} 5411.72\ncontainer_cpu_user_seconds_total{container_name=\"exporter\",id=\"/kubepods/besteffort/pod94ad7fd4-8351-11e7-9534-0a97ed59c75e/cabd4c16d300232a8b823bd5a9553816ff7f0830c6d91634651b4f723035664f\",image=\"prom/memcached-exporter@sha256:b814aa209e2d5969be2ab4c65b5eda547ba657fd81ba47f48b980d20b14befb7\",name=\"k8s_exporter_memcached-296817331-t3q5v_kube-system_94ad7fd4-8351-11e7-9534-0a97ed59c75e_0\",namespace=\"kube-system\",pod_name=\"memcached-296817331-t3q5v\"} 142.5\ncontainer_cpu_user_seconds_total{container_name=\"exporter\",id=\"/kubepods/besteffort/pode4c7eace-8352-11e7-9534-0a97ed59c75e/81fd164c5cc91a483b73ada15ce13f19d3171fc6beddc940fc2b6e747141905d\",image=\"tomwilkie/nats_exporter@sha256:189354d9c966f94d9685009250dc360582baf02f76ecbaa2233e15cff2bc8f7f\",name=\"k8s_exporter_nats-651776541-6vrk3_scope_e4c7eace-8352-11e7-9534-0a97ed59c75e_0\",namespace=\"scope\",pod_name=\"nats-651776541-6vrk3\"} 107.62\ncontainer_cpu_user_seconds_total{container_name=\"fluentd-loggly\",id=\"/kubepods/burstable/podcf18531c-8365-11e7-9534-0a97ed59c75e/6fe6a67e02419f47a21854b73734042c0d457d42704be4302356180e4f357935\",image=\"quay.io/weaveworks/fluentd-loggly@sha256:19a02a2f8627573572cc2ee3c706aa4ccdab0f59c3a04e577d28035681d30ddc\",name=\"k8s_fluentd-loggly_fluentd-loggly-z9jp4_monitoring_cf18531c-8365-11e7-9534-0a97ed59c75e_0\",namespace=\"monitoring\",pod_name=\"fluentd-loggly-z9jp4\"} 17386.12\ncontainer_cpu_user_seconds_total{container_name=\"flux\",id=\"/kubepods/besteffort/podd2b82b9c-8355-11e7-9534-0a97ed59c75e/d4ef6d20b97c7f0fefc9d13c0f4b94290eb661035bf21b7f07f38acdd18cb85d\",image=\"quay.io/weaveworks/flux@sha256:e462c0a7c316f5986b3808360dc7c8c269466033c75a1b9553aa8175e02646f7\",name=\"k8s_flux_fluxd-3608285890-x4bz7_kube-system_d2b82b9c-8355-11e7-9534-0a97ed59c75e_0\",namespace=\"kube-system\",pod_name=\"fluxd-3608285890-x4bz7\"} 36097.96\ncontainer_cpu_user_seconds_total{container_name=\"fluxsvc\",id=\"/kubepods/burstable/podcb5d3cc0-8364-11e7-9534-0a97ed59c75e/aa00624319b1a96a18e0a4717f13e7456e558fea8b84e2694dc8d2b168a44d3d\",image=\"quay.io/weaveworks/fluxsvc@sha256:8d91991f2f6894def54afda4b4afb858b0502ed841a7188db48210b94bfdae4a\",name=\"k8s_fluxsvc_fluxsvc-438909710-2jtz8_fluxy_cb5d3cc0-8364-11e7-9534-0a97ed59c75e_0\",namespace=\"fluxy\",pod_name=\"fluxsvc-438909710-2jtz8\"} 897247.03\ncontainer_cpu_user_seconds_total{container_name=\"kube-proxy\",id=\"/kubepods/burstable/pod7964f3e653196edee64f6bad72589dee/8d2eb34023eab40d08ba6e4be149e315c3844749f8321f44be2dcda024534757\",image=\"gcr.io/google_containers/kube-proxy-amd64@sha256:dba7121df9f74b40901fb655053af369f58c82c3636d8125986ce474a759be80\",name=\"k8s_kube-proxy_kube-proxy-ip-172-20-3-76.ec2.internal_kube-system_7964f3e653196edee64f6bad72589dee_1\",namespace=\"kube-system\",pod_name=\"kube-proxy-ip-172-20-3-76.ec2.internal\"} 368.98\ncontainer_cpu_user_seconds_total{container_name=\"kured\",id=\"/kubepods/besteffort/pod965b711b-8262-11e7-9534-0a97ed59c75e/12b3c19d2f114a6a111fdc0375bb0c27fb9e108c166e6f674aeddcd5178faa0b\",image=\"weaveworks/kured@sha256:305b073cd3fff9ba0f21a570ee8a9c018d30274fc35045134164c762f44828e0\",name=\"k8s_kured_kured-wp23j_kube-system_965b711b-8262-11e7-9534-0a97ed59c75e_1\",namespace=\"kube-system\",pod_name=\"kured-wp23j\"} 5.91\ncontainer_cpu_user_seconds_total{container_name=\"logging\",id=\"/kubepods/burstable/pod55af46fe-834c-11e7-9534-0a97ed59c75e/8d7e46f3d99d2f13b04b7e07a4f1062e82450f02f8f7f03c8fb33a83f0248857\",image=\"quay.io/weaveworks/logging@sha256:63c4e6783884e6fcdd24026606756748e5913ab4978efa61ed09034074ddbe27\",name=\"k8s_logging_authfe-1607895901-bjnd9_default_55af46fe-834c-11e7-9534-0a97ed59c75e_0\",namespace=\"default\",pod_name=\"authfe-1607895901-bjnd9\"} 41780.76\ncontainer_cpu_user_seconds_total{container_name=\"memcached\",id=\"/kubepods/besteffort/pod94ad7fd4-8351-11e7-9534-0a97ed59c75e/e5d81ddecc6a587e55491e837db3ed46f274e3b02c764f4d6d1ca2e6228fbe0c\",image=\"memcached@sha256:00b68b00139155817a8b1d69d74865563def06b3af1e6fc79ac541a1b2f6b961\",name=\"k8s_memcached_memcached-296817331-t3q5v_kube-system_94ad7fd4-8351-11e7-9534-0a97ed59c75e_0\",namespace=\"kube-system\",pod_name=\"memcached-296817331-t3q5v\"} 222.96\ncontainer_cpu_user_seconds_total{container_name=\"nats\",id=\"/kubepods/besteffort/pode4c7eace-8352-11e7-9534-0a97ed59c75e/511ce33319ecc50b928e3dda7025d643c310a5573d89596f89798496d9868342\",image=\"nats@sha256:2dfb204c4d8ca4391dbe25028099535745b3a73d0cf443ca20a7e2504ba93b26\",name=\"k8s_nats_nats-651776541-6vrk3_scope_e4c7eace-8352-11e7-9534-0a97ed59c75e_0\",namespace=\"scope\",pod_name=\"nats-651776541-6vrk3\"} 44.25\ncontainer_cpu_user_seconds_total{container_name=\"prom-node-exporter\",id=\"/kubepods/besteffort/pod5f43c843-7db5-11e7-9534-0a97ed59c75e/1ceb1514b5339c67c70ec37d609d361d5ba656ee3697a12de0918f9902d0a134\",image=\"weaveworks/node_exporter@sha256:4f0c14e89da784857570185c4b9f57acb20f4331ef10e013731ac9274243a5a8\",name=\"k8s_prom-node-exporter_prom-node-exporter-w4nvq_monitoring_5f43c843-7db5-11e7-9534-0a97ed59c75e_1\",namespace=\"monitoring\",pod_name=\"prom-node-exporter-w4nvq\"} 707.54\ncontainer_cpu_user_seconds_total{container_name=\"prom-run\",id=\"/kubepods/besteffort/pod6b2e45d7-7db5-11e7-9534-0a97ed59c75e/75468eaf52cf3577dbb462d586fc5aa49a3f5a151fb668a734f8e99f825c1fc5\",image=\"quay.io/weaveworks/docker-ansible@sha256:452d1249e40650249beb700349c7deee26c15da2621e8590f3d56033babb890b\",name=\"k8s_prom-run_reboot-required-rn9h4_monitoring_6b2e45d7-7db5-11e7-9534-0a97ed59c75e_1\",namespace=\"monitoring\",pod_name=\"reboot-required-rn9h4\"} 70.57\ncontainer_cpu_user_seconds_total{container_name=\"prometheus\",id=\"/kubepods/burstable/podc7af9dff-8364-11e7-9534-0a97ed59c75e/e4e3b4f6285c9a12415f347aadbf150c6d782e6b881d2701d4257bf3a4de2651\",image=\"prom/prometheus@sha256:4bf7ad89d607dd8de2f0cff1df554269bff19fe0f18ee482660f7a5dc685d549\",name=\"k8s_prometheus_prometheus-2177618048-kgczb_monitoring_c7af9dff-8364-11e7-9534-0a97ed59c75e_0\",namespace=\"monitoring\",pod_name=\"prometheus-2177618048-kgczb\"} 438158.08\ncontainer_cpu_user_seconds_total{container_name=\"scope-probe\",id=\"/kubepods/burstable/pod53559243-7db5-11e7-9534-0a97ed59c75e/e57413febbcc1c28321ccb99df3bf30b9d6555a1db62b743d1b4ee877f23346b\",image=\"quay.io/weaveworks/scope@sha256:bc6ee4a4a568f8075573a8ac44c27759307fce355c22ad66acb1e944b6361b62\",name=\"k8s_scope-probe_scope-probe-master-3cktj_kube-system_53559243-7db5-11e7-9534-0a97ed59c75e_1\",namespace=\"kube-system\",pod_name=\"scope-probe-master-3cktj\"} 278471.28\ncontainer_cpu_user_seconds_total{container_name=\"watch\",id=\"/kubepods/burstable/podc7af9dff-8364-11e7-9534-0a97ed59c75e/fe6cdaa2c542c90cbca951cd97952d35c8c42fcd5e8f452030369a98e27c9b3f\",image=\"weaveworks/watch@sha256:bb113953e19fff158de017c447be337aa7a3709c3223aeeab4a5bae50ee6f159\",name=\"k8s_watch_prometheus-2177618048-kgczb_monitoring_c7af9dff-8364-11e7-9534-0a97ed59c75e_0\",namespace=\"monitoring\",pod_name=\"prometheus-2177618048-kgczb\"} 0.1\n```\nDifferent metrics in the same scrape will be fine, e.g. container_fs_inodes_free. I think I figured out what is going wrong.\nThe function DefaultContainerLabels() conditionally adds various metric labels from container labels - name, image, etc.  When used inside kubelet this function is containerPrometheusLabels() but essentially the same.\nHowever, when it receives the metrics, Prometheus checks that all metrics in the same family have the same label set, and rejects those that do not.\nSince containers are collected in (somewhat) random order, depending on which kind is seen first you get one set of metrics or the other.\nChanging the container labels function to always add the same set of labels, adding \"\" when it doesn't have a real value, eliminates the issue in my testing.. Actually it looks like the function in cAdvisor is more complicated, as it copies all Docker labels, etc.\nI will make a PR for kubelet.\nTo clarify, so far my testing has been in a stand-alone program bringing in parts of kubelet to find out what it was doing.. I know nothing of stand-alone cAdvisor usage. Are there any users reading this?\nCould we have a pre-defined set of container labels which are copied as metric labels?. It's only checked by the Prometheus client library at each scrape, but surely the desire from Prometheus is that labelling be consistent over all samples for the same metric?  \nIt's not even \"across time\"; consider two samples from different machines taken at the same time.. oops, just changed it the other way.  hold on.... Sure, will get to that later.\nIt appears there is a test which checks that CPU usage is computed by adding.. I have fixed the failing test, and changed my mind about the metric name.\nI believe most people are currently using queries like\nsum (rate(container_cpu_usage_seconds_total{}[5m])) by (some_criteria)\n\nIf we change the name then everybody has to go round and edit all those queries.\nInstead, if we keep the same name and just switch from cpu01, cpu02, etc., to total under the cpu label if percpu is disabled, then everything will Just Work.\nStill haven't done the sanity check.. Thanks for the reminder - I got sidetracked onto 50 other things. Will squash and re-check tomorrow. . OK, I have run the k8s.gcr.io/stress:v1 container a few times with different parameters, while running my build of cAdvisor from this branch, and checked the CPU metrics tallied with expectations.\nI had forgotten to check >0 before returning a CPU value so added that, for the percpu-disabled case.\nAnd squashed commits as requested.. I think it happens infrequently, although the only evidence is that nobody noticed it happening much.  I have set a Prometheus alarm on count by (instance)(container_memory_rss) unless count by (instance)(container_memory_rss{namespace!=\"\"}) which should let us pick it up faster next time.\nI don't have any bad nodes at the moment, since we have variously restarted and blown things away.  I will check next time it happens.\nThe Docker root dir is nonstandard, but I do have a bunch of files at /mnt/containers/docker/image/overlay2/layerdb/mounts/<CONTAINER_UID>/mount-id.  What am I looking for wrt \"r/w lager id\" ?\n```\ndocker info\nContainers: 116\n Running: 105\n Paused: 0\n Stopped: 11\nImages: 2853\nServer Version: 1.12.2\nStorage Driver: overlay2\n Backing Filesystem: extfs\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: host null overlay\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: apparmor seccomp\nKernel Version: 4.4.0-119-generic\nOperating System: Ubuntu 16.04.1 LTS\nOSType: linux\nArchitecture: x86_64\nCPUs: 16\nTotal Memory: 29.44 GiB\nName: ip-172-20-2-209\nID: 4KM4:IOI7:XXAZ:NZG7:XR3K:ETZG:JKWT:S5I2:V7VX:PHUX:CISI:MI2A\nDocker Root Dir: /mnt/containers/docker\nDebug Mode (client): false\nDebug Mode (server): false\nRegistry: https://index.docker.io/v1/\nWARNING: No swap limit support\nInsecure Registries:\n 127.0.0.0/8\n```. It's happened again, on one node in each of two clusters, both immediately after a reboot.\nWe do not get the /var/lib/docker/image messages on working nodes.\nI suspect a race in RootDir() which falls back to the \"wrong\" directory if it encounters an error, without logging.  Immediately after reboot it may not be possible to talk to Docker.\nI'll try setting the --docker-root flag . @jejer I think it just doesn't work at all if it can't initialize.\nWhere do you see --docker-root deprecated?. BTW after I set --docker-root in our system we never saw the problem again.\nTo fix the underlying problem we need some alternative to sync.Once that will run again if it went wrong.. > \"DEPRECATED: docker root is read from docker info (this is a fallback, default: /var/lib/docker)\"\nOK, that is exactly this bug - if Docker is not ready, the info is not available.\nNote it was known at the time https://github.com/google/cadvisor/pull/1275#discussion_r62554982:\n\nWe cannot deprecate it until the clients of this package can deal with the fact that docker might not be available when cAdvisor is initialized.. Do you need someone to port this into kubelet?. I am no expert in systemd either.\n\nFrom systemd.scope I read:\n\nScope units are [...] created programmatically using the bus interfaces of systemd. [...] \nThe main purpose of scope units is grouping worker processes of a system service for organization and for managing resources.\n\nI might speculate that these units are created internally by systemd to run processing relating to each mount.  Negating my \"never going to be any load ...\"\nMaybe a different approach would be to limit the depth to which certain cgroups are monitored?  E.g. I could declare to cAdvisor I'm happy to monitor /system.slice as a whole, and don't need to see detail under it?. See previous comment. . ",
    "endofcake": "-v /cgroup:/sys/fs/cgroup:ro might be able to fix this.. ",
    "AndresPineros": "@endofcake This solution actually worked for me. Thank you very much. Now I can run cAdvisor on Amazon Linux with Docker Swarm.. ",
    "mavenugo": "@johnharris85 network-scoped DNS records are gossiped/synchronized only on the Swarm nodes where there is atleast 1 task is attached to the corresponding network. In this case, the network monitoring is made active in 1 node where the elasticsearch task is launched. Since no other node is scheduled with any other tasks in that network monitoring, it is not gossiped yet. \nAfter the cadvisor service launches the tasks in other nodes, the network scoped DNS records are gossiped to these nodes. It is possible that the cadvisor containers are aggressively failing if the DNS record is not synchronized by the time the cadvisor container starts. It sounds like a timing issue.\nJust to confirm this theory, can you launch a dummy service and keep it running in network monitoring with --mode global before launching the cadvisor service. That will make sure the DNS records are synchronized in these nodes when cadvisor container comes up.\n. ",
    "johnharris85": "Thanks @mavenugo, other containers (on other hosts) are able to connect to elasticsearch via the service name. I also did as you suggested just to confirm and brought up a dummy global service on that network and then started cadvisor, same result. It seems like an issue with the elastic client library maybe (or potentially the elasticsearch listening host settings?) except when using the IP it works fine, and my colleague under an identical setup has no issues at all, yet mine are consistently reproducible :p\n. So this is fixed by adding --advertise-addr <node_IP> to docker swarm join ... on my secondary managers. There's no warning, and the (Swarm) docs say it should be very rarely required. Can't explain technically what's going on here though, that curl etc works but cadvisor doesn't.\n. ",
    "andrew-jones": "Should have looked, current requirement is 'gopkg.in/olivere/elastic.v2'.  v5 isn't stable yet.\n. Should have looked, current requirement is 'gopkg.in/olivere/elastic.v2'.  v5 isn't stable yet.\n. ",
    "w-p": "Elastic is showing GA on v5, so I'd like to +1 the release.\n. ",
    "ppraj": "Yes , same issue , it fails on initiliazing . When can this be available. ",
    "losoft": "+1. ",
    "mqasim1983": "+1. ",
    "sukki07": "+1. ",
    "dinoba": "+1. ",
    "se-jaeger": "+1. ",
    "entropeer-dev": "+1. ",
    "superbarne": "+1. ",
    "anderson4u2": "+1. ",
    "mpereira": "+1. ",
    "robinong79": "+1. I'm having exactly the same problem. This blocks us from using cAdvisor in Swarm mode.. ",
    "slacksec": "+1. ",
    "disillusions": "+1. ",
    "OferE": "+1. ",
    "JinsYin": "+1. @shamimgeek \"hostPort\" must be 0.\n{\n    \"id\": \"/cadvisor\",\n    \"args\": [],\n    \"mem\": 128.0,\n    \"cpus\": 0.2,\n    \"instances\": 1,\n    \"constraints\": [[\"hostname\", \"UNIQUE\"]],\n    \"labels\": {\n        \"VERSION\": \"0.24.1\",\n        \"WEB_PORT\": \"8080\"\n    },\n    \"container\": {\n        \"type\": \"DOCKER\",\n        \"docker\": {\n            \"image\": \"google/cadvisor:v0.24.1\",\n            \"forcePullImage\": true,\n            \"network\": \"BRIDGE\",\n            \"privileged\": true,\n            \"portMappings\": [\n                { \"containerPort\": 8080, \"hostPort\": 0, \"servicePort\": 18080, \"protocol\": \"tcp\" }\n            ]\n        },\n        \"volumes\": [\n            {\n                \"containerPath\": \"/rootfs\",\n                \"hostPath\": \"/\",\n                \"mode\": \"RO\"\n            },\n            {\n                \"containerPath\": \"/var/run\",\n                \"hostPath\": \"/var/run\",\n                \"mode\": \"RW\"\n            },\n            {\n                \"containerPath\": \"/sys\",\n                \"hostPath\": \"/sys\",\n                \"mode\": \"RO\"\n            },\n            {\n                \"containerPath\": \"/var/lib/docker/\",\n                \"hostPath\": \"/var/lib/docker\",\n                \"mode\": \"RO\"\n            }\n        ]\n    },\n    \"healthChecks\": [\n        { \"protocol\": \"HTTP\", \"portIndex\": 0, \"path\": \"/\" }\n    ]\n}. ",
    "man4j": "+1. +1. ",
    "maheshvra": "+1. ",
    "sadok-f": "+1. ",
    "david-z-johnson": "+1. ",
    "vishalud": "+1 . ",
    "selimekizoglu": "+1. ",
    "jarronshih": "+1. ",
    "shyd": "+1. ",
    "realcbb": "+1. ",
    "enemchy": "+1. ",
    "raji90": "Does this issue still exist that cadvisor works fine only with elasticserach version 2 and not above that? I have elasticserach instance with version 5.4.0 and on running cadvisor it gives me following error.\ndocker run                                      \\\n\n--volume=/:/rootfs:ro                         \\\n   --volume=/var/run:/var/run:rw                 \\\n   --volume=/sys:/sys:ro                         \\\n   --volume=/var/lib/docker/:/var/lib/docker:ro  \\\n   --publish=9091:8080                           \\\n   --detach=true                                 \\\n   --name=cadvisor5                               \\\n   docker.fmr.com/google/cadvisor:latest\\\n   -storage_driver=elasticsearch -storage_driver_es_host=\"http://my_elk.fcorp.com:9200\"\n\nF0912 07:50:13.460296       1 cadvisor.go:117] Failed to initialize storage driver: failed to create the elasticsearch client - no Elasticsearch node available\n. @vishh @tallclair Can you please help here?. Thanks a lot @tallclair and @dashpole for the response. so with #1517 issues it looks like cadvisor still does not support version 5. I thought the post was a year back post and cadvisor might have been updated to use latest version of elasticsearch.\nI tried with influxdb also and that also threw me error. I have an issue opened for it. https://github.com/google/cadvisor/issues/1757. @tallclair I saw couple comments from you for influxdb. CAn you or someone from team help me understanding/fixing the issue? https://github.com/google/cadvisor/issues/1757\n. I just need a mechanism through which I can store the stats. I was initially trying stdout option but did not work. I don't see any doc for that option. Then I tried with elasticsearch and now with influxdb and these are also not working.. @HarenBroog Thanks a lot for the comment. I will try similar thing and will check. @dashpole Thank you for responding. If this is not an error then can you suggest what can I check or look into to make this work. My influx db instance from the docker container does not have any data from cadvisor,  I did not use docker compose. All of these are independent containers. Is that the matter?. ",
    "justinzyw": "+1. ",
    "nicolasgere": "+1. ",
    "cheneypan": "@timstclair It's OK. Thanks.\n. ",
    "judexzhu": "28.3 has the same issue, but 27.3 works fine. Just FYI. ",
    "krutsko": "\n28.3 has the same issue, but 27.3 works fine. Just FYI\n\n+1 \nFiled https://github.com/google/cadvisor/issues/1850\n  . ",
    "vikaschoudhary16": "cc @jeremyeder @timstclair @ConnorDoyle @derekwaynecarr @vishh @psuriset @timothysc \n. @derekwaynecarr \n\nSo the topology with cells and cpus listed?\n\nyes. But for naming instead of 'cells', I would prefer 'numa_nodes' or something similar to make naming more intuitive. \n\nWould you do this on MachineInfo?\n\nYes.\n\nGot a response format in mind?\n\nDoes the following format sounds reasonable? :\n``\ntype NumaNode struct {\n        Id intjson:\"numa_node_id\"// Per-numa-node memory\n        // Representating 'free:' from 'numactl --hardware' output\n        MemoryFree uint64json:\"numa_memory_free\"// Representating 'size:' from 'numactl --hardware' output\n        MemorySize uint64json:\"numa_memory_size\"// Number of cores belonging to this numa node\n        NumCores intjson:\"numa_num_cores\"// Representating 'cpus:' from 'numactl --hardware' output\n       Cores  [ ]stringjson:\"cores\"`\n}\ntype MachineInfo struct {\n               .............\n               .......\n               // Number of NUMA nodes detected on machine\n               NumNumaNodes int json:\"num_numa_nodes\"\n               ........\n               // NUMA Topology\n               // Describes cpu cores and memory available with each NUMA node\n               NumaTopology   map[string]*NumaNode json:\"numa_topolgy\"\n               ........\n               .........\n}         \n```\nThanks\n-Vikas\n. @vish thanks a lot!!!\n. ping @jeremyeder @timstclair @ConnorDoyle @derekwaynecarr @vishh @psuriset @timothysc @vishh @balajismaniam\n. ping @vishh \n. If you dont use --volume=/sys:/sys:ro then it starts fine. Another solution is:\n\nmount -o remount,rw '/sys/fs/cgroup'\nln -s /sys/fs/cgroup/cpu,cpuacct /sys/fs/cgroup/cpuacct,cpu. Moved here, https://github.com/google/cadvisor/pull/1861. thanks, will update.\n. I dint remove manually. godep save did.\ngodep removed couple of packages which are needed actually. for ex:\ngithub.com/SeanDolphin/bqschema\n\nWill sort out.\n-Thanks\n. ",
    "ConnorDoyle": "cc @balajismaniam\n\nOn Oct 28, 2016, at 20:44, Vikas Choudhary notifications@github.com wrote:\ncc @jeremyeder @timstclair @ConnorDoyle @derekwaynecarr\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "mqliang": "@vishh @timothysc Is is worth doing? I can post a pr soon if community folks think such a change is ok.\ncc @ddysher\n. ",
    "michaelalhilly": "I don't disagree with you.\nIn cluster environments it is a common task to run a script in response to a change in a container state. In my mind it would be added to cadvisor as an event that we can hook into. It seems reasonable since the container state is already being tracked. However if it would add too much overhead or whatever then it's not worth it.. ",
    "anandkumarpatel": "I signed it!\n. @k8s-bot ok to test\n. ",
    "Thermi": "summary.txt\nfree.txt\n. ",
    "vdavidoff": "Is the decision to include active page cache in working set memory deliberate and arguably correct, or do cAdvisor devs believe it should be excluded? Looking at kubernetes release 1.5.3, it appears that cAdvisor itself is what's subtracting total inactive file memory from memory.usage_in_bytes when calculating working set (cadvisor/container/libcontainer/helpers.go func setMemoryStats).\nThis is interesting to me because of how it impacts kubelet's eviction settings. It's pretty difficult to choose a memory eviction threshold that won't erroneously trip since predicting the amount of active page cache at any time, in general, is not really possible. Since the kernel can reclaim the page cache (use this for context around \"erroneously\" above), it seems like it shouldn't count against available memory.\nThis might be something that should be pushed back up into kubelet, but I wonder if the general case of including the active page cache, which it appears to be cAdvisor is responsible for, is generally unexpected?\n(I ended up opening this against kubelet/kubernetes https://github.com/kubernetes/kubernetes/issues/43916). ",
    "bitglue": "\npredicting the amount of active page cache at any time, in general, is not really possible.\n\nI contend in a situations where the file cache is significantly used (for example, a database), it's quite easy to predict the size of the active file list: It's simply all the RAM which isn't occupied by anonymous pages, kernel slab, and a few other minor things; multiplied by a fraction which starts at 0.5 and approaches 1 as the total RAM increases.\nIn other words, for an IO workload where cache performance is critical, all the \"extra\" RAM (that is, RAM which could be reclaimed without significant detriment to performance) ends up in the active file LRU list. This is the opposite of what's likely desirable for some definition of a \"working set\".\nMore detail at https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-393228487.. > predicting the amount of active page cache at any time, in general, is not really possible.\nI contend in a situations where the file cache is significantly used (for example, a database), it's quite easy to predict the size of the active file list: It's simply all the RAM which isn't occupied by anonymous pages, kernel slab, and a few other minor things; multiplied by a fraction which starts at 0.5 and approaches 1 as the total RAM increases.\nIn other words, for an IO workload where cache performance is critical, all the \"extra\" RAM (that is, RAM which could be reclaimed without significant detriment to performance) ends up in the active file LRU list. This is the opposite of what's likely desirable for some definition of a \"working set\".\nMore detail at https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-393228487.. ",
    "danielkrainas": "/bump. ",
    "adohe": "@Random-Liu That's why I use time.Sleep in node problem detector.  :)\n. ",
    "zaquestion": "I signed it!\n. Change was introduced here: https://github.com/google/cadvisor/commit/cd41c0bd603392da535c529fe55757b9e09c98b3. ",
    "perfeyhe": "I found the reason. It seems cAdvisor can not work well  with kubernetes 1.2.0 and docker-engine 1.2.1. I tried to downgrade docker to lxc-docker 1.9.1, the pod metric came out, since kube-push.sh is broken, I didn't try to upgrade kubernetes to 1.4.6 to see if there are same problems in this version.. ",
    "ged15": "any progress on this?. ",
    "bvis": "I see it works again in 1.13.0-rc2. Then it's probably related with a bug in the docker endpoint.\nClosing.. ",
    "bakins": "Closing in favor of #1555. @dashpole I'll rebase and squash. Happy to answer any questions, as well.  This is based on the devicemapper code in cadvisor but does zfs rather than thin_ls to get disk/filesystem stats.. The current zfs support does not work. It will log errors continually and doesn't provide stats. I copied the devicemapper approach as the zfs commands can block so they are done in a watcher goroutine.. Squashed.  \nBTW - we've been running this for a while by patching Kubernetes.. The error printing the command incorrectly is mentioned here: https://github.com/mistifyio/go-zfs/pull/61\nI'll take a look at the error message about \"-r\" - looks like the wrong arguments are being passed but not sure if that's a cadvisor or go-zfs issue.. This appears to be an issue with go-zfs related to newer versions of the zfs utilities.  I'll see if I can get a fix merged there. If I can't, I'll see what workaround I can do here.. @maxramqvist have you tried master to see if you have the same issue?. @maxramqvist I generally use cadvisor in kubelet, so I'm not familiar with that Docker image.  Apologies.. @dashpole I can squash. I thought I only did one commit anyway.. @dashpole squashed. Sorry it took so long. . Statfs used to not work correctly with ZFS. I can test again to see if it does now.. ",
    "dnordberg": "http://xxx.pw:8080/ yes, ui and all without auth.. ",
    "vovkanaz": "Ubuntu 16.04, cAdvisor latest, Docker version 1.12.3 build 6b644ec,  the same ui without auth.\n. ",
    "ashishka": "Yes, for me not working too. ",
    "dashesy": "v0.24.1 works for me.. ",
    "zhengqisong": "curl localhost:8080/containers   need auth\ncurl localhost:8080/api/v1.0/containers/  No certification required\nHow do I set up API access authentication?\nHow do I set access authentication in kebulet?\n. ",
    "l8nite": "Would be nice to see some movement on this one.. Thanks @nlopez - just tried it out and worked for me too :D. ",
    "guydavis": "Hmm, what am I missing?  I  am seeing a similar error on CentOS 7.3 with Docker 1.11.2 using CAdvisor v0.25.0 still.\nsudo docker run --privileged=true --volume=/cgroup:/cgroup:ro --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro --publish=8725:8080 --detach=true --name=cadvisor google/cadvisor:v0.25.0 -storage_driver_db=influxdb -storage_driver_host=$INFLUX_HOST_NAME:8086\nDocker log for CAdvisor shows:\n```\nI0331 21:28:20.635127       1 storagedriver.go:50] Caching stats in memory for 2m0s\nI0331 21:28:20.635248       1 manager.go:143] cAdvisor running in container: \"/docker/62aa88c1fba84452bc9fa677910b9d167e9fc2987014db6f6a50c1682defd161\"\nW0331 21:28:20.639549       1 manager.go:151] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused\nI0331 21:28:20.645359       1 fs.go:117] Filesystem partitions: map[/dev/xvda1:{mountpoint:/var/lib/docker major:202 minor:1 fsType:xfs blockSize:0}]\nI0331 21:28:20.648092       1 manager.go:198] Machine: {NumCores:8 CpuFrequency:2299950 MemoryCapacity:33299312640 MachineID:f32e0af35637b5dfcbedcb0a1de8dca1 SystemUUID:EC219D7B-858E-9D29-B49E-5B3626BCC467 BootID:3bd0d947-256a-470c-83b6-62ce13a66093 Filesystems:[{Device:/dev/xvda1 Capacity:536852488192 Type:vfs Inodes:524280240 HasInodes:true} {Device:overlay Capacity:536852488192 Type:vfs Inodes:524280240 HasInodes:true}] DiskMap:map[202:0:{Name:xvda Major:202 Minor:0 Size:536870912000 Scheduler:deadline}] NetworkDevices:[{Name:eth0 MacAddress:0a:15:73:57:9f:c2 Speed:0 Mtu:9001}] Topology:[{Id:0 Memory:34359332864 Cores:[{Id:0 Threads:[0 4] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[1 5] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:2 Threads:[2 6] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:3 Threads:[3 7] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:47185920 Type:Unified Level:3}]}] CloudProvider:AWS InstanceType:m4.2xlarge InstanceID:i-0bab1058339a31fd7}\nI0331 21:28:20.648470       1 manager.go:204] Version: {KernelVersion:3.10.0-514.10.2.el7.x86_64 ContainerOsVersion:Alpine Linux v3.4 DockerVersion:1.11.2 CadvisorVersion:v0.25.0 CadvisorRevision:17543be}\nI0331 21:28:20.652067       1 factory.go:309] Registering Docker factory\nW0331 21:28:20.652088       1 manager.go:247] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused\nI0331 21:28:20.652094       1 factory.go:54] Registering systemd factory\nI0331 21:28:20.652771       1 factory.go:86] Registering Raw factory\nI0331 21:28:20.653430       1 manager.go:1106] Started watching for new ooms in manager\nW0331 21:28:20.653541       1 manager.go:275] Could not configure a source for OOM detection, disabling OOM events: unable to find any kernel log file available from our set: [/var/log/kern.log /var/log/messages /var/log/syslog]\nI0331 21:28:20.654947       1 manager.go:288] Starting recovery of all containers\nI0331 21:28:20.723176       1 manager.go:293] Recovery completed\nI0331 21:28:20.802634       1 cadvisor.go:157] Starting cAdvisor version: v0.25.0-17543be on port 8080\nE0331 21:29:20.798291       1 fsHandler.go:121] failed to collect filesystem stats - rootDiskErr: du command failed on /rootfs/var/lib/docker/overlay/51e6d23bd552aaa84596811a5498da34464777d8d7f601338bae032c1bf83b21 with output stdout: 57120       /rootfs/var/lib/docker/overlay/51e6d23bd552aaa84596811a5498da34464777d8d7f601338bae032c1bf83b21\n, stderr: du: /rootfs/var/lib/docker/overlay/51e6d23bd552aaa84596811a5498da34464777d8d7f601338bae032c1bf83b21/merged/proc/35/task/35/fdinfo/10: No such file or directory\n - exit status 1, rootInodeErr: cmd [find /rootfs/var/lib/docker/overlay/51e6d23bd552aaa84596811a5498da34464777d8d7f601338bae032c1bf83b21 -xdev -printf .] failed. stderr: find: unrecognized: -printf\nBusyBox v1.24.2 (2017-01-18 14:13:46 GMT) multi-call binary.\nUsage: find [-HL] [PATH]... [OPTIONS] [ACTIONS]\nSearch for files and perform actions on them.\nFirst failed action stops processing of current file.\nDefaults: PATH is current directory, action is '-print'\n```\nAlso tried, with google/cadvisor:latest.  Same error, with no data error arriving in InfluxDB.  Any tips would be most appreciated!\n. ",
    "Pluto1010": "I'm experiencing the same problem with 0.25.0. ",
    "slamont": "I'm having the same problem with v0.25.0-17543be. ",
    "toni-moreno": "I have the same problem, It seems th problem is in the find version ( not gnu find) in the busybox , there is plans to fix the current official cAdvisor docker image? \ncadvisor_1       | ; err: exit status 1, extraDiskErr: <nil>\ncadvisor_1       | I0419 21:18:27.544592       1 fs.go:487] killing cmd [find /rootfs/var/lib/docker/aufs/diff/75ace77bc15b09760d644a587c414e4272fce7759381001781e905c63d59eefb -xdev -printf .] due to timeout(2m0s)\ncadvisor_1       | E0419 21:25:58.730259       1 fsHandler.go:121] failed to collect filesystem stats - rootDiskErr: <nil>, rootInodeErr: cmd [find /rootfs/var/lib/docker/aufs/diff/e1d23eaf8fca369eef932902fc8c53b28a3de9a9d6d764fd785433a248e6f0c1 -xdev -printf .] failed. stderr: find: unrecognized: -printf\nThank you very much. I have the same problem, It seems th problem is in the find version ( not gnu find) in the busybox , there is plans to fix the current official cAdvisor docker image? \ncadvisor_1       | ; err: exit status 1, extraDiskErr: <nil>\ncadvisor_1       | I0419 21:18:27.544592       1 fs.go:487] killing cmd [find /rootfs/var/lib/docker/aufs/diff/75ace77bc15b09760d644a587c414e4272fce7759381001781e905c63d59eefb -xdev -printf .] due to timeout(2m0s)\ncadvisor_1       | E0419 21:25:58.730259       1 fsHandler.go:121] failed to collect filesystem stats - rootDiskErr: <nil>, rootInodeErr: cmd [find /rootfs/var/lib/docker/aufs/diff/e1d23eaf8fca369eef932902fc8c53b28a3de9a9d6d764fd785433a248e6f0c1 -xdev -printf .] failed. stderr: find: unrecognized: -printf\nThank you very much. there is any plans to update the official image with the findutils package?. ",
    "lio-li": "Use: apk -U add findutils in you container and commit it to your image . ",
    "rauschbit": "any news on this topic?\nwe are experiencing the same problem - hopefully there will be a patch for this shortly.... ",
    "blep": "I confirm that using apk -U add findutils solves the issue. I've made a docker automated build with that fix:\nhttps://hub.docker.com/r/blep/cadvisor_bugfix1556/\nOn a side note, google/cadvisor:latest still target v0.25.0 instead of v0.26.1.. ",
    "endeepak": "Thanks @blep. @google-admin please add this fix in official image.. ",
    "SimonSK": "has this been fixed in official image?. ",
    "ebuildy": "Use docker API!. Use docker API!. How did you run cadvisor?\nFor me:\ndocker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  google/cadvisor:latest\nworks fine.. How did you run cadvisor?\nFor me:\ndocker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  google/cadvisor:latest\nworks fine.. ",
    "tadivenkat": "I have a java client which reports the monitoring metrics. Hence I am constrained in using REST API.\nThere seems to be a formula which I need to find out in calculating cpu usage at a given instant of time.. \nI am making this request http://16.103.209.61:8081/api/v2.0/summary/\nI see the following response:\n{\"/\":{\"timestamp\":\"2016-12-12T18:42:57.459650443Z\",\"latest_usage\":{\"cpu\":100,\"memory\":1779097600},\"minute_usage\":{\"percent_complete\":100,\"cpu\":{\"present\":true,\"mean\":144,\"max\":677,\"fifty\":124,\"ninety\":240,\"ninetyfive\":254},\"memory\":{\"present\":true,\"mean\":1767151665,\"max\":1779077120,\"fifty\":1764810752,\"ninety\":1778312806,\"ninetyfive\":1778353766}},\"hour_usage\":{\"percent_complete\":1,\"cpu\":{\"present\":true,\"mean\":144,\"max\":677,\"fifty\":240,\"ninety\":240,\"ninetyfive\":240},\"memory\":{\"present\":true,\"mean\":1767151665,\"max\":1779077120,\"fifty\":1778312806,\"ninety\":1778312806,\"ninetyfive\":1778312806}},\"day_usage\":{\"percent_complete\":0,\"cpu\":{\"present\":true,\"mean\":144,\"max\":677,\"fifty\":240,\"ninety\":240,\"ninetyfive\":240},\"memory\":{\"present\":true,\"mean\":1767151665,\"max\":1779077120,\"fifty\":1778312806,\"ninety\":1778312806,\"ninetyfive\":1778312806}}}}\nI don't see CpuInstStats in the response.\nMy question is still not answered.. ",
    "sykesm": "Build was successful after a rebase. Thanks.. ",
    "bergerx": "I think I also did hit the same problem, it seems like the docker_env_metadata_whitelist parameter is not used by influxdb storage plugin.\nAppending this to cadvisor cmd:\n-docker_env_metadata_whitelist=MESOS_TASK_ID,MARATHON_APP_ID,MARATHON_APP_VERSION\nHaving the docker_env_metadata_whitelist parameter with marathon labels in place, I can see the labels in /metrics endpoint of cadvisor, so the docker_env_metadata_whitelist parameter is working:\ncontainer_cpu_system_seconds_total{container_env_marathon_app_id=\"/cadvisor\",container_env_marathon_app_version=\"2016-12-30T13:04:59.869Z\",container_env_mesos_task_id=\"cadvisor.91dc395f-ce90-11e6-b28b-70b3d5800003\",id=\"/docker/d4a910001c30574b6bc1161ba0260f76bfcaf290206faf502215761b95d00e17\",image=\"google/cadvisor:v0.24.1\",name=\"mesos-ee28b263-ad31-451d-aeb7-9b7856eddd44-S95.fbb98d0b-79de-4287-a5a9-6e126b00ee70\"} 1.16\nBut can't see these tags in InfluxDB database, assuming that docker_env_metadata_whitelist parameter is not used in influxdb storage:\n```\nbdogan@Bekirs-MacBook-Pro:~$ influx -host influx-db.marathon.l4lb.thisdcos.directory -port 8086\nVisit https://enterprise.influxdata.com to register for updates, InfluxDB server management, and monitoring.\nConnected to http://influx-db.marathon.l4lb.thisdcos.directory:8086 version 0.13.0\nInfluxDB shell version: v1.1.1\n\nuse cadvisor\nUsing database cadvisor\nSHOW TAG KEYS FROM \"cpu_usage_total\"\nname: cpu_usage_total\ntagKey\n\n\nbuild-date\ncontainer_name\nlicense\nmachine\nname\nvendor\n\n```. Oh, there is already a merge request for this: #1427 . \n",
    "bamarni": "I signed it!. I signed it!. @dchen1107 : I've added a test. Integration test support for overlay2 is also ready, however I think it'd require a machine running docker with overlay2 during CI? I'm not sure how to set this up.. @dchen1107 : I've added a test. Integration test support for overlay2 is also ready, however I think it'd require a machine running docker with overlay2 during CI? I'm not sure how to set this up.. @dchen1107 : should I resolve conflicts and squash? Please let me know if anything else is missing for that PR.. @dchen1107 : should I resolve conflicts and squash? Please let me know if anything else is missing for that PR.. @dashpole : yes I had checked on the web interface, there were a place showing disk usage with a red bar. It wasn't displayed at all before my changes and displayed properly after, so it seemed fine to me, is there anything else to be careful about?\nI don't have a setup at the moment to check it again now that I've rebased, but it was trivial conflicts.. @dashpole : yes I had checked on the web interface, there were a place showing disk usage with a red bar. It wasn't displayed at all before my changes and displayed properly after, so it seemed fine to me, is there anything else to be careful about?\nI don't have a setup at the moment to check it again now that I've rebased, but it was trivial conflicts.. I've rebased and tested it again with docker and overlay2, cf. below some screenshots from the container overview page on the web interface : \nbefore :\n\nafter : \n. ",
    "ebreton": "Resolved by the Dockerfile, but that means I am not using the \"official\" image anymore, which is ... not optimal. Ideally, there would be a way to set the timezone without the need to build an extra layer :) \nOr Cadvisor would use the host timezone ?. Resolved by the Dockerfile, but that means I am not using the \"official\" image anymore, which is ... not optimal. Ideally, there would be a way to set the timezone without the need to build an extra layer :) \nOr Cadvisor would use the host timezone ?. ",
    "ankushchadha": "Need to query influxDB. There are tag keys stored per measurement that includes container name and the machine name (cadvisor instance)\nAnkush\n\nOn Dec 22, 2016, at 4:35 AM, houzehong notifications@github.com wrote:\nhttps://cloud.githubusercontent.com/assets/9264468/21425723/c77bfa0e-c885-11e6-90f2-15fd11c42ed3.png\nthis is the all measurements.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub https://github.com/google/cadvisor/issues/1563, or mute the thread https://github.com/notifications/unsubscribe-auth/AVvkLsBJmE6osBo8u7mC2njiPLWwHj9Mks5rKm6ogaJpZM4LT8NI.\n\n\n. ",
    "guydou": "Hi maybe you will know,\nHow can I derive the utilization of the CPU, out values in cpu_usage_*?  \nI also store the cadvisor stats in influxdb \nthanks . ",
    "VariantMonkey": "@guydou I get it from the source of cadvisor,\n\nit is calculus.\nbut i don't end up in the way,i write a collect server\uff0cand it write the stats of each container to influxdb,by collect the outputstream of the command\uff08docker stats $yourcontainer --no-stream\uff09.the result like this:\n\nthe data don't need calculate\u3002. ",
    "getvivekv": "cadvisor push the cpu core information in the 'cpu_usage_per_cpu' measurement . ",
    "muralibala": "How do I run cAdvisor on a different port. I have Jenkins running on port 8080. This is what I have tried but i am not able to bring up the instance on 8090. TIA\nsudo docker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8090:8080 \\\n  --name=cadvisor \\\n  google/cadvisor:latest  . ",
    "mohammedna": "docker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8090:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  google/cadvisor:latest\nThis works for me as well, make sure the port 8090 is not being blocked.. . ",
    "EdSchouten": "I think it's not just disk I/O that's not present, but also file system space/inode statistics, right? We would be very interested in having such information exported as well, as it allows us to keep an eye on usage of EBS volumes.. Would it make sense to open up a ticket elsewhere?. ",
    "digiwhite1980": "Hi,\nI've tried the solution above but it did not work out. Further I took a loot at the current pull requests. There is a pull request regarding logstash / JSON storage driver. This  pull request seems like a good solution for the current problem I'm facing. Could we get an update on this pull request as it's been out there for over a half year.. ",
    "wroujoulah": "+1. ",
    "SpComb": "Thanks for getting this fixed promptly!\nIs there's going to be a cAdvisor 0.24.x or other release containing this fix in the near future?. > Is there a workaround for this (from the user's perspective)?\nThe workaround that we used was to stop using the /api/v1.2/docker/ API, and instead, query the /api/v1.2/subcontainers API. The subcontainers API will have both of the overlapping Docker container entries (/docker/...  and /system.slice/var-lib-docker-containers-...-shm.mount). We just ignore any entry ending in *.mount.. ",
    "nicolinux": "Is there a workaround for this (from the user's perspective)?. ",
    "rata": "@derekwaynecarr just curious (I'm hitting this), will this get to a 1.5.x or 1.6 kubernetes release? (as far as I know, cadvisor is included in the kubelet, so there is no easy way to fix this).\nIIUC, k8s 1.5 and 1.6 use these cadvisor versions:\nhttps://github.com/kubernetes/kubernetes/blob/release-1.5/Godeps/Godeps.json#L1141 and 1.6 https://github.com/kubernetes/kubernetes/blob/release-1.6/Godeps/Godeps.json#L1239\nBut 1.6 \"revision\" in json, c30a9e7d3642fffb422f08be34a7bbc15d69cdbf, includes it. So I guess this will be fixed in 1.6 IIUC.\nAny change to fix it in 1.5 too?\nThanks! :). @dashpole thanks!. ",
    "alinoeabrassart": "Hello,\nAny update on this ? We'd really like to use it here. I can spend some time on the subject if necessary (though I'm not fluent in go) Let me know.... ",
    "abessifi": "Hi, is there a workaround about this \"issue\" ?. @derekwaynecarr @pmorie as this fix seems to still en progress, could you please point out a workaround to avoid getting these messages in the kubernetes/openshift logs ?\nOpenShift: v1.5\nDocker: 1.12.6\nCentOS: 7.4. ",
    "NickrenREN": "@k8s-bot test this. @dashpole  Thanks. Done, ptal. ",
    "nlopez": "I had success using the docker run --device param:\ndocker run \\\n  -v /:/rootfs:ro \\\n  -v /var/run:/var/run:rw \\\n  -v /sys:/sys:ro \\\n  -v /var/lib/docker:/var/lib/docker:ro \\\n  --device /dev/zfs:/dev/zfs \\\n  -p 8080:8080 \\\n  --rm \\\n  google/cadvisor:latest\nI tried the (r)eadonly flag for the /dev/zfs device mapping and while the errors went away, I wasn't getting filesystem stats.. ",
    "aaronjwood": "My boot drive is running btrfs but my other drives are setup for RAIDZ2. Docker reports that it's using the btrfs storage driver which sounds right but I do have some of my containers mapping things back into a ZFS dataset. I can confirm that --device /dev/zfs:/dev/zfs is necessary in this scenario as well. --privileged also takes care of this but that is too broad for me :). @andyxning it's not really that bad but yeah, it's a little too much for me to stick on my server full time. I see it using between 5-18% CPU and it spikes around wildly. I tried tuning it a bit using settings similar to what people were talking about here https://github.com/google/cadvisor/issues/1498 which made it idle better. It still spikes pretty high (0.5-9% CPU) though.. @andyxning sure, I totally expect that. I didn't think it would be a silver bullet for my case. I was just suggesting that cAdvisor take advantage of the big optimizations from newer versions of the language. My problem is (somewhat) orthogonal to what I'm asking.. ",
    "from-nibly": "Are there any other solutions to this? I would like to run this as a daemonset in kubernetes but kubernetes does not have a passthrough for the --device flag. . I tried the latest released version v0.27.4 and the latest pre-release v0.29.0 and I get the first error error unless I mount -v /dev/zfs:/dev/zfs in which case I get the seccond error are their other mounts I am not getting right?\nI have mounted the following as volumes in kubernetes (should be equivelent to -v)\n-v /mnt/data/docker:/var/lib/docker:ro\n-v /:/rootfs:ro\n-v /var/run:/var/run:rw\n-v /sys:/sys:ro\n-v /dev/zfs:/dev/zfs\nIt seems to work fine if I run manually on the host with the docker run command and use --device /dev/zfs:/dev/zfs. Ahh ok. So I set privileged: true in the containers securityContext and everything is working fine. thanks!. ",
    "kusmierz": "Unfortunately it doesn't work on a swarm (Note: This option is ignored when deploying a stack in swarm mode with a (version 3) Compose file. https://docs.docker.com/compose/compose-file/#devices). privileged flag is unsupported as well.... ",
    "krzyzacy": "@k8s-bot test this. Hummmm the ssh key is not changed on the jenkins agent, is there any changes to service account in the cadvisor project?\ncc @kubernetes/test-infra-maintainers. that alias did not work well :-(. cc @ixdy @Random-Liu . oh yeah I did get\nConnection Failed\nThe 32KB metadata limit for this project has been exceeded. Please delete some common or instance specific metadata or SSH keys.\nError code: 27\nError message: Max metadata size limit exceeded\nwhen I try to ssh to pr-21 from pantheon.. there's a million keys for pr-23, is that normal?. I was out yesterday afternoon so nothing's been done yet.\n@ixdy shall I pop some keys and push the ones for pr-21 and pr-80?\nbtw, it's stated in gcloud site. @k8s-bot test this. @k8s-bot test this. Seems user configuration on e2e-cadvisor-rhel-7 is wrong? I re-added jenkins and see if it helps.. magically fixed... @fejta seems we need to migrate context here as well. @fejta ./migratestatus --retire=\"Jenkins GCE e2e\" --org=kuberneres --repo=cadvisor --tokenfile=token right?. ohhhh --org is google here. @dashpole have run the above command but seems the context did not retire, @fejta any idea?. from our side, bootstrap will abort presubmit runs if your branch has merge conflicts with HEAD. I think it doesn't have to be up-to-date, as long as it does not have merge conflicts, it's fine.. I0117 22:29:28.446] Call:  git fetch --quiet --tags https://github.com/kubernetes/cadvisor master +refs/pull/1868/head:refs/pr/1868\nW0117 22:29:28.682] fatal: could not read Username for 'https://github.com': No such device or address\nwhoops it's not in kubernetes\n/shrug. https://prow.k8s.io/log?job=pull-cadvisor-e2e&id=5 ^^. /test pull-cadvisor-e2e. probably that's a change from gcloud side. @BenTheElder hummm, yeah sure this probably should be handled by bootstrap. feel free to drop the switch account back part, we can make a bootstrap change. /retest. /test pull-cadvisor-e2e. /test pull-cadvisor-e2e. /test pull-cadvisor-e2e. GREEN!. nit: add a comment say we need to switch back using this service account for uploading logs etc. --strict-host-key-checking=no. add --zone as well. lol the env binded was CADVISOR_APPLICATION_CREDENTIALS. also you want to switch back the service account regardless of test results. hummm, you can store exit value from runner and exit with that in the end, and don't set -e?. ",
    "liftedkilt": "737 containers, 'fs.inotify.max_user_watches' is set to 8192.       5  875708 root     java\n      5  681845 root     java\n      5   63880 root     java\n      5   61450 root     java\n      5   61448 root     java\n      5   57620 root     java\n      5   52067 root     java\n      5   49636 root     java\n      5   47877 root     java\n      5   45639 root     java\n      5   43724 root     java\n      5   41675 root     java\n      5   40566 root     java\n      5   40553 root     java\n      5 3951445 root     java\n      5 3938788 root     java\n      5 3626989 root     java\n      5 3582307 root     java\n      5   34833 root     java\n      5   34818 root     java\n      5 3362735 root     java\n      5  254863 root     java\n      5  250666 root     java\n      5  244775 root     java\n      5  234905 root     java\n      5  224418 root     java\n      5       1 root     systemd\n      5  185994 root     java\n      5 1827734 root     java\n      5 1326212 root     java\n      4   63878 root     java\n      2 2722168 root     cadvisor\n      2    1791 root     polkitd\n      1    9828 root     agetty\n      1     906 root     systemd-udevd\n      1 3010109 wgf      systemd\n      1  182943 root     java\n      1    1794 dnsmasq  dnsmasq\n      1    1737 message+ dbus-daemon\n      1    1720 root     acpid\n      1    1718 root     accounts-daemon\n      1    1564 systemd+ systemd-timesyn. ",
    "tallclair": "Usually it should be OK to watch a single common cgroup subsystem (e.g. cpu), but that would risk missing containers that don't use that subsystem. That is probably a rare case that could be addressed with a configuration option though. I'm not super familiar with this part of the code, so perhaps someone more familiar can chime in. @vishh ?. @naveenkandakur - It looks like something went wrong. Was there a specific change in there you were trying to create a pull request for?. Hmm, any idea why https://github.com/google/cadvisor/blob/master/build/jenkins_e2e.sh#L25-L32 didn't catch this?. Also, what was the PR that triggered this? It looks like there haven't been any changes to the UI in the past 4 months?. @dashpole friendly ping. I'm not familiar with elastic search. Perhaps someone else in the community can chime in.. /lgtm. Thanks for the tests. We've tried to avoid dependencies on Kuberneties to avoid the circular dependency, but I think it's ok in this case since it's a small utility package, and only used for testing.. /lgtm\nWhat's the issue? Are we going to be able to fix it and switch back to fsnotiy?. Can we cherrypick this into the 0.26 & 0.27 branches, and pull those into Kubernetes 1.7 & 1.8 respectively?. Took a closer look at this, and it's not going to be that simple. It needs to account for:\n\nUse of inline scripts (use a hash)\nLoading external content\n. I believe the containers page is served from here: https://github.com/google/cadvisor/blob/04437130ab5e4358ec99731b95f58eebe922c5db/pages/containers.go#L166\n\nAnd the docker page from here: https://github.com/google/cadvisor/blob/04437130ab5e4358ec99731b95f58eebe922c5db/pages/docker.go#L54\nAn easy way to get started is to to add the header I proposed in the issue description, load the UI in chrome, and check the developer tools. There should be errors there (a lot) about all the things that couldn't be loaded, and how to fix them.\nFor now, I think we should just whitelist externally loaded content (don't use a hash), and reduce (or eliminate) the use of inline scripts. Also, use the same headers for all pages - I'm not sure what the best way to do that is, probably just a function that you call from every UI handler.\nThanks for picking this up!. Can you describe the code changes, or better yet add the original commit so I can see the diff?. /lgtm. Hmm, you need the write permission to connect to a socket, but maybe the volume doesn't need to be writeable?. Should we make this a whitelist instead of a blacklist?. I meant that maybe the NewPrometheusCollector function should take a whitelist. If the plan is to call this from Kubernetes, I think that makes more sense (so we don't need to update k8s for metrics it's not using). WDYT?. test failure looks legit? Other than that, lgtm.. I agree this would be useful, but we have previously resisted adding too much k8s-specific stuff to cadvisor, and this is relying on internal k8s details.\nIs there a way to pipe this information through from kubernetes directly instead?. nit: just delete this. I wonder whether it would be more useful to specify this as MaxAge, and update the stats if they are older. 0 means always update, and something else (-1?) means never update.. nit: create the timer outside the loop, and use Reset here.. drain the finishedChan after this step. I.e.\ngo\nfor {\n  select finishedChan := <-c.onDemandChan:\n    close(finishedChan)\n  default:\n    break\n}. Yeah, I think it's still preferable. It's definitely useful outside of k8s, but even for the k8s usecase, there is probably some max age you could set for the memcg trigger (e.g. 10ms). It means that if a burst of requests came in simultaneously (for whatever reason), they can reuse the cached results.. why not? If there are multiple routines waiting for new stats and we just computed new stats, we should unblock them?. nit: add \"that housekeeping is finished\". nit: make this a defer. Use Run instead of Return to ensure this is called, and grab the current time.. I would like to see a few other cases: non-zero call get's stats if necessary, but not if unnecessary, nil call does not get new stats, and multiple simultaneous calls only get stats once, and all return. You might need to plumb a fakeclock through to make this non-flaky.. Correct me if I'm wrong, but I think you can get away with not mocking this:\n```go\nonDemandCache := make(chan chan struct{}, numConcurrentCalls)\nfor i := 0; i < numConcurrentCalls; i++ {\n    go func() {\n        c.OnDemandHousekeeping(0time.Second)\n        waitForHousekeeping.Done()\n    }\n    // Wait for work to be queued\n    onDemandCache = append(onDemandCache, <-c.onDemandChan)\n}\n// Requeue work:\nfor _, ch := range onDemandCache {\n    c.OnDemandChan <- ch\n}. nit: I'd prefer a slice for the cache. It makes the requeue more straightforward.. Does this correctly handle the case when there are more than 2 layers created simultaneously? I'm not sure what entryPath & containerName would be in that case.. for Kubernetes* versions ...\nsame below.. We want to make sure this is kept up-to-date, but I'm not sure what the best way to do that is... We could have a test that parses the release notes headers and ensures this is kept in sync?. Let's use k8s.gcr.io/cadvisor instead. It should point to the same place.. nit: prefer\nyaml\ncommand:\n  - /usr/bin/cadvisor\n  - --logtostderr\narguments:\n- --disable_metrics=percpu,disk,network,tcp,udp\n- --docker_only\nAlternatively, move the command to the base template. let's add a cadvisor namespace. nit: add automountServiceAccountToken: false. I think we should either add an optional overlay with the kubernetes default arguments:\nyaml\n            - --housekeeping_interval=10s\n            - --max_housekeeping_interval=15s\n            - --event_storage_event_limit=default=0\n            - --event_storage_age_limit=default=0. I'm surprised cAdvisor doesn't need to run as privileged. I guess that's just because it reads everything from the mounted hostPath volumes?. It would be great if we could make this readOnly... do you know why cAdvisor requires this to be writeable?. As of k8s 1.9, there's apps/v1. Does it still work if you run with seccomp?\nannotations:\n      seccomp.security.alpha.kubernetes.io/pod: 'docker/default'. Add:\nyaml\nsecurityContext:\n  runAsNonRoot: true\n  readOnlyRootFilesystem: true  # I think this should work?\n  allowPrivilegeEscalation: false. 6061? https://github.com/GoogleCloudPlatform/k8s-stackdriver/blob/master/prometheus-to-sd/main.go#L62. Consider adding a link to this from the main cadvisor readme. I'd prefer to keep this versioned.. ",
    "wangzi19870227": "maybe your inotify resources exhausted, increase max_user_watches works for me.\n$ cat /proc/sys/fs/inotify/max_user_watches  # default is 8192\n$ sudo sysctl fs.inotify.max_user_watches=1048576 # increase to 1048576. ",
    "zuozuo": "@wangzi19870227 thanks, works for me. ",
    "bamb00": "@timstclair Is there anyway I can get the listing of the actual culprit? I cannot tell if the kubelet process is taking up majority of the watches even though the error is from kubelet.\n# cat /proc/sys/fs/inotify/max_user_watches\n8192\n\n# tail -f /var/log/messages\ntail: inotify resources exhausted\ntail: inotify cannot be used, reverting to polling\n\n# journalctl -u kubelet | grep device\nAug 13 12:48:05 sys-multinode-minion-1 kubelet[81153]: W0813 12:48:05.682241   81153 raw.go:87] Error while processing event (\"/sys/fs/cgroup/cpuset/kubepods/burstable/podd405e4ff-9aab-11e8-aaea-70695a988249/485ae7dc7e34b76666815053f75a705d03ca2c0ee3ba2d2edbb6fa4128ec7810\": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/cpuset/kubepods/burstable/podd405e4ff-9aab-11e8-aaea-70695a988249/485ae7dc7e34b76666815053f75a705d03ca2c0ee3ba2d2edbb6fa4128ec7810: no space left on device\n\n\n#  find /proc/*/fd -lname anon_inode:inotify |    cut -d/ -f3 |    xargs -I '{}' -- ps --no-headers -o '%p %U %c' -p '{}' |    uniq -c | sort -nr\nfind: \u2018/proc/81153/fd/40\u2019: No such file or directory\nfind: \u2018/proc/81153/fd/88\u2019: No such file or directory\n          6      1 root     systemd\n          4  81153 root     kubelet\n          2   4605 gdm      gnome-shell\n          2   1882 root     NetworkManager\n          2   1805 polkitd  polkitd\n          1  83112 root     prometheus-conf\n          1   6340 gdm      ibus-engine-sim\n          1   6065 colord   colord\n          1   6044 gdm      gsd-sound\n          1   5983 gdm      gsd-color\n          1   5964 gdm      gsd-xsettings\n          1   5952 root     packagekitd\n          1   5876 gdm      ibus-portal\n          1   5871 gdm      ibus-x11\n          1   5862 gdm      ibus-dconf\n          1   5825 gdm      ibus-daemon\n          1   5057 gdm      pulseaudio\n          1   4480 gdm      dbus-daemon\n          1   4409 gdm      dbus-daemon\n          1   4398 gdm      gnome-session-b\n          1   4047 nobody   dnsmasq\n          1   2459 root     crond\n          1   2450 root     rsyslogd\n          1   1846 avahi    avahi-daemon\n          1   1841 root     abrt-watch-log\n          1   1838 root     abrt-watch-log\n          1   1837 root     abrtd\n          1   1813 dbus     dbus-daemon\n          1   1802 root     accounts-daemon\n          1   1312 root     systemd-udevd\n\n. @timstclair Is there anyway I can get the listing of the actual culprit? I cannot tell if the kubelet process is taking up majority of the watches even though the error is from kubelet.\n# cat /proc/sys/fs/inotify/max_user_watches\n8192\n\n# tail -f /var/log/messages\ntail: inotify resources exhausted\ntail: inotify cannot be used, reverting to polling\n\n# journalctl -u kubelet | grep device\nAug 13 12:48:05 sys-multinode-minion-1 kubelet[81153]: W0813 12:48:05.682241   81153 raw.go:87] Error while processing event (\"/sys/fs/cgroup/cpuset/kubepods/burstable/podd405e4ff-9aab-11e8-aaea-70695a988249/485ae7dc7e34b76666815053f75a705d03ca2c0ee3ba2d2edbb6fa4128ec7810\": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/cpuset/kubepods/burstable/podd405e4ff-9aab-11e8-aaea-70695a988249/485ae7dc7e34b76666815053f75a705d03ca2c0ee3ba2d2edbb6fa4128ec7810: no space left on device\n\n\n#  find /proc/*/fd -lname anon_inode:inotify |    cut -d/ -f3 |    xargs -I '{}' -- ps --no-headers -o '%p %U %c' -p '{}' |    uniq -c | sort -nr\nfind: \u2018/proc/81153/fd/40\u2019: No such file or directory\nfind: \u2018/proc/81153/fd/88\u2019: No such file or directory\n          6      1 root     systemd\n          4  81153 root     kubelet\n          2   4605 gdm      gnome-shell\n          2   1882 root     NetworkManager\n          2   1805 polkitd  polkitd\n          1  83112 root     prometheus-conf\n          1   6340 gdm      ibus-engine-sim\n          1   6065 colord   colord\n          1   6044 gdm      gsd-sound\n          1   5983 gdm      gsd-color\n          1   5964 gdm      gsd-xsettings\n          1   5952 root     packagekitd\n          1   5876 gdm      ibus-portal\n          1   5871 gdm      ibus-x11\n          1   5862 gdm      ibus-dconf\n          1   5825 gdm      ibus-daemon\n          1   5057 gdm      pulseaudio\n          1   4480 gdm      dbus-daemon\n          1   4409 gdm      dbus-daemon\n          1   4398 gdm      gnome-session-b\n          1   4047 nobody   dnsmasq\n          1   2459 root     crond\n          1   2450 root     rsyslogd\n          1   1846 avahi    avahi-daemon\n          1   1841 root     abrt-watch-log\n          1   1838 root     abrt-watch-log\n          1   1837 root     abrtd\n          1   1813 dbus     dbus-daemon\n          1   1802 root     accounts-daemon\n          1   1312 root     systemd-udevd\n\n. ",
    "machinekoder": "I think it's worth mentioning that the inotify limit is a property of the host system, not the Docker image itself. So if you get this error, increase the inotify limit in you host system, not inside the Docker image.. ",
    "yousong": "In case it may help, I just wrote a few lines of shell script to count inotify watches used by each inotify instance.  We have a kubelet installation using up almost all the max_user_watches quota\nhttps://github.com/yousong/gists/blob/master/shell/inotify_watchers.sh. ",
    "jjqq2013": "@googlebot I signed it!. @dashpole ok, moved to running.cmd. Note: cAdvisor command itself does not show error to user, it just fails to list all containers in its web UI, shows an error \"Can not connect to docker daemon\".\nSee screenshot:\n\n\nAnd this is how i rua my docker daemon with user namespace enabled.( in docker-machine on Mac OS X)\n```\nq@mac$ docker-machine ssh default\ndocker@default:~$ cat /etc/subuid \ndockremap:165536:65536\ndocker@default:~$ cat /etc/subgid \ndockremap:165536:65536\ndocker@default:~$ cat /etc/docker/daemon.json\n{\n  \"userns-remap\": \"default\"\n}\ndocker@default:~$ sudo adduser --system dockremap\nadduser: user 'dockremap' in use\ndocker@default:~$ exit\nq@mac$ docker-machine restart default\n```\n. @dashpole  sorry i used another email to commit, now i have added the email to github account settings. It seems now the author icon in my commit correctly show my avatar.. cAdvisor command itself does not show error to user, it just failed to list all containers in its web UI, shows an error \"Can not connect to docker daemon\".\n. ",
    "sebhoss": "I came up with the following config which seems to work ok for now:\n```\nserver {\n    listen 443 ssl;\n        server_name dashboards.domain.tld;\nlocation /cadvisor/ {\n    proxy_pass http://cadvisor-host:12345/;\n    proxy_redirect ~^/containers/ /cadvisor/containers/;            \n    proxy_redirect ~^/docker/ /cadvisor/docker/;\n}\n\n}\n```\ncAdvisor is available at https://dashboards.domain.tld/cadvisor once the config is loaded. ",
    "mfominov": "@sebhoss add 1 more redirect\nproxy_redirect ~^/metrics/ /cadvisor/metrics/;. ",
    "kosiraljaz": "Same problem, downgrading to docker 1.12.5 solves the issue for me.... ",
    "snavien": "I also have this issue in 1.12.6. ",
    "andrewlarioza": "Same on AWS ECS instances which also runs on 1.12.6.. ",
    "alicek106": "I have no problem in 1.12.6 version but not in later versions(1.13 and later). Anyone knows the exact root cause?. ",
    "JumpingYang001": "The error is defined here: https://github.com/google/cadvisor/blob/d7a44cb1a2c66e1688ccdc5d09e56069eecb659a/vendor/github.com/docker/engine-api/client/errors.go#L9\nand return here : https://github.com/google/cadvisor/blob/d7a44cb1a2c66e1688ccdc5d09e56069eecb659a/vendor/github.com/docker/engine-api/client/request.go#L102\n. Please notice that https://github.com/google/cadvisor/blob/master/docs/running.md\nRHEL and CentOS need  --privileged=true  and  --volume=/cgroup:/cgroup:ro \\ . ",
    "BradleyA": "I am using Ubuntu 14.04.5 LTS. ",
    "ebastos": "This seems to be fixed now:\nDriver Status\nDocker Version 17.09.0-ce\nKernel Version 3.13.0-132-generic\nOS Version Ubuntu 14.04.5 LTS\n. ",
    "CBR09": "Yea, I can see it, we need a mechanism which allowing user select interface as they want.. ",
    "Alger7w": "i met the same problem..... ",
    "mostolog": "Already added +1.\nHaving this blocked by #1458 for so long doesn't make any sense to me.\nAdding this driver as \"elasticsearch5\" while the other is still mantained as \"elasticsearch\" would do the trick.\n. ",
    "immesys": "I have the same problem. Can someone give a pointer to how this is meant to work? I have tried cadvisor inside and ouside a container and in both configurations it cannot find the file which exists inside the container\nedit: for context, the file used to exist in the image but it seemed like cadvisor did not translate \"localhost\" in the endpoint to the container IP, so now the file gets created in the entrypoint of the container to refer to the container IP, so there is a race between container creation and when the file exists. I would assume cadvisor keeps checking with a backoff, but perhaps that is not the case?. I see, I needed recursive=true. ",
    "tacy": "@immesys  Could you use init containers to set pod ip? . ",
    "timchenxiaoyu": "I signed it!. we run container in vm,vm mount many disks, in qingcloud vm limit mount 6 disks,so we must monitor   this !  get mounted disk intime. ",
    "avaitla": "Similar issue with v0.24.1 on Ubuntu 16.04.2 (Note that Ubuntu 14.04.2 does not report the error)\n# ./cadvisor --version\ncAdvisor version v0.24.1 (ae6934c)\n# ./cadvisor\nE0309 02:17:15.862421   19527 factory.go:291] devicemapper filesystem stats will not be reported: unable to find thin_ls binary\n\nThe validate endpoint gives:\ncAdvisor version: v0.24.1\n\nOS version: Ubuntu 16.04.2 LTS\n\nKernel version: [Supported and recommended]\n    Kernel version is 4.4.0-64-generic. Versions >= 2.6 are supported. 3.0+ are recommended.\n\n\nCgroup setup: [Supported and recommended]\n    Available cgroups: map[cpuset:1 blkio:1 memory:1 freezer:1 perf_event:1 net_prio:1 hugetlb:1 cpu:1 cpuacct:1 devices:1 net_cls:1 pids:1]\n    Following cgroups are required: [cpu cpuacct]\n    Following other cgroups are recommended: [memory blkio cpuset devices freezer]\n    Hierarchical memory accounting enabled. Reported memory usage includes memory used by child containers.\n\n\nCgroup mount setup: [Supported and recommended]\n    Cgroups are mounted at /sys/fs/cgroup.\n    Cgroup mount directories: blkio cpu cpu,cpuacct cpuacct cpuset devices freezer hugetlb memory net_cls net_cls,net_prio net_prio perf_event pids systemd \n    Any cgroup mount point that is detectible and accessible is supported. /sys/fs/cgroup is recommended as a standard location.\n    Cgroup mounts:\n    cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0\n    cgroup /sys/fs/cgroup/pids cgroup rw,nosuid,nodev,noexec,relatime,pids 0 0\n    cgroup /sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0\n    cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n\n\nDocker version: [Supported and recommended]\n    Docker version is 1.12.1. Versions >= 1.0 are supported. 1.2+ are recommended.\n\n\nDocker driver setup: [Supported and recommended]\n    Docker exec driver is . Storage driver is devicemapper.\n\n\nBlock device setup: [Supported, but not recommended]\n    None of the devices support 'cfq' I/O scheduler. No disk stats can be reported.\n     Disk \"dm-4\" Scheduler type \"none\".\n     Disk \"sda\" Scheduler type \"deadline\".\n     Disk \"vda\" Scheduler type \"none\".\n     Disk \"dm-0\" Scheduler type \"none\".\n     Disk \"dm-1\" Scheduler type \"none\".\n     Disk \"dm-2\" Scheduler type \"none\".\n     Disk \"dm-3\" Scheduler type \"none\".\n\n\nInotify watches:\n\n\nManaged containers: \n    /system.slice/mdadm.service\n    /system.slice/iscsid.service\n    /system.slice/keyboard-setup.service\n    /system.slice/cloud-init-local.service\n    /system.slice/cloud-final.service\n    /system.slice/kmod-static-nodes.service\n    /system.slice/systemd-udev-trigger.service\n    /system.slice/lvm2-lvmetad.service\n    /system.slice/rsyslog.service\n    /user.slice/user-0.slice/session-4.scope\n    /system.slice/systemd-tmpfiles-setup.service\n    /system.slice/irqbalance.service\n    /system.slice/system-serial\\x2dgetty.slice\n    /init.scope\n    /system.slice/systemd-journald.service\n    /system.slice/snapd.service\n    /user.slice/user-0.slice\n    /system.slice/console-setup.service\n    /system.slice/lxd-containers.service\n    /system.slice/docker.service\n    /system.slice/apparmor.service\n    /system.slice/setvtrgb.service\n    /system.slice/systemd-timesyncd.service\n    /system.slice\n    /system.slice/lxcfs.service\n    /system.slice/grub-common.service\n    /docker\n    /system.slice/ufw.service\n    /system.slice/systemd-random-seed.service\n    /system.slice/systemd-update-utmp.service\n    /system.slice/systemd-machine-id-commit.service\n    /system.slice/systemd-tmpfiles-setup-dev.service\n    /system.slice/systemd-user-sessions.service\n    /system.slice/networking.service\n    /system.slice/acpid.service\n    /system.slice/systemd-sysctl.service\n    /system.slice/rc-local.service\n    /user.slice/user-0.slice/user@0.service\n    /system.slice/system-getty.slice\n    /user.slice/user-0.slice/session-3.scope\n    /system.slice/apport.service\n    /system.slice/lvm2-monitor.service\n    /system.slice/resolvconf.service\n    /system.slice/ondemand.service\n    /system.slice/polkitd.service\n    /user.slice\n    /system.slice/ssh.service\n    /system.slice/cron.service\n    /system.slice/open-iscsi.service\n    /system.slice/systemd-udevd.service\n    /system.slice/accounts-daemon.service\n    /user.slice/user-0.slice/session-1.scope\n    /system.slice/systemd-remount-fs.service\n    /\n    /system.slice/cgroupfs-mount.service\n    /system.slice/cloud-init.service\n    /system.slice/atd.service\n    /system.slice/systemd-journal-flush.service\n    /system.slice/cloud-config.service\n    /system.slice/systemd-modules-load.service\n    /system.slice/dbus.service\n    /system.slice/systemd-logind.service\n\nAlso an issue with: Docker version is 17.03.0-ce\n. \n. ",
    "JerryDog": "yum install device-mapper-persistent-data. try to reinstall. or get the lastest cadvisor.\nps: My env is CentOS7.2.\ncat cadvisor\\devicemapper\\util.go\n`// ThinLsBinaryPresent returns the location of the thin_ls binary in the mount\n// namespace cadvisor is running in or an error.  The locations checked are:\n//\n// - /sbin/\n// - /bin/\n// - /usr/sbin/\n// - /usr/bin/\n//\n// The thin_ls binary is provided by the device-mapper-persistent-data\n// package.\nfunc ThinLsBinaryPresent() (string, error) {\n    var (\n        thinLsPath string\n        err        error\n    )\nfor _, path := range []string{\"/sbin\", \"/bin\", \"/usr/sbin/\", \"/usr/bin\"} {\n    // try paths for non-containerized operation\n    // note: thin_ls is most likely a symlink to pdata_tools\n    thinLsPath = filepath.Join(path, \"thin_ls\")\n    _, err = os.Stat(thinLsPath)\n    if err == nil {\n        return thinLsPath, nil\n    }\n}\n\nreturn \"\", fmt.Errorf(\"unable to find thin_ls binary\")\n\n}`. ",
    "phhutter": "@JerryDog nope, already installed: \n```\n[root@localhost ~]# yum install device-mapper-persistent-data\nFedora 26 - x86_64 - Test Updates                                                                                                                                            2.2 MB/s |  17 MB     00:07\nFedora 26 - x86_64                                                                                                                                                           2.2 MB/s |  53 MB     00:23\nLast metadata expiration check: 0:00:00 ago on Wed Apr 19 18:19:05 2017 CEST.\nPackage device-mapper-persistent-data-0.6.3-3.fc26.x86_64 is already installed, skipping.\nDependencies resolved.\nNothing to do.\nComplete!\n```. ",
    "cruwe": "At the moment, I have a similiar problem: When /var/lib/docker is inside of a container and ZFS, then only docker can use only VFS as storage backend. I am passing zvols into the container at the moment.. docker snapshots and clones a ZFS dataset to provision images and containers. To be able to snapshot and clone an image inside of a container, the container needs access to ZFS - on Solaris, there is something called zfs delegation, thats the zoned property on Solaris and the jailed property on FreeBSD. I do not know of any similar feature on Linux and it is claimed (https://stgraber.org/2016/04/13/lxd-2-0-docker-in-lxd-712/, https://docs.docker.com/engine/userguide/storagedriver/selectadriver/ and somewhere newer which I cannot find atm) that the overlay driver does not / cannot work with ZFS.\nHowever, I am not terribly unhappy with the zvol solution atm and besides, that's the lab setup on my notebook and not meant for production ...\nCheers!. ",
    "omalley": "I hit the same problem. Can you describe what your 'zvol solution' is?\nThanks, @cruwe . ",
    "harsheetbhatia": "Nothing , I was to download and run the container in my environment and nothing else.\nRegards,\nHarsheet Bhatia\n\nFrom: Tim St. Clair notifications@github.com\nSent: Tuesday, February 28, 2017 2:47:11 AM\nTo: google/cadvisor\nCc: harsheetbhatia; Mention\nSubject: Re: [google/cadvisor] Release v0.24 (#1605)\nWhat were you trying to do with this PR?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://github.com/google/cadvisor/pull/1605#issuecomment-282856091, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AY3SCAQG6rN01sX2rxc6fcRjaKLkv5tbks5rgz1XgaJpZM4MNky3.\n. ",
    "Mr-Right2": "I cannot find any events at that time that might have caused the issue. The same thing happened a few times more but cAdvisor did not restart (docker ps shows Up 2 weeks as you can see). ",
    "hiscal2015": "+1 I have the same issue.. Any update on this, I have plenty of invalid container restart alerts which makes me crazy.... @dashpole Thanks for the answer, but what does \"Monitoring docker directly for this information is probably the best solution\" mean?. @dashpole Thanks for the advise. For stopped containers, there will be no metrics by cpu or memory rather than returning 0, and seems \"absent\" can not handle multiple container stop events, so maybe a customized exporter would be a better choice.. looks it has not been merged. . ",
    "maptile": "Any update on this? We have meet some problems described in #1704 in 0.26.2 and fall back to 0.25.0, then meet this problem. Aha, still receive some restart emails per day. Hope it can be resolved in the near future.\nThe graph in Prometheus\n\nThe docker ps\nCONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS              PORTS                     NAMES\n5f769338aef8        cadvisor:latest          \"/usr/bin/cadvisor...\"   43 hours ago        Up 43 hours         0.0.0.0:32779->8080/tcp   production_cadvisor_1\nb68be65fec5f        couchdb                  \"couchdb\"                5 days ago          Up 5 days           0.0.0.0:32782->5984/tcp   couchdb\n2ae4f05dd35c        alpine-discover:latest   \"node index.js\"          10 days ago         Up 10 days          0.0.0.0:8379->8379/tcp    production_discover_1\n9ee6800151c4        node-exporter:latest     \"/bin/node_exporter\"     10 days ago         Up 10 days          0.0.0.0:32769->9100/tcp   production_node_exporter_1\n. Yes, and sorry for the duplicate issue. Should I close it?\nI've searched before open this issue, but nothing related was found. May be the keyword I'm using is wrong :)\n. ",
    "hairyhenderson": "I've been experiencing this bug fairly consistently lately with cAdvisor 0.28.0. After some seemingly-random period of time, the container_start_time_seconds starts climbing:\n\nIt's to the point now that I need to disable my \"container flapping\" alert entirely. I can't trust this metric anymore.. ",
    "sentinelt": "Looks like the problem comes from taking the container creation time from cgroups properties.\nI suspect cgroup.clone_children property of the cgroup is updated although the container has not really been restarted.\nHere is the part from container/common/helpers.go responsible for this:\ngo\n      // Assume unified hierarchy containers.\n      // Get the lowest creation time from all hierarchies as the container creation time.\n      now := time.Now()\n      lowestTime := now\n      for _, cgroupPath := range cgroupPaths {\n          // The modified time of the cgroup directory changes whenever a subcontainer is created.\n          // eg. /docker will have creation time matching the creation of latest docker container.\n          // Use clone_children as a workaround as it isn't usually modified. It is only likely changed\n          // immediately after creating a container.\n          cgroupPath = path.Join(cgroupPath, \"cgroup.clone_children\")\n          fi, err := os.Stat(cgroupPath)\n          if err == nil && fi.ModTime().Before(lowestTime) {\n              lowestTime = fi.ModTime()\n          }\n      }\n      if lowestTime != now {\n          spec.CreationTime = lowestTime\n      }. ",
    "I3olle": "Also having this issue. Weirdly enough I so far only have it on a single host though. Not sure why. ",
    "aackerman": "This is how I expected it to work, but after finding this issue it doesn't seem to be the case :/. ",
    "jmatusewicz": "+1. ",
    "MGD1981": "@dashpole Has any progress been made to this? I've never written any go, but if this is as simple as you make it sound it seems the reward would be well worth the effort. This having been a year ago, are you still happy to review a theoretical PR?. Awesome. Time to refresh myself on how go works! ;-). ",
    "mkumatag": "@dashpole can you merge this PR as tests are passed.. @dashpole what next? Can you please rerun the tests?. ping @dashpole . @k8s-bot test this. @FrenchBen When are you hitting this error?. @dashpole I addressed your comment and refactored the code, PTAL... Thanks @dashpole . Not at all. There are many places this function used and I don't want them to disturb.. This TODO is creating confusion, lemme remove it. Any of the docker versions are not following semver format(Refer Docker Issue 31075) so it does not make sense to use any prebuilt semver library anyways.. Added tests in latest commit., PTAL. ",
    "FrenchBen": "LGTM. @mkumatag you may need a rebase. Looks like the error is linked to a broken git repo:\nimports github.com/jteeuwen/go-bindata: unable to parse output of git config remote.origin.url. @mkumatag I was looking at the output of the ci robot:\nhttps://k8s-gubernator.appspot.com/build/kubernetes-jenkins/pr-logs/pull/google_cadvisor/1623/pull-cadvisor-e2e/179/. ",
    "shamimgeek": "@JinsYin : Thanks alot for correct json :+1: . ",
    "supereagle": "@edevil  @euank Any processs about this PR?. ",
    "edevil": "At this moment I don't know how to solve the problem and I don't have much time to dedicate to this.... ",
    "irisdingbj": "Is there other way to get the filesystem usage per container when using devicemapper as driver?\nThanks a lot!. ",
    "robbiewu007": "Got the same error, after restarted cAdvisor.. robbie@S001:/mydata/projects/docker/mynextcloud$ sudo docker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro -i -t alpine:3.4 /bin/sh\n/ # zfs get -Hp all mydata/mirrors/github/shidenggui\n/bin/sh: zfs: not found\n. ",
    "zette": "According to docker docs, the TLS connection does not need any special API calls. You can use curl for call testing:\n```\nTo use curl to make test API requests, you need to use three extra command line flags:\n$ curl https://$HOST:2376/images/json \\\n  --cert ~/.docker/cert.pem \\\n  --key ~/.docker/key.pem \\\n  --cacert ~/.docker/ca.pem\n```\nSource: https://docs.docker.com/engine/security/https/#daemon-modes. ",
    "maxramqvist": "Seeing the same issue. \nRunning cAdvisor 0.26.1 as container.\ncurl:ing the prometheus endpoint and grep for unique container_label_image 30 times in a row with a seconds pause would get me either 0, 8 or 18. Never the correct 28.\nDocker version 17.05.0-ce, build 89658be\nUbuntu 16.10 kernel 4.8 x86_64. @bakins coool, nice work! All seems well in the latest binary 0.28.0 pre-release. It usually shows up right away in the logs - but now nothing.\nI'm missing it in the Docker image, cadvisor:canary though. There I got something about zfs was missing from $PATH. I forgot to check the version of the cadvisor binary, but should be more recent - right?. ",
    "mindw": "we had our metrics disappear after exactly 24 hours and tracked down the issue to be caused by the period 24 hour rkt-gc.timer.\n kubelet is run natively on the host (kubelet-wrapper not used)\n k8s 1.3.x-1.5.x\n CoreOs 13xx.x\n rkt was used to run one-shot containers during boot.\nEither disabling the rkt-gc.timer or using the kubelet-wrapper resolves the issue.. ",
    "zeisss": "We are seeing this too. Cadvisor as a systemd service, with Docker 1.13.1. No Kubernetes.\n\ncadvisor_version_info{cadvisorRevision=\"d19cc94\",cadvisorVersion=\"v0.26.1\",dockerVersion=\"1.13.1\",kernelVersion=\"3.16.0-4-amd64\",osVersion=\"Debian GNU/Linux 8 (jessie)\"} 1#\n\nThe WebUI shows the disappearing containers just fine.\n. you need to connect to port 8080, since you are running a docker container wird a port forwarding for 8080 to container port 9010.. Running the binary without root permissions fixes the problems, but now container labels are missing. Using the -docker-only flag or accessing docker via tcp/ip leads to no change from the initial behavior.. We currently have a workaround by running cadvisor as an explicit user. this is ok for us, as having the CPU and memory graphs is already a win for us. But afaict this mode is missing the docker container labels as well as network and disk I/O metrics.. @fabxc no, we are still running a 1.x prometheus version - but having Prometheus work around this bug in cadvisor is not a good solution IMO.. We are currently in the progress of updating our DEV cluster to Docker 17.06-ce where we are still seeing this behavior, if run as root (/opt/cadvisor/bin/cadvisor -port 8701 -logtostderr):\n$ while true; do curl -sS docker-host:8701/metrics | fgrep container_cpu_system_seconds_total | wc -l; sleep 1; done\n      28\n      28\n       9\n       9\n       5\n       6\n^C\n```\nHELP cadvisor_version_info A metric with a constant '1' value labeled by kernel version, OS version, docker version, cadvisor version & cadvisor revision.\nTYPE cadvisor_version_info gauge\ncadvisor_version_info{cadvisorRevision=\"057293a\",cadvisorVersion=\"v0.26.0.20+057293a1796d6a-dirty\",dockerVersion=\"17.06.0-ce\",kernelVersion=\"3.16.0-4-amd64\",osVersion=\"Debian GNU/Linux 8 (jessie)\"} 1\n```. ",
    "wangsha": "non-official image: https://hub.docker.com/r/sha1/cadvisor/. ",
    "AleksanderGrzybowski": "Hey I just started going through this tutorial https://botleg.com/stories/monitoring-docker-swarm-with-cadvisor-influxdb-and-grafana/ and I have the same problem. Good to know it's being fixed.. I would also greatly appreciate an update, I did similar to @pangzheng (thanks!)\nFROM google/cadvisor\nRUN apk add --update findutils && rm -rf /var/cache/apk/*\nand it works fine now, but it is strange why it would not work out of the box.. ",
    "Hermain": "An update to the official image would be really nice. . Having the same issue with docker 17.06, prometheus and docker swarm.\nRunning v0.24.1 solved it for me. ",
    "brancz": "My review is mostly based on the resulting metric output, and that lgtm. I only had a chance to glance over the implementation, but generally looks good as well. (coming from sig-instrumentation). Interesting, thanks for the insight @dashpole. Seems like we should maybe work towards exposing the stats API as Prometheus metrics by the kubelet.. @gnufied told me there is something already in the works, maybe he can comment on where we should go next.. @gnufied I don't want to hijack that PR so I'll ask here. Do you think it's reasonable to also add all of those metrics as metrics exposed in the /metrics endpoint of the kubelet?. @jingxu97 that sounds great!\nRegarding:\n\nOne problem is that it seems some users might want to use PVC as index and some want to use PV name\n\nI think this is something we can discuss further in the proposal. Feel free to tag us once you have something ready. :slightly_smiling_face: . Would love to see a dashboard and or alerting rules with these! :slightly_smiling_face: . @tiloso awesome! I can definitely see very nice predict_linear queries to do early alerts on volumes filling up! :slightly_smiling_face: . How about we make this a runtime configuration? As in we can pass via flags mappings of labels, like:\n--prometheus-label-mapping=io.kubernetes.pod.name=pod_name\nAt least that's how I read Kubernetes essentially modifying labels: https://github.com/kubernetes/kubernetes/blob/ae8a046985d1e768dc589509d584b6fef13e900a/pkg/kubelet/cadvisor/cadvisor_linux.go#L75-L101\nThe way I understand it (and I may very well be wrong) the io.kubernetes.pod.name label is a generic docker label.. Happy to discuss a way forward. :slightly_smiling_face: . ",
    "bobrik": "Any chance to get a release with this PR included soon?. Hmm, I applied this patch on 0.25.0 and with -disable_metrics=network,tcp,disk I have disk metrics:\n$ curl -s http://36com10.in.cfops.it:3059/metrics | grep ^container_fs | fgrep one\ncontainer_fs_reads_bytes_total{device=\"/dev/md127\",id=\"/docker/729323ff33006e04043142a4c79bd0028ec5e35f01e38d362a4fe62014620e5d\",image=\"debian:jessie\",name=\"one\"} 0\ncontainer_fs_reads_total{device=\"/dev/md127\",id=\"/docker/729323ff33006e04043142a4c79bd0028ec5e35f01e38d362a4fe62014620e5d\",image=\"debian:jessie\",name=\"one\"} 0\ncontainer_fs_writes_bytes_total{device=\"/dev/md127\",id=\"/docker/729323ff33006e04043142a4c79bd0028ec5e35f01e38d362a4fe62014620e5d\",image=\"debian:jessie\",name=\"one\"} 2.91930112e+09\ncontainer_fs_writes_total{device=\"/dev/md127\",id=\"/docker/729323ff33006e04043142a4c79bd0028ec5e35f01e38d362a4fe62014620e5d\",image=\"debian:jessie\",name=\"one\"} 5122\nThe actual difference in reported metrics between defaults and my case:\n$ diff -rup before.txt after.txt\n--- before.txt  2017-05-18 22:15:13.897839830 +0000\n+++ after.txt   2017-05-18 22:17:25.008781070 +0000\n@@ -30,14 +30,6 @@ container_memory_rss\n container_memory_swap\n container_memory_usage_bytes\n container_memory_working_set_bytes\n-container_network_receive_bytes_total\n-container_network_receive_errors_total\n-container_network_receive_packets_dropped_total\n-container_network_receive_packets_total\n-container_network_transmit_bytes_total\n-container_network_transmit_errors_total\n-container_network_transmit_packets_dropped_total\n-container_network_transmit_packets_total\n container_scrape_error\n container_spec_cpu_period\n container_spec_cpu_quota\n@@ -79,7 +71,6 @@ http_request_duration_microseconds_sum\n http_request_size_bytes\n http_request_size_bytes_count\n http_request_size_bytes_sum\n-http_requests_total\n http_response_size_bytes\n http_response_size_bytes_count\n http_response_size_bytes_sum\nLooks like no disk metrics were dropped. Is this expected?. @dashpole, thanks, makes sense now.. These metrics are supposed to be counters, but if a PID disappears from a cgroup, it takes its delay stats with it. This causes counter to go backward. If counter goes backwards, prometheus detects reset and next rate will be calculated not from increment, but from the whole counter value.\nIs there any way to live with this?. I definitely see counter go backwards, especially for cron.service where PIDs have often limited lifespan. There's no way I'm wrapping around PID space in a few minutes.. I created a ticket and pinged cgroups mailing list about this:\n\nhttps://bugzilla.kernel.org/show_bug.cgi?id=200593. Wow, it was so long I almost file another ticket.\n\nLooks like @nielsole took care of it with schedstats: #1872.\n```\n$ sudo ./getdelays -d -p 18369\nprint delayacct stats ON\nPID 18369\nCPU             count     real total  virtual total    delay total  delay average\n            149067570  9804323000000 10493764364074   508452504298          0.003ms\nIO              count    delay total  delay average\n                    0              0              0ms\nSWAP            count    delay total  delay average\n                    0              0              0ms\nRECLAIM         count    delay total  delay average\n                    0              0              0ms\n```\nThe second field is total delay:\n$ cat /proc/18369/schedstat\n10493764364074 508452504298 149067570. To give some perspective: we have cadvisor instances burning more than one full CPU core with tcp and udp metrics disabled.\n\n. To illustrate difference in resource usage, let's consider the following ugly patch:\n```diff\ndiff --git a/manager/container.go b/manager/container.go\nindex 295479f0..82993ae2 100644\n--- a/manager/container.go\n+++ b/manager/container.go\n@@ -103,6 +103,10 @@ func jitter(duration time.Duration, maxFactor float64) time.Duration {\n }\nfunc (c containerData) Start() error {\n+   if err := c.updateStats(); err != nil {\n+       return err\n+   }\n+\n    go c.housekeeping()\n    return nil\n }\n@@ -501,12 +505,12 @@ func (c containerData) housekeepingTick(timer <-chan time.Time, longHousekeepin\n    case <-timer:\n    }\n    start := c.clock.Now()\n-   err := c.updateStats()\n-   if err != nil {\n-       if c.allowErrorLogging() {\n-           glog.Warningf(\"Failed to update stats for container \\\"%s\\\": %s\", c.info.Name, err)\n-       }\n-   }\n+   // err := c.updateStats()\n+   // if err != nil {\n+   //  if c.allowErrorLogging() {\n+   //      glog.Warningf(\"Failed to update stats for container \\\"%s\\\": %s\", c.info.Name, err)\n+   //  }\n+   // }\n    // Log if housekeeping took too long.\n    duration := c.clock.Since(start)\n    if duration >= longHousekeeping {\ndiff --git a/manager/manager.go b/manager/manager.go\nindex 7e665be9..bad085ed 100644\n--- a/manager/manager.go\n+++ b/manager/manager.go\n@@ -370,17 +370,17 @@ func (self *manager) Start() error {\n    glog.V(2).Infof(\"Recovery completed\")\n// Watch for new container.\n\n\nquitWatcher := make(chan error)\nerr = self.watchForNewContainers(quitWatcher)\nif err != nil {\nreturn err\n}\nself.quitChannels = append(self.quitChannels, quitWatcher)\n// quitWatcher := make(chan error)\n// err = self.watchForNewContainers(quitWatcher)\n// if err != nil {\n//  return err\n// }\n\n// self.quitChannels = append(self.quitChannels, quitWatcher)\n// Look for new containers in the main housekeeping thread.\n-   quitGlobalHousekeeping := make(chan error)\n-   self.quitChannels = append(self.quitChannels, quitGlobalHousekeeping)\n-   go self.globalHousekeeping(quitGlobalHousekeeping)\n+   // quitGlobalHousekeeping := make(chan error)\n+   // self.quitChannels = append(self.quitChannels, quitGlobalHousekeeping)\n+   // go self.globalHousekeeping(quitGlobalHousekeeping)\nreturn nil\n }\n@@ -582,6 +582,10 @@ func (self manager) getContainer(containerName string) (containerData, error)\n }\n\n\nfunc (self manager) getSubcontainers(containerName string) map[string]containerData {\n+   if err := self.detectSubcontainers(containerName); err != nil {\n+       glog.Errorf(\"Error detecting containers: %s\", err)\n+   }\n+\n    self.containersLock.RLock()\n    defer self.containersLock.RUnlock()\n    containersMap := make(map[string]containerData, len(self.containers))\n@@ -591,6 +595,10 @@ func (self manager) getSubcontainers(containerName string) map[string]*containe\n    for i := range self.containers {\n        name := self.containers[i].info.Name\n        if name == containerName || strings.HasPrefix(name, matchedName) {\n+           if err := self.containers[i].updateStats(); err != nil {\n+               glog.Errorf(\"Error updating stats for container %s: %s\", name, err)\n+           }\n+\n            containersMap[self.containers[i].info.Name] = self.containers[i]\n        }\n    }\n```\nTL;DR: only detect new containers and refresh stats on scrape.\nThis is how it affected CPU usage on one of our machines where cadvisor used to take 50% of one core (this is an old Xeon E5-2630 v2 @ 2.60GHz):\n\nThat's a nice 5x-6x drop in CPU usage with no effect on what metrics we get.\nFlamegraph on a machine with these changes:\n\nFlamegraph on a similar machine without these changes:\n\n. > Since cAdvisor serves endpoints other than prometheus, and we don't want to make them all update metrics on each query, we can't make the change you are suggesting.\nI'm suggesting to add a mode for people who do not need other endpoints. Prometheus seems to be a de-facto system to manage metrics and it seems useful to support it better with much lower resource requirements.\n\nThe correct change to make for you is to increase your housekeeping interval, and also increase the window you are taking your rate over. Try making the window 4x the scrape interval, and making the scrape interval 2x the collection interval. The ratio of these, rather than the absolute durations is what matters for obtaining an accurate graph. Tweak them until it is close enough.\n\nThis is going to reduce precision of my metrics. I don't want that.. Prometheus does support passing arguments, see params:\n\nhttps://prometheus.io/docs/prometheus/latest/configuration/configuration/#%3Cscrape_config%3E\n\nYou can also have a completely different endpoint for ondemand metrics.. ",
    "ZhiqinYang": "Hi, I try this in ubuntu and centos, the container_fs_writes_total show that ubuntu is corrected and centos are not? why ?. Thank you for your reply, I do it with this command on centos7:\nsudo docker run  -d \\\n   --userns=hos\\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --privileged=true \\\n  --publish=1004:8080 \\\n--volume=/cgroup:/cgroup:ro \\\n  --name=cadvisor \\\n  google/cadvisor\nI found the result not correct.  I don't know how to do it? Any help please, thank!\ndocker info:\nContainers: 55\n Running: 50\n Paused: 0\n Stopped: 5\nImages: 31\nServer Version: 1.13.1\nStorage Driver: overlay\n Backing Filesystem: xfs\n Supports d_type: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: bridge host macvlan null overlay\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nInit Binary: docker-init\ncontainerd version: aa8187dbd3b7ad67d8e5e3a15115d3eef43a7ed1\nrunc version: 9df8b306d01f59d3a8029be411de015b7304dd8f\ninit version: 949e6fa\nSecurity Options:\n seccomp\n  Profile: default\nKernel Version: 3.10.0-514.el7.x86_64\nOperating System: CentOS Linux 7 (Core)\nOSType: linux\nArchitecture: x86_64\nCPUs: 4\nTotal Memory: 3.702 GiB\nName: node2\nID: 4N6Y:IWI4:TNPD:BBBT:YWYN:MHFD:PJTG:IWSW:XPYY:6CPH:MNOA:76NI\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): false\nRegistry: https://index.docker.io/v1/\nWARNING: bridge-nf-call-iptables is disabled\nWARNING: bridge-nf-call-ip6tables is disabled\nExperimental: false\nInsecure Registries:\n 10.233.0.0/18\n 127.0.0.0/8\nLive Restore Enabled: false\n. Thank you for help, I take a mistake, The  cgroup is set to error!. thank you!  hope hear from you soon! this is troubled for a long time!. ",
    "Danno040": "I'm not getting disk usage stats either, here's what /docker is reporting:\nDriver devicemapper\nPool Name docker-docker--pool\nBacking Filesystem ext4\nMetadata Space Total 25.17 MB\nBase Device Size 10.74 GB\nData file\nData Space Available 21.95 GB\nMetadata Space Available 22.76 MB\nUdev Sync Supported true\nDeferred Deletion Enabled true\nDeferred Deleted Device Count 0\nLibrary Version 1.02.135-RHEL7 (2016-11-16)\nPool Blocksize 524.3 kB\nData Space Used 1.409 GB\nData Space Total 23.35 GB\nThin Pool Minimum Free Space 2.335 GB\nMetadata file\nMetadata Space Used 2.404 MB\nDeferred Removal Enabled true\nAny suggestions?. ",
    "warroyo": "@Christopher-Bui is this data available in the /metrics endpoint?. ",
    "hpresnall": "See the command I ran - the containers were running gcr.io/google_containers/pause so there was almost no CPU utilization from the containers once they are running. I started the containers via Docker but was using the cadvisor install from kubernetes, which was also running on the VM. I am sure that running on a clean system with just Docker and cadvisor would produce the same results.\nOverall CPU on the system was low in general with spikes to 25% CPU (of 4 vCPUs total) when new containers were started and / or the metrics endpoint was curl'ed. I think memory usage was < 8 GB (of 32GB total) even with 750 containers running.. Based on that profile, the 2 hotspots are prometheus/common/expfmt.escapeString and opencontainers/runc/libcontainer/cgroups/fs.(*MemoryGroup).GetStats. I think the Prometheus one is to be expected given that it has to format all the output.\nThe runc one may be more important. Why does getting the memory stats take so much longer than the other stats?. ",
    "knepperjm": "Hi, @dashpole Thanks for the prompt feedback.  I can inspect the mounted docker socket inside my container, and all appears to be working as expected.  I do receive metrics in Grafana / influxdb, etc.  Below is my compose file for cAdvisor:\n1 version: '2'\n  2 services:\n  3   cadvisor:\n  4     image: \"google/cadvisor:latest\"\n  5     ports:\n  6      - \"20027:8080\"\n  7     labels:\n  8       io.rancher.scheduler.global: \"true\"\n  9       io.rancher.container.pull_image: always\n 10       io.rancher.container.start_once: 'true'\n 11       io.rancher.stack.name: \"google/cadvisor:latest\"\n 12       com.demandbase.environment: \"$ENVIRONMENT\"\n 13     volumes:\n 14       - \"/:/rootfs:ro\"\n 15       - \"/var/run:/var/run:rw\"\n 16       - \"/sys:/sys:ro\"\n 17       - \"/var/lib/docker/:/var/lib/docker:ro\"\n 18     environment:\n 19       - ENV=${ENV}\n 20       - INFLUX_USER=${INFLUX_USER}\n 21       - INFLUX_PASSWD=${INFLUX_PASSWD}\"\n 22     stdin_open: true\n 23     command:  [\"-storage_driver=influxdb\", \"-storage_driver_host=influxdb${ENV}.demandbase.com:8086\", \"-storage_driver_user=${INFLUX_USER}\", \"-storage_driver_password=${INFLUX_PASSWD}\"]\n. It's hard to say - any recommendations to detecting that in retrospect?. Ah, great advice! I will check it out after lunch.  I am very appreciative of your help, @dashpole . ",
    "tomqwu": "Interestingly if I remove --volume=/var/run:/var/run:rw, then container runs normally.. ",
    "sirlatrom": "Any chance of a follow-up where this would be a separate metric, instead of or in addition to being a metric label?. ",
    "jianzi123": "@dashpole  I've already finished the work, try to submit these days. @dashpole when add thread to cadvisor meta... whondering put it to  ContainerSpec or CpuSpec or just as func in cadvisor.... @dashpole em... I have forgotten this... and I have my  Need i push the code?. In the evening I will see #1891 and try to push it. . I'm sorry to misunderstand what you said. And i collect the total number of threads for each cgroup  instead of just the total on the machine...Because i collects threads nums from Threads in /proc/*/status in the container... And we put it to metrics, system loss is very small.... @dashpole e...There is a similar method in the old version, which was not implemented at that time.. i thought cadvisor has some... and k8s has one.... ",
    "seth-priya": "@dashpole thanks for you response. I was using the go test command. @dashpole  thanks for your response.\nSorry, my bad! There is a typo in the above description, what works on ppc64le is\ngo test -short go list ./... | grep -v vendor\nI will submit a PR for you review. Thanks!\n. ",
    "Leibniz137": "I signed it!. ",
    "tony612": "I started a k8s cluster in global(ap-northeast-1) using AMI k8s-1.4-debian-jessie-amd64-hvm-ebs-2017-01-31 (ami-dc4403bb) and print /proc/PID/cgroup every 1 minute then I found it changes at a time:\nWed May 24 06:33:48 UTC 2017\n10:pids:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n9:perf_event:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n8:net_cls,net_prio:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n7:freezer:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n6:devices:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n5:memory:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n4:blkio:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n3:cpu,cpuacct:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n2:cpuset:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n1:name=systemd:/system.slice/docker.service/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\nWed May 24 06:34:48 UTC 2017\n10:pids:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n9:perf_event:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n8:net_cls,net_prio:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n7:freezer:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n6:devices:/\n5:memory:/\n4:blkio:/\n3:cpu,cpuacct:/\n2:cpuset:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n1:name=systemd:/system.slice/docker.service/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\nBut the docker inspect result keeps same.\nThen I found some log about systemd at that time:\nMay 24 06:41:38 ip-172-20-38-126 kubelet[1307]: I0524 06:41:38.050820    1307 operation_executor.go:900] MountVolume.SetUp succeeded for volume \"kubernetes.io/secret/2d0a6901-3fb2-11e7-a32c-06fc4c71bfd5-default-token-vayh3\" (spec.Name: \"default-token-vayh3\") pod \"2d0a6901-3fb2-11e7-a32c-06fc4c71bfd5\" (UID: \"2d0a6901-\n3fb2-11e7-a32c-06fc4c71bfd5\").\nMay 24 06:41:45 ip-172-20-38-126 kubelet[1307]: I0524 06:41:45.968978    1307 operation_executor.go:900] MountVolume.SetUp succeeded for volume \"kubernetes.io/secret/60799fe1-3fb2-11e7-a32c-06fc4c71bfd5-default-token-vayh3\" (spec.Name: \"default-token-vayh3\") pod \"60799fe1-3fb2-11e7-a32c-06fc4c71bfd5\" (UID: \"60799fe1-\n3fb2-11e7-a32c-06fc4c71bfd5\").\nMay 24 06:41:55 ip-172-20-38-126 systemd[1]: Started Kubernetes Kubelet Server.\nMay 24 06:41:55 ip-172-20-38-126 docker[1269]: I0524 06:41:55.830502       1 kube_boot.go:112] Not in role master; won't scan for volumes\nMay 24 06:41:55 ip-172-20-38-126 docker[1269]: I0524 06:41:55.834118       1 kube_boot.go:142] ensuring that kubelet systemd service is running\nMay 24 06:42:55 ip-172-20-38-126 docker[1269]: I0524 06:42:55.837219       1 kube_boot.go:112] Not in role master; won't scan for volumes\nMay 24 06:42:55 ip-172-20-38-126 docker[1269]: I0524 06:42:55.837248       1 kube_boot.go:142] ensuring that kubelet systemd service is running\nMay 24 06:42:55 ip-172-20-38-126 systemd[1]: Started Kubernetes Kubelet Server.\nMay 24 06:42:56 ip-172-20-38-126 kubelet[1307]: I0524 06:42:56.031439    1307 operation_executor.go:900] MountVolume.SetUp succeeded for volume \"kubernetes.io/secret/60799fe1-3fb2-11e7-a32c-06fc4c71bfd5-default-token-vayh3\" (spec.Name: \"default-token-vayh3\") pod \"60799fe1-3fb2-11e7-a32c-06fc4c71bfd5\" (UID: \"60799fe1-3fb2-11e7-a32c-06fc4c71bfd5\").\nMay 24 06:43:07 ip-172-20-38-126 kubelet[1307]: I0524 06:43:07.055826    1307 operation_executor.go:900] MountVolume.SetUp succeeded for volume \"kubernetes.io/secret/2d0a6901-3fb2-11e7-a32c-06fc4c71bfd5-default-token-vayh3\" (spec.Name: \"default-token-vayh3\") pod \"2d0a6901-3fb2-11e7-a32c-06fc4c71bfd5\" (UID: \"2d0a6901-3fb2-11e7-a32c-06fc4c71bfd5\").\nMay 24 06:43:55 ip-172-20-38-126 docker[1269]: I0524 06:43:55.840313       1 kube_boot.go:112] Not in role master; won't scan for volumes\nMay 24 06:43:55 ip-172-20-38-126 docker[1269]: I0524 06:43:55.842704       1 kube_boot.go:142] ensuring that kubelet systemd service is running\nMay 24 06:43:55 ip-172-20-38-126 systemd[1]: Started Kubernetes Kubelet Server.\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: Reloading.\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: [/lib/systemd/system/docker.service:18] Unknown lvalue 'Delegate' in section 'Service'\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: Stopping RPC Port Mapper.\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: Stopped target RPC Port Mapper.\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: Stopping LSB: RPC portmapper replacement...\nMay 24 06:44:01 ip-172-20-38-126 rpcbind: rpcbind terminating on signal. Restart with \"rpcbind -w\"\nMay 24 06:44:01 ip-172-20-38-126 rpcbind[17877]: Stopping rpcbind daemon....\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: Stopped LSB: RPC portmapper replacement.\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: Reloading.\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: [/lib/systemd/system/docker.service:18] Unknown lvalue 'Delegate' in section 'Service'\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: Reloading.\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: [/lib/systemd/system/docker.service:18] Unknown lvalue 'Delegate' in section 'Service'\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: Reloading.\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: [/lib/systemd/system/docker.service:18] Unknown lvalue 'Delegate' in section 'Service'\nMay 24 06:44:03 ip-172-20-38-126 systemd[1]: Reloading.\nMay 24 06:44:03 ip-172-20-38-126 systemd[1]: [/lib/systemd/system/docker.service:18] Unknown lvalue 'Delegate' in section 'Service'\nMay 24 06:44:03 ip-172-20-38-126 kubelet[1307]: I0524 06:44:03.984904    1307 operation_executor.go:900] MountVolume.SetUp succeeded for volume \"kubernetes.io/secret/60799fe1-3fb2-11e7-a32c-06fc4c71bfd5-default-token-vayh3\" (spec.Name: \"default-token-vayh3\") pod \"60799fe1-3fb2-11e7-a32c-06fc4c71bfd5\" (UID: \"60799fe1-3fb2-11e7-a32c-06fc4c71bfd5\").\nMay 24 06:44:09 ip-172-20-38-126 systemd[1]: Reloading.\nMay 24 06:44:09 ip-172-20-38-126 systemd[1]: [/lib/systemd/system/docker.service:18] Unknown lvalue 'Delegate' in section 'Service'\nMay 24 06:44:09 ip-172-20-38-126 systemd[1]: Reloading.\nMay 24 06:44:09 ip-172-20-38-126 systemd[1]: [/lib/systemd/system/docker.service:18] Unknown lvalue 'Delegate' in section 'Service'\nMay 24 06:44:09 ip-172-20-38-126 systemd[1]: Starting LSB: RPC portmapper replacement...\nMay 24 06:44:09 ip-172-20-38-126 rpcbind[22714]: Starting rpcbind daemon....\nMay 24 06:44:09 ip-172-20-38-126 systemd[1]: Started LSB: RPC portmapper replacement.\nMay 24 06:44:09 ip-172-20-38-126 systemd[1]: Starting RPC Port Mapper.\nMay 24 06:44:09 ip-172-20-38-126 systemd[1]: Reached target RPC Port Mapper.\nMay 24 06:44:36 ip-172-20-38-126 kubelet[1307]: I0524 06:44:36.053457    1307 operation_executor.go:900] MountVolume.SetUp succeeded for volume \"kubernetes.io/secret/2d0a6901-3fb2-11e7-a32c-06fc4c71bfd5-default-token-vayh3\" (spec.Name: \"default-token-vayh3\") pod \"2d0a6901-3fb2-11e7-a32c-06fc4c71bfd5\" (UID: \"2d0a6901-3fb2-11e7-a32c-06fc4c71bfd5\").\nMay 24 06:44:52 ip-172-20-38-126 kubelet[1307]: W0524 06:44:52.097362    1307 container_manager_linux.go:662] CPUAccounting not enabled for pid: 1009\nMay 24 06:44:52 ip-172-20-38-126 kubelet[1307]: W0524 06:44:52.097389    1307 container_manager_linux.go:665] MemoryAccounting not enabled for pid: 1009\nMay 24 06:44:52 ip-172-20-38-126 kubelet[1307]: I0524 06:44:52.097394    1307 container_manager_linux.go:372] Discovered runtime cgroups name: /system.slice/docker.service\nMay 24 06:44:52 ip-172-20-38-126 kubelet[1307]: W0524 06:44:52.097440    1307 container_manager_linux.go:662] CPUAccounting not enabled for pid: 1307\nMay 24 06:44:52 ip-172-20-38-126 kubelet[1307]: W0524 06:44:52.097445    1307 container_manager_linux.go:665] MemoryAccounting not enabled for pid: 1307\nMay 24 06:44:55 ip-172-20-38-126 docker[1269]: I0524 06:44:55.847298       1 kube_boot.go:112] Not in role master; won't scan for volumes\nMay 24 06:44:55 ip-172-20-38-126 docker[1269]: I0524 06:44:55.847336       1 kube_boot.go:142] ensuring that kubelet systemd service is running\nMay 24 06:44:55 ip-172-20-38-126 systemd[1]: Started Kubernetes Kubelet Server.\nMay 24 06:45:01 ip-172-20-38-126 CRON[23523]: (root) CMD (command -v debian-sa1 > /dev/null && debian-sa1 1 1)\nAny idea?. I started a k8s cluster in global(ap-northeast-1) using AMI k8s-1.4-debian-jessie-amd64-hvm-ebs-2017-01-31 (ami-dc4403bb) and print /proc/PID/cgroup every 1 minute then I found it changes at a time:\nWed May 24 06:33:48 UTC 2017\n10:pids:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n9:perf_event:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n8:net_cls,net_prio:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n7:freezer:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n6:devices:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n5:memory:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n4:blkio:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n3:cpu,cpuacct:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n2:cpuset:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n1:name=systemd:/system.slice/docker.service/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\nWed May 24 06:34:48 UTC 2017\n10:pids:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n9:perf_event:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n8:net_cls,net_prio:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n7:freezer:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n6:devices:/\n5:memory:/\n4:blkio:/\n3:cpu,cpuacct:/\n2:cpuset:/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\n1:name=systemd:/system.slice/docker.service/docker/740db97b83b7e45c83964fe98f52254e66c00636b34087787c4874793cefcd2c\nBut the docker inspect result keeps same.\nThen I found some log about systemd at that time:\nMay 24 06:41:38 ip-172-20-38-126 kubelet[1307]: I0524 06:41:38.050820    1307 operation_executor.go:900] MountVolume.SetUp succeeded for volume \"kubernetes.io/secret/2d0a6901-3fb2-11e7-a32c-06fc4c71bfd5-default-token-vayh3\" (spec.Name: \"default-token-vayh3\") pod \"2d0a6901-3fb2-11e7-a32c-06fc4c71bfd5\" (UID: \"2d0a6901-\n3fb2-11e7-a32c-06fc4c71bfd5\").\nMay 24 06:41:45 ip-172-20-38-126 kubelet[1307]: I0524 06:41:45.968978    1307 operation_executor.go:900] MountVolume.SetUp succeeded for volume \"kubernetes.io/secret/60799fe1-3fb2-11e7-a32c-06fc4c71bfd5-default-token-vayh3\" (spec.Name: \"default-token-vayh3\") pod \"60799fe1-3fb2-11e7-a32c-06fc4c71bfd5\" (UID: \"60799fe1-\n3fb2-11e7-a32c-06fc4c71bfd5\").\nMay 24 06:41:55 ip-172-20-38-126 systemd[1]: Started Kubernetes Kubelet Server.\nMay 24 06:41:55 ip-172-20-38-126 docker[1269]: I0524 06:41:55.830502       1 kube_boot.go:112] Not in role master; won't scan for volumes\nMay 24 06:41:55 ip-172-20-38-126 docker[1269]: I0524 06:41:55.834118       1 kube_boot.go:142] ensuring that kubelet systemd service is running\nMay 24 06:42:55 ip-172-20-38-126 docker[1269]: I0524 06:42:55.837219       1 kube_boot.go:112] Not in role master; won't scan for volumes\nMay 24 06:42:55 ip-172-20-38-126 docker[1269]: I0524 06:42:55.837248       1 kube_boot.go:142] ensuring that kubelet systemd service is running\nMay 24 06:42:55 ip-172-20-38-126 systemd[1]: Started Kubernetes Kubelet Server.\nMay 24 06:42:56 ip-172-20-38-126 kubelet[1307]: I0524 06:42:56.031439    1307 operation_executor.go:900] MountVolume.SetUp succeeded for volume \"kubernetes.io/secret/60799fe1-3fb2-11e7-a32c-06fc4c71bfd5-default-token-vayh3\" (spec.Name: \"default-token-vayh3\") pod \"60799fe1-3fb2-11e7-a32c-06fc4c71bfd5\" (UID: \"60799fe1-3fb2-11e7-a32c-06fc4c71bfd5\").\nMay 24 06:43:07 ip-172-20-38-126 kubelet[1307]: I0524 06:43:07.055826    1307 operation_executor.go:900] MountVolume.SetUp succeeded for volume \"kubernetes.io/secret/2d0a6901-3fb2-11e7-a32c-06fc4c71bfd5-default-token-vayh3\" (spec.Name: \"default-token-vayh3\") pod \"2d0a6901-3fb2-11e7-a32c-06fc4c71bfd5\" (UID: \"2d0a6901-3fb2-11e7-a32c-06fc4c71bfd5\").\nMay 24 06:43:55 ip-172-20-38-126 docker[1269]: I0524 06:43:55.840313       1 kube_boot.go:112] Not in role master; won't scan for volumes\nMay 24 06:43:55 ip-172-20-38-126 docker[1269]: I0524 06:43:55.842704       1 kube_boot.go:142] ensuring that kubelet systemd service is running\nMay 24 06:43:55 ip-172-20-38-126 systemd[1]: Started Kubernetes Kubelet Server.\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: Reloading.\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: [/lib/systemd/system/docker.service:18] Unknown lvalue 'Delegate' in section 'Service'\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: Stopping RPC Port Mapper.\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: Stopped target RPC Port Mapper.\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: Stopping LSB: RPC portmapper replacement...\nMay 24 06:44:01 ip-172-20-38-126 rpcbind: rpcbind terminating on signal. Restart with \"rpcbind -w\"\nMay 24 06:44:01 ip-172-20-38-126 rpcbind[17877]: Stopping rpcbind daemon....\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: Stopped LSB: RPC portmapper replacement.\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: Reloading.\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: [/lib/systemd/system/docker.service:18] Unknown lvalue 'Delegate' in section 'Service'\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: Reloading.\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: [/lib/systemd/system/docker.service:18] Unknown lvalue 'Delegate' in section 'Service'\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: Reloading.\nMay 24 06:44:01 ip-172-20-38-126 systemd[1]: [/lib/systemd/system/docker.service:18] Unknown lvalue 'Delegate' in section 'Service'\nMay 24 06:44:03 ip-172-20-38-126 systemd[1]: Reloading.\nMay 24 06:44:03 ip-172-20-38-126 systemd[1]: [/lib/systemd/system/docker.service:18] Unknown lvalue 'Delegate' in section 'Service'\nMay 24 06:44:03 ip-172-20-38-126 kubelet[1307]: I0524 06:44:03.984904    1307 operation_executor.go:900] MountVolume.SetUp succeeded for volume \"kubernetes.io/secret/60799fe1-3fb2-11e7-a32c-06fc4c71bfd5-default-token-vayh3\" (spec.Name: \"default-token-vayh3\") pod \"60799fe1-3fb2-11e7-a32c-06fc4c71bfd5\" (UID: \"60799fe1-3fb2-11e7-a32c-06fc4c71bfd5\").\nMay 24 06:44:09 ip-172-20-38-126 systemd[1]: Reloading.\nMay 24 06:44:09 ip-172-20-38-126 systemd[1]: [/lib/systemd/system/docker.service:18] Unknown lvalue 'Delegate' in section 'Service'\nMay 24 06:44:09 ip-172-20-38-126 systemd[1]: Reloading.\nMay 24 06:44:09 ip-172-20-38-126 systemd[1]: [/lib/systemd/system/docker.service:18] Unknown lvalue 'Delegate' in section 'Service'\nMay 24 06:44:09 ip-172-20-38-126 systemd[1]: Starting LSB: RPC portmapper replacement...\nMay 24 06:44:09 ip-172-20-38-126 rpcbind[22714]: Starting rpcbind daemon....\nMay 24 06:44:09 ip-172-20-38-126 systemd[1]: Started LSB: RPC portmapper replacement.\nMay 24 06:44:09 ip-172-20-38-126 systemd[1]: Starting RPC Port Mapper.\nMay 24 06:44:09 ip-172-20-38-126 systemd[1]: Reached target RPC Port Mapper.\nMay 24 06:44:36 ip-172-20-38-126 kubelet[1307]: I0524 06:44:36.053457    1307 operation_executor.go:900] MountVolume.SetUp succeeded for volume \"kubernetes.io/secret/2d0a6901-3fb2-11e7-a32c-06fc4c71bfd5-default-token-vayh3\" (spec.Name: \"default-token-vayh3\") pod \"2d0a6901-3fb2-11e7-a32c-06fc4c71bfd5\" (UID: \"2d0a6901-3fb2-11e7-a32c-06fc4c71bfd5\").\nMay 24 06:44:52 ip-172-20-38-126 kubelet[1307]: W0524 06:44:52.097362    1307 container_manager_linux.go:662] CPUAccounting not enabled for pid: 1009\nMay 24 06:44:52 ip-172-20-38-126 kubelet[1307]: W0524 06:44:52.097389    1307 container_manager_linux.go:665] MemoryAccounting not enabled for pid: 1009\nMay 24 06:44:52 ip-172-20-38-126 kubelet[1307]: I0524 06:44:52.097394    1307 container_manager_linux.go:372] Discovered runtime cgroups name: /system.slice/docker.service\nMay 24 06:44:52 ip-172-20-38-126 kubelet[1307]: W0524 06:44:52.097440    1307 container_manager_linux.go:662] CPUAccounting not enabled for pid: 1307\nMay 24 06:44:52 ip-172-20-38-126 kubelet[1307]: W0524 06:44:52.097445    1307 container_manager_linux.go:665] MemoryAccounting not enabled for pid: 1307\nMay 24 06:44:55 ip-172-20-38-126 docker[1269]: I0524 06:44:55.847298       1 kube_boot.go:112] Not in role master; won't scan for volumes\nMay 24 06:44:55 ip-172-20-38-126 docker[1269]: I0524 06:44:55.847336       1 kube_boot.go:142] ensuring that kubelet systemd service is running\nMay 24 06:44:55 ip-172-20-38-126 systemd[1]: Started Kubernetes Kubelet Server.\nMay 24 06:45:01 ip-172-20-38-126 CRON[23523]: (root) CMD (command -v debian-sa1 > /dev/null && debian-sa1 1 1)\nAny idea?. It's because systemd and config of kops https://github.com/kubernetes/kops/issues/2660. It's because systemd and config of kops https://github.com/kubernetes/kops/issues/2660. ",
    "msinakitajr": "Yes, I can see the container_fs_inodes_xxx as well as the container_fs_usage_xxx metrics.\nIs there anyway to exclude the disk device namespace ... other than modifying the source code?. thanks dashpole!. ",
    "ronenn1": "+1. We're observing the same behavior with version 0.27.0 and Docker 17.06.1.\nMetrics always contain cAdvisor, alertmanager and Prometheus, but every couple of minutes, our applications' containers metrics are missing.\nCould you please update if (and when) a fix would be available?\n@mfournier workaround URL is broken.\nThanks.. ",
    "mnuic": "Hi @Christopher-Bui,\nWe are using consul image in our production as a service discovery tool, alongside cadvisor and statad images. The problem is that we manually have to define statsd ip address for every cadvisor container on every host in our environment and we have a lot of hosts.\nIf cadvisor image would have a consul-template running when starting the container we wouldn't have to define statsd ip manually. It would automaticaly pick up (discover) the closest statsd service from consul and get the ip address. The example for it is above.\n. We have tried that, and it works when you have few (static) hosts... but with a production system with 100+ hosts when hosts are automatically scaled up or down (start and destroyed) it is not very convenient to edit all the hosts manually. That's why I am asking if cadvisor could have consul-template inside container, so that starting a container on a host could automatic discover a service from consul, in my case statsd service.\nThanks. I'll try this way, thanks @colinrymer . ",
    "colinrymer": "You can use consul-template or envconsul outside of the cadvisor container, e.g. having consul-template running on the host manage the cadvisor lifecycle and providing a file for the --env-file flag.. I'm not sure you're following what I'm saying. Instead of putting consul-template in the container, invert the dependency so that consul-template sits outside the container and manages it. Nothing in that scenario would require manually editing any hosts.. ",
    "tomcsi": "Solution?. ",
    "flavio": "@googlebot  I signed it!. Yes I did: all the error messages disappeared and everything seems to be working fine.. Rebase done.. @dashpole the change looks good to me. I worked with @MaximilianMeister on that ;). ",
    "rickardrosen": "I'm also seeing this. . ",
    "hajhatten": "Did get womething similar as @pangzheng at work, where we use the --device flag in docker, to get /dev/zfs metrics from cadvisor.\nTried google/cadvisor-canary out on my freebsd machine at home, and it works perfectly for 40s, then prints out the following:\n2 error(s) occurred:\n* collected metric container_fs_reads_total label:<name:\"device\" value:\"/dev/loop0\" > label:<name:\"id\" value:\"/\" > counter:<value:0 >  was collected before with the same name and label values\n* collected metric container_fs_writes_total label:<name:\"device\" value:\"/dev/loop0\" > label:<name:\"id\" value:\"/\" > counter:<value:0 >  was collected before with the same name and label values\nCannot recreate this in google/cadvisor-canary or with a custom build with the release-v0.26 branch on my docker toolbox machine. I'm not running the --device flag there.. ",
    "DevWojtekC": "I have similar issue. I've launched a whole stack using example from https://github.com/stefanprodan/dockprom.\nSetup: Ubuntu 16.04 via Vagrant / VirtualBox on Windows box.\ncompose file contains such specification of cadvisor service:\ncadvisor:\n    image: google/cadvisor:v0.24.1\n    container_name: cadvisor\n    command:\n      - '-v=4'\n    volumes:\n      - /:/rootfs:ro\n      - /var/run:/var/run:rw\n      - /sys:/sys:ro\n      - /var/lib/docker/:/var/lib/docker:ro\n      - /var/log:/var/log:ro\n    restart: unless-stopped\n    expose:\n      - 8080\n    ports:\n      - 9010:8080\n    networks:\n      - monitor-net\n    labels:\n      org.label-schema.group: \"monitoring\"\nNote: I've added  volume mapping to get OOM reporting - /var/log:/var/log:ro. But container_tasks_state was empty (always zero) both before and after adding this mapping.\nAfter enabling verbose logging I am seeing this in cadvisor logs:\nI0619 11:27:45.622192       1 factory.go:115] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-aufs-mnt-f25a63e9dcd7ea8dce4158c8c16e05f3e0f124d9da48c9d6b27ca7e5d3065d5f.mount\"\nI0619 11:27:45.622208       1 factory.go:108] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-aufs-mnt-f25a63e9dcd7ea8dce4158c8c16e05f3e0f124d9da48c9d6b27ca7e5d3065d5f.mount\", but ignoring.\nI0619 11:27:45.622220       1 manager.go:843] ignoring container \"/system.slice/var-lib-docker-aufs-mnt-f25a63e9dcd7ea8dce4158c8c16e05f3e0f124d9da48c9d6b27ca7e5d3065d5f.mount\"\nI0619 11:27:45.622235       1 factory.go:104] Error trying to work out if we can handle /system.slice/dev-hugepages.mount: invalid container name\nI0619 11:27:45.622246       1 factory.go:115] Factory \"docker\" was unable to handle container \"/system.slice/dev-hugepages.mount\"\nI0619 11:27:45.622258       1 factory.go:108] Factory \"systemd\" can handle container \"/system.slice/dev-hugepages.mount\", but ignoring.\nI0619 11:27:45.622268       1 manager.go:843] ignoring container \"/system.slice/dev-hugepages.mount\"\nI0619 11:27:45.622284       1 factory.go:104] Error trying to work out if we can handle /system.slice/run-docker-netns-1\\x2d5ncqrmhhap.mount: invalid container name\nI0619 11:27:45.622293       1 factory.go:115] Factory \"docker\" was unable to handle container \"/system.slice/run-docker-netns-1\\\\x2d5ncqrmhhap.mount\"\nI0619 11:27:45.622454       1 factory.go:108] Factory \"systemd\" can handle container \"/system.slice/run-docker-netns-1\\\\x2d5ncqrmhhap.mount\", but ignoring.\nI0619 11:27:45.622475       1 manager.go:843] ignoring container \"/system.slice/run-docker-netns-1\\\\x2d5ncqrmhhap.mount\"\nI0619 11:27:45.622490       1 factory.go:104] Error trying to work out if we can handle /system.slice/dev-mqueue.mount: invalid container name\nI0619 11:27:45.622500       1 factory.go:115] Factory \"docker\" was unable to handle container \"/system.slice/dev-mqueue.mount\"\nI0619 11:27:45.622512       1 factory.go:108] Factory \"systemd\" can handle container \"/system.slice/dev-mqueue.mount\", but ignoring.\nI0619 11:27:45.622523       1 manager.go:843] ignoring container \"/system.slice/dev-mqueue.mount\"\nI0619 11:27:45.626292       1 factory.go:104] Error trying to work out if we can handle /system.slice/var-lib-docker-aufs-mnt-458d9912e6ab495e7d2ef56dae3414ddba595d6e7edc25c77cd6361c46ed57d5.mount: error inspecting container: Error: No such container: 458d9912e6ab495e7d2ef56dae3414ddba595d6e7edc25c77cd6361c46ed57d5\nI0619 11:27:45.626330       1 factory.go:115] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-aufs-mnt-458d9912e6ab495e7d2ef56dae3414ddba595d6e7edc25c77cd6361c46ed57d5.mount\"\nI0619 11:27:45.626342       1 factory.go:108] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-aufs-mnt-458d9912e6ab495e7d2ef56dae3414ddba595d6e7edc25c77cd6361c46ed57d5.mount\", but ignoring.\nI0619 11:27:45.626349       1 manager.go:843] ignoring container \"/system.slice/var-lib-docker-aufs-mnt-458d9912e6ab495e7d2ef56dae3414ddba595d6e7edc25c77cd6361c46ed57d5.mount\"\nI0619 11:27:45.626358       1 factory.go:104] Error trying to work out if we can handle /system.slice/var-lib-lxcfs.mount: invalid container name\nI0619 11:27:45.626364       1 factory.go:115] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-lxcfs.mount\"\nI0619 11:27:45.626368       1 factory.go:108] Factory \"systemd\" can handle container \"/system.slice/var-lib-lxcfs.mount\", but ignoring.\nI0619 11:27:45.626371       1 manager.go:843] ignoring container \"/system.slice/var-lib-lxcfs.mount\"\nI0619 11:27:45.626379       1 factory.go:104] Error trying to work out if we can handle /system.slice/run-docker-netns-cf6bf9ac3743.mount: invalid container name\nI0619 11:27:45.626384       1 factory.go:115] Factory \"docker\" was unable to handle container \"/system.slice/run-docker-netns-cf6bf9ac3743.mount\"\nI0619 11:27:45.626388       1 factory.go:108] Factory \"systemd\" can handle container \"/system.slice/run-docker-netns-cf6bf9ac3743.mount\", but ignoring.\nI0619 11:27:45.626392       1 manager.go:843] ignoring container \"/system.slice/run-docker-netns-cf6bf9ac3743.mount\"\nI0619 11:27:45.626421       1 factory.go:104] Error trying to work out if we can handle /system.slice/run-docker-netns-ingress_sbox.mount: invalid container name\nI0619 11:27:45.626428       1 factory.go:115] Factory \"docker\" was unable to handle container \"/system.slice/run-docker-netns-ingress_sbox.mount\"\nI0619 11:27:45.626432       1 factory.go:108] Factory \"systemd\" can handle container \"/system.slice/run-docker-netns-ingress_sbox.mount\", but ignoring.\nI0619 11:27:45.626436       1 manager.go:843] ignoring container \"/system.slice/run-docker-netns-ingress_sbox.mount\"\nI0619 11:27:45.626442       1 factory.go:104] Error trying to work out if we can handle /system.slice/run-user-1000.mount: invalid container name\nI0619 11:27:45.626447       1 factory.go:115] Factory \"docker\" was unable to handle container \"/system.slice/run-user-1000.mount\"\nI0619 11:27:45.626451       1 factory.go:108] Factory \"systemd\" can handle container \"/system.slice/run-user-1000.mount\", but ignoring.\nI0619 11:27:45.626454       1 manager.go:843] ignoring container \"/system.slice/run-user-1000.mount\"\ncontainer_tasks_state metrics is zero for everything in cadvisor:\ncontainer_tasks_state{id=\"/\",state=\"iowaiting\"} 0\ncontainer_tasks_state{id=\"/\",state=\"running\"} 0\ncontainer_tasks_state{id=\"/\",state=\"sleeping\"} 0\ncontainer_tasks_state{id=\"/\",state=\"stopped\"} 0\ncontainer_tasks_state{id=\"/\",state=\"uninterruptible\"} 0\ncontainer_tasks_state{id=\"/docker\",state=\"iowaiting\"} 0\ncontainer_tasks_state{id=\"/docker\",state=\"running\"} 0\ncontainer_tasks_state{id=\"/docker\",state=\"sleeping\"} 0\ncontainer_tasks_state{id=\"/docker\",state=\"stopped\"} 0\ncontainer_tasks_state{id=\"/docker\",state=\"uninterruptible\"} 0\n...\ncontainer_tasks_state{id=\"/system.slice/acpid.service\",state=\"uninterruptible\"} 0\ncontainer_tasks_state{id=\"/system.slice/apparmor.service\",state=\"iowaiting\"} 0\ncontainer_tasks_state{id=\"/system.slice/apparmor.service\",state=\"running\"} 0\ncontainer_tasks_state{id=\"/system.slice/apparmor.service\",state=\"sleeping\"} 0\ncontainer_tasks_state{id=\"/system.slice/apparmor.service\",state=\"stopped\"} 0\ncontainer_tasks_state{id=\"/system.slice/apparmor.service\",state=\"uninterruptible\"} 0\ncontainer_tasks_state{id=\"/system.slice/apport.service\",state=\"iowaiting\"} 0\ncontainer_tasks_state{id=\"/system.slice/apport.service\",state=\"running\"} 0\ncontainer_tasks_state{id=\"/system.slice/apport.service\",state=\"sleeping\"} 0\ncontainer_tasks_state{id=\"/system.slice/apport.service\",state=\"stopped\"} 0\ncontainer_tasks_state{id=\"/system.slice/apport.service\",state=\"uninterruptible\"} 0\ncontainer_tasks_state{id=\"/system.slice/atd.service\",state=\"iowaiting\"} 0\ncontainer_tasks_state{id=\"/system.slice/atd.service\",state=\"running\"} 0\ncontainer_tasks_state{id=\"/system.slice/atd.service\",state=\"sleeping\"} 0\ncontainer_tasks_state{id=\"/system.slice/atd.service\",state=\"stopped\"} 0\n...\ncontainer_tasks_state{container_label_com_docker_compose_config_hash=\"460a694821ac8a3b069369ad933926aa407b2dc437cf6ba0dadf2e6d89efdae4\",container_label_com_docker_compose_container_number=\"1\",container_label_com_docker_compose_oneoff=\"False\",container_label_com_docker_compose_project=\"dockprom\",container_label_com_docker_compose_service=\"cadvisor\",container_label_com_docker_compose_version=\"1.13.0\",container_label_org_label_schema_group=\"monitoring\",id=\"/docker/ce5a159a015df93f2d0a3b534cdcab64cd9ebcdc5c21b31f7955da81785ca91d\",image=\"google/cadvisor:v0.24.1\",name=\"cadvisor\",state=\"iowaiting\"} 0\ncontainer_tasks_state{container_label_com_docker_compose_config_hash=\"460a694821ac8a3b069369ad933926aa407b2dc437cf6ba0dadf2e6d89efdae4\",container_label_com_docker_compose_container_number=\"1\",container_label_com_docker_compose_oneoff=\"False\",container_label_com_docker_compose_project=\"dockprom\",container_label_com_docker_compose_service=\"cadvisor\",container_label_com_docker_compose_version=\"1.13.0\",container_label_org_label_schema_group=\"monitoring\",id=\"/docker/ce5a159a015df93f2d0a3b534cdcab64cd9ebcdc5c21b31f7955da81785ca91d\",image=\"google/cadvisor:v0.24.1\",name=\"cadvisor\",state=\"running\"} 0\ncontainer_tasks_state{container_label_com_docker_compose_config_hash=\"460a694821ac8a3b069369ad933926aa407b2dc437cf6ba0dadf2e6d89efdae4\",container_label_com_docker_compose_container_number=\"1\",container_label_com_docker_compose_oneoff=\"False\",container_label_com_docker_compose_project=\"dockprom\",container_label_com_docker_compose_service=\"cadvisor\",container_label_com_docker_compose_version=\"1.13.0\",container_label_org_label_schema_group=\"monitoring\",id=\"/docker/ce5a159a015df93f2d0a3b534cdcab64cd9ebcdc5c21b31f7955da81785ca91d\",image=\"google/cadvisor:v0.24.1\",name=\"cadvisor\",state=\"sleeping\"} 0. ",
    "nshttpd": "Similar with cAdvisor running in Docker on a Debian GCE instance. All the container_task_states are reporting zero.\n```\ncAdvisor version: v0.25.0\nOS version: Alpine Linux v3.4\nKernel version: [Supported and recommended]\n    Kernel version is 3.16.0-4-amd64. Versions >= 2.6 are supported. 3.0+ are recommended.\nCgroup setup: [Supported, but not recommended]\n    Cgroup memory not enabled. Available cgroups: map[cpuacct:1 memory:0 devices:1 freezer:1 blkio:1 net_prio:1 cpu:1 net_cls:1 perf_event:1 cpuset:1]\n    Following cgroups are required: [cpu cpuacct]\n    Following other cgroups are recommended: [memory blkio cpuset devices freezer]\nCgroup mount setup: [Supported and recommended]\n    Cgroups are mounted at /sys/fs/cgroup.\n    Cgroup mount directories: blkio cpu cpu,cpuacct cpuacct cpuset devices freezer net_cls net_cls,net_prio net_prio perf_event systemd\n    Any cgroup mount point that is detectible and accessible is supported. /sys/fs/cgroup is recommended as a standard location.\n    Cgroup mounts:\n    cgroup /sys/fs/cgroup/systemd cgroup ro,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /sys/fs/cgroup/cpuset cgroup ro,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /sys/fs/cgroup/cpu,cpuacct cgroup ro,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /sys/fs/cgroup/devices cgroup ro,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /sys/fs/cgroup/freezer cgroup ro,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /sys/fs/cgroup/net_cls,net_prio cgroup ro,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /sys/fs/cgroup/blkio cgroup ro,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /sys/fs/cgroup/perf_event cgroup ro,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /rootfs/sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /rootfs/sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /rootfs/sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /rootfs/sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /rootfs/sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/systemd cgroup ro,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/cpuset cgroup ro,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/cpu,cpuacct cgroup ro,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/devices cgroup ro,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/freezer cgroup ro,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/net_cls,net_prio cgroup ro,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/blkio cgroup ro,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/perf_event cgroup ro,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/systemd cgroup ro,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/cpuset cgroup ro,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/cpu,cpuacct cgroup ro,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/devices cgroup ro,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/freezer cgroup ro,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/net_cls,net_prio cgroup ro,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/blkio cgroup ro,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/perf_event cgroup ro,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/systemd cgroup ro,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/cpuset cgroup ro,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/cpu,cpuacct cgroup ro,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/devices cgroup ro,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/freezer cgroup ro,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/net_cls,net_prio cgroup ro,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/blkio cgroup ro,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/perf_event cgroup ro,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\nDocker version: [Supported and recommended]\n    Docker version is 17.03.1-ce. Versions >= 1.0 are supported. 1.2+ are recommended.\nDocker driver setup: [Supported and recommended]\n    Docker exec driver is . Storage driver is aufs.\nBlock device setup: [Supported, but not recommended]\n    None of the devices support 'cfq' I/O scheduler. No disk stats can be reported.\n     Disk \"sdb\" Scheduler type \"noop\".\n     Disk \"sda\" Scheduler type \"noop\".\nInotify watches:\n``. Similar with cAdvisor running in Docker on a Debian GCE instance. All thecontainer_task_states` are reporting zero.\n```\ncAdvisor version: v0.25.0\nOS version: Alpine Linux v3.4\nKernel version: [Supported and recommended]\n    Kernel version is 3.16.0-4-amd64. Versions >= 2.6 are supported. 3.0+ are recommended.\nCgroup setup: [Supported, but not recommended]\n    Cgroup memory not enabled. Available cgroups: map[cpuacct:1 memory:0 devices:1 freezer:1 blkio:1 net_prio:1 cpu:1 net_cls:1 perf_event:1 cpuset:1]\n    Following cgroups are required: [cpu cpuacct]\n    Following other cgroups are recommended: [memory blkio cpuset devices freezer]\nCgroup mount setup: [Supported and recommended]\n    Cgroups are mounted at /sys/fs/cgroup.\n    Cgroup mount directories: blkio cpu cpu,cpuacct cpuacct cpuset devices freezer net_cls net_cls,net_prio net_prio perf_event systemd\n    Any cgroup mount point that is detectible and accessible is supported. /sys/fs/cgroup is recommended as a standard location.\n    Cgroup mounts:\n    cgroup /sys/fs/cgroup/systemd cgroup ro,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /sys/fs/cgroup/cpuset cgroup ro,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /sys/fs/cgroup/cpu,cpuacct cgroup ro,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /sys/fs/cgroup/devices cgroup ro,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /sys/fs/cgroup/freezer cgroup ro,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /sys/fs/cgroup/net_cls,net_prio cgroup ro,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /sys/fs/cgroup/blkio cgroup ro,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /sys/fs/cgroup/perf_event cgroup ro,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /rootfs/sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /rootfs/sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /rootfs/sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /rootfs/sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /rootfs/sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /rootfs/sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/systemd cgroup ro,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/cpuset cgroup ro,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/cpu,cpuacct cgroup ro,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/devices cgroup ro,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/freezer cgroup ro,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/net_cls,net_prio cgroup ro,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/blkio cgroup ro,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/perf_event cgroup ro,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/systemd cgroup ro,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/cpuset cgroup ro,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/cpu,cpuacct cgroup ro,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/devices cgroup ro,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/freezer cgroup ro,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/net_cls,net_prio cgroup ro,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/blkio cgroup ro,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/perf_event cgroup ro,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/systemd cgroup ro,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/cpuset cgroup ro,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/cpu,cpuacct cgroup ro,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/devices cgroup ro,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/freezer cgroup ro,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/net_cls,net_prio cgroup ro,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/blkio cgroup ro,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/rootfs/var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/perf_event cgroup ro,nosuid,nodev,noexec,relatime,perf_event 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/lib/systemd/systemd-cgroups-agent,name=systemd 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0\n    cgroup /var/lib/docker/aufs/mnt/a1c0713e359b223b5cd05e33d53818017568d17e00e28932b65fd8f7aa180c3b/sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0\nDocker version: [Supported and recommended]\n    Docker version is 17.03.1-ce. Versions >= 1.0 are supported. 1.2+ are recommended.\nDocker driver setup: [Supported and recommended]\n    Docker exec driver is . Storage driver is aufs.\nBlock device setup: [Supported, but not recommended]\n    None of the devices support 'cfq' I/O scheduler. No disk stats can be reported.\n     Disk \"sdb\" Scheduler type \"noop\".\n     Disk \"sda\" Scheduler type \"noop\".\nInotify watches:\n``. Just updated toStarting cAdvisor version: v0.26.1-d19cc94 on port 8080.. and same thing.. Just updated toStarting cAdvisor version: v0.26.1-d19cc94 on port 8080` .. and same thing.. ",
    "caiohaz": "+1. ",
    "ghost": "Have you tried enabling load reader -enable_load_reader?. ",
    "igorkatz": "Hi,\nI have similar issue. container_tasks_state metrics which are exposed on the /metrics endpoint always returns zero. \nMy configuration is:\ncadvisor_version_info{cadvisorRevision=\"aaaa65d\",cadvisorVersion=\"v0.29.0\",dockerVersion=\"17.12.0-ce\",kernelVersion=\"4.15.0-1.el7.elrepo.x86_64\",osVersion=\"Alpine Linux v3.4\"} \nMy docker compose file contains such specification of cadvisor service:\ncadvisor:\n    deploy:\n      mode: global\n      resources:\n        limits:\n          memory: 128M\n        reservations:\n          memory: 64M\n    image: google/cadvisor:v0.29.0\n    hostname: '{{.Node.Hostname}}'\n    networks:\n      - default\n    ports:\n      - 9098:8080\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n      - /:/rootfs:ro\n      - /var/run:/var/run\n      - /sys:/sys:ro\n      - /var/lib/docker/:/var/lib/docker:ro\n    command:\n      - '-docker_only'\n      - '-housekeeping_interval=5s'\n      - `'-disable_metrics=disk,tcp,udp'\nThere are no errors in the log.. I want to verify that a container is down without explicitly specifying a name of container. I thought that if \ntime() - container_last_seen  > scrape_interval than the container is not running. --storage_duration flag has to define how long to store the latest historical data in memory, but metrics of a stopped container are immediately removed and I can not use my formula. Any suggestions?. ",
    "lzpsqzr": "I have a similar issue too.\nI added the -enable_load_reader=true, but it's not working.\nhere is my compose file:\n```\n cadvisor:\n    image: google/cadvisor:latest\n    stdin_open: true\n    volumes:\n    - /:/rootfs:ro\n    - /var/run:/var/run:rw\n    - /sys:/sys:ro\n    - /var/lib/docker/:/var/lib/docker:ro\n    - /dev/disk/:/dev/disk/:ro\n    tty: true\n    ports:\n    - 1080:8080/tcp\n    command:\n    - -enable_load_reader=true\n    labels:\n      io.rancher.container.pull_image: always\n      io.rancher.host.agent: cadvisor-pr\n```. ",
    "JBS5": "Have the same issue at the moment, also after adding the row below to my cAdvisor docker compose:\n--enable_load_reader=true. ",
    "BirkhoffLee": "Same here, adding--enable_load_reader=true will not work. This is my docker-compose:\nyaml\ncadvisor:\n  image: google/cadvisor\n  command: \"--enable_load_reader=true\"\n  privileged: true\n  volumes:\n    - /:/rootfs:ro\n    - /var/run:/var/run:rw\n    - /sys:/sys:ro\n    - /var/lib/docker/:/var/lib/docker:ro\n    - /cgroup:/cgroup:ro\n  network_mode: \"host\"\n  restart: always. ",
    "siwyd": "Same behaviour here, would be nice if someone could enlighten us on what might be wrong?. ",
    "fabtrompet": "same problem here, can anyone solve it?. ",
    "mapshen": "@dashpole still encounter this issue running the latest v0.32.0. Adding --enable_load_reader=true doesn't seem to help. Would you be able to provide some guidance for us? Let me know if you need any help to reproduce this or there is something I can do to assist.. Thanks for prompt response!\nDid a quick test and it looks like to be working. This is the compose file I use to deploy in swarm mode (thanks to moby/issues/25873):\n```\nversion: \"3.7\"\nnetworks:\n  host_network:\n    external: true\n    name: host\nservices:\n  cadvisor:\n    image: google/cadvisor:v0.32.0\n      command: \"--enable_load_reader=true\"\n    networks:\n      - host_network\n    deploy:\n      mode: global\n```\nLet me do more tests and will confirm back. . ",
    "fredsig": "Also seeing this on ECS instances running latest AMI:\nI0725 15:13:55.075525   19818 manager.go:201] Version: {KernelVersion:4.9.27-14.33.amzn1.x86_64 ContainerOsVersion:Amazon Linux AMI 2017.03 DockerVersion:17.03.1-ce CadvisorVersion:v0.24.1 CadvisorRevision:ae6934c}\nI have 2 cores, but seeing 15 instead:\ncontainer_cpu_usage_seconds_total{cpu=\"cpu00\",id=\"/\"} 135670.315078045\ncontainer_cpu_usage_seconds_total{cpu=\"cpu01\",id=\"/\"} 132425.496029752\ncontainer_cpu_usage_seconds_total{cpu=\"cpu02\",id=\"/\"} 0\ncontainer_cpu_usage_seconds_total{cpu=\"cpu03\",id=\"/\"} 0\ncontainer_cpu_usage_seconds_total{cpu=\"cpu04\",id=\"/\"} 0\ncontainer_cpu_usage_seconds_total{cpu=\"cpu05\",id=\"/\"} 0\ncontainer_cpu_usage_seconds_total{cpu=\"cpu06\",id=\"/\"} 0\ncontainer_cpu_usage_seconds_total{cpu=\"cpu07\",id=\"/\"} 0\ncontainer_cpu_usage_seconds_total{cpu=\"cpu08\",id=\"/\"} 0\ncontainer_cpu_usage_seconds_total{cpu=\"cpu09\",id=\"/\"} 0\ncontainer_cpu_usage_seconds_total{cpu=\"cpu10\",id=\"/\"} 0\ncontainer_cpu_usage_seconds_total{cpu=\"cpu11\",id=\"/\"} 0\ncontainer_cpu_usage_seconds_total{cpu=\"cpu12\",id=\"/\"} 0\ncontainer_cpu_usage_seconds_total{cpu=\"cpu13\",id=\"/\"} 0\ncontainer_cpu_usage_seconds_total{cpu=\"cpu14\",id=\"/\"} 0\nI0725 15:13:55.075125   19818 manager.go:195] Machine: {NumCores:2 CpuFrequency:2299815 MemoryCapacity:8373030912 MachineID: SystemUUID:EC28EA6F-6BAA-4E0F-5A1A-6DFF63352727 BootID:4c7e176e-7669-44f9-afac-660f5f82e737 Filesystems:[{Device:/dev/xvda1 Capacity:8318783488 Type:vfs Inodes:524288 HasInodes:true} {Device:docker-docker--pool Capacity:23353884672 Type:devicemapper Inodes:0 HasInodes:false}] DiskMap:map[253:11:{Name:dm-11 Major:253 Minor:11 Size:10737418240 Scheduler:none} 202:26368:{Name:xvdcz Major:202 Minor:26368 Size:23622320128 Scheduler:noop} 253:1:{Name:dm-1 Major:253 Minor:1 Size:23353884672 Scheduler:none} 253:3:{Name:dm-3 Major:253 Minor:3 Size:10737418240 Scheduler:none} 253:5:{Name:dm-5 Major:253 Minor:5 Size:10737418240 Scheduler:none} 202:0:{Name:xvda Major:202 Minor:0 Size:8589934592 Scheduler:noop} 253:0:{Name:dm-0 Major:253 Minor:0 Size:25165824 Scheduler:none} 253:10:{Name:dm-10 Major:253 Minor:10 Size:10737418240 Scheduler:none} 253:12:{Name:dm-12 Major:253 Minor:12 Size:10737418240 Scheduler:none} 253:2:{Name:dm-2 Major:253 Minor:2 Size:23353884672 Scheduler:none} 253:4:{Name:dm-4 Major:253 Minor:4 Size:10737418240 Scheduler:none} 253:8:{Name:dm-8 Major:253 Minor:8 Size:10737418240 Scheduler:none} 253:6:{Name:dm-6 Major:253 Minor:6 Size:10737418240 Scheduler:none} 253:7:{Name:dm-7 Major:253 Minor:7 Size:10737418240 Scheduler:none} 253:9:{Name:dm-9 Major:253 Minor:9 Size:10737418240 Scheduler:none}] NetworkDevices:[{Name:eth0 MacAddress:06:62:a0:7c:2d:a8 Speed:10000 Mtu:9001}] Topology:[{Id:0 Memory:8373030912 Cores:[{Id:0 Threads:[0 1] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:47185920 Type:Unified Level:3}]}] CloudProvider:AWS InstanceType:m4.large InstanceID:i-071cb365455e17303}. I think this is a duplicate of https://github.com/google/cadvisor/issues/1704.. ",
    "stelzner": "@dashpole Done.. ",
    "CaoShuFeng": "@timstclair . ",
    "Nklya": "I have had the same situation with cadvisor v0.29.0.\nAfter a little debug session I found that this is was caused by images with tag <none>.\nYou can find them with docker images|grep '<none>' and delete.\nIt fixed my problem.. ",
    "YogeshGoyal": "I also looking for below information through Cadvisor REST API, but not able to find.\nnumber of containers\nnumber of running containers\nnumber of stopped containers\nnumber of images\n. ",
    "luanna97": "Dear google. I'm so sorry about this pull request. I just want to learn more about github so that i try to create Pull Request in your repository. I didn't know that it can't be deleted. I closed it and i hope that my fault disturb you.. ",
    "wiki12": "I have come up with a question, can you tell me how to get the percent of the cpu usage?. ",
    "wdp-007": "I just want to whether cadvisor has some methods or API or labels to filter metrics. And I don't use this in k8s.. Can someone give some idea or advice?. ",
    "jharshman": "Would be awesome to have this fix merged in.  The related error in issue #1641 is an annoyance.. @dashpole #1712. ",
    "liuliuzi": "You could get metrics by cadvisor API. such as  'curl http://cadvisor IP:4194/api/v1.0/containers/docker/dockerId'. ",
    "hartmut-pq": "Hi @dashpole so you're saying all relevant information is available / provided by cAdvisor already but not mapped/interpreted by the kubelet->summary implementation yet?. @brancz so would this be escalated with kubernetes/kubernetes then? \nMay this be closely related to the works/proposal for 1.8? (https://github.com/coreos/prometheus-operator/issues/485#issuecomment-317410371)\nWho's going to push it to the next round then? :-). anyone pls correct my if I'm wrong - but @dashpole prometheus merely reads what's there - and it's entirely up to the prometheus consumers do deal with changed labels/data... The major usage basically are prometheus alert rules and grafana to visualise based on the metrics.. @dashpole the proposal you mentioned will simply expose the storage metrics - and seem to aim to map/provide PVCReference. \nMost benefit in terms of monitoring - if you have a quick look at the original comment/issue here https://github.com/coreos/prometheus-operator/issues/485#issue-244086169 would be to be allowed to alert if PVs are running out of storage.... or e.g. mirror existing metrics like\nnode_filesystem_avail\nnode_filesystem_files\nnode_filesystem_files_free\nnode_filesystem_free\nnode_filesystem_readonly\nnode_filesystem_size. ",
    "eedugon": "Hi @dashpole ,\nYes, the information we are looking for is available in :10255/stats/summary, and as you said, that's not translated to any kind of metric (that was actually the issue to me).\nI don't want to ask for anything out of the scope for any component, that's the reason my question is more or less.... \"who do you think should be responsible of translating that into a prometheus metric\"?\nYour proposal \"just adding volume metrics to the kubelet's prometheus endpoint\" is exactly what I was looking for, but i don't know if by kubelet or whom (controller-manager?).\nI also fully agree that cadvisor should know nothing about k8s concepts like persistent volumes, but from container perspective, if a docker has a filesystem mounted, I expect prometheus metrics to be exported about that filesystem, which is actually very important to track.\nLet me know your view and thanks in advance!\nEdu\n. @gnufied : I think it should be :10255/metrics, what other endpoint do you see?\n@dashpole : When you mention cadvisor port, what port do you mean? I thought in terms of metrics, cadvisor/kubelet where going to expose only one set of metrics. If there are 2 endpoints let me know which is the other one and I will check if the info we are looking for is already available there.\nAnd as you mention, the \"summary API\" has the correct and complete set of information. So, I don't know who, but someone needs to put that information into a \"metrics\" format.\nBut when you talk about \"prometheus\" and \"kubelet\" relationship I don't completely get you. \n\nthe prometheus endpoint should mirror the information provided by the kubelet's http endpoints (e.g summary API)\nThere are no prometheus endpoints, as prometheus does not generate metrics. If with that you mean cadvisor or kubelet \"metrics\" endpoint, then I agree, but prometheus won't be able to mirror anything, as prometheus is only a metrics consumer.\n\nRegarding metrics names and changes I agree with @hartmut-pq , there won't be any important disruption in prometheus if that happens, just that all components that read data from prometheus should be updated to start digging for the new names.\nThe information we are looking for from containers point of view is the same information you are already providing for other filesystems of the container (size, usage bytes, free bytes, ...). (container_fs_*)... \nThis is info from a container is taken from stats/summary endpoint:\n\"volume\": [\n     {\n      \"time\": \"2017-07-28T07:32:22Z\",\n      \"availableBytes\": 97228500992,\n      \"capacityBytes\": 105555197952,\n      \"usedBytes\": 2941210624,\n      \"inodesFree\": 6551983,\n      \"inodes\": 6553600,\n      \"inodesUsed\": 1617,\n      \"name\": \"datadir\"\n     }\nAnd that volume (container_fs) is completely missing in the kubelet metrics endpoint (that I also thought it was cadvisor endpoint).\n. @jingxu97 : just for curiosity... any progress about this issue? Thanks in advance!. Thanks a lot, @tiloso  and @jingxu97 !. ",
    "gnufied": "@brancz yes I was talking about this proposal - https://github.com/kubernetes/community/pull/855 cc @jingxu97 . @brancz I think that is doable. but @jingxu97 will have more information. BTW - which metric endpoint, there seems to be 2 of them with kubelet. :-) . @cyrus-mc  that was fixed recently and we have backported it to 1.9 https://github.com/kubernetes/kubernetes/pull/60013 \nIf you are still on 1.9 - you should upgrade to next version with the fix.\n. @juliohm1978 you are not seeing those metrics for nfs volumes because nfs volume plugin does not implement necessary metric interface.  If you are up to it - see https://github.com/kubernetes/kubernetes/issues/62644 github issue about how to fix it. \nThe reason some of the volume types don't implement metric interface is because - we haven't had a pressing need until now. We welcome any patches for fixing it though.\n. ",
    "jingxu97": "@eedugon Thanks for your comment and feedback. I am currentlto y working on a feature to expose storage metrics to users which will address your issue.\nMy understanding is that when we say \"prometheus endpoint\" because kubelet registers metrics to prometheus which allows to expose the registered metrics via HTTP. Based on your feedback and others, I think we plan to register the volume metrics to prometheus so that user can use /metric endpoint to check the information. One problem is that it seems some users might want to use PVC as index and some want to use PV name. @eedugon sorry that I missed your message. Currently you can use the following ways to get PVC metrcis\n\nRun \"kubectl proxy\" first. Then \" curl localhost:8001/api/v1/proxy/nodes/\\<nodename>:10250/metrics\"\nSsh to the node, Run \"curl -k http://localhost:10255/metrics\"\n\nPlease let me know if you have problems or questions. Thanks~!. ",
    "tiloso": "@eedugon Kubernetes 1.8 exposes following volume related metrics for Prometheus which can be used to monitor PVC disk usage:\nkubelet_volume_stats_available_bytes\nkubelet_volume_stats_capacity_bytes\nkubelet_volume_stats_inodes\nkubelet_volume_stats_inodes_free\nkubelet_volume_stats_inodes_used\nkubelet_volume_stats_used_bytes. @eedugon Kubernetes 1.8 exposes following volume related metrics for Prometheus which can be used to monitor PVC disk usage:\nkubelet_volume_stats_available_bytes\nkubelet_volume_stats_capacity_bytes\nkubelet_volume_stats_inodes\nkubelet_volume_stats_inodes_free\nkubelet_volume_stats_inodes_used\nkubelet_volume_stats_used_bytes. PVC volume related metrics have been introduced by this commit which seems to be part of Kubernetes >= 1.8.0.\nHere's a very simple example of how we use it to get an idea of the disk usage:\nkubectl's output (removed some columns)\nkubectl get pvc --all-namespaces\nNAMESPACE    NAME                    STATUS    CAPACITY   ACCESS MODES   STORAGECLASS\nmonitoring   data-prometheus-0       Bound     60Gi       RWO            gp2\nstreaming    data-nats-streaming-0   Bound     60Gi       RWO            gp2\nGrafana query\nkubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes * 100\nGrafana legend\n{{ namespace }} | {{ persistentvolumeclaim }}\n. PVC volume related metrics have been introduced by this commit which seems to be part of Kubernetes >= 1.8.0.\nHere's a very simple example of how we use it to get an idea of the disk usage:\nkubectl's output (removed some columns)\nkubectl get pvc --all-namespaces\nNAMESPACE    NAME                    STATUS    CAPACITY   ACCESS MODES   STORAGECLASS\nmonitoring   data-prometheus-0       Bound     60Gi       RWO            gp2\nstreaming    data-nats-streaming-0   Bound     60Gi       RWO            gp2\nGrafana query\nkubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes * 100\nGrafana legend\n{{ namespace }} | {{ persistentvolumeclaim }}\n. ",
    "piwi91": "I don't see these metrics too... @f0 did you found a solution?. @giancarlocosta Unfortunately not... I'm going to monitor it with Zabbix (with auto-discovery) when I have setup connectivity to the monitoring network. If someone has a better solution, I'm all ears.. ",
    "giancarlocosta": "@f0 @piwi91 Any updates on a solution? I'm using 1.8.3 as well and I don't see these metrics. ",
    "cyrus-mc": "I am seeing these metrics, however it shows metrics for volumes that are no longer attached/present. \nIf I stop and restart kubelet that clears them from the metrics reported. . ",
    "chhetripradeep": "Quick question - are these metrics only available for k8s version >= 1.8.0 ? I am running k8s v1.7.11 but dont see this metrics. Is upgrading the k8s cluster an only solution for consuming PV related metrics ?. Hi, this is really important for me since i need to monitor disk space usage for all PV. Can someone suggest me if it is available in any of k8s 1.7 minor versions. Strange, that i am seeing stale metrics for the volumes which are no longer attached to the host. I am running 1.8.11. Anyone has seen this issue ? Is upgrading to 1.9.latest the only way to solve this. . ",
    "juliohm1978": "Apologies for joining late on this thread... but I am confused by @dashpole's comment.\nRegardless of how the kubelet collects and aggregates cAdvisor metrics, your explanation does not make it clear to me why the device in question (/dev/xvdbg) does not appear in cAdvisor metrics to begin with.\nI'm running into a similar issue on our k8s clusters, where we need to gather metrics on volumes mounted by each container. Independently of how k8s concepts, I expected cAdvisor to spit metrics on every device it finds inside the container. I'm not sure the answer provides an explanation.\nI'm glad to use kublet's kubelet_volume_stats_* metrics, but I'm running the latest k8s v1.10.2 and these metrics are still missing.\nAnyone else still having issues on this? I can't find a way to get kubelet_volume_stats_* do show up.. I could be wrong. Isn't this the kubelet endpoint?\nroot@infra03-lab:~# curl -s http://infra03-lab:10255/metrics | grep kubelet_volume | wc -l\n0\ninfra03-lab being the node hosting a container with a mounted nfs volume.. Excellent. Thank you!. ",
    "aghassabian": "@tiloso Actually I can fetch the kubelet_volume_* metrics, but I get multiple values for a single persistent volume on different nodes. I have a cluster on GKE and claim a PV for a pod in namespace X.  when I query for kubelet_volume_stats_used_bytes, I get multiple records for same persistent volume in the same namespace with different values on different cluster nodes. Do you have any idea or have you ever faced this issue?. ",
    "andrezaycev": "Have the same situation. pv+pvc under nfs-deployment+gce disk\ncontainer_fs and kubelet_volume have not needed nfsstorage stats\nMaybe anyone have new solution since Jun2018?. ",
    "szediktam": "\n@juliohm1978 you are not seeing those metrics for nfs volumes because nfs volume plugin does not implement necessary metric interface. If you are up to it - see kubernetes/kubernetes#62644 github issue about how to fix it.\nThe reason some of the volume types don't implement metric interface is because - we haven't had a pressing need until now. We welcome any patches for fixing it though.\n\nhost-path and nfs volume does not implement metric interface for now? @gnufied  . what is the difference bewteem docker stats and \nrate(container_cpu_usage_seconds[10m])/(container_spec_cpu_quota / container_spec_cpu_period)?\n@dashpole \nI use rate(container_cpu_usage_seconds[10m])/(container_spec_cpu_quota /  container_spec_cpu_period) to show container in Prometheus.\nBut the result is difference with docker stats.\nwhat is their releationship \uff1f\nThanks.. ",
    "DexterHD": "@fabxc I'm using Prometheus 1.5.2 and cAdvisor on host machine and I also have this problem.\nAs @zeisss said, if I run cAdvisor without root permission, this fix the problem except that container labels is missing.\nWorst of all with this bug is that Prometheus sometimes lose some containers metrics... In Grafana my graph with running containers looks like this:\n\nAnd I see Alerts from AlertManager that containers is down, but actually all containers working all time.. ",
    "sylr": "I've the same issue with Kubernetes 1.7.2 & 1.7.3.\nhttps://github.com/kubernetes/kubernetes/issues/50151. ~If someone can indicate me how to build hyperkube with a custom cAdvisor commit I'd like to make some tests.~ I think I found how to do this.\nThanks.. ",
    "bassebaba": "I have the exact same problem as @DexterHD, makes me crazy, my container-down alert spams me with false alerts all the time.\n\n. @Hermain @roman-vynar According to release notes 0.26 \"Bug: Fix prometheus metrics.\"\nSo when reverting to 0.25, one misses out on whatever they fixed (but at the same time did break something and introduced the gaps)? I cant find the prometheus-commit that's connected to v0.26 in order to see whats \"fixed\".\nDo we have an ETA on fixing this? No devs in this issue? And no assignee?. ",
    "igortg": "I just started to explore cAdvisor. Seems to have the same issue using InfluxDB:\n\n. ",
    "dixudx": "/cc. ",
    "roman-vynar": "Same thing, 0.26, 0.26.1 are unusable with Prometheus (in our case 1.7.x).\nThey provide a random number of metrics - different number of metrics exposed by /metrics path at a single moment. Had to go back to the old good 0.25.\nDocker 17.03/17.06.. ",
    "Cas-pian": "I meet the same promblem using cadvisor 0.26.1 and prometheus 1.7.1, but it's OK when I changed cadvisor to v0.25.0, and it OK with cadvisor 0.26.1 and prometheus 1.5.3, I'm a little confused, it seems to be a compatibility issue. . ",
    "mfournier": "For those stuck on 0.25.0 because of this issue, I've cherry-picked (04fc089) the patch to kube-state-metrics mentioned above (https://github.com/google/cadvisor/issues/1704#issuecomment-325418911) onto cadvisor's local copy of client_golang/prometheus/registry.go. This simply voids the labels consistency checking introduced in 0.26.0. I also pushed an image with the workaround to docker.io/camptocamp/cadvisor:v0.27.1_with-workaround-for-1704\nNB: this is merely a workaround until a proper fix is available in a release !. ",
    "marcbachmann": "Thank you all \u2665\ufe0f. I hit the same problem; labels are also completely gone.\nThis might be a regression bug introduced in https://github.com/google/cadvisor/pull/1831\nI'm using that dockerfile to build and then run cadvisor outside of docker\n```dockerfile\nDockerfile\nFROM amd64/golang:1.8\nRUN go get -d github.com/google/cadvisor\nWORKDIR /go/src/github.com/google/cadvisor\nRUN git checkout v0.28.3\nRUN GO_CMD=build GOARCH=$ARCH ./build/build.sh && cp cadvisor /bin/cadvisor\n```. ",
    "DatPhanTien": "Dear,\nI am also interested in having the customizability on the dashboard. Were there any consideration taken regarding this aspect, or the answer is a definite NO?\nBest\nDat . ",
    "Brandonage": "I have exactly the same problem. Same configuration for the docker run command. Is there any solution for this?. @dashpole What do you mean exactly with cgroup mounts are correct?. I do mount cgroup in the container with --volume=/cgroup:/cgroup:ro. This is not working either. At least in CentOS is exiting with the error\nF1128 10:03:52.762935       1 cadvisor.go:153] Failed to start container manager: inotify_add_watch /sys/fs/cgroup/cpuacct,cpu: no such file or directory\nWhich was fixed in newer versions.. ",
    "lcfang": "Hi @dashpole , Do you have some idea about this problem? Maybe  I have the similar question.. ",
    "yguo0905": "/retest. The directory (/dev/disk/by-uuid) which we use to get the uuid to device name mapping has been available since (at least) 2.26. It's the tool lsblk that's too old to print the UUID on my local machine. Also, if this directory does not exist, cadvisor will report no filesystem with uuid xxx exists for all uuid queries.. Done. Thanks!. Are we going to change all the call sites to use the new API instead of docker/engine-api and eventually remove the old dependency in a separate PR?. /lgtm\nWe can do that later since it's not a blocker.. This indicates that the filesystem with the UUID does not exist on local machine, which is different from an error on fetching the stats for a known UUID. I think kubelet would need this information -- print an info message at some verbosity when the UUID does not exist, but return or log an error if cadvisor fails to get the stats. WDYT?. Good catch. Removed the irreverent code. I originally used this to get the partition from uuid by iterating the partition slice, and later changed to use a separate fsUUIDToDeviceName map, but didn't clean this up... This is related to the the first comment.. Done. Removed the unnecessary function.. Done. PTAL.. ",
    "inbalzoran": "same here, (prometheus v2.2.0, cadvisor v0.29.0)\ni'm trying to compare docker stats to the results i'm getting from container_cpu_usage_seconds_total, and it doesn't add up.\ni have 2 cpu's so the docker stats supposes to be out of 200%. \nmy query is\navg(irate(container_cpu_usage_seconds_total{id=~\"/docker/.*\"}[30s])) by (container_label_com_docker_swarm_service_name) * 100 \nso my result should be out of 100%. but it looks like the query result is much smaller than 2 times the docker stats.\nalso the sum of container_cpu_usage_seconds_total is not equal to container_cpu_user_seconds_total, which one should i use?. ",
    "Nitrosito": "You can find it?. I using SWARM\nHow we can check is correct?\nSee process ram consumption and make (total ram used - cache ram) ?\nEl vie., 18 ene. 2019 22:30, David Ashpole notifications@github.com\nescribi\u00f3:\n\ndocker is probably reporting usage, whereas cadvisor reports working_set,\nwhich is usage - inactive_file. working_set is the value used by kubernetes\nto make eviction decisions, which is why cadvisor produces it.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/2149#issuecomment-455693608,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AEUlVAkA5x0aB2ZLKRnaYEFmsG5a_tK1ks5vEjz3gaJpZM4aHWh_\n.\n. \n",
    "AlexJakeGreen": "I signed it!. /reopen. ",
    "fbuecklers": "Any chances that this comes up into master?\nThe Patch is awesome, we used it to monitor our swarm cluster.\nThe following metrics are reduced from our cadvisors:\nMemory consumption 250MB -> 80MB\nUsed CPU from 20% -> 0.5-1%\n/metrics response time: 20s -> ~ 3s\n/metrics response size from 25MB -> 3MB\n. ",
    "chrisboulton": "@dashpole, @cirocosta - I'd be super interested in these changes as well, because we heavily rely on labels on our containers for a bunch of other reasons which aren't really applicable to Prometheus monitoring. If it's just a matter of addressing those nitpicks (and resolving the recent merge conflicts), I'd be happy to get that done, sign the CLA, and contribute changes back. Was that all you'd be looking to address with this or are there other changes made of late that should be considered? . ",
    "runcom": "failure is because there's no CRI-O in the CI to test this with:\nW0831 16:13:03.580] W0831 16:13:03.498099    2569 manager.go:166] unable to connect to CRI-O api service: Get http://localhost:7373/info: dial tcp 127.0.0.1:7373: getsockopt: connection refused. /retest. failure doesn't seem related (?). Updated to make calls on the CRI-O socket. @derekwaynecarr addressed your comments. removing WIP label, I think it's fully ready for review (added some unit tests also):\n\n\n\n`kubectl get --raw /api/v1/nodes/127.0.0.1/proxy/stats/summary`\n\n\n\n```\n{\n  \"node\": {\n   \"nodeName\": \"127.0.0.1\",\n   \"startTime\": \"2017-09-04T11:40:43Z\",\n   \"cpu\": {\n    \"time\": \"2017-09-04T16:13:10Z\",\n    \"usageNanoCores\": 454768832,\n    \"usageCoreNanoSeconds\": 4424417531376\n   },\n   \"memory\": {\n    \"time\": \"2017-09-04T16:13:10Z\",\n    \"availableBytes\": 11485138944,\n    \"usageBytes\": 10475511808,\n    \"workingSetBytes\": 9253203968,\n    \"rssBytes\": 3624992768,\n    \"pageFaults\": 61345034,\n    \"majorPageFaults\": 14271\n   },\n   \"fs\": {\n    \"time\": \"2017-09-04T16:13:10Z\",\n    \"availableBytes\": 13987569664,\n    \"capacityBytes\": 52710469632,\n    \"usedBytes\": 36021768192,\n    \"inodesFree\": 2681895,\n    \"inodes\": 3276800,\n    \"inodesUsed\": 594905\n   },\n   \"runtime\": {\n    \"imageFs\": {\n     \"time\": \"2017-09-04T16:13:10Z\",\n     \"availableBytes\": 13987569664,\n     \"capacityBytes\": 52710469632,\n     \"usedBytes\": 560776649,\n     \"inodesFree\": 2681895,\n     \"inodes\": 3276800,\n     \"inodesUsed\": 594905\n    }\n   }\n  },\n  \"pods\": [\n   {\n    \"podRef\": {\n     \"name\": \"kube-dns-4124969034-6gqh6\",\n     \"namespace\": \"kube-system\",\n     \"uid\": \"fc25928b-918a-11e7-b392-507b9d4141fa\"\n    },\n    \"startTime\": \"2017-09-04T16:06:33Z\",\n    \"containers\": [\n     {\n      \"name\": \"sidecar\",\n      \"startTime\": \"2017-09-04T16:06:34Z\",\n      \"cpu\": {\n       \"time\": \"2017-09-04T16:13:09Z\",\n       \"usageNanoCores\": 1040646,\n       \"usageCoreNanoSeconds\": 627473919\n      },\n      \"memory\": {\n       \"time\": \"2017-09-04T16:13:09Z\",\n       \"usageBytes\": 11571200,\n       \"workingSetBytes\": 11444224,\n       \"rssBytes\": 10747904,\n       \"pageFaults\": 4191,\n       \"majorPageFaults\": 1\n      },\n      \"rootfs\": {\n       \"time\": \"2017-09-04T16:13:09Z\",\n       \"availableBytes\": 13987569664,\n       \"capacityBytes\": 52710469632,\n       \"usedBytes\": 42471424,\n       \"inodesFree\": 2681895,\n       \"inodes\": 3276800,\n       \"inodesUsed\": 14\n      },\n      \"logs\": {\n       \"time\": \"2017-09-04T16:13:09Z\",\n       \"availableBytes\": 13987569664,\n       \"capacityBytes\": 52710469632,\n       \"usedBytes\": 20480,\n       \"inodesFree\": 2681895,\n       \"inodes\": 3276800,\n       \"inodesUsed\": 594905\n      },\n      \"userDefinedMetrics\": null\n     },\n     {\n      \"name\": \"dnsmasq\",\n      \"startTime\": \"2017-09-04T16:06:34Z\",\n      \"cpu\": {\n       \"time\": \"2017-09-04T16:13:07Z\",\n       \"usageNanoCores\": 848354,\n       \"usageCoreNanoSeconds\": 169743690\n      },\n      \"memory\": {\n       \"time\": \"2017-09-04T16:13:07Z\",\n       \"usageBytes\": 7319552,\n       \"workingSetBytes\": 7315456,\n       \"rssBytes\": 6520832,\n       \"pageFaults\": 2759,\n       \"majorPageFaults\": 0\n      },\n      \"rootfs\": {\n       \"time\": \"2017-09-04T16:13:07Z\",\n       \"availableBytes\": 13987569664,\n       \"capacityBytes\": 52710469632,\n       \"usedBytes\": 42090496,\n       \"inodesFree\": 2681895,\n       \"inodes\": 3276800,\n       \"inodesUsed\": 15\n      },\n      \"logs\": {\n       \"time\": \"2017-09-04T16:13:07Z\",\n       \"availableBytes\": 13987569664,\n       \"capacityBytes\": 52710469632,\n       \"usedBytes\": 20480,\n       \"inodesFree\": 2681895,\n       \"inodes\": 3276800,\n       \"inodesUsed\": 594905\n      },\n      \"userDefinedMetrics\": null\n     },\n     {\n      \"name\": \"kubedns\",\n      \"startTime\": \"2017-09-04T16:06:33Z\",\n      \"cpu\": {\n       \"time\": \"2017-09-04T16:13:10Z\",\n       \"usageNanoCores\": 609739,\n       \"usageCoreNanoSeconds\": 348014134\n      },\n      \"memory\": {\n       \"time\": \"2017-09-04T16:13:10Z\",\n       \"availableBytes\": 170975232,\n       \"usageBytes\": 7282688,\n       \"workingSetBytes\": 7282688,\n       \"rssBytes\": 6557696,\n       \"pageFaults\": 2524,\n       \"majorPageFaults\": 0\n      },\n      \"rootfs\": {\n       \"time\": \"2017-09-04T16:13:10Z\",\n       \"availableBytes\": 13987569664,\n       \"capacityBytes\": 52710469632,\n       \"usedBytes\": 50057216,\n       \"inodesFree\": 2681895,\n       \"inodes\": 3276800,\n       \"inodesUsed\": 15\n      },\n      \"logs\": {\n       \"time\": \"2017-09-04T16:13:10Z\",\n       \"availableBytes\": 13987569664,\n       \"capacityBytes\": 52710469632,\n       \"usedBytes\": 20480,\n       \"inodesFree\": 2681895,\n       \"inodes\": 3276800,\n       \"inodesUsed\": 594905\n      },\n      \"userDefinedMetrics\": null\n     }\n    ],\n    \"network\": {\n     \"time\": \"2017-09-04T16:13:03Z\",\n     \"rxBytes\": 290526,\n     \"rxErrors\": 0,\n     \"txBytes\": 266470,\n     \"txErrors\": 0\n    },\n    \"volume\": [\n     {\n      \"time\": \"2017-09-04T16:07:15Z\",\n      \"availableBytes\": 10369159168,\n      \"capacityBytes\": 10369171456,\n      \"usedBytes\": 12288,\n      \"inodesFree\": 2531527,\n      \"inodes\": 2531536,\n      \"inodesUsed\": 9,\n      \"name\": \"kube-dns-token-8b24g\"\n     }\n    ]\n   }\n  ]\n }\n```\n\n. pretty sure tests failure is unrelated. Added some unit tests also.. > @runcom -- looks like a gofmt error on handler_test.go i think.\n@derekwaynecarr fixed. > @derekwaynecarr Yeah, we are returning the path to container rootfs. So, we need to add \"diff\" here to do the equivalent for cri-o - https://github.com/google/cadvisor/blob/master/container/crio/handler.go#L144\nduring my testing overlay2 didn't show this issue, I'm not sure.... we need this in cri-o as well @derekwaynecarr @mrunalp . @mrunalp yes, I'm working on a cri-o fix. @derekwaynecarr done. This should be included yeah. The bot says maintainers should check that and merge cause it's not going to, read comment from bot above. > can you confirm the 1.9 cri-o packaging is in the new location?\nyeah, the plan is to move to /var/run/crio/crio.sock for 1.9 (we're at 1.8 still). CRI-O PR is here now https://github.com/kubernetes-incubator/cri-o/pull/1154. @derekwaynecarr @sjenning @mrunalp PTAL. > Just curious, what behavior did you see that lead you to this fix?\nwe got a panic about a concurrent map access :). docker has the exact same value when using socket connection. ",
    "ylfforme": "\nThanks for pointing this out.\n\ncan I understand like this\uff1f \u201dcontainer_memory_usage_bytes\u201d is rss + cache + swap\n\u201ccontainer_memory_rss\u201d is just rss not include cache and swap\n. ",
    "jstangroome": "@ylfforme \ncontainer_memory_usage_bytes == container_memory_rss + container_memory_cache + container_memory_swap + kernel memory.\nBut kernel memory is not exposed as a metric yet, see #2138 . ",
    "HarenBroog": "@raji90 I have managed to collect metrics from cAdvisor. I used storage driver stdout alongside with logspout.  I think elasticsearch driver is broken with ES v5.\nSnippet from my docker-stack.yml:\n```yml\nversion: '3'\nservices:\n  cadvisor:\n    image: google/cadvisor:v0.26.1\n    deploy:\n      replicas: 1\n    volumes:\n      - \"/../:/rootfs:ro\"\n      - \"/var/run:/var/run:rw\"\n      - \"/sys:/sys:ro\"\n      - \"/var/lib/docker/:/var/lib/docker:ro\"\n      - \"/dev/disk/:/dev/disk:ro\"\n    command: -storage_driver=\"stdout\" -storage_driver_es_host=\"http://elk:9200\"\n    ports:\n      - 8080:8080\n  logspout:\n    image: local/logspout\n    volumes:\n       - /var/run/docker.sock:/var/run/docker.sock\n    environment:\n      ROUTE_URIS: \"logstash://elk:5000?filter.name=my_actual_stack_to_monitor,logstash://elk:5000?filter.name=elk_stack_cadvisor\"\n      DOCKER_LABELS: \"true\"\n  elk:\n    image: sebp/elk:551\n    volumes:\n      - /home/user/apps/elk_stack/data:/var/lib/elasticsearch\n      - /home/user/apps/elk_stack/deploy/logstash/01-json.conf:/etc/logstash/conf.d/01-json.conf\n      - /home/user/apps/elk_stack/deploy/logstash/30-output.conf:/etc/logstash/conf.d/30-output.conf\n    ports:\n     - 5601:5601\n````\nMoreover, you probably want to differentiate index name for metrics (CPU, memory etc) and your app logs. I used dynamic logstash index for that, with docker stack name as a variable. \nI hope that will solve your issues :) . ",
    "bsingr": "I am using AWS ECS. /retest. /retest. @dashpole Yes, I tested this on a AWS EC2 instance with Debian 9.\nFor testing I started a container with memory limits like so: docker run --name prom -l prometheus -d -m 100m --memory-reservation 60m prom/prometheus\nThe limits properly show up in the cadvisor web UI:\n\nAnd also in the /metris endpoint:\n\n(6.291456e+07 / 1024 / 1024 = 60 MB). Is there anything I can do here? For my team these fixes work as expected. Please let me know if I can do anything to get this done here.. ",
    "DeepakSahoo-Reflektion": "I am also getting the similar issue here. I can see containers in web ui of the cadvisor , but i am not getting any data in the influxdb.\n[CONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS              PORTS                                                                NAMES\nb6c40a29359d        google/cadvisor:latest   \"/usr/bin/cadvisor...\"   10 minutes ago      Up 10 minutes       0.0.0.0:8080->8080/tcp                                               cadvisor\n858b9d6ce40c        tutum/influxdb           \"/run.sh\"                19 minutes ago      Up 19 minutes       0.0.0.0:8083->8083/tcp, 8090/tcp, 0.0.0.0:8086->8086/tcp, 8099/tcp   influxsrv](url)\nwhen i checked the docker logs for cadvisor container, it says something like below which is unclear why the stats are not getting pushed to influxdb:\nI1010 23:39:42.981917       1 storagedriver.go:50] Caching stats in memory for 2m0s\nI1010 23:39:42.982204       1 manager.go:143] cAdvisor running in container: \"/docker/b6c40a29359dd039a6d684605975bc102f4eb2495646f65b3e9689d0bafa0812\"\nW1010 23:39:42.995084       1 manager.go:151] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp 127.0.0.1:15441: getsockopt: connection refused\nI1010 23:39:43.011775       1 fs.go:117] Filesystem partitions: map[/dev/sda1:{mountpoint:/var/lib/docker major:8 minor:1 fsType:ext4 blockSize:0}]\nI1010 23:39:43.015365       1 info.go:47] Couldn't collect info from any of the files in \"/rootfs/etc/machine-id,/var/lib/dbus/machine-id\"\nI1010 23:39:43.015407       1 manager.go:198] Machine: {NumCores:4 CpuFrequency:2194520 MemoryCapacity:2095878144 MachineID: SystemUUID:E06182AD-A1A2-C93F-B696-B5CF0E11F9EE BootID:60fdba38-c69a-4841-adc5-51f87cb4cb63 Filesystems:[{Device:/dev/sda1 Capacity:67371577344 Type:vfs Inodes:4194304 HasInodes:true} {Device:overlay Capacity:67371577344 Type:vfs Inodes:4194304 HasInodes:true}] DiskMap:map[43:0:{Name:nbd0 Major:43 Minor:0 Size:0 Scheduler:none} 43:12:{Name:nbd12 Major:43 Minor:12 Size:0 Scheduler:none} 43:13:{Name:nbd13 Major:43 Minor:13 Size:0 Scheduler:none} 43:3:{Name:nbd3 Major:43 Minor:3 Size:0 Scheduler:none} 43:14:{Name:nbd14 Major:43 Minor:14 Size:0 Scheduler:none} 43:15:{Name:nbd15 Major:43 Minor:15 Size:0 Scheduler:none} 43:4:{Name:nbd4 Major:43 Minor:4 Size:0 Scheduler:none} 43:5:{Name:nbd5 Major:43 Minor:5 Size:0 Scheduler:none} 43:9:{Name:nbd9 Major:43 Minor:9 Size:0 Scheduler:none} 43:10:{Name:nbd10 Major:43 Minor:10 Size:0 Scheduler:none} 43:2:{Name:nbd2 Major:43 Minor:2 Size:0 Scheduler:none} 43:6:{Name:nbd6 Major:43 Minor:6 Size:0 Scheduler:none} 43:7:{Name:nbd7 Major:43 Minor:7 Size:0 Scheduler:none} 43:8:{Name:nbd8 Major:43 Minor:8 Size:0 Scheduler:none} 43:1:{Name:nbd1 Major:43 Minor:1 Size:0 Scheduler:none} 43:11:{Name:nbd11 Major:43 Minor:11 Size:0 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:68719476736 Scheduler:deadline}] NetworkDevices:[{Name:bond0 MacAddress:92:d6:1b:d9:e3:d6 Speed:0 Mtu:1500} {Name:dummy0 MacAddress:9a:e0:3a:58:fa:60 Speed:0 Mtu:1500} {Name:eth0 MacAddress:02:50:00:00:00:01 Speed:-1 Mtu:1500} {Name:gre0 MacAddress:00:00:00:00 Speed:0 Mtu:1476} {Name:gretap0 MacAddress:00:00:00:00:00:00 Speed:0 Mtu:1462} {Name:ip6_vti0 MacAddress:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00 Speed:0 Mtu:1500} {Name:ip6gre0 MacAddress:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00 Speed:0 Mtu:1448} {Name:ip6tnl0 MacAddress:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00 Speed:0 Mtu:1452} {Name:ip_vti0 MacAddress:00:00:00:00 Speed:0 Mtu:1332} {Name:sit0 MacAddress:00:00:00:00 Speed:0 Mtu:1480} {Name:teql0 MacAddress: Speed:0 Mtu:1500} {Name:tunl0 MacAddress:00:00:00:00 Speed:0 Mtu:1480}] Topology:[{Id:0 Memory:0 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:6291456 Type:Unified Level:3} {Size:134217728 Type:Unified Level:4}]} {Id:1 Memory:0 Cores:[{Id:0 Threads:[1] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:6291456 Type:Unified Level:3} {Size:134217728 Type:Unified Level:4}]} {Id:2 Memory:0 Cores:[{Id:0 Threads:[2] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:6291456 Type:Unified Level:3} {Size:134217728 Type:Unified Level:4}]} {Id:3 Memory:0 Cores:[{Id:0 Threads:[3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:6291456 Type:Unified Level:3} {Size:134217728 Type:Unified Level:4}]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}\nI1010 23:39:43.016126       1 manager.go:204] Version: {KernelVersion:4.9.41-moby ContainerOsVersion:Alpine Linux v3.4 DockerVersion:17.06.2-ce CadvisorVersion:v0.25.0 CadvisorRevision:17543be}\nI1010 23:39:43.028048       1 factory.go:309] Registering Docker factory\nW1010 23:39:43.028073       1 manager.go:247] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp 127.0.0.1:15441: getsockopt: connection refused\nI1010 23:39:43.028078       1 factory.go:54] Registering systemd factory\nI1010 23:39:43.029058       1 factory.go:86] Registering Raw factory\nI1010 23:39:43.030026       1 manager.go:1106] Started watching for new ooms in manager\nW1010 23:39:43.030139       1 manager.go:275] Could not configure a source for OOM detection, disabling OOM events: unable to find any kernel log file available from our set: [/var/log/kern.log /var/log/messages /var/log/syslog]\nI1010 23:39:43.030572       1 manager.go:288] Starting recovery of all containers\nI1010 23:39:43.048220       1 manager.go:293] Recovery completed\nI1010 23:39:43.058476       1 cadvisor.go:157] Starting cAdvisor version: v0.25.0-17543be on port 8080. ",
    "chinajuanbob": "+1. ",
    "pradeepdogga": "I am having the same issue mentioned by @DeepakSahoo-Reflektion . Can someone please suggest what to do ?. ",
    "gorbiz": "Misclick, so sorry!. ",
    "unixwitch": "Also seeing this on GCE with CentOS 7.4 and Kubernetes 1.8:\nOct 10 05:29:39 staging-head-2 kubelet[1513]: E1010 05:29:39.001380    1513 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs\nLinux staging-head-2 3.10.0-693.2.2.el7.x86_64 #1 SMP Tue Sep 12 22:26:13 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\nI understood from #1711 that this should only affect 4.7+, so I'm not entirely sure it's the same issue, but RedHat backports a lot of changes from newer kernels.. ",
    "dada941": "Thanks for looking into this @euank \nYes, the Kubelet runs in a RKT container in CoreOS, but I've tried out of a container with the binary, and get the same result.\nHere's the output of the command, run from the RKT container :\n# cat /sys/fs/cgroup/cpu,cpuacct/cpuacct.usage_percpu\n87983471343 95801030681 0 0 0 0 0 0 0 0 0 0 0 0 0\n. ",
    "ravisantoshgudimetla": "@sjenning Sometimes, I have noticed that du operation is taking more than 30 seconds, which means this line will be printed 300 times after which it should stop logging, which I believe should work fine. Any specific reason for not decreasing log level from 2 to higher value?. > at first I did just decrease the log level. But then someone running at the higher logging level would have the same complaint\nMakes sense. . Can we move this variable to trackUsage fn?. ",
    "JokerZHA": "Updating docker version to 17.09.0-ce, build afdb6d4 fixes the issue. ",
    "warmchang": "\ud83d\udc4d . /hold pls\nfound some other misspell, will fix and commit later.. Finished, please check it, thanks.. /hold cancel. ",
    "lf1029698952": "LGTM \ud83d\udc4d. ",
    "janderson2": "Still seeing this behaviour with version 0.28.3 (1e567c2) while running the binary natively on an EC2 instance running ubuntu 16.04 and using aufs storage driver .. ",
    "tiagoad": "I'm still getting my logs filled with this message every second. Any fix possible while the image is not updated?. ",
    "amihura": "+++. ",
    "nightah": "@dashpole while it doesn't look like #1847 is necessarily a fix for this, is it likely there might be a change to lower the log verbosity of said error in the instance of devicemapper/zfs filesystems?\nAt this point, even if we run cadvisor with --v=0 option, the error still appears. You can imagine if containers are being removed frequently, without cadvisor being restarted it can result in hundreds of lines of error logs per second.. Not sure if related so happy to lodge this as another issue if advised but I appear to be having an issue whenever a docker container that cadvisor is monitoring is removed:\nE0213 22:55:53.446365       1 fs.go:418] Stat fs failed. Error: exit status 1: \"/usr/sbin/zfs zfs list -Hp -o name,origin,used,available,mountpoint,compression,type,volsize,quota,referenced,written,logicalused,usedbydataset nerv/ROOT/arch/13d5ce6a00b5331dfcf92c9b3cd50c7b5eefb570bb6d471b002489eb79d8fb61\" => cannot open 'nerv/ROOT/arch/13d5ce6a00b5331dfcf92c9b3cd50c7b5eefb570bb6d471b002489eb79d8fb61': dataset does not exist\nThis behaviour can be re-produced by starting cadvisor with a number of running docker containers on the ZFS storage driver, removing any of the said containers.\nWhen the container in question is removed, cadvisor throws an error as above, I guess this is because the zfs dataset respective to the removed container has also been removed.\nRestarting cadvisor does stop the error from appearing, but I suspect that's because of the datasets that are discovered on startup.. @dashpole thanks for the heads up, I tried looking around to see if there was anything relating to ZFS didn't think to check devicemapper too.\nI'll follow both of these issues moving forward to see how they progress. Thanks.. ",
    "joshperry": "FYI, having this problem with the btrfs driver as well.. ",
    "eklitzke": "In addition to lowering the log verbosity, I suggest adding an option like prometheus-node-exporter's --collector.filesystem.ignored-fs-types which would tell cadvisor to ignore the listed filesystem types. This could be used to explicitly ignore aufs etc.. ",
    "ZOXEXIVO": "Same issue: 8core x2 CPU with 9 running containers ~ CAdvisor take 13% of processor resources. ",
    "schabrolles": "Same here: 1core VM with ~10 containers (running but doing nothing). CAdvisor takes 18% cpu. . ",
    "else": "Same here. Lots of iowait. cadvisor USED mem (top) > 2G. Running cAdvisor version v0.27.4 (492322b). Turned out there were too many open files.. ",
    "bmerry": "I'm seeing something similar (CPU usage consistently 30-70%). Profile below:\n\nThis profile is on a machine running only 12 containers, and we've seen it on machines with even fewer, but there doesn't seem to be a clear pattern. Restarting cadvisor doesn't resolve the problem, but rebooting the machine does. We have noticed that the affected machines have been gradually losing available memory over time, which /proc/meminfo shows is going into Slab memory, and slabtop shows the slab memory is about 80%+ dentry. We're not sure which direction the causality is i.e. whether cadvisor is hammering the FS and causing lots of dentries to be cached, or if something else on the machine is causing memory issues and that is making cadvisor CPU-heavy. The machine from which that profile was taken is also not showing the same memory leak, so it may be a red herring.\nWe're running cadvisor 0.29.0 with /usr/local/bin/cadvisor --port 5003 --logtostderr=true. Some Docker info from the machine from which the profile was taken:\n```\nContainers: 71\n Running: 12\n Paused: 0\n Stopped: 59\nImages: 80\nServer Version: 18.03.1-ce\nStorage Driver: overlay2\n Backing Filesystem: extfs\n Supports d_type: true\n Native Overlay Diff: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: bridge host ipvlan macvlan null overlay\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nInit Binary: docker-init\ncontainerd version: 773c489c9c1b21a6d78b5c538cd395416ec50f88\nrunc version: 4fc53a81fb7c994640722ac585fa9ca548971871\ninit version: 949e6fa\nSecurity Options:\n apparmor\n seccomp\n  Profile: default\nKernel Version: 4.13.0-41-generic\nOperating System: Ubuntu 16.04.4 LTS\nOSType: linux\nArchitecture: x86_64\nCPUs: 8\nTotal Memory: 125.9GiB\nName: lab1\nID: GRBO:TBRS:Z5BJ:4RVS:HBEA:V24X:Q33J:66JI:7D4F:XX2O:ZD2M:WPYS\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): false\nRegistry: https://index.docker.io/v1/\nLabels:\nExperimental: true\nInsecure Registries:\n 127.0.0.0/8\nLive Restore Enabled: false\nWARNING: No swap limit support\n```. @dashpole thanks for taking a look. I've noticed that the pprof report claims ~4% CPU usage, which doesn't match what top reports (40%+). I've confirmed by taking another profile and the graph header reports ~4% while top showed 30%+ over the entire 30s. So there is something odd in the way the profiler is working - perhaps it only profiles user time? htop shows the CPU usage is mostly in the kernel.\nRunning perf top, the top hits are\n31.04%  [kernel]                                      [k] memcg_stat_show\n  18.61%  [kernel]                                      [k] memcg_sum_events.isra.22\n   9.41%  [kernel]                                      [k] mem_cgroup_iter\n   6.94%  [kernel]                                      [k] css_next_descendant_pre\n   6.11%  [kernel]                                      [k] _find_next_bit\n   3.96%  [kernel]                                      [k] mem_cgroup_usage.part.43\n   1.75%  [kernel]                                      [k] find_next_bit\n   1.38%  [kernel]                                      [k] mem_cgroup_node_nr_lru_pages\n   1.22%  [kernel]                                      [k] nmi\nwhich does seem to confirm your idea that it's related to cgroups.\nThe machine with the graph (using ~40% CPU) has 14669 files in /sys/fs/cgroup (counted with find . | wc), while another machine using ~1% CPU has 9935. So the file count alone doesn't seem to explain the order-of-magnitude differences in CPU usage.\nFWIW, I've just tried with cadvisor 0.30.2 and --disable_metrics=tcp,udp,disk,network - no improvement. Is there anything else I should try turning off?\n\nAccessing cgroup files repeatedly should make the dentry cache grow, but it should be reclaimed when free memory gets low.\n\nI'm hoping so - for now it's been \"leaking\" slowly enough that there hasn't been any memory pressure. On one machine the memory was in SUnreclaim rather than SReclaimable, but when I dropped the dentry cache manually the memory was returned.. It seems like it's definitely some slow path in the kernel. Simply time cat /sys/fs/cgroup/memory/memory.stat takes 0.376s on the affected machine, and 0.002s on an unaffected machine. The affected machine has about 50% more cgroups, but that doesn't explain a >100x slowdown.\nThat sounds more like a kernel issue than cadvisor's problem and if I get time I may try to take it up on the LKML, but if you have any suggestions on fixing or diagnosing it, I'll be happy to hear them.\nThis is probably a separate issue from the original report, where pprof showed high CPU usage.  @ZOXEXIVO @schabrolles are you seeing the same behaviour I am?. After some discussions on the linux-mm mailing list and some tests, it sounds like the problem may be \"zombie\" cgroups: cgroups that have no processes and have been deleted but still have memory charged to them (in my case, from the dentry cache, but it could also be from page cache or tmpfs). These are still iterated over when computing the top-level memory stats. We had a service that was repeatedly failing and being restarted (by systemd), which probably churned through a lot of cgroups over a few weeks.\nI still need to experiment with ways to fix the underlying problem (including checking if it is better in newer kernels), but I'd like to find out if there is a way to work around it. In particular, we tend to use it only to get per-Docker-container metrics into Prometheus, and not so much for aggregate or system metrics (we have node-exporter for that). So if there is a way we can turn off collection of /sys/fs/cgroup/memory/memory.stats and /sys/fs/cgroup/memory/system.slice/memory.stats while still collecting memory stats on individual Docker containers that would probably help.. > We don't have the option to disable collection of the root cgroup. We have an option --docker_only, but that keeps the root cgroup around.\nDo you think that would be reasonably easy for someone not familiar with the code to implement as a command-line option, or is it pretty core?. That didn't work for me:\nF0726 15:36:56.808759    7808 cadvisor.go:168] Failed to start container manager: no known factory can handle creation of container\nI also tried taking out the / case in this line and running with --docker_only: it runs, but from strace I can see that it is still opening /sys/fs/cgroup/memory/memory.stat (as well as a bunch of cgroups under /sys/fs/cgroup/memory/system.slice, despite the --docker_only).. I never did get all the way to the bottom of it. Some step in creating and destroying cgroups interacts in a non-deterministic way with the reads that cadvisor does to cause cached dentries that keep the cgroups alive as zombies. There is a patch (which I assume will go into the next Linux release) which makes the stats collection a lot faster and thus reduces the impact, but doesn't prevent the zombies in the first place.\nI gave up trying to fix the problem, and now we have a cron job that times reading /sys/fs/cgroup/memory/memory.stat and if it takes too long, drops the dentry cache.. ",
    "awestendorf": "Hopping in to say thank you @bmerry and @WnP on how you diagnosed and fixed this.  We're seeing the same thing, on the same kernel, unrelated to cadvisor, and clearing the pagecache fixed it. . ",
    "michaelkrog": "I am seeing the same on Ubuntu 16.04 even after clearing all caches. cadvisor goes up to 50% CPU every few seconds.\nCONTAINER ID        NAME                                                                                    CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O           PIDS\n57571530dfa5        cadvisor.fia8k180v3zlerr6zct851eh2.zhxr0g6bt2mh63x8t2mx462e8                            53.56%              58.35MiB / 128MiB     45.58%              690B / 0B           22.7MB / 0B         26\nd1372924fdbe        gabrielle-web-v1-test.1.q6l14fnvkibdyvlr2d0zogph5                                       0.00%               5.445MiB / 2GiB       0.27%               184kB / 8.77MB      10.6MB / 8.19kB     3\nc7f9a50145cb        eg-giulia-services-vehicles-v1-test.1.jqwh5iqtduf9gm4vczkxse20k                         1.13%               212.6MiB / 1.219GiB   17.04%              15.1MB / 4.36MB     328kB / 32.8kB      29\n117d19335355        xena-seges-webapp-v1-test.1.wpgwtwhksvnr3c3jue0off8ig                                   0.00%               5.758MiB / 2GiB       0.28%               159kB / 7.15MB      0B / 8.19kB         3\n54e4f759865c        xena-identityserver-v1-test.2.1g8rqmk2beap8iulc9nrw8bmw                                 0.63%               282.2MiB / 480MiB     58.80%              85.6MB / 106MB      2.7MB / 1.13MB      78\n3d3d4b3299d3        eg-emma-frontend-v1-production.1.rze5xi565td665cd2vath17tt                              0.00%               4.539MiB / 2GiB       0.22%               558kB / 13.5MB      0B / 0B             2\nc00dd9323968        xena-seges-services-login-v1-test.1.0kh148klp61kutkj5dxgb6g6c                           0.91%               140MiB / 256MiB       54.68%              274MB / 95.1MB      598kB / 16.4kB      27\n75b88775e556        xena-seges-apigateway-v1-test.1.02ahxufzqq6wahf1gi0vynrhy                               0.77%               641.5MiB / 2GiB       31.32%              53.9MB / 41.6MB     803kB / 197kB       25\n22c696046ef1        eg-scalepoint-frontend-v1-test.1.l2wlfrz2xp57xw9214a4uuxr3                              0.00%               3.098MiB / 1GiB       0.30%               249kB / 0B          0B / 8.19kB         3\n50ece1bf9c38        eg-emma-services-settings-v1-production.1.s6ssn8z28ycb2dcdtgi6uxhkb                     0.27%               191.5MiB / 2GiB       9.35%               215MB / 36.5MB      7.08MB / 360kB      29\nf97ea262491f        eg-emma-apigateway-v2-production.2.b3hbkwjzxqfpcoovt37nldz4o                            0.51%               113.2MiB / 2GiB       5.53%               185kB / 826B        2.79MB / 0B         24\nca8f6201e056        eg-one-payroll-frontend-v1-production.1.u86sazda8lctzndsfruh0q2d7                       0.00%               5.004MiB / 32MiB      15.64%              464kB / 2.02MB      8.19kB / 8.19kB     3\n450aba876826        xena-identityserver-v2-production.1.eoumb3vrhtafsltsjw52hqt0x                           0.71%               269.8MiB / 1GiB       26.35%              1.58GB / 2.67GB     3.87MB / 238kB      45\ne902bf65f1c2        eg-emma-services-xena-v1-production.1.j68n82fhd592ioia8lu6er59y                         0.69%               136.8MiB / 2GiB       6.68%               3.92MB / 3.82MB     2.91MB / 0B         28\ne4bc84719061        eg-gaia-services-importers-v1-test.1.qhg5jop1h2y71ayjvihrnbejo                          1.00%               180.4MiB / 2GiB       8.81%               412MB / 86.6MB      115kB / 0B          29\ndb532afa31d9        eg-one-storage-api-v1-production.1.jojhb7irnodi5fcuhzuk7jaa0                            0.07%               210MiB / 2GiB         10.25%              411MB / 85.8MB      5.18MB / 6.03MB     28\n8f5ec434f67b        xena-identityserver-developerconsole-plugin-v1-production.1.wo9zkx3cdkcnl0qwf60k6llhx   0.04%               152.4MiB / 512MiB     29.76%              1.07MB / 1.11MB     29.5MB / 213kB      23\n627634e2cd23        xena-summaxmocksite-v1-test.1.uh8v9r2yepsbdq4mfcts27t2q                                 0.00%               4.652MiB / 2GiB       0.23%               281kB / 725kB       8.19kB / 0B         2\n2db550e1f31d        codezoo-public-web-v1-production.1.n3js3xf8avq511zpwsdzlw64v                            0.00%               5.055MiB / 32MiB      15.80%              2.34MB / 2.04MB     1.26MB / 0B         2\nbe769c18d100        eg-nbl-onboardingfrontend-v1-production.1.j8drb639m1hrii0wvycjdmlvq                     0.00%               6.152MiB / 32MiB      19.23%              444kB / 1.87MB      4.52MB / 8.19kB     3\nb1425127ddeb        eg-giulia-services-signalr-v1-test.1.smm685n54hz9w87n5f8pdvyt0                          0.97%               69.7MiB / 256MiB      27.22%              8.74MB / 7.2MB      1.82MB / 0B         29\nde3a74e719d6        eg-gaia-services-ingress-v1-test.1.62x06sfrhe2hedl55yo3x51cl                            1.02%               53.27MiB / 2GiB       2.60%               3.88MB / 3.82MB     623kB / 0B          26\n5736687c3827        eg-one-payroll-api-v1-test.1.ka5atlimjxhogtsxp0gk2b6f8                                  0.55%               254.7MiB / 2GiB       12.44%              15.3MB / 4.02MB     105MB / 0B          27\n281e5c816764        eg-one-fiscalhelper-api-v1-production.2.chu2axoeu80wkcbka949rhjbu                       0.06%               205.3MiB / 2GiB       10.02%              25.4MB / 14.7MB     15.6MB / 16.4kB     21\n8ecfc17d7124        eg-emma-apigateway-v2-test.1.4dh5u50ob35h088dycwtfavl0                                  0.56%               129.4MiB / 256MiB     50.53%              274kB / 0B          31.2MB / 0B         23\n6fb65c0380d9        eg-byg-mail-v1-production.1.kxty1uatk8r9i8xzu4wvhapq4                                   0.00%               40.85MiB / 2GiB       1.99%               439kB / 45.7kB      7.75MB / 0B         16\ncd798045f74d        kong-gateway.1.kimumy5kzn4jtnluhalgrukpp                                                0.36%               303.4MiB / 23.53GiB   1.26%               11.8GB / 11.5GB     729kB / 98.3kB      5\nbbc3948e3e5f        health-aggregator-v1-production.1.ke6jg6keaydxvtvz6yj58ag2w                             0.22%               253.4MiB / 256MiB     99.00%              290MB / 299MB       1.86GB / 1.55GB     35\n584bd86deb69        kong-tool.1.2zjt4kg95j6p94d1wow4c5zth                                                   0.02%               156.6MiB / 23.53GiB   0.65%               643MB / 242MB       0B / 32.8kB         11\ne11e89702a19        logstash                                                                                2.92%               1.358GiB / 23.53GiB   5.77%               37.3GB / 103GB      186MB / 11.8MB      53\n. ",
    "Aschenbecher": "Same issue here on Ubuntu 18.04.1 4.15.0-43 . Cadvisor options and echo 2 > /proc/sys/vm/drop_caches doenst help for me :( Any updates for the Issue?. > @Aschenbecher then your high CPU load issue is not related to memory stats, you need to profile cadvisor using pprof in order to find what cause those high CPU loads.\n\n@zerthimon +1 comment are always useless, use github comments thumbs up feature. Please do not pollute issues with useless comments, Thanks\n\nthanks, I will take a look into it!. ",
    "zerthimon": "+1. ",
    "majst01": "I signed it!. I signed it! and added secondary email. Got your point and will make 2 distinct new PR. Thanks for review.. OK, will check. Testing locally works, might be a flaky test ?\n/retest. Hi David, i have gone through this test code and i have no clue why it does not work in the testing infrastructure, it works locally and on several other machines as well.. Hi @dashpole, thanks for letting tests run again, i am unsure how to proceed here. I think my PR is correct and solves big io issues here, but tests are failing on the google testing infra. I cannot reproduce this behavior by running all tests on different machines here.\nAny idea whats the reason why this tests are failing ?\nIf no idea pops up we should close this PR, what do you think ?\nGreetings\nStefan. invalid, fixed with: https://github.com/google/cadvisor/commit/816b8ff001bb86a730d5b8ca44488520b63c56f6. ",
    "abhi": "I signed it!. /test pull-cadvisor-e2e. /test pull-cadvisor-e2e. @Random-Liu Done. ready for review.. @dashpole thanks for the heads up. . @dashpole @Random-Liu thanks for reviewing. I have addressed the comments and rebased. \n@dashpole : also current docker master already uses containerd 1.0 beta. But we would still use the docker apis to collect stats for standalone cadvisor case ? . @Random-Liu yes thats right. We are currently covered by the namespace seperation. We can revisit for docker+containerd support. @dashpole yes the intention is to remove the code. It appears that the newer version of rkt doesnt require the tcp dialer.  I was relying on the way rkt client is being used in kubernetes/kubernetes.  I was having trouble running the test-runner locally and had to rely on CI to confirm the change. If the rest of the PR looks good , I can update the PR. if its in non host namespace then proc is located at /var/run/containerd/io.containerd.runtime.v1.linux/k8s.io/<containerid>/rootfs. This from what I verified during coding. Am I wrong here ? . I am going to get back on this. Running on a single machine doesnt seem to be an issue . Its only when the CI runs ssh and runs it that the timeout needs to be increased. Will come back with a valid explaination. Done. to avoid the retries by the dialer. So I set the timeout and use withblock and backoffdelay. . +1. ",
    "donghwicha": "Can someone please add to the document about how I can find GPU information on CAdvisor web UI? I can see GPU info on matrics but can not find on Web UI.. Hello @mindprince . Thanks for the clear answer! Then is it the only way to use metrics api for extracting gpu data per containers? I know that Cadvisor has RESTful API, and datastore like influxdb, and I would like to use those rather then using metrics.  . any update on this topic? . I already implemented it by myself. will send pr soon. . @pineking I'm sorry but I'm too busy with my project. Based on my experience, it shouldn't be hard to implement it and just coding following what's implemented was enough. . Will this be enough? I just changed number 1.6 to 1.7.\nhttps://github.com/google/cadvisor/pull/1867. I signed it!. ",
    "skuda": "The value of the label was the hostname of the node where the metric originated from, it was used to filter nodes in Grafana dashboards like the one I linked: https://grafana.com/dashboards/315\nThe alternative for that is to use the instance label after splitting the port, but it's nicer to see the hostname than the internal cluster network IP.\n. ",
    "Blasterdick": "+1 @skuda that same issue, and for an old ones GKE clusters that label was a must-have one . ",
    "shaileshpadave49": "@skuda Can you please provide some sample example how you used the alternative? \nInstead of kubernetes_io_hostname what can we used to see internal cluster network IP?. ",
    "onorua": "The same behavior experienced on version v0.28.0. / # cadvisor --port=8484 --v=10 --disable_metrics=disk -logtostderr --profiling=true\nI1109 20:58:43.500442      19 storagedriver.go:50] Caching stats in memory for 2m0s\nI1109 20:58:43.510434      19 manager.go:149] cAdvisor running in container: \"/sys/fs/cgroup/cpu,cpuacct\"\nW1109 20:58:43.571056      19 manager.go:161] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused\nW1109 20:58:43.571346      19 manager.go:172] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory\nI1109 20:58:43.637358      19 fs.go:139] Filesystem UUIDs: map[]\nI1109 20:58:43.637375      19 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev major:0 minor:327 fsType:tmpfs blockSize:0} /dev/sda5:{mountpoint:/var/lib/docker/overlay2 major:8 minor:5 fsType:ext4 blockSize:0} /dev/sda1:{mountpoint:/rootfs/boot major:8 minor:1 fsType:ext2 blockSize:0} shm:{mountpoint:/rootfs/var/lib/docker/containers/9561e6e1eba809d7c2f83cfc9d5726fdf03f18ecd9c4dd9e640a030bd606386b/shm major:0 minor:43 fsType:tmpfs blockSize:0}]\nI1109 20:58:43.640471      19 manager.go:223] Machine: {NumCores:8 CpuFrequency:3800000 MemoryCapacity:16783671296 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] MachineID:05208f172dfa47c58f6545e610fd9900 SystemUUID:00000000-0000-0000-0000-00259047C178 BootID:d48cdfa2-552c-4f8c-a295-5f6031023de8 Filesystems:[{Device:overlay DeviceMajor:0 DeviceMinor:138 Capacity:117417000960 Type:vfs Inodes:7290880 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:327 Capacity:67108864 Type:vfs Inodes:2048788 HasInodes:true} {Device:/dev/sda5 DeviceMajor:8 DeviceMinor:5 Capacity:117417000960 Type:vfs Inodes:7290880 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:494512128 Type:vfs Inodes:124928 HasInodes:true} {Device:shm DeviceMajor:0 DeviceMinor:43 Capacity:67108864 Type:vfs Inodes:2048788 HasInodes:true}] DiskMap:map[8:0:{Name:sda Major:8 Minor:0 Size:120034123776 Scheduler:deadline}] NetworkDevices:[{Name:dummy0 MacAddress:22:5d:da:c1:cb:2e Speed:0 Mtu:1500} {Name:dummy1 MacAddress:72:37:88:63:b0:d8 Speed:0 Mtu:1500} {Name:dummy2 MacAddress:da:6c:40:1a:65:02 Speed:0 Mtu:1500} {Name:dummy3 MacAddress:42:f3:3e:c1:6f:de Speed:0 Mtu:1500} {Name:dummy4 MacAddress:de:c8:ca:e2:33:12 Speed:0 Mtu:1500} {Name:dummy5 MacAddress:2e:96:b8:8c:25:71 Speed:0 Mtu:1500} {Name:dummy6 MacAddress:ce:cb:6d:e3:4e:45 Speed:0 Mtu:1500} {Name:dummy7 MacAddress:ce:73:3b:c2:9c:ef Speed:0 Mtu:1500} {Name:eno1 MacAddress:00:25:90:47:c1:78 Speed:1000 Mtu:1500} {Name:eno2 MacAddress:00:25:90:47:c1:79 Speed:0 Mtu:1500} {Name:ethsnitch MacAddress:d1:5f:32:bc Speed:0 Mtu:1480} {Name:hydra0 MacAddress: Speed:10 Mtu:1500} {Name:hydra1 MacAddress: Speed:10 Mtu:1500} {Name:hydra2 MacAddress: Speed:10 Mtu:1500} {Name:hydra3 MacAddress: Speed:10 Mtu:1500} {Name:hydra4 MacAddress: Speed:10 Mtu:1500} {Name:hydra5 MacAddress: Speed:10 Mtu:1500} {Name:hydra6 MacAddress: Speed:10 Mtu:1500} {Name:hydra7 MacAddress: Speed:10 Mtu:1500} {Name:trecm1 MacAddress: Speed:10 Mtu:1460} {Name:trecm2 MacAddress: Speed:10 Mtu:1460} {Name:trecm3 MacAddress: Speed:10 Mtu:1460} {Name:tregm1 MacAddress: Speed:10 Mtu:1460} {Name:tregm2 MacAddress: Speed:10 Mtu:1460} {Name:tregm3 MacAddress: Speed:10 Mtu:1460} {Name:tresm1 MacAddress: Speed:10 Mtu:1460} {Name:tresm2 MacAddress: Speed:10 Mtu:1460} {Name:tresm3 MacAddress: Speed:10 Mtu:1460} {Name:tun0 MacAddress: Speed:10 Mtu:1500} {Name:tun1 MacAddress: Speed:10 Mtu:1500} {Name:tunl0 MacAddress:00:00:00:00 Speed:0 Mtu:1480}] Topology:[{Id:0 Memory:16783671296 Cores:[{Id:0 Threads:[0 4] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[1 5] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:2 Threads:[2 6] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:3 Threads:[3 7] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:8388608 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}\nI1109 20:58:43.642587      19 manager.go:229] Version: {KernelVersion:4.4.0-53-generic ContainerOsVersion:Alpine Linux v3.4 DockerVersion:17.09.0-ce DockerAPIVersion:1.32 CadvisorVersion:v0.28.0 CadvisorRevision:3d2e7fc}\nI1109 20:58:43.697747      19 factory.go:355] Registering Docker factory\nW1109 20:58:43.697768      19 manager.go:273] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused\nW1109 20:58:43.697864      19 manager.go:284] Registration of the crio container factory failed: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory\nI1109 20:58:43.697875      19 factory.go:54] Registering systemd factory\nI1109 20:58:43.704128      19 factory.go:86] Registering Raw factory\nI1109 20:58:43.710648      19 manager.go:1161] Started watching for new ooms in manager\nI1109 20:58:43.711085      19 nvidia.go:47] No NVIDIA devices found.\nI1109 20:58:43.711120      19 factory.go:116] Factory \"docker\" was unable to handle container \"/\"\nI1109 20:58:43.711143      19 factory.go:105] Error trying to work out if we can handle /: / not handled by systemd handler\nI1109 20:58:43.711150      19 factory.go:116] Factory \"systemd\" was unable to handle container \"/\"\nI1109 20:58:43.711158      19 factory.go:112] Using factory \"raw\" for container \"/\"\nI1109 20:58:43.711438      19 manager.go:953] Added container: \"/\" (aliases: [], namespace: \"\")\nI1109 20:58:43.711625      19 handler.go:325] Added event &{/ 2017-11-09 14:02:29.866193364 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:43.711655      19 manager.go:322] Starting recovery of all containers\nI1109 20:58:43.711795      19 container.go:413] Start housekeeping for container \"/\"\nI1109 20:58:43.714232      19 handler.go:325] Added event &{/docker/44e9feee6fd1df24bac55d7f7277c1776a884111c8ece477ed3c002833162f8b 2017-11-09 20:57:06.879304674 +0000 UTC m=-96.568208101 oom {<nil>}}\nI1109 20:58:43.714291      19 manager.go:1181] Created an OOM event in container \"/docker/44e9feee6fd1df24bac55d7f7277c1776a884111c8ece477ed3c002833162f8b\" at 2017-11-09 20:57:06.879304674 +0000 UTC m=-96.568208101\nI1109 20:58:43.714306      19 handler.go:325] Added event &{/docker/44e9feee6fd1df24bac55d7f7277c1776a884111c8ece477ed3c002833162f8b 2017-11-09 20:57:06.879304674 +0000 UTC m=-96.568208101 oomKill {0xc4208c0360}}\nI1109 20:58:43.932461      19 factory.go:112] Using factory \"docker\" for container \"/docker/b15ecfee126191f21435d8daa55787ec86f626cf6f6cffe5a0815e9a0a41e375\"\nI1109 20:58:43.941671      19 manager.go:953] Added container: \"/docker/b15ecfee126191f21435d8daa55787ec86f626cf6f6cffe5a0815e9a0a41e375\" (aliases: [af-ulog-rid b15ecfee126191f21435d8daa55787ec86f626cf6f6cffe5a0815e9a0a41e375], namespace: \"docker\")\nI1109 20:58:43.941915      19 handler.go:325] Added event &{/docker/b15ecfee126191f21435d8daa55787ec86f626cf6f6cffe5a0815e9a0a41e375 2017-11-09 13:58:13.256272093 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:43.942065      19 container.go:413] Start housekeeping for container \"/docker/b15ecfee126191f21435d8daa55787ec86f626cf6f6cffe5a0815e9a0a41e375\"\nI1109 20:58:43.956297      19 factory.go:112] Using factory \"docker\" for container \"/docker/52662609b4433a7c31f6e37246091204cc5476612ea7f75d58ad7c15223eef99\"\nI1109 20:58:43.974260      19 manager.go:953] Added container: \"/docker/52662609b4433a7c31f6e37246091204cc5476612ea7f75d58ad7c15223eef99\" (aliases: [node-exporter 52662609b4433a7c31f6e37246091204cc5476612ea7f75d58ad7c15223eef99], namespace: \"docker\")\nI1109 20:58:43.974611      19 handler.go:325] Added event &{/docker/52662609b4433a7c31f6e37246091204cc5476612ea7f75d58ad7c15223eef99 2017-11-09 14:02:29.870193331 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:43.974718      19 container.go:413] Start housekeeping for container \"/docker/52662609b4433a7c31f6e37246091204cc5476612ea7f75d58ad7c15223eef99\"\nI1109 20:58:43.984241      19 factory.go:112] Using factory \"docker\" for container \"/docker/f34566c25010fe50e166df09e5527df14648412ae35420d018a25b33c9ea518a\"\nI1109 20:58:43.987299      19 manager.go:953] Added container: \"/docker/f34566c25010fe50e166df09e5527df14648412ae35420d018a25b33c9ea518a\" (aliases: [smtp-gated f34566c25010fe50e166df09e5527df14648412ae35420d018a25b33c9ea518a], namespace: \"docker\")\nI1109 20:58:43.987732      19 handler.go:325] Added event &{/docker/f34566c25010fe50e166df09e5527df14648412ae35420d018a25b33c9ea518a 2017-11-09 13:58:13.260272061 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:43.987798      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/networking.service\"\nI1109 20:58:43.987963      19 factory.go:105] Error trying to work out if we can handle /system.slice/networking.service: /system.slice/networking.service not handled by systemd handler\nI1109 20:58:43.988007      19 factory.go:116] Factory \"systemd\" was unable to handle container \"/system.slice/networking.service\"\nI1109 20:58:43.988038      19 factory.go:112] Using factory \"raw\" for container \"/system.slice/networking.service\"\nI1109 20:58:43.988487      19 manager.go:953] Added container: \"/system.slice/networking.service\" (aliases: [], namespace: \"\")\nI1109 20:58:43.988768      19 handler.go:325] Added event &{/system.slice/networking.service 2017-11-09 14:02:30.090191556 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:43.988822      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-containers-5a581b51727227f5b7d9c4fe54d53c25d1cc52b0711bba3e46e1346f58c5ef48-shm.mount\"\nI1109 20:58:43.989012      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-containers-5a581b51727227f5b7d9c4fe54d53c25d1cc52b0711bba3e46e1346f58c5ef48-shm.mount\", but ignoring.\nI1109 20:58:43.989059      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-containers-5a581b51727227f5b7d9c4fe54d53c25d1cc52b0711bba3e46e1346f58c5ef48-shm.mount\"\nI1109 20:58:43.989370      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-containers-8ccb25d7b137d4078ccaf907cc6c8f673b7a0ef294a78d84f806b780a54a9ad1-shm.mount\"\nI1109 20:58:43.989406      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-containers-8ccb25d7b137d4078ccaf907cc6c8f673b7a0ef294a78d84f806b780a54a9ad1-shm.mount\", but ignoring.\nI1109 20:58:43.989606      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-containers-8ccb25d7b137d4078ccaf907cc6c8f673b7a0ef294a78d84f806b780a54a9ad1-shm.mount\"\nI1109 20:58:43.989642      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-overlay2-b27d62f9f53ab40d8cbb19c1d25fda164d05fc157422a39c07aed584edb85e6c-merged.mount\"\nI1109 20:58:43.989830      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-overlay2-b27d62f9f53ab40d8cbb19c1d25fda164d05fc157422a39c07aed584edb85e6c-merged.mount\", but ignoring.\nI1109 20:58:43.989867      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-overlay2-b27d62f9f53ab40d8cbb19c1d25fda164d05fc157422a39c07aed584edb85e6c-merged.mount\"\nI1109 20:58:43.990698      19 container.go:413] Start housekeeping for container \"/docker/f34566c25010fe50e166df09e5527df14648412ae35420d018a25b33c9ea518a\"\nI1109 20:58:43.990962      19 container.go:413] Start housekeeping for container \"/system.slice/networking.service\"\nI1109 20:58:43.992827      19 factory.go:112] Using factory \"docker\" for container \"/docker/edb97b86be877fc5210161d04c80e70732930f722ecc612d2be16ebf45e48526\"\nI1109 20:58:43.994396      19 manager.go:953] Added container: \"/docker/edb97b86be877fc5210161d04c80e70732930f722ecc612d2be16ebf45e48526\" (aliases: [af-ulog-ugd edb97b86be877fc5210161d04c80e70732930f722ecc612d2be16ebf45e48526], namespace: \"docker\")\nI1109 20:58:43.994559      19 handler.go:325] Added event &{/docker/edb97b86be877fc5210161d04c80e70732930f722ecc612d2be16ebf45e48526 2017-11-09 13:58:13.260272061 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:43.994598      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-containers-6bfb6d7d77c9573be46a3f1dd25f81279101b3b7ff9a9024161c64309953c9aa-shm.mount\"\nI1109 20:58:43.994617      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-containers-6bfb6d7d77c9573be46a3f1dd25f81279101b3b7ff9a9024161c64309953c9aa-shm.mount\", but ignoring.\nI1109 20:58:43.994644      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-containers-6bfb6d7d77c9573be46a3f1dd25f81279101b3b7ff9a9024161c64309953c9aa-shm.mount\"\nI1109 20:58:43.994666      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/watchdog.service\"\nI1109 20:58:43.994679      19 factory.go:105] Error trying to work out if we can handle /system.slice/watchdog.service: /system.slice/watchdog.service not handled by systemd handler\nI1109 20:58:43.995232      19 factory.go:116] Factory \"systemd\" was unable to handle container \"/system.slice/watchdog.service\"\nI1109 20:58:43.995254      19 factory.go:112] Using factory \"raw\" for container \"/system.slice/watchdog.service\"\nI1109 20:58:43.995556      19 manager.go:953] Added container: \"/system.slice/watchdog.service\" (aliases: [], namespace: \"\")\nI1109 20:58:43.995682      19 handler.go:325] Added event &{/system.slice/watchdog.service 2017-11-09 13:58:12.740276291 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:43.996353      19 container.go:413] Start housekeeping for container \"/docker/edb97b86be877fc5210161d04c80e70732930f722ecc612d2be16ebf45e48526\"\nI1109 20:58:43.996734      19 container.go:413] Start housekeeping for container \"/system.slice/watchdog.service\"\nI1109 20:58:43.997670      19 factory.go:112] Using factory \"docker\" for container \"/docker/46880ae7ddad5650dac326fcb519e9f781e074ded81e397653073a8cbd533fa3\"\nI1109 20:58:43.999633      19 manager.go:953] Added container: \"/docker/46880ae7ddad5650dac326fcb519e9f781e074ded81e397653073a8cbd533fa3\" (aliases: [hydra6 46880ae7ddad5650dac326fcb519e9f781e074ded81e397653073a8cbd533fa3], namespace: \"docker\")\nI1109 20:58:44.000010      19 handler.go:325] Added event &{/docker/46880ae7ddad5650dac326fcb519e9f781e074ded81e397653073a8cbd533fa3 2017-11-09 14:02:29.870193331 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.000789      19 container.go:413] Start housekeeping for container \"/docker/46880ae7ddad5650dac326fcb519e9f781e074ded81e397653073a8cbd533fa3\"\nI1109 20:58:44.007074      19 factory.go:112] Using factory \"docker\" for container \"/docker/8473e9cf6b68df4e4a14fba5d7470d529424a0c57ef1e4ef74326be0a80c29c8\"\nI1109 20:58:44.010853      19 manager.go:953] Added container: \"/docker/8473e9cf6b68df4e4a14fba5d7470d529424a0c57ef1e4ef74326be0a80c29c8\" (aliases: [hydra4 8473e9cf6b68df4e4a14fba5d7470d529424a0c57ef1e4ef74326be0a80c29c8], namespace: \"docker\")\nI1109 20:58:44.011062      19 handler.go:325] Added event &{/docker/8473e9cf6b68df4e4a14fba5d7470d529424a0c57ef1e4ef74326be0a80c29c8 2017-11-09 14:02:29.874193299 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.011100      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-containers-940de92bc00b0f94e9a705f2e915da2cdcb2b6995d123015ac71c7c850d53464-shm.mount\"\nI1109 20:58:44.011127      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-containers-940de92bc00b0f94e9a705f2e915da2cdcb2b6995d123015ac71c7c850d53464-shm.mount\", but ignoring.\nI1109 20:58:44.011156      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-containers-940de92bc00b0f94e9a705f2e915da2cdcb2b6995d123015ac71c7c850d53464-shm.mount\"\nI1109 20:58:44.011557      19 container.go:413] Start housekeeping for container \"/docker/8473e9cf6b68df4e4a14fba5d7470d529424a0c57ef1e4ef74326be0a80c29c8\"\nI1109 20:58:44.013957      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-containers-cfac7cb5672211d7f87ac25269a369c6eeca6a2d1c9caee34989ef1e8976822a-shm.mount\"\nI1109 20:58:44.013997      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-containers-cfac7cb5672211d7f87ac25269a369c6eeca6a2d1c9caee34989ef1e8976822a-shm.mount\", but ignoring.\nI1109 20:58:44.014015      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-containers-cfac7cb5672211d7f87ac25269a369c6eeca6a2d1c9caee34989ef1e8976822a-shm.mount\"\nI1109 20:58:44.014029      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-containers-f1a573ece6fd2e932825dfda4781ef5af36615fe2a168a4b00bb79393f69758d-shm.mount\"\nI1109 20:58:44.014041      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-containers-f1a573ece6fd2e932825dfda4781ef5af36615fe2a168a4b00bb79393f69758d-shm.mount\", but ignoring.\nI1109 20:58:44.014053      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-containers-f1a573ece6fd2e932825dfda4781ef5af36615fe2a168a4b00bb79393f69758d-shm.mount\"\nI1109 20:58:44.014065      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-overlay2-9e04a2d8a81d689830cc5497b6910d20cbdbb7da2b3fd566e5204a1a8010d4d4-merged.mount\"\nI1109 20:58:44.014077      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-overlay2-9e04a2d8a81d689830cc5497b6910d20cbdbb7da2b3fd566e5204a1a8010d4d4-merged.mount\", but ignoring.\nI1109 20:58:44.014090      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-overlay2-9e04a2d8a81d689830cc5497b6910d20cbdbb7da2b3fd566e5204a1a8010d4d4-merged.mount\"\nI1109 20:58:44.014103      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-overlay2-ee0e62db85e98335c44fb1303bda83c3f8e1e0a57bcb8629177c4dd7632bab56-merged.mount\"\nI1109 20:58:44.014116      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-overlay2-ee0e62db85e98335c44fb1303bda83c3f8e1e0a57bcb8629177c4dd7632bab56-merged.mount\", but ignoring.\nI1109 20:58:44.014129      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-overlay2-ee0e62db85e98335c44fb1303bda83c3f8e1e0a57bcb8629177c4dd7632bab56-merged.mount\"\nI1109 20:58:44.014141      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-overlay2-f46c007ccf42b82af47578e544f0f75be4eef569d7aadb1ccd4daf75a3db65ec-merged.mount\"\nI1109 20:58:44.014153      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-overlay2-f46c007ccf42b82af47578e544f0f75be4eef569d7aadb1ccd4daf75a3db65ec-merged.mount\", but ignoring.\nI1109 20:58:44.014166      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-overlay2-f46c007ccf42b82af47578e544f0f75be4eef569d7aadb1ccd4daf75a3db65ec-merged.mount\"\nI1109 20:58:44.014196      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/dev-disk-by\\\\x2did-wwn\\\\x2d0x55cd2e414d4a27cc\\\\x2dpart6.swap\"\nI1109 20:58:44.014214      19 factory.go:105] Error trying to work out if we can handle /system.slice/dev-disk-by\\x2did-wwn\\x2d0x55cd2e414d4a27cc\\x2dpart6.swap: /system.slice/dev-disk-by\\x2did-wwn\\x2d0x55cd2e414d4a27cc\\x2dpart6.swap not handled by systemd handler\nI1109 20:58:44.014244      19 factory.go:116] Factory \"systemd\" was unable to handle container \"/system.slice/dev-disk-by\\\\x2did-wwn\\\\x2d0x55cd2e414d4a27cc\\\\x2dpart6.swap\"\nI1109 20:58:44.014263      19 factory.go:112] Using factory \"raw\" for container \"/system.slice/dev-disk-by\\\\x2did-wwn\\\\x2d0x55cd2e414d4a27cc\\\\x2dpart6.swap\"\nW1109 20:58:44.015069      19 container.go:358] Failed to create summary reader for \"/system.slice/dev-disk-by\\\\x2did-wwn\\\\x2d0x55cd2e414d4a27cc\\\\x2dpart6.swap\": none of the resources are being tracked.\nI1109 20:58:44.015096      19 manager.go:953] Added container: \"/system.slice/dev-disk-by\\\\x2did-wwn\\\\x2d0x55cd2e414d4a27cc\\\\x2dpart6.swap\" (aliases: [], namespace: \"\")\nI1109 20:58:44.015168      19 handler.go:325] Added event &{/system.slice/dev-disk-by\\x2did-wwn\\x2d0x55cd2e414d4a27cc\\x2dpart6.swap 2017-11-09 14:32:16.285804448 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.015471      19 container.go:413] Start housekeeping for container \"/system.slice/dev-disk-by\\\\x2did-wwn\\\\x2d0x55cd2e414d4a27cc\\\\x2dpart6.swap\"\nI1109 20:58:44.016800      19 factory.go:112] Using factory \"docker\" for container \"/docker/733b89d52d91766011e68690312de2587ca5dc14d1e28836aa27b2618478856c\"\nI1109 20:58:44.046828      19 manager.go:953] Added container: \"/docker/733b89d52d91766011e68690312de2587ca5dc14d1e28836aa27b2618478856c\" (aliases: [cron-afconfig-cats 733b89d52d91766011e68690312de2587ca5dc14d1e28836aa27b2618478856c], namespace: \"docker\")\nI1109 20:58:44.047563      19 handler.go:325] Added event &{/docker/733b89d52d91766011e68690312de2587ca5dc14d1e28836aa27b2618478856c 2017-11-09 14:02:29.874193299 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.048327      19 container.go:413] Start housekeeping for container \"/docker/733b89d52d91766011e68690312de2587ca5dc14d1e28836aa27b2618478856c\"\nI1109 20:58:44.083681      19 factory.go:112] Using factory \"docker\" for container \"/docker/976f62d59b70762366763a1790338a8c3af0d7e7d4e957d3c6e5508e8f86b1fe\"\nI1109 20:58:44.092240      19 manager.go:953] Added container: \"/docker/976f62d59b70762366763a1790338a8c3af0d7e7d4e957d3c6e5508e8f86b1fe\" (aliases: [af-reverse-goodmail3 976f62d59b70762366763a1790338a8c3af0d7e7d4e957d3c6e5508e8f86b1fe], namespace: \"docker\")\nI1109 20:58:44.092423      19 handler.go:325] Added event &{/docker/976f62d59b70762366763a1790338a8c3af0d7e7d4e957d3c6e5508e8f86b1fe 2017-11-09 19:42:30.586179149 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.092970      19 container.go:413] Start housekeeping for container \"/docker/976f62d59b70762366763a1790338a8c3af0d7e7d4e957d3c6e5508e8f86b1fe\"\nI1109 20:58:44.094721      19 factory.go:112] Using factory \"docker\" for container \"/docker/f47940762da5fa709526db29066ead21b0c6f8f167ea335d5542c4159bc28cea\"\nI1109 20:58:44.128132      19 manager.go:953] Added container: \"/docker/f47940762da5fa709526db29066ead21b0c6f8f167ea335d5542c4159bc28cea\" (aliases: [afmon f47940762da5fa709526db29066ead21b0c6f8f167ea335d5542c4159bc28cea], namespace: \"docker\")\nI1109 20:58:44.128325      19 handler.go:325] Added event &{/docker/f47940762da5fa709526db29066ead21b0c6f8f167ea335d5542c4159bc28cea 2017-11-09 13:58:13.260272061 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.128359      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/-.mount\"\nI1109 20:58:44.128368      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/-.mount\", but ignoring.\nI1109 20:58:44.128388      19 manager.go:913] ignoring container \"/system.slice/-.mount\"\nI1109 20:58:44.128403      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/systemd-logind.service\"\nI1109 20:58:44.128412      19 factory.go:105] Error trying to work out if we can handle /system.slice/systemd-logind.service: /system.slice/systemd-logind.service not handled by systemd handler\nI1109 20:58:44.128417      19 factory.go:116] Factory \"systemd\" was unable to handle container \"/system.slice/systemd-logind.service\"\nI1109 20:58:44.128440      19 factory.go:112] Using factory \"raw\" for container \"/system.slice/systemd-logind.service\"\nI1109 20:58:44.143619      19 container.go:413] Start housekeeping for container \"/docker/f47940762da5fa709526db29066ead21b0c6f8f167ea335d5542c4159bc28cea\"\nI1109 20:58:44.201883      19 manager.go:953] Added container: \"/system.slice/systemd-logind.service\" (aliases: [], namespace: \"\")\nI1109 20:58:44.218064      19 handler.go:325] Added event &{/system.slice/systemd-logind.service 2017-11-09 14:32:16.28980442 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.218118      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-overlay2-59f487a0969bba8a8d7796b745d1318c29be2137d9709e06d21287a7c1a015aa-merged.mount\"\nI1109 20:58:44.218135      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-overlay2-59f487a0969bba8a8d7796b745d1318c29be2137d9709e06d21287a7c1a015aa-merged.mount\", but ignoring.\nI1109 20:58:44.218147      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-overlay2-59f487a0969bba8a8d7796b745d1318c29be2137d9709e06d21287a7c1a015aa-merged.mount\"\nI1109 20:58:44.218155      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-overlay2-84804a3ff85447b590a3c1a372f5dca8139b06bb6cfe631452333ec585684f30-merged.mount\"\nI1109 20:58:44.218163      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-overlay2-84804a3ff85447b590a3c1a372f5dca8139b06bb6cfe631452333ec585684f30-merged.mount\", but ignoring.\nI1109 20:58:44.218172      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-overlay2-84804a3ff85447b590a3c1a372f5dca8139b06bb6cfe631452333ec585684f30-merged.mount\"\nI1109 20:58:44.218360      19 container.go:413] Start housekeeping for container \"/system.slice/systemd-logind.service\"\nI1109 20:58:44.235804      19 factory.go:112] Using factory \"docker\" for container \"/docker/2ae16917e9a521eeff041f194b1dd0b0f1417b7cdedbb3daca20e4482f02b375\"\nI1109 20:58:44.239337      19 manager.go:953] Added container: \"/docker/2ae16917e9a521eeff041f194b1dd0b0f1417b7cdedbb3daca20e4482f02b375\" (aliases: [af-reverse-suspectmail1 2ae16917e9a521eeff041f194b1dd0b0f1417b7cdedbb3daca20e4482f02b375], namespace: \"docker\")\nI1109 20:58:44.247469      19 handler.go:325] Added event &{/docker/2ae16917e9a521eeff041f194b1dd0b0f1417b7cdedbb3daca20e4482f02b375 2017-11-09 19:42:30.582179182 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.247620      19 container.go:413] Start housekeeping for container \"/docker/2ae16917e9a521eeff041f194b1dd0b0f1417b7cdedbb3daca20e4482f02b375\"\nI1109 20:58:44.254414      19 factory.go:112] Using factory \"docker\" for container \"/docker/b305a35287caf1bc9ba57975bb839b9eb096902669f0446df24772118bc9796a\"\nI1109 20:58:44.273325      19 manager.go:953] Added container: \"/docker/b305a35287caf1bc9ba57975bb839b9eb096902669f0446df24772118bc9796a\" (aliases: [cron-hssunified b305a35287caf1bc9ba57975bb839b9eb096902669f0446df24772118bc9796a], namespace: \"docker\")\nI1109 20:58:44.273618      19 handler.go:325] Added event &{/docker/b305a35287caf1bc9ba57975bb839b9eb096902669f0446df24772118bc9796a 2017-11-09 13:58:13.256272093 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.273646      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-containers-7163a4366470c7bba3a6f2dc02e022ef0152e84507b1613e49f8a7ac4e3f9d5b-shm.mount\"\nI1109 20:58:44.273658      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-containers-7163a4366470c7bba3a6f2dc02e022ef0152e84507b1613e49f8a7ac4e3f9d5b-shm.mount\", but ignoring.\nI1109 20:58:44.273670      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-containers-7163a4366470c7bba3a6f2dc02e022ef0152e84507b1613e49f8a7ac4e3f9d5b-shm.mount\"\nI1109 20:58:44.273679      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-overlay2-875cf23a095584084a758e1be5b42d8db4640bd1942484934ce0da94b2fddbd4-merged.mount\"\nI1109 20:58:44.273687      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-overlay2-875cf23a095584084a758e1be5b42d8db4640bd1942484934ce0da94b2fddbd4-merged.mount\", but ignoring.\nI1109 20:58:44.273695      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-overlay2-875cf23a095584084a758e1be5b42d8db4640bd1942484934ce0da94b2fddbd4-merged.mount\"\nI1109 20:58:44.273703      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-overlay2-deebccffe9f3d9a251637733e2f7fcbd0ec3a16280f404e0f5db1efa8326f31a-merged.mount\"\nI1109 20:58:44.273711      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-overlay2-deebccffe9f3d9a251637733e2f7fcbd0ec3a16280f404e0f5db1efa8326f31a-merged.mount\", but ignoring.\nI1109 20:58:44.273720      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-overlay2-deebccffe9f3d9a251637733e2f7fcbd0ec3a16280f404e0f5db1efa8326f31a-merged.mount\"\nI1109 20:58:44.276077      19 container.go:413] Start housekeeping for container \"/docker/b305a35287caf1bc9ba57975bb839b9eb096902669f0446df24772118bc9796a\"\nI1109 20:58:44.279664      19 factory.go:112] Using factory \"docker\" for container \"/docker/96100f49b3540a601e97b09bec15649e69a7bf133bced6638148734ae966aa93\"\nI1109 20:58:44.290431      19 manager.go:953] Added container: \"/docker/96100f49b3540a601e97b09bec15649e69a7bf133bced6638148734ae966aa93\" (aliases: [hydra0 96100f49b3540a601e97b09bec15649e69a7bf133bced6638148734ae966aa93], namespace: \"docker\")\nI1109 20:58:44.291347      19 handler.go:325] Added event &{/docker/96100f49b3540a601e97b09bec15649e69a7bf133bced6638148734ae966aa93 2017-11-09 13:58:13.196272581 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.291385      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/monit.service\"\nI1109 20:58:44.291395      19 factory.go:105] Error trying to work out if we can handle /system.slice/monit.service: /system.slice/monit.service not handled by systemd handler\nI1109 20:58:44.291402      19 factory.go:116] Factory \"systemd\" was unable to handle container \"/system.slice/monit.service\"\nI1109 20:58:44.291408      19 factory.go:112] Using factory \"raw\" for container \"/system.slice/monit.service\"\nI1109 20:58:44.291904      19 container.go:413] Start housekeeping for container \"/docker/96100f49b3540a601e97b09bec15649e69a7bf133bced6638148734ae966aa93\"\nI1109 20:58:44.302169      19 manager.go:953] Added container: \"/system.slice/monit.service\" (aliases: [], namespace: \"\")\nI1109 20:58:44.302522      19 handler.go:325] Added event &{/system.slice/monit.service 2017-11-09 14:02:30.090191556 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.302564      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/system-systemd\\\\x2dfsck.slice\"\nI1109 20:58:44.302574      19 factory.go:105] Error trying to work out if we can handle /system.slice/system-systemd\\x2dfsck.slice: /system.slice/system-systemd\\x2dfsck.slice not handled by systemd handler\nI1109 20:58:44.302582      19 factory.go:116] Factory \"systemd\" was unable to handle container \"/system.slice/system-systemd\\\\x2dfsck.slice\"\nI1109 20:58:44.302591      19 factory.go:112] Using factory \"raw\" for container \"/system.slice/system-systemd\\\\x2dfsck.slice\"\nI1109 20:58:44.302851      19 manager.go:953] Added container: \"/system.slice/system-systemd\\\\x2dfsck.slice\" (aliases: [], namespace: \"\")\nI1109 20:58:44.303022      19 handler.go:325] Added event &{/system.slice/system-systemd\\x2dfsck.slice 2017-11-09 14:32:16.293804392 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.303043      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-containers-8473e9cf6b68df4e4a14fba5d7470d529424a0c57ef1e4ef74326be0a80c29c8-shm.mount\"\nI1109 20:58:44.303101      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-containers-8473e9cf6b68df4e4a14fba5d7470d529424a0c57ef1e4ef74326be0a80c29c8-shm.mount\", but ignoring.\nI1109 20:58:44.303112      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-containers-8473e9cf6b68df4e4a14fba5d7470d529424a0c57ef1e4ef74326be0a80c29c8-shm.mount\"\nI1109 20:58:44.303121      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-containers-c627d8e0eaaeaaa6a0fe8a425df1249b3b84886c5ba26ebf066773952186b9a8-shm.mount\"\nI1109 20:58:44.303129      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-containers-c627d8e0eaaeaaa6a0fe8a425df1249b3b84886c5ba26ebf066773952186b9a8-shm.mount\", but ignoring.\nI1109 20:58:44.303140      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-containers-c627d8e0eaaeaaa6a0fe8a425df1249b3b84886c5ba26ebf066773952186b9a8-shm.mount\"\nI1109 20:58:44.303147      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-overlay2-0b2923a4c2013e9df0ddabbd5b3a9900d1cda7ea27f4d1b6d4690e51148af06c-merged.mount\"\nI1109 20:58:44.303155      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-overlay2-0b2923a4c2013e9df0ddabbd5b3a9900d1cda7ea27f4d1b6d4690e51148af06c-merged.mount\", but ignoring.\nI1109 20:58:44.303163      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-overlay2-0b2923a4c2013e9df0ddabbd5b3a9900d1cda7ea27f4d1b6d4690e51148af06c-merged.mount\"\nI1109 20:58:44.303171      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-overlay2-b72ab418c8daec4784a433b83bee277fa5e116fd36401695cb446f1ba473f3ae-merged.mount\"\nI1109 20:58:44.303178      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-overlay2-b72ab418c8daec4784a433b83bee277fa5e116fd36401695cb446f1ba473f3ae-merged.mount\", but ignoring.\nI1109 20:58:44.303187      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-overlay2-b72ab418c8daec4784a433b83bee277fa5e116fd36401695cb446f1ba473f3ae-merged.mount\"\nI1109 20:58:44.303291      19 container.go:413] Start housekeeping for container \"/system.slice/monit.service\"\nI1109 20:58:44.303875      19 container.go:413] Start housekeeping for container \"/system.slice/system-systemd\\\\x2dfsck.slice\"\nI1109 20:58:44.322249      19 factory.go:112] Using factory \"docker\" for container \"/docker/39fa22b6dd05cfd8b9bf8787d54a05daf798654d251443d107050178771a4b84\"\nI1109 20:58:44.362965      19 manager.go:953] Added container: \"/docker/39fa22b6dd05cfd8b9bf8787d54a05daf798654d251443d107050178771a4b84\" (aliases: [af-messenger-adapter 39fa22b6dd05cfd8b9bf8787d54a05daf798654d251443d107050178771a4b84], namespace: \"docker\")\nI1109 20:58:44.363186      19 handler.go:325] Added event &{/docker/39fa22b6dd05cfd8b9bf8787d54a05daf798654d251443d107050178771a4b84 2017-11-09 14:02:29.870193331 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.364193      19 container.go:413] Start housekeeping for container \"/docker/39fa22b6dd05cfd8b9bf8787d54a05daf798654d251443d107050178771a4b84\"\nI1109 20:58:44.390969      19 factory.go:112] Using factory \"docker\" for container \"/docker/f1a573ece6fd2e932825dfda4781ef5af36615fe2a168a4b00bb79393f69758d\"\nI1109 20:58:44.415444      19 manager.go:953] Added container: \"/docker/f1a573ece6fd2e932825dfda4781ef5af36615fe2a168a4b00bb79393f69758d\" (aliases: [hydra2 f1a573ece6fd2e932825dfda4781ef5af36615fe2a168a4b00bb79393f69758d], namespace: \"docker\")\nI1109 20:58:44.437345      19 handler.go:325] Added event &{/docker/f1a573ece6fd2e932825dfda4781ef5af36615fe2a168a4b00bb79393f69758d 2017-11-09 13:58:13.260272061 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.438013      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-containers-e28de26ad0a100d562d9d2de6dbfbb45e47f2994d7b93a6d3f543fe75c9655cd-shm.mount\"\nI1109 20:58:44.438395      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-containers-e28de26ad0a100d562d9d2de6dbfbb45e47f2994d7b93a6d3f543fe75c9655cd-shm.mount\", but ignoring.\nI1109 20:58:44.438728      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-containers-e28de26ad0a100d562d9d2de6dbfbb45e47f2994d7b93a6d3f543fe75c9655cd-shm.mount\"\nI1109 20:58:44.438933      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/dev-hugepages.mount\"\nI1109 20:58:44.439333      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/dev-hugepages.mount\", but ignoring.\nI1109 20:58:44.439525      19 manager.go:913] ignoring container \"/system.slice/dev-hugepages.mount\"\nI1109 20:58:44.439927      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/systemd-modules-load.service\"\nI1109 20:58:44.440227      19 factory.go:105] Error trying to work out if we can handle /system.slice/systemd-modules-load.service: /system.slice/systemd-modules-load.service not handled by systemd handler\nI1109 20:58:44.440680      19 factory.go:116] Factory \"systemd\" was unable to handle container \"/system.slice/systemd-modules-load.service\"\nI1109 20:58:44.440862      19 factory.go:112] Using factory \"raw\" for container \"/system.slice/systemd-modules-load.service\"\nI1109 20:58:44.441379      19 manager.go:953] Added container: \"/system.slice/systemd-modules-load.service\" (aliases: [], namespace: \"\")\nI1109 20:58:44.441794      19 handler.go:325] Added event &{/system.slice/systemd-modules-load.service 2017-11-09 14:32:16.281804476 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.441865      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/unbound.service\"\nI1109 20:58:44.442410      19 factory.go:105] Error trying to work out if we can handle /system.slice/unbound.service: /system.slice/unbound.service not handled by systemd handler\nI1109 20:58:44.442461      19 factory.go:116] Factory \"systemd\" was unable to handle container \"/system.slice/unbound.service\"\nI1109 20:58:44.442490      19 factory.go:112] Using factory \"raw\" for container \"/system.slice/unbound.service\"\nI1109 20:58:44.443158      19 manager.go:953] Added container: \"/system.slice/unbound.service\" (aliases: [], namespace: \"\")\nI1109 20:58:44.443608      19 handler.go:325] Added event &{/system.slice/unbound.service 2017-11-09 14:32:16.28980442 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.443678      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/def-net-dev.service\"\nI1109 20:58:44.443906      19 factory.go:105] Error trying to work out if we can handle /system.slice/def-net-dev.service: /system.slice/def-net-dev.service not handled by systemd handler\nI1109 20:58:44.443933      19 factory.go:116] Factory \"systemd\" was unable to handle container \"/system.slice/def-net-dev.service\"\nI1109 20:58:44.444238      19 factory.go:112] Using factory \"raw\" for container \"/system.slice/def-net-dev.service\"\nI1109 20:58:44.444694      19 manager.go:953] Added container: \"/system.slice/def-net-dev.service\" (aliases: [], namespace: \"\")\nI1109 20:58:44.438643      19 container.go:413] Start housekeeping for container \"/docker/f1a573ece6fd2e932825dfda4781ef5af36615fe2a168a4b00bb79393f69758d\"\nI1109 20:58:44.445168      19 handler.go:325] Added event &{/system.slice/def-net-dev.service 2017-11-09 14:02:30.090191556 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.450886      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/resolvconf.service\"\nI1109 20:58:44.450919      19 factory.go:105] Error trying to work out if we can handle /system.slice/resolvconf.service: /system.slice/resolvconf.service not handled by systemd handler\nI1109 20:58:44.450931      19 factory.go:116] Factory \"systemd\" was unable to handle container \"/system.slice/resolvconf.service\"\nI1109 20:58:44.450962      19 factory.go:112] Using factory \"raw\" for container \"/system.slice/resolvconf.service\"\nI1109 20:58:44.451764      19 manager.go:953] Added container: \"/system.slice/resolvconf.service\" (aliases: [], namespace: \"\")\nI1109 20:58:44.453598      19 handler.go:325] Added event &{/system.slice/resolvconf.service 2017-11-09 14:02:30.094191523 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.451991      19 container.go:413] Start housekeeping for container \"/system.slice/def-net-dev.service\"\nI1109 20:58:44.454533      19 container.go:413] Start housekeeping for container \"/system.slice/resolvconf.service\"\nI1109 20:58:44.445262      19 container.go:413] Start housekeeping for container \"/system.slice/unbound.service\"\nI1109 20:58:44.445190      19 container.go:413] Start housekeeping for container \"/system.slice/systemd-modules-load.service\"\nI1109 20:58:44.532008      19 factory.go:112] Using factory \"docker\" for container \"/docker/05d35d788cc10ba8f2c5aaa097a61fc2afd17f6831e4ac6310506e7f61327bec\"\nI1109 20:58:44.632949      19 manager.go:953] Added container: \"/docker/05d35d788cc10ba8f2c5aaa097a61fc2afd17f6831e4ac6310506e7f61327bec\" (aliases: [vanilla-nginx 05d35d788cc10ba8f2c5aaa097a61fc2afd17f6831e4ac6310506e7f61327bec], namespace: \"docker\")\nI1109 20:58:44.633147      19 handler.go:325] Added event &{/docker/05d35d788cc10ba8f2c5aaa097a61fc2afd17f6831e4ac6310506e7f61327bec 2017-11-09 14:02:29.870193331 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.633495      19 container.go:413] Start housekeeping for container \"/docker/05d35d788cc10ba8f2c5aaa097a61fc2afd17f6831e4ac6310506e7f61327bec\"\nI1109 20:58:44.640926      19 factory.go:112] Using factory \"docker\" for container \"/docker/860e4f12b57b54db97e9132a9b0c723370fda577cbd90172970f9d9e1045fb84\"\nI1109 20:58:44.643889      19 manager.go:953] Added container: \"/docker/860e4f12b57b54db97e9132a9b0c723370fda577cbd90172970f9d9e1045fb84\" (aliases: [af-policyd 860e4f12b57b54db97e9132a9b0c723370fda577cbd90172970f9d9e1045fb84], namespace: \"docker\")\nI1109 20:58:44.644333      19 handler.go:325] Added event &{/docker/860e4f12b57b54db97e9132a9b0c723370fda577cbd90172970f9d9e1045fb84 2017-11-09 19:42:30.586179149 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.644410      19 factory.go:116] Factory \"docker\" was unable to handle container \"/init.scope\"\nI1109 20:58:44.644577      19 factory.go:105] Error trying to work out if we can handle /init.scope: /init.scope not handled by systemd handler\nI1109 20:58:44.644615      19 factory.go:116] Factory \"systemd\" was unable to handle container \"/init.scope\"\nI1109 20:58:44.644644      19 factory.go:112] Using factory \"raw\" for container \"/init.scope\"\nI1109 20:58:44.645041      19 manager.go:953] Added container: \"/init.scope\" (aliases: [], namespace: \"\")\nI1109 20:58:44.645303      19 handler.go:325] Added event &{/init.scope 2017-11-09 14:02:29.878193267 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.645364      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/systemd-tmpfiles-setup.service\"\nI1109 20:58:44.645512      19 factory.go:105] Error trying to work out if we can handle /system.slice/systemd-tmpfiles-setup.service: /system.slice/systemd-tmpfiles-setup.service not handled by systemd handler\nI1109 20:58:44.645548      19 factory.go:116] Factory \"systemd\" was unable to handle container \"/system.slice/systemd-tmpfiles-setup.service\"\nI1109 20:58:44.645573      19 factory.go:112] Using factory \"raw\" for container \"/system.slice/systemd-tmpfiles-setup.service\"\nI1109 20:58:44.663059      19 manager.go:953] Added container: \"/system.slice/systemd-tmpfiles-setup.service\" (aliases: [], namespace: \"\")\nI1109 20:58:44.663191      19 handler.go:325] Added event &{/system.slice/systemd-tmpfiles-setup.service 2017-11-09 14:32:16.285804448 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.663217      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-containers-e21241206f1d80dab2250aab19103d9e65e587b1cdccd60cab840027a6f94088-shm.mount\"\nI1109 20:58:44.663231      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-containers-e21241206f1d80dab2250aab19103d9e65e587b1cdccd60cab840027a6f94088-shm.mount\", but ignoring.\nI1109 20:58:44.663251      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-containers-e21241206f1d80dab2250aab19103d9e65e587b1cdccd60cab840027a6f94088-shm.mount\"\nI1109 20:58:44.663332      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-overlay2-9702d4f42e1cdc0bad51152011cef39f81a9d7358232ac6bdaeb5d6ac45cdbac-merged.mount\"\nI1109 20:58:44.663342      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-overlay2-9702d4f42e1cdc0bad51152011cef39f81a9d7358232ac6bdaeb5d6ac45cdbac-merged.mount\", but ignoring.\nI1109 20:58:44.663351      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-overlay2-9702d4f42e1cdc0bad51152011cef39f81a9d7358232ac6bdaeb5d6ac45cdbac-merged.mount\"\nI1109 20:58:44.663359      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-overlay2-bef7894609e3cbb197541b3cea3b3afb8c67db1cb68f2f29d24927a8d8bd52c7-merged.mount\"\nI1109 20:58:44.663367      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-overlay2-bef7894609e3cbb197541b3cea3b3afb8c67db1cb68f2f29d24927a8d8bd52c7-merged.mount\", but ignoring.\nI1109 20:58:44.663376      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-overlay2-bef7894609e3cbb197541b3cea3b3afb8c67db1cb68f2f29d24927a8d8bd52c7-merged.mount\"\nI1109 20:58:44.664006      19 container.go:413] Start housekeeping for container \"/init.scope\"\nI1109 20:58:44.664387      19 container.go:413] Start housekeeping for container \"/system.slice/systemd-tmpfiles-setup.service\"\nI1109 20:58:44.646641      19 container.go:413] Start housekeeping for container \"/docker/860e4f12b57b54db97e9132a9b0c723370fda577cbd90172970f9d9e1045fb84\"\nI1109 20:58:44.723866      19 factory.go:112] Using factory \"docker\" for container \"/docker/269af41ea7e073bc0f5dd72d951ff7966cae67208c8d2da67e5200ad5a0d76d6\"\nI1109 20:58:44.882950      19 manager.go:953] Added container: \"/docker/269af41ea7e073bc0f5dd72d951ff7966cae67208c8d2da67e5200ad5a0d76d6\" (aliases: [sniproxy 269af41ea7e073bc0f5dd72d951ff7966cae67208c8d2da67e5200ad5a0d76d6], namespace: \"docker\")\nI1109 20:58:44.883119      19 handler.go:325] Added event &{/docker/269af41ea7e073bc0f5dd72d951ff7966cae67208c8d2da67e5200ad5a0d76d6 2017-11-09 19:42:30.578179214 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:44.883771      19 container.go:413] Start housekeeping for container \"/docker/269af41ea7e073bc0f5dd72d951ff7966cae67208c8d2da67e5200ad5a0d76d6\"\nI1109 20:58:44.896578      19 factory.go:112] Using factory \"docker\" for container \"/docker/8ccb25d7b137d4078ccaf907cc6c8f673b7a0ef294a78d84f806b780a54a9ad1\"\nI1109 20:58:44.947742      19 manager.go:953] Added container: \"/docker/8ccb25d7b137d4078ccaf907cc6c8f673b7a0ef294a78d84f806b780a54a9ad1\" (aliases: [af-collectd 8ccb25d7b137d4078ccaf907cc6c8f673b7a0ef294a78d84f806b780a54a9ad1], namespace: \"docker\")\nI1109 20:58:45.070162      19 handler.go:325] Added event &{/docker/8ccb25d7b137d4078ccaf907cc6c8f673b7a0ef294a78d84f806b780a54a9ad1 2017-11-09 14:02:29.874193299 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:45.166475      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-containers-1b16ab538dd86ae4b420929f9cb6db79706bf40081e9600f817f3a11121ab2af-shm.mount\"\nI1109 20:58:45.166514      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-containers-1b16ab538dd86ae4b420929f9cb6db79706bf40081e9600f817f3a11121ab2af-shm.mount\", but ignoring.\nI1109 20:58:45.166541      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-containers-1b16ab538dd86ae4b420929f9cb6db79706bf40081e9600f817f3a11121ab2af-shm.mount\"\nI1109 20:58:45.166561      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-containers-ced9c1c8a3fd8c3c7156a2aff17223e05ac633a969b2a66bc038341a57f9c6cc-shm.mount\"\nI1109 20:58:45.166580      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-containers-ced9c1c8a3fd8c3c7156a2aff17223e05ac633a969b2a66bc038341a57f9c6cc-shm.mount\", but ignoring.\nI1109 20:58:45.166599      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-containers-ced9c1c8a3fd8c3c7156a2aff17223e05ac633a969b2a66bc038341a57f9c6cc-shm.mount\"\nI1109 20:58:45.166762      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/var-lib-docker-overlay2-a02a905509a9c87f959148b41ea4106c3127a9963c0cff9ba3b30b1e3d9568bf-merged.mount\"\nI1109 20:58:45.166840      19 factory.go:109] Factory \"systemd\" can handle container \"/system.slice/var-lib-docker-overlay2-a02a905509a9c87f959148b41ea4106c3127a9963c0cff9ba3b30b1e3d9568bf-merged.mount\", but ignoring.\nI1109 20:58:45.166859      19 manager.go:913] ignoring container \"/system.slice/var-lib-docker-overlay2-a02a905509a9c87f959148b41ea4106c3127a9963c0cff9ba3b30b1e3d9568bf-merged.mount\"\nI1109 20:58:45.166890      19 factory.go:116] Factory \"docker\" was unable to handle container \"/system.slice/dev-disk-by\\\\x2did-ata\\\\x2dINTEL_SSDSC2BB120G6_PHWA637505HR120CGN\\\\x2dpart6.swap\"\nI1109 20:58:45.166911      19 factory.go:105] Error trying to work out if we can handle /system.slice/dev-disk-by\\x2did-ata\\x2dINTEL_SSDSC2BB120G6_PHWA637505HR120CGN\\x2dpart6.swap: /system.slice/dev-disk-by\\x2did-ata\\x2dINTEL_SSDSC2BB120G6_PHWA637505HR120CGN\\x2dpart6.swap not handled by systemd handler\nI1109 20:58:45.166928      19 factory.go:116] Factory \"systemd\" was unable to handle container \"/system.slice/dev-disk-by\\\\x2did-ata\\\\x2dINTEL_SSDSC2BB120G6_PHWA637505HR120CGN\\\\x2dpart6.swap\"\nI1109 20:58:45.166945      19 factory.go:112] Using factory \"raw\" for container \"/system.slice/dev-disk-by\\\\x2did-ata\\\\x2dINTEL_SSDSC2BB120G6_PHWA637505HR120CGN\\\\x2dpart6.swap\"\nW1109 20:58:45.167248      19 container.go:358] Failed to create summary reader for \"/system.slice/dev-disk-by\\\\x2did-ata\\\\x2dINTEL_SSDSC2BB120G6_PHWA637505HR120CGN\\\\x2dpart6.swap\": none of the resources are being tracked.\nI1109 20:58:45.167275      19 manager.go:953] Added container: \"/system.slice/dev-disk-by\\\\x2did-ata\\\\x2dINTEL_SSDSC2BB120G6_PHWA637505HR120CGN\\\\x2dpart6.swap\" (aliases: [], namespace: \"\")\nI1109 20:58:45.167416      19 handler.go:325] Added event &{/system.slice/dev-disk-by\\x2did-ata\\x2dINTEL_SSDSC2BB120G6_PHWA637505HR120CGN\\x2dpart6.swap 2017-11-09 14:32:16.285804448 +0000 UTC containerCreation {<nil>}}\nI1109 20:58:45.185655      19 container.go:413] Start housekeeping for container \"/docker/8ccb25d7b137d4078ccaf907cc6c8f673b7a0ef294a78d84f806b780a54a9ad1\"\nI1109 20:58:45.186690      19 container.go:413] Start housekeeping for container \"/system.slice/dev-disk-by\\\\x2did-ata\\\\x2dINTEL_SSDSC2BB120G6_PHWA637505HR120CGN\\\\x2dpart6.swap\"\nI1109 20:58:45.231779      19 factory.go:112] Using factory \"docker\" for container \"/docker/cc76380b649ae973cc66487e306baeb4c2866c91140142f35188785d1d622d6c\"\nKilled\n/ #. ",
    "jiayingz": "/lgtm. ",
    "xiamujun": "docker run --volume=/:/rootfs:ro \\\n--volume=/var/run:/var/run:rw \\\n--volume=/sys:/sys:ro \\\n--volume=/var/lib/docker/:/var/lib/docker:ro \\\n--volume=/cgroup:/cgroup:ro \\\n--volume=/dev/disk/:/dev/disk:ro \\\n--privileged=true \\\n--publish=8080:8080 \\\n--detach=true \\\n--link influxsrv:influxsrv \\\n--name=cadvisor \\\ngoogle/cadvisor:v0.28.0 \\\n-storage_driver_db=influxdb \\\n-storage_driver_host=influxsrv:8086 \\\n-storage_driver_db=cadvisor\n. I obviously run docker, why cadvisor this version will even rkt? I refer to the following link configuration, I remember at that time is operating normally on ubuntu14.04.\nhttps://www.brianchristner.io/how-to-setup-docker-monitoring/?spm=5176.100239.blogcont5065.15.H88S2l\n. thanks. ",
    "mister2d": "@dashpole Is there a flag to tell cadvisor to not attempt to connect to Rkt or CRI-O runtimes?. No other issues. I just was confused as to what the -docker_only argument does.. ",
    "Asher-Shoshan": "Can not connect to Docker in my machine, when starting cadvisor in docker.   --privileged=true is not accepted. Sorry for not being clear.   The cadvisor container saying it's unable to connect to docker daemon...\n(This I see when running it interactively -  run -i -t ...). But I need it for monitoring the containers in Kubernetes and use the metrics for Autoscaling...\nWhat good is it without?\nIs that a bug? Has anyone else faced it?\n\nDigest: sha256:ed53f9f93bb52d64fa8f95daece42d95d18d1e48a8ec9237bd5944ed634348a1\nI1205 16:22:15.350054       1 storagedriver.go:50] Caching stats in memory for 2m0s\nI1205 16:22:15.350664       1 manager.go:143] cAdvisor running in container: \"/system.slice/docker-08f5a85678ff273715fb1e96a00d6b3611ea7edbe114e836aa8c6ef316ea3057.scope\"\nW1205 16:22:15.351153       1 manager.go:147] Unable to connect to Docker: Cannot connect to the Docker daemon. Is the docker daemon running on this host?\nW1205 16:22:15.352133       1 manager.go:151] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused\nI1205 16:22:15.353449       1 fs.go:117] Filesystem partitions: map[/dev/mapper/docker-253:0-1101618-1f833050e2de3377b1069d5964f68b703bd4334304629dc6eb1fbaa9010d1a84:{mountpoint:/ major:253 minor:20 fsType:xfs blockSize:0} /dev/mapper/rhel-root:{mountpoint:/etc/resolv.conf major:253 minor:0 fsType:xfs blockSize:0}]\n. It's a separate one...   I don't know how to check the embedded one in 'kubelet' or how to install/deploy it.\nDocumentation of Heapster is not so clear w.r.t cadvisor/grafana/etc. \n",
    "kazagkazag": "Yeah, we are waiting for that change for weeks.... This is our last hope.. ",
    "khenidak": "@dashpole I think https://github.com/kubernetes/kubernetes/issues/58081 is caused by Close() on golang's inotify is not closing the watch fd. Any chance that we move back to fsnotify? which seems to correctly close it https://github.com/fsnotify/fsnotify/blob/master/inotify.go#L183 . Thanks :-) didn't mean to \"FIX IT!\" - i was just saying.\nMore than happy to help. Do you want to move back to fsnotify or fix the existing inotify deprecated package?. ",
    "fristonio": "Hey @tallclair  I would like to help on this issue. I am new to the community and I would really appreciate it if you help me get started here :). Thanks, I will try to resolve the issue and will make a PR as soon as possible. :). ",
    "choury": "\nit's unlikely that there are containers with GPUs attached when the kubelet starts up\n\nWe should not require to restart all containers if we just want to restart or upgrade kubelet.\nMay be call  GetCollector after NVML initialized or wait it in GetCollector?. ",
    "c4po": "I get the same issue. It seems the google/cadvisor:latest image is a quite old version (8 month ago). \ntry this version google/cadvisor:v0.28.2. @majst01 the docker image google/cadvisor:latest is still an old build.\nCan you rebuild it with v0.28?. ",
    "sundeepk1": "@dashpole \nI am using  google/cadvisor:v0.27.2 image . @dashpole thanks for the update .. i reverted to google/cadvisor:v0.25.0 where i am able to get the required details \n. thanks a lot David @dashpole , Now this has really helped me to understand the background . your help is much appreciated. . ",
    "jsravn": "@dashpole That's right - which is why I added the warn log. We can export it, sure. Even better would be to return the error.. I've updated it to return the error, then client code can handle it appropriately.. @dashpole Okay, addressed your comments.. @dashpole Squashed.. ping @dashpole . @dashpole If you cut a release then I can try to get this updated in Kubernetes. Thanks!. Can you reopen https://github.com/kubernetes/kubernetes/issues/53207? Github closed it.. doh.. As alternative to https://github.com/google/cadvisor/pull/1830. Will require updating k8s for the api changes.. Hmm, I think it's better to just return the error, to keep the api simpler. I've done that in my prior PR https://github.com/google/cadvisor/pull/1830.. /retest. Build is failing when trying to build assets, I think. It doesn't seem related to my change?\nI0103 16:28:36.513] Run: >> building assets\nW0103 16:28:36.520] + echo 'Run: >> building assets'\nW0103 16:28:36.520] + exit 1\nW0103 16:28:36.520] Traceback (most recent call last):\nW0103 16:28:36.520]   File \"/var/lib/jenkins/workspace/pull-cadvisor-e2e/./test-infra/jenkins/../scenarios/execute.py\", line 50, in <module>\nW0103 16:28:36.521]     main(ARGS.env, ARGS.cmd + ARGS.args)\nW0103 16:28:36.521]   File \"/var/lib/jenkins/workspace/pull-cadvisor-e2e/./test-infra/jenkins/../scenarios/execute.py\", line 41, in main\nW0103 16:28:36.521]     check(*cmd)\nW0103 16:28:36.521]   File \"/var/lib/jenkins/workspace/pull-cadvisor-e2e/./test-infra/jenkins/../scenarios/execute.py\", line 30, in check\nW0103 16:28:36.521]     subprocess.check_call(cmd)\nW0103 16:28:36.521]   File \"/usr/lib/python2.7/subprocess.py\", line 186, in check_call\nW0103 16:28:36.521]     raise CalledProcessError(retcode, cmd)\nW0103 16:28:36.522] subprocess.CalledProcessError: Command '('./build/jenkins_e2e.sh',)' returned non-zero exit status 1\nE0103 16:28:36.522] Command failed\nI0103 16:28:36.522] process 20604 exited with code 1 after 0.0m\nE0103 16:28:36.522] FAIL: pull-cadvisor-e2e. @dashpole Can you clarify? I don't see any new commits on release-v0.27. Do I need to cherry pick or update something? Or just amend my commit to reset the date?. Cool, done.. @dashpole I think it's already in v0.28 from https://github.com/google/cadvisor/pull/1830.. Can you cut a release for v0.27 so I can get this in k8s 1.8?. https://github.com/kubernetes/kubernetes/pull/58074 with the v0.27.4 update.. It looks like you're hitting the timeout which is hard coded to 5s. I'm surprised your docker info takes so long - I haven't observed it take more than 1s even with hundreds of containers in my own testing.\nI suggest we 1. increase the default timeout and 2. make timeout configurable via cadvisor flag.\nI'm not sure if start-up should ignore the timeout - as docker calls can hang indefinitely, so that would just cause cadvisor to hang on startup indefinitely.\n. @dashpole let me know what you think is the best approach. The simplest for now would be to just increase the timeout.\nI'm not too happy with the hardcoded value. But I'm not sure the best way to make it configurable, because it will break the API all over the place to pass a timeout around. And there isn't a nice way to add config values (e.g. a conf struct on manager.New). I could do it via a global var on the docker package, but it's pretty horrible.\nWe could also revert the timeout change for now and do it properly via a breaking API change in master. I notice as well none of the rkt and cri-o calls have timeouts.. @dashpole Okay, I made https://github.com/google/cadvisor/pull/1871 for it. I think we should bump the default timeout up as well (better safe than sorry).. @tn-osimis yes, I agree. It is a more systemic problem in cadvisor though, it doesn't use timeouts for anything (until the recent PR where I added this timeout to just docker calls). So it should really be fixed throughout. I'd prefer using a larger timeout than no timeout on startup.. Okay, I can make that change \ud83d\udc4d . Okay, have a look.. @dashpole Addressed your comments.. @dashpole Okay, rebased.. ",
    "davidkarlsen": "@dashpole If that is the case maybe the docs on dockerhub should be updated as they are misleading. What is the recommended stable version to run?. ",
    "TambetP": "Has someone already done that or maybe working on it, or planning to add it?. ",
    "dfredell": "I would enjoy metrics for the nvidia GPUs for processes not in a docker container. Right now we have GPU services running in the host Ubuntu OS. I really just want GPU load, for hardware forecasting, and viewing system usage. Maybe this comment belongs in a new issue. . @mindprince Thanks. Upon more research I learned that cadvisor is more designed to monitor docker. I found https://github.com/tankbusta/nvidia_exporter and https://github.com/prometheus/node_exporter which better fits my use case and needs. . ",
    "alrf": "I have the same issue - \"Failed to start container manager: inotify_add_watch /sys/fs/cgroup/cpuacct: no such file or directory\"\n```\nI0326 10:31:33.675230       1 manager.go:231] Version: {KernelVersion:4.4.30-32.54.amzn1.x86_64 ContainerOsVersion:Alpine Linux v3.4 DockerVersion:17.12.0-ce DockerAPIVersion:1.35 CadvisorVersion:v0.28.3 CadvisorRevision:1e567c2}\nE0326 10:31:33.684262       1 factory.go:340] devicemapper filesystem stats will not be reported: usage of thin_ls is disabled to preserve iops\nI0326 10:31:33.684748       1 factory.go:356] Registering Docker factory\nI0326 10:31:35.685162       1 factory.go:54] Registering systemd factory\nI0326 10:31:35.685727       1 factory.go:86] Registering Raw factory\nI0326 10:31:35.686198       1 manager.go:1178] Started watching for new ooms in manager\nI0326 10:31:35.686649       1 manager.go:329] Starting recovery of all containers\nI0326 10:31:35.686677       1 manager.go:334] Recovery completed\nF0326 10:31:35.686690       1 cadvisor.go:156] Failed to start container manager: inotify_add_watch /sys/fs/cgroup/cpuacct: no such file or directory\n```\n. I have the same issue - there are no container names.\n```\ncadvisor_version_info{cadvisorRevision=\"1e567c2\",cadvisorVersion=\"v0.28.3\",dockerVersion=\"17.12.0-ce\",kernelVersion=\"4.4.30-32.54.amzn1.x86_64\",osVersion=\"Alpine Linux v3.4\"} 1\nHELP container_cpu_load_average_10s Value of container cpu load average over the last 10 seconds.\nTYPE container_cpu_load_average_10s gauge\ncontainer_cpu_load_average_10s{id=\"/\"} 0\nHELP container_cpu_system_seconds_total Cumulative system cpu time consumed in seconds.\nTYPE container_cpu_system_seconds_total counter\ncontainer_cpu_system_seconds_total{id=\"/\"} 0.07\nHELP container_cpu_usage_seconds_total Cumulative cpu time consumed per cpu in seconds.\nTYPE container_cpu_usage_seconds_total counter\ncontainer_cpu_usage_seconds_total{cpu=\"cpu00\",id=\"/\"} 0.123321595\ncontainer_cpu_usage_seconds_total{cpu=\"cpu01\",id=\"/\"} 0.127031709\ncontainer_cpu_usage_seconds_total{cpu=\"cpu02\",id=\"/\"} 0.131574615\ncontainer_cpu_usage_seconds_total{cpu=\"cpu03\",id=\"/\"} 0.123025697\nHELP container_cpu_user_seconds_total Cumulative user cpu time consumed in seconds.\nTYPE container_cpu_user_seconds_total counter\ncontainer_cpu_user_seconds_total{id=\"/\"} 0.09\nHELP container_fs_inodes_free Number of available Inodes\nTYPE container_fs_inodes_free gauge\ncontainer_fs_inodes_free{device=\"/dev/mapper/docker-202:1-263391-1d5b3b65676d417f5f0a060975be7b4dbc566f700df2cb246ddb832acb1a409e\",id=\"/\"} 1.0483326e+07\ncontainer_fs_inodes_free{device=\"/dev/mapper/docker-202:1-263391-d9c3e444740218ff68d35502ac51fcc57f50f69f8d24db86db60067673386f80\",id=\"/\"} 1.0436712e+07\ncontainer_fs_inodes_free{device=\"/dev/xvda1\",id=\"/\"} 7.812362e+06\ncontainer_fs_inodes_free{device=\"shm\",id=\"/\"} 2.05452e+06\ncontainer_fs_inodes_free{device=\"tmpfs\",id=\"/\"} 2.054359e+06\nHELP container_fs_inodes_total Number of Inodes\nTYPE container_fs_inodes_total gauge\ncontainer_fs_inodes_total{device=\"/dev/mapper/docker-202:1-263391-1d5b3b65676d417f5f0a060975be7b4dbc566f700df2cb246ddb832acb1a409e\",id=\"/\"} 1.0484736e+07\ncontainer_fs_inodes_total{device=\"/dev/mapper/docker-202:1-263391-d9c3e444740218ff68d35502ac51fcc57f50f69f8d24db86db60067673386f80\",id=\"/\"} 1.0484736e+07\ncontainer_fs_inodes_total{device=\"/dev/xvda1\",id=\"/\"} 7.86432e+06\ncontainer_fs_inodes_total{device=\"shm\",id=\"/\"} 2.054521e+06\ncontainer_fs_inodes_total{device=\"tmpfs\",id=\"/\"} 2.054521e+06\nHELP container_fs_io_current Number of I/Os currently in progress\nTYPE container_fs_io_current gauge\ncontainer_fs_io_current{device=\"/dev/mapper/docker-202:1-263391-1d5b3b65676d417f5f0a060975be7b4dbc566f700df2cb246ddb832acb1a409e\",id=\"/\"} 0\ncontainer_fs_io_current{device=\"/dev/mapper/docker-202:1-263391-d9c3e444740218ff68d35502ac51fcc57f50f69f8d24db86db60067673386f80\",id=\"/\"} 0\ncontainer_fs_io_current{device=\"/dev/xvda1\",id=\"/\"} 0\ncontainer_fs_io_current{device=\"shm\",id=\"/\"} 0\ncontainer_fs_io_current{device=\"tmpfs\",id=\"/\"} 0\nHELP container_fs_io_time_seconds_total Cumulative count of seconds spent doing I/Os\nTYPE container_fs_io_time_seconds_total counter\ncontainer_fs_io_time_seconds_total{device=\"/dev/mapper/docker-202:1-263391-1d5b3b65676d417f5f0a060975be7b4dbc566f700df2cb246ddb832acb1a409e\",id=\"/\"} 0\ncontainer_fs_io_time_seconds_total{device=\"/dev/mapper/docker-202:1-263391-d9c3e444740218ff68d35502ac51fcc57f50f69f8d24db86db60067673386f80\",id=\"/\"} 0\ncontainer_fs_io_time_seconds_total{device=\"/dev/xvda1\",id=\"/\"} 5.5168e-05\n```. ",
    "robertofabrizi": "Fails on the latest AWS Amazon Linux as well.. ",
    "summera": "Also having this issue on Amazon Linux. Removing /sys:/sys:ro worked but missing data now.. What worked for me was mounting the following volumes on amazon linux\n- /:/rootfs:ro\n      - /var/run:/var/run:rw\n      - /sys:/sys:ro\n      - /var/lib/docker/:/var/lib/docker:ro\n      - /cgroup:/sys/fs/cgroup:ro\n      - /dev/disk/:/dev/disk:ro\nI think the key was /cgroup:/sys/fs/cgroup:ro. ",
    "edasque": "Expected Behaviour\nOn a Synology, I should be able to get metrics on containers. /metrics should show a lot of metrics on containers\nCurrent Behaviour\nAfter a restart due to updating the minor version of Docker (to ), I get this in the err log\nI1227 04:16:53.543073       1 storagedriver.go:50] Caching stats in memory for 2m0s\nI1227 04:16:53.543643       1 manager.go:151] cAdvisor running in container: \"/sys/fs/cgroup/cpu\"\nI1227 04:16:53.813431       1 fs.go:139] Filesystem UUIDs: map[]\nI1227 04:16:53.813532       1 fs.go:140] Filesystem partitions: map[cgmfs:{mountpoint:/var/run/cgmanager/fs major:0 minor:18 fsType:tmpfs blockSize:0} /dev/vg1000/lv:{mountpoint:/etc/resolv.conf major:253 minor:0 fsType:ext4 blockSize:0} shm:{mountpoint:/dev/shm major:0 minor:185 fsType:tmpfs blockSize:0} /dev/md0:{mountpoint:/var/lib/docker major:9 minor:0 fsType:ext4 blockSize:0} tmpfs:{mountpoint:/dev major:0 minor:188 fsType:tmpfs blockSize:0} none:{mountpoint:/ major:0 minor:184 fsType:aufs blockSize:0} /run:{mountpoint:/var/run major:0 minor:15 fsType:tmpfs blockSize:0}]\nW1227 04:16:53.827315       1 info.go:52] Couldn't collect info from any of the files in \"/etc/machine-id,/var/lib/dbus/machine-id\"\nI1227 04:16:53.827474       1 manager.go:225] Machine: {NumCores:4 CpuFrequency:2400000 MemoryCapacity:16820633600 HugePages:[] MachineID: SystemUUID:78563412-3412-7856-90AB-CDDEEFAABBCC BootID:d3ee6742-368e-43c0-bb86-1fb864d15278 Filesystems:[{Device:none DeviceMajor:0 DeviceMinor:184 Capacity:27536640897024 Type:vfs Inodes:1707507712 HasInodes:true} {Device:/run DeviceMajor:0 DeviceMinor:15 Capacity:8410316800 Type:vfs Inodes:2053300 HasInodes:true} {Device:cgmfs DeviceMajor:0 DeviceMinor:18 Capacity:102400 Type:vfs Inodes:2053300 HasInodes:true} {Device:/dev/vg1000/lv DeviceMajor:253 DeviceMinor:0 Capacity:27536640897024 Type:vfs Inodes:1707507712 HasInodes:true} {Device:shm DeviceMajor:0 DeviceMinor:185 Capacity:67108864 Type:vfs Inodes:2053300 HasInodes:true} {Device:/dev/md0 DeviceMajor:9 DeviceMinor:0 Capacity:2442780672 Type:vfs Inodes:155648 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:188 Capacity:8410316800 Type:vfs Inodes:2053300 HasInodes:true}] DiskMap:map[9:1:{Name:md1 Major:9 Minor:1 Size:2147418112 Scheduler:none} 8:96:{Name:sdg Major:8 Minor:96 Size:6001175126016 Scheduler:cfq} 253:0:{Name:dm-0 Major:253 Minor:0 Size:27975772798976 Scheduler:none} 8:48:{Name:sdd Major:8 Minor:48 Size:6001175126016 Scheduler:cfq} 252:2:{Name:zram2 Major:252 Minor:2 Size:2522873856 Scheduler:none} 135:240:{Name:synoboot Major:135 Minor:240 Size:125829120 Scheduler:cfq} 9:0:{Name:md0 Major:9 Minor:0 Size:2549940224 Scheduler:none} 9:2:{Name:md2 Major:9 Minor:2 Size:11972709974016 Scheduler:none} 9:3:{Name:md3 Major:9 Minor:3 Size:16003067543552 Scheduler:none} 8:16:{Name:sdb Major:8 Minor:16 Size:8001563222016 Scheduler:cfq} 8:32:{Name:sdc Major:8 Minor:32 Size:6001175126016 Scheduler:cfq} 8:64:{Name:sde Major:8 Minor:64 Size:6301233340416 Scheduler:cfq} 8:112:{Name:sdh Major:8 Minor:112 Size:2000398934016 Scheduler:cfq} 252:0:{Name:zram0 Major:252 Minor:0 Size:2522873856 Scheduler:none} 252:1:{Name:zram1 Major:252 Minor:1 Size:2522873856 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:6201213935616 Scheduler:cfq} 8:80:{Name:sdf Major:8 Minor:80 Size:2000398934016 Scheduler:cfq} 252:3:{Name:zram3 Major:252 Minor:3 Size:2522873856 Scheduler:none}] NetworkDevices:[{Name:eth0 MacAddress:00:11:32:72:84:6f Speed:1000 Mtu:1500} {Name:eth1 MacAddress:00:11:32:72:84:70 Speed:4294967295 Mtu:1500} {Name:eth2 MacAddress:00:11:32:72:84:71 Speed:4294967295 Mtu:1500} {Name:eth3 MacAddress:00:11:32:72:84:72 Speed:4294967295 Mtu:1500} {Name:sit0 MacAddress:00:00:00:00 Speed:0 Mtu:1480} {Name:tun0 MacAddress: Speed:10 Mtu:1500} {Name:tun1000 MacAddress: Speed:10 Mtu:1400}] Topology:[{Id:0 Memory:0 Cores:[{Id:0 Threads:[0] Caches:[{Size:24576 Type:Data Level:1} {Size:32768 Type:Instruction Level:1}]} {Id:1 Threads:[1] Caches:[{Size:24576 Type:Data Level:1} {Size:32768 Type:Instruction Level:1}]} {Id:2 Threads:[2] Caches:[{Size:24576 Type:Data Level:1} {Size:32768 Type:Instruction Level:1}]} {Id:3 Threads:[3] Caches:[{Size:24576 Type:Data Level:1} {Size:32768 Type:Instruction Level:1}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}\nI1227 04:16:53.831323       1 manager.go:231] Version: {KernelVersion:3.10.102 ContainerOsVersion:Alpine Linux v3.4 DockerVersion:17.05.0-ce DockerAPIVersion:1.29 CadvisorVersion:v0.28.3 CadvisorRevision:1e567c2}\nI1227 04:16:53.992531       1 factory.go:356] Registering Docker factory\nI1227 04:16:55.993148       1 factory.go:54] Registering systemd factory\nI1227 04:16:55.993944       1 factory.go:86] Registering Raw factory\nI1227 04:16:55.994759       1 manager.go:1178] Started watching for new ooms in manager\nW1227 04:16:55.994918       1 manager.go:313] Could not configure a source for OOM detection, disabling OOM events: open /dev/kmsg: no such file or directory\nI1227 04:16:55.999676       1 manager.go:329] Starting recovery of all containers\nE1227 04:16:56.147983       1 manager.go:1103] Failed to create existing container: /docker/58f3f4c3a29cbe56341dce542b6d836afb867b9c680cca40f3186a1f7e2a0129: failed to identify the read-write layer ID for container \"58f3f4c3a29cbe56341dce542b6d836afb867b9c680cca40f3186a1f7e2a0129\". - open /volume1/@docker/image/aufs/layerdb/mounts/58f3f4c3a29cbe56341dce542b6d836afb867b9c680cca40f3186a1f7e2a0129/mount-id: no such file or directory\nThe impact I see is seeing lots of stats on subcontainers but nothing on containers.\nDocker version: 17.05.0-ce\nOS: Linux 3.10.102 #15217 SMP Wed Dec 20 18:18:56 CST 2017 x86_64 GNU/Linux synology_avoton_1815+\ndocker run --name=cadvisor --env=\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" --env=\"GLIBC_VERSION=2.23-r3\" --volume=\"/var/lib/docker:/var/lib/docker:ro\" --volume=\"/var/run:/var/run:rw\" --volume=\"/sys:/sys:ro\" -p 0.0.0.0:8005:8080/tcp --detach=true google/cadvisor\n/validate gives me:\n```\ncAdvisor version: v0.28.3\nOS version: Alpine Linux v3.4\nKernel version: [Supported and recommended]\n    Kernel version is 3.10.102. Versions >= 2.6 are supported. 3.0+ are recommended.\nCgroup setup: [Supported and recommended]\n    Available cgroups: map[devices:1 freezer:1 blkio:1 cpuset:1 cpu:1 cpuacct:1 memory:1]\n    Following cgroups are required: [cpu cpuacct]\n    Following other cgroups are recommended: [memory blkio cpuset devices freezer]\n    Hierarchical memory accounting enabled. Reported memory usage includes memory used by child containers.\nCgroup mount setup: [Supported and recommended]\n    Cgroups are mounted at /sys/fs/cgroup.\n    Cgroup mount directories: blkio cgmanager cpu cpuacct cpuset devices freezer memory \n    Any cgroup mount point that is detectible and accessible is supported. /sys/fs/cgroup is recommended as a standard location.\n    Cgroup mounts:\n    cgroup /sys/fs/cgroup/cpuset cgroup ro,nosuid,nodev,noexec,relatime,cpuset,release_agent=/run/cgmanager/agents/cgm-release-agent.cpuset,clone_children 0 0\n    cgroup /sys/fs/cgroup/cpu cgroup ro,nosuid,nodev,noexec,relatime,cpu,release_agent=/run/cgmanager/agents/cgm-release-agent.cpu 0 0\n    cgroup /sys/fs/cgroup/cpuacct cgroup ro,nosuid,nodev,noexec,relatime,cpuacct,release_agent=/run/cgmanager/agents/cgm-release-agent.cpuacct 0 0\n    cgroup /sys/fs/cgroup/memory cgroup ro,nosuid,nodev,noexec,relatime,memory,release_agent=/run/cgmanager/agents/cgm-release-agent.memory 0 0\n    cgroup /sys/fs/cgroup/devices cgroup ro,nosuid,nodev,noexec,relatime,devices,release_agent=/run/cgmanager/agents/cgm-release-agent.devices 0 0\n    cgroup /sys/fs/cgroup/freezer cgroup ro,nosuid,nodev,noexec,relatime,freezer,release_agent=/run/cgmanager/agents/cgm-release-agent.freezer 0 0\n    cgroup /sys/fs/cgroup/blkio cgroup ro,nosuid,nodev,noexec,relatime,blkio,release_agent=/run/cgmanager/agents/cgm-release-agent.blkio 0 0\n    cgroup /sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset,release_agent=/run/cgmanager/agents/cgm-release-agent.cpuset,clone_children 0 0\n    cgroup /sys/fs/cgroup/cpu cgroup rw,relatime,cpu,release_agent=/run/cgmanager/agents/cgm-release-agent.cpu 0 0\n    cgroup /sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct,release_agent=/run/cgmanager/agents/cgm-release-agent.cpuacct 0 0\n    cgroup /sys/fs/cgroup/memory cgroup rw,relatime,memory,release_agent=/run/cgmanager/agents/cgm-release-agent.memory 0 0\n    cgroup /sys/fs/cgroup/devices cgroup rw,relatime,devices,release_agent=/run/cgmanager/agents/cgm-release-agent.devices 0 0\n    cgroup /sys/fs/cgroup/freezer cgroup rw,relatime,freezer,release_agent=/run/cgmanager/agents/cgm-release-agent.freezer 0 0\n    cgroup /sys/fs/cgroup/blkio cgroup rw,relatime,blkio,release_agent=/run/cgmanager/agents/cgm-release-agent.blkio 0 0\nDocker version: [Supported and recommended]\n    Docker version is 17.05.0-ce. Versions >= 1.0 are supported. 1.2+ are recommended.\nDocker driver setup: [Supported and recommended]\n    Storage driver is aufs.\nBlock device setup: [Supported and recommended]\n    At least one device supports 'cfq' I/O scheduler. Some disk stats can be reported.\n     Disk \"zram3\" Scheduler type \"none\".\n     Disk \"md1\" Scheduler type \"none\".\n     Disk \"sda\" Scheduler type \"cfq\".\n     Disk \"sdg\" Scheduler type \"cfq\".\n     Disk \"zram2\" Scheduler type \"none\".\n     Disk \"sdh\" Scheduler type \"cfq\".\n     Disk \"synoboot\" Scheduler type \"cfq\".\n     Disk \"zram1\" Scheduler type \"none\".\n     Disk \"sde\" Scheduler type \"cfq\".\n     Disk \"md2\" Scheduler type \"none\".\n     Disk \"md3\" Scheduler type \"none\".\n     Disk \"sdb\" Scheduler type \"cfq\".\n     Disk \"sdd\" Scheduler type \"cfq\".\n     Disk \"sdf\" Scheduler type \"cfq\".\n     Disk \"zram0\" Scheduler type \"none\".\n     Disk \"dm-0\" Scheduler type \"none\".\n     Disk \"md0\" Scheduler type \"none\".\n     Disk \"sdc\" Scheduler type \"cfq\".\nInotify watches: \nManaged containers: \n    /pgsql\n    /synoagentregisterd\n    /crond\n    /pkgctl-Plex Media Server\n    /pkgctl-Java7\n    /pkgctl-Perl\n    /pkgctl-Git\n    /pkgctl-SynoFinder\n    /pkgctl-CloudSync\n    /iscsi_pluginengined\n    /pkgctl-git\n    /pkgctl-nessentials\n    /synoscgi\n    /pkgctl-DownloadStation\n    /pkgctl-NoteStation\n    /pkgctl-monit\n    /tty\n    /pkgctl-nzbdrone\n    /pkgctl-VPNCenter\n    /synosnmpcd\n    /sshd\n    /\n    /pkgctl-TextEditor\n    /pkgctl-Init_3rdparty.conf\n    /pkgctl-VideoStation\n    /pkgctl-Docker\n    /netatalk\n    /pkgctl-darkstat\n    /nginx\n    /syslog-ng\n    /pkgctl-PhotoStation\n    /dhcp-client\n    /synologd\n    /synoindexd\n    /pkgctl-Java8\n    /pkgctl-HyperBackupVault\n    /dbus-session\n    /snmpd\n    /synocontentextractd\n    /pkgctl-PHP7.0\n    /pkgctl-Init_3rdparty\n    /pkgctl-zsh\n    /pkgctl-notifier\n    /pkgctl-oracle-java\n    /pkgctl-AudioStation\n    /pkgctl-FileStation\n    /synologrotated\n    /pkgctl-mono\n    /pkgctl-SurveillanceStation\n    /pkgctl-LogCenter\n    /s2s_daemon\n    /udevd\n    /synocrond\n    /pkgctl-tmux\n    /synonetd\n    /pkgctl-HyperBackup\n    /synocgid\n    /cupsd\n    /synomkflvd\n    /pkgctl-chromaprint\n    /pkgctl-ffmpeg\n    /pkgctl-PerlCGI\n    /docker\n    /pkgctl-python\n    /ntpd\n    /synorelayd\n    /pkgctl-radarr\n    /findhostd\n    /smbd\n    /pkgctl-net_notifier\n    /pkgctl-mediainfo\n    /avahi\n    /nmbd\n    /iscsi_pluginserverd\n    /scemd\n    /synobackupd\n    /pkgctl-CloudStation\n    /pkgctl-StorageAnalyzer\n    /synoconfd\n    /pkgctl-PHP5.6\n    /pkgctl-filebot\n    /synomkthumbd\n    /dbus-system\n    /synotifyd\n    /synostoraged\n    /hotplugd\n    /minissdpd\n    /pkgctl-WebTools\n    /inetd\n```\nWhich lists a lot of what would seem to be \"system\" containers and none of the ones that I am running, including cadvisor btw:\nCONTAINER ID        IMAGE                       COMMAND                  CREATED             STATUS              PORTS                                                                                                                                                                         NAMES\n2f6a81590701        google/cadvisor             \"/usr/bin/cadvisor...\"   3 days ago          Up 2 hours          0.0.0.0:8005->8080/tcp                                                                                                                                                        cadvisor\nb1f28a08e7a2        prom/prometheus             \"/bin/prometheus -...\"   3 days ago          Up 2 hours          0.0.0.0:9090->9090/tcp                                                                                                                                                        prometheus_ocho\n58f3f4c3a29c        pipeline-to-graphite        \"/bin/sh -c 'bash ...\"   9 days ago          Up 2 hours                                                                                                                                                                                        pipeline-to-graphite-ocho. Note that I didn't map   --volume=/dev/disk/:/dev/disk:ro \\ because /dev/disk doesn't exist on the synology. Doing some research to figure out what's equivalent.. Hmmm, maybe all I was missing was --volume=/:/rootfs:ro\nclosing for now. This might have been simply resolved by the newer Docker subversion on Synology and having removed   --volume=/:/rootfs:ro when troubleshooting.. or running:\necho 104857 > /proc/sys/fs/inotify/max_user_watches\nsince\ndocker run --name=cadvisor --volume=\"/var/lib/docker:/var/lib/docker:ro\" --volume=/:/rootfs:ro --volume=\"/var/run:/var/run:rw\" --volume=\"/sys:/sys:ro\" -p 8005:8080 --detach=true google/cadvisor seems to work.. ",
    "timzaak": "+1\nreally need it, because someone who brings their project into docker always leave log file inside docker.. ",
    "sylvek": "same problem here. We're downgrading to 0.27.3.\nMy host computer is a CentOS 7.3 with a Linux kernel version at 3.11.. ",
    "jbgachot": "I had the same issue on Amazon linux (ECS). Needed to mount /cgroup (host) on /sys/fs/cgroup (cadvisor) wich is the path expected by cadvisor.\ndocker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/cgroup:/sys/fs/cgroup:ro --volume=/var/lib/docker/:/var/lib/docker:ro --volume=/dev/disk/:/dev/disk:ro --publish=8080:8080 --detach=true --name=cadvisor google/cadvisor:latest\n/sys/fs/cgroup exist on amazon linux but is empty.\nhttps://github.com/google/cadvisor/blob/b9ab5d6ba96e2d840270b75c765faed0dad5004f/validate/validate.go#L197\n. ",
    "rodrigocostasensedia": "@jbgachot, awesome!!!\nThanks ;-) . ",
    "gabetocci": "@jbgachot, THANKYOU!!!. ",
    "jpslopes": "Thank you very much for the answer. I have another 2 questions: \n- How much instrusive is cAdvisor with the containers that it is collecting metrics from?\n- What is the best way to gather metrics from containers: running cAdvisor as a container or as \"standalone\"?. Thank you for answering.. ",
    "ptdel": "hello, could this be related to docker versions?  I am running 18.01-ce locally, and I am able to query /api/v1.3/docker with no issues, however i am trying to use cadvisor in amazon ecs, which runs 17.09.1-ce and i am getting the empty result back from this.  Using the standard mount points.. ",
    "siuwwong5": "[root@hkgcvdv00396 docker]# cat /etc/redhat-release\nRed Hat Enterprise Linux Server release 7.3 (Maipo)\n[root@hkgcvdv00396 docker]# docker version\nClient:\n Version:      1.12.6\n API version:  1.24\n Go version:   go1.6.4\n Git commit:   78d1802\n Built:        Tue Jan 10 20:20:01 2017\n OS/Arch:      linux/amd64\nServer:\n Version:      1.12.6\n API version:  1.24\n Go version:   go1.6.4\n Git commit:   78d1802\n Built:        Tue Jan 10 20:20:01 2017\n OS/Arch:      linux/amd64\n. ",
    "nielsole": "I read #431 that sched_debug (not schedstat) is not suitable for measuring load. It doesn't mention schedstat but just removes it. I could well imagine that schedstat is not suitable for measuring load on a system, but that is not what we are attempting to do.\n. I signed it!. There is an issue where the metrics are only collected for some containers. E.g. it works for all docker containers, except when they are kubelet containers. I suspect GetAllPids() to not do what I expect it to do, but haven't had the time to dig in.. https://github.com/google/cadvisor/pull/1872#issuecomment-360165511 was resolved. I was running it within docker and had not set --pid=host . googleapi: Error 403: Quota 'IN_USE_ADDRESSES' exceeded. Limit: 8.0 in region us-central1.\n/retest. /retest. We had some doubts about the unit used in the /schedstat file because the documentation just said \"time\".\nHere is the list of places that you can follow to find out the units:\nrun_delay\ndefined in https://github.com/torvalds/linux/blob/61f14c015f5be9151ba25e638d349f4d40cb7cd4/include/linux/sched.h#L250-L250\nIndirectly uses rq_clock: https://github.com/torvalds/linux/blob/ab2d92ad881da11331280aedf612d82e61cb6d41/kernel/sched/sched.h#L925\nWhich is updated here: https://github.com/torvalds/linux/blob/a2e5790d841658485d642196dbb0927303d6c22f/kernel/sched/core.c#L232\nhttps://github.com/torvalds/linux/blob/8e9a2dba8686187d8c8179e5b86640e653963889/kernel/sched/clock.c#L353 \nhttps://github.com/torvalds/linux/blob/8e9a2dba8686187d8c8179e5b86640e653963889/kernel/sched/clock.c#L18\nso in fact it is measured in nanoseconds. I just rebased onto the changes from https://github.com/google/cadvisor/pull/1887. \nWhat do you think @dashpole ?. As it is part of the CpuStats, it is already part of the V2 API:\n/api/v2.0/stats\n\n/api/v1.3/containers\n\nCould you elaborate what still needs to be done?. IIRC we cache the values, so the counter never goes backwards when a process dies.. Now that I think about it: We have this cache by PID so if a process dies and (after some time) another process comes up with the same PID, it would overwrite the cache and only then would the counter go backwards. I have no idea how frequently the same PID will be handed out again, so this might become an issue for some workloads.. I will take a closer look at it tomorrow. On first glance it lgtm. :+1:  lgtm \n. Note to self: The indices need to be adapted.. \ud83d\udc4d . I am not so sure what the difference is, but they look quite distinct:\n\n. Made a little something here: https://github.com/flix-tech/cadvisor/pull/1 So it is feasible.\nI do not really like the code changes this requires.. Actually I did not figure it out. #1872 was referring to a problem I had with running cAdvisor. . I share your gripe that ContainerHandler contains so many fields it should not care about. Also I did not like the seen, but luckily that is gone. . Maybe move all methods of the Handler struct together? Right now they are so scattered throughout the file. Maybe even move them out of helpers.go? . We missed removing the ignoreMetrics from all handlers. wrong line actually, aimed for 48-49. You are right. \nProcesses spawned and exiting between housekeeping intervals would still be lost, but I guess that is not comment-worthy. Took it out.. ",
    "sabras75": "This is permanent. \nI'll check the logs and post it in a future comment.. here are the logs. I raise the verbosity to get something that I hope to be interesting (--v=42)\ncadvisor-log.txt\nout of the log, maybe those messages can be helpfull to you\n\nI0117 13:23:08.720138       1 manager.go:158] Docker not connected: context deadline exceeded\nI0117 13:23:18.745673       1 manager.go:270] Registration of the Docker container factory failed: failed to validate Docker info: failed to detect Docker info: context deadline exceeded.\nI0117 13:23:21.053932       1 manager.go:970] Added container: \"/docker/6cd62bf3133bcc6fd49b1f0c23d4d08c194011eb774afe553535068d3da6b502\" (aliases: [], namespace: \"\")\nI0117 13:23:21.054319       1 handler.go:325] Added event &{/docker/6cd62bf3133bcc6fd49b1f0c23d4d08c194011eb774afe553535068d3da6b502 2018-01-17 12:46:31.779005956 +0000 UTC containerCreation {}}\nI0117 13:23:21.054359       1 factory.go:105] Error trying to work out if we can handle /docker/a1afe3b2d76c7a268331cd1ee6a3814d8bd0e84e9619556d35e7d530b3ee8a11: /docker/a1afe3b2d76c7a268331cd1ee6a3814d8bd0e84e9619556d35e7d530b3ee8a11 not handled by systemd handler\nI0117 13:23:21.054371       1 factory.go:116] Factory \"systemd\" was unable to handle container \"/docker/a1afe3b2d76c7a268331cd1ee6a3814d8bd0e84e9619556d35e7d530b3ee8a11\"\nI0117 13:23:21.054384       1 factory.go:112] Using factory \"raw\" for container \"/docker/a1afe3b2d76c7a268331cd1ee6a3814d8bd0e84e9619556d35e7d530b3ee8a11\"\n\n. For information, on this machine, the docker info command seems rather slow (like 6.7 seconds). (see below)\nSo maybe, is it just simply a timeout in cadvisor. What is the value of the timeout ? Is there a way to alter this value from command line or a config file ?\n\n$ time docker info\nContainers: 90\n Running: 86\n Paused: 0\n Stopped: 4\nImages: 17632\nServer Version: 17.12.0-ce\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 13046\n Dirperm1 Supported: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: bridge host macvlan null overlay\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nInit Binary: docker-init\ncontainerd version: 89623f28b87a6004d4b785663257362d1658a729\nrunc version: b2567b37d7b75eb4cf325b77297b140ea686ce8f\ninit version: 949e6fa\nSecurity Options:\n apparmor\n seccomp\n  Profile: default\nKernel Version: 4.4.0-108-generic\nOperating System: Ubuntu 16.04.3 LTS\nOSType: linux\nArchitecture: x86_64\nCPUs: 16\nTotal Memory: 55.03GiB\nName: jenkins-master\nID: IFYG:3LOB:TGH3:NXLV:NYZX:6XS6:BQVL:ZMEJ:AQIC:AVUP:RO7A:MH5Q\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): false\nRegistry: https://index.docker.io/v1/\nLabels:\nExperimental: false\nInsecure Registries:\n 127.0.0.0/8\nLive Restore Enabled: false\nWARNING: No swap limit support\nreal  0m6.710s\nuser  0m0.016s\nsys   0m0.004s\n. The version of cAdvisor is the one from the public docker image \"google/cadvisor:v0.28.3\". \n",
    "BenTheElder": "/restest. /retest\n\ud83e\udd37\u200d\u2642\ufe0f . /test pull-cadvisor-e2e. @krzyzacy we probably want to fix tot? \ud83e\udd37\u200d\u2642\ufe0f . https://github.com/kubernetes/test-infra/pull/6320\nOn Wed, Jan 17, 2018 at 2:32 PM, Sen Lu notifications@github.com wrote:\n\nI0117 22:29:28.446] Call:  git fetch --quiet --tags https://github.com/kubernetes/cadvisor master +refs/pull/1868/head:refs/pr/1868\nW0117 22:29:28.682] fatal: could not read Username for 'https://github.com': No such device or address\nwhoops it's not in kubernetes\n/shrug\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1868#issuecomment-358471668, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AA4Bq8nrT25UFN_N-sZjUaByoxbg4Rfdks5tLnT5gaJpZM4Rgoj5\n.\n. /retest. /test pull-cadvisor-e2e. > W0117 22:43:46.722] build/assets.sh: line 31: go: command not found\n\nWe should move this to the kubekins-master image.. opening another PR. @dashpole \nbash\n # Build & test with go 1.8\n docker run --rm \\\n        -w \"/go/src/github.com/google/cadvisor\" \\\n        -v \"${GOPATH}/src/github.com/google/cadvisor:/go/src/github.com/google/cadvisor\" \\\n        golang:1.8 make all test-runner\ncan we replace this with something like:\nbash\nexport GOPATH=/go\nCADVISOR_GOPATH=$GOPATH/src/github.com/google/cadvisor\nmkdir -p $CADVISOR_GOPATH\nln -s $CADVISOR_GOPATH $PWD\nmake all test-runner\nWe will have an image with go installed running this job, so mostly we just need the $GOPATH to be right. (xref: https://github.com/kubernetes/test-infra/pull/6322). @dashpole 1.9.2 currently but we maintain a variant of the image for each k8s release branch so we could get EG 1.8.3 by switching to the k8s 1.8 or 1.7 image.. https://github.com/kubernetes/test-infra/blob/f6546fbe531316615649aca5a43b6c310772692d/images/kubekins-e2e/Makefile#L18-L47. Hmmm... kubekins-e2e does:\n```\nENV GOPATH /go\nENV PATH /usr/local/go/bin:$PATH\nENV PATH $GOPATH/bin:$PATH\nsetup k8s repo symlink\nRUN mkdir -p /go/src/k8s.io/kubernetes \\\n    && ln -s /go/src/k8s.io/kubernetes /workspace/kubernetes\n```\nSo we probably shouldn't set $GOPATH but i'm not sure why go list ./... is giving _/workspace/... results.. /test pull-cadvisor-e2e. https://cloud.google.com/sdk/gcloud/reference/compute/ssh\n--strict-host-key-checking= can be one of yes, no, ask. We probably don't even need to set this flag?\n\nBy default, StrictHostKeyChecking is set to 'no' the first time you connect to an instance, and will be set to 'yes' for all subsequent connections.. /retest. You could use a trap for exit instead.\n\nOn Wed, Jan 17, 2018, 19:58 Sen Lu notifications@github.com wrote:\n\n@krzyzacy commented on this pull request.\nIn build/jenkins_e2e.sh\nhttps://github.com/google/cadvisor/pull/1868#discussion_r162244655:\n\n${golden_nodes[*]}\n+\n+if [[ -a \"${GOOGLE_APPLICATION_CREDENTIALS:-}\" ]] ; then\n\nhummm, you can store exit value from runner and exit with that in the end,\nand don't set -e?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1868#discussion_r162244655, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AA4Bq63WP9qqMkjtp8NExks2et9hMX0Gks5tLsF8gaJpZM4Rgoj5\n.\n. But really why doesn't bootstrap just always switch back after the scenario\nSen?\n\nOn Wed, Jan 17, 2018, 20:11 Benjamin Elder bentheelder@google.com wrote:\n\nYou could use a trap for exit instead.\nOn Wed, Jan 17, 2018, 19:58 Sen Lu notifications@github.com wrote:\n\n@krzyzacy commented on this pull request.\nIn build/jenkins_e2e.sh\nhttps://github.com/google/cadvisor/pull/1868#discussion_r162244655:\n\n${golden_nodes[*]}\n+\n+if [[ -a \"${GOOGLE_APPLICATION_CREDENTIALS:-}\" ]] ; then\n\nhummm, you can store exit value from runner and exit with that in the\nend, and don't set -e?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1868#discussion_r162244655, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AA4Bq63WP9qqMkjtp8NExks2et9hMX0Gks5tLsF8gaJpZM4Rgoj5\n.\n\n\n. Still SSH connection timeout and then log upload fails because we haven't\npatched bootstrap yet\n\nOn Thu, Jan 18, 2018, 10:41 k8s-ci-robot notifications@github.com wrote:\n\n@dashpole https://github.com/dashpole: The following test failed, say\n/retest to rerun them all:\nTest name Commit Details Rerun command\npull-cadvisor-e2e da7e1b4\nhttps://github.com/google/cadvisor/commit/da7e1b4a65eb43ccc9769568276005cd99de419e\nlink\nhttps://k8s-gubernator.appspot.com/build/kubernetes-jenkins/pr-logs/pull/google_cadvisor/1868/pull-cadvisor-e2e/23/ /test\npull-cadvisor-e2e\nFull PR test history\nhttps://k8s-gubernator.appspot.com/pr/google_cadvisor/1868. Your PR\ndashboard https://k8s-gubernator.appspot.com/pr/dashpole. Please help\nus cut down on flakes by linking to\nhttps://git.k8s.io/community/contributors/devel/flaky-tests.md#filing-issues-for-flaky-tests\nan open issue\nhttps://github.com/google/cadvisor/issues?q=is:issue+is:open when you\nhit one in your PR.\nInstructions for interacting with me using PR comments are available here\nhttps://git.k8s.io/community/contributors/devel/pull-requests.md. If\nyou have questions or suggestions related to my behavior, please file an\nissue against the kubernetes/test-infra\nhttps://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:\nrepository. I understand the commands that are listed here\nhttps://go.k8s.io/bot-commands.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/pull/1868#issuecomment-358741630, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AA4BqzWf7091NVX_YdHqLbGAZF9E3Mkpks5tL5A9gaJpZM4Rgoj5\n.\n. /retest. /retest. /test pull-cadvisor-e2e. /retest. /test pull-cadvisor-e2e. /test pull-cadvisor-e2e. /test pull-cadvisor-e2e. @dashpole ssh works, something else is borked in the test code once the runner is up on the test VMs:\nW0119 01:45:53.908] E0119 01:45:53.908069    5651 runner.go:155] Failed to read host attributes: failed to get attributes - Get http://e2e-cadvisor-ubuntu-trusty:8080/api/v2.1/attributes: dial tcp: lookup e2e-cadvisor-ubuntu-trusty on 10.63.240.10:53: no such host\nhttps://k8s-gubernator.appspot.com/build/kubernetes-jenkins/pr-logs/pull/google_cadvisor/1868/pull-cadvisor-e2e/34/. /test pull-cadvisor-e2e. > GREEN!\n\n:tada: about time. nice work all :-)\nOnly 44 @zSys-bot comments later.... /test pull-cadvisor-e2e. should test here for fixed log upload paths after https://github.com/kubernetes/test-infra/pull/6625. It works! https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/pr-logs/pull/google_cadvisor/1880/pull-cadvisor-e2e/632/. ",
    "jcalderin": "I am having the same problem: the containerid=\"\\\" is consuming a big chunk of disk and can't discover what container is doing it due to... no name/id. ",
    "gogeof": "@dashpole yes, you're right, in info/v2, there also use glog package, and will cause the problem if want to v2. But if really want make cadvisor client to be independent, there must be some ways. So, what do you think?. I signed it!. @dashpole thanks for response. ok, let me try:\n1. I think here should not be fatal if some error occur, because it will cause the program to exit. And if the error just return, after catch that, I can try again or do something else. \n2. yes, I have set the log_dir in my project. But I think that if cadvisor client is just the client, maybe you can choose which log package to use as you want. The whole client package, except test code, just here use the glog package, so, I want return the error instead. \n3. if just return the error, #1875 fix.. ",
    "bhuvanchandra": "@dashpole, rebased on top of latest master.. Here is the error log:\n```\n$ make\n\n\nvetting code\nchecking go formatting\nchecking file boilerplate\nbuilding assets\nbuilding binaries\nbuilding cadvisor\ngo install net: open /usr/lib/go/pkg/linux_amd64/net.a: permission denied\nmake: *** [Makefile:43: build] Error 1\n```. \n\n",
    "jeroenhendricksen": "I can confirm this behavior. After cadvisor was failing, I restarted cadvisor, and all the container_start_time_seconds for the host were cadvisor was running were reset. The output differs from that of docker ps which I would not expect. It looks like this issue is related: https://github.com/google/cadvisor/issues/1607\nDocker version: 18.01.0-ce\nCadvisor version: v0.28.3 \nPrometheus: v1.7.2. ",
    "moreiramarti": "Does anyone know when it will be merged?. ",
    "miaoyq": "\n@miaoyq What is the data after this optimization?\n\nSee: https://github.com/containerd/cri-containerd/issues/587#issuecomment-363704921. Will do.. When containerd is restarted, the client connect will be break, then cadvisor will lose the link to containerd until restart the cadvisor process if we use sync.Once.. I think we should check whether the link is available when get the client.. Yeah, will do.. Done.. This PR remove grpc.FailOnNonTempDialError(true), so that gRPC can reconnect containerd sock when it fail the connection to the link.. ",
    "NaitikShah": "Hi @dashpole as you must have seen in the above docker-compose file, I have been specifying ports as:\nports:\n- 9080:9080\nBut still, it doesn't work on the specified port and as said I have another app running on 8080.\nThis strangely happens only when I run cadvisor through docker-compose and it works fine if I run it normally like creating/running a container out of the image.. I guess I found the issue, \nIt's mainly because I was using\nnetwork_mode: host. ",
    "zhulh200868": "@dashpole I know if I use the command like this :\nmount -o remount,rw '/sys/fs/cgroup'\nln -s /sys/fs/cgroup/cpu,cpuacct /sys/fs/cgroup/cpuacct,cpu\nBut I don't want to use it,and when I start the cadvisor use the commad like this:\ndocker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --volume=/sys/fs/cgroup/cpu,cpuacct:/sys/fs/cgroup/cpuacct,cpu:rw \\\n  --publish=8080:8080 \\\n  --name=cadvisor \\\n  google/cadvisor:latest\nIt raises the warning is \"System error: could not synchronise with container process \".\nI want to know why the version is v0.27.4 it can work and the latest can't work ?. ",
    "maurorappa": "@zhulh200868, if you remove '--volume=/sys:/sys:ro' and the latest works fine on RHEL 7.4 BUT without container stats. . @lindhor, @dashpole  I used successfully this workaround;\nI added a volume which basically does just the translation of the wrong path :\n--volume=/sys/fs/cgroup/cpu,cpuacct:/sys/fs/cgroup/cpuacct,cpu\nall containers are visible :). ",
    "lindhor": "I get this problem in all releases between 0.28.1 and 0.29.1 when running on CentOS 7.4.1708 (on host) with Docker version 1.12.6. Same problems with either the official google/cadvisor image based on Alpine 3.4 or my own image based on Alpine 3.5.2.\n0.27.3, 0.27.4 and 0.28.0 works well (have not tried earlier).\nWhen failing, log says:\nF0303 13:38:53.371627       1 cadvisor.go:157] Failed to start container manager: inotify_add_watch /sys/fs/cgroup/cpuacct,cpu: no such file or directory\nI start container with\ndocker run \\\n--privileged \\\n--publish=8080:8080 \\\n--volume=/:/rootfs:ro \\\n--volume=/var/run:/var/run:rw \\\n--volume=/sys:/sys:ro \\\n--volume=/var/lib/docker/:/var/lib/docker:ro \\\n--volume=/dev/disk/:/dev/disk:ro \\\n--name=cadvisor \\\ngoogle/cadvisor:latest\nStarting without --volume=/sys:/sys:ro works but as @maurorappa says, no containers are visible (just container images).\n. ",
    "dearkz": "@maurorappa thanks!! It can work for me. ",
    "adrien-f": "Thanks for the feedback, I'll look into it and get back to you !. Alright, I've update the pull request, pids should appear in all versions of the API, is that alright ?\nI've been a bit troubled with the recent changes with container/common and container/libcontainer, let me know if I forgot anything.. Is there something wrong with the e2e tests or is it my PR ?. I've nailed down the issue to cadvisor requesting stats for the root container / that resolves to /sys/fs/cgroup/. Trying to get its pids info in /vendor/github.com/opencontainers/runc/libcontainer/cgroups/fs/pids.go leads to an error since the pids.current file does not exist.\nFailed to update stats for container \"/\": failed to parse pids.current - open /sys/fs/cgroup/pids/pids.current: no such file or directory, continuing to push stats\nI'm still looking on how to avoid this behaviour.. I'm not sure I took the best approach here, let me know what you think :+1: . I'm sorry for the wait in this PR, I hope to be able to work on it soon :smile: . ",
    "hshahar": "HI tnx for your answer.\nI changed to v0.29.0\nThis is the log from the docker:\n 1 storagedriver.go:50] Caching stats in memory for 2m0s\nI0304 15:06:45.141205       1 manager.go:154] cAdvisor running in container: \"/sys/fs/cgroup/cpu,cpuacct\"\nI0304 15:06:45.215549       1 fs.go:142] Filesystem UUIDs: map[]\nI0304 15:06:45.215570       1 fs.go:143] Filesystem partitions: map[tmpfs:{mountpoint:/dev major:0 minor:103 fsType:tmpfs blockSize:0} /dev/xvda1:{mountpoint:/var/lib/docker/overlay2 major:202 minor:1 fsType:ext4 blockSize:0} /dev/xvdf:{mountpoint:/rootfs/gluster/data major:202 minor:80 fsType:xfs blockSize:0} shm:{mountpoint:/rootfs/var/lib/docker/containers/f375c3eae11fd9939170b0d93d485774408483ff19afc15cc32d76d6c0a998ab/shm major:0 minor:78 fsType:tmpfs blockSize:0}]\nI0304 15:06:45.219300       1 manager.go:227] Machine: {NumCores:4 CpuFrequency:2400068 MemoryCapacity:16825765888 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] MachineID:e9e76c2f45f5416390755da00379b451 SystemUUID:EC2ADA12-F744-1CF5-5CFD-5A9BDF79232C BootID:3954118f-f1d1-43e8-996a-7cc2aafe824e Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:103 Capacity:67108864 Type:vfs Inodes:2053926 HasInodes:true} {Device:/dev/xvda1 DeviceMajor:202 DeviceMinor:1 Capacity:20749852672 Type:vfs Inodes:2560000 HasInodes:true} {Device:/dev/xvdf DeviceMajor:202 DeviceMinor:80 Capacity:8579448832 Type:vfs Inodes:4194304 HasInodes:true} {Device:shm DeviceMajor:0 DeviceMinor:78 Capacity:67108864 Type:vfs Inodes:2053926 HasInodes:true} {Device:overlay DeviceMajor:0 DeviceMinor:42 Capacity:20749852672 Type:vfs Inodes:2560000 HasInodes:true}] DiskMap:map[202:0:{Name:xvda Major:202 Minor:0 Size:21474836480 Scheduler:none} 202:80:{Name:xvdf Major:202 Minor:80 Size:8589934592 Scheduler:none}] NetworkDevices:[{Name:eth0 MacAddress:0a:58:d9:18:bf:2a Speed:0 Mtu:9001}] Topology:[{Id:0 Memory:16825765888 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[1] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:2 Threads:[2] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:3 Threads:[3] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:31457280 Type:Unified Level:3}]}] CloudProvider:AWS InstanceType:t2.xlarge InstanceID:i-00368e8946e2112b8}\nI0304 15:06:45.220074       1 manager.go:233] Version: {KernelVersion:4.4.0-1049-aws ContainerOsVersion:Alpine Linux v3.4 DockerVersion:17.09.1-ce DockerAPIVersion:1.32 CadvisorVersion:v0.29.0 CadvisorRevision:aaaa65d}\nI0304 15:06:45.249195       1 factory.go:356] Registering Docker factory\nI0304 15:06:47.249563       1 factory.go:54] Registering systemd factory\nI0304 15:06:47.250974       1 factory.go:86] Registering Raw factory\nI0304 15:06:47.252178       1 manager.go:1205] Started watching for new ooms in manager\nW0304 15:06:47.252197       1 manager.go:340] Could not configure a source for OOM detection, disabling OOM events: open /dev/kmsg: no such file or directory\nI0304 15:06:47.252919       1 manager.go:356] Starting recovery of all containers\nI0304 15:06:47.332977       1 manager.go:361] Recovery completed\nI0304 15:06:47.440577       1 cadvisor.go:163] Starting cAdvisor version: v0.29.0-aaaa65d on port 8080\n2018/03/04 15:07:05 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial unix:///var/run/containerd/containerd.sock: timeout\"; Reconnecting to {unix:///var/run/containerd/containerd.sock \nIt looks like its not restarting but dont know what this err means? ( failed to ceate....). ",
    "delskev": "I got the same problem, here's the error message:\n2018/03/16 11:10:17 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial unix:///var/run/containerd/containerd.sock: timeout\"; Reconnecting to {unix:///var/run/containerd/containerd.sock }\nand here's my docker's version:\nDocker version 18.02.0-ce, build fc4de447b5. ",
    "mikeywaites": "Hey thanks for such a quick reply \ud83d\udc4f.\nI did actually have the --disable-metrics flag configured wrong in the entry point which I've now resolved.\nHere's what the output from docker inspect shows from entrypoint\n\"Entrypoint\": [\n                \"/usr/bin/cadvisor\",\n                \"--logtostderr\",\n                \"--housekeeping_interval\",\n                \"10s\",\n                \"--disable_metrics\",\n                \"disk,tcp,udp\"\n            ],\nSo it's come down ever so slightly.  It's not sitting around 27% cpu according to Prometheus.\nIs there anything else I can try or provide you with ? . Hey thanks for such a quick reply \ud83d\udc4f.\nI did actually have the --disable-metrics flag configured wrong in the entry point which I've now resolved.\nHere's what the output from docker inspect shows from entrypoint\n\"Entrypoint\": [\n                \"/usr/bin/cadvisor\",\n                \"--logtostderr\",\n                \"--housekeeping_interval\",\n                \"10s\",\n                \"--disable_metrics\",\n                \"disk,tcp,udp\"\n            ],\nSo it's come down ever so slightly.  It's not sitting around 27% cpu according to Prometheus.\nIs there anything else I can try or provide you with ? . OK.  Find the cause of this issue.  I'm an idiot and totally read the data wrong \ud83d\ude43\nThanks for the speedy response.. OK.  Find the cause of this issue.  I'm an idiot and totally read the data wrong \ud83d\ude43\nThanks for the speedy response.. ",
    "akomic": "They are using docker for all containers. Other than this there is no visible difference between containers. Would it make sense to have Cadvisor use \"CgroupParent\" attribute for each container?\nbash\ndocker run --help | grep cgroup-parent\n      --cgroup-parent string           Optional parent cgroup for the container\n. Fixed.\nInstead of \nsInfo, err := c.SubcontainersInfo(\"/docker\", &request)\nUse\nsInfo, err := c.SubcontainersInfo(\"/\", &request)\n. ",
    "Hurricanezwf": "branch: master\nLatest commit 08f0c2397cbca790a4db0f1212cb592cc88f6e26. ",
    "vinkdong": "@dashpole thank you for the reply\n I had a simple test:\nStarted a container and download a 500M file, then I find the disk io and write bytes is always 0...  but on the host using iotop find the process io is not 0. ",
    "IvanVlasic": "I signed it.. ",
    "wrfly": "@dashpole stand-alone. ",
    "mayconbeserra": "Hi @dashpole I started having the same issue, but it was working fine yesterday.\nI just removed all the cadvisor containers from my cluster and redeployed again, and then I started getting that error.\nCould you highlight why cadvisor can't stop working? I still have the same version Docker version when it was working before. Docker version - 17.06.2-ee-6, build e75fdb8\ncadvisor compose:\ncadvisor:\n    image: google/cadvisor:v0.29.0\n    command:\n      - '--port=8484'\n    ports:\n      - target: 8484\n        published: 8484\n    networks:\n      - net\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock,readonly\n      - /var/run:/var/run:rw\n      - /:/rootfs:ro\n      - /sys:/sys:ro\n      - /var/lib/docker:/var/lib/docker:ro\n      - /dev/disk/:/dev/disk:ro\n    deploy:\n      mode: global\n      restart_policy:\n        condition: on-failure\n        delay: 10s\n        max_attempts: 20\n      resources:\n        limits:\n          # cpus: '0.50'\n          memory: 64M\n        reservations:\n          # cpus: '0.50'\n          memory: 32M. ",
    "jouve": "Hi,\nsame as @xgt001 , cadvisor starts and logs up to this:\nI0418 12:41:07.241078       1 manager.go:233] Version: {KernelVersion:3.10.0-693.21.1.el7.x86_64 ContainerOsVersion:Alpine Linux v3.4 DockerVersion:17.12.1-ce DockerAPIVersion:1.35 CadvisorVersion:v0.29.0 CadvisorRevision:aaaa65d}\nI0418 12:41:07.270718       1 factory.go:356] Registering Docker factory\nI0418 12:41:09.273406       1 factory.go:54] Registering systemd factory\nI0418 12:41:09.277547       1 factory.go:86] Registering Raw factory\nI0418 12:41:09.280324       1 manager.go:1205] Started watching for new ooms in manager\nI0418 12:41:09.286467       1 manager.go:356] Starting recovery of all containers\nthen if I try \"curl localhost:8080\", it fails:\n$ curl localhost:8080\ncurl: (56) Recv failure: Connection reset by peer\nand the following error appears in the logs:\n2018/04/18 12:41:27 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial unix:///var/run/containerd/containerd.sock: timeout\"; Reconnecting to {unix:///var/run/containerd/containerd.sock <nil>}. ",
    "Scukerman": "Docker version: 17.12.1-ce\nCadvisor image: google/cadvisor:v0.29.0\nSame here.\n2018/05/10 15:31:19 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial unix:///var/run/containerd/containerd.sock: timeout\"; Reconnecting to {unix:///var/run/containerd/containerd.sock <nil>}. ",
    "ameyrk18": "Hey @xgt001 did you guys managed to resolve this issue? \nI am also facing the same problem. When bind cadvisor with systemd daemon the container never comes up and stuck at the \"Starting recovery of all containers\". However the container starts up perfectly fine if I execute outside systemd. \nusr/bin/docker run \\\n  --name cadvisor \\\n  --restart=always \\\n  --memory=256m \\\n  --detach=true \\\n  --pid=host \\\n  --privileged=true \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --volume=/dev/disk/:/dev/disk:ro \\\n  --volume=/cgroup:/cgroup:ro \\\n  --publish=8572:8080 \\\n  google/cadvisor:v0.28.0. @xgt001  Tried with 0.29.0 its same issue. Container never starts up when I start with systemd sometimes my docker daemon goes defunct. \n. @xgt001 how are you starting? Systemd or via docker compose or standalone? . Tried v0.30.0 same issues. @dashpole do you want me to share some logs?\n. Hi @dashpole \nWhat I feel is there something bundled with docker 1.13 version which is breaking this. Yes the package is available via yum please see the below. \nyum list cadvisor\nLoaded plugins: fastestmirror\nLoading mirror speeds from cached hostfile\n * base: ftp.heanet.ie\n * epel: s3-mirror-eu-west-1.fedoraproject.org\n * extras: ftp.heanet.ie\n * updates: ftp.heanet.ie\nAvailable Packages\ncadvisor.x86_64                                                                              0.4.1-0.3.git6906a8ce.el7                                                                              extras\nThanks,\nAmey\n. @waterplily \ntry this role if your interested in host only installation. https://github.com/ameyrk18/ansible-cAdvisor\nThanks,\nAmey\n. @robinwangrubin are you trying to start cadvisor with systemd unit?. @robinwangrubin  please check your wrong logs. Your Kernel version isn't supported. \nE0607 03:16:00.481139       1 factory.go:340] devicemapper filesystem stats will not be reported: RHEL/Centos 7.x kernel version 3.10.0-366 or later is required to use thin_ls - you have \"3.10.0-229.el7.x86_64\"\n. Thanks @dashpole . I will try to configure cadvisor to restart if docker daemon fails. \nHowever, I believe we should have some notes updated mentioning cadvisor excepts docker daemon to be running (for host only installation)\nBest Regards,\nAmey. ",
    "xgt001": "@ameyrk18 I am sorry but your cadvisor version is not 0.29.x where I could reproduce it on my side. Could you try the same with 0.29.1 as well if possible?\n. ",
    "anthraxn8b": "For me still the same problem - and restarting the deamon or the system should be no option:\nMy compose-file snipped (running in swarm):\ncadvisor:\n  image: google/cadvisor:v0.29.0\n  privileged: true\n  networks:\n    - mynet\n  ports:\n    - \"9902:8080\"\n  volumes:\n    - /:/rootfs:ro\n    - /var/run:/var/run:rw\n    - /sys:/sys:ro # removing does not help\n    - /var/lib/docker/:/var/lib/docker:ro\n  #- /cgroup:/sys/fs/cgroup:ro # does not help\n    - /dev/disk/:/dev/disk:ro\n  #- /dev/mapper:/dev/mapper:ro # does not help\n  deploy:\n    mode: global\n    endpoint_mode: vip\n    resources:\n      limits:\n        memory: 256M\n      reservations:\n        memory: 64M\n  healthcheck:\n    test: wget --quiet --spider http://localhost:8080\n    retries:       4\n    interval:     30s\n    timeout:      25s\n    start_period: 60s\n\ndocker --version:\nDocker version 18.03.1-ce, build 9ee9f40\ncAdvisor log entry:\nI0706 13:07:35.909949       1 storagedriver.go:50] Caching stats in memory for 2m0s,\nI0706 13:07:35.911174       1 manager.go:154] cAdvisor running in container: \"/sys/fs/cgroup/cpu,cpuacct\",\nI0706 13:07:35.975468       1 fs.go:142] Filesystem UUIDs: ...,\nI0706 13:07:35.975581       1 fs.go:143] Filesystem partitions: ...,\nI0706 13:07:35.983029       1 manager.go:227] Machine: ...,\nI0706 13:07:35.984454       1 manager.go:233] Version: {KernelVersion:3.10.0-693.21.1.el7.x86_64 ContainerOsVersion:Alpine Linux v3.4 DockerVersion:18.03.1-ce DockerAPIVersion:1.37 CadvisorVersion:v0.29.0 CadvisorRevision:aaaa65d},\nE0706 13:07:36.020107       1 factory.go:340] devicemapper filesystem stats will not be reported: usage of thin_ls is disabled to preserve iops,\nI0706 13:07:36.020799       1 factory.go:356] Registering Docker factory,\nI0706 13:07:38.021227       1 factory.go:54] Registering systemd factory,\nI0706 13:07:38.022976       1 factory.go:86] Registering Raw factory,\nI0706 13:07:38.024327       1 manager.go:1205] Started watching for new ooms in manager,\nW0706 13:07:38.024367       1 manager.go:340] Could not configure a source for OOM detection, disabling OOM events: open /dev/kmsg: no such file or directory,\nI0706 13:07:38.029194       1 manager.go:356] Starting recovery of all containers,\nI0706 13:07:38.203094       1 manager.go:361] Recovery completed,\nI0706 13:07:38.400871       1 cadvisor.go:163] Starting cAdvisor version: v0.29.0-aaaa65d on port 8080,\n2018/07/06 13:07:56 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial unix:///var/run/containerd/containerd.sock: timeout\"; Reconnecting to {unix:///var/run/containerd/containerd.sock <nil>},\n\nAny ideas?. ",
    "ediri": "i can confirm the problem @anthraxn8b explained. We have the same issue here. \n. ",
    "pipech": "I got this too but using v0.28.5 (newer than v0.29.x according to https://hub.docker.com/r/google/cadvisor/tags) fixed the problem.. ",
    "profhase": "There is no way to do conditionals (afaik). Thanks for the statement, unfortunately it cannot work . I will close this issue, apparently the limit_size is by design. Thank you for your help. ",
    "itnilesh": "Thanks for the reply .. \nI am able to collect the GPU metrics without k8s \nHere is c-advisor docker image \nsudo nvidia-docker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --volume=/dev/disk/:/dev/disk:ro \\\n  --volume=/var/lib/nvidia-docker/volumes/nvidia_driver/387.26:/usr/local/nvidia \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  --device /dev/nvidia0:/dev/nvidia0 \\\n  --device /dev/nvidiactl:/dev/nvidiactl \\\n  --device /dev/nvidia-uvm:/dev/nvidia-uvm \\\n  -e LD_LIBRARY_PATH=/usr/local/nvidia/lib64 \\\n  google/cadvisor:latest\nEC2 instance should be  \"nvidia cuda\"  base image \nGPU code ran on the same machine where c-advisor docker container  is running\nI have modified the code to run in a loop to create some load \nyou will need cuda and conda installed \nhttps://github.com/siddharthsharmanv/cudacasts/blob/master/InstallingCUDAPython/VectorAdd.py\nSample output for GPU \nCall http://{HOST}:8080/metrics\n```\nHELP cadvisor_version_info A metric with a constant '1' value labeled by kernel version, OS version, docker version, cadvisor version & cadvisor revision.\nTYPE cadvisor_version_info gauge\ncadvisor_version_info{cadvisorRevision=\"1e567c2\",cadvisorVersion=\"v0.28.3\",dockerVersion=\"1.13.1\",kernelVersion=\"4.4.111-k8s\",osVersion=\"Alpine Linux v3.4\"} 1\nHELP container_accelerator_duty_cycle Percent of time over the past sample period during which the accelerator was actively processing.\nTYPE container_accelerator_duty_cycle gauge\ncontainer_accelerator_duty_cycle{acc_id=\"GPU-642094d0-7acf-a6cc-0e15-790e1d269839\",id=\"/docker/6ec56ca39196ff3e013be895fc9f6d46e9956fbd373ad559afee11a340537b6f\",image=\"google/cadvisor:latest\",make=\"nvidia\",model=\"Tesla K80\",name=\"cadvisor\"} 0\nHELP container_accelerator_memory_total_bytes Total accelerator memory.\nTYPE container_accelerator_memory_total_bytes gauge\ncontainer_accelerator_memory_total_bytes{acc_id=\"GPU-642094d0-7acf-a6cc-0e15-790e1d269839\",id=\"/docker/6ec56ca39196ff3e013be895fc9f6d46e9956fbd373ad559afee11a340537b6f\",image=\"google/cadvisor:latest\",make=\"nvidia\",model=\"Tesla K80\",name=\"cadvisor\"} 1.1995578368e+10\nHELP container_accelerator_memory_used_bytes Total accelerator memory allocated.\nTYPE container_accelerator_memory_used_bytes gauge\ncontainer_accelerator_memory_used_bytes{acc_id=\"GPU-642094d0-7acf-a6cc-0e15-790e1d269839\",id=\"/docker/6ec56ca39196ff3e013be895fc9f6d46e9956fbd373ad559afee11a340537b6f\",image=\"google/cadvisor:latest\",make=\"nvidia\",model=\"Tesla K80\",name=\"cadvisor\"} 1.2058624e+07\nHELP container_cpu_load_average_10s Value of container cpu load average over the last 10 seconds.\n``. I checked k8s versions with Godeps.json in [https://github.com/kubernetes/kubernetes ](https://github.com/kubernetes/kubernetes), I see that GPU metrics support is  k8s 1.9 version onward. \nSo I checked\"ImportPath\": \"github.com/google/cadvisor/accelerators\"` only 1.9 onward accelerator support is there.\nCan somebody just confirm this?\n. I have also verified with k8s 1.9 version and heapster 1.5.1 version,  cluster deployed in gcp.\n. GPU metrics is currently not supported on UI .\nplease refer https://github.com/google/cadvisor/issues/1912. /close. ",
    "Starnop": "@Random-Liu There's a little question, why changes the address from '/var/run/containerd/containerd.sock' to '/run/containerd/containerd.sock' which will lead to compatibility issues. ^_^. cc @Random-Liu PTAL. Thanks ;). ",
    "answer3y": "\nGPU metrics is currently not supported on UI .\nplease refer #1912\n\nhello, @itnilesh  @dashpole  can I  ask, why cadvisor not support gpu info on UI for so long time?\nIs there any hiden problem about this issue\nBecause we just want develop for this Features. ",
    "spirrello": "We're browsing directly to our K8s node like this and I don't see anything stating disk utilization.  Do I need to run cAdvisor as a daemonset to get a newer version perhaps to see the disk utilization?\nAlso, we're using K8s 1.5.4 - is this an issue?\nhttp://nodeip/4194/containers/docker/cfa34ca5d3f2731564fa3b855e48b903c896c440b789e236a50f2dc1f64f243c. Okay I'll try that.  Thanks.. Got it working thanks.. ",
    "serathius": "@andyxning is this integrated into k8s?. ",
    "mavidser": "I signed it!. ",
    "gauravkarens": "The UI is not opening on the 8080 port also can u suggest is there an API which can be used to stock the metric data coming from cadvisor. I need to do an analysis of what resource utilization is associated with per container basis for a period of one day and to do that I will have to have record of the metric data produced by cadvisor so how do I do that. ",
    "Eric-LiuGang": "@dashpole \nHi, I cacth this problem too.\nThe container not running and can't start the container.\nOS : Centos 7.4.1 docker version : 1.13.1\nAnd Install docker by yum as default.. When I run cadvisor with\n\nsudo docker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:rw \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  google/cadvisor:latest\n\nThe container run up about 2 seconds  and then  stoped,so port 8080 is not up\nThe log as follows:\nApr 27 10:44:48 docker2.gang_test dockerd-current[6646]: I0427 02:44:48.700521       1 storagedriver.go:50] Caching stats in memory for 2m0s\nApr 27 10:44:48 docker2.gang_test dockerd-current[6646]: I0427 02:44:48.701273       1 manager.go:151] cAdvisor running in container: \"/sys/fs/cgroup/cpuacct,cpu\"\nApr 27 10:44:48 docker2.gang_test dockerd-current[6646]: time=\"2018-04-27T10:44:48.723316610+08:00\" level=warning msg=\"failed to retrieve docker-runc version: unknown output format: runc version 1.0.0-rc2\\nspec: 1.0.0-rc2-dev\\n\"\nApr 27 10:44:48 docker2.gang_test dockerd-current[6646]: time=\"2018-04-27T10:44:48.723412514+08:00\" level=warning msg=\"failed to retrieve docker-init version\"\nApr 27 10:44:48 docker2.gang_test dockerd-current[6646]: time=\"2018-04-27T10:44:48.748355750+08:00\" level=warning msg=\"failed to retrieve docker-runc version: unknown output format: runc version 1.0.0-rc2\\nspec: 1.0.0-rc2-dev\\n\"\nApr 27 10:44:48 docker2.gang_test dockerd-current[6646]: time=\"2018-04-27T10:44:48.748467364+08:00\" level=warning msg=\"failed to retrieve docker-init version\"\nApr 27 10:44:48 docker2.gang_test dockerd-current[6646]: I0427 02:44:48.753382       1 fs.go:139] Filesystem UUIDs: map[]\nApr 27 10:44:48 docker2.gang_test dockerd-current[6646]: I0427 02:44:48.753428       1 fs.go:140] Filesystem partitions: map[/dev/mapper/docker-253:0-50554515-850de0dbc61dcb167094b4cd7e0f60c4dec47c37cf2e2479fb245a92cfae9331:{mountpoint:/ major:253 minor:3 fsType:xfs blockSize:0} tmpfs:{mountpoint:/dev major:0 minor:39 fsType:tmpfs blockSize:0} /dev/mapper/cl-root:{mountpoint:/var/lib/docker major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/rootfs/boot major:8 minor:1 fsType:xfs blockSize:0} shm:{mountpoint:/dev/shm major:0 minor:36 fsType:tmpfs blockSize:0}]\nApr 27 10:44:48 docker2.gang_test dockerd-current[6646]: I0427 02:44:48.760924       1 manager.go:225] Machine: {NumCores:4 CpuFrequency:2400085 MemoryCapacity:3974868992 HugePages:[{PageSize:2048 NumPages:0}] MachineID:5bb269792be0410d8e8cf4bcc561b3c5 SystemUUID:423735B7-EB95-DE79-C03D-EDA16581FDD0 BootID:8ae20ada-81dd-4d42-84c2-dbd650375df7 Filesystems:[{Device:shm DeviceMajor:0 DeviceMinor:36 Capacity:67108864 Type:vfs Inodes:485213 HasInodes:true} {Device:/dev/mapper/docker-253:0-50554515-850de0dbc61dcb167094b4cd7e0f60c4dec47c37cf2e2479fb245a92cfae9331 DeviceMajor:253 DeviceMinor:3 Capacity:10725883904 Type:vfs Inodes:5242368 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:39 Capacity:1987432448 Type:vfs Inodes:485213 HasInodes:true} {Device:/dev/mapper/cl-root DeviceMajor:253 DeviceMinor:0 Capacity:18238930944 Type:vfs Inodes:8910848 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:1063256064 Type:vfsInodes:524288 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:18249416704 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 253:2:{Name:dm-2 Major:253 Minor:2 Size:107374182400 Scheduler:none} 253:3:{Name:dm-3 Major:253 Minor:3 Size:10737418240 Scheduler:none} 2:0:{Name:fd0 Major:2 Minor:0 Size:4096 Scheduler:deadline} 8:0:{Name:sda Major:8 Minor:0 Size:21474836480 Scheduler:deadline}] NetworkDevices:[{Name:ens160 MacAddress:00:50:56:b7:49:9c Speed:10000 Mtu:1500} {Name:ens192 MacAddress:00:50:56:b7:75:05 Speed:10000 Mtu:1500}] Topology:[{Id:0 Memory:4294500352 Cores:[{Id:0 Threads:[0] Caches:[]} {Id:1 Threads:[1] Caches:[]}] Caches:[{Size:12582912 Type:Unified Level:3}]} {Id:1 Memory:0 Cores:[{Id:0 Threads:[2] Caches:[]} {Id:1 Threads:[3] Caches:[]}] Caches:[{Size:12582912 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}\nApr 27 10:44:48 docker2.gang_test dockerd-current[6646]: I0427 02:44:48.762428       1 manager.go:231] Version: {KernelVersion:3.10.0-693.21.1.el7.x86_64 ContainerOsVersion:Alpine Linux v3.4 DockerVersion:1.13.1 DockerAPIVersion:1.26 CadvisorVersion:v0.28.3 CadvisorRevision:1e567c2}\nApr 27 10:44:48 docker2.gang_test dockerd-current[6646]: time=\"2018-04-27T10:44:48.781672520+08:00\" level=warning msg=\"failed to retrieve docker-runc version: unknown output format: runc version 1.0.0-rc2\\nspec: 1.0.0-rc2-dev\\n\"\nApr 27 10:44:48 docker2.gang_test dockerd-current[6646]: time=\"2018-04-27T10:44:48.781791536+08:00\" level=warning msg=\"failed to retrieve docker-init version\"\nApr 27 10:44:48 docker2.gang_test dockerd-current[6646]: E0427 02:44:48.786038       1 factory.go:340] devicemapper filesystem stats will not be reported: usage of thin_ls is disabled to preserve iops\nApr 27 10:44:48 docker2.gang_test dockerd-current[6646]: I0427 02:44:48.787405       1 factory.go:356] Registering Docker factory\nApr 27 10:44:50 docker2.gang_test dockerd-current[6646]: I0427 02:44:50.788041       1 factory.go:54] Registering systemd factory\nApr 27 10:44:50 docker2.gang_test dockerd-current[6646]: I0427 02:44:50.788776       1 factory.go:86] Registering Raw factory\nApr 27 10:44:50 docker2.gang_test dockerd-current[6646]: I0427 02:44:50.789363       1 manager.go:1178] Started watching for new ooms in manager\nApr 27 10:44:50 docker2.gang_test dockerd-current[6646]: W0427 02:44:50.789410       1 manager.go:313] Could not configure a source for OOM detection, disabling OOM events: open /dev/kmsg: no such file or directory\nApr 27 10:44:50 docker2.gang_test dockerd-current[6646]: I0427 02:44:50.794910       1 manager.go:329] Starting recovery of all containers\nApr 27 10:44:50 docker2.gang_test dockerd-current[6646]: I0427 02:44:50.877151       1 manager.go:334] Recovery completed\nApr 27 10:44:50 docker2.gang_test dockerd-current[6646]: F0427 02:44:50.908107       1 cadvisor.go:156] Failed to start container manager: inotify_add_watch /sys/fs/cgroup/cpuacct,cpu: no such file or directory\nApr 27 10:44:50 docker2.gang_test oci-systemd-hook[6916]: systemdhook : 408094c2ab7d: Skipping as container command is /usr/bin/cadvisor, not init or systemd\nApr 27 10:44:50 docker2.gang_test oci-umount[6917]: umounthook : 408094c2ab7d: only runs in prestart stage, ignoring\nApr 27 10:44:50 docker2.gang_test dockerd-current[6646]: time=\"2018-04-27T10:44:50.979845203+08:00\" level=error msg=\"containerd: deleting container\" error=\"exit status 1: \\\"container 408094c2ab7d550bda56d583f9541a5af8e482ba021b993cc2a7d5daf6bf0842 does not exist\\none or more of the container deletions failed\\n\\\"\"\nApr 27 10:44:51 docker2.gang_test kernel: docker0: port 1(veth0f40d62) entered disabled state\nApr 27 10:44:51 docker2.gang_test NetworkManager[647]:   [1524797091.0181] manager: (veth2a03b9c): new Veth device (/org/freedesktop/NetworkManager/Devices/28)\nApr 27 10:44:51 docker2.gang_test kernel: docker0: port 1(veth0f40d62) entered disabled state\nApr 27 10:44:51 docker2.gang_test kernel: device veth0f40d62 left promiscuous mode\nApr 27 10:44:51 docker2.gang_test kernel: docker0: port 1(veth0f40d62) entered disabled state\nApr 27 10:44:51 docker2.gang_test NetworkManager[647]:   [1524797091.0314] device (veth0f40d62): released from master device docker0\nApr 27 10:44:51 docker2.gang_test kernel: XFS (dm-3): Unmounting Filesystem\nApr 27 10:44:51 docker2.gang_test dockerd-current[6646]: time=\"2018-04-27T10:44:51.102927017+08:00\" level=warning msg=\"408094c2ab7d550bda56d583f9541a5af8e482ba021b993cc2a7d5daf6bf0842 cleanup: failed to unmount secrets: invalid argument\"\nThe docker service start status:\n /usr/bin/dockerd-current --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current --default-runtime=docker-runc --exec-opt native.cgroupdriver=systemd --userland-proxy-path=/usr/libexec/docker/docker-proxy-current --seccomp-profile=/etc/docker/seccomp.json --selinux-enabled --log-driver=journald --signature-verification=false\nIs \u201cselinux enabled\u201d OK?. ",
    "jwmarshall": "I have a similar issue where cadvisor never completes the \"Starting recovery of all containers\" phase. According to the logs \"Recovery completed\" event is never emitted and the web UI port is never opened. I've strace'd cadvisor and there is an overwhelming number of futex errors all pointing to the same address.\nfutex(0x13ae8b0, FUTEX_WAIT, 0, NULL)   = -1 EAGAIN (Resource temporarily unavailable)\nThe issue happens running in docker as well as via systemd on the host. It was working yesterday without any issues, but I suspect that it's something related to Docker. Unfortunately I cannot restart the docker daemon nor the host at this time to see if it fixes the issue. AFAICT there are no stuck or zombie containers. Cadvisor has been running for over three hours now with no change. I'm trying to replicate the issue on another system that I can restart if necessary.\nNot sure why cadvisor fails to report its version number in the log, it was installed via apt, but maybe this will help:\nVersion: {KernelVersion:4.15.0-20-generic ContainerOsVersion:Ubuntu 18.04 LTS DockerVersion:17.12.1-ce DockerAPIVersion:1.35 CadvisorVersion: CadvisorRevision:}\napt-cache says its cadvisor version 0.2.71\n. Pointing cadvisor at an invalid socket file for docker causes it to start immediately, though without docker statistics obviously. The futex errors remain so maybe theyre unrelated.. ",
    "mariusbld": "sure, thank you!. I signed it!. ",
    "jejer": "It happened in our cluster. \ndocker graph changed to /data0/docker.\ncontainer / pod name lost in :10255/metrics/cadvisor\nAnd lot of kubelet logs:\nJul 10 08:51:54 j-daily-e2e-control-worker-edge-storage-01 kubelet[2129]: E0710 08:51:54.776667    2129 manager.go:1130] Failed to create existing container: /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-podd66a596523b1edaba6565ad02fbf61d1.slice/docker-1f43e4af14b5bb32a708f5a49546ee69ceccf635db6a494c0b934380e563a11f.scope: failed to identify the read-write layer ID for container \"1f43e4af14b5bb32a708f5a49546ee69ceccf635db6a494c0b934380e563a11f\". - open /var/lib/docker/image/overlay2/layerdb/mounts/1f43e4af14b5bb32a708f5a49546ee69ceccf635db6a494c0b934380e563a11f/mount-id: no such file or directory\nkubernetes 1.10.2\nRedhat docker 1.13.1\nStorage Driver: overlay2\nKernel Version: 3.10.0-693.21.1.el7.x86_64\nOperating System: Red Hat Enterprise Linux Server 7.4 (Maipo)\n. seems a race condition between kubelet start and docker start.\nI can reproduce by manually start kubelet and start docker.\n--docker-root is deprecated, if docker daemon not running, kubelet failed to read from docker info, will kubelet use default docker graph to init cadvisor?. @bboreham ,\nkubelet --help print --docker-root deprecated. and in the code: https://github.com/google/cadvisor/blob/f7576313bd1df261f90bca93ab91ba019eb823d5/container/docker/factory.go#L62\nI will try set --docker-root, thank you!. added --docker-root for kubelet and never saw this issue again.. ",
    "MoSunDay": "Thank you very much, but why is it disabled by default? Is it because of functional defects?. Oh, I see. Thank you very much for your answer. . ",
    "bnounours": "Hello,\nI have the same issue. Cadvisor try to get data from zfs but fails and use lots of CPU. I get this log all the time for each dataset : \nE0721 15:30:16.615631       1 fs.go:418] Stat fs failed. Error: exit status 1: \"/usr/sbin/zfs zfs list -Hp -o name,origin,used,available,mountpoint,compression,type,volsize,quota,referenced,written,logicalused,usedbydataset data/docker/lib/1e7524fa964b400881d88c770d67e72c70157cc4d729e92e1b96c13791634a05\" => cannot open 'data/docker/lib/1e7524fa964b400881d88c770d67e72c70157cc4d729e92e1b96c13791634a05': dataset does not exist\nCadvisor himself tell me the docker use ~ 30% CPU\n\nHere is my docker-compose file\ncontainer-exporter:\n    image: google/cadvisor\n    privileged: true\n    command:\n      - \"--port=9104\"\n    devices:\n      - \"/dev/zfs\"\n    volumes:\n      - /:/rootfs:ro\n      - /cgroup:/cgroup:ro\n      - /dev/zfs:/host/dev/zfs:ro\n      - /proc:/host/proc:ro\n      - /sys:/sys:ro\n      - /var/lib/docker/:/var/lib/docker:ro\n      - /var/run:/var/run:rw\n    ports:\n      - 9104:9104\n. ",
    "ianabc": "I'm facing this as well. I tried the following\n```\ndiff --git a/container/docker/factory.go b/container/docker/factory.go\nindex 9f544ee..78f8cd6 100644\n--- a/container/docker/factory.go\n+++ b/container/docker/factory.go\n@@ -346,12 +346,14 @@ func Register(factory info.MachineInfoFactory, fsInfo fs.FsInfo, includedMetrics\n        }\n    var zfsWatcher *zfs.ZfsWatcher\n\n\nif storageDriver(dockerInfo.Driver) == zfsStorageDriver {\nzfsWatcher, err = startZfsWatcher(dockerInfo)\nif err != nil {\nglog.Errorf(\"zfs filesystem stats will not be reported: %v\", err)\n}\n}\nif includedMetrics.Has(container.DiskUsageMetrics) {\nif storageDriver(dockerInfo.Driver) == zfsStorageDriver {\nzfsWatcher, err = startZfsWatcher(dockerInfo)\nif err != nil {\nglog.Errorf(\"zfs filesystem stats will not be reported: %v\", err)\n}\n}\n}\n``\nbut I guess there I'm just avoiding the usage check so unsurprisingly, it didn't help. I also tried removing the-v /:/rootfs:ro`, which stops the zfs filesystems from showing up in the logs for cadvisor, but the CPU usage is still really high. Currently this machine has a lot of zfs filesystems (~3000), but is only running two containers and only one of them is using a single zfs filesystem. I have no idea what is going on.\n\nThere are ways of making zfs list faster, depending on what information you are looking to list e.g. https://github.com/zfsonlinux/zfs-auto-snapshot/pull/82 but I'm not sure that's the issue.\nEDIT: If it helps, I turned up debugging and see lots of messages like\n1 factory.go:112] Factory \"systemd\" can handle container \"/system.slice/tank-fs-sample334.mount\", but ignoring.\ncorresponding with the filesystem /tank/fs/sample334, but that filesystem is nothing to do with the running containers. I'm trying ways to hide all of the /tank/fs filesystems from cadvisor.. ",
    "hunkjun": "in swarm , It is ok follow!!\n  command:\n      - '-disable_metrics=sched'\n      - '-disable_metrics=tcp'\n      - '-disable_metrics=udp'\n      - '-disable_metrics=process'\n      - '-disable_metrics=disk'. ",
    "rsevilla87": "I signed it!. ",
    "knightXun": "maybe we can use some cache. ",
    "waterplily": "Hi,\nI'm seeing this issue as well.  I'm trying to install onto Centos 7.4 with docker-ce version 18.03.1.  I know Centos 7.5 was released recently, but I'm not ready to upgrade to that.  The cadvisor package version supplied with Centos 7.5 is the same as 7.4 - cadvisor.x86_64 0.4.1-0.3.git6906a8ce.el7 \nTrying the --skip-broken workaround still results in nothing getting installed because of dependency issues. \n\nRegards,\nLily . @dashpole \nThank you.  I wasn't aware of that.  I'll try another method of using cadvisor. . I just wanted to provide a further update about the cadvisor RPM, so that the information is visible to anyone that comes across this posted issue.  I found a link that named the authors of the rpm, so I emailed them and asked if the package was going to be maintained further.  I received a response and the answer was that the package has been deprecated and there are no plans to update it further on the RHEL side.\nI hope that's useful. . ",
    "sashankreddya": "I do see some of the blkio stats are being reported as \"DiskIOstats\" \nhttps://github.com/google/cadvisor/blob/master/container/libcontainer/handler.go#L486. Sure will address the nits. Thanks David. type alias is introduced in go.1.9(https://github.com/golang/proposal/blob/master/design/18130-type-alias.md). @dashpole : So this means that we don't need to commit this change, but update the jenkins go version ?. sure David. Will let you know by EOD tomorrow.. > @dims thanks for catching this.\n\nIll let @sashankreddya review before I merge. I don't think this is worth a cherrypick, as it is mostly to reduce testing log volume...\n\nIdeally we need to have and opt-in for every container type as mentioned in https://github.com/google/cadvisor/issues/1483 in the longterm\n. I am fine with the short-term solution too.. @dashpole . > can you rebase on-top of #2081 to fix the test failures?\nDone. > when is this going to be released ? I am using --> https://github.com/google/cadvisor/releases/download/ to get the latest and seems like it doesnt have that metric being exported yet\n\n@dashpole\n@sashankreddya\n\nHi @humayunjamal: These metrics were added pretty recently. Hopefully 0.31.1 should have it. @dashpole: Can give more context on the release version. . @codnam007 : Looks like v0.32.0 has these metrics . By default they are disabled, you need to enable them via \"disable_metrics\" metrics flag. By default the list of disabled includes \"'disk', 'network', 'tcp', 'udp', 'percpu', 'sched', 'process'\". Default disabled_metrics='disk,network,tcp,udp,percpu,sched,process'. \nRun some thing like below to see \"process\" metrics.\nsudo ./cadvisor --disable_metrics='disk,network,tcp,udp,percpu,sched'. @orisano / @dashpole : Looks like this commit broke listing \"mesos\" containers. The mesos containers cgroup path looks like a docker path like\n\"/sys/fs/cgroup//mesos\".\nI have verified without including this commit things work fine. \nI am debugging it, you guys could share any inputs you have.. no symlinks are used and yes it should be the cgroups path is \"/sys/fs/cgroup//mesos\"\nI could see the error thrown by \ndirents, err := godirwalk.ReadDirents(dirpath, buf)\nin listDirectories when \"updateSubcontainers\" tries to fetch the subcontainers.\nError\nerror(*github.com/google/cadvisor/vendor/github.com/pkg/errors.withStack) *{\n    error: error(*github.com/google/cadvisor/vendor/github.com/pkg/errors.withMessage) *{\n        cause: error(*os.PathError) ...,\n        msg: \"cannot Open\",},\n    stack: *github.com/google/cadvisor/vendor/github.com/pkg/errors.stack len: 15, cap: 32, [11136028,11135472,11164142,11163830,11167113,18958881,19150681,19113714,19174192,19183722,19178323,19514576,19512011,19484179,4585041],}\n(dlv) n. @orisano : The error is \"cannot Open: open /sys/fs/cgroup/cpuset/mesos/59fe1f25-abe1-4cbb-8be0-441bab19a5d5: no such file or directory\".\nWe don't have some of the cgroups enabled by default. So probably you should ignore \"directories\" not present.\n. @dashpole : Addressed the comments. @dashpole: Addressed the comments. @tongbinxiang : is the mesos handler registered ? I don't see it in the logs for a message \"Registering mesos factory\" ?. @tongbinxiang : I am able to see the message when I start with the latest clone of cadvisor with \"v=4\". \nCouple of things to make sure\n1)  Check if the mesosAgentAdress is correct as in here. The default is as present here\n2) Check if you are not seeing any of these error conditions. I referred to the one in \"https://github.com/google/cadvisor/blob/master/container/containerd/client_test.go#L24\". Isnt it the preferred way ?\n. Addressed it. I added it as we might add in upcoming PR's. Let me know if you want me to add it later.. These are defined now in the mesos-go library. We don't need to look for multiple versions especially with the info needed with state and container endpoints of mesos. \nAlso as discussed, once the wrappers defined here are available in opensource mesos go client library, then I can remove these from here.. done. done. Done. done. done. make doesn't complain any error with fresh checkout of the repo. Not sure why this is failing. Yes as part of factory, we need to see if the container info object  can be created with the given id.. Addressed it.. its conflicting with the container package that was imported. So need to rename the local variable.. By default not all cgroups are enabled for different types of containerizer platforms. Most commonly the \"memory\" and \"cpu\" cgroups are enabled. \nFor Docker most of the cgroups are enabled.\nFor Mesos we have \"cpu\", \"memory\" and \"disk\" as enabled default. Rest are sharing with the host cgroups.\n. Done. Done. ",
    "niedbalski": "I signed it!. @dashpole any comments on this? . /retest. ",
    "ElfoLiNk": "@dashpole when i merged back in master i wrongly removed curl needed by the health check \ud83d\ude1e Opened https://github.com/google/cadvisor/pull/2120. @dashpole when i merged back in master i wrongly removed curl needed by the health check \ud83d\ude1e Opened https://github.com/google/cadvisor/pull/2120. Wrongly added. Just removed . ",
    "jaloren": "Given those constraints that makes sense. \nOut of curiosity, has anyone considered what it would look if cadvisor supported the ability to embedded the container name in the metric itself? This would then allow a user to avoid having empty labels like this.. Fair point. Okay, let me take this from a different angle. \nIs there a way to filter what docker labels are turned into prometheus labels or alternatively simply turn \"convert docker labels into prom labels\" off? I use a large number of docker labels that vary a lot across images so the set of labels will be enormous, where most of the labels will be empty. \nIn my case, labels provide a useful metadata set for a variety of reasons outside of prometheus and many of them are of questionable value as a prometheus label.\nIn other words, could this func get some filtering options?\ngolang\nfunc DefaultContainerLabels(container *info.ContainerInfo) map[string]string {}\nPS I'd  be happy to submit a PR along those lines if this something the project would consider accepting.. @dashpole done. Note that I defaulted the flag to true since that's the current behavior so this won't be a breaking change. On the other hand, I suspect my scenario is the norm and that people would actually want to opt into this behavior but that's just a gut feeling backed by no empirical data. . @dashpole Hmmm, that's interesting. I am leaning towards overkill since its unclear to me at least whether this functionality would be useful to docker containers. On the other hand, cadvisor is for control groups and thus covers a broader range of use cases outside of docker so perhaps it makes sense in that case? Either way, It gives me the ability to turn this functionality off which is all that I was looking for so either sounds fine to me.. To avoid an XY problem, I will throw out this idea. For docker containers at least, the current label exporting behavior is sub-optimal because it forces all containers on a system to get labels from every other container.  So even with whitelisting, it doesn't address the problem of wanting to export a metric for one container but not another. And as you explained, this is was done to deal with the design of prometheus.\nBut what if cadvisor could actually open a port for each container such that each container metric list is a separate endpoint + whitelisting? Then, labels of one container would have no impact on another when exporting metrics. To deal with the service discovery problem, cadvisor could publish a GET rest endpoint that allowed a client to download a JSON document that contains a metric port mapping for each container. A tool could then use the JSON doc to register the service endpoint into a service discovery service eg consul. \nUnderstood this would be a much more significant change and increased complexity so might be undesireable for that reason but I thought I'd at least ask even if the answer is no. :). @dashpole fair enough whitelabel change or my PR. Either sounds find to me.. I signed it!. @dashpole flag name store_container_labels seems unnecessarily verbose for someone executing this on a command line, especially since its a boolean. But to be consistent with the other flags, I followed the \"flag name should match variable name but underscores instead of camel case\". ",
    "robinwangrubin": "\n@robinwangrubin please check your wrong logs. Your Kernel version isn't supported.\nE0607 03:16:00.481139 1 factory.go:340] devicemapper filesystem stats will not be reported: RHEL/Centos 7.x kernel version 3.10.0-366 or later is required to use thin_ls - you have \"3.10.0-229.el7.x86_64\"\n\nBut, With the same Kernel.  my others CAdvisor can start.  It's weird.. ",
    "anomaly256": "Are you sure it doesn't just need ro to /var/lib/docker/containers, like other monitoring solutions use?  Bind mounting all of the host's rootfs seems incredibly excessive and unnecessary.  I'd also seriously question the need for rw to /var/run.  I assume this is so it can use the docker socket.. does it really need to write to it though?  I really think you should change the documentation to have more sanely scoped access to host resources.  Specially if you want people to believe you've given security any kind of consideration with this project.. Unless someone uses a docker auth plug-in to mitigate that.  I really don\u2019t\nthink one security issue\u2019s presence justifies adding a second equally large\none\nOn Wed, 22 Aug 2018 at 2:56 am, David Ashpole notifications@github.com\nwrote:\n\n@anomaly256 https://github.com/anomaly256\nYou are correct that we don't need write access to the docker socket\n(turns out you only need read access to write to a socket). Ill update the\ndocumentation to reflect that. However, having read access to the docker\nsocket is the equivalent to root on the host (as you can create a container\nwith arbitrary privileges), so there isn't a security boundary to be\nenforced by carefully scoped mounts.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/cadvisor/issues/1955#issuecomment-414745664,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAZv3hZVUlHKJ16UeCRTRdSGs9tkjKXpks5uTDu-gaJpZM4UfZdb\n.\n. sigh.. \n",
    "gvessere": "I signed it!. ",
    "dsaydon90": "Hi @dashpole , \nthank you for your answer.\nI'm not familiar with the Go code. can you or someone else that familiar with this code of the prometheus.go  (I saw many contributes) can help with this? \nI believe that my feature can be really helpful.\nThanks!. ",
    "ArvinDevel": "Up to now, I found the problem is that the metrics generated by cadvisor(except those can be displayed by prometheus) has lots of null labels, shown on the picture. The output metrics is output of one node/kubelet from http://localhost:8001/api/v1/nodes/MYNODE/proxy/metrics/cadvisor after apply kubectl proxy.\nOutput of \nshell\nkubectl version\nis\nshell\nClient Version: version.Info{Major:\"1\", Minor:\"10\", GitVersion:\"v1.10.1\", GitCommit:\"d4ab47518836c750f9949b9e0d387f20fb92260b\", GitTreeState:\"clean\", BuildDate:\"2018-04-13T22:27:55Z\", GoVersion:\"go1.9.5\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"8\", GitVersion:\"v1.8.6\", GitCommit:\"6260bb08c46c31eea6cb538b34a9ceb3e406689c\", GitTreeState:\"clean\", BuildDate:\"2017-12-21T06:23:29Z\", GoVersion:\"go1.8.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nI think this is related to #1704. Is my k8s cluster's version too small to suffer such problem?\n. \n. @dashpole thanks. \nyeah, not null labels, just empty strings.\nWhat does the id field of metric labels mean? is there any docs to explain this?. I solved my question using this method, I forgot to clarify that some of my kubelet don't grant the kube-state to access it. And now the metrics what I want has emerged, but these still exists lots of metrics with empty labels. I think these too much metrics maybe is related to system.. since the problem has been solved, close it now. thanks for you @dashpole .. ",
    "vpapako": "18 docker containers are running in the node when I am gathering such metrics. Here is the driver status:\n\nBelow I report the values of three fs related metrics for the entire life-cycle of the container I am interesting in, gathered every 1 second. As shown, the container_fs_usage_bytes metric only changes after a minute passed (highlighted in red), independently of the write activity (highlighted in green) reported by container_fs_writes_bytes_total metric. Is this reasonable with respect to the followed implementation strategies?   \n[1] container_fs_reads_bytes_total\n[2] container_fs_writes_bytes_total\n[3] container_fs_usage_bytes\ndiff\nTime        [1]     [2]     [3]\n09:56:34.812    null        null        null\n09:56:35.814    null        null        null\n09:56:36.817    1839104     0       0\n09:56:37.821    1839104     0       0\n-09:56:38.824   6873088     32768       53248\n09:56:39.827    8945664     32768       53248\n09:56:40.829    8945664     32768       53248\n+09:56:41.834   9113600     48984064    53248\n+09:56:42.840   9715712     55431168    53248\n09:56:43.843    9715712     55431168    53248\n09:56:44.847    9715712     55431168    53248\n+09:56:45.856   17219584    77705216    53248\n09:56:46.858    17219584    77705216    53248\n+09:56:47.860   35328000    78880768    53248\n09:56:48.874    35328000    78880768    53248\n09:56:49.876    37265408    78880768    53248\n09:56:50.879    37265408    78880768    53248\n09:56:51.884    37265408    78880768    53248\n09:56:52.892    37265408    78880768    53248\n09:56:53.894    37265408    78880768    53248\n09:56:54.901    37265408    78880768    53248\n09:56:55.903    37265408    78880768    53248\n09:56:56.905    37265408    78880768    53248\n09:56:57.907    37265408    78880768    53248\n09:56:58.910    37265408    78880768    53248\n09:56:59.913    37265408    78880768    53248\n09:57:00.915    37265408    78880768    53248\n09:57:01.918    37265408    78880768    53248\n09:57:02.923    37265408    78880768    53248\n09:57:03.929    37265408    78880768    53248\n09:57:04.931    37269504    78880768    53248\n09:57:05.934    37269504    78880768    53248\n09:57:06.936    37269504    78880768    53248\n09:57:07.938    37269504    78880768    53248\n09:57:08.940    37269504    78880768    53248\n09:57:09.953    37269504    78880768    53248\n09:57:10.956    37269504    78880768    53248\n09:57:12.156    37359616    78880768    53248\n09:57:13.159    37359616    78880768    53248\n09:57:14.162    37359616    78880768    53248\n09:57:15.164    37416960    78880768    53248\n09:57:16.166    37416960    78880768    53248\n09:57:17.168    37416960    78880768    53248\n09:57:18.170    37416960    78880768    53248\n09:57:19.172    37416960    78880768    53248\n09:57:20.176    37416960    78880768    53248\n09:57:21.178    37965824    78880768    53248\n09:57:22.180    37965824    78880768    53248\n09:57:23.182    37965824    78880768    53248\n09:57:24.185    37965824    78880768    53248\n09:57:25.187    37965824    78880768    53248\n09:57:26.189    37965824    78880768    53248\n09:57:27.191    37965824    78880768    53248\n09:57:28.192    37965824    78880768    53248\n09:57:29.194    37965824    78880768    53248\n09:57:30.197    37965824    78880768    53248\n09:57:31.408    37965824    78880768    53248\n09:57:32.430    37965824    78880768    53248\n09:57:33.440    37965824    78880768    53248\n09:57:34.443    38551552    78880768    53248\n09:57:35.446    38551552    78880768    53248\n-09:57:36.448   38551552    78880768    71495680\n09:57:37.451    38551552    78880768    71495680\n09:57:38.477    38551552    78880768    71495680\n09:57:39.479    38551552    78880768    71495680\n09:57:40.482    38551552    78880768    71495680\n09:57:41.484    38551552    78880768    71495680\n09:57:42.486    38551552    78880768    71495680\n09:57:43.488    38551552    78880768    71495680\n09:57:44.491    38551552    78880768    71495680\n09:57:45.496    38551552    78880768    71495680\n09:57:46.500    38551552    78880768    71495680\n09:57:47.511    38735872    78880768    71495680\n09:57:48.513    38735872    78880768    71495680\n09:57:49.515    38735872    78880768    71495680\n09:57:50.517    38735872    78880768    71495680\n+09:57:51.520   40370176    81707008    71495680\n09:57:52.522    40370176    81707008    71495680\n+09:57:53.629   75358208    112644096   71495680\n09:57:54.632    75358208    112644096   71495680\n09:57:55.642    75358208    112644096   71495680\n+09:57:56.645   75964416    116043776   71495680\n09:57:57.647    75964416    116043776   71495680\n+09:57:58.660   77275136    117821440   71495680\n+09:57:59.663   77275136    124317696   71495680\n09:58:00.666    77275136    124317696   71495680\n09:58:01.670    77275136    124317696   71495680\n09:58:02.671    77275136    124317696   71495680\n+09:58:03.679   77275136    124821504   71495680\n+09:58:04.685   77766656    134217728   71495680\n09:58:05.687    77766656    134217728   71495680\n09:58:06.693    77766656    134217728   71495680\n09:58:07.696    77766656    134217728   71495680\n09:58:08.707    77766656    134217728   71495680\n+09:58:09.709   78049280    144076800   71495680\n09:58:10.713    78049280    144076800   71495680\n09:58:11.719    78049280    144076800   71495680\n09:58:12.723    78049280    144076800   71495680\n09:58:13.727    78049280    144076800   71495680\n+09:58:14.730   78049280    157437952   71495680\n09:58:15.733    78049280    157437952   71495680\n09:58:16.738    78086144    157437952   71495680\n09:58:17.743    78086144    157437952   71495680\n09:58:18.751    78086144    157437952   71495680\n09:58:19.776    78086144    157437952   71495680\n09:58:20.778    78086144    157437952   71495680\n09:58:21.783    78249984    157437952   71495680\n09:58:23.540    78249984    157437952   71495680\n09:58:24.546    78249984    157437952   71495680\n09:58:25.548    78249984    157437952   71495680\n09:58:26.550    91168768    157437952   71495680\n09:58:27.552    91168768    157437952   71495680\n09:58:28.555    91181056    157437952   71495680\n09:58:29.558    91181056    157437952   71495680\n09:58:30.562    91181056    157437952   71495680\n09:58:31.564    91848704    157437952   71495680\n09:58:32.568    91930624    157437952   71495680\n09:58:33.572    91930624    157437952   71495680\n09:58:34.574    92585984    157437952   71495680\n09:58:35.576    92585984    157437952   71495680\n09:58:36.578    92672000    157437952   71495680\n-09:58:37.581   93491200    157437952   155394048\n09:58:38.587    93491200    157437952   155394048\n09:58:39.591    96731136    157437952   155394048\n09:58:40.594    96731136    157437952   155394048\n09:58:41.598    96731136    157437952   155394048\n09:58:42.602    96731136    157437952   155394048\n09:58:43.606    96731136    157437952   155394048\n09:58:44.612    96731136    157437952   155394048\n09:58:45.616    96731136    157437952   155394048\n09:58:46.622    96731136    157437952   155394048\n09:58:47.625    96731136    157437952   155394048\n09:58:48.629    96731136    157437952   155394048\n09:58:49.633    96731136    157437952   155394048\n09:58:50.636    96731136    157437952   155394048\n09:58:51.639    96747520    157437952   155394048\n09:58:52.644    96747520    157437952   155394048\n09:58:53.648    96747520    157437952   155394048\n09:58:54.651    96747520    157437952   155394048\n09:58:55.655    96747520    157437952   155394048\n09:58:56.658    96768000    157437952   155394048\n09:58:57.662    97222656    157437952   155394048\n09:58:58.664    97222656    157437952   155394048\n09:58:59.665    97222656    157437952   155394048\n09:59:00.667    97222656    157437952   155394048\n09:59:01.669    97222656    157437952   155394048\n09:59:02.671    97263616    157437952   155394048\n09:59:03.674    97263616    157437952   155394048\n09:59:04.675    97263616    157437952   155394048\n09:59:05.677    97263616    157437952   155394048\n09:59:06.680    97263616    157437952   155394048\n09:59:07.683    97263616    157437952   155394048\n09:59:08.688    97263616    157437952   155394048\n09:59:09.692    97263616    157437952   155394048\n09:59:10.695    97263616    157437952   155394048\n09:59:11.698    97263616    157437952   155394048\n09:59:12.700    97263616    157437952   155394048\n09:59:13.705    97263616    157437952   155394048\n09:59:14.709    97263616    157437952   155394048\n09:59:15.713    97263616    157437952   155394048\n09:59:16.715    97263616    157437952   155394048\n09:59:17.717    97263616    157437952   155394048\n09:59:18.720    97337344    157437952   155394048\n09:59:19.723    97337344    157437952   155394048\n09:59:20.727    97337344    157437952   155394048\n09:59:21.731    97337344    157437952   155394048\n09:59:22.735    97337344    157437952   155394048\n09:59:23.742    97337344    157437952   155394048\n09:59:24.744    97337344    157437952   155394048\n09:59:25.747    97337344    157437952   155394048\n09:59:26.751    97337344    157437952   155394048\n09:59:27.757    97337344    157437952   155394048\n09:59:28.759    97337344    157437952   155394048\n09:59:29.762    null        null        null\n09:59:30.767    null        null        null. Any news on this?. Ah, ok that makes sense. \nOne more question, why the other fs related metrics e.g. the container_fs_reads_bytes_total or the container_fs_writes_bytes_total do not follow this 1 minute period?. Thank you very much for the information!. ",
    "jessfraz": "it can wait :). Thanks!!. awesome, thanks!. ",
    "lichuqiang": "\nWhat is the entry-point of your upper components? Do they call AllDockerContainers directly, or are they making an API call?\n\napi call: http://127.0.0.1:4194/api/v1.2/docker\ni.e: https://github.com/google/cadvisor/blob/master/api/versions.go#L198. I signed it!. Rollback the change here, and validate error content in AllDockerContainers explicitly. done. ",
    "MaximilianMeister": "@googlebot I signed it!. @dashpole thanks! should I do the vendor update in k8s once this is released on v0.30 ? or will you take care about it anyways?. > I will do all of the godep updates in k/k\nthanks @dashpole :+1: . ",
    "moooofly": "After doing some tests, I think using 'kubernetes/pause' may be anther reason.\n[#28#root@ubuntu-1604 ~]$docker run --rm --env test_var=FOO --label bar=baz -it busybox env\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nHOSTNAME=f86381c700f3\nTERM=xterm\ntest_var=FOO\nHOME=/root\n[#29#root@ubuntu-1604 ~]$\n[#29#root@ubuntu-1604 ~]$docker run --rm --env test_var=FOO --label bar=baz -it kubernetes/pause env\n[#30#root@ubuntu-1604 ~]$. I signed it!. I will check it out today. Thanks.\n\nUPDATE: After upgrading golang version from 1.8.3 into 1.9.7, make build succeed.. Yep, after upgrading golang version from 1.8.3 into 1.9.7, everything is ok.. ",
    "BertHartm": "Sorry for leaving this open so long. It appears that starting cadvisor before docker was the issue. ",
    "Nayana-ibm": "@dashpole build is successful after upgrading golang to v1.9.4 \nhttps://148.100.110.186/jenkins/job/cAdvisor_RHEL7_s390x_GHPRB/257/. ",
    "sebermann": "@igorkatz I worked around this issue by using the max_over_time() function over a large enough interval:\ntime() - max_over_time(container_last_seen[1h]) > scrape_interval\n\nI chose 1 hour so that even with a little downtime of the metric server it still sees if a container went absent. To get rid of some alerts misfiring due to stale data I wrapped this in another max() by (name):\ntime() - max(max_over_time(container_last_seen[1h])) by (name) > scrape_interval\n\nThis seems to work well enough so far.. ",
    "acondrat": "@dashpole we are on GKE, version v1.8.9-gke.1 \nNot sure how to find the version of cadvisor.\nHere is the storage driver section of docker info\nStorage Driver: overlay\n Backing Filesystem: extfs\n Supports d_type: true\nI'll try to test on the latest available GKE version and post the results.. So we are running on GKE v1.8.9-gke.1 which is bundled with cAdvisor v0.27.3\nSeems like a related issue was fixed in cAdvisor v0.29.0\nI run a test with GKE 1.10.5 which is bundled with cAdvisor v0.29.1 and can't reproduce the problem anymore. I assume we can consider this closed.\n@dashpole thanks for your help!. ",
    "shikhasriva": "It is, but not exposed via cAdvisor. I am looking fior a way to read labels for all container running in my K8 cluster. ",
    "rbastic": "Any update on this?. ",
    "lucperkins": "@dashpole Could you possibly expedite this a bit? I have some Prometheus + cAdvisor documentation that I'm working on and would love to be able to link to this!. @dashpole Many thanks!. ",
    "Betula-L": "It's fixed in google/cadvisor:canary. @ringtail i mean, the root cause is the gap of timestamp between cadvisor and prometheus. \"Only add rate metrics\" is not proper way to fix this bug.\nIf exposed exact cadvisor timestamp for prometheus as  documentation , to fix this gap, we do not need add this rate metrics.. I signed it!. /retest. I forgot test. Finished now.. This bug is introduced by my PR https://github.com/google/cadvisor/pull/2124. I'm trying to fix it as well as add more testcase.. Why this issue?\n@dashpole  PR https://github.com/google/cadvisor/pull/2124 led an error behavior when prometheus scrapes /metrics. \nIt's because i ignored metrics container_last_seen at prometheus.go:138, and ignoreRe at prometheus_test.go:271 ignores it also. Thus, the bad effect of https://github.com/google/cadvisor/pull/2124 was not detected. \nCurrent metric container_last_seen is as follow,\ncontainer_last_seen{id=\"/system.slice/avahi-daemon.service\",image=\"\",name=\"\"} 1.546053157e+09 -62135596800000\nRevised metrics container_last_seen of https://github.com/google/cadvisor/pull/2135\ncontainer_last_seen{id=\"/system.slice/avahi-daemon.service\",image=\"\",name=\"\"} 1.546053157e+09 1546053157000\nFurthermore\n1. I don't think container_last_seen should be metric at namespace container. It could be individual metrics out of func collectContainersInfo. \n2. prometheus_test.go ought to be improved.\nHope for insightful solution.. /retest. I can't understand why this pr tested failed.. @blakebarnett Excuse me, i did not reproduce this panic in Kubernetes 1.11.5. Would you specify more details about it? . container_tasks_state is not removed.  http_request_duration_microseconds is removed since it's not cAdvisor metrics, but prometheus metrics, you will not catch it in cAdvisor /metrics.. For prometheus client_golang 0.8.1, it does not provide metrics with timestamp due to https://github.com/prometheus/prometheus/issues/398. The function NewMetricWithTimestamp is introduced in prometheus client_golang 0.9. A detailed issue track of that is in https://github.com/prometheus/client_golang/issues/187. ",
    "wgliang": "Who can take a look?  thanks!. ",
    "salaxander": "Signed the CLA! :). Comment for another CLA check!. Comment for another CLA check!. @dashpole - Thanks for the guidance on that! Turns out I was missing an alternate email on my Google and GitHub account. ",
    "myaspm": "I didn't understand what you mean by querying prometheus endpoint?\nThis is my level 4 debug logs: https://paste.ubuntu.com/p/wY6tsfWV6q/\nMy run command for those logs is :\nsudo docker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro \\\n--volume=/opt/docker/docker/:/opt/docker/docker:ro --publish=8011:8080 \\\n--userns=host --name=cadvisor2   google/cadvisor:latest \\\n--logtostderr --docker_root=\"/opt/docker/docker\" -v=4 --enable_load_reader=true. Sorry for the late reply.\nI got this error as you predicted:\nRegistration of the Docker container factory failed: failed to validate Docker info: cAdvisor requires docker version [1 0 0] or above but we have found version [0 0 0] reported as \"0.0.0-20180720214833-f61e0f7\".\nIs this about me using Docker CE Edge version for 18.04? Docker should have a stable repo by now, will try and get back to you.. Backed up all my docker files and switched to stable version and everything works fine now.\nThanks a lot for your help guys! . ",
    "vboginskey": "It's not clear to me why the runc changes are necessary, since cadvisor uses MemoryStats.Stats[\"cache\"] directly rather than MemoryStats.Cache. Wouldn't something like this address the discrepancy?. Sure, here it is: https://github.com/google/cadvisor/pull/2014. I signed the CLA.. ",
    "sbakir": "Thank you for a quick response. So, can I use a known secure webserver and integrate with cadvisor for more secure usage? Or, what is the best practice for such a case or any way that you can suggest? . Ok, thank you very much for the information. . Thanks @tghosgor  for the fix.. ",
    "stevenjm": "I signed it!. cla update. @dashpole Both the committer and the PR author are covered by the same signed corporate CLA, but the automation doesn't seem to be recognising that. Is there anything left for us to do?. Closing this so the commit author can open one and hopefully get the CLA check to pass.. Replacement PR: https://github.com/google/cadvisor/pull/2034. ",
    "gwkunze": "I signed it!\nCorporate CLA has been signed, I checked the email address for the commit and it matches as far as I can tell.. ",
    "zhangjianweibj": "I signed it!. ",
    "gautamdivgi": "It came down from this PR - https://github.com/google/cadvisor/pull/1576 where an fsHandler is created per container. If I have N containers, that means N fsHandlers with each running a loop to trackUsage https://github.com/google/cadvisor/blob/master/container/common/fsHandler.go#L111. These will all iterate over a the 20 maxConcurrentOps (https://github.com/google/cadvisor/blob/master/fs/fs.go#L51). So even though cAdvisor doesn't do housekeeping in parallel I think you will have a backlog of N-20 waiting on a \"token\". \nI have typically seen the high wait times in https://github.com/kubernetes/kubernetes/issues/61999 with a large number of kubelet threads (>1000) with most of them stuck on a futex_wait_queue_me system call.\nBut I guess I may owe you an experiment here. ",
    "piaoyu": "something that hdfs do can refence https://issues.apache.org/jira/browse/HADOOP-9884. ",
    "mshaverdo": "@dashpole \nFixed. Sorry.. You're right, it's looks much better. I think, it's better to leave assignation to a new var, but move it under the lock, to avoid possible data race.. With assignation placed under the lock, code looks ugly without defer, but with defer we returning to the first variant. Maybe, it's better to use initial variant?\nm.containersLock.RLock()\n    cont, ok := m.containers[namespacedContainerName{\n        Name: containerName,\n    }]\n    if !ok {\n        m.containersLock.RUnlock()\n        return nil, nil, fmt.Errorf(\"failed to find container %q while checking for new containers\", containerName)\n    }\n    contHandler := cont.handler\n    m.containersLock.RUnlock(). Yes, there are no data races in the current codebase, but i have one concern:\nWhen m.containersLock locked, we expect exclusive access to the m.containers and it's members' fields. It's unexpected, that cont.handler could be read by anyone else during lock, so it could lead to tricky data race in future.\nWhat do you think about it?. Got it. Fixed.. ",
    "bpownow": "I signed it!. @dashpole ah ok thanks. made it a link.. ",
    "michelgokan": "@dashpole That\u2019s a perfect explanation. Thank you. Can you suggest a way to calculate average CPU usage (in percentage) per pod/container using cAdvisor\u2019s prometheus export data? I didn\u2019t find anything directly related. . @dashpole  Thank you for your detailed explanation and time. I appreciate it. To clarify: What do you mean by \"some constant\" and where did you get the formula above? Any links/references would be appreciated.. ",
    "ringtail": "Here is the result after the fix.\n\n. @dashpole real-time data point may not be a good idea because of it's time cost. In heapster,It scrapy metrics with a specific time range based on metric_resolution like this.\n\nBecause the most of aggregating methods will not be aware of the metrics size.If the metrics need divide some number (core number) then it will aggregate a much more confused value(such as: over 100% cpu_usage_rate on each core).\nThe metrics api should not return the out of date metrics. Three times of defaultHousekeepingInterval is reasonable because the official scraping interval is 30s in prometheus. Less than 30s is strongly not recommended. So,the prometheus api will guarantee the real-time and accuracy of metrics.. @dashpole I agree with you and I'll help to find the reason of out-of-date metrics. . @dashpole #2059 I found another reasonable way to avoid this problem.. @Betulle I fixed it in prometheus node exporter. prometheus collect metrics from kubelet(cadvisor) with timestamp. So that the rate calculation is not correct when cadvisor can not provide latest metrics in time. The code is here(https://github.com/ringtail/node_exporter/blob/feature/fix_usage_rate/collector/container_rate_stats.go).. @Betula-L Yes\uff0cthis pr may not be the solution and I find any way to fix it. But I don't know how to find the root cause and fix the problem in cadvisor. ",
    "kongbo1987": "@dashpole I also encountered this problem. Please fix it according to @ringtail \u2018s suggestion. I  have test this patch,  It works well on my kubernetes cluster, Thanks! . ",
    "Betulle": "@ringtail @dashpole  I don't think this PR is not a proper solution for your problem. \nActually, it's due to inconsistency between Prometheus scraped timestamp from /metrics with cadvisor scraped timestamp from cgroup. The problem is, CAdvisor do not expose /metrics properly.\nCAdvisor should expose data in /metrics with timestamp as documentation.\nExpected:\ncontainer_cpu_system_seconds_total{container_name=\"\",id=\"/kubepods\",image=\"\",name=\"\",namespace=\"\",pod_name=\"\"} 6.21201942e+06 1395066363000\nCurrent:\ncontainer_cpu_system_seconds_total{container_name=\"\",id=\"/kubepods\",image=\"\",name=\"\",namespace=\"\",pod_name=\"\"} 6.21201942e+06\nPrecise timestamp is essential for type Counter metrics.\nSupposed that \n- CAdvisor scraped 6.21201942e+06 from cgroup at 1395066363000 2014/3/17 22:26:3\n- Prometheus scraped the same data 6.21201942e+06 from CAdvsior /metrics at 1395066395000 2014/3/17 22:26:35\nan inconsistency exist if without exposing metrics  timestamp.\n. ",
    "lubinszARM": "@vishh \nCan you check it?\nThanks.. hi @vishh \nThe current data of topology in cAdvisor is not consistent with the output of 'lstopo'.\nThis is the main purpose of my patch. I want to fix it in this PR.\nSo, you can review it.\nAlso, the difference between the contents of \u2019/proc/cpuinfo' on x86/arm is very large.\nOn Arm platform, there are no 'core id' and 'physical id' in '/proc/cpuinfo'. So it means that the function of getTopology in cAdvisor is not working on Arm platform.\nI will fix it by checking '/sys/bus' to get the whole topology in next PR by using the same method in 'lstopo'.\nFor your reference, we can get the correct topology to show Node/Core/Thread/Pci layout by checking following sysfs:\nconst nodeBusPath = \"/sys/bus/node/devices/\"\nconst cpuBusPath   = \"/sys/bus/cpu/devices/\"\nconst pciBusPath     = \"/sys/bus/pci/devices/\"\nAlso , please see a typical output of  '/proc/cpuinfo' on Arm platform:\n...\nprocessor       : 222\nBogoMIPS        : 400.00\nFeatures        : fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics cpuid asimdrdm\nCPU implementer : 0x43\nCPU architecture: 8\nCPU variant     : 0x1\nCPU part        : 0x0af\nCPU revision    : 0\nprocessor       : 223\nBogoMIPS        : 400.00\nFeatures        : fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics cpuid asimdrdm\nCPU implementer : 0x43\nCPU architecture: 8\nCPU variant     : 0x1\nCPU part        : 0x0af\nCPU revision    : 0\n. > Once you've signed (or fixed any issues), please\nI signed it!. @vishh \nHi, can you help to check it?\nThanks.. I signed it!\nFrom: googlebot notifications@github.com\nSent: Thursday, November 29, 2018 10:40 AM\nTo: google/cadvisor cadvisor@noreply.github.com\nCc: Bin Lu (Arm Technology China) Bin.Lu@arm.com; Author author@noreply.github.com\nSubject: Re: [google/cadvisor] Fix bug: getTopology will be failed on Arm platform if no 'core id' o\u2026 (#2108)\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA datahttps://cla.developers.google.com/clas and verify that your email is set on your git commitshttps://help.github.com/articles/setting-your-email-in-git/.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoothttp://go/cla#troubleshoot (Public versionhttps://opensource.google.com/docs/cla/#troubleshoot).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA datahttps://cla.developers.google.com/clas and verify that your email is set on your git commitshttps://help.github.com/articles/setting-your-email-in-git/.\nThe email used to register you as an authorized contributor must also be attached to your GitHub accounthttps://github.com/settings/emails.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/google/cadvisor/pull/2108#issuecomment-442685174, or mute the threadhttps://github.com/notifications/unsubscribe-auth/Agi0gY4S6UbT3i8TXkNkPXBF9gyUXicZks5uz0kPgaJpZM4Y40r5.\nIMPORTANT NOTICE: The contents of this email and any attachments are confidential and may also be privileged. If you are not the intended recipient, please notify the sender immediately and do not disclose the contents to any other person, use it for any purpose, or store or copy the information in any medium. Thank you.\n. @dashpole \nHi, I have fixed the CLA issue.\nAny comments on this?. > /ok-to-test\n\nWhat is the effect of reporting coreId = threadId/nodeId=0? Can you expand on that a little bit?\n\nJust same with the topology of 'no numa'.\nOn a non-numa board, coreId = threadId = physical-cpu-id. And only contains node 0.. ",
    "lubinsz": "Hi @vishh \nI think the information is enough.\nIf we want to know which node the devices(net,gpu,fpga) belongs to, we can  get the pci address of this device firstly, then check the list of PCI device IDs per NUMA node  which was provided by cAdvisor.\nSuch as, If you want to know which numa-node this NIC is on. 2 steps were required in the code:\nstep1, look for sysfs net path, get the value of 'PCI_SLOT_NAME'\nroot@entos-thunderx2-02:~# cat /sys/class/net/enp11s0f1/device/uevent\n  DRIVER=qede\n  PCI_CLASS=20000\n  PCI_ID=1077:8070\n  PCI_SUBSYS_ID=1458:0011\n  PCI_SLOT_NAME=0000:0b:00.1\n  MODALIAS=pci:v00001077d00008070sv00001458sd00000011bc02sc00i00\nstep2, look for sysfs pci path, and get the value of 'numa_node' \nroot@entos-thunderx2-02:~# cat /sys/bus/pci/devices/0000\\:0b\\:00.1/numa_node\n  1\nFinally, you can know that your NIC is on numa-node 1.\n. @dashpole @vishh \nAny comments on this?\nThanks.. ping @vishh . @dashpole  @vishh     any comments on this?. @dashpole \nHi, \nTest cases for  getCoreIdFromCpuBus/getNodeIdFromCpuBus were added into \"machine/topology_test.go\"\nPlease check it.\nThanks.. /retest. /retest. @dashpole \nHi, \nThe old comment of \"report threadId if no NUMA\" is inappropriate.\nThis file is exist even on machines that do not support numa. \nSo I removed it.\nWe can confirm it in a VM that don't support numa.\nAnother problem is that the kernel is starting to support numa from version 2.5.\nSo it means the file does not exit on the old kernel (<2.5).\nI think we can report the error and let user to check the error and decide to use the same threadId or not.\nThanks.. Done. \"nodeBusRegExp\" was added for it.. Done.. Done. Done. I have removed it.. regexp.MatchString was added for it.. Yes. This field contains a list of PCI IDs.\nSuch as: [ \"0000:00:00.0\", \"0000:00:00.1\", \"0000:00:01.0\"].\nWe can see the test cases in \"machine/topology_test.go\" as more reference.\nThe basic logic for the code is like this:\n1, Look for all sysfs pci path containing node_id.\nSuch as:\n  root@ubuntu:~# cat /sys/bus/pci/devices/0000\\:00\\:00.0/numa_node\n  0\n2, Match the relationship between all pci device id and node_id.\nThen add the pci device id to the corresponding struct \"Node\" by using API 'AddNodePci'.\nThe field of 'Pcis' was belong to struct \"Topology\".\nThe struct of 'Topology' was already added into v2 API (.info/v2/machine.go).\nI think this field can be obtained by accessing the sturct \"Topology\" in v2 API.\nSo 'Pcis' does not necessary to be added inot v2 API.\nI don't know if my understanding is wrong.\nThanks.\n. ",
    "rashmichandrashekar": "Thanks for the quick response.\nWe are reading this metric from cadvisor every minute. And we are dividing it by 1000000000 to get the percentage since it is a single core machine. Do you think this might result in incorrect data?. I see that the cpu usage goes higher than 1000000000.\nThis is the metrics from cadvisor.\n\"cpu\": {\n    \"time\": \"2018-08-28T23:29:54Z\",\n    \"usageNanoCores\": 1002123571,\n    \"usageCoreNanoSeconds\": 1124064567447750\n   }\nIs the cache returning the wrong metrics? Since the usage cannot go beyond the limit on a node.\n. Thanks! I can look into using the usageCoreNanoSeconds metric.\nIs there an api we can use to get the rate of usagecpucorenanoseconds over a period of time?. ",
    "apratina": "Sorry if I was not clear earlier. I am looking for % of cpu utilized by the container at any point of time. So it looks like there is no metric other than container_cpu_usage_seconds_total to calculate it?. ",
    "jraby": "Ignoring these would be really useful for us. We have thousands of k8s pods with a few mounts each, and these system.slice/run-*.scope metrics account for the vast majority of the metrics reported by cadvisor.\nHaving a way to ignore them at the source would be awesome.\njunta:tmp jean$ grep -E '^[^#]'  host1.cadvisor | wc -l\n   30523\njunta:tmp jean$ grep -E '^[^#]'  host1.cadvisor | grep system.slice/run | wc -l\n   24312. ",
    "lw8008": "@sjenning  thanks.\nGetting per-container LV usage would be helpful. Based on that, we found one application has code bug in our system with spike space usage.\nDo we have workaround or enable plan in the future? . @sjenning  we will migrate dm to overlay2 in the future, thanks for your advise.. ",
    "denghuancong": "@dashpole  the log as follow, we has 96 docker containers running in that host.\nI1205 03:40:41.033311       1 manager.go:233] Version: {KernelVersion:4.14.0-1.el7.ucloud.x86_64 ContainerOsVersion:Alpine Linux v3.4 DockerVersion:18.03.0-ce DockerAPIVersion:1.37 CadvisorVersion:v0.29.0 CadvisorRevision:aaaa65d}\nI1205 03:40:41.065438       1 factory.go:356] Registering Docker factory\nI1205 03:40:43.065787       1 factory.go:54] Registering systemd factory\nE1205 03:40:43.070918       1 manager.go:328] Registration of the raw container factory failed: inotify_init: too many open files\nF1205 03:40:43.075149       1 cadvisor.go:157] Failed to start container manager: inotify_init: too many open files\nI1205 03:41:43.568514       1 storagedriver.go:50] Caching stats in memory for 2m0s\nI1205 03:41:43.568838       1 manager.go:154] cAdvisor running in container: \"/sys/fs/cgroup/cpu,cpuacct\"\nI1205 03:41:43.635769       1 fs.go:142] Filesystem UUIDs: map[]\nI1205 03:41:43.635799       1 fs.go:143] Filesystem partitions: map[tmpfs:{mountpoint:/dev major:0 minor:151 fsType:tmpfs blockSize:0} /dev/sda3:{mountpoint:/var/lib/docker major:8 minor:3 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/rootfs/boot major:8 minor:1 fsType:xfs blockSize:0} /dev/sdb:{mountpoint:/rootfs/data major:8 minor:16 fsType:ext4 blockSize:0} shm:{mountpoint:/dev/shm major:0 minor:146 fsType:tmpfs blockSize:0}]\nI1205 03:41:43.640506       1 manager.go:227] Machine: {NumCores:48 CpuFrequency:2900000 MemoryCapacity:270097031168 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] MachineID:5d2e4f777a8b46e29a99453e032d56f4 SystemUUID:57D38424-EFC1-11E7-03D2-74EACB7545F6 BootID:a179a7e9-230e-4474-982b-f84da2a6fe5a Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:151 Capacity:67108864 Type:vfs Inodes:32970829 HasInodes:true} {Device:/dev/sda3 DeviceMajor:8 DeviceMinor:3 Capacity:996693123072 Type:vfs Inodes:486904256 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:311070720 Type:vfs Inodes:153600 HasInodes:true} {Device:/dev/sdb DeviceMajor:8 DeviceMinor:16 Capacity:11904318177280 Type:vfs Inodes:366182400 HasInodes:true} {Device:shm DeviceMajor:0 DeviceMinor:146 Capacity:67108864 Type:vfs Inodes:32970829 HasInodes:true} {Device:overlay DeviceMajor:0 DeviceMinor:50 Capacity:996693123072 Type:vfs Inodes:486904256 HasInodes:true}] DiskMap:map[8:0:{Name:sda Major:8 Minor:0 Size:999643152384 Scheduler:deadline} 8:16:{Name:sdb Major:8 Minor:16 Size:11999064883200 Scheduler:noop}] NetworkDevices:[{Name:net0 MacAddress:74:ea:cb:75:45:f8 Speed:-1 Mtu:1500} {Name:net1 MacAddress:74:ea:cb:75:45:f9 Speed:-1 Mtu:1500} {Name:net2 MacAddress:48:df:37:1e:75:70 Speed:10000 Mtu:1500} {Name:net3 MacAddress:74:ea:cb:75:45:fa Speed:-1 Mtu:1500} {Name:net4 MacAddress:74:ea:cb:75:45:fb Speed:-1 Mtu:1500} {Name:net5 MacAddress:48:df:37:1e:75:71 Speed:-1 Mtu:1500}] Topology:[{Id:0 Memory:134810673152 Cores:[{Id:0 Threads:[0 24] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[1 25] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:2 Threads:[2 26] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:3 Threads:[3 27] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:4 Threads:[4 28] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:5 Threads:[5 29] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:8 Threads:[6 30] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:9 Threads:[7 31] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:10 Threads:[8 32] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:11 Threads:[9 33] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:12 Threads:[10 34] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:13 Threads:[11 35] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:31457280 Type:Unified Level:3}]} {Id:1 Memory:135286358016 Cores:[{Id:0 Threads:[12 36] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[13 37] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:2 Threads:[14 38] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:3 Threads:[15 39] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:4 Threads:[16 40] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:5 Threads:[17 41] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:8 Threads:[18 42] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:9 Threads:[19 43] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:10 Threads:[20 44] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:11 Threads:[21 45] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:12 Threads:[22 46] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:13 Threads:[23 47] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:31457280 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}\nI1205 03:41:43.641481       1 manager.go:233] Version: {KernelVersion:4.14.0-1.el7.ucloud.x86_64 ContainerOsVersion:Alpine Linux v3.4 DockerVersion:18.03.0-ce DockerAPIVersion:1.37 CadvisorVersion:v0.29.0 CadvisorRevision:aaaa65d}\nI1205 03:41:43.672039       1 factory.go:356] Registering Docker factory\nI1205 03:41:45.672396       1 factory.go:54] Registering systemd factory\nE1205 03:41:45.678565       1 manager.go:328] Registration of the raw container factory failed: inotify_init: too many open files\nF1205 03:41:45.683071       1 cadvisor.go:157] Failed to start container manager: inotify_init: too many open files. we change the params as follow, cadvisor run success.\nfs.inotify.max_user_instances = 128 ===>  fs.inotify.max_user_instances = 8000. ",
    "andrewghobrial": "I also see the same thing and would also like to understand the relation between the root cgroup memory metrics and that of /proc/meminfo. ",
    "DeliangFan": "\nWhere is privilage enabled in the podSpec?\n\nsorry for the mistake, the podSpec has been updated. \nCadvisor get GPU devices from cgroup device.list. However, the device.list of privilege container looks like following.\n* rw *\nAs a result, cadvisor can not get any GPU devices for privilege container.. ",
    "liucimin": "Yes.\nSometime we need use port bond in container when we use container in product env.But in virtual network,the switch dont support these.\nSo we try to get the interfaces' stat in container. ",
    "alanh0vx": "tried to disable several metrics, but it still keeps super high CPU, up to 3xx %\nI have also modified sudo sysctl fs.inotify.max_user_watches=1048576 in order to start the container\nhere is my deployment:\n```\napiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  name: cadvisor\n  namespace: kube-system\n  labels:\n    app: cadvisor\nspec:\n  selector:\n    matchLabels:\n      name: cadvisor\n  template:\n    metadata:\n      labels:\n        name: cadvisor\n    spec:\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n      containers:\n      - name: cadvisor\n        image: google/cadvisor:latest\n        volumeMounts:\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: var-run\n          mountPath: /var/run\n          readOnly: false\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        ports:\n          - name: http-cadvisor\n            containerPort: 8080\n            hostPort: 10255\n            protocol: TCP\n        args:\n          - --allow_dynamic_housekeeping=true\n          - --housekeeping_interval=30s\n          - --global_housekeeping_interval=2m\n          - --disable_metrics=disk,tcp,udp\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: var-run\n        hostPath:\n          path: /var/run\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: docker\n        hostPath:\n          path: /var/lib/docker\n\nkind: Service\napiVersion: v1\nmetadata:\n  name: cadvisor\n  namespace: kube-system\nspec:\n  type: ClusterIP\n  clusterIP: None\n  selector:\n    name: cadvisor\n  ports:\n  - name: http-cadvisor\n    protocol: TCP\n    port: 10255\n``. i have added the args--docker_only`, CPU high problem seems to be solved.\nbut look like it does not grab the metrics from all containers, will check again. ",
    "flands": "1.11.0. ",
    "aviratna": "@dashpole Hi David, Thanks for your response. I just wanted to check if there is any plan to get the container details and related metrics using API instead of dependency on host filesystem in future which will be more scalable. The reason I am asking is there are container solution which doesn't run directly on Host. e.g. VMWare Integrated Containers.. ",
    "hangongithub": "It looks like NetworkStats has the numbers:\nhttps://github.com/google/cadvisor/blob/49e7c7ead43c3a5ac1f90d9b184b736e57ca1abf/info/v2/container.go\n``\ntype NetworkStats struct {\n    // Network stats by interface.\n    Interfaces []v1.InterfaceStatsjson:\"interfaces,omitempty\"// TCP connection stats (Established, Listen...)\n    Tcp TcpStatjson:\"tcp\"// TCP6 connection stats (Established, Listen...)\n    Tcp6 TcpStatjson:\"tcp6\"// UDP connection stats\n    Udp v1.UdpStatjson:\"udp\"// UDP6 connection stats\n    Udp6 v1.UdpStatjson:\"udp6\"`\n}\n```\nBut the prometheus output doesn't try to output it at all:\nhttps://github.com/google/cadvisor/blob/f6334480669bf68c181350ea2fef310e183e31c6/metrics/prometheus.go\nIt only calls s.Network.Tcp without calling s.Network.Tcp6:\nreturn metricValues{\n                        {\n                            value:  float64(s.Network.Tcp.Established),\n                            labels: []string{\"established\"},\n                        },\n                        {\n                            value:  float64(s.Network.Tcp.SynSent),\n                            labels: []string{\"synsent\"},\n                        },\n                        {\n                            value:  float64(s.Network.Tcp.SynRecv),\n                            labels: []string{\"synrecv\"},\n                        },\n                        {\n                            value:  float64(s.Network.Tcp.FinWait1),\n                            labels: []string{\"finwait1\"},\n                        },\n                        {\n                            value:  float64(s.Network.Tcp.FinWait2),\n                            labels: []string{\"finwait2\"},\n                        },\n                        {\n                            value:  float64(s.Network.Tcp.TimeWait),\n                            labels: []string{\"timewait\"},\n                        },\n                        {\n                            value:  float64(s.Network.Tcp.Close),\n                            labels: []string{\"close\"},\n                        },\n                        {\n                            value:  float64(s.Network.Tcp.CloseWait),\n                            labels: []string{\"closewait\"},\n                        },\n                        {\n                            value:  float64(s.Network.Tcp.LastAck),\n                            labels: []string{\"lastack\"},\n                        },\n                        {\n                            value:  float64(s.Network.Tcp.Listen),\n                            labels: []string{\"listen\"},\n                        },\n                        {\n                            value:  float64(s.Network.Tcp.Closing),\n                            labels: []string{\"closing\"},\n                        },\n                    }\n                },. I've signed it!. I've signed it again!. I've signed it!. I've updated my repository but instead of amending of my old commit, it looks like it created 3 commits instead (my 2 changes plus my pull from origin and rebase). How do I squash it all into one commit?. Let me know if there's anything else I need to do on my end to get this merged. Otherwise I'll just hang tight and wait. Thanks for all the help!. Can I get a quick review on this? Thanks!. Done. Done. ",
    "Lynzabo": "@dashpole I used the google/cadvisor:latest image to test directly, the version is v0.28.3.. @dashpole The disk data returned by the cadvisor interface looks more like the container writable layer size. In fact, I need to get the sum of the data of all layers. Because some basic images will be very large.. @dashpole Thank you very much!. ",
    "humayunjamal": "when is this going to be released ? I am using --> https://github.com/google/cadvisor/releases/download/ to get the latest and seems like it doesnt have that metric being exported yet \n@dashpole \n@sashankreddya . ",
    "codnam007": "Has this been released?\nIf it has, what is the name for the exported metric for container file descriptors?\nprocess_fd or container_file_descriptors?\nI got the latest version a few days ago and container_file_descriptors was not present.. @sashankreddya  I installed v0.32.0 and didn't get container_file_descriptors. Or is there a method to enable it that I missed out? \ud83e\udd14 . @sashankreddya I am pretty new to this. I had installed it using the google/cadvisor:v0.32.0 docker container. How do I enable them? . ",
    "orisano": "What is this PR's status now?. rebased. sorry, my mistake.\ni fixed it.. passed e2e. for licensing problem. my code based on golang/go private functions.\ngolang/go license is here.\nWhen I try to specify license on code, boilerplate check fails.\nwhat will i do?. ## References\nhttps://github.com/golang/go/blob/master/src/syscall/syscall_linux.go#L862-L880 (no edit)\nhttps://github.com/golang/go/blob/master/src/os/dir_unix.go#L47-L88 (editted readdirnames)\nhttps://github.com/golang/go/blob/master/src/syscall/dirent.go (editted ParseDirent)\nhttps://github.com/golang/go/blob/master/src/syscall/endian_big.go (no edit)\nhttps://github.com/golang/go/blob/master/src/syscall/endian_little.go (no edit)\nI combined and edited these.\n. to used github.com/karrick/godirwalk to resolve license problem.\n. ping @dashpole . @sashankreddya \nIs using symlink in \"/sys/fs/cgroup/mesos\" ?\ncurrently, maybe unhandle symlink type.\nby the way, Is \"/sys/fs/cgroup//mesos\" equals \"/sys/fs/cgroup/mesos\" ?. sorry, please show me the results of \"err.Error()\".. @sashankreddya \nsorry, my mistake. fix by #2094 . please review. parseDirent is edited for handle Dirent.Type.\noriginal implementation can't handle Dirent.Type.. sorry, I don't know the right way of redistributing the licensed software.. ",
    "liwei00wade": "@dashpole \nthank you very much.. ",
    "xiongbenben": "that is v2,version is V0.27.4. ",
    "sigma": "@dashpole as requested on Slack, this is for Kubernetes vendoring. ",
    "tiwalter": "Sure. \n\nThe orange-rimmed id's are different in length (the one above is longer). The green-rimmed metrics are only existent in the first metric. \nAs mentioned, the float values have a minimal deviation (maybe caused by the different scrape times).\n. Thanks @dashpole. \nIs there an opportunity to deactivate exposing the metrics from the pod's cgroup?. Hi @caitong93, \ni use Kubernetes v1.11 . ",
    "caitong93": "Hi @dashpole , does cadvisor always expose pod cgroup metrics? I recently found these metrics in my v1.12.3 k8s cluster. But I haven't seen them with v1.10.x k8s or older.. But I'm pretty sure the metrics I got from kubelet cadvisor endpoint, didn't contain pod cgroup metrics. Maybe due to kubelet code change? I will look deeper when I got time. \nAnd hi @tiwalter , what's the version of k8s you are using? . ",
    "WanLinghao": "/cc. @Cherishty hello, I am interested in both GPU and cadvisor, I want to know any update about this issue. And I will share if I find any solution about this^_^ . @dashpole upgrade gcc solve my problem! Now the gcc version is:\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nThanks!\n. @dashpole @SamSaffron  PTAL. /test pull-cadvisor-e2e. /test cla/google. /retest. I signed it!. I signed it!. @dashpole  CLA cert has been fixed, PTAL. @dashpole All comments addressed, PTAL. comments addressed @dashpole . @dashpole  I don't think we could achieve what @chenchun needs with the current approach. What he wants is stop  collecting system raw containers information like /system.slice/sshd.service. \nAnd no matter where  kubelet or runtime cgroup exist, the cAdvisor will collect every child cgroup data in root cgroups.  It will not change by where kubelet or runtime cgroup exist. \nI think if we want to stop collecting system raw containers. We could set docker_only=true&& rawPrefixWhiteList = \"/system.slice/kubelet.service, /system.slice/docker.service..\". This is just a raw plan but I want to confirm if this is what we want at first place..  It is a DiskUsageMetrics bug that people will see has_filesystem be true even they disable disk metrics. I will extract it and launch a new patch later if what I thought is correct @dashpole . As far as I can see, if we don't remove the blkio cgroup, it would keep extracting data from /sys/fs/cgroup/blkio/(ref:https://github.com/google/cadvisor/blob/45d3b774d690cf0221f06fc799e62e5cd8715e58/vendor/github.com/opencontainers/runc/libcontainer/cgroups/fs/blkio.go#L115). And the data it extracted would be just dropped cause we have disabled the diskIO metrics. So here I am trying to do is avoiding useless IO request when diskIO was disabled.. ",
    "reachmeselva": "@ dashpole: I am using KOPS cluster. AWS cloud environment. 1 Master and 2 Worker nodes (p2.xlarge and t2.medium)\nI am trying to collect GPU metrics by using cAdvisor but unable to gather them. The below is my (cadvisor.yaml) script. \n**---\napiVersion: extensions/v1beta1 \nkind: DaemonSet\nmetadata:\n  name: cadvisor\n  namespace: monitoring\n  labels:\n    app: cadvisor\nspec:\n  selector:\n    matchLabels:\n      name: cadvisor\n  template:\n    metadata:\n      labels:\n        name: cadvisor\n    spec:\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n      containers:\n      - name: cadvisor\n        image: google/cadvisor:v0.24.1 # k8s.gcr.io/cadvisor:v0.30.2 # google/cadvisor:v0.24.1\n        volumeMounts:\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: var-run\n          mountPath: /var/run\n          readOnly: false\n        - name: docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: sysfs\n          mountPath: /sys\n          readOnly: true\n        - name: cgroup\n          mountPath: /sys/fs/cgroup\n          readOnly: true\n        - name: libnvidia\n          mountPath: /usr/local/cuda-9.0/lib64\n          readOnly: true \n- name: kubelet-podresources\nmountPath: /var/lib/kubelet/\n    - name: dev-nvidiactl\n      mountPath: /dev/nvidiactl\n    - name: dev-nvidia0\n      mountPath: /dev/nvidia0\n    - name: dev-nvidia-uvm\n      mountPath: /dev/nvidia-uvm\n    - name: dev-cgroup\n      mountPath: /dev/cgroup\n    - name: dev\n      mountPath: /dev/\n    securityContext:\n      privileged: true\n    args: \n    - --device-cgroup-rule 'c 195:* mrw'\n    ports:\n      - name: http\n        containerPort: 8899\n        protocol: TCP\n    env:\n    - name: LD_LIBRARY_PATH\n      value: \"/usr/local/cuda-9.0/lib64/\"\n    args:\n      - --housekeeping_interval=10s\n      - --port=8899\n      - --storage_driver=influxdb \n      - --storage_driver_host=172.20.60.112:31116\n      - --storage_driver_db=cadvisor\n\n- --device-cgroup-rule 'c 195:* mrw'\n  terminationGracePeriodSeconds: 30\n  volumes:\n  - name: rootfs\n    hostPath:\n      path: /\n  - name: var-run\n    hostPath:\n      path: /var/run\n  - name: docker\n    hostPath:\n      path: /var/lib/docker\n  - name: sysfs\n    hostPath:\n      path: /sys\n  - name: cgroup\n    hostPath:\n      path: /cgroup\n  - name: libnvidia\n    hostPath:\n      path: /usr/local/cuda-9.0/lib64\n\n- name: kubelet-podresoures\nhostPath:\npath: /var/lib/kubelet/\n  - name: dev-nvidiactl\n    hostPath:\n      path: /dev/nvidiactl\n  - name: dev-nvidia0\n    hostPath:\n      path: /dev/nvidia0\n  - name: dev-nvidia-uvm\n    hostPath:\n      path: /dev/nvidia-uvm\n  - name: dev-cgroup\n    hostPath:\n      path: /cgroup\n  - name: dev\n    hostPath:\n      path: /dev/\n\n---**\nIt would be great if you could please check and update, what did I miss here as it help me to resolve the issue at the earliest. Thanks a tone in advance.\nRegards,\nSelva. @ dashpole: Thanks for your reply.  It would be great, if you could please provide gpu metrics names and sample link for how to add metrics in influxdb\nThanks in advance.. ",
    "Mortinke": "Hi @dashpole,\nthanks for your feedback and sorry for the delay. I tried to find out why this code is not executed correctly/not executed at all with --v=10 but I couldn't find anything. Is there another way to debug/find out why this doesn't work e.g. with another logging configuration?\nThanks in advance!. @dashpole thanks again for your time. I have a day off today. I will check this tomorrow.. Yes, the directory exists in the cAdvisor container:\n/ # ls  /rootfs/proc\n1              1344           170            1965           2178           25593          2715           2864           3044           497            crypto         key-users      mtrr           thread-self\n10             1353           1702           1966           2199           25634          2720           2865           3045           540            devices        keys           net            timer_list\n1008           137            171            1968           2208           25655          27555          2873           3085           6              diskstats      kmsg           pagetypeinfo   tty\n1029           14             172            1969           2220           25678          27556          2890           3112           7              dma            kpagecgroup    partitions     uptime\n1030           1405           1723           1975           2251           25770          27560          2911           3130           8              driver         kpagecount     sched_debug    version\n1071           14532          173            1976           22595          2649           27561          29463          3595           9              execdomains    kpageflags     schedstat      vmallocinfo\n109            15             1737           1977           2260           2650           27592          29464          3596           933            filesystems    latency_stats  self           vmstat\n11             1562           174            1978           22958          2651           27627          29468          3600           acpi           fs             loadavg        slabinfo       zoneinfo\n1138           16             1747           1979           22960          2655           27645          29469          3601           buddyinfo      interrupts     locks          softirqs\n12             162            176            1980           22961          2697           280            29515          3654           bus            iomem          mdstat         stat\n1293           1653           1781           1985           2373           2699           283            29536          3673           cgroups        ioports        meminfo        swaps\n13             16882          18             2              23816          2702           2837           29558          395            cmdline        irq            misc           sys\n1306           16922          19             20             25057          2709           2859           3039           4              consoles       kallsyms       modules        sysrq-trigger\n1316           16923          1952           2167           25592          2711           2860           3040           401            cpuinfo        kcore          mounts         sysvipc\nWe mount the following directories from the EC2 Instance:\n| EC2 path | Container path |\n| --------- | ---------------- |\n| /cgroup | /sys/fs/cgroup |\n| /dev/kmsg | /dev/kmsg |\n| /dev/disk/ | /dev/disk |\n| /var/lib/docker | /var/lib/docker |\n| /sys | /sys |\n| /var/run | /var/run |\n| / | /rootfs |\nRegarding the machine-id, both files don't exists on the EC2 instance:\n[ec2-user@ip-**-**-28-129 ~]$ ls -ltr /etc/ | grep machine\n[ec2-user@ip-**-**-28-129 ~]$ ls -ltr /var/lib/ | grep dbus\n[ec2-user@ip-**-**-28-129 ~]$\nAdditionally, I checked the file /sys/class/dmi/id/product_version from aws.go which exists in the container but without content:\n```\n/ # cat /sys/class/dmi/id/product_version\n/ #\nOn the EC2 instance this file exists also without content:\n[ec2-user@ip---28-129 ~]$ cat /sys/class/dmi/id/product_version\n[ec2-user@ip---28-129 ~]$\n```\nI also suspected that the container needed more rights to get the nessary information in getAwsMetadata, but after I added the AWS managed policy AmazonEC2ReadOnlyAccess nothing changed. \nUnfortunately, I don't have much experience with GO/debugging GO applications to provide more information. . Sorry @dashpole, I did not realize that you had answered. What information do you need in the /etc/machine-id file? As I wrote in my first comment, I already created the file with wget -q -O - http://169.254.169.254/latest/meta-data/instance-id > /etc/machine-id on the host, but nothing changed.. ",
    "ffilippopoulos": "yes, that is true, we are still running the kubelet one and can query those metrics as well. . ",
    "george-angel": "Containers: 109\n Running: 79\n Paused: 0\n Stopped: 30\nImages: 143\nServer Version: 18.06.1-ce\nStorage Driver: overlay2\n Backing Filesystem: extfs\n Supports d_type: true\n Native Overlay Diff: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: bridge host macvlan null overlay\n Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nInit Binary: docker-init\ncontainerd version: 468a545b9edcd5932818eb9de8e72413e616e86e\nrunc version: 69663f0bd4b60df09991c08812a60108003fa340\ninit version: v0.13.2 (expected: fec3683b971d9c3ef73f284f176672c44b448662)\nSecurity Options:\n seccomp\n  Profile: default\n selinux\nKernel Version: 4.14.78-coreos\nOperating System: Container Linux by CoreOS 1911.3.0 (Rhyolite)\nOSType: linux\nArchitecture: x86_64\nCPUs: 8\nTotal Memory: 31.42GiB\nName: ip-10-66-21-100\nID: A5KU:5YMU:GZXF:UURN:XA7X:JCO4:FS3I:RYQ4:KGE2:FC4A:37F2:P52M\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): false\nRegistry: https://index.docker.io/v1/\nLabels:\nExperimental: false\nInsecure Registries:\n 127.0.0.0/8\nLive Restore Enabled: false. ",
    "blueskyll": "\ncAdvisor detects which GPUs are being used by a container using the devices cgroup. I'm not actually sure how the nvidia device plugin gets access to GPUs, as the daemonset doesn't mount /dev. It must use some other method to get access to GPUs so it can manage them which uses the devices cgroup.\n\n@dashpole I guess the GPUs are exposed to nvidia device plugin container as the docker runtime need to be nvidia for nvidia device plugin installation. Is there any method to avoid the wrong metrics problem if it is the cause?. ",
    "gtie": "Getting the same error on a CentOS 7 AWS instance. Here is the layout of the mounted cgroup layout:\n[root@cadvisor-t4krz /] # ls -la /sys/fs/cgroup\ntotal 0\ndrwxr-xr-x. 13 root root 340 Nov 20 16:36 .\ndrwxr-xr-x.  7 root root   0 Dec  5 21:25 ..\ndrwxr-xr-x.  6 root root   0 Dec  5 21:24 blkio\nlrwxrwxrwx.  1 root root  11 Nov 20 16:36 cpu -> cpu,cpuacct\ndrwxr-xr-x.  6 root root   0 Dec  5 21:24 cpu,cpuacct\nlrwxrwxrwx.  1 root root  11 Nov 20 16:36 cpuacct -> cpu,cpuacct\ndrwxr-xr-x.  4 root root   0 Nov 20 16:36 cpuset\ndrwxr-xr-x.  6 root root   0 Dec  5 21:24 devices\ndrwxr-xr-x.  4 root root   0 Nov 20 16:36 freezer\ndrwxr-xr-x.  4 root root   0 Nov 20 16:36 hugetlb\ndrwxr-xr-x.  6 root root   0 Dec  5 21:24 memory\nlrwxrwxrwx.  1 root root  16 Nov 20 16:36 net_cls -> net_cls,net_prio\ndrwxr-xr-x.  4 root root   0 Nov 20 16:36 net_cls,net_prio\nlrwxrwxrwx.  1 root root  16 Nov 20 16:36 net_prio -> net_cls,net_prio\ndrwxr-xr-x.  4 root root   0 Nov 20 16:36 perf_event\ndrwxr-xr-x.  6 root root   0 Dec  5 21:24 pids\ndrwxr-xr-x.  6 root root   0 Dec  5 21:24 systemd\nNotice something amiss? The directory I got is called cpu,cpuacctrather than cpuacct,cpu - hence the failure.. ",
    "tiagoapimenta": "Ok, I'm forking it, but in my opinion it should be as clean as possible, as I have stated on the other issue, so I will remove wget and ca-certificates as it seems it was only installed in order to download glibc.\nI understand it will not be possible to remove findutils by now, but is it really necessary to have device-mapper, zfs, thin-provisioning-tools and a custom /etc/nsswitch.conf? Could you please explain me why?. I signed it!. ",
    "mikkeloscar": "@dashpole Thanks for the review, I addressed your comments!. @dashpole Thanks! I rebased!. @dashpole Arg, you are right. I didn't caught this when I tested it, but after looking at #1704 I clearly see the same behavior.\nI'll try to save memory in another way :) Thanks for the review!. ",
    "tghosgor": "\nI'm not super-familiar with this space. Can you go into a little more detail on why you need to add a prefix to all endpoints to work with some proxies?\n\nSure.\nWe need to serve cAdvisor behind a reverse proxy on a sub-path, e.g. /cadvisor. I have seen you have done some work to support reverse proxies (#667, #680). It would normally be enough for reverse proxies such as nginx with it's proxy_redirect as it supports rewriting Location: header in 3xx redirects. Unfortunately, this is not an option for all reverse proxies.\nWhat is happening is that ServeMux does the redirects with absolute paths to add a trailing slash. This happens quite a lot while navigating around (i.e. <a href=\"../docker\">Docker Containers</a>).\nI would expect nesting ServeMux would be enough instead of all this but it is not aware of its parents when doing redirects and it provides no way to override/extend its redirect behavior. The link /cadvisor/containers/../docker resolves to a 3xx to /docker/ which is out of /cadvisor sub-path. The official way seems to be adding a mux handler to /cadvisor/containers/../docker to do a correct redirect to /cadvisor/docker/. These eventually resulted in the url_base_prefix, serving everything on a sub-path.. Good idea.. ",
    "dsandersAzure": "Hi All, is there any timeline when this will make it into a Docker build? We're waiting for this feature to run cAdvisor behind Traefik.\nThanks, David.. @dashpole Thanks for the guidance - much appreciated :) . ",
    "iyesin": "@dashpole things you are saying is really contrasting with command-line options I observing for cAdvisor. Say --docker-prefixed option set says that cAdvisor can talk with docker daemon via the socket to retrieve container-related data (like env. variables, storage parameters and etc.). Furthermore, to get environment variables and container labels cAdvisor should call /containers/<container id or name>/json endpoint anyway. By calling this endpoint cAdvisor will receive this data anyway, so why not expose more data from there? (sorry for typos and editing). ",
    "changlan": "/retest. @dashpole Fixed.. ",
    "vishiy": "describe node gives ~2 cores\nAddresses:\n  InternalIP:  10.240.0.7\n  Hostname:    aks-agentpool-31908504-0\nCapacity:\ncpu:                2\nephemeral-storage:  30428648Ki\nhugepages-1Gi:      0\nhugepages-2Mi:      0\nmemory:             7137152Ki\npods:               110\nAllocatable:\ncpu:                1940m\nAlso see values from 5 samples over a small period -\n\"time\": \"2018-12-05T15:31:26Z\",\n    \"usageNanoCores\": 1280545579,\n    \"usageCoreNanoSeconds\": 1225467515929555\n\"time\": \"2018-12-05T15:36:47Z\",\n    \"usageNanoCores\": 1356248720,\n    \"usageCoreNanoSeconds\": 1225557494826444\ntime\": \"2018-12-05T15:41:48Z\",\n    \"usageNanoCores\": 1537002742,\n    \"usageCoreNanoSeconds\": 1225652276053291\n\"time\": \"2018-12-05T15:48:10Z\",\n    \"usageNanoCores\": 1273520551,\n    \"usageCoreNanoSeconds\": 1225760886100932\n\"time\": \"2018-12-05T15:53:42Z\",\n    \"usageNanoCores\": 1269020197,\n    \"usageCoreNanoSeconds\": 1225852938179140\nkubectl top nodes \n19% cpu 88% memory\n. does kubectl top nodes does long run averages ? Yeah, interesting why 10s averages are so different.. @dashpole  - Is usageCoreNanoSeconds cumulative ?. Thanks @dashpole . How about node & pod network metrics (rxBytes, TxBytes, rxErrors, txErrors etc..) ? Basically which metrics from the summary end point are cumulative & which are not ?. ",
    "GaelPerret": "Right, this is a very old log, but we don't use this version any longer, actually this is the trace that I the cadvisor containers are not reaching, they hang before.\nYes the daemon is responsive. \nI attached more logs with the verbose level = 10 (strangely it seems more verbose than with level = 42)\nlogs2.txt\nI got similar logs on all nodes, the last line is \"Start housekeeping for container...\"\nAlso, when I remove the lines:\n\n/sys:/sys:ro\n\nthen the cadvisor containers complete their initialization (but no metrics are present).. Thank you, I will restart the daemon in the coming days, let's hope this fix the problem. Do you see anything else that could help to investigate the issue? This is our production environment, so I am sure that docker is working well. \n. Hello, the problem disappears after I restarted the daemon.\nI don't know where is the problem, docker or cadisor, and I don't have any releavent logs, so let's close it.. ",
    "viberan": "thanks for your quick response. \ndoes it mean we should NOT see  /system.slice/* metrics when using docker_only flag? What do you refer to as machine level metrics?\nI'm just looking for a way to tell cAdvisor not to collect/expose /system.slice/ metrics as there is noticeable performance impact. We don't use kubernetes. so, is there a way not to collect system.slice/ metrics?. I wanted to elaborate more on our use case for cAdvisor and Prometheus.\nWe're running fleets of spot instances that come and go many times a day with peak number of 1500-2000 instances at a moment. Each spot instance hosts a dozen of docker containers which are not static either. All in all it is pretty dynamic environment which in turn produces millions of different time series and leads to extremely high cardinality metrics. \nBasically we need just a handful of metrics to monitor CPU and memory usage of every docker container at resolution of every few seconds.\ncAdvisor exposes over 2700 time series on every spot instance at any given time. While trying to ingest all of those from every spot into Prometheus, I observed over 400 millions(!) of head time series before it crashed the biggest Prometheus server I could try.\nYes, I can drop the unnecessary metrics at Prometheus side and it does help to get cardinality under control. Still the 2700 time series is over one megabyte of data which Prometheus has difficult time to scrape at high pace. Given there are some 2000 endpoints to scrape, it's a lot of traffic too. It seems there is cpu overhead for collecting un-needed metrics as well.\nFor the above reasons, it would be helpful to have the option to collect and expose docker related metrics only.. @dashpole thanks for taking care of this. Is the pull request #72787 above going to solve the issue when cAdvisor is used outside of kubernetes?. @dashpole at the moment cadvisor with the --docker_only flag collects metrics from the /system.slice cgroup which is exactly what we are trying to avoid. Is it going to change that behavior?. @dashpole sorry for delay in response. Below is excerpt from docker compose file:\ncadvisor:\n    image: google/cadvisor:v0.32.0\n    network_mode: bridge\n    container_name: cadvisor\n    ports:\n      - 9600:8080\n    volumes:\n      - /:/rootfs:ro\n      - /var/run:/var/run:rw\n      - /sys:/sys:ro\n      - /var/lib/docker/:/var/lib/docker:ro\n      - /dev/disk/:/dev/disk:ro\n    restart: unless-stopped\n    command: [\"--docker_only=true\"]\nthe output of docker top command:\nroot@xxxx# docker top cadvisor\nUID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD\nroot                1495                1477                8                   15:45               ?                   00:01:59            /usr/bin/cadvisor -logtostderr --docker_only=true\ngetting ~ 3000 timeseries for prometheus:\ncurl -s localhost:9600/metrics | wc -l\n3010\nwhich is about 1Mb of data\ncurl -s localhost:9600/metrics | wc -c\n1182807\nand  there are about 2000 system.slice related timeseries out of the above:\ncurl -s localhost:9600/metrics | grep 'system.slice' | wc -l\n2014\nmetric example:\ncontainer_tasks_state{container_label_com_docker_compose_config_hash=\"\",container_label_com_docker_compose_container_number=\"\",container_label_com_docker_compose_oneoff=\"\",container_label_com_docker_compose_project=\"\",container_label_com_docker_compose_service=\"\",container_label_com_docker_compose_version=\"\",container_label_name=\"\",id=\"**/system.slice/unattended-upgrades.service**\",image=\"\",name=\"\",state=\"uninterruptible\"} 0\ncontainer_tasks_state{container_label_com_docker_compose_config_hash=\"\",container_label_com_docker_compose_container_number=\"\",container_label_com_docker_compose_oneoff=\"\",container_label_com_docker_compose_project=\"\",container_label_com_docker_compose_service=\"\",container_label_com_docker_compose_version=\"\",container_label_name=\"\",id=\"**/system.slice/watchdog.service**\",image=\"\",name=\"\",state=\"iowaiting\"} 0\nThere are tons of system.slice stuff:\n```\ncurl -s localhost:9600/metrics | grep system.slice | cut -d'{' -f1 | sort | uniq -c\n     64 container_cpu_load_average_10s\n     64 container_cpu_system_seconds_total\n    136 container_cpu_usage_seconds_total\n     64 container_cpu_user_seconds_total\n     38 container_fs_reads_bytes_total\n     38 container_fs_reads_total\n     38 container_fs_writes_bytes_total\n     38 container_fs_writes_total\n     64 container_last_seen\n     64 container_memory_cache\n     64 container_memory_failcnt\n    256 container_memory_failures_total\n     64 container_memory_mapped_file\n     64 container_memory_max_usage_bytes\n     64 container_memory_rss\n     64 container_memory_swap\n     64 container_memory_usage_bytes\n     64 container_memory_working_set_bytes\n     64 container_spec_cpu_period\n     64 container_spec_cpu_shares\n     64 container_spec_memory_limit_bytes\n     64 container_spec_memory_reservation_limit_bytes\n     64 container_spec_memory_swap_limit_bytes\n     64 container_start_time_seconds\n    320 container_tasks_state\ncurl -s localhost:9600/metrics | grep -Eo 'id=\"/system.slice[^\"]+' | sort  | uniq -c\n     35 id=\"/system.slice/accounts-daemon.service\n     33 id=\"/system.slice/acpid.service\n     27 id=\"/system.slice/apparmor.service\n     27 id=\"/system.slice/apport.service\n     35 id=\"/system.slice/apt-daily-upgrade.service\n     34 id=\"/system.slice/atd.service\n     27 id=\"/system.slice/cgroupfs-mount.service\n     35 id=\"/system.slice/cloud-config.service\n     35 id=\"/system.slice/cloud-final.service\n     27 id=\"/system.slice/cloud-init-local.service\n     35 id=\"/system.slice/cloud-init.service\n     27 id=\"/system.slice/console-setup.service\n     35 id=\"/system.slice/cron.service\n     35 id=\"/system.slice/dbus.service\n     35 id=\"/system.slice/docker.service\n     35 id=\"/system.slice/exim4.service\n     35 id=\"/system.slice/filebeat.service\n     27 id=\"/system.slice/grub-common.service\n     35 id=\"/system.slice/ifup@ens5.service\n     35 id=\"/system.slice/irqbalance.service\n     35 id=\"/system.slice/iscsid.service\n     27 id=\"/system.slice/keyboard-setup.service\n     27 id=\"/system.slice/kmod-static-nodes.service\n     35 id=\"/system.slice/lvm2-lvmetad.service\n     27 id=\"/system.slice/lvm2-monitor.service\n     35 id=\"/system.slice/lxcfs.service\n     27 id=\"/system.slice/lxd-containers.service\n     35 id=\"/system.slice/mdadm.service\n     27 id=\"/system.slice/networking.service\n     35 id=\"/system.slice/node_exporter.service\n     35 id=\"/system.slice/nscd.service\n     35 id=\"/system.slice/nslcd.service\n     35 id=\"/system.slice/ntp.service\n     27 id=\"/system.slice/ondemand.service\n     27 id=\"/system.slice/open-iscsi.service\n     35 id=\"/system.slice/polkitd.service\n     35 id=\"/system.slice/protologbeat.service\n     27 id=\"/system.slice/rc-local.service\n     27 id=\"/system.slice/resolvconf.service\n     35 id=\"/system.slice/rsyslog.service\n     27 id=\"/system.slice/setvtrgb.service\n     27 id=\"/system.slice/snapd.seeded.service\n     39 id=\"/system.slice/snapd.service\n     35 id=\"/system.slice/ssh.service\n     35 id=\"/system.slice/systemd-journald.service\n     27 id=\"/system.slice/systemd-journal-flush.service\n     35 id=\"/system.slice/systemd-logind.service\n     27 id=\"/system.slice/systemd-modules-load.service\n     27 id=\"/system.slice/systemd-random-seed.service\n     27 id=\"/system.slice/systemd-remount-fs.service\n     27 id=\"/system.slice/systemd-sysctl.service\n     27 id=\"/system.slice/systemd-tmpfiles-setup-dev.service\n     27 id=\"/system.slice/systemd-tmpfiles-setup.service\n     39 id=\"/system.slice/systemd-udevd.service\n     27 id=\"/system.slice/systemd-udev-trigger.service\n     27 id=\"/system.slice/systemd-update-utmp.service\n     27 id=\"/system.slice/systemd-user-sessions.service\n     34 id=\"/system.slice/system-getty.slice\n     35 id=\"/system.slice/system-serial\\x2dgetty.slice\n     27 id=\"/system.slice/ufw.service\n     27 id=\"/system.slice/unattended-upgrades.service\n     35 id=\"/system.slice/watchdog.service\n```\nDo I miss anything in the configuration? Thanks.. @dashpole not sure whether it's a bug or a feature, what I could understand from the source below, accept will always be true for default []string{\"/\"} value of rawPrefixWhiteList \nhttps://github.com/google/cadvisor/blob/150629c099b66e13223ec0601fdf9d49a3282c68/container/raw/factory.go#L70-L75\nThis would allow all raw cgroups to be always collected, hence there is all that /system.slice/* stuff reported by cAdvisor regardless of -docker_only flag being set.. @dashpole  thank you for the #2161\nWould you mind to add the ability to white list certain cgroup prefixes via a command line flag - #2164  . @dashpole done. @dashpole would raw_cgroup_prefix_whitelist be a better fit?. ",
    "lixianyang": "We don't use kubernetes too, but want to use cadvisor to collect docker container metrics. It collect all cgroup metrics in / , prometheus curl these metrics take a long time.. ",
    "bascouba": "Thank you for your answer.\nWe're using Rancher 1.x because we don't use kubernetes yet. Our rancher environnement is based on cattle. And cattle doesn't seems to provide this kind of information. Found out I could maybe use the cadvisor_version_info's instance tag to categorize container's hosts. I'll dig into that and return results here.\nedit: Ok so it was what I was looking for, sorry I didn't find it out before posting. Thank you again. ",
    "RobertKrawitz": "\n\ncadvisor currently does not provide free bytes and inodes for container volumes;\n\nBy container volumes, do you mean PVs? Or are you including \"ephemeral local storage\" ones as well (empty-dir backed volumes)?\n\nEphemeral also (including writable layers).\n\n\nthis information is useful for e. g. eviction decisions based on inode pressure in Kubernetes/OpenShift.\n\nCan you expand on this? We currently only take usage into account for inode pressure. We wouldn't want to take PV usage into account, as those are on their own filesystems and have a separate pool of inodes. We already take empty-dir usage into account, but those metrics are provided by the kubelet.\n\nThe issue is that the container_fs_inodes_free metric is returning 0 for all volumes, so we can't alert on pods that are close to running out of inodes on one or more of their filesystems.. > > Ephemeral also (including writable layers).\n\nSo this is really just for container writable layers, right? Because cAdvisor doesn't monitor empty-dir backed volumes.\n\nYes.\n\n\nThe issue is that the container_fs_inodes_free metric is returning 0 for all volumes, so we can't alert on pods that are close to running out of inodes on one or more of their filesystems.\n\ncontainer_fs_inodes_free for a container should be the same as that metric for the node, right? Can we just alert on that metric? Does that work for devicemapper, or does that have a separate filesystem for each writable layer?\n\nI don't have a devicemapper cluster set up to test that (and the underlying issue referred to use of an overlay2 filesystem); with overlay2, I've verified that the free inodes is reported correctly with\ncontainer_fs_inodes_free{container_name=\"\"}. If we decide not to do this, then perhaps we shouldn't be reporting the -free information for containers at all, to eliminate a source of confusion.. It does appear that when using devicemapper that each container's root filesystem shows a different number of free inodes.\nI created one pod that created 18 files under the local /tmp, and another that created only 3.  df -i on the pod with 18 files showed\nFilesystem              Inodes      Used Available Use% Mounted on\n/dev/mapper/docker-259:2-11537289-8d1bdb5a474934e969166e66a573a53584de9698d4f9bbc544589f31a1f6f324\n                       5242880        63   5242817   0% /\nOn the one with only three files under the local /tmp, however, I got this:\nFilesystem              Inodes      Used Available Use% Mounted on\n/dev/mapper/docker-259:2-11537289-f1b5d44a0eccafa0ed8333428696e068c84998e8680970669591eb26506db05e\n                       5242880        47   5242833   0% /\nSo there may be value in doing this, after all.. ",
    "SuperQ": "/cc @brancz. ",
    "dick-twocows": "(apologies, hit enter too soon)\nIt would be useful if the web UI could be configured to auto refresh at a set frequency, i.e. 10 seconds.. ",
    "k0nstantinv": "@dashpole thanks. I first tried latest tag, but it was outdated, so I desided to use v0.30.2 the same way as in https://github.com/google/cadvisor/blob/master/deploy/kubernetes/base/daemonset.yaml\nI'm not sure which tag should i test now. Can you advise? . Seems like disabling network really causing tcp metrics be zero even if I use the v.0.32.0 tag. I have 4 running containers on localhost:\nCONTAINER ID        IMAGE                     COMMAND                  CREATED             STATUS              PORTS                    NAMES\n5f820c33c9ea        google/cadvisor:v0.32.0   \"/usr/bin/cadvisor -\u2026\"   2 minutes ago       Up 2 minutes        0.0.0.0:8080->8080/tcp   cadvisor\n40e0315e658f        debian:wheezy             \"bash\"                   4 weeks ago         Up 47 seconds                                puppet-agent-2\ncabe397f89ab        devopsil/puppet           \"bash\"                   4 weeks ago         Up About a minute                            puppet-agent\na37616aab79a        devopsil/puppet           \"bash\"                   4 weeks ago         Up 29 seconds                                puppet-master\nAs you can see i have Puppet master with 2 registered agents and they are configured correctly:\n```\n$ docker exec puppet-agent puppet agent -t\nInfo: Retrieving pluginfacts\nInfo: Retrieving plugin\nInfo: Caching catalog for puppet-agent.my.local\nInfo: Applying configuration version '1547453764'\nNotice: Finished catalog run in 0.02 seconds\n$ docker exec puppet-agent-2 puppet agent -t\nInfo: Retrieving pluginfacts\nInfo: Retrieving plugin\nInfo: Caching catalog for puppet-agent-2.my.local\nInfo: Applying configuration version '1547453764'\nNotice: Finished catalog run in 0.01 seconds\n[root@puppet-master /]# netstat -tlnp\nActive Internet connections (only servers)\nProto Recv-Q Send-Q Local Address               Foreign Address             State       PID/Program name\ntcp        0      0 0.0.0.0:8140                0.0.0.0:                   LISTEN      -\ntcp        0      0 127.0.0.11:42191            0.0.0.0:                   LISTEN      -\nIf cAdvisor's command to run is equal to:\n$ docker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:ro \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  google/cadvisor:v0.32.0 --disable_metrics=udp,network --docker_only\nthen:\ncontainer_network_tcp_usage_total{container_label_build_date=\"20180402\",container_label_license=\"GPLv2\",container_label_name=\"CentOS Base Image\",container_label_vendor=\"CentOS\",id=\"/docker/a37616aab79a1374046aada7bb69d0c6ed41c63953098bc450dae7166868c5ec\",image=\"devopsil/puppet\",name=\"puppet-master\",tcp_state=\"listen\"} 0\nIf cAdvisor's command to run is equal to:\n$ docker run \\\n  --volume=/:/rootfs:ro \\\n  --volume=/var/run:/var/run:ro \\\n  --volume=/sys:/sys:ro \\\n  --volume=/var/lib/docker/:/var/lib/docker:ro \\\n  --publish=8080:8080 \\\n  --detach=true \\\n  --name=cadvisor \\\n  google/cadvisor:v0.32.0 --disable_metrics=udp --docker_only\nthen it is starting to show values:\ncontainer_network_tcp_usage_total{container_label_build_date=\"20180402\",container_label_license=\"GPLv2\",container_label_name=\"CentOS Base Image\",container_label_vendor=\"CentOS\",id=\"/docker/a37616aab79a1374046aada7bb69d0c6ed41c63953098bc450dae7166868c5ec\",image=\"devopsil/puppet\",name=\"puppet-master\",tcp_state=\"listen\"} 2\nThen, disabling all the metrics except tcp via:\n--disable_metrics=network,udp,percpu,sched,process --docker_only\nmakes the `/metrics` endpoint return this list of metrics:\ncontainer_cpu_load_average_10s\ncontainer_cpu_system_seconds_total\ncontainer_cpu_usage_seconds_total\ncontainer_cpu_user_seconds_total\ncontainer_fs_inodes_free\ncontainer_fs_inodes_total\ncontainer_fs_io_current\ncontainer_fs_io_time_seconds_total\ncontainer_fs_io_time_weighted_seconds_total\ncontainer_fs_limit_bytes\ncontainer_fs_read_seconds_total\ncontainer_fs_reads_bytes_total\ncontainer_fs_reads_merged_total\ncontainer_fs_reads_total\ncontainer_fs_sector_reads_total\ncontainer_fs_sector_writes_total\ncontainer_fs_usage_bytes\ncontainer_fs_write_seconds_total\ncontainer_fs_writes_bytes_total\ncontainer_fs_writes_merged_total\ncontainer_fs_writes_total\ncontainer_last_seen\ncontainer_memory_cache\ncontainer_memory_failcnt\ncontainer_memory_failures_total\ncontainer_memory_mapped_file\ncontainer_memory_max_usage_bytes\ncontainer_memory_rss\ncontainer_memory_swap\ncontainer_memory_usage_bytes\ncontainer_memory_working_set_bytes\ncontainer_network_tcp_usage_total\ncontainer_scrape_error\ncontainer_spec_cpu_period\ncontainer_spec_cpu_shares\ncontainer_spec_memory_limit_bytes\ncontainer_spec_memory_reservation_limit_bytes\ncontainer_spec_memory_swap_limit_bytes\ncontainer_start_time_seconds\ncontainer_tasks_state\n``\nWell...with the current version I can't see always zero values and disabled metrics really disappeared from the endpoint, but it still has a group of metrics I don't need.--disable_metrics` option does not provide mechanism to exclude above metrics from the endpoint.  You would certainly know better than I, but I think this is incorrect. . @dashpole, so is there a way to fix this in 1.8.11? I can't upgrade one of my clusters right now due to legacy reasons. ",
    "njouve": "Oh really ? IMHO that makes it very unpractical to use for application metrics, if you have to handle port mappings for all the containers you want to monitor.....  Ultimately my goal was to monitor these containerized application metrics with Prometheus. Can you think to any alternative solution for this ?. You are right for \"pure application metrics\" who not depend on the container. But for JVM metrics that are container specific, you have to implement some kind of container discovery. Currently we still use docker swarm (we may switch to Kubernetes in a near future), and container discovery for docker swarm is not available AFAIK.... Yeah, this was my last resort option. But I will dig this. Thank you for your precious insights and advices. Have a good day. I close the issue. ",
    "namreg": "I signed it. /retest. @dashpole fixed. @dashpole Do you mean how much data in the directory when benchmarking? To be honest, I have ran it in a small directory that contains ~10 items. Also, I ran benchmark only for du command.\nBut I have remade benchmarks and now have the following results:\ngoos: linux\ngoarch: amd64\npkg: github.com/namreg/find-du-bench\nBenchmarkDiskUsageNew-2        20000         85928 ns/op       22720 B/op         90 allocs/op\nBenchmarkDiskUsageOld-2           50      34123838 ns/op       46605 B/op        182 allocs/op\nBenchmarks is running on the cadvisor source code directory, including .git (2557 files, ~57M). The benchmarks source code you can see in the repo https://github.com/namreg/find-du-bench. @dashpole @euank So, I've tried to run the benchmark on the one of the kubernetes node in the directory /var/lib/docker (the size is ~17GB), as was proposed above. And now I have the following results:\ngoos: linux\ngoarch: amd64\nBenchmarkDiskUsageNew-4            5     249946403 ns/op    96558353 B/op     245327 allocs/op\nBenchmarkDiskUsageOld-4            1    1132808839 ns/op       63032 B/op        238 allocs/op\nAs we can see, new implementation is still faster, but it consumes a lot more memory. I believe this happens because go runtime does not track memory for processes that were spawned by exec.Command.\nNevertheless, here are pprof results:\nShowing nodes accounting for 777.70MB, 99.49% of 781.71MB total\nDropped 8 nodes (cum <= 3.91MB)\nShowing top 10 nodes out of 26\n      flat  flat%   sum%        cum   cum%\n  548.19MB 70.13% 70.13%   557.21MB 71.28%  os.(*File).readdirnames\n   85.52MB 10.94% 81.07%   145.03MB 18.55%  os.lstatNolog\n   64.51MB  8.25% 89.32%    64.51MB  8.25%  syscall.ByteSliceFromString\n   61.01MB  7.80% 97.12%    61.01MB  7.80%  strings.Join\n    9.03MB  1.15% 98.28%     9.03MB  1.15%  syscall.ParseDirent\n    5.50MB   0.7% 98.98%     5.50MB   0.7%  os.newFile\n    3.96MB  0.51% 99.49%     3.96MB  0.51%  _/tmp/find-du-bench.DiskUsageNew.func1\n         0     0% 99.49%   781.20MB 99.94%  _/tmp/find-du-bench.BenchmarkDiskUsageNew\n         0     0% 99.49%   781.20MB 99.94%  _/tmp/find-du-bench.DiskUsageNew\n         0     0% 99.49%   557.21MB 71.28%  os.(*File).Readdirnames\n(pprof) list .readdirnames\nTotal: 781.71MB\nROUTINE ======================== os.(*File).readdirnames in /usr/local/go/src/os/dir_unix.go\n  548.19MB   557.21MB (flat, cum) 71.28% of Total\n         .          .     45:}\n         .          .     46:\n         .          .     47:func (f *File) readdirnames(n int) (names []string, err error) {\n         .          .     48:   // If this file has no dirinfo, create one.\n         .          .     49:   if f.dirinfo == nil {\n    2.50MB     2.50MB     50:       f.dirinfo = new(dirInfo)\n         .          .     51:       // The buffer must be at least a block long.\n  454.03MB   454.03MB     52:       f.dirinfo.buf = make([]byte, blockSize)\n         .          .     53:   }\n         .          .     54:   d := f.dirinfo\n         .          .     55:\n         .          .     56:   size := n\n         .          .     57:   if size <= 0 {\n         .          .     58:       size = 100\n         .          .     59:       n = -1\n         .          .     60:   }\n         .          .     61:\n   91.66MB    91.66MB     62:   names = make([]string, 0, size) // Empty with room to grow.\n         .          .     63:   for n != 0 {\n         .          .     64:       // Refill the buffer if necessary\n         .          .     65:       if d.bufp >= d.nbuf {\n         .          .     66:           d.bufp = 0\n         .          .     67:           var errno error\n         .          .     68:           d.nbuf, errno = f.pfd.ReadDirent(d.buf)\n         .          .     69:           runtime.KeepAlive(f)\n         .          .     70:           if errno != nil {\n         .          .     71:               return names, wrapSyscallError(\"readdirent\", errno)\n         .          .     72:           }\n         .          .     73:           if d.nbuf <= 0 {\n         .          .     74:               break // EOF\n         .          .     75:           }\n         .          .     76:       }\n         .          .     77:\n         .          .     78:       // Drain the buffer\n         .          .     79:       var nb, nc int\n         .     9.03MB     80:       nb, nc, names = syscall.ParseDirent(d.buf[d.bufp:d.nbuf], n, names)\n         .          .     81:       d.bufp += nb\n         .          .     82:       n -= nc\n         .          .     83:   }\n         .          .     84:   if n >= 0 && len(names) == 0 {\n         .          .     85:       return names, io.EOF. @dashpole \n\nYou mentioned at the beginning: \"both du and find overwhelm our audit log\". Have you done any experiments to show that this helps your issue?\n\nYeah, I have deployed this patch into a kubernetes cluster. As a result, number of the audit log items drastically decreased. It turns out, ionice and nice was also being there. Hence, this patch removes from audit logs four classes of records: du, find, ionice, nice. @dashpole yeah, exactly :)\nDoes anything else concern you in this PR? . @dashpole is there anything else I should fix? . I have added GetDirInodeUsage(string, time.Duration) (uint64, error) and GetDirDiskUsage(string, time.Duration) (uint64, error) back in and have marked its as deprecated. Also GetDirUsage(string) (UsageInfo, error) was added. I think this make sense according to the semver. removed its. ",
    "zq-david-wang": "the write may still in kernel buffer and is not counted yet, this happens when you have large amount of free memory. Try sync and verify the cgroup metrics . ",
    "CpuID": "Tried that, same outcome. Tried it RO and RW.. ",
    "tommmlij": "Run the cadvisor container with privileged: true. ",
    "torhoehn": "@tommmlij That was working for me.. ",
    "arjun-dandagi": "@dashpole  so cAdvisor only exposes container metrics ( for Prometheus by default), what if I want to scrape the disk space available on each node? (should I install node_exporter on each of my GK instances in the cluster ? ) what I am trying to find is, I need only one exporter about entire cluster in GKE\n2>> while trying out container_cpu_load_average_10s, I see 0 values all the time when referred to  this link, I see that I need to pass the flag to cAdvisor since you say that it is builtin to Kubelet, how do I enable enable_load_reader=true. ",
    "6sossomons": "Sorry, I wound up making changes and didn't realize it..\nWe DO remove the container after stopping and then run the new one, with the same name.   Edited the original to reflect it.. ",
    "dtrejod": "Agreed. I updated the original commit. . I signed it!. @dashpole Any blockers to accepting this PR? . ",
    "BootsSiR": "@dashpole I get that single line of logging and it's DOA until I stop the container.  I cannot access the webui at all.. ",
    "hatharom": "I've just followed a simple tutorial link\nSo it isnt a recommended way to this? Just because a metric is either 0 or accurate (it seems there isnt any inaccurate value - at least at first glance).\n. After running docker inspect container_id  it seems that the State.StartedAt updates successfully.\nCreating and starting the container at 12:15 then stopping it at 12:29 then restarting at 12:30.\n{\n      \"Id\": \"150aceb457d66818288f9b0404eb8ea423b959521930dfa29b2ed43345d4d90b\",\n      \"Created\": \"2019-02-14T12:15:18.7356321Z\",\n      \"Path\": \"/postgres_exporter\",\n      \"Args\": [\n          \"--extend.query-path=/queries.yaml\"\n      ],\n      \"State\": {\n          \"Status\": \"running\",\n          \"Running\": true,\n          \"Paused\": false,\n          \"Restarting\": false,\n          \"OOMKilled\": false,\n          \"Dead\": false,\n          \"Pid\": 21225,\n          \"ExitCode\": 0,\n          \"Error\": \"\",\n          \"StartedAt\": \"2019-02-14T12:30:47.2003058Z\",\n          \"FinishedAt\": \"2019-02-14T12:29:21.1354284Z\"\n...etc\nLaunching the (time()-container_start_time_seconds)/60 query at 12:31 shows 16min. Seems it fetches from the 'created' field.. ",
    "darkpssngr": "I think it would be great if we can get the blog fixed. But if its not possible we should fix the doc.. ",
    "gaorong": "/assign @vmarmol . the CI is green now. ",
    "dustinschultz": "To general commands, yes, docker responds with no problems (docker ps, docker info, etc).\nAny command you issue to the problematic instance will hang though (docker stop <instance_id>, docker rm -f <instance_id>, etc). ",
    "mullikine": "Hi @dashpole ,\nShane from CodeLingo here. We stuffed up, sorry.\nWe made an error when writing a Tenet that was used on your repository that resulted in incorrect changes to your code in some edge cases. We've fixed the bug and improved our internal process to prevent this happening again.\nWe're keen to learn how we can best help dev teams without getting in your way. Your feedback would really help us, my email's shane@codelingo.io.\nThanks,\nShane & the CodeLingo Team. ",
    "replicajune": "I'm quite unaware of all specifications that could exist at this time. I'm under the impression (and could be wrong) that the OCI had or would propose something standard for this.\nSo, I've no idea unfortunately\nThe need I have is to have a metric that is about the work produced by a container rather than a state (container_tasks_state) of a processus or the fact that a container might be up or not.\nThe healthcheck instruction and related statistics with docker helps to really figure out if a container actually does what it should and I don't really see metrics about that for now\n. ",
    "nekop": "I signed it!. > Can you describe such a case?\nWhen a container and the cgroups removed before read the file. This error log appears multiple times every day in healthy k8s clusters.. ",
    "idelgarde": "If I have a cluster with different nodes, I would not know which machine I`m using,except for checking the IP on the link. I tried, from the available API on documentation, none of them returns the value of the hostname. Actually my idea is only adding the hostname under the logo on every possible page.. ",
    "wongchao": "rancher1.6.14 + cattles\ncAdvisor version v0.32.0 (8949c822)\nDocker version 1.12.6, build 78d1802\nIn my ENV, there is no ContainerID changed by restarting docker. then container_start_time_seconds is the same. \nbut if Updating container makes ID changed ,  container_start_time_seconds will be changed.\n(time()-container_start_time_seconds) couldn't be used as container restart metric\n. ",
    "ravjanga": "Even we are hitting the same issue.\nIdeally it should be like below\ncontainer_start_time_seconds should be renamed as container_create_time_seconds\nAnd then container_start_time_seconds should be modified to use the proper start time from the docker daemon. This would work for both docker/swarm and k8 based installations.\nAt this point looks like container_start_time_seconds is primarily catered towards K8, where new pods are created.. Actually I tried modifying the code to add start time as a new metric as it is already available as part of the docker spec that is being for collecting create time and other static metrics like memory limit.\nIt is working fine. But looks like this part of the code doesn\u2019t get executed for every poll and hence restarts of the container are not picking up new start time.\nSimilar problem seems to exist if I update the docker memory limits after cadvisor is launched.\n. I understand the impact, but atleast if some control can be provided like how often to query these parameters would definitely help.  I tried making these changes, but getting into few other issues. Will share the details if I am able to get this working.\nStart Time, memory limits are not static and can change over the life of the container.  \nDocker offers update of the below properties to be changed at runtime as well.\n  --blkio-weight uint16        Block IO (relative weight), between 10 and 1000, or 0 to disable (default 0)\n  --cpu-period int             Limit CPU CFS (Completely Fair Scheduler) period\n  --cpu-quota int              Limit CPU CFS (Completely Fair Scheduler) quota\n  --cpu-rt-period int          Limit the CPU real-time period in microseconds\n  --cpu-rt-runtime int         Limit the CPU real-time runtime in microseconds\n\n-c, --cpu-shares int             CPU shares (relative weight)\n      --cpus decimal               Number of CPUs\n      --cpuset-cpus string         CPUs in which to allow execution (0-3, 0,1)\n      --cpuset-mems string         MEMs in which to allow execution (0-3, 0,1)\n      --kernel-memory bytes        Kernel memory limit\n  -m, --memory bytes               Memory limit\n      --memory-reservation bytes   Memory soft limit\n      --memory-swap bytes          Swap limit equal to memory plus swap: '-1' to enable unlimited swap\n      --restart string             Restart policy to apply when a container exits\n. ",
    "fontanacalifornia": "Thanks for the quick reply.\nWhen running the docker info command from the host directly. I have to specify the following to get it to respond:\n\"docker -H tcp://[host IP]:2376 --tlsverify --tlscacert=/path/to/ca.pem --tlscert=/path/to/cert.pem --tlskey=/path/to/key.pem info\"\nIt is only listening to 2376 and not 2375 and also when trying the command with the -H I get the following:\n\"certificate is valid for [network name].localhost., not localhost\"\nI am guessing I need to tweak cAdvisor to call the docker daemon specifically for these needs but do not know how to do so.. Is there a way to set the docker= option to a network interface instead of a socket?\nIf so what would be the format? The same as what we call with \"-H tcp://xxx.xxx.xxx.xxx:2376\"\nI have tried many combinations but can't seem to get it right.\nThe default socket does not exist on this server. but /var/run/docker/libcontainerd/docker-containerd.sock does exist. I tried using that with no other options, and I get the \"Are you trying to connect to a TLS-enabled daemon without TLS?\" error.\nSo I tried that with all the tls options to no success.\nIs the tls cert options local to where I am running the Docker command from? Which is my windows workstation.\nPlease advise.. Just Docker for now.\nSo you are saying that I need to include the certificate file in the container itself. I can work with that and let you know how it goes.\nThanks for the update.. One more question. Do the tls-cert commands take a absolute or relative path, or just a file name and assume a specific location?. These should be the certificates used to access the docker host correct?\nJust for testing purposes, I am copying the the certificates into the container using a dockerfile/custom image. I just put them in /home and used an absolute path in the tls commands. The weird thing is that the container will launch but not run no matter what I used for the TLS commands.\n. The container will not stay running, so I cannot check the validate page for more info.. I am sorry to say that I figured it out : )\nI ended up just needed to pull the latest vmware/dch-photon image and it now allows for both the network interface and /var/run/docker sockets to run simultaneously. \nThanks for all your guidance. I learned quite a bit along the way.. ",
    "ankitrohi": "I'll take a look at the network metrics. But I have no idea for fs ones.\nPlease have a look.\nThanks. ",
    "tongbinxiang": "cadvisor log is as follows:\nI0219 09:20:37.933860       1 storagedriver.go:50] Caching stats in memory for 2m0s\nI0219 09:20:37.934186       1 manager.go:151] cAdvisor running in container: \"/sys/fs/cgroup/cpu,cpuacct\"\nI0219 09:20:38.018088       1 fs.go:139] Filesystem UUIDs: map[79632962-af28-491d-ad41-8147a6c26188:/dev/sda1 a0de5d03-6714-41e9-abae-fc1219131e26:/dev/sdb1 f09b9eec-fc73-48a4-bd46-08c5f71543c2:/dev/sda2]\nI0219 09:20:38.018134       1 fs.go:140] Filesystem partitions: map[/dev/sdb1:{mountpoint:/rootfs/home major:8 minor:17 fsType:ext4 blockSize:0} shm:{mountpoint:/rootfs/var/lib/docker/containers/245c6964608dada9c65d6ba95d293419e43e13a4d99332de0e228dae7ae0916d/mounts/shm major:0 minor:56 fsType:tmpfs blockSize:0} tmpfs:{mountpoint:/dev major:0 minor:59 fsType:tmpfs blockSize:0} /dev/sda1:{mountpoint:/var/lib/docker major:8 minor:1 fsType:ext4 blockSize:0}]\nI0219 09:20:38.025345       1 manager.go:225] Machine: {NumCores:40 CpuFrequency:3400000 MemoryCapacity:135081676800 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] MachineID:398a0844cc22fd0844eb7106582008b0 SystemUUID:4C4C4544-005A-3910-8031-B4C04F4D4732 BootID:26cbef0a-5a1f-4b75-9bfe-9851ccd47692 Filesystems:[{Device:/dev/sdb1 DeviceMajor:8 DeviceMinor:17 Capacity:787474333696 Type:vfs Inodes:48840704 HasInodes:true} {Device:shm DeviceMajor:0 DeviceMinor:56 Capacity:67108864 Type:vfs Inodes:16489462 HasInodes:true} {Device:none DeviceMajor:0 DeviceMinor:55 Capacity:779633565696 Type:vfs Inodes:48357376 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:59 Capacity:67108864 Type:vfs Inodes:16489462 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:779633565696 Type:vfs Inodes:48357376 HasInodes:true}] DiskMap:map[8:0:{Name:sda Major:8 Minor:0 Size:800166076416 Scheduler:deadline} 8:16:{Name:sdb Major:8 Minor:16 Size:800166076416 Scheduler:deadline}] NetworkDevices:[{Name:eno1 MacAddress:f4:8e:38:ce:eb:64 Speed:1000 Mtu:1500} {Name:eno2 MacAddress:f4:8e:38:ce:eb:66 Speed:-1 Mtu:1500} {Name:enp20s0f0 MacAddress:a0:36:9f:c9:ad:ac Speed:10000 Mtu:1500} {Name:enp20s0f1 MacAddress:a0:36:9f:c9:ad:ae Speed:-1 Mtu:1500} {Name:mesos168088 MacAddress:f4:8e:38:ce:eb:64 Speed:10000 Mtu:1500} {Name:mesos61637 MacAddress:f4:8e:38:ce:eb:64 Speed:10000 Mtu:1500}] Topology:[{Id:0 Memory:67450953728 Cores:[{Id:0 Threads:[0 20] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[2 22] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:2 Threads:[4 24] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:3 Threads:[6 26] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:4 Threads:[8 28] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:8 Threads:[10 30] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:9 Threads:[12 32] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:10 Threads:[14 34] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:11 Threads:[16 36] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:12 Threads:[18 38] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:26214400 Type:Unified Level:3}]} {Id:1 Memory:67630723072 Cores:[{Id:0 Threads:[1 21] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[3 23] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:2 Threads:[5 25] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:3 Threads:[7 27] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:4 Threads:[9 29] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:8 Threads:[11 31] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:9 Threads:[13 33] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:10 Threads:[15 35] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:11 Threads:[17 37] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:12 Threads:[19 39] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:26214400 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}\nI0219 09:20:38.026413       1 manager.go:231] Version: {KernelVersion:4.4.0-141-generic ContainerOsVersion:Alpine Linux v3.4 DockerVersion:18.03.1-ce DockerAPIVersion:1.37 CadvisorVersion:v0.28.3 CadvisorRevision:1e567c2}\nI0219 09:20:38.055068       1 factory.go:356] Registering Docker factory\nI0219 09:20:40.055562       1 factory.go:54] Registering systemd factory\nI0219 09:20:40.057960       1 factory.go:86] Registering Raw factory\nI0219 09:20:40.059964       1 manager.go:1178] Started watching for new ooms in manager\nI0219 09:20:40.067562       1 nvidia.go:110] NVML initialized. Number of nvidia devices: 8\nI0219 09:20:40.127912       1 manager.go:329] Starting recovery of all containers\nI0219 09:20:40.290675       1 manager.go:334] Recovery completed\nI0219 09:20:40.483756       1 cadvisor.go:162] Starting cAdvisor version: v0.28.3-1e567c2 on port 8080. After cadvsior restarts, cadvsior log is as follows:\nI0219 09:20:37.933860       1 storagedriver.go:50] Caching stats in memory for 2m0s\nI0219 09:20:37.934186       1 manager.go:151] cAdvisor running in container: \"/sys/fs/cgroup/cpu,cpuacct\"\nI0219 09:20:38.018088       1 fs.go:139] Filesystem UUIDs: map[79632962-af28-491d-ad41-8147a6c26188:/dev/sda1 a0de5d03-6714-41e9-abae-fc1219131e26:/dev/sdb1 f09b9eec-fc73-48a4-bd46-08c5f71543c2:/dev/sda2]\nI0219 09:20:38.018134       1 fs.go:140] Filesystem partitions: map[/dev/sdb1:{mountpoint:/rootfs/home major:8 minor:17 fsType:ext4 blockSize:0} shm:{mountpoint:/rootfs/var/lib/docker/containers/245c6964608dada9c65d6ba95d293419e43e13a4d99332de0e228dae7ae0916d/mounts/shm major:0 minor:56 fsType:tmpfs blockSize:0} tmpfs:{mountpoint:/dev major:0 minor:59 fsType:tmpfs blockSize:0} /dev/sda1:{mountpoint:/var/lib/docker major:8 minor:1 fsType:ext4 blockSize:0}]\nI0219 09:20:38.025345       1 manager.go:225] Machine: {NumCores:40 CpuFrequency:3400000 MemoryCapacity:135081676800 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] MachineID:398a0844cc22fd0844eb7106582008b0 SystemUUID:4C4C4544-005A-3910-8031-B4C04F4D4732 BootID:26cbef0a-5a1f-4b75-9bfe-9851ccd47692 Filesystems:[{Device:/dev/sdb1 DeviceMajor:8 DeviceMinor:17 Capacity:787474333696 Type:vfs Inodes:48840704 HasInodes:true} {Device:shm DeviceMajor:0 DeviceMinor:56 Capacity:67108864 Type:vfs Inodes:16489462 HasInodes:true} {Device:none DeviceMajor:0 DeviceMinor:55 Capacity:779633565696 Type:vfs Inodes:48357376 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:59 Capacity:67108864 Type:vfs Inodes:16489462 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:779633565696 Type:vfs Inodes:48357376 HasInodes:true}] DiskMap:map[8:0:{Name:sda Major:8 Minor:0 Size:800166076416 Scheduler:deadline} 8:16:{Name:sdb Major:8 Minor:16 Size:800166076416 Scheduler:deadline}] NetworkDevices:[{Name:eno1 MacAddress:f4:8e:38:ce:eb:64 Speed:1000 Mtu:1500} {Name:eno2 MacAddress:f4:8e:38:ce:eb:66 Speed:-1 Mtu:1500} {Name:enp20s0f0 MacAddress:a0:36:9f:c9:ad:ac Speed:10000 Mtu:1500} {Name:enp20s0f1 MacAddress:a0:36:9f:c9:ad:ae Speed:-1 Mtu:1500} {Name:mesos168088 MacAddress:f4:8e:38:ce:eb:64 Speed:10000 Mtu:1500} {Name:mesos61637 MacAddress:f4:8e:38:ce:eb:64 Speed:10000 Mtu:1500}] Topology:[{Id:0 Memory:67450953728 Cores:[{Id:0 Threads:[0 20] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[2 22] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:2 Threads:[4 24] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:3 Threads:[6 26] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:4 Threads:[8 28] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:8 Threads:[10 30] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:9 Threads:[12 32] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:10 Threads:[14 34] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:11 Threads:[16 36] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:12 Threads:[18 38] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:26214400 Type:Unified Level:3}]} {Id:1 Memory:67630723072 Cores:[{Id:0 Threads:[1 21] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[3 23] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:2 Threads:[5 25] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:3 Threads:[7 27] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:4 Threads:[9 29] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:8 Threads:[11 31] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:9 Threads:[13 33] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:10 Threads:[15 35] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:11 Threads:[17 37] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:12 Threads:[19 39] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:26214400 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}\nI0219 09:20:38.026413       1 manager.go:231] Version: {KernelVersion:4.4.0-141-generic ContainerOsVersion:Alpine Linux v3.4 DockerVersion:18.03.1-ce DockerAPIVersion:1.37 CadvisorVersion:v0.28.3 CadvisorRevision:1e567c2}\nI0219 09:20:38.055068       1 factory.go:356] Registering Docker factory\nI0219 09:20:40.055562       1 factory.go:54] Registering systemd factory\nI0219 09:20:40.057960       1 factory.go:86] Registering Raw factory\nI0219 09:20:40.059964       1 manager.go:1178] Started watching for new ooms in manager\nI0219 09:20:40.067562       1 nvidia.go:110] NVML initialized. Number of nvidia devices: 8\nI0219 09:20:40.127912       1 manager.go:329] Starting recovery of all containers\nI0219 09:20:40.290675       1 manager.go:334] Recovery completed\nI0219 09:20:40.483756       1 cadvisor.go:162] Starting cAdvisor version: v0.28.3-1e567c2 on port 8080\nI0220 00:47:41.172113       1 manager.go:1168] Exiting thread watching subcontainers\nI0220 00:47:41.172156       1 manager.go:396] Exiting global housekeeping thread\nI0220 00:47:41.177712       1 cadvisor.go:196] Exiting given signal: terminated\nI0220 00:47:41.934458       1 storagedriver.go:50] Caching stats in memory for 2m0s\nI0220 00:47:41.934792       1 manager.go:151] cAdvisor running in container: \"/sys/fs/cgroup/cpu,cpuacct\"\nI0220 00:47:42.033740       1 fs.go:139] Filesystem UUIDs: map[f09b9eec-fc73-48a4-bd46-08c5f71543c2:/dev/sda2 79632962-af28-491d-ad41-8147a6c26188:/dev/sda1 a0de5d03-6714-41e9-abae-fc1219131e26:/dev/sdb1]\nI0220 00:47:42.033778       1 fs.go:140] Filesystem partitions: map[/dev/sdb1:{mountpoint:/rootfs/home major:8 minor:17 fsType:ext4 blockSize:0} shm:{mountpoint:/rootfs/var/lib/docker/containers/245c6964608dada9c65d6ba95d293419e43e13a4d99332de0e228dae7ae0916d/mounts/shm major:0 minor:56 fsType:tmpfs blockSize:0} tmpfs:{mountpoint:/dev major:0 minor:59 fsType:tmpfs blockSize:0} /dev/sda1:{mountpoint:/var/lib/docker major:8 minor:1 fsType:ext4 blockSize:0}]\nI0220 00:47:42.041158       1 manager.go:225] Machine: {NumCores:40 CpuFrequency:3400000 MemoryCapacity:135081676800 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] MachineID:398a0844cc22fd0844eb7106582008b0 SystemUUID:4C4C4544-005A-3910-8031-B4C04F4D4732 BootID:26cbef0a-5a1f-4b75-9bfe-9851ccd47692 Filesystems:[{Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:779633565696 Type:vfs Inodes:48357376 HasInodes:true} {Device:/dev/sdb1 DeviceMajor:8 DeviceMinor:17 Capacity:787474333696 Type:vfs Inodes:48840704 HasInodes:true} {Device:shm DeviceMajor:0 DeviceMinor:56 Capacity:67108864 Type:vfs Inodes:16489462 HasInodes:true} {Device:none DeviceMajor:0 DeviceMinor:55 Capacity:779633565696 Type:vfs Inodes:48357376 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:59 Capacity:67108864 Type:vfs Inodes:16489462 HasInodes:true}] DiskMap:map[8:0:{Name:sda Major:8 Minor:0 Size:800166076416 Scheduler:deadline} 8:16:{Name:sdb Major:8 Minor:16 Size:800166076416 Scheduler:deadline}] NetworkDevices:[{Name:eno1 MacAddress:f4:8e:38:ce:eb:64 Speed:1000 Mtu:1500} {Name:eno2 MacAddress:f4:8e:38:ce:eb:66 Speed:-1 Mtu:1500} {Name:enp20s0f0 MacAddress:a0:36:9f:c9:ad:ac Speed:10000 Mtu:1500} {Name:enp20s0f1 MacAddress:a0:36:9f:c9:ad:ae Speed:-1 Mtu:1500} {Name:mesos168088 MacAddress:f4:8e:38:ce:eb:64 Speed:10000 Mtu:1500} {Name:mesos61637 MacAddress:f4:8e:38:ce:eb:64 Speed:10000 Mtu:1500} {Name:mesos6676 MacAddress:f4:8e:38:ce:eb:64 Speed:10000 Mtu:1500}] Topology:[{Id:0 Memory:67450953728 Cores:[{Id:0 Threads:[0 20] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[2 22] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:2 Threads:[4 24] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:3 Threads:[6 26] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:4 Threads:[8 28] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:8 Threads:[10 30] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:9 Threads:[12 32] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:10 Threads:[14 34] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:11 Threads:[16 36] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:12 Threads:[18 38] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:26214400 Type:Unified Level:3}]} {Id:1 Memory:67630723072 Cores:[{Id:0 Threads:[1 21] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[3 23] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:2 Threads:[5 25] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:3 Threads:[7 27] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:4 Threads:[9 29] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:8 Threads:[11 31] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:9 Threads:[13 33] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:10 Threads:[15 35] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:11 Threads:[17 37] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:12 Threads:[19 39] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:26214400 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}\nI0220 00:47:42.042417       1 manager.go:231] Version: {KernelVersion:4.4.0-141-generic ContainerOsVersion:Alpine Linux v3.4 DockerVersion:18.03.1-ce DockerAPIVersion:1.37 CadvisorVersion:v0.28.3 CadvisorRevision:1e567c2}\nI0220 00:47:42.085252       1 factory.go:356] Registering Docker factory\nI0220 00:47:44.085677       1 factory.go:54] Registering systemd factory\nI0220 00:47:44.088278       1 factory.go:86] Registering Raw factory\nI0220 00:47:44.090789       1 manager.go:1178] Started watching for new ooms in manager\nI0220 00:47:44.098410       1 nvidia.go:110] NVML initialized. Number of nvidia devices: 8\nI0220 00:47:44.159904       1 manager.go:329] Starting recovery of all containers\nI0220 00:47:44.317242       1 manager.go:334] Recovery completed\nI0220 00:47:44.500761       1 cadvisor.go:162] Starting cAdvisor version: v0.28.3-1e567c2 on port 8080. Thanks for your answer. That is to say, is this the problem of mesos itself?. Sorry to disturb you, cadvisor runs with  \"--v 4\" and doesn't show the log: https://github.com/google/cadvisor/blob/master/manager/manager.go#L1024\nhttps://github.com/google/cadvisor/blob/master/accelerators/nvidia.go#L163 accesses devices.list\nthen, I view mesos container cgroup file devices.list in /sys/fs/cgroup/devices/mesos/2cee36cb-6374-49b4-bcc7-666bbf5f0174 directory as follows:\nc : m\nb : m\nc 5:1 rwm\nc 4:0 rwm\nc 4:1 rwm\nc 136:* rwm\nc 5:2 rwm\nc 10:200 rwm\nc 1:3 rwm\nc 1:5 rwm\nc 1:7 rwm\nc 5:0 rwm\nc 1:9 rwm\nc 1:8 rwm\nc 243:0 rwm\nc 195:255 rwm\nc 195:5 rwm\nthe last line c 195:5 rwm has nvidiaMinorNumber 5\nmesos container[2cee36cb-6374-49b4-bcc7-666bbf5f0174] starts after cadvisor starts up. Now cadvisor can't get this container GPU uuid and relevant metrics.\n. Thanks. You mean every time cAdvisor reads from the devices cgroup, then the GPU is added? If not, that the GPU is added happened for the first time, so cAdvisor should read GPU metrics next time. I will also add extra log lines to confirm that. @sashankreddya I don't see the message \"Registering mesos factory\" in the log. Is my cadvisor version too low? . Hello, @sashankreddya . I pull the latest  image and don't also see the log \"Registering mesos factory\". ",
    "amitlodhiIT": "Thank you for your answer. I am now clear about this. :+1:  @dashpole . I understand but is there any api which directly give the name of container/subcontainer? . It worked for me. Thanks @dashpole . And If I run microservices in docker containerThen how to get which services is running in dockers/docker_containers? Is it possible to get running microservices in docker using restApi without using other system like promotheus?. Can you help me @dashpole . ",
    "DevinCampbell": "Ok, so stats.CPU.Usage.Total measures CPU usage by the amount of time in nanoseconds each container utilizes the CPU?. ",
    "jinxiao": "\nWhat is querying cAdvisor? Usually the aggregator (e.g. prometheus server) adds information about the thing being queried, such as host ip. cAdvisor does have a machine api /api/v1.3/machine that provides some information, such as cloud provider and instance id.\n\nyes , i'm using prometheus ,but i only got a instance column which showed the ip:port of cadvisor , but no machine column . > What is querying cAdvisor? Usually the aggregator (e.g. prometheus server) adds information about the thing being queried, such as host ip. cAdvisor does have a machine api /api/v1.3/machine that provides some information, such as cloud provider and instance id.\ni tried this api path , but no hostname , only machine-id showed . i think this is the limitation of cadvisor. if i am using static configs , nothing about hostname would be shown. i will choose dns_sd_configs instead, dns-name which is hostname fqdn will be showed , then i can relabel instance with this dns-name , thank u all . ",
    "ktsakalozos": "/test pull-cadvisor-e2e. Thank you for the review @dashpole. I moved the /dev/zfs check inside the ZFS case. Are you firmly set on using goto with an extra label (I believe the case label default does not qualify as a goto target)? I went with fallthrough what do you think?. ",
    "gkoerk": "I see - so any idea why my server would be killing cAdvisor repeatedly while docker Swarm keeps restarting it. Even a hint on where to look would be appreciated.. cadvisor:\n    image: google/cadvisor\n    networks:\n      - internal\n    command: -logtostderr -docker_only\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n      - /:/rootfs:ro\n      - /var/run:/var/run\n      - /sys:/sys:ro\n      - /var/lib/docker/:/var/lib/docker:ro\n    deploy:\n      mode: global\n      resources:\n        limits:\n          memory: 128M\n        reservations:\n          memory: 64M\n...\nnetworks:\n  traefik_public:\n    external: true\n  internal:\n    driver: overlay\n    ipam:\n      config:\n        - subnet: 172.16.29.0/24. Will try turning up log verbosity, though I see that -logtostderr is set in the command: (I pulled this from a recipe for swarmprom).. Silly question - where would I set log_verbosity via the docker image? Using an ENV variable with the same name?\n. Like this?\ncadvisor:    image: google/cadvisor\nnetworks:      - internal\ncommand: -logtostderr -v=4 -docker_only    volumes:\n- /var/run/docker.sock:/var/run/docker.sock:ro      - /:/rootfs:ro\n- /var/run:/var/run      - /sys:/sys:ro\n- /var/lib/docker/:/var/lib/docker:ro    deploy:\nmode: global      resources:\nlimits:          memory: 128M\nreservations:          memory: 64M\n. Same error:\nE0308 23:05:54.984106       1 info.go:140] Failed to get system UUID: open /etc/machine-id: no such file or directory,\nW0308 23:05:54.984173       1 info.go:53] Couldn't collect info from any of the files in \"/rootfs/etc/machine-id,/var/lib/dbus/machine-id\",\nW0308 23:05:55.172163       1 manager.go:349] Could not configure a source for OOM detection, disabling OOM events: open /dev/kmsg: no such file or directory,. ",
    "daviddavis83": "Grafana Query to influxdb \nSELECT non_negative_derivative(mean(\"value\"), 1s) FROM \"autogen\".\"cpu_value\" WHERE \"host\" =~ /^$host$/ AND \"type_instance\" = 'system' AND $timeFilter GROUP BY time(10s) fill(null). Update: I found that there is missing data in the database. since the data is null the non negative derivative function is making the cpu value spike exponentially. I have the function fill(null) in the query but it doesn't seem to fix the issue. . ",
    "dummyMaxy": "Hi, kubelet also use the data that cadivsor provided , what kind of format data that cadvisor expose \n to kubelet?\nBTW, is there cadivsor written in java ?. ",
    "ivenk": "I signed it!. @dashpole just to get the value to the ui.. I'm going to look into the test results in the next few days.. ",
    "masikmos": "curl localhost:8080/metrics\ncurl: (7) Failed to connect to localhost port 8080: Connection refused\nnetstat -plunt | grep 8080 empty. if i say\nsudo ./cadvisor -port 999999 --logtostderr -listen_ip 999.999.999.999 -v=999\nhe just started!!!\n. strace don't see any of attempts to listen ports or IPs!. only after reboot of all server, this problem resolved . ",
    "grjiq": "Sorry. Misoperation. ",
    "mcastilho": "I like this feature. Agreed, NetworkStats would be more suited for this stats.\n. ",
    "jonboulle": "api service\n. nit, elsewhere you return this error directly, here you're wrapping\n. service\n. does the raw one not work here?\n. simpler to just inline on https://github.com/google/cadvisor/pull/1154/commits/206670a655ba587b6c794b1d238d6f1819ef69e8#diff-06ccdffbfc974d7801151a26ffee4f8fR66 IMHO\n. Needs update\n. So if we initialise a manager and rkt is unavailable, we just warn and then we're stuck with that for the lifetime of the manager? is that the same behaviour as with docker? surprised it doesn't try to recover...\n. ",
    "thomaso-mirodin": "Done :D\n. Fixed, ty ty\n. ",
    "cvlc": "I initially thought that too, but I tried removing it and didn't see any issues when passing values with multiple '='. It seems that SplitN only splits on the first occurrence of the delimiter, so omitting the len(splits) == 2provides additional flexibility in that we can have equal signs in the environment variable values if we want to. I couldn't say whether that's useful or not for the particular reason behind this snippet?. Touch\u00e9! If there's no equal sign, splits will be a single element list so the comparison with splits[1] would cause the application to fail. An len(splits) == 2 is the only reasonable case, as my previous assumption that len(splits) could ever be more than 2 is wrong considering that the split happens only on the first equals. I'll update accordingly.. Okay, let's work from the assumption that we'll want to match the behaviour of Docker (with POSIX specs as a fallback since cAdvisor seems to focus on Linux only):\n\nIf a variable is passed without any equals sign, according to docker/docker#10141 this should be treated as the container inheriting the client environment's value for that variable if such a value exists, otherwise it should not set any value.\nIf a variable is passed with an equals sign but no value, this should set the value of the variable to \"\" (blank string) as in standard shell convention.\nIf a variable is passed with one or more equals signs, the first equals sign is used to demarcate the first part as key and the second as value. Escape/null characters and equals signs are not permitted in the variable name as per the POSIX specifications.\n\nAssuming that we want to avoid fatal errors and warn of invalid values so the user can correct them in their configuration we should:\n\nSilently ignore in the case of a blank variable name (envVar == \"\") \ni. This is a totally nil element, probably created by a buggy client as most of the Docker clients I've used wouldn't allow such a thing)\nii. I suggest to ignore it since cAdvisor should not be expected to report the existence of a nil element that doesn't affect anything during runtime and shouldn't be responsible for potential client-side bugs that are not in the standard docker clients.\nSilently ignore len(splits) == 1\ni. This is expected behaviour when inheriting the environment from the client when the client has not set any value. In this case, we do not need to do anything as the container's environment will not be affected (the environment variable is neither explicitly nil nor set to a value).\nii. I am not 100% sure of this one but the difficulty is in differentiating between an explicit blank-string variable and an implicit 'my client doesn't know about this, so it has not passed it' variable. In the current case, the behaviour we'll observe is that an explicit blank string will be stored by cAdvisor but a lack of client-side setting won't - otherwise, we'd need some sort of placeholder value or the ability to store a variable without a value but that's more than I'm up for writing right now.\nPermit variable values to contain as many equal signs as users want to pass, but only demarcate the variable key from value using the first = regardless of any other characters. I don't think there's any need for other behaviours.\n\nI'm interested in thoughts, especially on 2.ii.. ",
    "dmrub": "Which line do you mean ? The empty one ?. I removed this line and added comment:\nhttps://github.com/dmrub/cadvisor/commit/3e43b4573d5d8ff638b36d5524eacfe6e65215b7. ",
    "jakon89": "Any tests to prove it's working?. ",
    "rphillips": "probably should be minUint32. ",
    "suihkulokki": "CC is C compiler. The correct line is CC=aarch64-linux-gnu-gcc. you can use export ARCH after this line to avoid the ARCH=$ARCH lines below. ",
    "gbolo": "lol. ",
    "ddtmachado": "yes, it does nothing. Sorry for that it was just my editor autoformat on save.. It can be done, but I'm not so familiar with the patterns and preferences here yet.\nAre you suggesting to pass it down on the storage constructor or maybe add it to the StorageDriver.AddStats interface? \nI was trying to avoid modifying anything on other storage drivers as I have no context on then but it makes sense that they also respect the labels when storing if possible.. "
}
{
    "tfmorris": "From dfhu...@gmail.com on May 14, 2010 17:09:28:\nThe intended way is to use a facet to narrow to the target rows and then invoke a menu \ncommand to remove / flag / star them. For example, create a custom text facet with the \nexpression startsWith(value,\"RMK\"), then select \"true\" in it and then star those rows.\n. From dfhu...@gmail.com on May 12, 2010 06:37:24:\nFixed in r719. This bug could be observed by starring or flagging several rows and then undoing that operation. \nThe undo would fail because the operation's change file could not be read back in.\nThad, could you please verify? Your existing file cannot be restored, but for future project files, you can star/flag \nrows and undo properly.\n. From thadguidry on May 13, 2010 16:22:00:\nVerified as fixed with 1 GB Aggregate load test and starring and flagging half of the \nrows from 826,000.  Undo and Redo worked correctly with flagged and starred rows being \nkept intact without errors.\n. From dfhu...@gmail.com on May 12, 2010 05:37:15:\nFixed in r627. Thad, could you please verify?\n. From thadguidry on May 13, 2010 16:19:01:\nVerified with 1 GB Aggregate load test. No auto-save occurred within 6 mins.\n. From dfhu...@gmail.com on May 14, 2010 17:04:11:\nFixed in r767. Thad, please verify!\n. From stefano.mazzocchi@gmail.com on May 10, 2010 16:23:50:\nHmm, this is going to be interesting to fix since we don't have access to versions of windows that don't use the \nASCII characters... how comfortable are you with programming? Could we send patches to you to try?\n. From ibeg...@gmail.com on May 10, 2010 16:56:08:\nI am comfortable with programming so if patches will be available I will try them.\n. From tfmorris on May 10, 2010 16:57:58:\nFrom the looks of the error message, it's the non-ASCII username, not necessarily the \nlocalized Windows version that is the issue.  There's a good chance that you'll be \nable to reproduce it by creating a test user account containing non-ASCII characters \non a standard English Windows distribution.\n. From stefano.mazzocchi@gmail.com on May 10, 2010 17:35:27:\nAh, good point Tom. I'll try that.\n. From dfhu...@gmail.com on May 10, 2010 20:11:52:\nI'll get to this when I get home on my Vista box with East Asian language support.\n. From dfhu...@gmail.com on May 11, 2010 04:18:30:\nFixed in r707. ibegtin, could you check out the source and verify? Thanks.\n. From iainsproat on May 10, 2010 11:25:07:\nI think it's an issue with the JAVA_HOME environmental variable in Cygwin.  I compiled \nthe java using Eclipse and it runs fine. (after I remembered to add the -\nDbuild.dir=build -Ddist.dir=dist environmental variables when running in Eclipse)\n. From iainsproat on May 10, 2010 11:53:11:\nResolved: My machine required a hard reboot in order for JAVA_HOME to be picked up by \nCygwin.  Added a comment to the [http://code.google.com/p/freebase-\ngridworks/wiki/DevelopersGuide developers guide] to help others.\n. From iainsproat on May 10, 2010 12:33:29:\nThe above seem to be resolved when the patch in issue #  9 is applied.  (I think the \nabove was trying to deal with a nonexistent directory in it's confusion with paths \ncontaining spaces)  http://code.google.com/p/freebase-gridworks/issues/detail?id=9\n. From stefano.mazzocchi@gmail.com on May 10, 2010 16:10:27:\nFixed by the patch to issue #  9\n. From iainsproat on May 12, 2010 22:25:32:\nRe-opening issue.  Seems it's transient and only occurs on the first './gridworks test' \nrun after initial checkout.\nOn running './gridworks test' again, the error disappears.\n. From stefano.mazzocchi@gmail.com on May 13, 2010 02:21:26:\nI can't repro on my machine, even with a fresh checkout. Can you repro after all the latest commits?\n. From iainsproat on May 13, 2010 08:03:12:\nStill happening, unfortunately.\nI tried running 'easy_install virtualenv' (in the offchance that might show a non-\ngridworks related issue between virtualenv and my machine), but it works perfectly.\n. From stefano.mazzocchi@gmail.com on June 01, 2010 07:21:17:\nI've tried to reproduce this problem but I can't, so I suspect this has something to do with your machine (or with \nmine), so I'm considering this fixed until somebody else complains about it.\n. From iainsproat on May 10, 2010 12:33:29:\nThe above seem to be resolved when the patch in issue #  9 is applied.  (I think the \nabove was trying to deal with a nonexistent directory in it's confusion with paths \ncontaining spaces)  http://code.google.com/p/freebase-gridworks/issues/detail?id=9\n. From stefano.mazzocchi@gmail.com on May 10, 2010 16:10:27:\nFixed by the patch to issue #  9\n. From stefano.mazzocchi@gmail.com on May 10, 2010 16:10:00:\nPatch committed (and tested on macosx)\n. From stefano.mazzocchi@gmail.com on May 10, 2010 16:27:10:\nAh-ha! \"Expired timestamp: given 1273504291 and now 1273504645 has a \ngreater difference than threshold 300\"\nI think your computer clock might be off... oh boy, this is going to be interesting to fix.\nAt the very least, could you try running a NTP client to keep your clock in synch and see if that solves the \nproblem?\nOther beta testers reported this issue but they were never able to give me enough info to understand what was \ngoing on, thanks much!\n. From iainsproat on May 10, 2010 17:09:00:\nUpdating the clock worked around it nicely, thanks.  If the tolerance limit can be \nadjusted, I'd hope it wouldn't be too difficult.\nYou could determine the time offset by having the client ping freebase for it's \nserver timestamp before doing the oAuth.  The tolerance could be increased to \nenvelope the offset in timestamps plus some buffer, allowing the oAuth request to \nsucceed.\nAt the very least a friendly error message should be displayed. \"Your computer's \nclock is not synchronised with world time\".\n. From stefano.mazzocchi@gmail.com on May 10, 2010 17:27:17:\nYeah, although I think the best way is to call directly an NTP server to get that timestamp and avoid any user \ninconvenience.\n. From stefano.mazzocchi@gmail.com on May 10, 2010 23:25:23:\nFixed in r704.\n. From iainsproat on May 11, 2010 07:13:13:\nUnfortunately it seems to have introduced a new bug (r708).  On opening the Load data \ninto Freebase dialog, the below error is seen.  The \"sign into freebase\" link is not \nshown at the bottom of the dialog.\n11:07:46.984 [          org.mortbay.log]   Error for /command/check-authorizatio\nn/www.freebase.com (0ms)\njava.lang.Error: Unresolved compilation problem:\n        FreebaseTimeCommonsHttpOAuthConsumer cannot be resolved to a type\nat com.metaweb.gridworks.oauth.OAuthUtilities.getConsumer(OAuthUtilities\n.java:46)\n        at com.metaweb.gridworks.oauth.OAuthUtilities.getConsumer(OAuthUtilities\n.java:54)\n        at com.metaweb.gridworks.util.FreebaseUtils.getUserInfo(FreebaseUtils.ja\nva:59)\n        at com.metaweb.gridworks.commands.auth.CheckAuthorizationCommand.doGet(C\nheckAuthorizationCommand.java:36)\n        at com.metaweb.gridworks.GridworksServlet.doGet(GridworksServlet.java:21\n1)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511\n)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:3\n90)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.jav\na:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:1\n82)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:7\n65)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\nat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:1\n52)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:54\n2)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpCo\nnnection.java:923)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:547)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.\njava:228)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source\n)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n        at java.lang.Thread.run(Unknown Source)\n. From stefano.mazzocchi@gmail.com on May 11, 2010 07:25:49:\ndid you recompile?\n. From iainsproat on May 11, 2010 08:08:59:\nMy bad - I was compiling the wrong branch (my working copy).  I'd modified .classpath, \nhence it wasn't updated by SVN and wasn't compiling \nFreebaseTimeCommonsHttpOAuthConsumer.\n. From dhrubas...@gmail.com on July 23, 2010 13:55:17:\nI cant find the issue solved yet. \nIn my case, \nI downloaded the latest build - r878 for Linux. \nHowever, when I try to load into Freebase, it pops out a window to authorize, and the authorization never completes. Everytime I click Connect Button, it throws me back to the same Connect/Cancel page. \nIn my case here in the log file, I can see something like this. \n19:20:47.008 [                    oauth]  Got remote timestamp 1279892146793 (0ms)\n19:20:47.805 [     org.apache.http.wire]  >> \"POST /api/oauth/request_token HTTP/1.1[EOL]\" (797ms)\n19:20:47.806 [     org.apache.http.wire]  >> \"Authorization: OAuth realm=\"freebase.com\", oauth_callback=\"http%3A%2F%2Flocalhost%3A3333%2Fcommand%2Fauthorize%2Fwww.freebase.com\", oauth_consumer_key=\"%239202a8c04000641f80000000150979b7\", oauth_version=\"1.0\", oauth_signature_method=\"HMAC-SHA1\", oauth_timestamp=\"1279892146\", oauth_nonce=\"566194627225361290\", oauth_signature=\"wGaLTIhEyMaI875jXXDwCsyOcaw%3D\"[EOL]\" (1ms)\n19:20:47.806 [     org.apache.http.wire]  >> \"Content-Length: 0[EOL]\" (0ms)\n19:20:47.806 [     org.apache.http.wire]  >> \"Host: www.freebase.com[EOL]\" (0ms)\n19:20:47.806 [     org.apache.http.wire]  >> \"Connection: Keep-Alive[EOL]\" (0ms)\n19:20:47.806 [     org.apache.http.wire]  >> \"User-Agent: Apache-HttpClient/4.0.1 (java 1.5)[EOL]\" (0ms)\n19:20:47.806 [     org.apache.http.wire]  >> \"[EOL]\" (0ms)\n19:20:48.594 [     org.apache.http.wire]  << \"HTTP/1.0 200 OK[EOL]\" (788ms)\n19:20:48.594 [     org.apache.http.wire]  << \"Date: Fri, 23 Jul 2010 13:35:48 GMT[EOL]\" (0ms)\n19:20:48.594 [     org.apache.http.wire]  << \"Server: Apache[EOL]\" (0ms)\n19:20:48.595 [     org.apache.http.wire]  << \"WWW-Authenticate: OAuth realm=\"http://freebase.com/\"[EOL]\" (1ms)\n19:20:48.595 [     org.apache.http.wire]  << \"X-Metaweb-Cost: cc=0.015, dt=0.019, mcs=0.0, mcu=0.0, minflt=2, nvcsw=22, tm=0.0, utime=0.015[EOL]\" (0ms)\n19:20:48.595 [     org.apache.http.wire]  << \"ETag: f70807526891af1c700d8c6ba444ede5[EOL]\" (0ms)\n19:20:48.595 [     org.apache.http.wire]  << \"Expires: Fri, 23 Jul 2010 13:35:49 GMT[EOL]\" (0ms)\n19:20:48.595 [     org.apache.http.wire]  << \"Vary: Accept-Encoding[EOL]\" (0ms)\n19:20:48.595 [     org.apache.http.wire]  << \"Content-Length: 172[EOL]\" (0ms)\n19:20:48.595 [     org.apache.http.wire]  << \"Content-Type: text/html; charset=utf-8[EOL]\" (0ms)\n19:20:48.595 [     org.apache.http.wire]  << \"X-Cache: MISS from cache04.p01.sjc1.metaweb.com[EOL]\" (0ms)\n19:20:48.595 [     org.apache.http.wire]  << \"Connection: keep-alive[EOL]\" (0ms)\n19:20:48.595 [     org.apache.http.wire]  << \"X-Metaweb-TID: cache;cache04.p01.sjc1:8101;2010-07-23T13:35:48Z;0024[EOL]\" (0ms)\n19:20:48.595 [     org.apache.http.wire]  << \"Set-Cookie: metaweb_tid=cache%3Bcache04.p01.sjc1%3A8101%3B2010-07-23T13%3A35%3A48Z%3B0024; path=/[EOL]\" (0ms)\n19:20:48.596 [     org.apache.http.wire]  << \"Cache-Control: public, no-cache=\"Set-Cookie\", max-age=1, s-maxage=1, stale-while-revalidate=1[EOL]\" (1ms)\n19:20:48.596 [     org.apache.http.wire]  << \"[EOL]\" (0ms)\n\nReceived at some seconds and its expired just after 1/2 seconds. \n. From stefa...@google.com on July 23, 2010 14:57:57:\nHmm, let me look into it.\n. From stefa...@google.com on August 16, 2010 16:11:35:\ndhrubaster, does it still happen?\n. From stefa...@google.com on August 31, 2010 18:06:26:\nFlagging this as fixed because I can't reproduce (nor can hear back from the bug reporter). If this is still an issue, please re-open.\n. From dhrubas...@gmail.com on July 23, 2010 13:55:17:\nI cant find the issue solved yet. \nIn my case, \nI downloaded the latest build - r878 for Linux. \nHowever, when I try to load into Freebase, it pops out a window to authorize, and the authorization never completes. Everytime I click Connect Button, it throws me back to the same Connect/Cancel page. \nIn my case here in the log file, I can see something like this. \n19:20:47.008 [                    oauth]  Got remote timestamp 1279892146793 (0ms)\n19:20:47.805 [     org.apache.http.wire]  >> \"POST /api/oauth/request_token HTTP/1.1[EOL]\" (797ms)\n19:20:47.806 [     org.apache.http.wire]  >> \"Authorization: OAuth realm=\"freebase.com\", oauth_callback=\"http%3A%2F%2Flocalhost%3A3333%2Fcommand%2Fauthorize%2Fwww.freebase.com\", oauth_consumer_key=\"%239202a8c04000641f80000000150979b7\", oauth_version=\"1.0\", oauth_signature_method=\"HMAC-SHA1\", oauth_timestamp=\"1279892146\", oauth_nonce=\"566194627225361290\", oauth_signature=\"wGaLTIhEyMaI875jXXDwCsyOcaw%3D\"[EOL]\" (1ms)\n19:20:47.806 [     org.apache.http.wire]  >> \"Content-Length: 0[EOL]\" (0ms)\n19:20:47.806 [     org.apache.http.wire]  >> \"Host: www.freebase.com[EOL]\" (0ms)\n19:20:47.806 [     org.apache.http.wire]  >> \"Connection: Keep-Alive[EOL]\" (0ms)\n19:20:47.806 [     org.apache.http.wire]  >> \"User-Agent: Apache-HttpClient/4.0.1 (java 1.5)[EOL]\" (0ms)\n19:20:47.806 [     org.apache.http.wire]  >> \"[EOL]\" (0ms)\n19:20:48.594 [     org.apache.http.wire]  << \"HTTP/1.0 200 OK[EOL]\" (788ms)\n19:20:48.594 [     org.apache.http.wire]  << \"Date: Fri, 23 Jul 2010 13:35:48 GMT[EOL]\" (0ms)\n19:20:48.594 [     org.apache.http.wire]  << \"Server: Apache[EOL]\" (0ms)\n19:20:48.595 [     org.apache.http.wire]  << \"WWW-Authenticate: OAuth realm=\"http://freebase.com/\"[EOL]\" (1ms)\n19:20:48.595 [     org.apache.http.wire]  << \"X-Metaweb-Cost: cc=0.015, dt=0.019, mcs=0.0, mcu=0.0, minflt=2, nvcsw=22, tm=0.0, utime=0.015[EOL]\" (0ms)\n19:20:48.595 [     org.apache.http.wire]  << \"ETag: f70807526891af1c700d8c6ba444ede5[EOL]\" (0ms)\n19:20:48.595 [     org.apache.http.wire]  << \"Expires: Fri, 23 Jul 2010 13:35:49 GMT[EOL]\" (0ms)\n19:20:48.595 [     org.apache.http.wire]  << \"Vary: Accept-Encoding[EOL]\" (0ms)\n19:20:48.595 [     org.apache.http.wire]  << \"Content-Length: 172[EOL]\" (0ms)\n19:20:48.595 [     org.apache.http.wire]  << \"Content-Type: text/html; charset=utf-8[EOL]\" (0ms)\n19:20:48.595 [     org.apache.http.wire]  << \"X-Cache: MISS from cache04.p01.sjc1.metaweb.com[EOL]\" (0ms)\n19:20:48.595 [     org.apache.http.wire]  << \"Connection: keep-alive[EOL]\" (0ms)\n19:20:48.595 [     org.apache.http.wire]  << \"X-Metaweb-TID: cache;cache04.p01.sjc1:8101;2010-07-23T13:35:48Z;0024[EOL]\" (0ms)\n19:20:48.595 [     org.apache.http.wire]  << \"Set-Cookie: metaweb_tid=cache%3Bcache04.p01.sjc1%3A8101%3B2010-07-23T13%3A35%3A48Z%3B0024; path=/[EOL]\" (0ms)\n19:20:48.596 [     org.apache.http.wire]  << \"Cache-Control: public, no-cache=\"Set-Cookie\", max-age=1, s-maxage=1, stale-while-revalidate=1[EOL]\" (1ms)\n19:20:48.596 [     org.apache.http.wire]  << \"[EOL]\" (0ms)\n\nReceived at some seconds and its expired just after 1/2 seconds. \n. From dfhu...@gmail.com on May 10, 2010 20:09:40:\nFixed in revision 700.\n. From tfmorris on May 10, 2010 19:26:52:\nThat sounds more like an enhancement request than a defect report.\nTo generalize things a bit, support for a standard data access API would allow people \nto plug in multiple DB backends.\n. From dfhu...@gmail.com on May 10, 2010 19:32:58:\nOne of the major challenges here is how to support undo/redo when changes can get into \nthe back-end database without going through Gridworks. Another major challenge is where \nto store metadata (such as reconciliation records) that is specific to Gridworks and \nnot native to any existing back-end database.\n. From mjlissner on May 10, 2010 19:36:53:\nHmm...I presume a new project would have to be created when doing this, and that\ncould hold the meta information.\nAs for undo/redo, maybe adding a commit button would make that easier. So changes can\nbe made to a snapshot of the data, but then no changes are made to the DB itself\nuntil commit is pressed?\n. From iainsproat on May 10, 2010 19:46:01:\nYou can either keep it synchronous with the database (effectively using the database \nas the backend), but lose undo/redo support and reconciliation with Freebase (unless \nyou add suitable tables to the database).  You're then using Gridworks for just the \nfacets really.\nThe other way, as you suggest, is to make it similar to a disconnected session with a \ncommit transaction.  The issue is then ensuring consistency between the remote \ndatabase and the snapshot held in Gridworks.  Merging the two back in would be an \nissue.  You'd also need to hold keys from the remote database in Gridworks for \nupdating records.\nFrameworks such as Hibernate or Spring would be worth considering for their database \nabstraction layers.\n. From thadguidry on May 10, 2010 20:05:13:\nI have read that good ORMs such as Hibernate now support ordered lists and other \nfeatures now incorporated into JPA 2.0 as of Dec 2009.\n. From 7...@ericjarvies.com on November 11, 2010 07:08:54:\nInitially, what I believe is most important/useful, is simply having the ability to direct-connect to MySQL/PostgreSQL/etc. data sources from the get-go(create a new project).  Also initially, being able to set/save multiple data sources, and multiple dBs within those sources.  When the user creates a new project, this would in effect create a disconnected session(as mentioned above), wherein the data is treated as is now the data.  Adding 'commit' features can come next, followed by more advanced connectivity options, until such a point synchronous functionality is in place to one degree or another.  But for now, it would certainly be nice to add data sources and pull data from those sources!\nEric Jarvies\n. From thadguidry on November 11, 2010 14:36:42:\nEric,  A cleanup tool using industry best practices is best used offline within a process.  There are existing ETL tools that easily consume from MySQL/PostgreSQL etc and offer excellent flow control, exporting, and connectivity to produce delimited files with relative ease.  Talend is one such product that I use along with Google Refine.  Talend (Open Source Community edition) does my scheduled daily gathering from 3 databases (MySQL and Oracle) and then dumps a customized TSV file that I open with Google Refine for further analysis and sometimes clean up.  There are other tools that provide ETL (Extract, Transform, Load) like Talend.  I'm not 100% sure if the team really feels the need to copy that and flesh out a full ETL platform, since Talend and other tools fill that need very nicely.  Incidentally, using Google Refine and a bit of clustering, I was able to find a few loop holes in our data storage processing that we fixed with a few stored procedures within Oracle.  Google Refine was instrumental as a discovery tool for that.  Talend does have an MDM component but does not have the interactivity of a discovery tool like Google Refine does.  If you do NOT need a daily process, but only one time cleanup, just dumping with MySQL or PostgreSQL would offer about the same and depending on the size of database takes only secs to minutes.  Dumping can also avoid potential live database locks, that if Refine supported might have to tip-toe around, depending on the teams' chosen implementation of database connectivity.  If you have large database size needs, give Talend or another ETL tool a try with Google Refine, and you'll soon see the powerful left-right combination.  I'm not sure how far the team ultimately decides to absorb direct connectivity support within Google Refine.  I'd like to hear other opinions as well on this Issue-12.\n. From Chris.Go...@gmail.com on April 14, 2011 14:23:04:\nI agree with thaguidry. Let the Refine team focus on bringing data quality issues to light. Let Talend focus on data Quality (they do have an data profiling tool that can identify some of this stuff) Talend is what we use for basic ETL. You could write some SQL to get the data out of MySQL anyway. If you analyze directly connected to db server for data quality against an entire large table your dba might become angry too.\nC\n. From stefano.mazzocchi@gmail.com on May 10, 2010 23:27:09:\nThe data sent actually is a string (it has \"\" quotes around it). Have you called value.toNumber() on those \ncolumns?\n. From iainsproat on May 11, 2010 05:51:59:\nYes, value.toNumber() has been used to transform all the cells.  The values remain \nquoted and still failing -> http://gridworks-loads.freebaseapps.com/job/25\n. From dfhu...@gmail.com on May 11, 2010 05:53:42:\nIain, I'll take a look soon!\n. From dfhu...@gmail.com on May 11, 2010 07:01:53:\nFixed in r708. Iain, could you check out and verify? Thanks.\n. From iainsproat on May 11, 2010 07:13:32:\nSeems to be fixed from the preview.  (no quotation marks around floats).  However, the \nfix to issue #10 threw a bug which prevents me from uploading at this time (see comments \nto #  10 for bug)\n. From iainsproat on May 11, 2010 07:13:32:\nSeems to be fixed from the preview.  (no quotation marks around floats).  However, the \nfix to issue #10 threw a bug which prevents me from uploading at this time (see comments \nto #  10 for bug)\n. From dfhu...@gmail.com on May 24, 2010 23:02:10:\nFixed by r853.\n. From dfhu...@gmail.com on May 17, 2010 19:25:13:\nFixed in r799. There's a \"rename\" link on hover for each project on the front page, and the Project > Rename... \nmenu command in the project page. Please verify.\n. From dfhu...@gmail.com on May 17, 2010 21:34:07:\nFixed by r804. Please verify.\n. From dfhu...@gmail.com on May 12, 2010 07:11:33:\nIain, could you attach your project file or upload it somewhere? It'd make it much easier for me to observe the \nproblem. Thanks.\n. From iainsproat on May 12, 2010 07:16:11:\nAttached.\n. From dfhu...@gmail.com on May 12, 2010 07:46:23:\nFixed in r720. Iain, you'd need to invoke the command \"Create A New Topic for Each Cell\" on that column again.\n. From iainsproat on May 12, 2010 08:21:49:\nworks, thanks\n. From stefano.mazzocchi@gmail.com on May 11, 2010 17:26:58:\nUh, nasty. I'll take this one.\n. From stefano.mazzocchi@gmail.com on May 12, 2010 07:53:28:\nThis issue was closed by revision r721.\n. From stefano.mazzocchi@gmail.com on May 12, 2010 07:53:52:\nFixed in r721. Please confirm.\n. From ibeg...@gmail.com on May 13, 2010 04:22:24:\nNow it works fine. Thanks!\n. From dfhu...@gmail.com on May 12, 2010 06:10:22:\nFixed in r717. eferonline, would you be able to check out the code and verify the fix?\n. From eferonline on May 12, 2010 08:27:46:\nThanks for the quick reaction!\nI loaded and built the new revision. It works better now, giving me the correct \nnumber of records. The line break in the description field however seems to be gone. \n(But I can add it manually in the gridworks editor via clipboard - so it seems \ntechnically possible to have line breaks in fields). This should be fixed, too.\nAs for the source code change: I shamefully have to admit that I don't really get the \nimporter code and what change exactly did the trick for this issue. I would have \nexpected a kind of finite state automaton or something to manage the parser \"modes\" \nbut could not find an equivalent in the sources. Unfortunately I'm a bit short on \ntime to review the code in detail.\n. From eferonline on May 12, 2010 08:38:03:\nPS: The fix only solves the problem if the separator chars are commas. For tabs the old \nbehavior occurs.\n. From dfhu...@gmail.com on May 17, 2010 05:57:11:\nFixed for TSV as well by r790. Please verify.\n. From eferonline on May 17, 2010 06:54:13:\nIt's nearly fixed now. But if one line break in a \"-escaped area comes directly after \nanother (which means there is a blank line before the text continues) the record is \nstill split. It should be possible to have an unlimited number of linebreaks in the \nfield value before the escape sequence ends and the next field is processed.\n. From iainsproat on May 17, 2010 07:04:56:\nI've added a unit test for this multiple blank line case in r794.  test fails.\n. From iainsproat on May 17, 2010 12:01:00:\nShould be fixed in r797.  Please verify.\n. From eferonline on May 17, 2010 12:14:39:\nVerified. It work's now as expected. Great!\n. From andreas....@gmail.com on May 20, 2011 08:23:26:\nIs there any way I can work around this problem without downloading and building Google Refine from source? Can I convert the input file to another format or escape characters differently?\n. From thadguidry on May 20, 2011 13:19:49:\nWhy not just use a text editor and do a find/replace for the double quote character \" to something like a triple carat ^^^ ?  Import without the splitting option or quote char option.  Then once it's in Google Refine, perform your splits manually with GREL or Add column against the commas and ^^^ ?  Would that work ?\n. From tfmorris on May 20, 2011 17:00:53:\nThat workaround would help for some cases with embedded tabs and commas, but not for line breaks, I suspect.\n. From stefano.mazzocchi@gmail.com on May 12, 2010 00:54:09:\npatched by hand in trunk\n. From stefano.mazzocchi@gmail.com on May 12, 2010 09:03:50:\nCommitted (I did some changed to the test but it's functionally equivalent)\n. From iainsproat on May 12, 2010 10:28:29:\nGreat.  Thanks Stefano - it wasn't quite production ready, so thanks for cleaning it \nup and committing.\nAs an aside, normally I'd have guard clauses on the functions to throw \nIllegalArgumentException if a null value is passed as a parameter (which I was \ntesting for).  I notice you changed the test to expect ServletException.\nI'd prefer to see IllegalArgumentException (or whatever the Java Equivalent of .Net's \nArgumentNullException is), as it's then obvious that the problem isn't with the \nmethod but with the parameter passed in to it - which saves time in the debugging.\n. From iainsproat on May 12, 2010 19:40:42:\nUpdated patch attached.  With further tests along the same line as before.\nI've started adding guard statements to functions, throwing IllegalArgumentException if \na parameter is null.\nAlso noticed that some of the code in Command.getEngine was a duplicate of \nCommand.getEngineConfig, so I've refactored the former to call the latter.\n. From iainsproat on May 12, 2010 22:17:30:\nMore tests (21 in total), and a bit more error catching and some slight refactoring.\nThis supersedes the above patches.\n. From stefano.mazzocchi@gmail.com on May 13, 2010 00:55:19:\nApplied in r744. Looks good Iain, my suggestion is to just commit in the tree whatever new test you come up \nwith, I'll watch over your shoulders from there (commit then review is a lot more effective than review than \ncommit once we get things going)\n. From stefano.mazzocchi@gmail.com on May 13, 2010 21:22:14:\nHmm, interesting. how did you get to make that dir? using ./gridworks ui_tests? I'm asking because that test has \nprovisions to delete the temp dirs in there which obviously failed in your case. Or you tried to test gridworks on \nyour own?\nIn any case, I do agree it should probably be best to put it somewhere else.. I'll take a stab at this\n. From stefano.mazzocchi@gmail.com on May 13, 2010 22:44:55:\nfixed in r760 (I'm using the OS's temp dir instead)\n. From stefano.mazzocchi@gmail.com on May 13, 2010 21:03:55:\napplied at r756\n. From thadguidry on May 13, 2010 20:30:48:\nYes, I also saw similar warnings in Netbeans regarding Sun proprietary stuff and warned \nthat they could go away at any time.  If I recall, a bunch of \"Signal\" stuff.\n. From stefano.mazzocchi@gmail.com on May 13, 2010 21:03:55:\napplied at r756\n. From stefano.mazzocchi@gmail.com on May 13, 2010 21:03:55:\napplied at r756\n. From dfhu...@gmail.com on May 17, 2010 23:47:41:\nFixed in r807. Please verify.\n. From stefa...@google.com on August 31, 2010 19:47:15:\nFixed in trunk\n. From dfhu...@gmail.com on May 14, 2010 17:10:40:\nThis should use the same setting mechanism as issue #31.\n. From stefano.mazzocchi@gmail.com on May 14, 2010 19:03:57:\nOr, better, use a cookie to make the language choice stick.\n. From staringmonkey on May 15, 2010 03:26:01:\nAt the risk of overkill it would be nice if it did both--a global switch to set the\ndefault and a cookie to remember you're last selection.  I'm thinking this would be\nmost useful in multi-user scenarios.\n. From dfhu...@gmail.com on May 27, 2010 01:46:27:\nFixed with cookie by r863.\n. From dfhu...@gmail.com on May 14, 2010 17:10:40:\nThis should use the same setting mechanism as issue #31.\n. From iainsproat on May 14, 2010 15:21:23:\nDavid, Stefano - Is the limit in compiled code?  Values like these should be in a \nseparate configuration text file at least.\n. From staringmonkey on May 14, 2010 15:26:56:\nIt is in src/main/java/com/metaweb/gridworks/browsing/facets/ListFacet.java, on line\n68.  Easy to change, but still annoying. :-)\n. From stefano.mazzocchi@gmail.com on May 14, 2010 16:15:40:\nYeah, agreed, it should be easier to configure. I'll make it configurable.\n. From dfhu...@gmail.com on May 14, 2010 16:26:45:\nI think as the first cut, we can have a .properties file storing all these settings. We \ndon't have to have UI yet.\n. From EmilStenstrom on May 20, 2010 21:15:16:\nI also just stumbled on this annoyance. \n1) I would like a message on what number of items that was found, or at least what\nthe limit was.\n2) I would like some suggestion on what to do to get the values. ie. \"Complement this\nwith a search to see your values\" or \"Click here to change the maximum value\".\n. From dfhu...@google.com on August 06, 2010 06:17:50:\nIn trunk, you can go to http://127.0.0.1:3333/preferences and set the facet limit using the preference key \"ui.browsing.listFacet.limit\". I'm not sure how to expose this through a more user-friendly UI just yet.\n. From dfhu...@google.com on August 14, 2011 01:05:54:\nThis issue was closed by revision r2201.\n. From dfhu...@gmail.com on August 14, 2011 01:06:24:\nFixed by r2201.\n. From kiminoa on April 24, 2012 04:07:17:\nPreferences was a great fix for this.  Thank you! :)\n. From iainsproat on May 14, 2010 15:01:12:\nissue #35 has been merged into this issue.\n. From dfhu...@gmail.com on May 14, 2010 16:35:34:\nI've been thinking about this. I think it's a 1.1 feature. I need to re-think the whole \nrecord model before getting into sorting.\nThere are actually 2 different kinds of sorting: one temporary for viewing only, and \none for actually re-ordering the rows permanently. For now, I'll refer to the former as \n\"sorting\" and the latter as \"re-ordering\".\n. From dfhu...@gmail.com on May 22, 2010 01:43:20:\nInitial implementation as of r840. Column sorting is temporary until you invoke Reorder Rows Permanently. We'll \nneed a few UI iterations to nail this feature properly. \nstaringmonkey, iainsproat, please try it out! Note that row-based vs. record-based modes are taken into account \nwhen sorting.\n. From dfhu...@gmail.com on May 24, 2010 23:50:36:\nCounted this as fixed. Please create new issues as you find bugs with the sorting feature.\n. From iainsproat on May 14, 2010 15:01:12:\nissue #35 has been merged into this issue.\n. From dfhu...@gmail.com on May 14, 2010 16:32:32:\nFor now, what you can do is as follows:\n- select None in the Judgment facet to see only those rows that haven't been matched. \n  Or if the facet isn't there, it's available in the column's menu Reconcile -> Facets ->\n  By Judgment.\n- just keep on the first page and process the first row. Once you manually match it, it \n  goes away (since its judgment is no longer None).\n. From staringmonkey on May 14, 2010 16:37:58:\n@dfhuynh That's an excellent stopgap solution--thanks for suggesting it!\n. From AndrewOf...@gmail.com on May 14, 2010 15:23:11:\n+1\nI have cells with values like: (OF 9)\nand I'm using the regexp: (OF [0-9]+)\nmy regexp tester says they should match, but it doesn't work as a text filter\n(I tried various quantities of backslashes for escaping due to past experience with java's awkwardness, but it \ndidn't help)\n. From dfhu...@gmail.com on May 14, 2010 16:52:17:\nFixed in r766. staringmonkey, please verify.\n. From AndrewOf...@gmail.com on May 14, 2010 14:59:16:\nApologies, this is a dupe issue #32 (http://code.google.com/p/freebase-gridworks/issues/detail?id=32)\n. From AndrewOf...@gmail.com on May 14, 2010 14:59:16:\nApologies, this is a dupe issue #32 (http://code.google.com/p/freebase-gridworks/issues/detail?id=32)\n. From stefano.mazzocchi@gmail.com on May 14, 2010 16:13:57:\nYeah, I agree, it would be useful to have your expression return an array and have gw treat that as a way to \nbuild multiple columns... the problem though is that an expression could yield arrays with different length on \neach row, which means that gw would have to do this in two passes: the first to understand how many \ncolumns will have to be created in total (taking the max of all the arrays returned by applying the expression \non each row) and the second to create and fill up the cells in the new columns.\nAnother issue is naming the columns, but we could just come up with random names (say columnXX with an \nincremental counter).\nAnother option is to have some sort of 'column creator manifest', something like\n{{{\n   value.split(',').make_columns({ \"something\" : result[0], \"whatever\" : result[1] });\n}}}\nbut gets very verbose pretty fast.\nThoughts?\n. From dfhu...@gmail.com on May 14, 2010 16:37:36:\nActually the existing column splitting command already deals with both issues. We only \nneed to make it take any arbitrary expression that produces arrays.\n. From thadguidry on May 29, 2010 20:18:52:\nDoes this issue also deal with a simple UI interface for Edit Column / Join ???  For \nexample, I have 2 or more columns (first name, last name) that I want to easily combine \nin order to reconcile with /person, and I simply just type the column names themselves \nwith a , separator to handle performing the join upon apply into my new column name.\nWe have Edit Cell / Join but no Edit Column / Join ??\n. From supp...@nickpoole.org.uk on July 31, 2011 17:44:33:\nIn case anyone is looking for a simple workaround to join two columns (for example, joining a firstname and lastname column into a single 'name' column) - I found the simplest solution was to export the data from Refine as an Excel spreadsheet, and then to use the 'concatenate' function in Excel to join them. \nThe concatenate formula (including a whitespace between the firstname and lastname) is:\n=A2&\" \"&B2\nI found that for this to work neatly, you should use Refine to trim the whitespace from before and after the text strings. Other than that, worked a charm!\n. From tfmorris on September 18, 2012 17:20:52:\nRemove obsolete milestone\n. From stefano.mazzocchi@gmail.com on May 14, 2010 16:07:26:\nGood idea, I like that. I'll take it.\n. From stefano.mazzocchi@gmail.com on June 01, 2010 08:57:40:\nI've added a match() GEL function that returns an array of strings with the capturing groups of the regexp. Now all \nwe need is a way to turn the returned GEL arrays into columns (which is issue #36)\n. From stefa...@google.com on August 31, 2010 19:51:42:\nThis was actually already implemented as specified in the issue so I'm closing it\n. From stefano.mazzocchi@gmail.com on June 01, 2010 08:57:40:\nI've added a match() GEL function that returns an array of strings with the capturing groups of the regexp. Now all \nwe need is a way to turn the returned GEL arrays into columns (which is issue #36)\n. From stefano.mazzocchi@gmail.com on May 14, 2010 16:18:26:\nah, didn't know one could use overflow differently on thead and tbody... if so, you're right, it should be pretty \ntrivial. I'll try it.\n. From iainsproat on October 14, 2010 13:29:53:\nissue #151 has been merged into this issue.\n. From dfhu...@gmail.com on April 17, 2011 03:46:27:\nissue #365 has been merged into this issue.\n. From PaulMake...@gmail.com on May 28, 2011 22:03:14:\nHere's a starter on this that I think produces the requisite table+thead/th+tbody/td structure. Only tested in Chrome! A CSS whiz can step in and finish it.\nFrom what I've seen & having tried it myself it's not straightforward with the non-fixed width column & horiz scrollbar requirements of Refine. https://github.com/golovko/Fixed-Header-Table seemed to me a good starting point (out of about two dozen I Iooked at...)\n. From dfhu...@google.com on September 01, 2011 22:03:15:\nThis issue was closed by revision r2229.\n. From iainsproat on October 14, 2010 13:29:53:\nissue #151 has been merged into this issue.\n. From dfhu...@gmail.com on April 17, 2011 03:46:27:\nissue #365 has been merged into this issue.\n. From stefano.mazzocchi@gmail.com on May 14, 2010 19:05:12:\nYeah, agreed... I was in fact looking at http://eligrey.com/blog/post/localization-in-javascript just the other \nday.\n. From dfhu...@gmail.com on May 17, 2010 01:41:44:\nEmil, if you have any idea what a date facet should look like to meet your needs, please include a mockup.\n. From knut.for...@gmail.com on June 09, 2010 22:07:57:\nIt would be nice if the numeric facet understood date values and interpreted them as datePart(value, \"time).\nEven nicer, the labels underneath the histogram in the facet widget could display the min/max values as dates rather than long integers.\nFor gold plating the buckets in the histogram could have some affinity towards aligning with the second/minute/hour/day/week/month/year boundaries depending on the overall range between min and max.\n. From knut.for...@gmail.com on June 23, 2010 19:20:27:\nCorrespondingly there could be scatter facets with dates on either the horizontal axis, the vertical axis or both.\nA related enhancement is listed as id 95, timeline facets: http://code.google.com/p/freebase-gridworks/issues/detail?id=95\n. From stefa...@google.com on August 31, 2010 18:51:44:\nLanded in trunk, will be part of 1.5\n. From iainsproat on May 16, 2010 18:11:38:\nActually, it appears that quotation marks should be double quoted.\ne.g.\"\\\"\\\"To be\nor not to be\\\"\\\" is a quote from Hamlet.\"\nhttp://publib.boulder.ibm.com/infocenter/wmbhelp/v6r1m0/index.jsp?\ntopic=/com.ibm.etools.mft.doc/ad20830_.htm\n. From dfhu...@gmail.com on May 17, 2010 00:41:10:\nSo it looks like the original csv parser did work correctly?\nIain, I think your patch has lost the functionality of auto-detecting between TSV and CSV. Now, I think it'd always \nassume CSV. Could you please fix it or revert the patch?\n. From iainsproat on May 17, 2010 02:48:45:\nGood catch - I'll fix it (and add some tests)\n. From iainsproat on May 17, 2010 03:26:05:\nDavid, could you check r787?\nI've reverted to your parser, but there are 4no tests failing. I've ignored them for \nthe moment (enabled=false) - could you also check they are valid tests, that the \nbehaviour I've assumed is as you would expect?\n. From dfhu...@gmail.com on May 17, 2010 05:28:06:\nIain, they should pass now. I'm closing this bug.\n. From rawl...@gmail.com on May 17, 2010 03:05:54:\nI've managed to narrow it down to the special hyphen character in the regex, but I'm \nnot yet sure why that causes the code to fail. The actual exception thrown is this:\nTraceback (most recent call last):\n  File \"\", line 5, in _temp_\n  File \"/home/vishal/Workspace/metaweb/freebase-gridworks-read-\nonly/lib/jython/re.py\", line 142, in search\n    return _compile(pattern, flags).search(string)\n  File \"/home/vishal/Workspace/metaweb/freebase-gridworks-read-\nonly/lib/jython/re.py\", line 241, in _compile\n    raise error, v # invalid expression\nsre_constants.error: nothing to repeat\nI'm still investigating.\n. From dfhu...@gmail.com on May 17, 2010 04:51:06:\nRaymond, I found that this works\nimport re\ng = re.search(ur\"\\u2014 (._),\\s_BWV\", value)\nreturn g.group(1)\nCould you check? Note the unicode character as well as the ur prefix. Unfortunately, I don't know a quick way \nto encode unicode characters within jython code without writing a parser for it. So right now you'd have to do \nthe encoding yourself. I did this by first using GEL on the expression \"\u2014\".unicode(), which gives 8212, and \nthen converting that from decimal to hex.\n. From iainsproat on May 17, 2010 12:00:04:\nShould be added, but with as a null.  row.cells.getSize() == 4;  row.cells.get(2) == \nnull;\nFixed in r797.\n. From stefano.mazzocchi@gmail.com on May 17, 2010 16:28:26:\nThe problem is that the eclipse project files contained only the jars required for compile time not for runtime... \nI'll add the other ones.\n. From stefano.mazzocchi@gmail.com on May 17, 2010 16:35:50:\nShould be fixed by r798. Please verify.\n. From dfhu...@gmail.com on May 18, 2010 00:03:48:\nPresumably fixed by r809, by sprinkling in a few \"synchronized\". Concurrency bugs are hard to test.\n. From tfmorris on May 18, 2010 01:46:50:\nUgh.  I'm not saying it's not fixed, but the \"sprinkling\" wording makes me profoundly \nuncomfortable.  The thing that makes our current antiquated concurrency models so \ndifficult is that they require expert analysis.  \"Sprinkling\" may fix it or it just \nmay just slow irrelevant operations down while not protecting the critical ones.\nBut perhaps I'm reading too much into the wording...\n. From thadguidry on May 18, 2010 02:08:09:\nNot fixed.\nClick Edit on cell \"Dallas\" and rename to \"ticktock\".\nThen tap CTRL-ENTER and cells update.\nClick Edit on different cell \"Richardson\" and rename to \"yoyo\".\nThen tap CTRL-ENTER and cells update, but \"Dallas\" cells re-appear as well.\n. From dfhu...@gmail.com on May 18, 2010 02:26:34:\nTfmorris, excuse my language. I meant putting synchronized on public methods so to ensure that the object is \nalways accessed in a coherent state.\nThad, that's a different issue that has to do with keystroke handling. Apparently somehow the Enter key is fired \non the Undo link of the previous operation.\n. From dfhu...@gmail.com on May 18, 2010 02:52:25:\nFixed by r812.\n. From thejeffl...@gmail.com on May 18, 2010 22:11:59:\nThis should probably be an enhancement. I didn't see the option. \n. From dfhu...@gmail.com on May 20, 2010 17:49:54:\nFixed by r833. Please verify.\n. From stefano.mazzocchi@gmail.com on May 19, 2010 07:08:06:\nI'll take it.\n. From stefano.mazzocchi@gmail.com on July 01, 2010 07:53:01:\nFixed in r1060\n. From iainsproat on May 19, 2010 07:00:41:\nThanks, I'll just take a look at it Tom.\n. From stefano.mazzocchi@gmail.com on May 19, 2010 07:07:37:\nLooks good to me.\n. From iainsproat on May 19, 2010 07:10:29:\nr821\n. From dfhu...@gmail.com on May 24, 2010 23:49:42:\nFixed by r855.\n. From dfhu...@gmail.com on May 27, 2010 18:41:14:\nAdded stemmed=1 in r870.\n. From stefano.mazzocchi@gmail.com on June 01, 2010 08:02:52:\nLanded in r930\n. From stefano.mazzocchi@gmail.com on May 20, 2010 16:16:19:\nCan you define \"name-like\" a little more rigorously? if a human can't understand what you want, it's really \ndifficult to imagine a computer program doing better :-)\n. From AndrewOf...@gmail.com on May 20, 2010 16:34:02:\nI'll try, but I think this is the kind of thing where an existing algorithm would be needed or someone with \nnatural language processing expertise.\nA quick look around turned up this http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.6337 \nBasically a combination of spotting capitalised words and other keywords such as 'by', 'from', 'said' would be \nused to identify potential names, then the user would be responsible for accepting or rejecting specific \nresults. Obviously the rules would vary from language to language, making things even more complicated. But \nEnglish would be a good starting point.\nI'm under no illusion that this is a small ask and I understand that this would need some pretty serious \nresearch. So I guess for now I'm just asking if in theory this kind of functionality matches the broader vision of \nGridworks and is worth looking at in greater detail.\n. From stefano.mazzocchi@gmail.com on May 20, 2010 16:47:56:\nLet's turn this around: instead of describing the solution you think it's best for a problem we don't know, why \ndon't you describe (in detail) your problem (maybe even show us a fragment of your data) and we can all \nbrainstorm together about what's the best solution?\n. From dfhu...@gmail.com on May 20, 2010 17:29:12:\nThad was asking for something similar, too. This is called Named Entity Extraction. I \nthink the method used depends a lot on how long your text is. If it's a short \nsnippet, e.g., \"Secretary of State Hillary Clinton\", it'd be hard to pick out the \nname alone. If it's at least one paragraph, capitalization detection almost works; or \nwe can use Reuters' opencalais, but that requires a lot of HTTP calls.\nOr we can use my poor man's NER:\nhttp://d19.ner.dfhuynh.user.dev.freebaseapps.com/ner?\ntext=secretary%20of%20state%20Hillary%20Clinton%20visit\n. From EmilStenstrom on May 21, 2010 06:20:05:\n@dfhuynh: I've also been looking into this kind of thing lately. My use case is that\nI have screen scraped thousands of articles from popular news sites, and now want to\nknow what the articles are about. So for a long article about gridworks I'd like to\nget the keywords: \"Gridworks\", \"Clustering algoritm\", \"Release 1\". \nWhat's poor about your NER implementation?\n. From AndrewOf...@gmail.com on May 21, 2010 09:31:05:\n@stefano, here's an example of some data I've been playing with:\n\"Written by Geoff Johns & Peter J. Tomasi Art by Ivan Reis, Patrick Gleason, Ardian Syaf, Scott Clark & Joe Prado Cover by David Finch & \nScott Williams 1:25 variant cover by Ivan Reis Deadman discovers the truth behind the formation of the White Lantern and what it \nmeans to the twelve returnees and the rest of the DC Universe. Plus, Aquaman, Martian Manhunter, Hawkman, Hawkgirl and Firestorm \ndiscover the price for their resurrections...and why they may be doing more harm than good to the world. Retailers please note: This \nissue will ship with two covers. Please see the Previews Order Form for more information. On sale JULY 21 32 pg, FC, $2.99 US'\"\nIdeally I'd like to be able to extract the names of the artists/writers as well as the names of the characters such as Aquaman.\n. From tfmorris on May 21, 2010 15:11:15:\nEntity extraction seems orthogonal given that Gridworks is fundamentally a rectangular \ndata thing, not a text thing.\nHere's a recent overview of several of the available APIs which may be useful.\nhttp://faganm.com/blog/2010/01/02/1009/\nBe sure to check the comments for mentions of services that they overlooked.\n. From stefano.mazzocchi@gmail.com on May 21, 2010 16:56:40:\n@AndrewOfPie, thanks for the example.\nWhile I agree with @tmorris that entity extraction is a different game from rectangular data operations (in fact, \nit's normally what you do to begin the structuring of unstructured text), it tickles me that Gridworks's contexts \nmight, in fact, help out in entity extraction processes.\nFor example, if you can somehow specify that the names you're looking for are names of a particular Freebase \ntype (here, Comic Book Author)... you can imagine being able to perform a simple NLP extraction of things \nthat can be names plus a Freebase search to make sure they actually are. You might need to do multiple \npasses if you expect multiple types to be named in there, but it shouldn't be too bad to do that.\nNow, does anybody know of a good NLP open source library (possibly in java?)\n. From dfhu...@gmail.com on May 21, 2010 17:17:52:\n@EmilStenstrom: it's poor because it only looks for capitalized words for name \ncandidates. Then it searches Freebase for those names. This is the result for your \nsample paragraph:\nhttp://tinyurl.com/296a3aw\nNot very good, and hell slow.\n@stefano: Stanford NLP Parser, but that's GPL. Grr. An extension framework would \nprobably isolate the GPL, right? Besides, that parser is hefty and shouldn't be \nshipped with the core anyway.\n. From stefano.mazzocchi@gmail.com on May 21, 2010 17:30:25:\n@david Sounds like a great argument for a real extension framework ;-) The alternative is for us to provide a web \nservice that wraps that... but the performance would be horrible and scalability of that service might be an issue.\n. From EmilStenstrom on May 21, 2010 17:46:10:\n@stefano.mazzocchi: I've heard good things about NLTK (Natural Language ToolKit), but\nnever used it myself. Might be worth a look: http://code.google.com/p/nltk/\n. From iainsproat on May 21, 2010 17:58:24:\n\nNow, does anybody know of a good NLP open source library (possibly in java?)\n\nNLTK - Apache 2.0, my preferred NLP framework but it's python http://www.nltk.org/\nUIMA - Apache 2.0, looks good and it's Java but I've not tried it before \nhttp://uima.apache.org/\nGATE - LGPL, http://en.wikipedia.org/wiki/General_Architecture_for_Text_Engineering\nRapidMiner has a text vector tool - Affero GPL, http://en.wikipedia.org/wiki/RapidMiner\n. From tfmorris on May 21, 2010 18:10:59:\nI think most of the Java-based NLP stuff plugs into UIMA (I'm pretty sure GATE does, \nfor example).\nAny type of entity extract will be greatly improved by domain knowledge (ie if you \nknow it's comic book authors, illustrators and super heros).  I'm sure there are NLP \nresearchers using Wikipedia and/or Freebase as a source for this type of domain \nknowledge, but I'm not sure how many of them are talking about it publicly.\nFor the comic book domain, I bet you if you dumped all comic book publishers, \nwriters, illustrators, fictional universes, and characters, you'd end up with a \npretty manageable sized dictionary that you could use for simple dictionary lookups \nof N-grams.\nStill nothing to do with Gridworks though, in my opinion.\n. From iainsproat on October 14, 2010 09:32:31:\nI believe the above feature would be best suited to an extension.\n. From thadguidry on November 08, 2010 21:01:01:\nRelEx, a narrow-AI component of OpenCog, is an English-language semantic dependency relationship extractor, built on the Carnegie-Mellon Link Grammar parser. It can identify subject, object, indirect object and many other syntactic dependency relationships between words in a sentence.\nhttp://wiki.opencog.org/w/RelEx_Semantic_Relationship_Extractor\n. From sharunsa...@gmail.com on December 13, 2010 05:10:21:\nI have a part of speech tagger (API) running on appengine using nltk's default tagger. Some notes here if anyone is interested\nhttp://www.google.com/buzz/sharunsanthosh/9E7UfxdVqgx\n. So, this discussion started back in 2010 and the state of the art has obviously moved on since then, but I believe it's something that's still of interest to the users of OpenRefine.\nAs @dfhuynh  mentioned in his May, 2010 comment, the \"best\" algorithms depend on the length of the target text, the domain, and a number of other factors.  OpenNLP, as well as more recent projects like cTAKES, might provide useful starting point.\nAt this point, this isn't something the core team has the bandwidth to be able to advance effectively, but if someone were to define a use case, research the best alternatives, implement one of them, and submit a pull request, we'd receive it very favorably.\n. From dfhu...@gmail.com on May 22, 2010 02:34:57:\nThis is not a problem in Chrome or Safari. It's only in Firefox. I'd strongly recommend using Chrome, which is \nway faster than Firefox.\n. From EmilStenstrom on May 22, 2010 08:40:35:\nSo Gridworks only works in WebKit based browsers? Since you already don't support\nInternet Explorer, you're down to ~50% of the browser market. When removing Firefox\nfrom the supported list you're down to ~20%. \n1) Since Gridworks opens in the default browser people uses, there's gonna be a lot\nof confused people out there, that don't know why their favourite feature is broken.\nThey would have to start gridworks, copy the url from  to another\nbrowser, each time they start it up.\n2) Another reason to support Firefox is that lots of web developers use Firefox. Web\ndevelopment tools are still better in Fx than most other browsers.\nI strongly recommend supporting Firefox fully.\n. From tfmorris on May 22, 2010 16:12:47:\nIt's also at odds with the Freebase.com folks who support Firefox, but not Chrome.\n. From stefano.mazzocchi@gmail.com on May 22, 2010 17:51:22:\nYeah, I don't think it's reasonable to ignore a problem that is only on Firefox. Now, the question is: is it \nbecause the browser is too slow to react? or because there is some inconsistency in behavior?\nIf the browser is too slow, there is not much we can do but wait for Firefox to get better (gw is pushing the \nlimits on the browser, substantially) so a WontFix is understandable.\nBut if the problem is just behavioral, I agree we should fix it.\nDo we know which one it is?\n. From stefano.mazzocchi@gmail.com on May 22, 2010 17:56:54:\n@tmorris, FYI most developers on freebase.com use Chrome, not sure where you got the idea that its not \nsupported but I can guarantee you that it is: Chrome is pretty much becoming the browser of choice for \ndevelopment at Metaweb, especially the dev branch, given its speed and its improved developer tools.\nBut that doesn't mean that firefox should not remain at the center of our radar: it is and it remains so no matter \nwhat we use to develop and test day by day.\n. From tfmorris on May 22, 2010 18:15:26:\nI've been told Chrome is unsupported in multiple contexts including bug report \nresponses and email conversations.  The following might also lead one to believe that \nto be the case.\nhttp://wiki.freebase.com/wiki/Browser_support\n\"Browser support\nSupported Web Browsers:\nThe freebase.com web site supports the following web browsers:\nMicrosoft Internet Explorer Version 7 or greater (PC)\nMozilla Firefox Version 2.0 or greater (Mac or PC)\nApple Safari Version 3.0 or greater (Mac or PC)\nThese browsers are tested whenever new software is released at freebase.com.\nFirefox Variants (e.g. Camino, Flock):\nThough we don't officially test and support web browsers that are based on the \n\"Gecko\" layout engine ( other than Firefox), these browsers should work fine with \nfreebase.com.\"\n. From dfhu...@gmail.com on May 22, 2010 21:03:33:\n@EmilStenstrom: This bug does not occur in IE8 and Opera 10.53, either. And our client-side testing framework \nuses Firefox as the test browser. So, we are supporting Firefox, but we're not supporting Firefox's quirks.\nI'm against special-casing for browsers' quirks lest they won't fix their quirks, unless the quirk really breaks a \nmajor functionality. And this bug doesn't break a major functionality. In the text search facet, I don't think it's \noften that we find ourselves using the mouse to position the text cursor. As far as I know, for text search fields \nin general (not just in Gridworks), the common operation is to backspace and type, or to use the keyboard arrow \nkeys to get the cursor to the right place.\nI'll keep this bug open in case anyone else wants to take a stab at it. The solution should not have an \"if it's \nFirefox\" test but should be generic code that works on all standard-compliant browsers.\n. From EmilStenstrom on May 23, 2010 14:14:51:\n@dfhuynh: I understand your stance here. You should not have to deal with 10\ndifferent browser quirks, and instead work with code that works cross browser. If\nsome browser is bad enough not to be used with Gridworks, you should put up a big\nsign saying \"Sorry, your browser lacks vital features needed for Gridworks to work\".\nAnyways. This is not one of these issues. I'm an interface developer myself, so I\ndived into the frontend code so see why Firefox behaved the way it does. This is why:\n... The input element is inside this ...\n-moz-user-select is a mozilla specific css property, that will make that element, and\nall subelements, unselctable. More info here:\nhttps://developer.mozilla.org/en/CSS/-moz-user-select\nI don't know what UI-framework you use, but probably you've set unselectable=true on\nsome parent element, not expecting it to inherit. If that guess is correct, resetting\nthe selectable status on the text box should do the trick.\nThe reason I filed a bug for this is because it annoyed me a lot, since I encountered\nit quite a lot. When using regexps to filter records I frequently go back an forth\nbetween the facet and filter boxes, and using the mouse for that is much easer than\ntabbing. So this is not a \"I think things should work differently\"-bug, it's a \"I'm\nfrequently experiencing this behaviour, and think others are too\"-bug.\nThat said, the fix should be simple. Thanks for your patience.\n. From dfhu...@gmail.com on May 23, 2010 17:36:41:\n@EmilStenstrom: Thanks for tracking the problem down to -moz-user-select. This CSS attribute was set by a \njQuery call that I copied and pasted from a jQuery UI sortable example. Removing that call fixes the bug and \ndoesn't seem to introduce any problem.\nAnd you are right: I can see that in regex mode people would want to position the text cursor using the mouse.\nFixed by r843. Please verify.\n. From EmilStenstrom on May 29, 2010 14:14:42:\n@dfhuynh: Sorry, I don't have the required build tools needed to run the svn version\n(ant), and don't quite have the time to go through and install them. So I'm eagerly\nawait the next stable release!\n. From iainsproat on May 29, 2010 14:52:53:\n@EmilStenstrom - version 1.1 is already available, please download from \nhttp://code.google.com/p/freebase-gridworks/wiki/Downloads\nCould you verify that this issue is fixed in version 1.1?  Thanks\n. From EmilStenstrom on May 29, 2010 15:21:28:\nDamn you're fast :) I can verify that it works perfectly, thanks!\n. From EmilStenstrom on May 22, 2010 08:40:35:\nSo Gridworks only works in WebKit based browsers? Since you already don't support\nInternet Explorer, you're down to ~50% of the browser market. When removing Firefox\nfrom the supported list you're down to ~20%. \n1) Since Gridworks opens in the default browser people uses, there's gonna be a lot\nof confused people out there, that don't know why their favourite feature is broken.\nThey would have to start gridworks, copy the url from  to another\nbrowser, each time they start it up.\n2) Another reason to support Firefox is that lots of web developers use Firefox. Web\ndevelopment tools are still better in Fx than most other browsers.\nI strongly recommend supporting Firefox fully.\n. From iainsproat on May 21, 2010 12:22:49:\nI believe you can already:\nFlag (or star) the offending row\nIn the dropdown above the flag you can get a facet, by going to Facet > Facet by\nflag.\nFrom the facet that opens select the 'true' option.\nIn the dropdown menu above the flag you can go to Edit Rows > Remove all matching\nrows.\nThis should delete all the flagged rows.  Is this what you were looking for?\n. From dfhu...@gmail.com on May 21, 2010 17:02:18:\nRight now the row removal operation isn't so optimized. I'd recommend flagging or \nstarring all the rows you want to remove before applying the operation just once.\nIn Gridworks, it's best to make changes en masse. Otherwise, you'd be going against the \ngrain of the tool.\n. From EmilStenstrom on May 21, 2010 17:43:19:\n@iansproat: Thanks for your reply. I think the reason I didn't find it was because I\ndidn't think it made sense to get to row editing operations from a column dropdown.\nThat's of course false, but explains my chain of thought.\n@dfhyynh: That makes sense. I'm slowly getting a grasp of everything I can do with\ngridworks. Thanks!\n. From dfhu...@gmail.com on May 22, 2010 02:28:46:\nFixed by r841.\n. From stefano.mazzocchi@gmail.com on May 21, 2010 17:32:52:\nWhat you mean by \"supporting\"? You mean being able to point gw to a wikipedia page containing a table and \ncreate a grid from it?\n. From dfhu...@gmail.com on May 21, 2010 17:41:56:\nThis is probably best supported by an independent bookmarklet. Gridworks itself should \nsupport creating a project by pointing to a data file URL and by pasting in raw text.\n. From iainsproat on May 21, 2010 17:46:17:\nAn html table importer would also be useful.  Adapting the xml importer to deal with \n,  and  would be a start.\n. From tfmorris on May 21, 2010 17:50:46:\nI'm open concerning implementation as long as it preserves the Wikipedia link and uses \nit to resolve to an exact Freebase topic without human intervention.\nPossibilities that come to mind include:\n- allowing an HTML table cut from a web page to be pasted into Gridworks\n- recognizing Excel / Open Office spreadsheets which contain Wikipedia links\nBonus points for the least number of manual steps to produce useful results.\n. From antonio....@gmail.com on May 25, 2010 22:12:45:\nA possibility is to convert table content in csv. A bookmarklet for that: \nhttp://table2csv.zeusi.user.dev.freebaseapps.com/index\nOne problem with wikipedia links is that blue and red links are mixed. If we convert\nthe good ones, we will get in the same column ids and names, but we should be able to\nreconcile them separately with facets.\n. From tfmorris on October 14, 2010 18:25:47:\nI figured out where HTML links are stored in OO Calc, so I may be able to easily add the ability to optionally convert linked cells in a value + link pair (or even convert the Wikipedia link to a properly escaped Freebase key).  Hmmm, thinking out loud, a general HTML link -> Freebase key function in Refine which was functionally the same as the old web client link parser could be very useful.  I think all the URI templates are still available even though they aren't being used (on input) any more.\nAnother peculiarity of Wikipedia tables that I just discovered the other day is the use of  elements as sort helpers.  I don't know about Excel, but OO Calc can't handle this at all.\nA typical usage (from memory) might be something like\n00001234560000001,234.56 \nwhere an invisible, zero padded, fixed point, numeric only string is created so that it will collate properly using an alpha sort which mimics the numeric sort.  Unfortunately the few tools that I tried ignored the styling and munged the two strings together making them pretty much useless without manual cleanup.\nNot sure what can be done about that one.\n. From dfhu...@gmail.com on May 21, 2010 17:41:06:\nCould you attach your project tar file? (Use the main menu Project -> Export Project)\n. From tfmorris on May 21, 2010 17:55:24:\nExported project file attached\n. From dfhu...@gmail.com on May 27, 2010 05:24:19:\n@tfmorris, maybe some bugs have gotten fixed along the way, but I can't repro this bug with your project file. \nCould you svn up and see if it's still a problem?\n. From tfmorris on May 29, 2010 22:21:20:\nI tried with the latest SVN and the schema skeleton didn't reappear (ie it wasn't just \nhidden).  I recreated the missing pieces and tried loading it again into the sandbox \nagain and this time nothing disappear, so that's a good sign.  Even though it's only \none try, it hadn't consistently disappeared several times before that.\nI'll try to find some time to test it a little more thoroughly so that it can be \nclosed with confidence.\n. From dfhu...@google.com on September 27, 2010 22:11:19:\nHi Tom, should we close this bug?\n. From tfmorris on September 28, 2010 03:28:53:\nYes, go ahead and close it please.\n. From dfhu...@gmail.com on May 23, 2010 05:56:15:\nUpon further thinking, it seems impossible to support more than one meta facets simultaneously. Consider two \nmeta facets A and B both of which have selections. In order for A to instantiate its RowFilter, it must first compute \nits choice/count pairs. In order to do so, A needs a FilteredRows that incorporates the RowFilter by B. And vice \nversa: for B to instantiate its RowFilter it needs the RowFilter by A.\nNot all is lost, though. Instead of meta facets, what we can easily support is a command that creates a new \ncolumn and fill it in with facet choice counts. Then a numeric range facet can be used on that new column. The \ndrawback is that those counts don't change dynamically.\n. From iainsproat on May 23, 2010 07:11:17:\nI tried to do this manually - creating two facets for two different columns (I was \nusing the movie sample data from the test data folder in the SVN trunk).  It seems to \nwork, so I'm not sure why we couldn't do it in code?\nI created one for performances-actor and another for performances-character.  I \nsorted performances-actor by count and selected all actors with counts above 5 (Mike \nMyers, Eddie Murphy & Cameron Diaz).  I then sorted characters and selected all \ncharacters with counts above 3 (Princess Fiona, Shrek, Donkey).  As each character \nwas selected the actor count was filtered and updated.\n. From dfhu...@gmail.com on May 23, 2010 18:05:48:\n@iainsproat: you were probably lucky to get a case that works. Actually you don't need 2 meta facets to have a \nproblem here. Just 1 meta facet and some regular facets are enough to cause problems.\nConsider a data set with 3 rows and 2 columns\nA  C\nA  D\nB  D\nConsider 3 facets\nText facet P on first column\nA (2)\nB (1)\nText facet Q on second column\nC (1)\nD (2)\nMeta facet M on first column (that is, M is meta with respect to P)\ncount of 2 (1)\ncount of 1 (1)\nNow, in M, select count of 2:\nM\ncount of 2 (1) -selected\ncount of 1 (1)\nP\nA (2)\nQ\nC (1)\nD (1)\nIf in Q you select C, what do you expect to happen? Ignore M for the moment and consider P. Selecting C in Q \nwould change P to\nP\nA (1)\nThis is because only exactly 1 row has C. But since M selects any row that corresponds to a choice in P with \ncount 2, now M must select no row at all.\nOne solution is as I mentioned before to create a column with the facet counts.\nAnother solution is to make meta facets not affected by other facets (nullifying the problem in the example \nabove). That is, making selections in other facets shouldn't change a meta facet's choices. M only work with the \nchoice counts in P when there is no facet selection whatsoever.\nThe pros here include less data getting stored, fewer clicks to get what you want, and thus more interactivity. \nThe cons include potential confusion as to how facets in general affect one another. I think with the right design \nthe confusion can be mitigated. It's probably one of those cases where it would work as you expect if you don't \nthink too much about it.\nI'm leaning toward the second approach. It shouldn't be too hard to implement.\n. From dfhu...@gmail.com on May 24, 2010 19:10:02:\nFixed by r848. For any text facet, scroll down to the bottom of its choice list. You should see \"facet by choice \ncounts\". Click on that and you'd get a numeric range facet.\n. From iainsproat on May 24, 2010 08:51:08:\nCSV export is not currently available - I'll pick this up.\n. From iainsproat on May 24, 2010 12:38:18:\nShould now be implemented in r845.\nsimon.roe could you verify?\n. From simon....@gmail.com on May 24, 2010 13:30:19:\nExcellent, this works as expected.  In firefox on ubuntu the 'download file' prompt reports that the file is a TSV \nfile, but the downloaded file is CSV.\nThanks for addressing this so quickly!\n. From iainsproat on May 25, 2010 19:37:31:\nShift+Reload sorted it.  Browser cache issue.\n. From iainsproat on May 26, 2010 17:20:40:\nI'll take this.  I'm creating some unit tests for the XmlImporter.\n. From iainsproat on May 26, 2010 18:27:47:\nI reckon it's to do with the line breaks in the xml text node.\n. From stefano.mazzocchi@gmail.com on May 26, 2010 18:42:54:\nProbably, but I agree it shouldn't do it. Whitespace in XML (generally speaking) is not structurally significant.\n. From iainsproat on May 26, 2010 19:24:14:\nWritten a unit test which verifies that line breaks are not at issue.\nThe problem I now think is that not all the europeana elements have the same structure.\ni.e. One Europeana element has 10 nested elements, while another has only 5 nested \nelements.\nr862 provides a unit test for this case.\n. From iainsproat on November 25, 2010 17:06:56:\nReturning to this issue, it seems to be the following combination of line returns and whitespace can break the xml importer \"\\n \\n\" ( Ux000AUx0020Ux000A in unicode).\nThis seems to be an issue with the XmlStreamReader rather than Refine's code, the following test will fail as the \"Author1\\n \\nThe\" is split into two tokens rather than one:\n@Test\n    public void testXmlStreamReaderWithLineBreak(){\n        try {\n            ByteArrayInputStream inputStream;\n            inputStream = new ByteArrayInputStream( \"<?xml version=\\\"1.0\\\"?>Author1,\\n \\nThe\".getBytes(\"UTF-8\"));\n            XMLStreamReader reader = XMLInputFactory.newInstance().createXMLStreamReader(inputStream);\n            reader.next();//START_ELEMENT\n            Assert.assertEquals(reader.getLocalName(),\"library\");\n            reader.next();\n            Assert.assertEquals(reader.getText(), \"Author1,\\n \\nThe\");\n        } catch (UnsupportedEncodingException e) {\n            Assert.fail(e.getMessage());\n        } catch (XMLStreamException e) {\n            Assert.fail(e.getMessage());\n        } catch (FactoryConfigurationError e) {\n            Assert.fail(e.getMessage());\n        }\n    }\n. From iainsproat on November 25, 2010 17:09:50:\nbut the following test passes (uses reader.getTextElement() rather than reader.getText()):\n@Test\npublic void testXmlStreamReaderWithLineBreak(){\n    try {\n        ByteArrayInputStream inputStream;\n        inputStream = new ByteArrayInputStream( \"<?xml version=\\\"1.0\\\"?><library>Author1,\\n \\nThe</library>\".getBytes(\"UTF-8\"));\n        XMLStreamReader reader = XMLInputFactory.newInstance().createXMLStreamReader(inputStream);\n        reader.next();//START_ELEMENT\n        Assert.assertEquals(reader.getLocalName(),\"library\");\n        Assert.assertEquals(reader.getElementText(), \"Author1,\\n \\nThe\");\n    } catch (UnsupportedEncodingException e) {\n        Assert.fail(e.getMessage());\n    } catch (XMLStreamException e) {\n        Assert.fail(e.getMessage());\n    } catch (FactoryConfigurationError e) {\n        Assert.fail(e.getMessage());\n    }\n}\nI'll try to adapt the code to work around this.\n. From tfmorris on November 27, 2010 22:09:27:\nRev 1939 should fix this.  I've turned on text coalescing in the XML parser.  As an added bonus, I also turned on XML entity replacement so it doesn't have to be done after the import.\n. From iainsproat on November 27, 2010 22:47:50:\nAwesome, thanks Tom.  I wasn't even aware about the IS_COALESCING property.\n. From dfhu...@gmail.com on May 27, 2010 04:50:06:\nFixed by r865. Please verify.\n. From thadguidry on November 03, 2010 13:52:54:\nWHAT ? You don't remember gopher:// ???\nhttp://awesomescreenshot.com/0f034dd95\nLet's add gopher protocol also...it's still around with valid urls shown in my screenshot that work in Firefox and gopher clients.\n. From thadguidry on May 27, 2010 03:51:00:\nAfter closing browser, restarting Windows PC, and relaunching gw, I was successful in \nopening and exporting my previous project.  Further inspection revealed that on the \nsecond Chrome window of the 2nd project export a message was showing \"multiple \ndownloads\" blocked on an empty tab that wasn't seen the first time when this bug was \ncreated.  Accepting on that message allowed the download to continue correctly.\n. From dfhu...@gmail.com on May 27, 2010 06:59:22:\nNot a gw issue.\n. From stefa...@google.com on August 31, 2010 19:39:26:\nNot really necessary since most functionalities work even without Freebase (or any internet connectivity)\n. From iainsproat on October 12, 2010 17:04:45:\nDavid pointed out some examples of a javascript pivot:\n[http://people.csail.mit.edu/dfhuynh/projects/www-conferences/www-history-pivot-category-year.html]\n[http://people.csail.mit.edu/dfhuynh/projects/www-conferences/www2007-papers-pivot.html]\n. From dfhu...@gmail.com on July 01, 2010 20:31:20:\nFixed by r1064. Please verify.\n. From stefano.mazzocchi@gmail.com on May 29, 2010 19:34:45:\nYeah, I stil have to make sure all the distros work after yesterday's refactor.\n. From stefano.mazzocchi@gmail.com on May 31, 2010 09:02:21:\nShould be fixed now, please confirm.\n. From iainsproat on June 01, 2010 13:19:13:\nI'm not following.  Can't this be done already?\nEdit Column > Add Column based on this Column\nUse the GEL expression:\ncells[\"col1\"].value + \",\" + cells[\"col2\"].value\nOr do you want an entirely UI way of doing this?\nThe workflow would have to be something similar to the following:\nEdit Column > Add Column by joining Columns\nDialog box with multiple dropdown lists of columns to join together, and a textbox \nfor the separator.\nI'm not convinced it would any quicker or easier.\n. From thadguidry on June 01, 2010 15:32:05:\nDepends entirely on who the user audience is intended with Gridworks, I guess.  But \neven Excel 2007 appealing to the masses has a little UI helper now to frame up this \nstatement: =CONCATENATE(B:B,\" \",C:C)  Look at FORMULAS / TEXT / CONCATENTATE.\n. From sogr...@gmail.com on November 19, 2010 16:26:57:\n\"I'm not following.  Can't this be done already?\nEdit Column > Add Column based on this Column\nUse the GEL expression:\ncells[\"col1\"].value + \",\" + cells[\"col2\"].value\nOr do you want an entirely UI way of doing this?\"\nIn my case at least, this is only a partial solution. This approach succeeds where the columns in question have assigned values. Where some are null, however, the concatenation fails. \n. From russell....@gmail.com on June 30, 2011 12:43:44:\nThis can be done as described only if there are no nulls in the data at hand; things become considerably uglier if there are. \n. From iainsproat on June 23, 2010 17:54:42:\nGood catch.\nFixed in r1024\nknut are you able to checkout the source and verify?\n. From knut.for...@gmail.com on June 23, 2010 18:57:40:\nVerified.\n. From iainsproat on June 23, 2010 19:01:37:\nthanks for verifying\n. From iainsproat on June 10, 2010 10:19:03:\nIf you're looking for a list of other libraries used in Gridworks, the License might be the best place to start.  http://code.google.com/p/freebase-gridworks/source/browse/trunk/LICENSE.txt\nBut I agree it would be good to have it on the wiki page, perhaps also with an explanation of how each is used in Gridworks.\n. From stefa...@google.com on August 31, 2010 18:50:31:\nAdded some info to that page.\n. From stefano.mazzocchi@gmail.com on June 10, 2010 21:53:09:\n-1 on his. I wouldn't want the build system to depend on the availability of other services running, it makes it more fragile.\n. From narphor...@gmail.com on June 11, 2010 05:14:19:\nWow, I thought this would be a easy sell. I've used Maven for some of my projects and I found it very flexible and easy to use. However, if those who have actually committed code prefer the current build system then who am I to disagree.\n. From tfmorris on June 11, 2010 14:29:42:\nMaven is a locally installed tool, not an online service, so I don't understand Stefano's comment.  If you're building offline (or online and a repository isn't available for some reason), you can use the offline flag to build using cached dependency information.\nHaving said that, I'm neutral on the proposal.  In a lot of cases I actually prefer manual dependency management over the automagical updating of hidden dependencies one or two levels down.\n. From dfhu...@gmail.com on June 13, 2010 05:11:49:\nWhile at Simile, we were very excited about Maven and used it quite pervasively for several projects. It worked mostly for our needs, but occasionally hiccupped when the maven repository site was down.\nHowever, in Gridworks' case, we probably don't want to use maven because we want fine-grained control on where the jars go (into the WEB-INF/lib and MOD-INF/lib dirs).\n. From tfmorris on June 13, 2010 18:47:40:\nTo be more concrete concerning offline builds, use the flag:\nmvn -o\n. From stefano.mazzocchi@gmail.com on June 19, 2010 07:07:49:\n@tmorris: maven is a locally installed tool, but the jar dependencies are loaded from a repository over the web. If the repository is down, or slow, you're impacted. Most importantly, not all jars that we depend on are available in the main maven repo, which means that we would have to have our own maven repos, further increasing fragility.\nOffline builds can only be done if the repository is local, which is no different than having the jars in svn like we do today.\n. From dfhu...@gmail.com on June 11, 2010 03:41:12:\nTry the column menu command Edit Cells > Common Transforms > To Blank. That blanks out the cells matching the facets' constraints. If that does what you want, please close the issue.\n. From aery...@gmail.com on June 11, 2010 03:49:07:\nYes it does. Wonderful! Erm, where is the 'close issue' button on this page?\n. From dfhu...@gmail.com on June 13, 2010 05:04:29:\nNo 'close issue' button :) But you could add a comment and change the Status to Done or something like that. Which I'm doing right now. So don't worry about it.\n. From iainsproat on June 14, 2010 09:07:51:\nNo, I believe Gridworks only needs to be installed on a single machine.\nGiven the address of a project e.g. http://127.0.0.1:3333/project.html?project=1874250169833  I think the problem is that the IP address 127.0.0.1 (http://en.wikipedia.org/wiki/Localhost) should be swapped with the external IP address of the machine on which Gridworks is installed - you can find it using 'ipconfig' from the command line on windows (I'm not sure for other OS's), or try http://www.whatismyip.com/\nYou may also have to ensure any firewall is set up correctly to allow your network access to the 3333 port.\nDoes this help?\n. From aery...@gmail.com on June 14, 2010 10:49:45:\nI shall try this on my home computer (no Gridworks installed) tonight before sending the URL of my data to my boss.  Work computer may or may not have a firewall, not keen to ask my sysadmin for what should be a rather simple thing.  Thanks again.\nP.S. Linux uses 'ifconfig' .\n. From aery...@gmail.com on June 15, 2010 09:23:38:\nHi, back again. Still having difficulties. First, does the host computer (\"your own machine\") have to be on for others to access the data?  Second, must the receiving computer be using the same OS as the host computer?  Third, if I'm not in a position to mess with a firewall to allow 3333 port access, does it all still work?\nGenerally I'm a bit confused by the sparse detail indicated by \"you can just send them a URL like you would do for any other web site.\"\nThank you for your time.\n. From iainsproat on June 15, 2010 11:34:33:\nGridworks works as a web server, it runs on top of Jetty http://en.wikipedia.org/wiki/Jetty_(web_server).  Everything is done over http, so accessing Gridworks is OS independent.\nLooking at it further (I tried it on my network), I couldn't get it to work either.\nI might be entirely wrong, but I'm guessing the issue might be that Gridworks binds to a default address, 127.0.0.1,  compared to other servers which default to binding to all addresses on the machine.  I think we might have to pass the Gridworks server a variable when starting it.  I'm not entirely sure how to do this, -Dgridworks.host perhaps?\nIf you don't have access to open up the firewall for the required port you would also run into problems there.  A workaround might be to change Gridwork's default port to something already open on your machine (providing it doesn't conflict with whatever else is meant to be using that port).  Again, this might be possible to do by passing a variable on startup.  -Dgridworks.port perhaps??\n. From iainsproat on June 15, 2010 11:44:48:\nOK, worked out how to change the IP binding:\n1. In the Gridworks installation directory there is a file gridworks.l4j.ini\n2. Add a new line to that file with the following  (changing 0.0.0.0 to the IP address of the machine on which it's hosted)\n   -Dgridworks.host=0.0.0.0\n3. To change the port add a line with -Dgridworks.port=30\nLet me know if this works, and I'll update the documentation\n. From tfmorris on June 15, 2010 15:08:45:\naeryise - The host computer does need to be on and connected to the network.  It should not need to be running the same operating system.  If the port that you're using is firewalled, you'll need to change to a non-firewalled port.  If the host machine doesn't normally run a web server, you can change to the default HTTP port of 80.\n. From aery...@gmail.com on June 17, 2010 06:52:01:\nThanks. I'll try these again on my home computer tonight.\nMy gridworks.ini looks like this, in the standard Linux version of GridWorks. Should I then append the -Dgridworks.host and -Dgridworks.port lines?  Or just change the relevant values for #GRIDWORKS_PORT and #GRIDWORKS_HOST?\nGRIDWORKS_PORT=3333\nGRIDWORKS_HOST=127.0.0.1\nGRIDWORKS_MEMORY=1024M\nJAVA_HOME=\"\"\nJAVA_OPTIONS=\"-XX:+UseParallelGC -XX:+UseLargePages\"\n. From aery...@gmail.com on June 17, 2010 07:14:35:\nSo far, on the exact same computer that GridWorks is installed and my data is hosted on, changing the IP binding with the -i flag to the external IP address of this computer on the ./gridworks run command, works fine.  Will try other combinations later at home.\n. From aery...@gmail.com on June 18, 2010 02:49:50:\niainsproat - On the exact same computer that GridWorks is installed and my data is hosted on, after appending the -Dgridworks.host line with my external IP address and restarting the computer this morning, the IP address still defaults to 127.0.0.1 .  This may be an OS-specific matter and I will now try modifying the #GRIDWORKS_HOST value instead of appending. But, from other programs I've only ever seen a -D(etcetc) line being used to compile code with cpp . Since I'm using the Linux download of GridWorks straight off the shelf, are the -D(etcetc) lines applicable for the prepackaged binary that I have?\nThanks.\nAm avoiding port/firewall issues for now by doing all these changes on the exact same computer that GridWorks is installed and my data is hosted on, instead of my home computer.\n. From aery...@gmail.com on June 18, 2010 10:22:14:\nFurther update: On this computer as above, changing the #GRIDWORKS_HOST value to my external IP address and not appending the suggested -Dgridworks.host line, upon restarting the computer the IP address still defaults to 127.0.0.1 . However as per comment #  8 passing the external IP address as an argument to the ./gridworks run command with the -i flag continues to work fine.\n. From iainsproat on June 18, 2010 12:19:39:\naeryise, thanks for feedback.  If the -i parameter works then please use that.\nWe've discussed this issue amongst the Gridworks developers, and we think this feature is not quite ready for the prime time yet.  As well as the difficulties you've had we're aware that Gridworks does not require any login details, which raises issues with security. For example, once the firewall was opened it would be possible for anyone to view and edit your Gridworks files (whether they were allowed to or not).  But we believe it shouldn't allow access to your system at large - only to Gridworks data.\nFor the moment, we've removed the mention of this feature from the Gridworks homepage (but it still works if you're prepared to change the configuration manually, as above, and deal with the security issue). Hopefully, at some point in the future a developer will have time to implement the necessary enhancements to Gridworks so we can properly support this feature.\n. From aery...@gmail.com on June 19, 2010 14:13:06:\nThanks :) no problem.\n. From stefa...@google.com on August 31, 2010 18:11:44:\nYes, Gridworks only binds to the 127.0.0.1 (localhost) IP interface by default, meaning that it will respond only to local calls. This is because Gridworks is not designed for concurrent multi-user operation and can result in data corruption. We are working on a system that allows multiple Gridworks to work on the same data concurrently, but this is not ready for prime time yet.\nBut in case you want to expose Gridworks anyway, you can run it as\n./gridworks -i 0.0.0.0\nwhich will make it listen to all requests coming to your machine, no matter what IP interface they come from.\n. From aery...@gmail.com on June 18, 2010 10:22:14:\nFurther update: On this computer as above, changing the #GRIDWORKS_HOST value to my external IP address and not appending the suggested -Dgridworks.host line, upon restarting the computer the IP address still defaults to 127.0.0.1 . However as per comment #  8 passing the external IP address as an argument to the ./gridworks run command with the -i flag continues to work fine.\n. From dfhu...@google.com on September 27, 2010 22:19:12:\nAlready fixed before r1326. Will be available in 2.0.\n. From tfmorris on September 18, 2012 17:20:52:\nRemove obsolete milestone\n. From iainsproat on June 17, 2010 20:53:22:\nI'll look into this.  We use an external library for TSV parsing so my guess is that it's working as should be expected, but I'll check it out.\n. From iainsproat on June 20, 2010 14:51:13:\nThe importer is working as expected.  The problem is that these examples are malformed TSV.\nTo resolve this I've added an option to import ignoring all quotation marks.  I've added this feature and committed the change (r1002) to the SVN trunk - there's now an 'ignore quotation marks' option in the importer.  Could you checkout and build the latest revision of Gridworks source and verify it works for you?\nPlease note that correct parsing behaviour using this option will rely on their being no tabs or newlines within quoted values.  If you've both malformed TSV and additional separator characters or newline characters within quoted values then it won't be possible to deal with it automatically.  You'll have to fix the data before or after import into Gridworks.\n. From iainsproat on June 20, 2010 14:54:04:\nThat should be r1010\n. From jaywgra...@gmail.com on June 20, 2010 17:05:44:\nI don't have the capability to build, I'll wait for the next release and test.\n. From iainsproat on June 20, 2010 13:33:32:\nManually added the missing folder (I'm not too hot on build scripts).  I then get the following error:\nBUILD FAILED\nsourcecode-directory\\build.xml:155: The following error occurred while executing this line:\nsourcecode-directory\\extensions\\build.xml:12: The following error occurred while executing this line:\nsourcecode-directory\\extensions\\sample-extension\\build.xml:53: sourcecode-directory\\main\\lib does not exist.\nOn manually adding that missing directory, I get all sorts of Java errors with missing Jython methods.\n. From iainsproat on June 20, 2010 14:28:25:\nJava errors as follows:\n[javac] sourcecode-directory\\extensions\\jython\\src\\com\\metaweb\\gridworks\\jython\\JythonEvaluable\n.java:19: package com.metaweb.gridworks.expr does not exist\n[javac] import com.metaweb.gridworks.expr.HasFields;\nSeems it's not finding the gridworks compiled classes?\n. From iainsproat on June 21, 2010 05:59:22:\nfixed r1012 and r1013\n. From dfhu...@gmail.com on June 21, 2010 05:10:46:\nprematouch: no, Gridworks doesn't support adding rows right now. For your case, is it the plan to manually export new data to a file and then append it to an existing Gridworks project? Or do you have some database trigger that can, say, do an HTTP POST to Gridworks?\nBut note that even if Gridworks lets you add more rows to an existing project, any operation you have applied to the project will not affect the newly appended data.\nI'd recommend picking a low traffic period of time to take your UI offline \"for maintenance\". Re-export from the database, import into Gridworks, re-apply the same operations, export from Gridworks, import back to your database. It's not ideal but maybe the best solution for now.\n. From premato...@gmail.com on June 22, 2010 01:59:59:\nThank you so much for your clarification and suggestions. In that case, there's not that much new data coming in so we'll probably just make sure it's added cleanly to the newly cleaned exported file from Gridworks before we add it back to the database.\nBTW, Very happy with Gridworks and can only see that it will continue to improve.\nThanks again\n. From dfhu...@gmail.com on June 22, 2010 20:43:55:\nOK, I'll close this issue for now.\n. From dfhu...@gmail.com on June 22, 2010 20:45:44:\nThere is already \"mod\".\n. From thadguidry on June 22, 2010 19:30:32:\nExample file of data to be split into columns is attached.  Sometimes the use case will include more than 2 rows of \"TYPES\" to be split into columns, but in this example file it's just 2 rows each.\n. From dfhu...@google.com on August 08, 2010 06:31:14:\nFixed by r1133.\n. From iainsproat on June 23, 2010 15:10:29:\nI'm currently working on teasing out an abstract tree-data importer from the XmlImporter, and then building the JsonImporter around that.\n. From iainsproat on October 04, 2010 10:07:11:\nIn trunk as of r1421\nIt passes all unit tests, but (for various reasons) I've not yet checked any real world examples on it so may have some bugs which don't (yet) have unit tests.\nIt should now be handling all .js and .json files.\n. From iainsproat on November 11, 2010 14:34:21:\nI think this would be almost entirely UI work.\n. From dfhu...@gmail.com on August 14, 2011 01:11:55:\nNow fixed in trunk by new importer UI work.\n. From iainsproat on November 11, 2010 13:17:57:\nadded a fixed width importer in r1857\nAlterations to the UI are still required to fully address this task.\n. From chr...@gmail.com on December 09, 2010 23:12:27:\nIs there any way to access this feature now?\n. From chr...@gmail.com on December 10, 2010 01:49:14:\nI used the current ability in r1836 to do this and it was relatively easy once I realized that Smultron tells you the length of a selection at the bottom of the window (assuming you have it enabled in Preferences > Appearance).\n1. uncheck \"Split into columns\" before Creating Project\n2. Edit Column > Split into several columns... > by field lengths\nThat being said, wysiwyg would be nice.\n. From dfhu...@gmail.com on August 14, 2011 02:24:12:\nFixed now in trunk by new importer UI work.\n. From dfhu...@google.com on September 28, 2010 05:15:13:\nThis already be fixed in trunk/.\n. From ch...@netconstructor.com on July 17, 2011 14:14:02:\nWould be excellent if one could execute a transform and just close his browser and have things run in the background. Opening up the :3333 webpage should show a processes that are executing along with the %completed and current record out of x total. Accessing the project should show the same info with updated info and the ability to pause/stop.\n. From tfmorris on September 18, 2012 17:20:52:\nRemove obsolete milestone\n. From narphor...@gmail.com on June 23, 2010 16:13:01:\nI'm not so sure that this is a good idea. Making the Json parser more tolerant might just remove the incentive for extension developers to fix their code (or extension users to upgrade). \nIt reminds me of how early web browsers supported malformed HTML to try and make the web \"just work\". We're still dealing with the aftermath of that decision.\n. From dfhu...@gmail.com on August 14, 2011 03:02:01:\nThis was actually fixed a while back for 2.0. It just fixed missing commas or [ ], which are easy to miss when editing a list of operations.\n. From dfhu...@google.com on August 07, 2010 23:22:42:\nI started by adding commands in the column header dropdown menu. They should unblock certain scenarios. Supporting drag and drop will require quite a bit more work.\n. From dfhu...@google.com on September 27, 2010 22:24:47:\nFixed before r1326. Will be available in 2.0\n. From fredrik....@gmail.com on October 29, 2010 13:23:41:\nSome related discussion to this from the May 2010 list archives: http://lists.freebase.com/pipermail/freebase-discuss/2010-May/001491.html\nI guess this could be manually implemented for now by making a joined column with all relevant fields, then clustering that field with existing tools, whereafter a manual GROUP BY on the resulting dataset using the clustered column will return only unique rows. \nThe issue then, however, would be to know how to utilize any complementary data found amongst the duplicates... Is there any official wiki talk page to discuss this feature yet?\n. From thadguidry on July 13, 2010 01:13:21:\nA Whitelist option within Search API was suggested to Faye and Andi as a possible performance improvement for indexing Commmons types VERSUS using mql_filter to accomplish this.  I would suggest this issue be discussed with each of them first regarding this needed feature.\n. From dfhu...@gmail.com on July 18, 2010 01:19:02:\nIain, it does support constraints right now: after you've added a property and the preview gets updated, the corresponding column header has Remove | Constraint. It certainly needs design work.\n. From perrone...@gmail.com on December 23, 2011 10:57:18:\nHello, I want to add population values for each country in my table but as I guess you know when you try to add a new column from Freebase choosing the \"Population number\" property you get that value for ten different years for each country but, in my case, I want only the last value. \nI saw the constraint option in the preview dialog and I changed the default JSON query which is as the following one:\n({\n  \"limit\": 10\n})\nputting 1 instead of 10 but unfortunately this does not perform what I want because anything has been modified.\n. From tfmorris on December 27, 2011 20:42:00:\nRefine has supports constraints in this context now (not sure what version they were introduced in).  \nre: comment 4 - A late change added for v2.5 was the ability to use the \"sort\" keyword in constraints (it was filtered before).  To get the latest population figure which is within the last 5 years, you could do something like\n{\"year>\":\"2006\",\n \"year<\":\"2012\",\n \"sort\":\"-year\",\n \"limit\":1\n}\nThe \"year<\":\"2012\" is necessary because some projected future population data (e.g. for the year 2025) have been added to Freebase by users.\n. From tfmorris on October 14, 2011 22:30:40:\nColumn groups are currently displayed during project editing.  Is this proposal for them to be displayed during the import preview or does the current display suffice?\n. Closing due to lack of response from original requester, so I'm assuming the current implementation is sufficient.  Feel free to reopen if something else is needed ( @iainsproat or anyone else)\n. From iainsproat on June 23, 2010 16:51:58:\nSome thoughts on how this might work (suggestions welcome):\nA: the server getting a map graphic and adding it to the image produced by the scatter facet.  Returned and displayed in the UI like any other scatter facet.  Some scatter facet features, such as 'Log', wouldn't be appropriate.\nB: the server returning json output which is drawn in the client (like google maps or Open Street Map).  There may be problems with too much data here which needs some thought (clustering of data points by proximity etc..)\n. From iainsproat on November 15, 2010 10:08:11:\nissue #208 has been merged into this issue.\n. From staringmonkey on December 12, 2010 23:29:29:\nSince this ticket has been accepted (yay!!), some thoughts:\nI would def. pursue option A.  The features that a complete client-side mapping implementation would offer (pan, alternate layers, etc.) aren't as pertinent in this case and it would likely make the ability to select a section of them significantly more complicated.  It should be feasible to fetch the Google tiles themselves (OpenLayers does this) and then crop and join the relevant sections to produce a map segment that will contain all the points.\nFrom that point I would like to see it operate essentially like the scatterplot facet.  What I think is the most powerful possibility from my perspective is watching the map change as other facets are altered--being able to experimentally identify geographic patterns that correlate with features of the data.\n. From iainsproat on November 15, 2010 10:08:11:\nissue #208 has been merged into this issue.\n. From stefa...@google.com on August 31, 2010 18:04:09:\njust landed in trunk\n. From dfhu...@gmail.com on July 18, 2010 01:21:01:\nIain, this is supported through the cross GEL function. Could we work together to verify if it does what you want?\n. From ioeas...@gmail.com on June 30, 2011 14:57:22:\nThere is something a bit screwy about the cross function in GREL in at least all versions I've tried up to v2.1 r2098 and on OSX10.5.8\nThe problem is not entirely straightforward to reproduce, but it goes like this. An entirely correct cross expression will return an \"Error: Cannot retrieve field from null\" when it is used on a project that has been receiving some work. \n** However, stopping refine, waiting a bit (which seems to be the important bit), and then restarting the app allows the exactly the same cross expression to work fine.  \nIt is not sufficient to force quit refine and restart it once, or even twice, the problem will persist, nor is it sufficient to rename projects or remove column heading whitespace the any of the other voodoo I've had cause to try. \n. From Martin.M...@gmail.com on December 02, 2011 01:09:20:\ngoogle refine 2.5 RC2 still carry this issue. \nWork around: export the project, do a vlookup in Excel and create a new project in refine.\n. From rockey.n...@leapgradient.com on May 02, 2012 19:10:30:\nI am also facing this issue and this is really causing lot of pain for me. Any workaround apart from excel?\n. From tfmorris on June 09, 2012 14:52:40:\nDoes anyone have a reliable way to reproduce this problem repeatably?  I can take a look at it, but I'd rather spend my time fixing the bug than trying to figure out how to reproduce it.\n. From Martin.M...@gmail.com on June 09, 2012 14:56:31:\nTom,\nFrom my experience and what others are posting it looks like this is a completely random bug and it has no relation with a particular data set. Could it comes from local settings?\nMartin\n. From david.a....@gmail.com on June 19, 2012 03:01:24:\nI experienced this as well. It occurred on my first use of Google Refine. Closing and restarting fixed the problem.\n. From ruben.v....@gmail.com on August 23, 2012 23:12:51:\nI followed the steps in the example at the bottom of GRELOtherFuntions page and have the same error. Even closing and reopening Refine the cross function doesn't work.\n. From ruben.v....@gmail.com on August 24, 2012 03:09:12:\nI figured out why isn't working for me: I pass the tables from the referred page to Refine by using copy/paste, this make that some values include one leading space, so the key values were not identical. Since the tables are small i remove manually the spaces. Now is working fine.\n. From mar...@fifty-five.com on August 30, 2012 15:19:40:\nSame problem here ! It is very random, but I observed that I had the problem when trying to merge large datasets (+500k rows). \n. From tfmorris on August 30, 2012 15:35:10:\nSimple exact match joins are supported by cross(), but cross project reconciliation with a UI similar to reconciling against Freebase still isn't supported, so I'll leave this feature request open.\nGeneral questions (Ruben) should go in the Google Group.\nPlease use issue #432 for discussion of the cross() caching problem and open separate bug reports for unrelated issues.\nThis issue should stay focused on the enhancement request.\n. From dfhu...@gmail.com on September 11, 2011 05:09:14:\nFixed in trunk by custom tabular exporter.\n. From dfhu...@gmail.com on July 18, 2010 01:26:00:\nIain, wouldn't it be better if the data is imported as one column and then split later using the Split Column command? Then it's much easier for us to show preview as the user types in the regex.\n. From iainsproat on July 18, 2010 09:22:47:\nAgreed, that is a much better way - closed.\n. From stefa...@google.com on August 31, 2010 19:13:01:\napplied to trunk (and retained the ability to work on Calendar as well since there can be both)\n. From knut.for...@gmail.com on August 31, 2010 22:51:57:\nThanks for fixing this, however from what I can tell the array bounds problem is still there as of revision 1229\nLine 25 says             Object o3 = args[3];\nwhich is outside the array, the intention must be     args[2]   instead \n. From stefa...@google.com on August 31, 2010 23:25:28:\nd'oh, that's what I get for applying patches by hand.\nFixed in r1250.\n. From thadguidry on June 26, 2010 16:34:11:\nAlso, interestingly, when the process-panel does appear correctly, the height: style parameter does NOT appear in Chrome's Dev Tools.\n. From thadguidry on October 25, 2010 14:20:34:\nSeems to have been resolved recently in trunk (with redesign of 2.0) or now with Chrome 7.0\n. From thadguidry on November 19, 2010 20:45:56:\nVerified as fixed in Version 2.0\n. From joshuaje...@gmail.com on November 15, 2011 19:10:41:\nWould be a useful addition. Thanks.\n. From stefano.mazzocchi@gmail.com on July 01, 2010 07:55:20:\nhmm, what about just using external recon to do this?\n. From iainsproat on July 01, 2010 08:48:54:\nUsing external recon loses all the benefits of doing merge & cluster.\nIf the reconciliation engine doesn't give a high score to the cell value, e.g. 'Paris 1' is not given a high match as being the city of Paris, I'd have to go through each of the Paris cells and ensure it gets reconciled to the correct Paris topic.  Not ideal.\n. From thadguidry on November 20, 2010 16:15:31:\nScreenshot from Refine of the problem: http://awesomescreenshot.com/0783q3qec\n. From thadguidry on November 20, 2010 16:27:15:\nSimilar to Issue-163\n. From tfmorris on November 27, 2010 02:47:21:\nFixed in rev 1935 by setting the encoding for the response.\n. From tfmorris on June 07, 2011 06:37:18:\nissue #357 has been merged into this issue.\n. From tfmorris on June 07, 2011 06:37:18:\nissue #357 has been merged into this issue.\n. From dfhu...@gmail.com on July 03, 2010 03:59:30:\nRichard & Fadi, thank you! That looks exciting! I'll try to incorporate your patch in very soon.\n. From dfhu...@gmail.com on July 03, 2010 04:02:42:\nFadi, could you just zip your extension's directory and attach it here? It'd be easier for me. Thanks!\n. From fadima...@gmail.com on July 03, 2010 14:02:14:\nHi David, \nthanks for your response.\nI attach the extension. I also attach a patch file containing changes to the core files without the newly created ones which are under the extension (not sure if needed but the code won't compile without modifying the core classes. I am not sure if this is a good way for the extension but I could not figure out a better way, for example, to add an option to the menu or to add a reference to the Project class to the RDFSchema I had to edit menu-bar.js and Project.java).\nThanks\n. From dfhu...@gmail.com on July 04, 2010 05:59:19:\nThanks, Fadi. A few more details please: could you tell me the versions and licenses of the jars? URLs to them would be great. Thanks.\nBy the way, I might have to take the member variable \"schema\" out of Project in favor of something more generic. (As it currently is, the core of Gridworks depends on the extension rdf-exporter, which is not right.)\n. From fadima...@gmail.com on July 06, 2010 10:20:09:\nHi,\nHere is a list of the Jars:\n1. Jena (2.6.0) http://jena.sourceforge.net/license.html\n2. ARQ  (2.7.0) http://jena.sourceforge.net/ARQ/license.html\n3. iri  (0.6) http://jena.sourceforge.net/iri/index.html http://jena.sourceforge.net/iri/license.html\n4. lucene (3.0.1) http://www.apache.org/dyn/closer.cgi/lucene/java/  Apache License, Version 2.0. (http://www.apache.org/licenses/LICENSE-2.0)\n5. xercesImpl (2.7.1) http://xerces.apache.org/\nThanks.\n. From dfhu...@gmail.com on July 06, 2010 22:34:48:\nThanks, Fadi. I've checked it in as of r1077. The core code base is not depending on the extension. The extension registers various things on init in its controller.js. I'll clean up these registration interfaces later.\nCould you check out the latest code and test it? You might want to have a completely fresh checkout.\n. From iainsproat on July 07, 2010 15:28:15:\nr1080 doesn't build as rdf-exporter is not part of the extensions build.xml file, and the classpath is not referring to version 26 of butterfly-trunk.\nAttached is a patch for adding rdf-exporter to the extensions build.xml and referring to the correct butterfly library.  However this patch doesn't build either due to java compilation errors -> complains it can't find the class javax.servlet.ServletException to import.\nDavid, could you have a look? I think it's a build file thing. (hoping it's not another ghost in my build system)\n. From dfhu...@gmail.com on July 07, 2010 20:36:06:\nbuild file fixed by r1082.\n. From fadima...@gmail.com on August 02, 2010 19:06:11:\nWhen trying to use the history entry for \"Save RDF Schema Skeleton\" of a saved project I got a ClassNotFoundException. The class loader is unable to load com.metaweb.gridworks.rdf.operations.SaveRdfSchemaOperation$RdfSchemaChange\nThis just happen after changing the RDF skeleton and restarting the server so classes need to be loaded from the history serialization.\nI tried to dig into the problem a little and it looks that this class is not part of the core (it is one of the extension's classes) thus it is not on the classpath (not included by $GRIDWORKS_CLASSES_DIR$ in gridworks shell file).\nCopying the classes and libs of the RDF-export extension to folders under WEB-INF solves the problem (but may be not a desired thing to have the extension mixed with the core)\nthanks\n. From stefa...@google.com on August 02, 2010 19:15:03:\nWe're aware of the issues with the extension mechanism. I'll be working on it this week. Sorry about the delay but the Metaweb->Google migration is getting in the way of doing actual work.\n. From stefa...@google.com on August 31, 2010 18:05:17:\nThis should be fixed now, re-open if you still see problems.\n. From dfhu...@google.com on September 28, 2010 03:18:26:\nFixed by r1367.\n. From stefa...@google.com on August 31, 2010 18:19:24:\nHmm, this is really weird: the request sent when you click \"apply and close\" is a POST request (precisely for this reason) and it should never get truncated... and even if it was the case, the project parameter is sent URLencoded (and not in the POST payload).\nUnfortunately, I'm unable to repro... can you give me your data to try it out?\n. From olivier....@gmail.com on September 01, 2010 00:39:39:\nI'll send you a data file by the end of the week at your email address.\nThe data is not very confidential but it's licensed from a big scary company so I won't attach it to this comment.\nBtw, we use firefox.\n. From stefa...@google.com on September 01, 2010 01:05:18:\nThanks, that works.\n. From stefa...@google.com on September 16, 2010 21:43:37:\nOlivier, did you send me the file? I haven't received it yet.\n. From stefa...@google.com on September 28, 2010 19:23:42:\nI'm closing this since I can't repro\n. From dfhu...@google.com on August 06, 2010 06:15:55:\nFixed by r1138\n. From stefa...@google.com on August 31, 2010 18:28:27:\nI can't reproduce the problem in trunk, everything works as expected. You'll get the fix as soon as we release a new version.\n. From thadguidry on August 31, 2010 20:18:44:\nVerified fixed on previously broken dataset export tests - Thad\n(issue #116 is duplicate)\n. From thadguidry on August 31, 2010 20:18:44:\nVerified fixed on previously broken dataset export tests - Thad\n(issue #116 is duplicate)\n. From tfmorris on September 16, 2010 20:38:01:\nI receive feedback on the email list that the proposed patch isn't acceptable.\nAttached is a second, more minimalist attempt.  If this needs to be modified in any way, please let me know.  This is a dependency for the Google Spreadsheet extension.\nNote that this makes an incompatible change the the UrlImporter interface, but no version with this interface has been released yet and it's a relatively new interface.\nConversely, I did deprecate and keep around the older method CreateProjectCommand.internalImport() which can be deleted if there's not a strong compatibility requirement.\n. From dfhu...@gmail.com on September 17, 2010 01:00:56:\nFixed by r1271.\n. From stefa...@google.com on August 16, 2010 15:59:42:\nAh, good catch Danny. This is a side effect of Java's date parser that actually uses zero-based month numbers, but I agree that's unfortunate. I'll fix that.\n. From stefa...@google.com on August 16, 2010 16:10:16:\nFixed in r1161.\n. From stefa...@google.com on August 31, 2010 19:33:08:\nFixed in trunk\n. From thadguidry on August 31, 2010 20:19:48:\nVerified fixed on previously broken dataset exports - Thad\n(Duplicate of issue #113)\n. From thadguidry on August 31, 2010 20:19:48:\nVerified fixed on previously broken dataset exports - Thad\n(Duplicate of issue #113)\n. From dfhu...@google.com on September 28, 2010 03:21:03:\nRob, which reconciliation service did you use? It could be the service's own problem.\n. From thadguidry on October 05, 2011 22:24:04:\nI think this is fixed now, since we found an issue with Standard Reconciliation Services being overwritten, if I recall, or something like that.  David ?\n. From thadguidry on August 24, 2010 17:46:27:\nGOOFED.  My build was pointed to another folder accidentally.  Please update to NOFIX.\n. From tfmorris on August 31, 2010 21:23:50:\nWhat type of file (and what's the file extension)?  \nIf the file isn't private, it may be useful to help reproduce the problem.\n. From stefa...@google.com on August 31, 2010 21:54:02:\nHmm, this is weird. I just tried on r1246 and it worked fine.\nWhat are you going to build and start gridworks?\nThe best way is to:\n\nsvn update\n./gridworks distclean\n./gridworks build\n./gridworks\n. From rob.02...@gmail.com on September 01, 2010 09:48:52:\nI can no longer reproduce with r1257. I emptied my browser cache as well. Maybe it was a problem from an earlier build, or a non clean build.\n\nYou can close this issue.\n. From tfmorris on January 26, 2012 18:42:41:\nAnother solution to this problem would be to make the operation restartable/continuable so that Refine keeps track of which cells have been successfully fetched.\nThis wouldn't take care of the use case where you wanted to update existing values, but it would take care of the error case.\n. From dfhu...@google.com on September 28, 2010 03:36:30:\nFixed by r1369.\n. From dfhu...@google.com on September 28, 2010 03:45:17:\nFixed by r1370\n. From dfhu...@google.com on September 28, 2010 03:47:45:\nI think some other changes must have fixed this. I can't repro this bug.\n. From nsincag...@gmail.com on September 06, 2010 17:37:02:\nI was able to get more information on this \"Unknown error\". When I run Gridworks on Windows XP there is a command window that displays the error. Here is the error message displayed in the command line window:\njava.io.IOException: Server returned HTTP response code: 503 for URL: http://api\n.freebase.com/api/service/search?indent=1&queries=%7B%22q0%3Asearch%22%3A%7B%22q\nuery%22%3A%22ALBUQUERQUE+++++++++++++++++++++++++++++%22%2C%22limit%22%3A3%2C%22\ntype%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_ex\nclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C\n%22stemmed%22%3A1%7D%2C%22q1%3Asearch%22%3A%7B%22query%22%3A%22LIVE+AT+PIEDMONT+\nPAR++++++++++++++++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%\n22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimag\ne%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q2%3\nAsearch%22%3A%7B%22query%22%3A%2215+PEGADITAS+DEL+MARIACHI+VARGAS++++++++%22%2C%\n22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22sh\nould%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A\n%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q3%3Asearch%22%3A%7B%22query%22%3A\n%22YO+SOY+EL+SON+CUBANO++++++++++++++++++++%22%2C%22limit%22%3A3%2C%22type%22%3A\n%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3\nA%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed\n%22%3A1%7D%2C%22q4%3Asearch%22%3A%7B%22query%22%3A%2220+%2F+4+MERENGUE+VOL.+2+++\n+++++++++++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22\ntype_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%\n22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q5%3Asearch%\n22%3A%7B%22query%22%3A%22Jazz+In+Film++++++++++++++++++++++++++++%22%2C%22limit%\n22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%\n2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffr\neebase%22%2C%22stemmed%22%3A1%7D%2C%22q6%3Asearch%22%3A%7B%22query%22%3A%2240+BA\nCHATAS+PODEROSA++++++++++++++++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmu\nsic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fc\nommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%\n7D%2C%22q7%3Asearch%22%3A%7B%22query%22%3A%22LIVE+IN+EUROPE%28EDITE+++++++++++++\n+++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_str\nict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain\nexclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q8%3Asearch%22%3A%7B\n%22query%22%3A%22THE+BOY+IS+MINE+++++++++++++++++++++++++%22%2C%22limit%22%3A3%2\nC%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22typ\ne_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%2\n2%2C%22stemmed%22%3A1%7D%2C%22q9%3Asearch%22%3A%7B%22query%22%3A%22DAMN+RIGHT%2C\n+I%27VE+GOT+THE+BLUES++++++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%\n2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommo\nn%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%7\nD\n        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(Unknown So\nurce)\n        at com.metaweb.gridworks.model.recon.HeuristicReconConfig.batchReconUsin\ngRelevance(HeuristicReconConfig.java:238)\n        at com.metaweb.gridworks.model.recon.HeuristicReconConfig.batchRecon(Heu\nristicReconConfig.java:198)\n        at com.metaweb.gridworks.operations.recon.ReconOperation$ReconProcess.ru\nn(ReconOperation.java:243)\n        at java.lang.Thread.run(Unknown Source)\nException in thread \"Thread-15\" java.lang.IndexOutOfBoundsException: Index: 0, S\nize: 0\n        at java.util.ArrayList.RangeCheck(Unknown Source)\n        at java.util.ArrayList.get(Unknown Source)\n        at com.metaweb.gridworks.operations.recon.ReconOperation$ReconProcess.ru\nn(ReconOperation.java:245)\n        at java.lang.Thread.run(Unknown Source)\n. _From tfmorris on September 06, 2010 18:31:40:\nAre you still getting the error?  When I try that query interactively now, it returns successfully.  Perhaps you hit a transient error on the server?  (It still should provide better error reporting though, so you don't have to resort to looking at the console)\n. From nsincag...@gmail.com on September 06, 2010 19:54:14:\nYes, I am still getting the \"Unknown error\" message when my reconcile process quits. However, I am not always seeing the errors dump to the console window. The last two times I tried to reconcile against Freebase, no error information was displayed in the console. Maybe the errors in the console are unrelated to the \"Unknown error\" warning I get in the lefthand side of my browser.\nIs there any thing I can do to help track down this issue?\n. From tfmorris on September 06, 2010 20:20:45:\nActually I'm seeing timeouts in the web client for simple things like the autocomplete dialog (which also uses the reconciliation/search services), so it could be a general site issue.  It's possible someone's hammering the server hard creating a high load or that there's a server issue, but in either case it's unlikely to get looked at until people are back from holiday tomorrow.\n. From nsincag...@gmail.com on September 06, 2010 21:13:04:\nMy reconcile quit and I this error was in the console window. It seems like this time it was correlated to the failure. \njava.io.IOException: Server returned HTTP response code: 503 for URL: http://api.freebase.com/api/service/search?indent=1&queries=%7B%22q0%3Asearch%22%3A%7B%22query%22%3A%22LOOSE+MY+BREATH+++++++++++++++++++++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q1%3Asearch%22%3A%7B%22query%22%3A%22STARDUST+%2830TH+ANNIVERSARY+LEGACY+EDITIO%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q2%3Asearch%22%3A%7B%22query%22%3A%22Fiesta+Songs%21+++++++++++++++++++++++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q3%3Asearch%22%3A%7B%22query%22%3A%22I+DO%21+I+DO%21+++++++++++++++++++++++++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q4%3Asearch%22%3A%7B%22query%22%3A%22You+In+Flames+++++++++++++++++++++++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q5%3Asearch%22%3A%7B%22query%22%3A%22Brahms++++++++++++++++++++++++++++++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q6%3Asearch%22%3A%7B%22query%22%3A%22MUSIC+FROM+%26+INSPIRED+BY+THE+MOTION+PICT%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q7%3Asearch%22%3A%7B%22query%22%3A%22ALL+THAT+IS+WITHIN+ME+++++++++++++++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q8%3Asearch%22%3A%7B%22query%22%3A%22HOURGLASS+-+VOLUME+II+-+THE+EPIC+YEARS+%28%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q9%3Asearch%22%3A%7B%22query%22%3A%22Sometimes+I+Dream%3A+Live+In+Concert++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%7D                                                                                 at sun.net.www.protocol.http.HttpURLConnection.getInputStream(Unknown Source)                                                                                   at com.metaweb.gridworks.model.recon.HeuristicReconConfig.batchReconUsingRelevance(HeuristicReconConfig.java:238)                                               at com.metaweb.gridworks.model.recon.HeuristicReconConfig.batchRecon(HeuristicReconConfig.java:198)                                                             at com.metaweb.gridworks.operations.recon.ReconOperation$ReconProcess.run(ReconOperation.java:243)                                                              at java.lang.Thread.run(Unknown Source)                                 Exception in thread \"Thread-16\" java.lang.IndexOutOfBoundsException: Index: 0, Size: 0                                                                                  at java.util.ArrayList.RangeCheck(Unknown Source)                               at java.util.ArrayList.get(Unknown Source)                                      at com.metaweb.gridworks.operations.recon.ReconOperation$ReconProcess.run(ReconOperation.java:245)                                                              at java.lang.Thread.run(Unknown Source)\n. From stefa...@google.com on September 28, 2010 19:30:19:\nI think this was an issue with the Freebase Relevance service, let's reopen if it shows up again\n. From nsincag...@gmail.com on September 06, 2010 17:37:02:\nI was able to get more information on this \"Unknown error\". When I run Gridworks on Windows XP there is a command window that displays the error. Here is the error message displayed in the command line window:\njava.io.IOException: Server returned HTTP response code: 503 for URL: http://api\n.freebase.com/api/service/search?indent=1&queries=%7B%22q0%3Asearch%22%3A%7B%22q\nuery%22%3A%22ALBUQUERQUE+++++++++++++++++++++++++++++%22%2C%22limit%22%3A3%2C%22\ntype%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_ex\nclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C\n%22stemmed%22%3A1%7D%2C%22q1%3Asearch%22%3A%7B%22query%22%3A%22LIVE+AT+PIEDMONT+\nPAR++++++++++++++++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%\n22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimag\ne%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q2%3\nAsearch%22%3A%7B%22query%22%3A%2215+PEGADITAS+DEL+MARIACHI+VARGAS++++++++%22%2C%\n22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22sh\nould%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A\n%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q3%3Asearch%22%3A%7B%22query%22%3A\n%22YO+SOY+EL+SON+CUBANO++++++++++++++++++++%22%2C%22limit%22%3A3%2C%22type%22%3A\n%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3\nA%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed\n%22%3A1%7D%2C%22q4%3Asearch%22%3A%7B%22query%22%3A%2220+%2F+4+MERENGUE+VOL.+2+++\n+++++++++++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22\ntype_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%\n22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q5%3Asearch%\n22%3A%7B%22query%22%3A%22Jazz+In+Film++++++++++++++++++++++++++++%22%2C%22limit%\n22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%\n2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffr\neebase%22%2C%22stemmed%22%3A1%7D%2C%22q6%3Asearch%22%3A%7B%22query%22%3A%2240+BA\nCHATAS+PODEROSA++++++++++++++++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmu\nsic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fc\nommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%\n7D%2C%22q7%3Asearch%22%3A%7B%22query%22%3A%22LIVE+IN+EUROPE%28EDITE+++++++++++++\n+++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_str\nict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain\nexclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q8%3Asearch%22%3A%7B\n%22query%22%3A%22THE+BOY+IS+MINE+++++++++++++++++++++++++%22%2C%22limit%22%3A3%2\nC%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22typ\ne_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%2\n2%2C%22stemmed%22%3A1%7D%2C%22q9%3Asearch%22%3A%7B%22query%22%3A%22DAMN+RIGHT%2C\n+I%27VE+GOT+THE+BLUES++++++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%\n2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommo\nn%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%7\nD\n        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(Unknown So\nurce)\n        at com.metaweb.gridworks.model.recon.HeuristicReconConfig.batchReconUsin\ngRelevance(HeuristicReconConfig.java:238)\n        at com.metaweb.gridworks.model.recon.HeuristicReconConfig.batchRecon(Heu\nristicReconConfig.java:198)\n        at com.metaweb.gridworks.operations.recon.ReconOperation$ReconProcess.ru\nn(ReconOperation.java:243)\n        at java.lang.Thread.run(Unknown Source)\nException in thread \"Thread-15\" java.lang.IndexOutOfBoundsException: Index: 0, S\nize: 0\n        at java.util.ArrayList.RangeCheck(Unknown Source)\n        at java.util.ArrayList.get(Unknown Source)\n        at com.metaweb.gridworks.operations.recon.ReconOperation$ReconProcess.ru\nn(ReconOperation.java:245)\n        at java.lang.Thread.run(Unknown Source)\n. _From nsincag...@gmail.com on September 06, 2010 21:13:04:\nMy reconcile quit and I this error was in the console window. It seems like this time it was correlated to the failure. \njava.io.IOException: Server returned HTTP response code: 503 for URL: http://api.freebase.com/api/service/search?indent=1&queries=%7B%22q0%3Asearch%22%3A%7B%22query%22%3A%22LOOSE+MY+BREATH+++++++++++++++++++++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q1%3Asearch%22%3A%7B%22query%22%3A%22STARDUST+%2830TH+ANNIVERSARY+LEGACY+EDITIO%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q2%3Asearch%22%3A%7B%22query%22%3A%22Fiesta+Songs%21+++++++++++++++++++++++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q3%3Asearch%22%3A%7B%22query%22%3A%22I+DO%21+I+DO%21+++++++++++++++++++++++++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q4%3Asearch%22%3A%7B%22query%22%3A%22You+In+Flames+++++++++++++++++++++++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q5%3Asearch%22%3A%7B%22query%22%3A%22Brahms++++++++++++++++++++++++++++++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q6%3Asearch%22%3A%7B%22query%22%3A%22MUSIC+FROM+%26+INSPIRED+BY+THE+MOTION+PICT%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q7%3Asearch%22%3A%7B%22query%22%3A%22ALL+THAT+IS+WITHIN+ME+++++++++++++++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q8%3Asearch%22%3A%7B%22query%22%3A%22HOURGLASS+-+VOLUME+II+-+THE+EPIC+YEARS+%28%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%2C%22q9%3Asearch%22%3A%7B%22query%22%3A%22Sometimes+I+Dream%3A+Live+In+Concert++++++%22%2C%22limit%22%3A3%2C%22type%22%3A%22%2Fmusic%2Falbum%22%2C%22type_strict%22%3A%22should%22%2C%22type_exclude%22%3A%22%2Fcommon%2Fimage%22%2C%22domain_exclude%22%3A%22%2Ffreebase%22%2C%22stemmed%22%3A1%7D%7D                                                                                 at sun.net.www.protocol.http.HttpURLConnection.getInputStream(Unknown Source)                                                                                   at com.metaweb.gridworks.model.recon.HeuristicReconConfig.batchReconUsingRelevance(HeuristicReconConfig.java:238)                                               at com.metaweb.gridworks.model.recon.HeuristicReconConfig.batchRecon(HeuristicReconConfig.java:198)                                                             at com.metaweb.gridworks.operations.recon.ReconOperation$ReconProcess.run(ReconOperation.java:243)                                                              at java.lang.Thread.run(Unknown Source)                                 Exception in thread \"Thread-16\" java.lang.IndexOutOfBoundsException: Index: 0, Size: 0                                                                                  at java.util.ArrayList.RangeCheck(Unknown Source)                               at java.util.ArrayList.get(Unknown Source)                                      at com.metaweb.gridworks.operations.recon.ReconOperation$ReconProcess.run(ReconOperation.java:245)                                                              at java.lang.Thread.run(Unknown Source)\n. From stefa...@google.com on September 16, 2010 23:08:07:\nFixed in r1269.\n. From dfhu...@google.com on September 28, 2010 04:23:28:\nFixed by r1373, at least for (value+0) and (value*1). I can't repro the case of just \"\" + value. Could you verify?\n. From dfhu...@google.com on September 10, 2010 05:05:49:\nFixed by r1261.\n. From stefa...@google.com on September 16, 2010 21:37:56:\nFixed in r1265\n. From stefa...@google.com on September 16, 2010 16:35:49:\nHow about kist changing #!/bin/sh at the top to #!/bin/bash? I've tried pretty hard to avoid using any bash-isms in those shell scripts, but it's hard to know on mac since /bin/sh symlinks to /bin/bash.\nthought?\n. From stefa...@google.com on September 16, 2010 21:52:52:\nAh, found this https://wiki.ubuntu.com/DashAsBinSh which helps a lot. I'll fix all the bashisms\n. From stefa...@google.com on September 16, 2010 22:18:56:\nI think I fixed them all. Open this again if not the case.\n. From dfhu...@google.com on September 28, 2010 04:28:14:\nTom, I think that's the last modified date, not the created date. Are you still seeing this bug?\n. From tfmorris on September 28, 2010 04:42:19:\nYes, both command line and graphic clocks on my system currently say 12:40, but the last modified date displayed on the Refine home page for the project I was working on is 4:21 am, several hours in the future.\n. From tfmorris on December 12, 2010 21:20:11:\nSo apparently Java looks for its timezone data in the one place that didn't get set up on my Linux system (Ubuntu 10.04 LTS) and it's the only thing that looks there.\nThis can be fixed using one of the following:\n1. System -> Administration -> Time & Date\n2. Adding this line to your .profile:\nTZ='America/New_York'; export TZ\n3. Setting the systemwide timezone by hand:\nsudo ln -sf /usr/share/zoneinfo/America/New_York /etc/localtime\n4. Using the following Java property when starting Refine:\n-Duser.timezone=America/New_York\n. From iainsproat on November 17, 2010 10:37:34:\nIf issue #228 gets fixed, then Refine should be able to read multiple files from an archive.\n. From dfhu...@gmail.com on September 11, 2011 05:11:07:\nNew importer UI in trunk/ allows for archive files as well as selecting files within an archive.\n. From iainsproat on November 17, 2010 10:37:34:\nIf issue #228 gets fixed, then Refine should be able to read multiple files from an archive.\n. From dfhu...@google.com on September 28, 2010 04:35:44:\nTim, this is actually by design. To save facets, use the \"permalink\" in the header of the page.\n. From mcnamara.tim@gmail.com on September 28, 2010 05:25:30:\nAh okay, feel free to mark the bug invalid.\n. From tfmorris on September 25, 2011 20:25:17:\nThis still exists in v2.1.  When columns are renamed, the dependent facets should be updated.  When they are deleted, the dependent facets should be removed.\n. From iainsproat on September 22, 2010 09:48:59:\nThe browser cache had stored an old page with incorrect links.  Shift + Reload cleared the cache, allowing the current page to be displayed solving the issue.\n. From wbl...@myspace-inc.com on September 21, 2010 18:30:35:\nThis was SVN revision 1261, last change date 2010-09-09.\n. From dfhu...@google.com on September 28, 2010 04:43:06:\nFixed by r1374.\n. From stefano.mazzocchi@gmail.com on September 29, 2010 01:51:41:\nThis issue was closed by revision r1406.\n. From dfhu...@google.com on September 28, 2010 04:46:40:\nIain, I can't import the project attached. It appears to be corrupted. Could you export just the data?\n. From iainsproat on September 28, 2010 08:20:20:\nAttached is the original data file I imported.\nClinicalTrials.gov only outputs in individual xml files (see issue #131), so I had an external script stitch them into a single xml file.\n. From dfhu...@google.com on September 28, 2010 19:18:38:\nI can repro the bug, although I haven't figured out how to fix it. Exporting the data as HTML table works--all the records seem to be there. But inside Refine, only the first 2 records are shown. I guess the JSON stream got cut off? Which is weird because that would cause a syntax error.\n. From tfmorris on November 27, 2010 22:14:19:\nAttached is a smaller version of the file which should make debugging a little easier.\nPart of the problem was the multi-line text elements were confusing the parser, but I've just committed a change that should fix that piece.  It's still splitting a single record into multiple rows when it shouldn't be.  I'll take a look and see why.\nAs an aside, I think the reason the browser goes berserk is that it's trying to deal with a large number of null cells.  Fixing the project import will probably solve this, but someone who's got stronger Javascript fu than I might want to take a look at whether there's another problem lurking there.\n. From tfmorris on October 14, 2011 18:49:13:\nI think I've figured out what the problem is here.  The column groups for this XML fragment are getting computed incorrectly:\n\n\nNational Center for Research Resources (NCRR)\nNIH\n\n\nHRSA/Maternal and Child Health Bureau\nU.S. Fed\n\n\nRefine is currently computing three column groups from this: lead_sponsor (2 columns), collaborator (2 columns), and sponsor (all 4 columns).  This last group is triggering unnecessary row dependencies when there is no collaborator element.\nI'm tempted to say that column groups which consist of nothing but other groups, without any individual ungrouped columns of their own should be eliminated, but it's a fairly critical piece of code, so I want to look at it a little more closely.\np.s. I suspect the reason for the sluggish browser performance is that it was trying to deal with the entire file at once since the monster second record effectively disabled the paging.\n. From tfmorris on October 27, 2011 00:04:03:\nReversing course, I now think that perhaps we should be computing row->record dependencies directly since we have that information from the parse.  We know what the path to the top level element is and, by definition, all rows that we create during the parse of its children are part of the same record.\nVisually the column groups look like this:\nCol #         2222333344445555\nSponsor group SSSSSSSSSSSSSSSS\nLead/Collab   CCCCccccLLLLllll\nThe three groups are group S, group Cc, and group Ll.Note that the columns have been reordered with the Collaborator columns before the Lead columns.  Group S is the problematic one.  Since the Collaborator sponsors are optional, that means that the \"key\" column #  2 can be blank for a top level record, causing it to get merged with the previous record (the algorithm searches back for a row with a non-blank \"key\" cell if any of the cells in the column group are non-blank).\nGroup S is a real column group (consisting solely of other column groups), but since we don't appear to use column groups for anything, I'm not sure what value it has.\nI think there are two options here:\n1. Eliminate column groups which consist solely of other column groups from the dependency analysis\n2. Compute row dependencies using a different method than column groups (eg use the tree structure directly from the parse).\nOpinions?\n. From tfmorris on October 28, 2011 19:56:34:\nThis is fixed, at least well enough for this example, with r2347.  The cell data counts weren't getting updated before sorting the column groups, causing them all to be zero which meant data rich columns weren't getting place where Refine needed them to be for the key column.\nI think there's still an underlying issue where the current counting strategy can get fooled into choosing the wrong column.  For example if Column A is mandatory but never has more than a single value and Column B values are optional, but high frequency (e.g. 50% of the records, but every record has 3 values, giving it 1.5x the number of cells as column A), then column B will get chosen in preference to column A.\nThere's also the case where no single column in a column group always has a value (e.g. a column group where either column A OR column B is populated for any given record).\n. From tfmorris on November 06, 2011 21:14:59:\nReopening.  The theoretical issue that I suspected has been confirmed in the wild by a new XML file received from a commenter on issue #393, so we're going to need a different approach to dealing with column groups.\n. From tfmorris on November 06, 2011 21:26:19:\nissue #393 has been merged into this issue.\n. From tfmorris on December 12, 2011 19:56:00:\nBump all unfinished 2.5 Milestone tags to next release\n. From emilytgriffiths on April 13, 2012 14:56:10:\nHello all, \nOutside user, working to clean-up someone else's messy server.  After importing three .csv files that total 5019 records in length, I delete the the 'file' column as it is not needed.  The record count then abates to 1333.  I do not have any facets selected.  When attempting click around and discover the source of the problem, Firefox tells me that I have an unresponsive script:\nhttp://127.0.0.1:3333/project-bundle.js:7973\nAnd asks me if I wish to continue or not.  If I continue, it will eventually ask me again.  If I stop the script, it will crash the tab I am running Refine in.  \nI converted my original .csv files into .tsv format, and reopened them in Refine.  This solved my issue.  \nQuirky little bug with a simple workaround for those of you out there in Userland that are not quite up to speed in programing languages. \n. From tfmorris on April 13, 2012 16:58:55:\nSince you're working with CSV, not XML, it's clearly not related to this bug.  Please feel free to post on Google Refine mailing list if you'd like assistance.  It sounds like you probably had a mostly blank initial column in one or more of your CSVs.  You can either process it in \"row\" mode (instead of \"record\" mode) or shuffle the columns around so it's not an issue.\n. From libo...@gmail.com on June 11, 2012 11:24:57:\nCould someone provide a pointer on how does the record detection exactly work? Is it import-time, or runtime? Where can I find the code responsible for this? \nWhen I provide a properly structured xml, it merges some of the records into one, but I can't see any features that distinguish those records from the others... I'd like to look at the source of the problem, but currently have no idea where to look.\nI'm attaching the offending xml.\nThanks\n. From tfmorris on June 12, 2012 20:15:51:\nClicking the rev above that I used to repair part of the problem (r2347) will get you in the right ballpark.  The XML and JSON importers are in the \"tree-shaped\" importer family.\nImportColumnGroup is the class which manages the column groups that are used to determine dependent rows/records.  Anything that references it is probably involved.  TreeImportUtilities and XmlImportUtilities have methods which are used with this.\nHope that helps get you started.  Let us know on the dev list if you have any questions.\n. From iainsproat on September 28, 2010 08:20:20:\nAttached is the original data file I imported.\nClinicalTrials.gov only outputs in individual xml files (see issue #131), so I had an external script stitch them into a single xml file.\n. From tfmorris on October 27, 2011 00:04:03:\nReversing course, I now think that perhaps we should be computing row->record dependencies directly since we have that information from the parse.  We know what the path to the top level element is and, by definition, all rows that we create during the parse of its children are part of the same record.\nVisually the column groups look like this:\nCol #         2222333344445555\nSponsor group SSSSSSSSSSSSSSSS\nLead/Collab   CCCCccccLLLLllll\nThe three groups are group S, group Cc, and group Ll.Note that the columns have been reordered with the Collaborator columns before the Lead columns.  Group S is the problematic one.  Since the Collaborator sponsors are optional, that means that the \"key\" column #  2 can be blank for a top level record, causing it to get merged with the previous record (the algorithm searches back for a row with a non-blank \"key\" cell if any of the cells in the column group are non-blank).\nGroup S is a real column group (consisting solely of other column groups), but since we don't appear to use column groups for anything, I'm not sure what value it has.\nI think there are two options here:\n1. Eliminate column groups which consist solely of other column groups from the dependency analysis\n2. Compute row dependencies using a different method than column groups (eg use the tree structure directly from the parse).\nOpinions?\n. From tfmorris on October 28, 2011 19:56:34:\nThis is fixed, at least well enough for this example, with r2347.  The cell data counts weren't getting updated before sorting the column groups, causing them all to be zero which meant data rich columns weren't getting place where Refine needed them to be for the key column.\nI think there's still an underlying issue where the current counting strategy can get fooled into choosing the wrong column.  For example if Column A is mandatory but never has more than a single value and Column B values are optional, but high frequency (e.g. 50% of the records, but every record has 3 values, giving it 1.5x the number of cells as column A), then column B will get chosen in preference to column A.\nThere's also the case where no single column in a column group always has a value (e.g. a column group where either column A OR column B is populated for any given record).\n. From tfmorris on November 06, 2011 21:14:59:\nReopening.  The theoretical issue that I suspected has been confirmed in the wild by a new XML file received from a commenter on issue #393, so we're going to need a different approach to dealing with column groups.\n. From tfmorris on November 06, 2011 21:26:19:\nissue #393 has been merged into this issue.\n. From dfhu...@google.com on September 29, 2010 00:18:00:\nFixed by r1404\n. From thadguidry on September 26, 2010 21:49:25:\nPossibly related to issue #135\n. From dfhu...@google.com on September 28, 2010 04:55:05:\nThad, if you have some data set where you can consistently reproduce this bug, please attach it. Thanks.\n. From thadguidry on September 28, 2010 13:49:09:\nI have attached the simple project used to test that includes the UNDO GREL statement for reference.  This issue was similar to issue #135, I think.  Within the LOC's API, you can change the format=html instead of format=atom or format=json.  The issue reared it's ugly head when I had a typo in my request string using \"?terms=\" instead of \"?&terms=\" missing the ampersand character and this resulted in over 1200 pages of data that stored into each of the cells.  In other words, a VERY large JSON page stored into each single cell for the 20 or so rows I was displaying and working with.  Still, I've seen this issue before when working with the partition command on large sets of strings inside cells (the old NFDC airport data, if I recall, but I dismissed it at the time since we were driving fast on development).\n. From dfhu...@google.com on September 28, 2010 18:06:43:\nWe probably need really large json data per cell to get this to show up. We'll close this issue for now. It's unlikely that anybody will run into it.\n. From thadguidry on September 26, 2010 21:49:25:\nPossibly related to issue #135\n. From thadguidry on September 28, 2010 13:49:09:\nI have attached the simple project used to test that includes the UNDO GREL statement for reference.  This issue was similar to issue #135, I think.  Within the LOC's API, you can change the format=html instead of format=atom or format=json.  The issue reared it's ugly head when I had a typo in my request string using \"?terms=\" instead of \"?&terms=\" missing the ampersand character and this resulted in over 1200 pages of data that stored into each of the cells.  In other words, a VERY large JSON page stored into each single cell for the 20 or so rows I was displaying and working with.  Still, I've seen this issue before when working with the partition command on large sets of strings inside cells (the old NFDC airport data, if I recall, but I dismissed it at the time since we were driving fast on development).\n. From tfmorris on September 28, 2010 03:57:30:\nFixed using the Java 6 Desktop class.  If this is an unacceptable dependency, another solution will be needed.\n. From tfmorris on September 30, 2010 07:18:59:\nTested on Linux and Windows.  Can someone please test on Mac?\nNote that the Linux directory changed from ./google-refine to to ./google/refine to align it with all the others.  If you've used a version built from the SVN trunk in the last few days, you may have the old name.\n. From dfhu...@gmail.com on September 30, 2010 16:30:51:\nTested on Mac! It worked fine.\n. From tfmorris on October 13, 2010 22:42:13:\nWell, somehow I managed to miss committing the critical module, so this actually wasn't fixed until just now.  Sorry about that!\nThe caveat above for Linux users still applies, but with, unfortunately, a longer time window now.  You may need to manually rename your ~/.local/share/google-refine directory to ~/.local/share/google/refine.  This change was made to bring things in to line with naming on the other platforms.\n. From thadguidry on September 28, 2010 18:50:46:\nSince your running Refine on XP behind corporate firewall.  Try to run Refine.bat /i 127.0.0.1 \n. From galbith...@galbithink.org on September 28, 2010 19:12:23:\nThanks for the response.  Just for clarification, I'm running gridworks, not refine.  This is confusing, because I went to http://code.google.com/p/google-refine/wiki/Downloads?tm=2, downloaded and unzipped from a link labeled \"Refine-1.1-r878.zip\", and I got a bunch of files named gridworks.  So I have gridworks.exe and gridworks.bat, not refine.exe and refine.bat  Is this the problem?\nI tried to run \"gridworks.bat /i 127.0.0.1\". I got the error \"You must set JAVA_Home to point at your Java Development Kit installation.\"  As far as I know, I don't have any Java Development Kit installation.\n. From stefa...@google.com on September 28, 2010 19:35:31:\nGoogle Refine used to be called Freebase Gridworks which is why there are still inconsistencies around with the various names. Please excuse our mess while we transition it.\nAlso Google Refine needs Java installed in your machine in order to work. Do you have a Java virtual machine installed on your computer?\n. From galbith...@galbithink.org on September 28, 2010 19:47:06:\nYup, version 6, update 21 ((build 1.6.0_21-b07).  If I click on gridworks.exe, the browser window \"Freebase Gridworks\" forms and the message on the bottom status bar of Firefox is \"waiting for 165.135.222.96\"  After a few minutes this call times out and Gridworks starts. So I get a browser page \"Welcome to Gridworks\"  Then I browse for a simple csv file on my computer, type in a project name, and click on \"create project\".  This returns a Firefox error page (\"problem loading page\"), \"Unable to connect Firefox can't establish a connection to the server at 165.135.222.96.\"  I tried a whois on 165.135.222.96 and it seems to have no entry. \n. From thadguidry on September 28, 2010 21:22:26:\nHmm... I'm wondering if you have a proxy connection setup in your Firefox preferences ? Can you download and try Google Chrome, or perhaps double-check your Firefox preferences ?  Normally, Refine (Gridworks) will start up and default to 127.0.0.1 automatically.  So, simply typing in your Firefox address bar 127.0.0.1:3333 should get you to the start page.  You can override the setting with the /i parameter at the command line like> refine /i 127.0.0.1 and that should bind it to your local Win XP computer/host.  It appears that your able to at least see the start page come up after a quick wait and able to try to select your csv file, so things are working correctly behind the scenes.  What is the URL in your address bar that shows once your able to select a file, I wonder ?  Is it 127.0.0.1:3333 ?  or something else ?\n. From galbith...@galbithink.org on September 29, 2010 13:14:06:\nI switched my Firefox options/advanced/network/connection/settings from \"use system proxy settings\" to \"auto-detect proxy settings for this network\" and Gridworks ran!  Just for reference, when it was failing, I was getting URL \"http://127.0.0.1:3333/\" in the address bar.  \nThanks a lot for the help. I look forward to using Refine/Gridworks.\n. From iainsproat on September 29, 2010 13:18:17:\nAwesome - Thanks for the feedback, I'll now close this issue.\n. From tfmorris on September 29, 2010 14:52:49:\nGlad to hear it's working.  What URL are you getting now that it is working?  127.0.0.1 is the loopback address, so should (I would have thought) bypass any proxy since it's always going to be on your local system.\n. From galbith...@galbithink.org on September 30, 2010 13:15:46:\nNow that it's working, I'm getting the same URL, http://127.0.0.1:3333/\n. From iainsproat on November 15, 2010 15:59:25:\nI can't reproduce this error using the Google Refine 2.0 r1836 distribution.  I assume it's been fixed in the last couple of months.\nTom, could you verify that it has been fixed?\n. From tfmorris on January 27, 2012 21:10:01:\nThe Linux distribution is OK and Mac uses a virtual disk image, so I think it's just the Windows zip file that needs to be fixed.\n. From tfmorris on January 27, 2012 21:29:02:\nFixed in r2437.\n. From iainsproat on September 30, 2010 11:23:49:\nYou can allocate more memory to Refine by editing the Refine.ini file* in the installation directory.  The line to edit is: #REFINES_MEMORY=1024M\n*it appears you're running an older version of Refine, so the file would be named Gridworks.ini\n. From iainsproat on October 14, 2010 16:51:59:\nPossibly could be related to issue #147\n. From little...@gmail.com on November 12, 2010 02:08:00:\nI had the same error with a 100mb xls file as well.\n. From thadguidry on November 12, 2010 02:40:45:\nlittlemog - check the FAQ http://code.google.com/p/google-refine/wiki/FaqAllocateMoreMemory on how to increase memory for Google Refine.\n. From andy.war...@gmail.com on November 20, 2010 20:55:41:\nWhile allocating more memory works fine, it would be really nice if the system handled this better. Throwing an OOM exception and 500ing is a pretty bad experience.\n. From iainsproat on October 14, 2010 16:51:59:\nPossibly could be related to issue #147\n. From dfhu...@gmail.com on September 30, 2010 17:18:53:\nFixed by r1415.\n. From tfmorris on September 30, 2010 14:26:04:\nDoing this before too many importers get written would be desirable, but note that:\na) this may leave you with a project too big to open again\nb) not all parsers support streaming modes (ie you may have to pay the penalty regardless of what the interface looks like)\n. From roytenn...@gmail.com on November 11, 2010 05:36:12:\nI would like to support a different way of doing things, as the size of metadata files I work with on a regular basis include millions of rows. I realize that trying to do this on a MacBook Pro with 3GB of RAM is not ideal (to say the least), but there are still many others with less RAM at their disposal. As it stands, I have yet to successfully load a file of 7 million rows (and about a dozen fields per row, with most not having more than a couple dozen or so bytes). Could there somehow be a background batch \"pre-processing\" step that I could run to set up the data for viewing and manipulation via my web browser? If you want to try this yourself, grab the file here and feel my pain: http://www.hathitrust.org/sites/www.hathitrust.org/files/hathifiles/hathi_full_20101101.txt.gz\n. From tfmorris on November 11, 2010 16:26:02:\nThe current architecture requires all data to be in memory for processing.  Changing that would require a major reworking of things and require GRefine to be backed by a \"real\" database to do all the processing.\nFor the forseable future, you should assume that 1.x N memory is required for an N-size database, where the goal is to keep x small (say 5-20%).\n. From thadguidry on November 11, 2010 16:36:11:\nAlso, in addition to what Tom says, beyond 4 GB Ram requires 64 bit Java to be installed.  I myself have the luxury of working with Refine using 9 GB Ram on Windows 7 using 64 bit Java and have successfully performed 1 million row transforms with 4 fields after allocating enough Ram to Refine. See FAQ http://code.google.com/p/google-refine/wiki/FaqAllocateMoreMemory\n. From roytenn...@gmail.com on November 11, 2010 17:32:49:\nThanks a lot, I guess I'll have to find a machine that is up to the task. I eventually got the \"java heap space\" error. I really appreciate the speedy and informative replies.\n. From tfmorris on November 11, 2010 17:50:26:\nYes, that's one of the unfortunate aspects of Java memory management.  In cases where it's going to fail eventually anyway, the garbage collector tries harder and harder, slowly grinding to a halt before eventually failing altogether.\nI'll take a look and see if we can add a warning for the situation where it looks like you are grossly underconfigured for the task you are attempting\n. From roytenn...@gmail.com on November 11, 2010 20:23:04:\nYeah, some sort of warning would have saved me a lot of time waiting around for something to happen, and my disk thrashing. ;-)\n. From mcnamara.tim@gmail.com on November 12, 2010 03:21:34:\nWhat about something like linking two or more projects together? E.g. automatically run operations on linked project(s) in serial by clicking a button. The user creates a set of operations in a subset of the uploaded file.*\nOnce this reaches the desired level of clea, the user then\nThis is the pipeline that I can think is manageable:\n- user uploads file\n- file checked for size\n- if the file is likely to hit the memory limit, then offer to partition data\n- when partitioning\n  - the system saves manageable chunks, 50mb (?)\n  - metadata are stored about the relationship between the chunks\n  - load first chunk into Refine\n  - user refines data\n  - user clicks \"Process linked projects\"\n    - operations from first project are replicated in serial to the others\n- happy user, happy data\n- I think that it would be possible to partition text files fairly easily, however I have my doubts about binary ones. \n. From tfmorris on October 21, 2011 16:26:27:\nI've created issue #467 to track the suggestion of a JVM heap monitor as part of the progress indication.  I've implemented an rudimentary version of this as a start.\n. From tfmorris on November 11, 2010 16:26:02:\nThe current architecture requires all data to be in memory for processing.  Changing that would require a major reworking of things and require GRefine to be backed by a \"real\" database to do all the processing.\nFor the forseable future, you should assume that 1.x N memory is required for an N-size database, where the goal is to keep x small (say 5-20%).\n. From tfmorris on October 21, 2011 16:26:27:\nI've created issue #467 to track the suggestion of a JVM heap monitor as part of the progress indication.  I've implemented an rudimentary version of this as a start.\n. @junwei-wang The conversation here pretty much summarizes things.  Is there a specific aspect that you don't understand?\n. From thadguidry on October 07, 2010 01:16:15:\nAdditional info that might be helpful.\nWhen performing the steps, I did have a facet showing only starred=true for 1 row.  The refine process shows (repeating cycle, when alternatively clicking rows/records buttons):\n20:11:33.956 [                   refine] GET /command/core/get-history (8598ms)\n20:11:33.968 [                   refine] POST /command/core/get-rows (12ms)\n20:11:34.007 [                   refine] POST /command/core/compute-facets (39ms\n. From thadguidry on October 10, 2011 14:24:17:\nFixed in Trunk (2.5)\n. From thadguidry on October 14, 2010 13:27:39:\nDuplicate of issue #38 (Same needs as 38, as well)\n. From thadguidry on October 14, 2010 13:27:39:\nDuplicate of issue #38 (Same needs as 38, as well)\n. From tfmorris on July 30, 2012 20:08:44:\nIf/when we do this, it should probably be done in such a way that the bulk of the code can be reused to do the same thing for XML parsing.\n. From thadguidry on October 12, 2010 13:46:36:\nCurious, what does Acre use itself ?\n. From dfhu...@gmail.com on October 12, 2010 17:38:01:\nThis is doable. I've done all the \"suggest\" stuff in query editor using code mirror. But let's wait for James to go over the whole UI for post 2.0 first. We'll need to think about how to offer more inline help for GREL in general.\n. From dfhu...@gmail.com on October 12, 2010 17:40:00:\nWhich version are you running? 1.1? Is it an RDF/XML file? Have you had success importing other (smaller) RDF/XML files before? Is this file publicly available so we can try it ourselves?\n. From amrapali...@gmail.com on October 12, 2010 18:47:51:\nYes, I am using the Refine-1.1-r878.dmg version for MAC.\nIt is an RDF/XML file. \nI have not tried importing a smaller file earlier.\nThis file is not publicly available though..is it because of the size?\n. From dfhu...@google.com on October 13, 2010 04:52:55:\nThere was certainly a bug in the XML importer in version 1.1. Could you wait until version 2.0 (due in a couple of weeks), or otherwise check out the trunk/ version? Instructions: http://code.google.com/p/google-refine/wiki/GetDevelopmentVersion\n. From amrapali...@gmail.com on October 13, 2010 06:59:36:\nOk, Thanks.\n. From amrapali...@gmail.com on November 12, 2010 15:10:02:\nHi,\nI updated to the new version and tried upload the RDF/XML file but it still did not load the data.\nPlease could you look into it.\nThanks.\n. From thadguidry on November 12, 2010 15:27:41:\nI think the size of the file is indeed leading to the problem with the blank screen.  Tom Morris is trying to work on a notification to warn users that you might not have enough memory allocated or that the file size might be to large to handle with your current configuration.\nAmrapali, I would like to see if you could allocate more memory to Refine with the instructions here in our FAQ http://code.google.com/p/google-refine/wiki/FaqAllocateMoreMemory and then let us know if it changes anything ?  Also what is your current machine configuration and RAM capability ?  (David, to bad we can't capture that stuff automatically when users open an issue!)\n. From amrapali...@gmail.com on November 13, 2010 15:34:36:\nHi, I followed the instructions for allocating more memory to Refine but my VM Options was already set to -Xms256M -Xmx1024M -Drefine.version=r1836, which is similar to what is expected to be changed. I am using a MAC with Intel Core 2 Duo (64 bit) with 2 GB RAM.\nThanks.\n. From matteo.b...@gmail.com on July 26, 2012 15:19:37:\nHi, \n I'm using the 2.5 version of refine and I'm having the same problem also with file of 4 Mb (rdf/XML) or 400kb (RDF/N3).\nIn both cases I get a empty preview and nothing seems to be imported.\nDid you now know what is the couse?\nThanks.\n. From amrapali...@gmail.com on August 01, 2012 14:50:53:\nHi,\nThe size of the file is indeed the cause of the problem of getting an empty preview. \nMaybe you need to split the file and then upload in batches.\nHope that helps !\n. From matteo.b...@gmail.com on August 01, 2012 15:09:28:\nI don't think it could be only a metter of file size because (as I said in the last post) I also tried with a 400K n3 file and the result seems to be the same.\nAny other idea?\n. From tfmorris on August 01, 2012 16:23:17:\nMatteo - please use the mailing list for question.  If you're pretty sure it's a bug, please create a new issue and, if possible, attach your file.  A 400 KB N3 file, when compressed, should be small enough to attach to your bug report.\nAmrapali - Did you successfully resolve your original problem from 2010?  If not, could you retest with the current version of Refine (2.5).  I'd like to close this issue if possible.\n. From amrapali...@gmail.com on August 02, 2012 13:31:38:\nYes, I tried with the latest version of google refine and now it works.\nThanks.\n. From tfmorris on August 04, 2012 23:40:44:\nAs far as I can tell we never implement an RDF/XML importer, only NTriples support.  There was an additional issue with the way the UI was wired up (issue #599), but even without that, things wouldn't have worked.\nI've added support for RDF/XML and N3 formats and will check it in after a little more testing.\n. From tfmorris on August 05, 2012 16:39:40:\nI've added RDF/XML support in r2526.  All the plumbing is there for RDF N3 support too, but there is no UI for it currently.  RDF/XML and RDF Ntriples are the two supported RDF formats currently.\nSorry about the confusion.  The developer who did the initial RDF support isn't currently active and none of the rest of us really use RDF at all, so we weren't aware of the state it was in.\nNote also that the XML importer is able to import RDF/XML files as well and in some cases may be preferable.  It's worth previewing both importers to see which one best suits your needs.\n. From tfmorris on August 04, 2012 23:40:44:\nAs far as I can tell we never implement an RDF/XML importer, only NTriples support.  There was an additional issue with the way the UI was wired up (issue #599), but even without that, things wouldn't have worked.\nI've added support for RDF/XML and N3 formats and will check it in after a little more testing.\n. From dfhu...@gmail.com on October 13, 2010 04:51:09:\nThis issue was closed by revision r1469.\n. From dfhu...@google.com on October 13, 2010 04:51:32:\nFixed by r1469.\n. From dfhu...@google.com on October 13, 2010 05:05:09:\nThis actually has nothing to do with aliases, I believe. The problem is that Refine tries to batch together several cells when it calls the recon service. And that can overload relevance or acre somehow. I'm lowering the batch size and hope that helps (r1470). Otherwise, filter for rows that haven't been matched, and re-reconcile them again.\n. From thadguidry on October 15, 2010 03:10:07:\nLowering the batch size seems to have helped with the attached project test file of with /saints.\nHowever, I'm now noticing that even though I uncheck the \"Auto-match candidates with high confidence\", Refine still auto-matches on some.  I would have thought that unchecking the box would explicitly say not to \"auto-match at all, and I want to see all my candidates and I'll pick the matches I want\".  Is that how it should work ? If not, can we get a good description of what the intended function is supposed to be ?  Thanks for working with me on this David & team, as always! \n. From thadguidry on October 15, 2010 03:12:35:\ncrap. that last comment came out wrong... rather...\nI would have thought that unchecking the box would explicitly say \"DON'T Auto-match anything, just show me the candidates and I will pick the matches that I want\".\n. This was an issue with the old reconciliation service.  The automatch behavior is done differently with the new service (which I think was the last piece remaining), so I'm closing this.\n. From iainsproat on October 14, 2010 16:59:01:\nTom, is this now done?\n. From tfmorris on October 14, 2010 18:10:21:\nThe code has been committed, but it's untested in its new home.\nRemaining to be done:\n1. (short term, hopefully 2.0) Test that it gets built and included in distribution properly\n2. Add export support\n3. Add authentication support (basic commands implemented, but more work required)\n   #  2 & #  3 are probably post-2.0 activities.\n. From tfmorris on October 14, 2010 18:25:59:\nThe code has been committed, but it's untested in its new home.\nRemaining to be done:\n1. (short term, hopefully 2.0) Test that it gets built and included in distribution properly\n2. Add export support\n3. Add authentication support (basic commands implemented, but more work required)\n   #  2 & #  3 are probably post-2.0 activities.\n. From tfmorris on October 14, 2010 18:27:08:\nSorry for the dupe.  Google Code gave me a 500 server error and I retried the operation without checking first to see whether it had (partially) succeeded.\n. From tfmorris on December 12, 2010 19:09:46:\nThe plugin was distributed with Refine 2.0.  It only handles reading public spreadsheets, but I'm closing this since the basics are covered.  Authentication support and export support can be dealt with separately.\n. From tfmorris on October 14, 2010 18:10:21:\nThe code has been committed, but it's untested in its new home.\nRemaining to be done:\n1. (short term, hopefully 2.0) Test that it gets built and included in distribution properly\n2. Add export support\n3. Add authentication support (basic commands implemented, but more work required)\n   #  2 & #  3 are probably post-2.0 activities.\n. From tfmorris on October 14, 2010 18:25:59:\nThe code has been committed, but it's untested in its new home.\nRemaining to be done:\n1. (short term, hopefully 2.0) Test that it gets built and included in distribution properly\n2. Add export support\n3. Add authentication support (basic commands implemented, but more work required)\n   #  2 & #  3 are probably post-2.0 activities.\n. From tfmorris on October 11, 2011 23:19:59:\nI've added Open Document Format (ODF) Spreadsheet (.ods) importer and exporter modules.  They didn't make the 2.5rc1 kit, but are available on the SVN trunk.\n. From Martin.M...@gmail.com on October 15, 2011 18:28:13:\nThanks for the update! :)\nI am not a dev. and I wanted to know if there is a way to use those modules. I succeed to spot them in the SVN trunk but I don't know how to update my google refine to enjoy them. \n. From tfmorris on October 15, 2011 21:15:51:\nIf you're not a developer, you'll need to wait for the next kit build.  It's likely that there will be another before the official 2.5 release, so keep an eye out for announcements.\n. From dfhu...@google.com on October 17, 2010 20:50:07:\nThis issue was closed by revision r1587.\n. From dfhu...@gmail.com on October 17, 2010 17:15:07:\nThad, could you svn up and shift-reload and try again? It looks like you have old cached js code.\n. From dfhu...@google.com on October 19, 2010 00:53:08:\nThis issue was closed by revision r1607.\n. From thadguidry on November 20, 2010 16:16:32:\nSimilar to Issue-107\n. From tfmorris on November 26, 2010 23:59:10:\nRaymond - I've fixed some character encoding issues which may have resolved this.  Can you retest (or provide the data file that didn't work before)?\nThad (if you're following this) - If you type issue #107, instead of issue-107, it will get hotlinked for you automatically, making the reference easy to follow.\n. From thadguidry on November 29, 2010 04:48:33:\nCool. I wondered why it wasn't working anymore.  Must have forgot the comment/wiki syntax for Google Code.  Good to know, about not including the dash after issue. Thanks Tom.\n. From raymond....@gmail.com on November 30, 2010 14:54:28:\nTom:  I tested the latest version of Refine for this problem and things seem to be working fine now.  Thanks!\n. From tfmorris on November 30, 2010 17:05:38:\nThanks for retesting!  Marking as fixed...\n. From tfmorris on November 30, 2010 17:05:38:\nThanks for retesting!  Marking as fixed...\n. From tfmorris on November 30, 2010 17:05:39:\nThanks for retesting!  Marking as fixed...\n. From iainsproat on November 30, 2010 17:12:08:\nShould we not be marking as 'verified' if the original reporter has tested it and concurs with the dev that it is fixed?  (Changing the status to 'verified'.)\n. From tfmorris on November 30, 2010 20:19:07:\nI haven't seen a description of the bug life cycle the project uses, but for my projects we usually \"verify\" against a released version, not SVN (it could get broken again or not get released properly or ...)\n. From tfmorris on November 30, 2010 20:19:08:\nI haven't seen a description of the bug life cycle the project uses, but for my projects we usually \"verify\" against a released version, not SVN (it could get broken again or not get released properly or ...)\n. From dfhu...@gmail.com on December 02, 2010 07:12:00:\ntfmorris, you're right about a \"verified\" bug still getting broken for the next release. I presume that's a low chance, though. An alternative is to go over each bug just before the next release and verify it, but that sounds more tedious. Do you know of any other alternative work flow?\n. From tfmorris on November 26, 2010 23:59:10:\nRaymond - I've fixed some character encoding issues which may have resolved this.  Can you retest (or provide the data file that didn't work before)?\nThad (if you're following this) - If you type issue #107, instead of issue-107, it will get hotlinked for you automatically, making the reference easy to follow.\n. From thadguidry on October 19, 2010 02:21:54:\nScreenshot http://awesomescreenshot.com/0b22ju666 of what step 4 looks like on my Windows PC with defaults selected when creating new project file from attached .TSV file.\n. From thadguidry on October 19, 2010 02:29:42:\nIf, however, I change the encoding of the attached file to UTF-8 using Notepad++ or similar text tool, and save and then create a new project from that newly encoded file, Refine does seem to detect UTF-8 completely and correctly display the diacritic characters for row 33.\nHmm, perhaps the fault lies at the beginning with the Export function for TSV in Refine?  Does the export use UTF-8 or default to ASCII instead ?  If either, should it ask you which encoding you want to export as ?\n. From jayl...@gmail.com on November 11, 2010 01:26:14:\nNote that you can use return value.decode('utf-8') using jython instead of GEL, and the values will be processed correctly.\n. From tfmorris on November 27, 2010 00:38:27:\nThis is a duplicate of issue #237.  The character encoding guesser was leaving the project encoding unset if it got a confidence value below the threshold.\nThe reason it's guessing wrong is that it only looks at the first 4k (approx.) of the file and your first non-ASCII characters are beyond that boundary.  I investigating increasing the lookahead, but it appears that only the first 3881 bytes are available at the time the guessing is done.  Changing this would require restructuring things.\n. From tfmorris on November 27, 2010 00:38:27:\nThis is a duplicate of issue #237.  The character encoding guesser was leaving the project encoding unset if it got a confidence value below the threshold.\nThe reason it's guessing wrong is that it only looks at the first 4k (approx.) of the file and your first non-ASCII characters are beyond that boundary.  I investigating increasing the lookahead, but it appears that only the first 3881 bytes are available at the time the guessing is done.  Changing this would require restructuring things.\n. From iainsproat on October 20, 2010 12:56:13:\nActually, does $.ajaxSetup overwrite locally defined error handling functions?  If so, probably not a good solution.\nIn particular I was testing importing a corrupted project, but the browser stays on the 'Starting Up' spinner.  (an error is output to the log though).\nFirebug reports the following:\nGET http://127.0.0.1:3333/command/core/get-project-metadata?project=2011335124043\n500 Failed to find project id #  2011335124043\nWould some error handling added to line 126 of /webapp/modules/core/scripts/project.js do the trick for this particular function?\n. From dfhu...@gmail.com on October 20, 2010 17:09:33:\nI think we should only resort to ajaxSetup's error handler when all else fails. The get-project-metadata command should just return an error message in proper json, e.g.,\n{\n  \"status\" : \"200 OK\",\n  \"code\" : \"/api/status/error\",\n  \"message\" : [{\n     \"message\" : \"Project file is corrupted and cannot be imported.\",\n     \"lang\" : \"en\"\n  }]\n}\nAll these messages need to be localized sooner or later.\n. From iainsproat on October 20, 2010 12:56:13:\nActually, does $.ajaxSetup overwrite locally defined error handling functions?  If so, probably not a good solution.\nIn particular I was testing importing a corrupted project, but the browser stays on the 'Starting Up' spinner.  (an error is output to the log though).\nFirebug reports the following:\nGET http://127.0.0.1:3333/command/core/get-project-metadata?project=2011335124043\n500 Failed to find project id #  2011335124043\nWould some error handling added to line 126 of /webapp/modules/core/scripts/project.js do the trick for this particular function?\n. From dfhu...@google.com on November 05, 2010 17:12:11:\nFixed by r1771.\n. From tfmorris on October 25, 2010 16:38:47:\nDavid - feel free to reassign to James if that's more appropriate.  (I'm happy to fix myself, but since it's part of the UI design didn't really think I should)\n. From dfhu...@gmail.com on October 25, 2010 17:29:07:\nFixed by r1659. I changed the color to gray for now. James will probably go over the whole thing for 2.1 anyway.\n. From dfhu...@gmail.com on October 25, 2010 18:37:55:\nThis is due to a bug in the \"relevance\" service.\n. From iainsproat on October 25, 2010 18:54:26:\nMy Refine log is attached.\n. This is for the old reconciliation service which has been retired.  Closing.\n. From tfmorris on September 18, 2012 19:50:46:\nInternet Explorer support was improved in Refine 2.5.  Can you retest and let us know if this issue still exists?\n. From runuppat...@gmail.com on September 19, 2012 11:17:13:\nUnfortunately my configuration has changed since then about the browser version (now IE 9), so I couldn't replicate exactly the previous test case.\nMoreover I think google refine has changed so much since 1.1 that even menus content is different.\nFrom the little I tested today this bug couldn't be reproduced so it might be considered closed. I managed to create a project from clipboard and pushing some buttons around.\nUnfortunately I couldn't create a project from file, that is pressing \"next\" after selecting a file with rw permission doesn't perform any reaction on the GUI.\nI don't know if the reason lies on some prerequisite/configuration I ignore because I don't use GR on a regular basis, if it's not the case a new bug report should be submitted.\n. From tfmorris on September 21, 2012 05:03:45:\nThank you for your prompt reply.\nIf you find that the new problem you discovered is reproduceable, please DO submit a new bug report.\n. From tfmorris on October 27, 2010 19:56:53:\nIsn't startsWith and endsWith redundant for a regex?  Those things can be specified in the expression itself.\nThe advantage to startsWith and endsWith is that they're simple and I think it would be good to keep them that way.\n. From thadguidry on October 27, 2010 20:45:03:\nYes, it is kind of redundant, I know. Thoughts further...\nHow do I perform a pattern match within arrayed sets using something like value.match(/.*(\\d\\d\\d+)/) that either provides a facet with True or False, or can be pipelined into a function that provides this boolean ? That function returns just a [] for me when it matches unfortunately, and my facet remains blank as well.  In other words, pulling out a specific pattern from my arrays where the pattern may appear at the beginning, or end, or somewhere in the middle of my string ?  Tricky stuff for me here, so I'm learning. I added sort of what I'm trying to do with 'Find a sub pattern that exists' in the Quick Recipes http://code.google.com/p/google-refine/wiki/Recipes Anyone ?\n. From thadguidry on October 27, 2010 20:58:08:\nEven more simplified question, in case I threw off everyone in that last comment.\nThe following expression is returning true for everything even when I have blank cells. Here, I'm just trying to see if the word SUITE appears anywhere in the string.  Yes, we have contains(), I know.  But sometimes I want to really flex some muscles with regex.\nnot(value.match(/.SUITE./))\n. From jluci...@gmail.com on January 12, 2011 19:25:22:\nI agree having the option of using regex in contains, startsWith and endsWith would be really helpful, since match does not return boolean. \n. From thadguidry on November 19, 2010 20:35:14:\nThis was a known Issue.  I have verified as fixed now for the attached files.  Please download and use Version 2.0.  Closing as Verified.\n. From dfhu...@gmail.com on October 30, 2010 21:08:50:\nWe have two Transpose commands in the upcoming version 2.0. You can apply them after you import your data. Could you provide some sample data that you need to transpose, so that we can tell whether the new Transpose commands can answer your needs?\n. From moog...@gmail.com on October 30, 2010 21:36:41:\nSure, below.  The result of the transpose should be four columns with headings \"Label\", \"Min\", \"Avg\", \"Max\".\nLabel,\"host1\",\"host2\",\"host3\",\"host4\",\"host5\",\"host6\",\"host7\",\"host8\",\"host9\",\"host10\",\"host11\",\"host12\",\"host13\",\"host14\",\"host15\",\"host16\",\"host17\",\"host18\",\"host19\",\"host21\",\"host22\",\"host23\",\"host25\",\"host26\",\"host27\",\"host29\",\"host30\",\"host31\",\"host32\",\"host41\",\"host42\",\"host43\",\"host44\",\"host45\",\"host46\"\nMin,8024,632,816,112,328,876,056,2748,0676,3704,3176,444,720,528,000,2412,2392,8352,6464,,1236,5692,844,340,0796,2320,360,752,092,680,836,664,4832,432,312\nAvg,8244,6767,1966,5515,9523,9250,8674,9335,070057549858,4782,1023,0674,7215,9413,9805,6900,4739,0924,0768,,7094,8598,7173,9638,5399,7911,3961,5059,1257,2204,2492,2083,1747,4622,3279\nMax,1248,1216,9600,0760,5432,3712,2700,9596,2136,4692,0276,6308,9300,2176,8984,9692,2224,9964,3524,,4652,5296,1332,6652,5796,0336,0236,4516,8740,7268,3372,6400,1808,4844,5044\n. From dfhu...@gmail.com on October 31, 2010 21:29:15:\nCurrently, it's do-able, but not convenient. I'll think about how to support transposing better.\n. From danpaulsmith on January 10, 2012 16:23:44:\nI think this is fixed now :)\n. From tfmorris on January 11, 2012 06:14:32:\nWe've got the two transpose commands that David mentioned for 2.0, but, unless I'm missing something (entirely possible), I don't think they handle this case easily.\n. From dfhu...@gmail.com on January 11, 2012 19:10:10:\nIt doesn't sound to me like those two commands would handle this case, either.\nThe problem here is how to preview this data. Previewing right now would parse only some limited amount of data at the beginning of the file to render the first 100 rows or so. But with this horizontal format, that approach cannot get to all of the columns unless we parse most of the file already (we read into some part of the last line in the file).\n. From moog...@gmail.com on January 11, 2012 22:32:59:\nWhy not display a limited amount of horizontal columns as well as rows.  I think it'd be acceptable to show the first 100 rows only, which means you could continue to just read the first 100 lines from the input.\n. From tfmorris on November 01, 2010 21:37:02:\nThe algorithm you describe is easy to implement, but I don't understand how it is better than either a) deleting the text or b) replacing all characters with a constant value such as 'x' or '*'.  Note also that leaving the word lengths intact represents a leak which may allow some amount of information to be recovered.\n. From mcnamara.tim@gmail.com on November 01, 2010 21:50:24:\nGood point. I guess I wanted to create the impression that the text still looked like names.\n. From dfhu...@gmail.com on November 01, 2010 22:01:30:\nDo you want the same name appearing in several cells to be anonymized to the same random string? And different names to different random strings?\n. From mcnamara.tim@gmail.com on November 01, 2010 22:13:05:\nThat's a good idea also David. It would allow sorting to be occur and if something relies on a name field, then it's less likely to break. \n. From tfmorris on November 16, 2010 06:05:24:\nThat doesn't really anonymize the data, as was mentioned the mailing list.\nI'm changing it from Accepted to New, not because it's unlikely to get implemented, but because no one is assigned to it.\nTim - I suspect you're using the developer's template for entering your bugs.  You can see it, but this is causing them to come in with a status of \"Accepted\" which makes them less likely to get triaged and assigned to someone.\n. From thadguidry on November 03, 2010 16:54:53:\nSome similarity here with issue #90\n. From fadima...@gmail.com on November 07, 2010 02:39:57:\nI am working on almost the same issue though from a different perspective. Thought that it might be helpful to describe it here... I am building an \"RDF reconciliation service registry\": a service that users can submit RDF files to and it turns this RDF into a GRefine standard reconciliation service. my next step is to build an extension that enables RDF export from a column in the project and submit this data to the registry so that other projects can reconcile against this data also.\n. From iainsproat on November 15, 2010 15:21:42:\nissue #211 has been merged into this issue.\n. From thadguidry on November 19, 2010 20:53:32:\nIain, do you think your Issue-96 could also be merged into this ?  or do you need other specific functionality ?  Looking at Issue-96 seems like your asking for the same thing that I am here?\nThis Issue-176 also adds the need for a better UI to handle reconciling between 2 Refine projects.  A high level overview has been given to James Home who will conceptualize this for the community for review later.\n. From thadguidry on November 03, 2010 16:54:53:\nSome similarity here with issue #90\n. From iainsproat on November 15, 2010 15:21:42:\nissue #211 has been merged into this issue.\n. @thadguidry - this is a few years old.  Safe to close as no longer relevant?\n. From kopo...@gmail.com on December 14, 2010 20:17:14:\nJust creating a project with ~1M rows. Taking 3 hours and I don't know if it will even finish today. An ETA would be really helpful for my workflow.\n. From tfmorris on December 14, 2010 20:49:27:\nNot really relevant to the issue at hand, but since people attempting to load large projects are likely to end up here, for large projects you should:\n1. Make sure that you've got sufficient real memory\n2. Make sure that your Java heap size is set large enough to accommodate your project\n3. Strongly consider turning off \"guess data types\" (which is on by default).  It will make multiple guesses for each cell and for the common case (i.e. text), it will try them all before falling through to accept the value as ordinary text.\n4. Consider turning off splitting and import as a single column.  There's a substantial amount of per-cell overhead which you may end up incurring unnecessarily for \"uninteresting\" columns that you'll just delete later.  Better to import things as a single column and do a little more work to split things by hand afterwards for very large projects.\n. From dfhu...@gmail.com on September 11, 2011 05:17:58:\nThe new importer UI framework in trunk/ should already show uploading progress.\n. From wfznimbl...@gmail.com on November 15, 2010 21:17:52:\nThis would be very helpful.\n. From incredible.angst on November 16, 2010 22:34:35:\nFor me, it returns some strangely formatted date string.\nrow value           value.toString()\n4.  2008-08-07T00:00:00Z    Thu Aug 07 00:00:00 MSD 2008\n5.  2009-12-22T00:00:00Z    Tue Dec 22 00:00:00 MSK 2009\n7.  2007-11-06T00:00:00Z    Tue Nov 06 00:00:00 MSK 2007\n24. 2010-10-01T00:00:00Z    Fri Oct 01 00:00:00 MSD 2010\n27. 2010-05-13T00:00:00Z    Thu May 13 00:00:00 MSD 2010\n30. 2010-07-06T00:00:00Z    Tue Jul 06 00:00:00 MSD 2010\nIt uses this format regardless of first argument that should be format string:\nrow value           value.toString('yyyy-MM')\n4.  2008-08-07T00:00:00Z    Thu Aug 07 00:00:00 MSD 2008\n5.  2009-12-22T00:00:00Z    Tue Dec 22 00:00:00 MSK 2009\nAlso, freebase does not accept 'date' objects stored in refine table, as they are converted to string using the same format:\nAction  Message Details fb_id\n1.  LOAD_TRIPLE triple is not valid to be loaded    (ERROR): Invalid literal Wed Jan 21 00:00:00 MSK 2009 of type /type/datetime: Literal value (Wed Jan 21 00:00:00 MSK 2009) must be valid mql datetime\n   \"$Name_en_0\" --(/internet/website/launched)--> \"Wed Jan 21 00:00:00 MSK 2009\" [{\"recon\":{\"s\":\"rec1289912419390812104\"}}]\n. From tfmorris on November 26, 2010 23:39:41:\nI'll take a look at this.  David's case is simple.  The code currently doesn't use a default format if none is provided.  It does work if you give it an explicit format string.\nI'm unsure if the behavior reported in comment #  1 is related.  How are you invoking toString() (ie in what context)?  When I try it, I get the behavior that David reported.  Can you also provide more information on your operating system, locale, and Java version? \n. From tfmorris on November 26, 2010 23:50:25:\nvalue.toString() on a date will now format the date using the default locale formatting.  value.toString('') will format the date according to the given format string (that part was already implemented).\nI'm of two minds as to whether this is the best solution or if we should make the default be ISO 8601 (and MQL) compatible.  If anyone has strong opinions, please speak up.\nincredible.angst - please open a separate bug report for your problem.  I can't see how it could be related.\n. From tfmorris on November 26, 2010 23:39:41:\nI'll take a look at this.  David's case is simple.  The code currently doesn't use a default format if none is provided.  It does work if you give it an explicit format string.\nI'm unsure if the behavior reported in comment #  1 is related.  How are you invoking toString() (ie in what context)?  When I try it, I get the behavior that David reported.  Can you also provide more information on your operating system, locale, and Java version? \n. From thadguidry on February 18, 2011 02:24:29:\nr1989 breaks using checkmarks.  Display not longer updates and user is forced to use Shift F5 to see changes.  Popup to undo does still appear however.\n. From dfhu...@gmail.com on February 18, 2011 05:40:31:\nThis was fixed by an earlier check in r1989. It caused a problem which was fixed by r2010.\n. From tfmorris on November 11, 2010 17:06:58:\nIs there anything specific about the dBase/xBase (DBF) files used in shapefile sets or the way that you'd use them?\nIf not, we can probably just convert this to a request for a generic xBase import/export capability.\n. From 7...@ericjarvies.com on November 11, 2010 17:29:09:\nA generic xBase import/export is the request.   Open the fileName.dbf, perform some 'refine' magic to the data, and write/save back to fileName.dbf so the shapefile set works as usual.  The first record in the .dbf file corresponds to the first record in the .shp and .shx files, so the GIS-refine user would need to make sure to pay attention not to delete rows or shift up/down rows, which would seldom be the case... mostly always ever need to edit the columns and the cell data itself.\nEric Jarvies\n. From iainsproat on November 11, 2010 18:05:20:\nAnyone used XBaseJ before? http://xbasej.sourceforge.net/\nWould it be appropriate as an xBase parser we could wrap an importer around?\n. From 7...@ericjarvies.com on November 11, 2010 18:22:19:\nHaven't used it, but it looks like it'll work!\nhttp://xbasej.sourceforge.net/\nhttp://freshmeat.net/projects/javadbf/\nEric Jarvies\n. From neale.st...@gmail.com on May 25, 2012 02:21:54:\nI'm wondering if any progress has been made on this. This would be of huge benefit to a project I'm involved in..\n. From thadguidry on May 25, 2012 04:15:48:\nNeale, it might be a long while.  You might want to use a copy of MS Access to import the DBF files and export those back out to CSV.  Another tool that could also accomplish this would be Talend Open Studio.\n. From iainsproat on November 11, 2010 14:14:45:\nThanks for the bug report.\nGridworks.bat was also mentioned in the Windows section.  I've fixed both typos.\n. From iainsproat on November 11, 2010 13:30:55:\nI can reproduce.  I suspect the bug might be in POI rather than in Refine, but I'll investigate.\n. From iainsproat on November 11, 2010 13:54:14:\nAccording to POI bug report 48261, https://issues.apache.org/bugzilla/show_bug.cgi?id=48261 , the issue is likely a corrupted Excel file (probably caused by the file being created from a non-Excel application).  Their advice is to open the file in Excel and resave.\nI've done this and can confirm that it works in Refine.  The file I used is attached.\n. From HyderAla...@gmail.com on November 11, 2010 13:58:09:\nI can confirm this as well. No fix is necessary, but it would be nice to inform users of the solution when this error is encountered.\n. From iainsproat on November 11, 2010 14:27:08:\nGood call.  I've added additional error dialog and committed it to the trunk, r1858\n. From hiren.t....@gmail.com on December 18, 2010 20:11:17:\norg.apache.poi.hssf.usermodel.HSSFWorkbook .I am using this API i am getting out of memory error while just opening the file.I get out of memory error.The file is 25 mb.How can i resolve this?\n. From tfmorris on December 18, 2010 20:42:15:\nIf you are having this problem with Google Refine, please open a new bug report with the details of your problem.\nIf your problem is with some other use of Apache POI, please contact the POI team.  There's no one here who can help.\n. From dfhu...@gmail.com on November 11, 2010 15:13:38:\nAs a temporary hack, you could try something like this:\nwith(1000000, div, floor(value / div).toString() + (div + mod(floor, div)).toString().substring(1))\n. From shinhan....@gmail.com on November 12, 2010 07:17:12:\nIf I copy paste that to transform, I get error \"Error: mod expects 2 numbers\"\nmod(floor,div)\ndiv seems to be a variable (not sure as I cant find documentation on the with function), floor doesnt. If second floor is supposed to be floor(value/div), then the full expression would be:\nwith(1000000, div, floor(value / div).toString() + (div + mod(floor(value / div), div)).toString().substring(1))\nBut the results of that are very much different from the input.\nBtw, the temporary hack I used was importing this to excel, making a field =CONCATENATE(\"s\",numfield), reloading it in google refine and then using value.substring(1). So, I know a hack, but this really needs a proper solution.\n. From tfmorris on November 27, 2010 03:02:13:\nIs this a number or just a string of digits?  If you're never going to do calculations with it, you are much better off telling Refine not to convert the string of digits to a number in the first place (ie turn \"auto-detect value types\" off when creating the project).  Once it's done its conversion, there's no guarantee that it will be reversible since it's using floating point representation.\nPerhaps we should have this off by default since it seems to cause a number of people trouble (and it's slowwww).\nissue #243 discusses this same topic.\n. From shinhan....@gmail.com on November 29, 2010 07:17:09:\nI just tried that. It seems \"auto-detect value types\" checkbox is ignored since even when loading a new file, with that checkbox NOT checked, the \"numeric\" fields are green and setting Text Facet on the tax ID number turns them to scientific notation. I also tried turning off both checkboxes (\"split into columns\" and \"auto-detect value types\"), no improvements, tax ID number is still green and still parsed as number.\n. From dfhu...@gmail.com on December 02, 2010 06:52:26:\nWeird, I just tried with \"auto-detect value types\" off and it behaved as expected.\nShihan, do you have a sample data set that's giving you trouble and that we can test?\n. From shinhan....@gmail.com on December 02, 2010 08:58:50:\nAttached is the file I've been testing this on, all of the data is from the public government database.\nI just noticed that converting to .csv (if I turn off the auto detect value types and add \";\" in the split columns by) works fine.\nBut when I open the .xls directly, even though the \"PIB\" field is formatted as Text and \"Auto-detect value types\" is off, it parses it numerically.\n. From tfmorris on June 08, 2011 00:13:34:\nExcel is the one making the determination as to whether the cells contain numbers or text.  As far as I can tell Refine is importing exactly what Excel gives it.\nAttached is a copy of your spreadsheet with the first three numbers in the PIB column converted to Text instead of numbers.  These correctly import into Refine as text.\nPart of the confusion may lie in the way Excel works.  Changing the formatting of a cell or column doesn't automatically coerce the contents to the new type.  Often this won't happen until you edit the cell value.\nPlease review and let us know whether we can close this.\n. From shinhan....@gmail.com on June 09, 2011 06:06:31:\nSo, there is still no solution to importing something like this? Editing every value in order to coerce the type change is not a solution. \n. From tfmorris on June 09, 2011 20:25:51:\nYou're really asking an Excel question and I'm not enough of an expert at Excel to give you a definitive answer, but this article http://office.microsoft.com/en-us/excel-help/three-ways-to-convert-numbers-to-text-HA001136619.aspx has some good suggestions.  In particular that TEXT() function looks like it'll do what you want.  This is similar to your previous CONCATENATE() solution, except it doesn't require the extra step after importing to Refine.\n. From tfmorris on November 27, 2010 03:02:13:\nIs this a number or just a string of digits?  If you're never going to do calculations with it, you are much better off telling Refine not to convert the string of digits to a number in the first place (ie turn \"auto-detect value types\" off when creating the project).  Once it's done its conversion, there's no guarantee that it will be reversible since it's using floating point representation.\nPerhaps we should have this off by default since it seems to cause a number of people trouble (and it's slowwww).\nissue #243 discusses this same topic.\n. From dfhu...@gmail.com on November 11, 2010 15:08:36:\nAny chance you can upload that data here or a subset of it that's giving you problem? You're probably hitting a bug that's only triggered by some particular piece of data.\n. From iainsproat on November 11, 2010 15:15:18:\nCarlF002, thanks for submitting the bug report.\nI can't reproduce this (I'm using a dataset with 7000+ rows).  Could you provide the log from the console screen?\nIf possible could you also provide the dataset you are working with? (Please note that any data posted here will be publically viewable, so please ensure you have the necessary permissions before you do so)\nThanks\n. From emuel...@esri.com on November 24, 2010 00:05:21:\nI had the same problem on a dataset with about 3000 rows, all the more maddening because the clustering was working perfectly for a coworker. After searching through bug reports and the documentation, I stumbled across one mention of Chrome and Firefox, in passing. Asked the coworker only to find out he was indeed running Chrome.  I switched from IE to Chrome and haven't had another issue. If one of those browsers is required, it would probably be useful to note that on the Installation Instructions and/or FAQs. If they aren't, I was running IE8 on Windows XP.\n. From vumahe...@gmail.com on January 31, 2011 18:09:03:\nI too am facing the same problem... I press the cluster button and nothing happens...\nI am uploading a sample set here... I used the following steps: In inventors column I applied Edit cells >> split multi valued cells.... then I tried applying cluster in the inventers column and :( plz help. \n. From vumahe...@gmail.com on January 31, 2011 18:16:26:\nNote: in the above process i split the multi valued cells by \"|\" ... \n. From dfhu...@gmail.com on January 31, 2011 18:19:05:\nI don't think Refine can take .ods files. Could you provide .tsv, .csv, or excel?\n. From vumahe...@gmail.com on January 31, 2011 18:31:30:\nhere you go\n. From dfhu...@gmail.com on January 31, 2011 18:39:54:\nSo now I see 60 rows. Clustering using method \"nearest neighbor\", distance function PPM, and a radius of 1.0 yields 1 cluster that contains Locke, Gerald and Locke, Gerald S. It seems to be working just fine on these 60 rows. How many rows does your full data set contain?\n. From vumahe...@gmail.com on January 31, 2011 18:45:39:\nMy file has lots of rows but I have tested on the sub set provided here and even on this the cluster popup doesn't appear....\n. From vumahe...@gmail.com on January 31, 2011 18:47:21:\nI tried this in both firefox and chrome\n. From vumahe...@gmail.com on January 31, 2011 19:06:37:\nI just tried it out on IE and its working.... but wats wrong with the other browsers???\n. From vumahe...@gmail.com on January 31, 2011 19:18:18:\nalso suppose my data set has a huge list of companies can these be clustered into groups of companies of same entities.... for example google and all its subsidaries in the data set appear in one cluster... can we make this happen???\n. From dfhu...@gmail.com on January 31, 2011 22:36:35:\nI'm using Chrome and Firefox and there's no issue with them on my machine. When I hear browser-related issues, they are often in IE.\nAs for the Google and subsidiary case, this is not easy. I'd recommend using facets to discover patterns, then create new columns that simplify the original column (such as removing anything following a comma), and then cluster by the new columns.\n. From dfhu...@gmail.com on November 11, 2010 15:10:58:\nWe haven't seen this problem before. It could be specific to your data, somehow. Is your data public? If so, could you upload it or part of it here so we can use it to test?\n. From felix.av...@gmail.com on November 11, 2010 15:17:06:\nSure.\nThis is the same problem with both of this data.\nThanks\n. From iainsproat on November 11, 2010 15:21:25:\nIt sounds fairly similar to http://code.google.com/p/google-refine/issues/detail?id=137\n. From felix.av...@gmail.com on November 11, 2010 15:31:34:\nWhen I load issue #137 I can do nothing, even see the data. But here I can see the file, do facets, edit cells manually ...\n. From thadguidry on November 11, 2010 16:02:22:\nFelix, can you copy and paste your JSON extract ?  Use Undo/Redo then Extract button and copy and paste the JSON into Notepad and attach here.  We'll take a look at what your operations are and where the problem might be.\n. From felix.av...@gmail.com on November 11, 2010 16:47:33:\nVoila.\nThank you\n. From felix.av...@gmail.com on November 11, 2010 15:31:34:\nWhen I load issue #137 I can do nothing, even see the data. But here I can see the file, do facets, edit cells manually ...\n. From AJDIPA...@gmail.com on November 11, 2010 16:01:46:\nYou may close this.  I figured out that I had to leave that command window open.  Is there a way we can do this without that command window open?  I just thought it was some setup process, but I now know its now - its critical to the system functioning.\n. From thadguidry on November 11, 2010 16:06:38:\nRefine runs as a active live server process on your machine and uses Java.  On most systems that means it runs Java using a command window which needs to remain open. The command window also shows log information and error output when using Refine.  Just minimize the command window if it's in your way.\n. From thadguidry on November 11, 2010 16:22:45:\nIs it possible that a security suite might be blocking this ?\n. From krues...@gmail.com on November 11, 2010 16:37:16:\nNot as far as I can tell.  I've tried it with the built-in OS X Firewall off, Waterroof firewall set to allow all connections.  Here's what the log says:\nNov 11 11:28:42 Omamori [0x0-0x571571].com.google.refine.Refine[74966]: 11:28:42.557 [                   refine] Sorry, some error prevented us from launching the browser for you.\nNov 11 11:28:42 Omamori [0x0-0x571571].com.google.refine.Refine[74966]:  Point your browser to http://127.0.0.1:3333/ to start using Refine. (866ms)\n. From krues...@gmail.com on November 11, 2010 16:56:13:\nI see two more possibly-relevant message in windowserver.log:\nNov 11 11:52:28  [66] Reserved range exhausted. (0x14bc91000 to 0x14bc99000 goes out of bounds).  Mapping new allocations into any available space\nNov 11 11:52:28  [66] kCGErrorIllegalArgument: CGXOrderWindow: Operation on a window 0x6 not owned by caller Google Refine\n. From jhea...@earthlink.net on December 14, 2010 20:14:15:\nSame issue, same Mac OS X version (10.5.8), but a different browser though: Safari 5.0.3\nSame Console messages. One possibly relevant message(?):\n12/14/10 11:26:47 AM Safari[7157] LSOpenFromURLSpec() returned -43 for application (null) path /Users/jheasly/Downloads/Google Refine 2.0. \n. This is a bunch of different, but system specific and old, issues, so I'm closing it.  Issue #733 covers the problem reported by @sparkica.  If anyone else encounters problems, please create a new issue describing your system configuration.\n. From tfmorris on November 11, 2010 20:08:44:\nI've checked a few possible parser candidates:\nJabRef - GPL\njavabib - GPL\nj4bib - BSD license, no recent activity, http://sourceforge.net/projects/j4bib/files/\nbibparse - no stated license (even in source zip), author's home page hasn't been updated since 2005 after fairly regular updates before that, so he may be retired or deceased http://ftp.math.utah.edu/pub//bibparse/\nI'll take a look at j4bib unless someone comes up with a better alternative.  It's not a very complex format, so writing from scratch is an option as well.\n. From mcnamara.tim@gmail.com on November 15, 2010 02:10:19:\nWriting from scratch wouldn't be too hard at all, it's just a format of key:value pairs. ..especially as an importer probably wouldn't need to do validation.\n. From wfznimbl...@gmail.com on November 15, 2010 21:17:29:\nI frequently use BibTex so I give this +1!\n. From thadguidry on November 19, 2010 23:08:46:\nattached single BibTex record from Google Books export [[http://books.google.com/books?id=d1tIAAAAYAAJ&pg=PR3#v=onepage&q&f=false]] for quality checking with diacritic characters when this feature is implemented.\n. From jan.schu...@gmail.com on September 28, 2011 11:26:17:\nI attached a more complicated record from Web of Science (first article for the query \"google\"). Note especially the multiple values in some fields. \nGoogle refine would be great for address cleaning and such things... Does it have a \"address guesser\"?\n. From tfmorris on October 15, 2011 17:31:39:\nSome additional possibilities for starting points:\nbibtext2rdf Apache 2.0 license, JavaCC grammar \n  http://sourceforge.net/projects/bibtex2rdf/\nANTLR grammar for BibTex - no stated license \n  http://stackoverflow.com/questions/7583982/bibtex-grammar-for-antlr\nMIT SIMILE bibtext-converter - MIT License, JavaCC grammar - doesn't attempt to interpret LaTex\n  http://code.google.com/p/simile-widgets/source/browse/babel/trunk/converters/bibtex-converter\n  https://simile.mit.edu/repository/babel/trunk/converters/bibtex-converter/\nj4bib (mentioned above) - BSD license, uses JLex and CUP\n  https://downloads.sourceforge.net/project/j4bib/j4bib/j4bib-0.2/j4bib-src-0.2.tar.gz\nI take back what I said last year about the format being simple.  On the surface it is, but because one can embedded arbitrary LaTex code, you'd need a full parser/render to faithfully parse everything.  Even for a basic level of support, you'd need to handle things like LaTex character composition e.g. {\\'E}mile \n. From jan.schu...@gmail.com on December 13, 2011 18:54:21:\nIf the latex thingy is a problem, maybe a RIS importer can be used, which does not allow latex commands.\nAlmost all bug databases can export RIS or bibtex and there are some bibtex to RIS converter, which should help if you are stuck with bibtex exports.\n. From tfmorris on December 13, 2011 20:29:26:\nThanks for the suggestion.  The entity substitution issue that I mentioned as an example of LaTex processing is actually pretty simple, so we'd probably do that first and see how if it covers the bulk of what people need.\nRIS or EndNote XML would be other bibliographic data formats to consider supporting for import, but I'm not sure they'd replace BibTex since many of the BibTex files are old hand-maintained bibliographies, not necessarily exports from a bib. web site or program.\n. From jan.schu...@gmail.com on December 13, 2011 21:51:10:\nThe interesting things for biblimetricians are probably the name and address cleaning part. Maybe even name disambiguating: is \"Chen, C\" of the first work in the list the same \"Chen, C\" as in the 1245th work? Or \"Meyer-L\u00fcdenscheid, CW\" the same as \"Meyer Luedenscheid, C\". Unfortunately, in the end, this is manual work, so I'm not sure how refine can help here. A string comparer which clusters names based on their string-distance function would be nice and also a cluster-algo based on the keywords/words in title/words in addresses (there are quite a few papers on Author name unambiguity, which use such methods) or the results of a google query (if there are similar authors and a google-query based on both titles returns some results, it is probably because of the authors webpage, which lists both works).\nThe name disambiguating part is probably interesting for others as well: merging two address databases, ...\n. From tfmorris on December 13, 2011 22:36:03:\nWe're getting off-topic (at least for this issue), so we should probably move the discussion to the mailing list/Google Groups, but Refine excels (so to speak) at precisely the kind of thing you're talking about -- allowing for and amplifying human judgments.\nFacets based on author name clusters, edit distances, keywords, and a number of other things are possible.  Various types of name cleanups is one of the current major uses of Refine.\nAs I said, if you want to discuss bibliographic data use cases more, let's move it to the list/group.\n. From andrewm....@gmail.com on December 26, 2010 21:09:45:\nI have the same problem. If you refresh the page you can see that the column was indeed deleted, but it's a very annoying bug nonetheless.\n. From tfmorris on December 26, 2010 21:47:24:\nWhat browsers do you guys see this problem with?\n. From andrewm....@gmail.com on December 26, 2010 22:21:58:\nI'm using Chrome. How about you hbechmann?\n. From GabrielS...@gmail.com on December 28, 2010 15:50:23:\nI have tried several XML files, but I can't reproduce this with WinXP + Chrome 9 + Refine 2.  Could one of you guys post a (masked) copy of your XML file, please?\n. From andrewm....@gmail.com on December 28, 2010 17:01:22:\nI'm actually using Chrome 8 (The stable verison.) but I doubt switching to 9 would make much of a difference. Anyway, here's the XML file I started with: http://ajedi32.webs.com/ged2.xml\n. From ja...@queso.com on April 26, 2011 14:29:22:\nI too see this; I'm using Chrome 12.0.742.9 dev and Firefox 3.6.16, both on Mac OS X 10.6.7.\nIt's 100% repeatable after importing all my XML files, and occurs on the sixth column-related action (either a column deletion or a column move, in my case). In other words, I can delete five columns successfully -- and then on the sixth column deletion (or move), I get the aforementioned Javascript alert box that reads something to the effect of \"index: X; size: Y\". This bug persists even after I reload the web page containing the data set; specifically, once Refine is in the state where I'm getting the Javascript alert box, then after a page reload, I get the Javascript alert box on the very first column-related action (in other words, I don't have a five-success window before the alerts start coming up).\nI am running Refine v2.0 [r1836], downloaded this morning.\n. From ja...@queso.com on April 26, 2011 14:32:34:\nAlso of note, once Refine gets into the state where it's throwing Javascript alert boxes for column-related actions, then it no longer applies actions to the undo/redo buffer -- so any column-related actions that generate the alerts are un-undoable (and unextractable).\n. From ja...@queso.com on April 26, 2011 14:42:13:\nAnd another data point: I just downloaded the latest dev trunk and am running it, and the bug still exists. This time, I got 11 successful column deletions before the 12th triggered the bug; the alert read \"Index: 70; size: 69\", and the console window read:\njava.lang.IndexOutOfBoundsException: Index: 70, Size: 69\n        at java.util.LinkedList.entry(LinkedList.java:365)\n        at java.util.LinkedList.get(LinkedList.java:315)\n        at com.google.refine.model.RecordModel.computeKeyedGroups(RecordModel.java:194)\n        at com.google.refine.model.RecordModel.update(RecordModel.java:112)\n        at com.google.refine.model.Project.update(Project.java:255)\n        at com.google.refine.model.changes.ColumnRemovalChange.apply(ColumnRemovalChange.java:75)\n        at com.google.refine.history.HistoryEntry.apply(HistoryEntry.java:127)\n        at com.google.refine.history.History.addEntry(History.java:131)\n        at com.google.refine.process.QuickHistoryEntryProcess.performImmediate(QuickHistoryEntryProcess.java:71)\n        at com.google.refine.process.ProcessManager.queueProcess(ProcessManager.java:69)\n        at com.google.refine.commands.Command.performProcessAndRespond(Command.java:214)\n        at com.google.refine.commands.column.RemoveColumnCommand.doPost(RemoveColumnCommand.java:62)\n        at com.google.refine.RefineServlet.service(RefineServlet.java:171)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n        at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n        at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:938)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:680)\nHope this is helpful.\n. From alexande...@gmail.com on June 02, 2011 19:54:32:\nImport this data and then try deleting the both of the columns labeled \"type\". Then, rename the remaining columns \"gname\" and cname\". All the changes that I have made after these initial changes have not been recorded in the \"undo/redo\" list! Plus, I continue to get the message \"Error: Index 4 size 3\"\n. From dfhu...@gmail.com on June 05, 2011 04:35:42:\nFixed in trunk/ by r2077, verified by using the file sparql (1).json.\n. From ja...@queso.com on April 26, 2011 14:29:22:\nI too see this; I'm using Chrome 12.0.742.9 dev and Firefox 3.6.16, both on Mac OS X 10.6.7.\nIt's 100% repeatable after importing all my XML files, and occurs on the sixth column-related action (either a column deletion or a column move, in my case). In other words, I can delete five columns successfully -- and then on the sixth column deletion (or move), I get the aforementioned Javascript alert box that reads something to the effect of \"index: X; size: Y\". This bug persists even after I reload the web page containing the data set; specifically, once Refine is in the state where I'm getting the Javascript alert box, then after a page reload, I get the Javascript alert box on the very first column-related action (in other words, I don't have a five-success window before the alerts start coming up).\nI am running Refine v2.0 [r1836], downloaded this morning.\n. From olliejo...@gmail.com on January 03, 2011 18:48:41:\nSome projects I created a couple of weeks ago, before the year rollover, are listed as \"-52 weeks old\"\n. From flybr...@gmail.com on January 05, 2011 03:03:16:\nSimilar issue - date is 1/4/2011; last modified in November or \"-10 months ago\"\n. From tfmorris on January 09, 2011 06:11:35:\nFixed in index.js to handle data wraparound on year boundaries.  Dates/times in the future (which was my original problem due to a confused O/S that had multiple ideas of what timezone it was in) are now all considered to be \"today.\"\n. From tfmorris on June 06, 2011 22:26:44:\nissue #329 has been merged into this issue.\n. From tfmorris on June 06, 2011 22:26:44:\nissue #329 has been merged into this issue.\n. From dfhu...@gmail.com on November 16, 2010 12:56:27:\nSeems mostly a UI design thing. Assigning to James.\n. From eljm...@gmail.com on November 16, 2010 06:27:52:\nCount me in for this. I'd love to be able to apply a change history to multiple files in an automated way.\n. From tfmorris on November 16, 2010 07:19:29:\n\"In\" as in contributing code to make it happen?  i.e. Should we assign the feature request to you so that people know it's being worked on? \n. From eljm...@gmail.com on November 16, 2010 07:24:04:\nI'm looking into it seriously, but not sure I have the time to commit. \nIt might be important enough to me to write it myself, so I'll keep my eye here as I poke around the source code. I'd love to contribute, so if I make any headway I'll let you know.\n. From dfhu...@gmail.com on October 03, 2011 17:47:36:\nPlease consider third-party libraries such as the following:\nhttp://pypi.python.org/pypi/refine-client/0.2.1\nhttps://github.com/maxogden/refine-processor\n. From dfhu...@gmail.com on November 14, 2010 03:42:27:\nYou're right--it's not possible right now. I'd say what you're asking for falls into a new feature area that includes grouping rows and computing sum, average, min, max within each group. That's necessary for visualization, too.\n. From jluci...@gmail.com on January 12, 2011 19:16:30:\nI have actually had success referring to another column's cell by using the 'cell.column_name' command. However if you want to perform operation on the cells in another column you are out of luck. I was trying to create a new column by summing two existing column with no avail e.g. sum([value,cells.column_name]). \n. From tfmorris on January 12, 2011 19:22:42:\njlucido2 - That doesn't sound related to enhancement that was requested.  Please ask general questions on the mailing list or create a new issue if you've got a problem or would like something enhanced.\nIt's likely that you just need cells.column_name.value for your example.\n. From TeamP...@gmail.com on June 16, 2011 11:43:17:\nThis is a very important feature for fixing common types of messy data.  For example, I have a 10000 record file where the dates have been input in different orders yy/mm/dd, dd/mm/yy, yyyy/mm/dd and dd/mm/yyyy (and sometimes noise), because different people entered the data.  I only have natural row ordering to help me out. In order to tell if 03/12/01 is March 12 or December 1st, I would look at the previous row to see what the 4 digit year was, and make a decision on that.\nThis could be implemented as a modification of \"Fill Down\" that takes a GREL expression. \n. From TeamP...@gmail.com on June 16, 2011 12:41:56:\nTyping out Comment 5 gave me an idea:  If I know the month is always in the middle of a date format, I can use that to my advantage.\nBy matching out the 2 digit (yy/mm/dd and dd/mm/yy) versions as v[0],v[1], v[2] ==> whenever v[0] == v[2] we know the year, and can convert it to a 4 digit year!\nFrom this I was able to establish a sufficient number of known good 4 digit years.  Then I created a new column called \"LastGoodDate\", based on the existence of 4 digit years, and used the fill down option on that new column.\nNow when I use a facet to display the remaining 2 digit versions, I have a reference to the last known good date that preceded it.  I can exploit the natural ordering to better predict which part is the day versus the year.\nThere are still more complex types of data where referencing a previous row is necessary to clean it up, I think I just lucked out this time.\n. From dfhu...@gmail.com on November 14, 2010 03:47:43:\nWe don't restrict to just the type you want because sometimes the entity you want has not been typed properly. But I agree that we should rank the properly typed entities first.\n. I believe that Suggest does prioritize the reconciled type, but I'll double check.\n@vad Can you explain what you want that checkbox to control (and which dialog/window you're talking about)?  Refine does know what type the user chose when reconciling a particular column.\n. From iainsproat on November 15, 2010 16:12:40:\nWould the proposed behaviour be correct for all languages which use a diacritic, or any other type of ancillary glyph?  I'm not so sure it would be.\nThe current behaviour of Refine takes the safe approach, but, as you point out, at the cost of usability.\nIt should be possible to work around this by creating a new column based on the existing, and converting all characters in the new column to ASCII-only.  You could then sort on that row.\n. From tfmorris on November 16, 2010 05:42:53:\nCollating sequences are locale specific, not language specific, but diacritic folding is a pretty basic need for many locales, the same way case folding is.  Fortunately, it's super easy in Java to do the right thing by using the built-in collators and their associated CollationKeys.\nhttp://download.oracle.com/javase/1.4.2/docs/api/java/text/CollationKey.html\n. From tfmorris on December 05, 2010 20:47:44:\nI hadn't noticed that this had been marked as an enhancement request.  Sorting is a pretty basic capability and Refine should be supporting a wider audience than just U.S. English, so I feel not having basic sorting capabilities is a bug.\nPerhaps fancy options are an enhancement, but Refine should do basic sorting using the system's default locale out of the box.\n. From tfmorris on December 12, 2010 18:21:40:\nRefine will now use the collating sequence associated with the system's default locale to collate strings.  It also does a normalizing decomposition of characters so that \u00e9 will collate the same independent of whether it is made up of a single character or a non-spacing accent decorating a base character.\nDefault collating strength is Collator.SECONDARY which maps roughly to the previous case-insensitive setting.  Setting the case sensitive bit changes this to Collator.IDENTICAL.  See the Java docs for more information on these settings and other possible options.\n. From dfhu...@gmail.com on November 15, 2010 00:13:23:\nDone!\n. From datamys...@gmail.com on November 15, 2010 01:49:50:\nThank you!\n. From tfmorris on November 16, 2010 05:46:06:\nWhat is your proposed format?  Why is it better?\nChanging the logging formatter is probably all that's required for this, but since it's a style preference you're never going to please everyone.\n. From stefa...@google.com on December 24, 2010 21:23:38:\nThe whitespace is necessary to provide enough information to show packages when log4j channels are actually java classes (which is most the time). This is trivial to change by using a different log formatter in the log4j.properties configuration file. The log formatter we're using is able to handle sophisticated uses of logs (for example > indents and < outdents at the beginning of log strings) but since it's very simple to change to use a more standard log4j formatter, I'm closing this one as a won't fix.\n. From dfhu...@gmail.com on November 16, 2010 12:58:30:\nCan this be supported by a template in the templating exporter? I'd imagine beside the default json template, we can also generate a wikitext template, and perhaps this sql dump file.\n. From thadguidry on November 16, 2010 15:15:11:\nShould be able to handle with templating exporter.  It will just be a text file anyway with a single CREATE statement for the table (per project?).  If folks need more than that, then they just use their database's restore options or other converter programs on the basic .sql text file generated.\n2 options that I could see useful for folks:\n1. Supporting COMMENTS statement\n2. A checkbox option to include exporting the data & schema or just the schema (for testing) \n. From haraldgr...@gmail.com on December 22, 2010 09:36:23:\nIf GR can connect and write to a SQL database by using a JDBC bridge, it wouldn't be necessary to dump the results as a SQL file. \nSimilar to issue: http://code.google.com/p/google-refine/issues/detail?id=12\n. From tfmorris on November 16, 2010 05:50:49:\nThe Google Refine server currently has no authentication or other security, so running it a different machine than your local one isn't really advisable.\nHaving said that, I'll leave this open as a feature request for the day when it is possible to run remotely.\n. I don't know what the original poster's issue was, but it seems likely that this is possible on Windows even if Refine doesn't implement it directly.  Here are some starting points from StackOverflow:\nhttp://stackoverflow.com/questions/289498/running-batch-file-in-background-when-windows-boots-up\nhttp://stackoverflow.com/questions/415409/run-batch-file-as-a-windows-service\nhttp://runasservice.sourceforge.net/\n. The technique will vary by operating system (although Linux and OSX will be\npretty similar) and it's not something that's unique to Refine.  You'd do\nit the same way that you start any other boot time service (e.g. a web\nserver).  We don't provide it out of the box because Refine currently\ndoesn't have the necessary security and multi-user infrastructure to allow\nit to be anything other than a personal server.\n. From mcnamara.tim@gmail.com on November 15, 2010 23:06:48:\nAn importer that supports a full implementation of YAML would be complex. It's actually far harder to parse than JSON, which is \nIt looks like the best Java implementation[1] is snakeyaml (Apache 2.0)[2]. Some issues reported with things being a little too low level to get started with. However , because refine we needs to do things fairly specifically that could be a good thing.\n[1] http://stackoverflow.com/questions/450399/which-java-yaml-library-should-i-use\n[2] http://code.google.com/p/snakeyaml/\nHere are some other projects:\n- jvyaml (MIT licence) https://jvyaml.dev.java.net/\n. From thadguidry on November 16, 2010 00:00:59:\nI will NOT support YAML implementations at all.  There are known issues with YAML parsing and the whole reason why there are so many YAML sucks pages.  XML and JSON are just as Human-readable as a serialization format, which was the primary motivator for YAML's creation.\n. From mcnamara.tim@gmail.com on November 16, 2010 00:14:38:\nGood point Thad. I disagree strongly that XML & JSON are as human readable as YAML, but think your main concern that parsing is difficult.  I wouldn't be too keen on Refine maintaining its own unique parser.  However, I don't see much harm in providing access to someone else's.\n. From thadguidry on November 16, 2010 00:26:14:\nNot just difficult.  It's error prone and has limitations within context.\n. From dfhu...@gmail.com on November 16, 2010 12:55:01:\nSounds like something to be developed as an extension to Refine. If it works fine enough and proves to be in popular demand, then we can decide whether to incorporate it into the core code base. Tim, do you do Java? ...\n. From iainsproat on November 15, 2010 15:21:41:\nThis might be achievable by having the master list in a separate Refine project from the import list, and using the Cross function http://code.google.com/p/google-refine/wiki/GRELOtherFunctions\nMore complexity would require for a reconcilation service between projects, see issue #176 http://code.google.com/p/google-refine/issues/detail?id=176\n. From dfhu...@gmail.com on November 15, 2010 17:37:30:\nAlso, a quick way to generate an abbreviation is\nforEach(value.replace(/\\W/, \" \").replace(/\\s+/, \" \").split(\" \"), v, v[0].toUppercase()).join(\".\")\n. From haraldgr...@gmail.com on December 22, 2010 09:44:53:\nComparing entities from different sources to match records that may refer to the same is a branch of computer science with 40 years research history, normally referred to as \"record linkage\".  \nGR is the easiest tool I have seen for doing record linkage, but even more advanced tools exists: \nhttp://en.wikipedia.org/wiki/Record_linkage#External_links\n. From iainsproat on November 15, 2010 15:21:41:\nThis might be achievable by having the master list in a separate Refine project from the import list, and using the Cross function http://code.google.com/p/google-refine/wiki/GRELOtherFunctions\nMore complexity would require for a reconcilation service between projects, see issue #176 http://code.google.com/p/google-refine/issues/detail?id=176\n. From tfmorris on November 16, 2010 05:54:31:\nCan you expand on this a little bit?  Perhaps some of the other developers know what you need, but the description is just a tiny bit too terse for me to figure out.\n. From iainsproat on November 16, 2010 09:24:07:\nI think this looks like a browser issue, and not an issue with Refine.  (although, as Tom points out there's a lack of info so I might be wrong).\nWhen you export from Refine, the file is handled as a download by the browser.  Whether the file is saved with the predefined name in a predefined location depends entirely on your browser preferences/options.  e.g. http://www.pcworld.com/article/155364/choose_where_to_save_downloads_in_firefox.html\n. From thadguidry on November 16, 2010 14:33:37:\nI think David is aware of this one already.  It's the use of Content-Type and Content-Disposition in the Header.  Currently, ExportProjectCommand.java only uses Content-Type.  In Google Chrome it saves WITHOUT prompting the user for the file location and name.  Also, there is a bug in Google Chrome 7 that causes the double .gz.gz extension in the saved filename which I filed with the Chrome 7 team already.\nDavid, how do you want to handle this one ?\n. From wfznimbl...@gmail.com on November 16, 2010 16:21:33:\nComment 3 is correct from my pov.\n. From thadguidry on December 13, 2010 17:17:51:\nRegarding my Comment 3 and bug of Google Chrome saving with a double .gz.gz in the filename, this has now been fixed by an update in Google Chrome browser 8.0.552.215\nPrompting for a save location always is still needed however by the reporter.\n. From thadguidry on November 15, 2010 21:28:33:\nThis is already on the Roadmap with \"traditional find and replace\"\n. From thadguidry on November 15, 2010 21:33:32:\nCould you not just filter or facet and then star those filtered or faceted rows ?  or is this a case were it is more painful to filter/facet because of the particular data ?\n. From wfznimbl...@gmail.com on November 15, 2010 21:43:42:\nright -- the situation is that there are metadata rows at arbitrary locations in the table which indicate that all subsequent currency values are in GBP rather than USD; however,  neither the preceding nor the subsequent rows contain data values that are unique to GBP/USD (ideally, the original column would have had GBP or USD for each row, but that is the problem I am fixing).\n. From dfhu...@gmail.com on November 16, 2010 13:02:39:\nYou could create a text facet on any column with this expression\nrow.index >= n\nwhere n is the current row's index. And then select true in the face. And star all matching rows. I just realize that row.index might be zero-based, but row indexes are displayed as one-based.\n. From mcnamara.tim@gmail.com on November 15, 2010 22:12:43:\n+1 for resizing, -1 for providing export support\nComplex visualisation, image export and so forth should be the domain of statistical packages. Admittedly, I've wanted these features too - but I think it would be a distraction for the product. Google Refine should stick to its points of difference: creating graph shaped data & data cleanup.\n. From thadguidry on November 15, 2010 22:34:18:\nHowever, the additional support could come later as an extension.  Oh, and patches welcome ! (wink)\nIdeas:\n- http://jung.sourceforge.net/\n- http://jmathtools.sourceforge.net/\n- http://www.jfree.org/jfreechart/\n- http://dsd.lbl.gov/~hoschek/colt/\n. From thadguidry on November 15, 2010 21:52:24:\nIt's important for the team to capture it for issues.  However, if your just creating generic enhancement requests, you can skip typing the Version/OS part I would imagine.\n. From iainsproat on November 15, 2010 21:55:42:\nThis is an issue with Google Code's tracking tool, and not something the Google Refine community can fix.\nPlease instead consider submitting this issue to Google Code support: http://code.google.com/p/support/issues/list\n. From thadguidry on November 15, 2010 22:11:53:\nHe doesn't have to do that.  This is just an artifact of the current user issue template that we use in Google Code and we can customize and change it.  But we don't want to, since we need to capture the OS/Version information for issues.\n. From thadguidry on November 15, 2010 22:16:19:\nCan this particular service accept the username/password within the url itself ?  Such as https://username:password@demogy.service-now.com/incident.do?CSV\n. @thadguidry The userinfo (username:password) is all handled client side.  This format is deprecated by the W3C because of the risk of exposing it when passing around URLs, but it'd certainly be the easiest approach for us to use.\nA better approach would be to add some more fields to the UI for username and password.  Now that we've switched to the Apache HTTP client, it'd be pretty easy to support a variety of authentication schemes.\n@timClicks (or anyone else) the service in the original issue is gone.  Is there a good substitute to test against?\n. The changes that I made in a7273625d7c33af70b6d16db5782c802186b3b99 add support for the userinfo style of URL with https://username:password@host/.  It's HTTPS only, basic auth only, undocumented and untested (although likely to work).\n. I'd have to do more research into how things are meant to be encoded, but a backslash () definitely won't help since it's not used in any of the URL encoding.\nYou could try replacing % with it's percent encoded equivalent (%25), but that's just a shot in the dark.\n. From iainsproat on November 15, 2010 22:31:37:\nissue #219 has been merged into this issue.\n. From tfmorris on December 19, 2011 23:19:44:\nTim - Can you provide an example (or more than one) of an API which requires the key in the header (and doesn't provide an alternative like YouTube does)?\nAnother use case for this (or very similar) functionality is the \"Accept: \" header field for specifying data formats.  This is used by, for example, end points which can return either RDF or HTML (like rdf.freebase.com) as well as other multiformat APIs such as CrossRefs http://www.crossref.org/CrossTech/2011/11/turning_dois_into_formatted_ci.html\n. From iainsproat on November 15, 2010 22:31:37:\nissue #219 has been merged into this issue.\n. From tfmorris on November 16, 2010 18:12:57:\nIs this to create a new project or populate a column or ...?\nA concrete use case, perhaps including an example of a real URL that you would use for this, would be helpful in understanding what you need.\n. From niels.bo...@gmail.com on November 16, 2010 20:13:47:\nTo keep it simple let say I have a coloumn of search phrases and I want to get the first result in Google SERP.\nSo real world URL: http://www.google.se/#hl=sv&source=hp&q=candy&aq=f&aqi=g10&aql=&oq=&gs_rfai=&fp=7f7a53791ad6dfb5\nUsing XPATH to get data out of webpages is so much easier than using regexpes. \n. From iainsproat on December 06, 2010 23:22:53:\nGREL functions have been added to allow HTML parsing in r1948.\nIt is now possible to use selector syntax to select DOM elements http://jsoup.org/cookbook/extracting-data/selector-syntax\ne.g.\nvalue.parseHtml().select(\"a\")[0].htmlAttr(\"href\")\n. From iainsproat on November 16, 2010 18:17:51:\nI like the idea.  Taking this further I think holding and dragging the column title would be a great way of achieving this.\n. From dfhu...@gmail.com on November 17, 2010 15:57:52:\nI'm a bit confused by the title of the issue. Move column n rows? By the way, in the drop-down menu next to \"All\" (left most), there's a Re-order column command that lets you drag and drop to re-order columns. Would that help here?\n. Original requester never replied to David's question 2 1/2 years ago, so I'm going to assume that the Reorder columns dialog is adequate unless I here otherwise.  If anyone comes up with a use case for this, feel free to reopen.\n. From thadguidry on November 16, 2010 17:03:43:\nHmm, actually I kind of like that idea.  Perhaps a star or mark next to the GREL history command that would add it to your favorite list stored within the /preferences ?  then the question of what format to store the list in that would allow easy sharing, or copying and pasting to friends.\n. From wfznimbl...@gmail.com on November 16, 2010 17:09:43:\ndon't sound so surprised! ;-)\n. From rodrod.s...@gmail.com on May 02, 2011 02:35:22:\nHello! Accept/reject patch for Starred Expressions.\n. From dfhu...@gmail.com on June 06, 2011 20:24:12:\nPatch applied in r2079.\n. From tfmorris on November 17, 2010 08:37:04:\nThere are already absolute value functions included.\n. From tfmorris on November 17, 2010 08:37:05:\nThere are already absolute value functions included.\n. From iainsproat on November 17, 2010 09:41:43:\nTom, I didn't see a GREL Abs function in the expr.function.maths package.\nI've committed one in r1890\n. From tfmorris on November 26, 2010 23:06:15:\nPresumably this is meant to reference issue #184\n. From tfmorris on March 03, 2012 19:20:05:\nThis is still broken in 2.5 (and SVN).  I need it to work, so I'll take a look.\n. From tfmorris on March 03, 2012 21:42:10:\nApologies for the long delay in getting this fixed.  It's repaired in r2451.\nIf you want to workaround the issue before the next release is available, you should be able to get things to work by converting the date to an ISO 8601 string yourself before attempting the load into Freebase. \nvalue.toString(\"yyyy-MM-dd'T'HH:mm:ss'Z'\")\n. From tfmorris on November 26, 2010 23:06:15:\nPresumably this is meant to reference issue #184\n. From tfmorris on November 17, 2010 05:33:08:\nFixed in both locations.\n. From musebrar...@gmail.com on November 17, 2010 05:18:45:\nAlso fails with similar error on tar.gz archive.\n. From iainsproat on November 17, 2010 10:26:39:\nThanks for the bug report.  I can reproduce this.\nIt's being caused by line 211 of com.google.RefineServlet.getTempDir calling a null variable, config.\nIt appears that com.google.RefineServlet.config, is only ever read or destroyed and doesn't seem to be assigned to.  I'd imagine it needs to be assigned to at some point in the com.google.RefineServlet.init method, but I'm not familiar enough with Servlets to be able to fix it at this moment.  Hopefully someone else can step in and fix it...(any dev want to take this task??)\n. From pjos...@gmail.com on November 17, 2010 19:14:59:\nI have run into exactly the same issue:\nRunning\nVersion 2.0\nWindows XP\nFF3.6.12\n. From dfhu...@google.com on November 20, 2010 23:08:58:\nI'll take a look.\n. From jagt...@gmail.com on December 04, 2010 09:41:46:\nI'm running Ubuntu x64 (10.10) and I get a very similar error on importing a tgz file:\njava.lang.NullPointerException\n    at com.google.refine.RefineServlet.getTempDir(RefineServlet.java:211)\n    at com.google.refine.RefineServlet.getTempFile(RefineServlet.java:220)\n    at com.google.refine.commands.project.CreateProjectCommand.save(CreateProjectCommand.java:345)\n    at com.google.refine.commands.project.CreateProjectCommand.internalImportFile(CreateProjectCommand.java:216)\n    at com.google.refine.commands.project.CreateProjectCommand.internalImport(CreateProjectCommand.java:169)\n    at com.google.refine.commands.project.CreateProjectCommand.doPost(CreateProjectCommand.java:112)\n    at com.google.refine.RefineServlet.service(RefineServlet.java:174)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n    at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n    at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n    at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n    at org.mortbay.jetty.Server.handle(Server.java:326)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:938)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\n    at java.lang.Thread.run(Thread.java:636)\n. From ehsanul...@gmail.com on December 29, 2010 07:48:44:\nGetting the same thing on Centos 5.5, and Refine 2.0:\njava.lang.NullPointerException\n    at com.google.refine.RefineServlet.getTempDir(RefineServlet.java:211)\n    at com.google.refine.RefineServlet.getTempFile(RefineServlet.java:220)\n    at com.google.refine.commands.project.CreateProjectCommand.save(CreateProjectCommand.java:345)\n    at com.google.refine.commands.project.CreateProjectCommand.internalImportFile(CreateProjectCommand.java:216)\n    at com.google.refine.commands.project.CreateProjectCommand.internalImport(CreateProjectCommand.java:169)\n    at com.google.refine.commands.project.CreateProjectCommand.doPost(CreateProjectCommand.java:112)\n    at com.google.refine.RefineServlet.service(RefineServlet.java:174)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n    at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n    at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n    at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n    at org.mortbay.jetty.Server.handle(Server.java:326)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:938)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\n    at java.lang.Thread.run(Thread.java:636)\n. From ehsanul...@gmail.com on December 29, 2010 15:25:42:\nI should add that, this issue occurs for me with .tar.gz files, zip files, .tar.bz2 files and I imagine any archive.\n. From f8f...@gmail.com on March 09, 2011 03:55:44:\nRan into the same trouble trying to load the attached file (if that can be of any help)\nVersion of refine\ngoogle-refine-2.0-r1836.zip\njava.lang.NullPointerException\n    at com.google.refine.RefineServlet.getTempDir(RefineServlet.java:211)\n    at com.google.refine.RefineServlet.getTempFile(RefineServlet.java:220)\n    at com.google.refine.commands.project.CreateProjectCommand.save(CreateProjectCommand.java:345)\n    at com.google.refine.commands.project.CreateProjectCommand.internalImportFile(CreateProjectCommand.java:216)\n    at com.google.refine.commands.project.CreateProjectCommand.internalImport(CreateProjectCommand.java:169)\n    at com.google.refine.commands.project.CreateProjectCommand.doPost(CreateProjectCommand.java:112)\n    at com.google.refine.RefineServlet.service(RefineServlet.java:174)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n    at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n    at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n    at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n    at org.mortbay.jetty.Server.handle(Server.java:326)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:938)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n    at java.lang.Thread.run(Unknown Source)\n. From sergio.b...@gmail.com on March 17, 2011 14:16:41:\nI also ran into this problem while uploading zip archive to Google Refine version 2.0, using Chrome (v10.0.648.127 beta) on OSX (v10.6.6).  Appreciate your help in resolving it. \n. From hamiltont on May 18, 2011 05:01:38:\nI was able to get the import to run successfully, but some of the data cells are corrupted. Instead of seeing the real data, I'm seeing what appears to be the filename (within the archive), with some sort of number. \nRunning Chrome 11.0.696.68 and OSX 10.6.7.  Attached is my (simplified) data file\n. From hamiltont on May 18, 2011 05:03:57:\nOops - Just noticed that *.tar is not supported, only the zipped versions (BTW, why not? If I'm working locally, then what's the point of compressing something that's about to be decompressed anyways?)  \nAnyway, I gzipped the archive and uploaded (the same data) in *.tar.gz format. This time, I got the exception that everyone else is seeing: \njava.lang.NullPointerException\n    at com.google.refine.RefineServlet.getTempDir(RefineServlet.java:211)\n    at com.google.refine.RefineServlet.getTempFile(RefineServlet.java:220)\n    at com.google.refine.commands.project.CreateProjectCommand.save(CreateProjectCommand.java:345)\n    at com.google.refine.commands.project.CreateProjectCommand.internalImportFile(CreateProjectCommand.java:216)\n    at com.google.refine.commands.project.CreateProjectCommand.internalImport(CreateProjectCommand.java:169)\n    at com.google.refine.commands.project.CreateProjectCommand.doPost(CreateProjectCommand.java:112)\n    at com.google.refine.RefineServlet.service(RefineServlet.java:174)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n    at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n    at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n    at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n    at org.mortbay.jetty.Server.handle(Server.java:326)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:938)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n    at java.lang.Thread.run(Thread.java:680)\nFile attached in case it's useful\n. From dfhu...@gmail.com on June 06, 2011 21:21:22:\nI'm going to mark this as fixed, by r1918. We'll worry about importing several files inside a single archive file later, in the new importer framework.\n. From iainsproat on June 07, 2011 07:56:25:\nIs there a new issue to track the \"importing several files inside a single archive\" feature?\n. From dfhu...@gmail.com on June 07, 2011 17:32:45:\nIain, no, but it's baked in in the new importer framework.\n. From musebrar...@gmail.com on June 07, 2011 17:38:09:\nThanks everyone!    If I understand the last comment correctly, the new importer framework will allow the use of multiple files without having to make an archive?  That's the real function I was looking for, although it still would be nice to allow \"importing several files from inside a single archive\" feature. \n. From dfhu...@gmail.com on August 14, 2011 03:16:37:\nYes, it should allow importing several files not inside an archive file, but you'd need to specify each file individually. You can't just provide a directory. This is because by operating through the web browser, Refine can't see the local file system directory and cannot list the files in a given directory.\n. From iainsproat on November 17, 2010 13:55:21:\nTony, try setting the 'Header lines' to zero.  Does that work as expected?\n. From keithwig...@gmail.com on January 07, 2011 16:37:04:\nI am experiencing the same issue. I am importing csv files with no header lines and whatever value I input for 'Header lines', Google refine continues to use the first row as the header row (I've tried 0,1,2,10,9999999).\nI have duplicated the error on two different machines using IE8.\nWindows Server 2008 64-bit\nWindows 7 64-bit\n. From tfmorris on January 07, 2011 17:44:52:\nkeithwiggins - It sounds like you are reporting exactly the opposite behavior as the original report here.  Can you open a new bug report please?\n. From tfmorris on December 20, 2011 03:53:50:\nThe problem in the original report appears to be fixed in Refine 2.5.\n. From martin.b...@gmail.com on November 17, 2010 14:12:51:\nIf you leave out step 2. the shape of the plot is correct but the labeling is still wrong.\nFollowing this step might be confusing. I used it to have the label for \"X\" on the abscissa, but since this might be caused by the wrong labeling it's not necessary to perform 2. to understand the problem.\n. From tfmorris on November 17, 2010 18:05:20:\nHow much physical memory do you have on your system?  You need to have enough to hold the entire database in memory, plus leave some room left over for Refine code, system processes, I/O buffers, etc.\nI haven't looked at how the JSON importer is structured, but you may need X * original file size + Y * result data size, which both X & Y are > 1.  ie for a 30 MB json file resulting in, say, 15 MB of data, I could easily see you needing 100 MB or more.  Having said that, 100 MB isn't very much these days and if it's actually needing more than 500 MB or so, then something definitely needs optimization.\n. From julian.r...@gmail.com on November 17, 2010 18:15:39:\n3 GB of physical memory. Even tried with all programs closed.\n. From tfmorris on November 17, 2010 18:25:37:\nThat should be plenty for a data file of this size.  How big did you make your heap?  I'd increase it to at least 1 GB (-Xmx1024m) and you can probably go to 1.5 or 2 GB without crowding the rest of your system.\nAny chance you can provide the full file to test with?\n. From iainsproat on November 17, 2010 17:45:48:\nThanks for the bug report. I can reproduce this, and will take a further look at what's going on.\n. From iainsproat on November 17, 2010 18:26:32:\nThe problem is:\n[XmlImportUtilities] No candidate elements were found in data - at least 6 similar elements are required (0ms)\nI can count more than 6  tags, so I don't think that's the issue.  It might be due to them being in a mixed element, and not getting counted correctly.\n. From chr...@gmail.com on November 17, 2010 19:07:22:\nDo you have an example of an XML file that does work?\n. From tfmorris on November 27, 2010 22:34:28:\nOne problem with each of these files is that there are characters before the initial <?xml> This is causing the parser to choke immediately.  We should probably consider trimming leading whitespace, but you can get the files imported with the existing code by deleting the initial blank line.\n. From chr...@gmail.com on November 28, 2010 19:57:08:\nthanks, that worked for me\n. From tfmorris on October 14, 2011 22:27:57:\nFixed in r2246.\n. From ron.ma...@gmail.com on November 17, 2011 16:48:15:\nI'm having a similar problem, and downloaded RC2314, but that version will only import the first record of a fairly flat XML file. \nI've enclosed a sample.  I selected the first  record of the file as the first record.\nAny suggestions?\n. From thadguidry on November 17, 2011 17:09:12:\nHmm, the schema seems a bit odd.  You have uniquely numbered elements such as   ... rather than just all of them being .  I used Refine's line-based import to pull it in and then did some text filtering and value.partition and replacing , and then exported to give you a cleaner XML file to work with.  Attached.  Does that contain all the records ? (I get 1239 of them using latest Trunk version)\n. From ron.ma...@gmail.com on November 17, 2011 17:20:47:\nThanks very much!\nUnfortunately, this is output from software I don't write, so I am unable to make changes to the schema.\nI'm just testing this out to see if Google Refine will let us prepare a report from this file.\nI guess I'll have to clean it up manually each time I want to import it.\nThanks again!\n. From thadguidry on November 17, 2011 17:38:47:\nProbably best thing is use Notepad++ or whatever text editor you have and just use a Find/Replace using regex such as  and  and then replace those with the string .  You could also create a python script to do that as part of a batch process, or if this is a constant feed process, perhaps use an ETL tool like Talend to pick up the files in a directory when they arrive and convert & clean them for later analysis in Refine.\n. From ron.ma...@gmail.com on November 17, 2011 17:41:44:\nYes, thanks!  That'll save some time rather than doing it in Refine.\nI appreciate this forum, everyone's so helpful!\n. From mitchell...@gmail.com on November 17, 2010 19:37:00:\nI was able to launch/run Google Refine only once -- on subsequent attempts, I get the 404 Error. Rebooting the PC did not solve the problem. (Windows XP Pro.)\n. From myrd...@gmail.com on November 17, 2010 21:49:32:\nI got this error too when double-clicking on the executable directly.\nTry opening a command prompt and going to the folder the app was extracted to. Then type 'refine.bat', and see if it complains about a missing JAVA_HOME environment variable.\nIf it does, you just need to create it. For example, my variable was set to \"C:\\Program Files\\Java\\jre6\". You may have to point at at a different location, depending on where Java is installed on your machine.\n. From reynolds...@gmail.com on December 19, 2010 22:06:52:\nFollowing up with myrrddin's comment, instructions for how to set the JAVA_HOME environment variable are here. http://confluence.atlassian.com/display/DOC/Setting+the+JAVA_HOME+Variable+in+Windows\nNon-developers/normal users should understand that they are not pointing at the Java Development Kit but rather at their Java installation (which in my Windows machine was in exactly the same place as myrddin's). The procedure for setting up the variable is exactly the same as for the Kit, but you're just pointing at Java.\nI had to relearn a bit of DOS to figure out how to get into refine's directory from a command line and run refine.bat. Thanks, for the memories!\n. From l.huebsc...@gmail.com on July 22, 2011 20:24:27:\nI'm experiencing the exact same issue on Debian testing with both the latest stable and development versions and it seems that something isn't properly handling directory names with characters like (in my case) a comma in it. Running it from /tmp works fine, running from /media/HOME/Downloads/binaries,source/data/google-refine-2.1 fails with the same exception.\n. From dfhu...@gmail.com on October 03, 2011 17:44:57:\nThad tested today and said that this had been fixed, presumably by the fix to issue #258.\n. From d.kloost...@bg.fnv.nl on October 19, 2011 12:42:32:\nI just tried to install google refine 2.1-r2136 on windows xp pro, but i'm afraid I got the same problem, including the macros.vm not found message. The solution in comment 4 did not solve the problem unfortunately.\nI also tried installing google refine 2.5-rc1-r2314 but ran into the same problem (also with the unable to find macros.vm message)\nThanks\n. From thadguidry on October 19, 2011 14:04:49:\nThat is a Velocity tools error when it mentions macros.vm.  So, it cannot find macros.vm in one of the Extension folders...Why ? Hmmm, maybe a Anti-Virus program picked it up and put it in quarantine, can you check that ?  Also, can you try giving us a bit on your Environment settings ?\n1. Open a command window.\n2. Type PATH and hit Enter, then copy and paste (mark) the results in a reply here.\n3. Type SET and hit Enter, then copy and paste (mark) the results in the same reply.\n4. Type java -version and hit Enter, then copy and paste (mark) the results in same reply.\nThanks - Thad\n. From joe.benn...@cascadeng.com on September 05, 2012 01:36:55:\nHad the same problem with my install on a WIN7 64bit. Just needed to use C:\\Program Files (x86)\\Java\\jre7\n. From dfhu...@gmail.com on October 03, 2011 17:44:57:\nThad tested today and said that this had been fixed, presumably by the fix to issue #258.\n. From thadguidry on November 17, 2010 18:13:08:\nForgot to also note that I had 2 projects open.  The other project was left in a state of having only a single text facet for an address1 field column. (which I was visually using to compare against this project in the screenshot.\n. From dfhu...@gmail.com on November 17, 2010 22:58:07:\nDoes it happen for other kinds of files? (It's fine for me.) How about on another computer?\n. From myrd...@gmail.com on November 17, 2010 23:16:45:\nThis is the first file I've attempted to open, on the first computer I've installed it on. So I don't know. It is the second VERSION of the file... the pre-anonymized version did the same thing; the only difference between the files was the contents of the first four columns.\n. From dfhu...@gmail.com on November 17, 2010 23:25:46:\nThat's really odd. Those Java classes should be in the same Jar.\nAre you comfortable with programming tools? If so, try the development version:\nhttp://code.google.com/p/google-refine/wiki/GetDevelopmentVersion\n. From dfhu...@gmail.com on November 17, 2010 23:05:31:\nDoes a new browser tab get opened when you invoke an export command?\n. From aidan.wa...@gmail.com on November 18, 2010 07:41:31:\nNo when I click on 'Export', then 'Excel' nothing happens. I think it stems from something earlier in the program. When I start up Refine I have an 'Error in Page':\nMessage: 'GoogleRefineReleases' is undefined\nLine: 632\nChar: 5\nCode: 0\nURI: http://127.0.0.1:3333/index-bundle.js\nHowever the program seems to function OK but when I get to the export functionality it doesn't. I researched if this is a java script issue without success.\n. From aidan.wa...@gmail.com on November 18, 2010 07:49:12:\nFurther information. It works OK using Firefox.\n. From tfmorris on November 26, 2010 22:59:08:\nI've confirmed this with IE 8.  The error that I got was\nMessage: Invalid argument.\nLine: 4448\nChar: 5\nCode: 0\nURI: http://127.0.0.1:3333/project-bundle.js\nwhich appears to be this line:\nwindow.open(\"about:blank\", \"refine-export\");\nin ExporterManager.handlers.exportRows\nUser Agent: Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.04506.30; .NET CLR 3.0.04506.648; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)\nTimestamp: Fri, 26 Nov 2010 22:53:18 UTC\n. From tfmorris on November 27, 2010 01:29:35:\nClicking the edit button on a cell throws a Not Implemented error at Line: 7768 Char: 5 in project-bundle.js which appears to be here:\nDataTableCellUI.prototype._startEdit = function(elmt) {\n    self = this;  // <------------------ this line\nLooks like Internet Explorer support needs some work.\n. From chima...@gmail.com on March 20, 2011 00:24:11:\nThe problem with Export is that IE does not allow arbitrary values for the second argument of window.open (e.g., see discussion at http://stackoverflow.com/questions/710756/ie8-var-w-window-open-message-invalid-argument).  Changing the value from \"refine-export\" to \"blank\" should in theory fix this problem for IE.\n. _From tfmorris on September 18, 2012 19:50:24:\nInternet Explorer support was improved in Refine 2.5.  Can you retest and let us know if this issue still exists?\n. From tfmorris on November 26, 2010 22:20:13:\nI've fixed a few things here:\n- project creation will no longer leave the encoding unset (the cause of your NullPointerExceptions)\n- I've lowered the minimum confidence threshold from 50 to 20 for the character set guesser.\nUltimately I think we should be allowing the user more control over the character encoding.\nNote that the reinterpret() function won't actually do what you desire.  You want to use something along the lines of value.replace(' ',' ') [Where the first literal contains a NBSP]\nFor anyone else who's attempting to reproduce this, if your system's default character encoding is UTF-8, as mine is, you won't even get as far as Thad.  Instead you'll end up with all the non-breaking spaces substituted with the replacement character (because the ISO Latin-1 NBSP character is invalid UTF-8).  No amount of reencoding will save you at that point.\n. From tfmorris on November 26, 2010 22:25:15:\nFixed in rev 1931.\n. From tfmorris on November 27, 2010 00:38:27:\nissue #164 has been merged into this issue.\n. From tfmorris on May 25, 2011 05:23:13:\nissue #386 has been merged into this issue.\n. From thadguidry on October 21, 2011 22:09:30:\nOK, those supported encodings SHOULD work, however they do not with reinterpret() function.  We have that issue logged, but I would really like to see that fixed. NOW.  Before we release 2.5\nA simple test such as:\n\"Can we fix this?!?\".toString().escape(\"html\")  WORKS :)\n\"Can we fix this?!?\".toString().reinterpret(\"utf-8\") Error: reinterpret: encoding 'utf-8' is not available or recognized.  FAILS :(\nreinterpret(\"Can we fix this?!?\",\"utf-8\")  Error: reinterpret: encoding 'utf-8' is not available or recognized.  FAILS :(\n$10 Paypal bucks for the person who fixes this first, from me !\n. From tfmorris on October 21, 2011 22:53:02:\nAll three examples work without error on my Ubuntu system when testing against SVN trunk.\nWhat O/S are you using?  Does the problem affect only utf-8 or all encodings?  Have you tried any variations such as \"UTF-8\" or \"UTF8\" ?\nI'll boot Windows to check it there after I've finished up some other stuff.\n. From thadguidry on October 21, 2011 23:11:58:\nMicrosoft Windows [Version 6.1.7601]\nCopyright (c) 2009 Microsoft Corporation.  All rights reserved.\nC:\\Windows\\system32>java -version\njava version \"1.7.0\"\nJava(TM) SE Runtime Environment (build 1.7.0-b147)\nJava HotSpot(TM) 64-Bit Server VM (build 21.0-b17, mixed mode)\nYes, tried \"ascii\" \"us-ascii\" \"US-ASCII\" \"UTF8\" utf8\" \"UTF-8\" \"utf-8\" \"latin-1\" \"LATIN-1\" \"BIG5\" \"big5\"\n. From thadguidry on October 21, 2011 23:14:32:\nWindows 7 64 bit\n. From tfmorris on October 26, 2011 23:43:01:\nBoth cases labelled as failing also work on Windows XP with a Sun Java 1.6.0 JVM.\nThere are only a couple of things which come to mind as possibilities:\n1. It's a Java 7 specific bug, although that seems like a pretty big thing for them to have broken and not noticed.\n2. The encoding stored in the project is messed up (perhaps an old project from back when the encoding could be null due to a bug).  There are two character encodings involved in this operation, the source encoding and the destination encoding, so it might not be the \"utf-8\" which is the problem.\nI suggest that we move the discussion someplace other than this bug report (the dev list?) since I'm pretty convinced it's not a regression of this bug fix.\n. From thadguidry on October 26, 2011 23:48:59:\nAgreed, push this up to the dev list so we can talk and test the crap out of this.  It is really bugging me.  I do have my JAVA_HOME path set to 1.6.24 version, btw.\n. From tfmorris on November 18, 2011 23:35:41:\nI'm fairly convinced that the underlying problem in comment 6 is that the project's encoding isn't set properly.  I've created a new issue #486 to track this.\n. From tfmorris on November 27, 2010 00:38:27:\nissue #164 has been merged into this issue.\n. From tfmorris on May 25, 2011 05:23:13:\nissue #386 has been merged into this issue.\n. From tfmorris on November 18, 2011 23:35:41:\nI'm fairly convinced that the underlying problem in comment 6 is that the project's encoding isn't set properly.  I've created a new issue #486 to track this.\n. From KarenaBibbins@gmail.com on November 18, 2010 15:50:39:\nForgot to mention I am using Windows XP\n. From iainsproat on November 18, 2010 17:21:00:\nKarena,\nCould you copy & paste the log (the program notes) here, or take a screenshot and attach it here?\nThanks\n. From tfmorris on September 18, 2012 19:31:38:\nNo response from user.  Closing.\n. From thadguidry on November 19, 2010 21:00:08:\nPaul, is your need similar or the exact same requirement as Issue-68 with a simple UI to merge 2 columns with perhaps a specific character of your choosing ?\n. From paulschreiber on November 19, 2010 22:13:43:\nYes, as long as you can merge more than two columns.\n. From tfmorris on March 03, 2012 19:17:35:\nPlease use the mailing list for questions.  The issues database is for bug reports and enhancement requests.\nFor your first example, you could use \"Add a new column based on this column\" based on the Year column and use an expression similar to\n(value+\"-\"+[\"01\",\"04\",\"07\",\"10\"][(cells['Period'][1]).toNumber()]+\"-01\").toDate()\nThis basically takes the quarter number and uses it to look up into a little array of month numbers and then convert the whole string to a date.\n. From thadguidry on November 19, 2010 20:21:05:\nYeah, I have to agree, we probably only want to execute after clicking an APPLY button. (this gets my vote)\n. From thadguidry on November 20, 2010 04:24:36:\nDo you have 64bit Java installed and also set as your default ?  In other words, Java_Home env variable ?  You'll need 64bit Java in order to go beyond 3.5 GB RAM usage.  See attached screenshot.\n. From leebel...@gmail.com on November 20, 2010 09:03:01:\nAddressed the x64 Java issue, set Xmx5120 and tried again - after 5 minutes of loading, then got\nHTTP ERROR 500\nProblem accessing /command/core/create-project-from-upload. Reason:\nGC overhead limit exceeded\nCaused by:\njava.lang.OutOfMemoryError: GC overhead limit exceeded\n    at au.com.bytecode.opencsv.CSVParser.parseLine(Unknown Source)\n    at au.com.bytecode.opencsv.CSVParser.parseLineMulti(Unknown Source)\n    at com.google.refine.importers.TsvCsvImporter.getCells(TsvCsvImporter.java:196)\n    at com.google.refine.importers.TsvCsvImporter.read(TsvCsvImporter.java:163)\n    at com.google.refine.importers.TsvCsvImporter.read(TsvCsvImporter.java:74)\n. From tfmorris on November 20, 2010 15:11:17:\nIs that a typo in your comment or did you really not have a trailing 'M' on your size?  If you used 5120 instead of 5120M, you probably set your heap to 5120 bytes or perhaps 5120 KBytes, neither of which will work very well.\n. From thadguidry on November 20, 2010 15:23:32:\nAlso...look at the 2nd line in your command window when you start refine with:\nC:\\your_path_to_refine.bat_file\\refine /m 5132m\nThe 2nd line in command window log should show how much memory was successfully executed to the Java.exe process.\nAnother thing to look at is start Task Manager, click on Processes tab, and look at how much memory is being utilized for Java.exe process.\nFinally double check things at our [FaqAllocateMoreMemory]\n. From leebel...@gmail.com on November 21, 2010 05:32:12:\nI actually did have \"Xmx5120m\". I then did another run using:\nrefine.bat /m 6000m >refine.out\nand, after about 30 minutes got\nHTTP ERROR 500\nProblem accessing /command/core/create-project-from-upload. Reason:\nJava heap space\nCaused by:\njava.lang.OutOfMemoryError: Java heap space\nThe 'Peak working set memory' got to around 6295076K. I've attched the refine.out file. Any help would be appreciated (and thank you for the prompt help that you have given so far - it is appreciated)\n. From thadguidry on November 21, 2010 05:52:43:\nCan you file split spatialinfo2.csv and try again ?  That's a BIG file.  I've tested with a 1 GB file before and 4 columns, but the data was interspersed and so Refine absorbed it in about 10 mins.  Your file on the other hand, might just be TOO BIG for the current architecture.  Anyone else have ideas for this fellow ?\n. From leebel...@gmail.com on November 21, 2010 08:28:14:\nYes, sorry it is a big file: 17 million records by 4 columns. It can split it, but it makes the analysis considerably more complicated. Refine looked like THE ideal way to analyse these records.\nThe file contains the complete location records of all species occurrences in the Australian region for the Atlas of Living Australia (www.ala.org.au: I am the Spatial Data Manager). Column 1 and 2 are latitude longitude in decimal degrees. Column 3 is spatial accuracy (numeric, text and both) and column 4 is text description of location. \nI am trying to analyse all the variations of column 3, and in conjunction with columns 1,2 and 4, can develop an estimate for spatial uncertainty.\nNeedless to say, any ideas would be greatly appreciated. I certainly have greatly appreciated your help on this one. \nI went out Saturday to purchase 4 x 2gb memory sticks in the hope that 8GB would suffice. I did BTW have 1GB setup as paging on an SDD. I will try to see if 6500mb gets me closer.\n. From tfmorris on November 21, 2010 12:43:54:\nThat should be plenty of memory for this case unless Refine is being grossly inefficient or the text descriptions are enormous.  What is the total raw (uncompressed) size of the input data?\nThad - for your 1M row case, what was the size of the input data and what was the resulting virtual size of the Refine process?  It will vary by data type, but I'd expect memory usage to be basically linear in this range (1M-17M rows).\n. From thadguidry on November 21, 2010 17:48:15:\nMy test filesize was 1 GB on disk.  1 million rows, interspersed data\nalong 4 columns - it was an injection of the NFDC data, so my column 1\nwas REALLY long, like 800 chars at times I recall, 20% blanks in\ncolumns 3 & 4.\nThe virtual size of the Refine (Gridworks) NFDC test project came out\nto around 350 - 400 MB. hmm...maybe the blanks helped reduce that\nhere?\nAfter 10 minutes of importing (back in 1.1 days) my Java.exe. process\npeaked to 800 MB in Windows7 using Java64bit and 8 GB Ram for heap.\nDuring my initial testing...the automatic saving project to disk was a\nbit too aggressive and David tuned it a bit in Issue-3.\nI'm thinking that he could probably use Rapid Miner instead to handle\nhis analysis.  It is a good match for doing exactly that kind of\nanalysis as well.  You might want to download it and give it a try.\nWe have the link under RelatedSoftware on wiki.\nStill, I think that we probably need to go back and really test\nRefine's memory utilization (post 1.1) to make sure that it is within\nparameters still.  I haven't done it to that capacity in a while.\n. From leebel...@gmail.com on November 21, 2010 20:39:39:\nThanks for the reference to Rapid Miner. I will take a look. But I hope you are not admitting defeat for Refine on my data :) I'd be happy for you guys to take a look at our data as a test case. To me, it looked like a classic fit for Refine.\nThe zipped file is 40mb (attachment limit is 10mb) so I've put it here: http://dl.dropbox.com/u/8650868/spatialinfo2.zip. \nPlease let me know when you have it (or if you don't want it). Thanks again for your support on this issue. Impressive.\n. From thadguidry on November 22, 2010 00:08:38:\nI downloaded it.  Sure enough, so far I get the same results you do which is unfortunate.  Using Refine /m 6144m and Java.exe climbed to 6545m usage and seemed to progress well until it got to 66% uploading complete, then tanked and rapidly swelled to 100% complete and the Error 500 Java heap space all within 5 mins.\nThanks we'll investigate more and let you know. (diving into Profiling now)\n. From thadguidry on November 22, 2010 01:15:13:\nwith JAVA_OPTIONS=\"-XX:-UseParallelGC\" and 8144m got to 82% and then\njava.lang.OutOfMemoryError: Java heap space\n    at java.lang.AbstractStringBuilder.(AbstractStringBuilder.java:45)\n    at java.lang.StringBuilder.(StringBuilder.java:80)\n    at au.com.bytecode.opencsv.CSVParser.parseLine(Unknown Source)\n    at au.com.bytecode.opencsv.CSVParser.parseLineMulti(Unknown Source)\n    at com.google.refine.importers.TsvCsvImporter.getCells(TsvCsvImporter.java:196)\n    at com.google.refine.importers.TsvCsvImporter.read(TsvCsvImporter.java:163)\n    at com.google.refine.importers.TsvCsvImporter.read(TsvCsvImporter.java:74)\n    at com.google.refine.commands.project.CreateProjectCommand.internalInvokeImporter(CreateProjectCommand.java:478)\n    at com.google.refine.commands.project.CreateProjectCommand.load(CreateProjectCommand.java:341)\n    at com.google.refine.commands.project.CreateProjectCommand.internalImportFile(CreateProjectCommand.java:327)\n    at com.google.refine.commands.project.CreateProjectCommand.internalImport(CreateProjectCommand.java:169)\n    at com.google.refine.commands.project.CreateProjectCommand.doPost(CreateProjectCommand.java:112)\n    at com.google.refine.RefineServlet.service(RefineServlet.java:170)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n    at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n    at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n    at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n    at org.mortbay.jetty.Server.handle(Server.java:326)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:938)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n. From thadguidry on November 22, 2010 01:41:55:\nSwitching testing on my Win7 system to JDK 6u21 and enabling GC1 experimental for differencing profile.\n. From thadguidry on November 22, 2010 04:29:06:\nIssue appears to be within CSVParser or the wiring to cells.  Refine was not originally designed to handle more than 100,000 rows.  This will require a revisit with possible underlying architecture changes in later revisions. (in other words, we're admitting defeat with spending more time working on the issue until we can devote more time to thinking through the architecture redesign to handle larger datasets such as this)\nThanks again for trying Google Refine and don't lose hope, we'll get there I'm sure. Like Tom says, this file should be easy cheesy.  For instance, I was able to fully open your .csv file in Notepad++ with it taking only about 600MB of memory, so it should be possible in Refine as well in theory.  Just need time to track down the bugs (yet discovered) it revolves around.\n. From leebel...@gmail.com on November 22, 2010 05:12:38:\nThanks Guys. Appreciate the work you put it. It would have been nice to Refine the file, but if it can help identify and address issues, not a total loss.\n. From tfmorris on November 22, 2010 05:53:51:\nI'm getting a 404 from the dropbox url.  Was it a one time download?\nThad - since you have the only copy, can you post the memory profile and the results of your debugging?\n. From leebel...@gmail.com on November 22, 2010 06:03:59:\nSorry, thought you had finished with it. I've put it back up at http://dl.dropbox.com/u/8650868/spatialinfo2.zip \n. From tfmorris on November 23, 2010 06:10:06:\nThanks for putting it back.  I've got my copy, although Stefano or David may want one as well.  We can probably arrange to share among the team members if you want to take your copy down.\nFrom the back of the envelope calculations that I did, assuming that the file is relatively homogeneous, you're looking at heap requirements of over 8GB to import the whole file.  If you set your max heap size to, say 9 or 10 GB, and had a sufficiently sized page file, you should be able to get it imported, but you'll be hitting disk for paging with every pass through the data, which could put a significant damper on performance (really depends on the access characteristics of the algorithms that you end up using for your analysis, although I'd assume the vast majority of them are just linear sweeps through the rows).\nA typical row contains four cells: two doubles, an empty cell, and an 88 character string, totaling 480 bytes (on a 64-bit machine, it'll be slightly less on a 32-bit processor).\nI think we can probably do better than this, but for now that's what you're dealing with...\n. From leebel...@gmail.com on November 23, 2010 06:44:26:\nThanks. I took down the file again but happy to put it back up if you need it. I looked at RapidMiner but it's way too broad a system for me to get into for this one application.\nSo, I split the file in half and got the first half into Refine with no problems. Now I'm starting to come to grips with it. Even in half, many operations take a while - but that is AOK with me. Slow is fine, busted is something else.\nThanks again for your support with this one!\n. From tfmorris on January 07, 2011 16:28:38:\nI've updated the header to align better with the actual issue.  I'm not sure it's something that's fixable, but I'll leave it open as a data point for some future person working on memory performance optimization.\n. From leebel...@gmail.com on January 09, 2011 21:18:08:\nThanks. When I get time, I'm still plugging away using half the file (using \"refine /m 6000m\"): Refine is a very neat tool.\n. From dfhu...@gmail.com on March 11, 2011 07:48:21:\nissue #346 has been merged into this issue.\n. From dylan.o....@gmail.com on June 28, 2012 08:30:37:\nHello,\nI am also a biologist working with large files, and I have the same issue that is discussed above when trying to load a 3.0G file in Refine. I think that my database contains many, many more cells than the example above. That is, many more columns but fewer rows. Haas there been any progress on fixing this bug since March? Thanks!\n. From thadguidry on June 28, 2012 15:30:16:\nDylan,\nYou may want to look at Taverna http://www.taverna.org.uk/ for your specific needs instead.\n. From thadguidry on November 21, 2010 17:48:15:\nMy test filesize was 1 GB on disk.  1 million rows, interspersed data\nalong 4 columns - it was an injection of the NFDC data, so my column 1\nwas REALLY long, like 800 chars at times I recall, 20% blanks in\ncolumns 3 & 4.\nThe virtual size of the Refine (Gridworks) NFDC test project came out\nto around 350 - 400 MB. hmm...maybe the blanks helped reduce that\nhere?\nAfter 10 minutes of importing (back in 1.1 days) my Java.exe. process\npeaked to 800 MB in Windows7 using Java64bit and 8 GB Ram for heap.\nDuring my initial testing...the automatic saving project to disk was a\nbit too aggressive and David tuned it a bit in Issue-3.\nI'm thinking that he could probably use Rapid Miner instead to handle\nhis analysis.  It is a good match for doing exactly that kind of\nanalysis as well.  You might want to download it and give it a try.\nWe have the link under RelatedSoftware on wiki.\nStill, I think that we probably need to go back and really test\nRefine's memory utilization (post 1.1) to make sure that it is within\nparameters still.  I haven't done it to that capacity in a while.\n. From thadguidry on November 22, 2010 00:08:38:\nI downloaded it.  Sure enough, so far I get the same results you do which is unfortunate.  Using Refine /m 6144m and Java.exe climbed to 6545m usage and seemed to progress well until it got to 66% uploading complete, then tanked and rapidly swelled to 100% complete and the Error 500 Java heap space all within 5 mins.\nThanks we'll investigate more and let you know. (diving into Profiling now)\n. From thadguidry on November 22, 2010 01:15:13:\nwith JAVA_OPTIONS=\"-XX:-UseParallelGC\" and 8144m got to 82% and then\njava.lang.OutOfMemoryError: Java heap space\n    at java.lang.AbstractStringBuilder.(AbstractStringBuilder.java:45)\n    at java.lang.StringBuilder.(StringBuilder.java:80)\n    at au.com.bytecode.opencsv.CSVParser.parseLine(Unknown Source)\n    at au.com.bytecode.opencsv.CSVParser.parseLineMulti(Unknown Source)\n    at com.google.refine.importers.TsvCsvImporter.getCells(TsvCsvImporter.java:196)\n    at com.google.refine.importers.TsvCsvImporter.read(TsvCsvImporter.java:163)\n    at com.google.refine.importers.TsvCsvImporter.read(TsvCsvImporter.java:74)\n    at com.google.refine.commands.project.CreateProjectCommand.internalInvokeImporter(CreateProjectCommand.java:478)\n    at com.google.refine.commands.project.CreateProjectCommand.load(CreateProjectCommand.java:341)\n    at com.google.refine.commands.project.CreateProjectCommand.internalImportFile(CreateProjectCommand.java:327)\n    at com.google.refine.commands.project.CreateProjectCommand.internalImport(CreateProjectCommand.java:169)\n    at com.google.refine.commands.project.CreateProjectCommand.doPost(CreateProjectCommand.java:112)\n    at com.google.refine.RefineServlet.service(RefineServlet.java:170)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n    at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n    at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n    at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n    at org.mortbay.jetty.Server.handle(Server.java:326)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:938)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n. From dfhu...@gmail.com on March 11, 2011 07:48:21:\nissue #346 has been merged into this issue.\n. From thadguidry on November 20, 2010 17:11:20:\nYou might try, Edit Cells --> Common Transforms --> To text\nor\nbefore you create your project, try un-checking \"Auto-detect value types\"\nthen try your splitByLength again.\n. From tfmorris on November 27, 2010 02:18:47:\nNumbers are stored as Doubles (ie floating point numbers).  By default when they are converted to strings, if they are less than .001 or greater than or equal to 10^7, then it is represented in so-called \"computerized scientific notation.\"\nIf you want to treat a string of digits as a string, you should make sure that they don't get turned into a number, by turning off auto-detect, as Thad suggested.  Once that conversion has been done, it's not necessarily reversable (particularly for large numbers or long strings of digits).\nIf someone needs to control floating point number formatting for some reason, please feel free to open an enhancement request with your use case, but for this particular case, I'm going to close this issue.\n. From iainsproat on November 23, 2010 13:44:21:\nDetails can be found in the wiki: http://code.google.com/p/google-refine/wiki/StrippingHTML\n. From thadguidry on January 09, 2011 20:22:45:\nUploading more Version 2 Schema Alignment screenshots for use in wiki help pages.\n. From iainsproat on November 22, 2010 18:05:16:\nThis seems to be an issue with Google's Translate service, rather than with Refine.  I get similar results from the service when I put text directly into the Google Translate web site http://translate.google.com\nYou will get a better response to this issue on the Google Translate discussion list http://groups.google.com/group/google-translate\n. From iainsproat on November 22, 2010 18:10:08:\nYou can get full addresses and other geocoded data from Google's Map API.  http://code.google.com/apis/maps/index.html\nYou can then use Refine's fetch from Web Service feature to call the Google Maps API. The documentation for this feature is here: http://code.google.com/p/google-refine/wiki/FetchingURLsFromWebServices\n. From tfmorris on November 23, 2010 00:03:44:\nI'm not following what you are requesting here.  If your field separator is included in your data, your only choices are to: 1) quote the contents of the field, 2) choose a different separator or 3) escape the separator.  Spreadsheet and database tools typically go with solution #  1.  ie for your data, output \"Di Gregorio\" and \"Smith\"\nWhat solution would you like to see?\n. From dfhu...@gmail.com on November 25, 2010 17:47:44:\nFor postal code, looking at this example\nhttp://maps.google.com/maps/api/geocode/json?sensor=false&address=77+Massachusetts+Ave,+Cambridge,+MA\nI'd suggest\nwith(value.parseJson().results[0], r, filter(r.address_components, c, c.types[0] == \"postal_code\")[0].long_name)\nThen for street name, change \"postal_code\" to \"route\".\n. Just a question which was already answered.  Closing.\n. From conley...@gmail.com on November 24, 2010 20:12:31:\nI meant to classify this as an enhancement, not a defect, but don't see a way to edit it.\n. From dfhu...@gmail.com on September 11, 2011 05:31:59:\nIf there's no pattern to the spaces and tabs, it'd be hard to split the columns. I'd recommend importing the data file without splitting into columns, then replace runs of spaces or tabs with some characters, e.g.,\nvalue.replace(/\\s{2,}/, '|')\nThen invoke the Split into several columns command.\n. From tfmorris on November 26, 2010 19:07:51:\nWhat character encoding is used in your original file?  Can you provide a sample?\nOne thing you might try is to use the reinterpret function.  If, for example, your original file is UTF-8 encoded and Refine incorrectly guessed it was ISO 8859-1, you could use Cells->Transform and the expression \nvalue.reinterpret(\"utf-8\")\nto fix things up again.\n. From tfmorris on January 09, 2011 06:19:35:\nUnless we can get additional information, we'll have to close this.  If you're still having the problem please help us fix it for you by providing some more info.\n. From tfmorris on February 04, 2011 02:20:06:\nNo response to request for information.\n. From nippo...@gmail.com on February 04, 2011 07:37:05:\nI Will send you a file\n. From tfmorris on February 04, 2011 14:29:50:\nReopening pending receipt of additional information.\n. From francisc...@gmail.com on May 20, 2011 16:10:28:\nmay or may not be related to this, but when i try to reinterpret i get the following error:\nError: reinterpret: encoding 'utf-8' is not available or recognized.\n. From tfmorris on May 20, 2011 17:05:11:\nNever received any data from the original reporter.  Closing again as unreproduceable.\n@francisc - Please post a query on the email list with what you're attempting to do (or open a new bug report if you're sure it's a bug)\n. From txarlieg...@gmail.com on August 02, 2012 08:36:37:\nI am also trying to work with \u00e1, \u00e9, \u00f1 and other characters (Spanish alphabet). \nUnfortunatelly Value.reinterpret(\"utf-8\") does not works :(\nFor example: inform\u00c3\u00a1tico should be inform\u00e1tico, this is \"\u00c3\u00a1\" should be \"\u00e1\". I also s\u00ed this one: \"\u00c3\ufffd\". It should show \"\u00c1\". \"\u00c3\u00b3 should be \"\u00f3\". \"\u00c3\u00ada\" should be \"\u00ed\", \"\u00c3\u00b1\" should be \"\u00f1\", and so on. \nThe original file contains correct characters (it is encoded under UTF8). \nAny help will be welcome. Thanks in advance.\n. From txarlieg...@gmail.com on August 02, 2012 08:38:57:\nI have just found a trick to avoid those bad characters: try copying the content to the clipboard, and the create de project in Google-Refine from the clipboard (instead of uploading the file). It works.\n. From thadguidry on August 02, 2012 13:11:22:\ntxarliegarcia,\nYou can change the encoding at the beginning of the import process, which overides the guesser at that point...we provide a button for that, so simply choose the UTF8 encoding from the list that displays.  Like Tom says up-comment, you can also change the encoding AFTER the import process within the data grid using the reinterpret() GREL function.  Use our mailing list for help & questions, this is issue tracking, not a general help forum.\n. From FrH...@gmail.com on August 24, 2012 08:51:49:\nHas there been any update yet? This enhancement would definitely help!\n. From tfmorris on August 24, 2012 14:05:00:\nUpdates are usually posted to the issue, so if you don't see anything here, there's usually no news to report.\nAnyone have suggestions for where to fit this into the UI?\n. From tfmorris on October 08, 2011 21:33:04:\nI think not having a Cell object for a cell with a null value could be considered a useful memory optimization.  \nWould it be acceptable to you to have cells.colName.value return null instead of an error, rather than wasting the space by allocating Cell objects for all the blank cells?\n. From jcreena...@gmail.com on October 09, 2011 12:07:55:\nYeah that sounds fine. The main point is that cells.colName.value returns null instead of an error; I'm not worried about how that is accomplished.\n. From iainsproat on November 25, 2010 12:36:41:\nThis sounds like it may be due to an agressive browser cache (I've run into similar things in the past).  Could you try using shift+refresh in your browser when you run Refine?  http://lifehacker.com/5574852/shift%252Brefresh-is-like-the-restart-button-for-web-sites\n. From hywelm.j...@gmail.com on November 25, 2010 14:05:41:\nWorked a treat. I feel like an idiot for not thinking of it myself. Thanks a lot.\n. From iainsproat on November 25, 2010 12:36:41:\nThis sounds like it may be due to an agressive browser cache (I've run into similar things in the past).  Could you try using shift+refresh in your browser when you run Refine?  http://lifehacker.com/5574852/shift%252Brefresh-is-like-the-restart-button-for-web-sites\n. From iainsproat on November 25, 2010 14:10:30:\nI assume you are talking about the Google Translate API you can call from within Refine?\nI would suggest asking the Google Translate discussion list: http://groups.google.com/group/google-translate\n. From r...@pyramiddata.co.uk on November 25, 2010 14:47:59:\nThank you\n. From dfhu...@gmail.com on November 26, 2010 17:56:30:\nPlease send questions to the mailing list (google-refine@googlegroups.com). This issues list is for bugs and feature requests.\n. From tfmorris on November 26, 2010 17:50:29:\nWhat would you like to see?  An option to assign fixed column names (e.g. col1, col2, etc) on import perhaps?\n. From dfhu...@gmail.com on November 26, 2010 17:52:37:\nMaybe specify \"0\" for the number of header lines before importing?\n. From josephmd...@gmail.com on November 29, 2010 01:36:49:\nIs this possible?\n- Always generate fixed column names (col1, col2\u2026)\n- Add a concept of display names to columns (a second name that the user will see)\n- Rename operations change the display name\n- Never change fixed names\n- Use fixed names for JSON and all internal operations\nI expect such a setup would fix issue #133 too.\n. From dfhu...@gmail.com on November 29, 2010 05:47:54:\n\n\nAlways generate fixed column names (col1, col2\u2026)\n\n\nThis already happens if you specify \"0\" as the number of header lines. Does that not happen for you?\n. From josephmd...@gmail.com on November 29, 2010 10:16:43:\nYou are right. Setting \"0\" as the number of header lines is the way to go.\nI missed that option. I read the checkbox text above it and completely skipped that option thinking it was extra details for the checkbox.\nIt looks like this issue is invalid. Sorry.\n. From josephmd...@gmail.com on November 29, 2010 01:36:49:\nIs this possible?\n- Always generate fixed column names (col1, col2\u2026)\n- Add a concept of display names to columns (a second name that the user will see)\n- Rename operations change the display name\n- Never change fixed names\n- Use fixed names for JSON and all internal operations\nI expect such a setup would fix issue #133 too.\n. From tfmorris on November 27, 2010 01:57:22:\nI've fixed one possible cause, but I was unable to reproduce the problem first.  My thesis is that the makeKey method was falling through and returning the original String instead of a Date.\n. From thadguidry on August 26, 2011 16:24:24:\nI have a similar case with an import of an initial JSON file, and the column did show green numbers, except a few where \"null\" values.  The \"null\" strings values in the Freebase \"type\" namespace I used edit in the facet by text to blankout the \"null\" values to nothing.  When I ran Sort, and choose \"numbers\" radio button, things kinda froze and I have attached a log showing the exception in this particular case.  The JSON file was the actual schema dump of /common types in Freebase with the number value being the /freebase/type_profile/instance_count just as in the query [1].  I then choose to transform cells to numbers in that column, and then the Sort by numbers worked fine after that.\nSo, I guess it would be damn skippy happy, to have a warning to the user that \"hey you have still some string values in this column, you might want to choose transform cells to numbers, and run this sort again. You can always undo, if we got this assumption wrong.\"\nIt was a general lockup that could be avoided, I think, with more error handling and warnings to the user.\n1.\n{\n  \"id\":      \"/\",\n  \"type\":    [],\n  \"name\":    null,\n  \"creator\": null,\n  \"/type/namespace/keys\": [{\n    \"value\":  null,\n    \"namespace\": {\n      \"id\":            null,\n      \"name\":          null,\n      \"type\":          [],\n      \"timestamp\":     null,\n      \"/freebase/domain_profile/hidden\": null,\n      \"/type/domain/types\": [{\n        \"id\":            null,\n        \"/freebase/type_profile/instance_count\": null,\n        \"limit\":         1000\n      }]\n    }\n  }]\n}\u200b\n. From tfmorris on December 05, 2010 17:53:08:\nAn approach which doesn't require code changes (and also makes your environment less confusing since everything will match) is to add $JAVA_HOME/bin to the appropriate place in your shell's path.\n. From fgr...@gmail.com on December 05, 2010 18:57:16:\nI'm sorry for a confusing bug report. The real problem is that the present script ignores JAVA_HOME if \"which java\" returns non-zero. \n. From fgr...@gmail.com on December 05, 2010 19:01:17:\nBut, if you don't want to change the logic flow, the error message should be changed, to tell the user to add java 1.6+ to their path instead of telling them to change $JAVA_HOME\n. From stefa...@google.com on December 24, 2010 21:17:44:\nfixed in trunk at r1957.\n. From tfmorris on March 03, 2012 22:18:44:\nNumbers with embedded commas are ambiguous because the comma could represent a thousands separator (U.S.) or a decimal point (Europe).\nThe problem with replace() not working I haven't been able to reproduce with the current release.  Note that if you want to convert to numbers, you can do it all a single pass using value.replace(',','').toNumber() as your expression.\n. From sp1d...@gmail.com on March 03, 2012 23:23:13:\nGive me a break. That's not a solution. We are both from the USA, Tom. It should be fairly unambiguous.\n. From iainsproat on November 27, 2010 20:11:21:\nThe importer doesn't assume that the root contains the array of elements to be imported.  It instead looks for a the lowest branch with 6 or more repeated elements.  It should probably be changed so that if it doesn't find 6+ repeated elements, it imports the elements immediately below the root element.\n. From tfmorris on November 27, 2010 22:00:19:\nI'm not sure I understand why this is considered an enhancement.  Is reporting errors solely to the (hidden) console, not considered a bug?\n. From tfmorris on October 21, 2011 15:51:11:\nThe new importer UI provides the user with interactive record selection mechanism for the XML importer.\n. From sande...@gmail.com on November 27, 2010 23:12:16:\nThis is an enhancement, not a defect, but I can't find where to change the type.\n. From tfmorris on November 28, 2010 00:41:57:\nThe way the bug system is set up now, I think you have to wait for a developer to change it from a bug to an enhancement request.  All set now...\n. From tfmorris on November 28, 2010 18:12:37:\nIs it?  Running, that is.  Do you have something else running which is responding to HTTP requests on port 3333?\nCan you run the following two commands in a shell and post the output?\nps ax | grep refine\nnetstat | grep 3333\n. From devshree.sane on November 28, 2010 18:23:07:\nOutput\nlaptop:~$ ps ax | grep refine\n12553 pts/0    S+     0:00 grep --color=auto refine\nlaptop:~$ netstat | grep 3333\nlaptop:~$ \n(nothing on 3333)\n. From tfmorris on November 29, 2010 00:13:23:\nThanks for the additional data.  There's not chance that you've got REFINE_HOST pointed somewhere else (perhaps for previous testing), is there?  What the script does is use either curl or wget to fetch \"http://${REFINE_HOST}:${REFINE_PORT}\" where REFINE_HOST is 127.0.0.1 by default and REFINE_PORT is 3333.\nI don't see any obvious way that the GET can succeed unless there's a web server running on that port and host.\nPerhaps the developer who wrote the startup script will have more insight into a possible failure mode that matches your symptoms.\n. From devshree.sane on November 29, 2010 04:25:59:\n@tfmorris: Thanks. Issue resolved. It was a proxy problem.\nI am behind a proxy and hence, I require an http proxy username and password to connect to the \"outside\" world. (Although refine runs on localhost, wget/curl still require the username,password) I have set the value of $http_proxy variable, and I can now run refine. Perhaps the developers would like to add this issue to the FAQ/troubleshooting pages.\nHow do I mark this as resolved?\n. From tfmorris on November 29, 2010 04:52:44:\nThanks for the update.  Glad to hear that you're up and running.\nI'm going to leave this open for the time being in hopes that we can improve the handling of this situation.  The startup script isn't checking the contents of the page it fetches.  I presume it's getting some kind of response from the proxy/firewall and assuming that it's the Refine homepage.  Perhaps we can add some type of sanity check for this kind of situation.\n. From stefa...@google.com on December 24, 2010 22:22:31:\nI added more solid running checks to r1958 so I think we can close this\n. From m...@blissett.me.uk on November 07, 2011 16:54:57:\nI had a similar problem with r2136.  Anyone with a similar issue should set the $no_proxy environment variable in the same place they've set $http_proxy etc.\n$ env | grep -i proxy\nhttp_proxy=http://proxy:8080/\nHTTP_PROXY=http://proxy:8080/\n$ ./refine\n[: 816: /tmp/refine.uR8M9NT: unexpected operator\nSomething is already running on http://127.0.0.1:3333/ but doesn't seem to be Google Refine\n$ curl -I http://localhost:3333/\nHTTP/1.1 502 Proxy Error ( Connection refused )\nVia: 1.1 KISA01\n...\n$ export no_proxy=\"localhost,127.0.0.1\"\n$ ./refine\n[: 816: /tmp/refine.REhMrIl: unexpected operator\nStarting Google Refine at 'http://127.0.0.1:3333/'\n. From lighton.phiri@gmail.com on November 22, 2011 06:07:20:\nthank you matt; setting the no_proxy environment variable worked for me.\n. From thomaslc...@gmail.com on January 24, 2012 18:55:14:\nThanks.. the no_proxy sace my life :D\n. From tfmorris on January 27, 2012 22:08:08:\nIn r2438 I've added\nno_proxy=\"localhost,127.0.0.1\"\nto the default refine.ini file which will hopefully mitigated these problems in the future.\n. From tfmorris on November 29, 2010 05:12:47:\nActually, you do want text to be parsed so that XML entity replacement, etc can get done, but, having said that, this is already fixed in the development source tree.\n. From chr...@gmail.com on November 29, 2010 14:59:02:\ngreat, is there a way to download a binary of trunk or documentation on how to compile the source? It looks like it is a GAE project but I can't get it to load in GoogleAppEngineLauncher on my mac.\n. From iainsproat on November 29, 2010 15:07:24:\nThe Developer's Guide on the wiki should help you:\nhttp://code.google.com/p/google-refine/wiki/DevelopersGuide\n. From thadguidry on November 29, 2010 15:08:45:\nYou might try just following this: http://code.google.com/p/google-refine/wiki/GetDevelopmentVersion \n. From chr...@gmail.com on November 30, 2010 02:09:24:\nthe link in comment 4 was just what I was looking for.\n. From tfmorris on June 06, 2011 22:32:11:\nissue #340 has been merged into this issue.\n. From tfmorris on June 06, 2011 22:33:19:\nJust for the record, this was fixed at the same time issue #61 was fixed, so that's where to look to see the history.\n. From tfmorris on June 06, 2011 22:32:11:\nissue #340 has been merged into this issue.\n. From tfmorris on June 06, 2011 22:33:19:\nJust for the record, this was fixed at the same time issue #61 was fixed, so that's where to look to see the history.\n. From dfhu...@gmail.com on December 05, 2010 02:13:13:\nget-processes is the UI of Refine polling the server of Refine, not the server of Refine fetching the URLs you specify. The delay between get-processes calls is independent from the delay between Refine fetching those URLs.\n. From iainsproat on November 29, 2010 16:15:13:\nQuestions on how to use Refine should be asked on the mailing list: http://groups.google.com/group/google-refine\nThe issue tracker is for bugs and feature requests only.\nHowever, the wiki does have some documentation for this: http://code.google.com/p/google-refine/wiki/FetchingURLsFromWebServices\nIf you have further questions, please post them to the mailing list.\n. From dfhu...@gmail.com on December 02, 2010 07:06:06:\nJesu, this is not an \"issue\"--it's not a bug or a feature request for the product. Rather, it's a question you have.  You should email the group for these questions.\nBut to answer your question here: you should add a new column with this expression\nvalue.partition(\"(\")[2].rpartition(\")\")[0].parseJson().data.translations[0].translatedText\n. From jesu.lu...@gmail.com on December 02, 2010 11:13:14:\nThanks a ton! \nDoes Google Refine, provide any specific documentation/help for such statements? There are some help, online, but it doesn't help on such statements. \nCould you advice me on some links for the same? Thanks!\n. From dfhu...@gmail.com on December 02, 2010 17:01:32:\nPlease email google-refine@googlegroups.com.\nDocumentation is here: http://code.google.com/p/google-refine/wiki/DocumentationForUsers\n. From iainsproat on November 29, 2010 16:36:55:\nHave you tried using the value.reinterpret(\"utf-8\") function?\nThere is more information on the wiki, http://code.google.com/p/google-refine/wiki/Recipes\nFor future reference, the mailing list is a better place to post questions about using Refine: http://groups.google.com/group/google-refine\n. From dfhu...@gmail.com on December 02, 2010 06:53:56:\nWhat do you mean by \"translate a column from DE to EN\"? How exactly do you do that?\n. From jesu.lu...@gmail.com on December 02, 2010 11:08:06:\nAm trying to translate a a column value, from DE(German) to EN (English). While doing so, \"=\" symbol is replaced by \"\\u003d\" and am looking to fix that issue. \nTo translate, I use a code similar to the one below:\n\"https://www.googleapis.com/language/translate/v2?key=&q=\" + escape(value.substring(0,50), \"url\") + \"&source=de&target=en&callback=handleResponse&prettyprint=true\"\nThanks for your help!\n. From dfhu...@gmail.com on December 02, 2010 17:00:13:\nJesu, that sounds like a problem with the translate service itself, not with Google Refine. Can you invoke such a URL directly in your browser and see what the result is? (I can't, because I don't have your key.)\nIn any case, it looks like \\u003d is a JSON string encoding for Unicode characters. Can you use the parseJson() function to decode it?\nIf you're still having problem, please carry on this discussion on the mailing list. This is not a valid \"issue\".\n. From dfhu...@gmail.com on December 02, 2010 07:13:44:\nCould you please provide some sample data that's giving you trouble and that we can test on?\n. From DanielJo...@gmail.com on December 03, 2010 16:37:19:\nMy original failed example was the full dataset with 26000 observataions.  I just tested it with a dataset having only 1001 observations and I am having the same problems, so I am sending the smaller dataset.\n. From dfhu...@gmail.com on December 05, 2010 01:55:26:\nI have been able to import your xlsx file, and to create numeric facets on \"weight\" and \"totht\". Do you have access to Firefox or Chrome? I'm suspecting that you might be seeing an IE-specific issue.\n(We coded Google Refine using standard web technologies, through the jQuery library. This works quite well for Firefox, Chrome, and Safari. We don't try to compensate for browser-specific non-standard behaviors.)\n. From DanielJo...@gmail.com on December 07, 2010 04:45:05:\nThe computer that I was hoping to use is limited to IE by company policy.  In order to see if this is the problem I installed  Refine on my home computer.  It gives me errors that I haven't been able to figure out:\nHTTP ERROR 500\nProblem accessing /. Reason:\nButterfly Error\nButterfly incurred in the following errors while initializing:\njava.lang.Exception: Failed to wire modules\n    at edu.mit.simile.butterfly.Butterfly.configure(Butterfly.java:435)\n    at edu.mit.simile.butterfly.Butterfly.init(Butterfly.java:308)\n    at org.mortbay.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:440)\n    at org.mortbay.jetty.servlet.ServletHolder.doStart(ServletHolder.java:263)\n    at com.google.refine.RefineServer.configure(Refine.java:278)\n    at com.google.refine.RefineServer.init(Refine.java:200)\n    at com.google.refine.Refine.init(Refine.java:117)\n    at com.google.refine.Refine.main(Refine.java:111)\nCaused by: java.lang.RuntimeException: Cannot wire module 'freebase' because no module implements the required interface 'core'\n    at edu.mit.simile.butterfly.Butterfly.wireModules(Butterfly.java:802)\n    at edu.mit.simile.butterfly.Butterfly.configure(Butterfly.java:433)\n. From dfhu...@gmail.com on December 07, 2010 06:30:53:\nWould you mind trying to check out the development version and see if that error is still there?\nhttp://code.google.com/p/google-refine/wiki/GetDevelopmentVersion\n. From DanielJo...@gmail.com on December 08, 2010 03:05:35:\nI considered doing so, but it looks pretty involved.  I'd install another program, but this looks like much more than that and the end result, though nice, might not be worth the time investment.\n. From danapoke...@yahoo.com on December 22, 2010 17:09:56:\nI see the exact same problem in IE 8 on XP with the automobile data set from the UCI Machine Learning repository. I can create a text facet from the \"make\" column, but Google Refine hangs with \"Working\" when trying to create a numeric facet from \"bore\" or \"price\". Could this be because some of the data in these columns are missing (with \"?\" instead of a numeric value)? A data cleaning app should be able to handle bogus values in a numeric column!\nSee here for the data: http://archive.ics.uci.edu/ml/datasets/Automobile\nRaw data without column names are here: http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\n. From thadguidry on December 22, 2010 17:36:24:\nTo quote one of our developers in this issue above:\n(We coded Google Refine using standard web technologies, through the jQuery library. This works quite well for Firefox, Chrome, and Safari. We don't try to compensate for browser-specific non-standard behaviors.)\nI tried the autos/imports-85.data and it works flawlessly in Google Chrome, and Firefox, but your right, the numeric facet hangs in IE8.\n. From thadguidry on December 22, 2010 17:39:05:\nI should also add that I choose to auto-detect the value types, but you can also uncheck that option prior to loading, and once loaded, then use Edit Cells -> Common Transforms -> To Number\n. From tfmorris on September 18, 2012 19:50:05:\nInternet Explorer support was improved in Refine 2.5.  Can you retest and let us know if this issue still exists?\n. From jonathan...@gmail.com on December 01, 2010 09:04:42:\nDespite this it looks like it processed up to about 300k records in the file, but the remainder is unchanged by my edits\n. From dfhu...@gmail.com on December 02, 2010 06:49:05:\nJonathan, are you seeing any Java exception in the console? What's the \"symptom\" of \"can't load large datasets\"? What do you observe?\nHave you tried this?\nhttp://code.google.com/p/google-refine/wiki/FaqAllocateMoreMemory\n. From jonathan...@gmail.com on December 03, 2010 06:11:09:\nno exceptions\nsymptoms: the file opens eventually, but shows 90,539 rows at the top of the screen, I make changes using \"edit cells - cluster and edit\" and as stated in my 2nd post I can see in the exported file that the edits have only effected the top half of the file.\n. From iainsproat on December 03, 2010 09:07:16:\nWhat format is the file in?  (is it CSV?)\nIs there anything special about the 90,540th line?\nAre you able to upload the data somewhere (you can attach it to the bug report if you don't mind the data being made publicly visible) so we can take a look at what's happening?\n. From jimfli...@gmail.com on March 14, 2011 02:38:45:\nI also hoped to manipulate large datasets with refine and failed, but my testing happened a few months ago, i dont have very many details.  I can tell you that increasing memory size allocated didnt fix the problem, and i was attempting to load 200mb files.\n. From jonathan...@gmail.com on December 01, 2010 12:21:58:\nClick \"next\" to see next 50, or \"last\" to go to end of table, it's on the top right.\n. From priy...@gmail.com on December 02, 2010 11:47:29:\nYes, but can they be viewed in one screen?\n. From tfmorris on December 02, 2010 14:58:33:\nYou want to see 5000 lines at once?  How tall is your screen?\nAre you saying you'd prefer scrolling to paging as a UI paradigm?  Are you asking for something else?\n. From tfmorris on December 02, 2010 14:58:34:\nYou want to see 5000 lines at once?  How tall is your screen?\nAre you saying you'd prefer scrolling to paging as a UI paradigm?  Are you asking for something else?\n. From tfmorris on December 02, 2010 14:58:34:\nYou want to see 5000 lines at once?  How tall is your screen?\nAre you saying you'd prefer scrolling to paging as a UI paradigm?  Are you asking for something else?\n. From dfhu...@gmail.com on December 02, 2010 06:58:05:\nI presume your CSV is missing the values for the first column in a lot of cells?\nJust as a work around, I would suggest: importing the file with \"split into columns\" turned off. Then transform the cells in that single column so that it contains something in the first column. For example, if the column separator is comma, then apply this transform: if(value.startsWith(\",\"), \"-\" + value, value). Then split that column up.\n. From aldo.buc...@gmail.com on December 02, 2010 07:06:55:\nYes it does have batches of empty cells on the first col ;)\nI found a way around it by modifying the source. Thanks for the tip.\nNow, I would suggest that you implement some sort of limit so as to prevent the browser from committing suicide. This may happen in other scenarios. I guess.\nThanks!\nA\n. From dfhu...@gmail.com on December 02, 2010 07:17:49:\nYes, the auto guessing of rows vs. records is too smart for its own good. I'm not entirely sure how to solve it right now, but it sounds like a valid issue.\n. From tfmorris on October 21, 2011 16:15:44:\nPerhaps we should have a limit on the number of rows per record which are displayed?  The XML importer bug which was causing records to be merged together triggered this behavior as well because, although Refine was only displaying two records, the second record was thousands of rows (effectively defeating the paging mechanism).\n. From tfmorris on January 16, 2012 22:03:04:\nI've recently discovered that this condition can be triggered after the project has been imported if you blank out cells in the first column as part of a cell transformation operation.  The workaround was to move the column to the right so it didn't trigger the record aggregation, but we probably need to think about a way to warn the user when this happens.\n. From tfmorris on December 03, 2010 17:48:52:\nSounds reasonable.\n. From iainsproat on December 04, 2010 08:49:49:\nFor the moment, a workaround would be to use the template exporter.\n. The Custom Tabular Exporter which was included with the 2.5 release has an option to control whether headers are included, among many other options.\n. From tfmorris on December 07, 2010 04:12:50:\nThanks for the report.  Could you add a little quantitative information please?  Number of seconds?  Hardware configuration?  What program are you comparing against and how long does it take to perform the same operations?\nAs always, the source data file would be a useful addition, but we'll understand if it's too proprietary to release.\n. From naiemk@gmail.com on December 07, 2010 04:21:17:\nI am using the clustering feature for Suburbs to validate against Australian post database:\n- Aussie post codes db has ~16000 records: PostCode, Suburb\n- All suburbs in clean data are uppercase.\n  I put the data at the end of my list of 40,000 addresses and selected clutering.\nThe clustering window includes a large number of clusters (a few tousands). \n- Client responds really poorly to any click on a checkbox or scroll, etc.\nComparision with other apps:\n- Any native (win, mac, etc) grid like UI can easily handle this number of records without poor performance.\n. From dfhu...@gmail.com on December 07, 2010 06:33:14:\nUI responsiveness in this case depends almost entirely on your browser. Which browser are you using? I would recommend the latest Firefox, Safari, or Chrome.\n. From naiemk@gmail.com on December 07, 2010 06:40:34:\nI am using chrome and a fairly powerfull PC.\nI have attached the file I am using.\n. From thadguidry on December 07, 2010 14:42:08:\nissue #241 has a suggestion that we don't quickly APPLY clustering until the user clicks on an APPLY button.  That feature may help here as well ?\n. From dfhu...@gmail.com on December 08, 2010 05:14:56:\nI was able to load the file after increasing the memory limit to 2G. But the data is all X's so I can't test the facets.\n. From naiemk@gmail.com on December 08, 2010 05:45:53:\nI actually had performance problem when clustering the Suburb column which is not XXXX\n. From tfmorris on September 18, 2012 19:45:34:\nThe largest number of clusters that I see using any of the methods with their default parameters is about 1800 and I don't see any performance issue.  My best guess is that there's some type of configuration issue or perhaps your system needs more memory.\n. From thadguidry on December 07, 2010 14:42:08:\nissue #241 has a suggestion that we don't quickly APPLY clustering until the user clicks on an APPLY button.  That feature may help here as well ?\n. From iainsproat on December 07, 2010 16:42:08:\nThanks for the comment.  I've updated the wiki ( r1949 )\n. From dong.ji...@gmail.com on December 10, 2010 14:55:27:\nThe workaround seems to be create a project with random English name and rename it to \u4f60\u597d\n. From tfmorris on December 12, 2010 19:01:09:\nI tested this using the current SVN code on Ubuntu 10.4 LTS with Chrome 9.0.597.16 dev and got \u4f60\u597d so the problem is either platform specific or it's already been fixed.\n. From tfmorris on January 09, 2011 16:28:08:\nI went back and tested with Refine 2.0 using the same Ubuntu/Chrome configuration and wasn't able to reproduce the problem with the released version, so I don't think anything has changed in the code.  It must be something related to operating system, browser, or locale differences.\n. From pxb1...@gmail.com on January 10, 2011 02:37:13:\nMy win7 local is zh_cn and char encoding is gbk,\nI modified the CreateProjectCommand and it works for me.\nThanks tfmorris\n. From tfmorris on February 04, 2011 03:18:06:\nI've applied a slightly modified version of the patch which preserves the request encoding if it has one, but sets it to utf-8 if there is none.\nCan you test what's in SVN now?\n. From pxb1...@gmail.com on February 08, 2011 15:13:55:\nI tested the trunk (r2003), and it's ok now.\n. From tfmorris on February 08, 2011 15:23:00:\nThanks very much for testing (and for the patch!).\n. From tfmorris on December 12, 2010 19:15:48:\nThis is related to the problem that was reported in issue #257.\n. From tfmorris on September 22, 2011 00:47:43:\nDavid has fully implemented Fusion Table support for Refine 2.5 (including export support).\n. From tfmorris on September 22, 2011 00:47:45:\nDavid has fully implemented Fusion Table support for Refine 2.5 (including export support).\n. From jost...@gmail.com on October 11, 2011 17:19:49:\nI am unable to get my Fusion Table to load in Refine; Try: https://www.google.com/fusiontables/DataSource?snapid=S284554lvpp\n. From dfhu...@gmail.com on October 11, 2011 20:27:56:\nAre you using the Google Data tab (scroll to the bottom) or pasting in the URL?\n. From dfhu...@gmail.com on September 11, 2011 05:34:21:\nThe new importer UI framework in trunk/ should have fixed this.\n. From dfhu...@gmail.com on September 11, 2011 05:34:52:\nFixed by new importer UI framework in trunk/.\n. From tfmorris on December 19, 2010 19:10:11:\nissue #291 has been merged into this issue.\n. From dfhu...@gmail.com on September 11, 2011 05:35:23:\nFixed by new importer UI framework in trunk/.\n. From tfmorris on December 19, 2010 19:10:11:\nissue #291 has been merged into this issue.\n. From tfmorris on September 18, 2012 19:48:00:\nInternet Explorer support was improved in Refine 2.5.  Can you retest and let us know if this issue still exists?\n. From thadguidry on September 18, 2012 19:57:12:\nHey Tom, isn't the Google Chrome frame needed however for Internet Explorer support ? http://www.google.com/chromeframe\n. From tfmorris on September 18, 2012 20:40:40:\nYes, but Refine will prompt the user to install it automatically \nhttp://code.google.com/p/google-refine/source/browse/trunk/main/webapp/modules/core/scripts/chrome-frame.js\n. From thadguidry on December 15, 2010 02:39:25:\nLooking good !\nDoes the link take them to http://code.google.com/p/google-refine/wiki/Importers ?\n. From iainsproat on December 15, 2010 09:56:21:\nI'm impressed, this looks great.\nWhen we get Fusion tables support, will we add an additional tab for that?\n. From jamesh...@google.com on December 15, 2010 20:23:04:\n@thad: yep!\n@iain: excellent! re: Fusion Tables, that sounds right.\n. From dfhu...@gmail.com on December 23, 2010 05:06:43:\nI'll see if I can start implementing this new UI over the next few weeks :)\n. From dfhu...@gmail.com on September 01, 2011 18:37:21:\nRevised and implemented, to be in 2.5.\n. From thadguidry on December 15, 2010 03:40:43:\nI would think Ignore Quotation Marks and Encoding should be options all the time ?  What do others think ?\n. From iainsproat on December 15, 2010 10:08:26:\nWow!\nParticularly, excited to see the 'create rows from tag' option for JSON and XML, that nicely removes the detectPath method from the JSON & XML Importers (this method required the entire dataset to be loaded twice over).\nI'd definitely agree that Encoding should be a parameter for all importers.  And a user definable quotation mark parameter would be a nice-to-have.\n. From thadguidry on December 15, 2010 15:36:30:\nFrom Freebase MQL reference:\nJSON itself supports 32-bit, 16-bit and 8-bit encodings of Unicode text. Metaweb, however, requires the 8-bit UTF-8 encoding.\nIs it still true that only UTF-8 encoding is accepted into the graph ? or is the data loaded with Refinery converted ?\n. From stefa...@google.com on December 15, 2010 20:18:17:\nRandom thoughts from top to bottom:\n6-refine-import-r1.png:\n- love the ability to switch the encoding with direct visual feedback at parse time... unfortunately, the encoding issues might be past the first few rows, so it would be useful at least to be able to paginate thru a more substantial part of the dataset\n- love the checkbox next to the header field... I find myself putting 0 there all the time, which requires an extra mouse->keyboard movement, this is so much better\n- might be worth indicating that \"auto-detect types\" has a performance penalty right there\n- do the top \"next\" and bottom \"next\" buttons do the same thing? one might mistake the one above for a paginating option\n7-refine-import-r1.png:\n- I wonder if 6-refine-import-r1.png should really always look like 7-refine-import-r1.png... the benefit there would be to show off all the stuff that refine can import data from, which is not exposed unless you decide to change parsing method (and some people might not even know what parsing means!)\n\"fixed-length workflow\": \n- not sure how understand how it's supposed to work, you click on the 'down arrow', that creates a cursor that you then move around horizontally to the desired location? or you just click on the bar at the top to select where the breakpoint is and the down triangle has a menu with various other options?\n\"JSON\"\n- \"click on the JSON row\" -> brilliant. \n\"XML\"\n- in a project from a past life (SIMILE Gadget) one thing I did that turned out to be very useful when loading and understanding big quantities of XML that others gave you was the ability to visualize the \"skeleton\" of the entire tree. See the screenshot at [http://simile.mit.edu/wiki/Gadget]. Basically it only shows the paths and uses sparklines to try to give you hints on what kind of data that is and how you should interpret it. Might be overkill for this stage, but I thought I would mention it as it might give ideas.\n- the only tricky difference between JSON and XML is that XML can contain mixed content while JSON never can. For mixed content I mean something like this and that. Parsing this is tricky because you don't know in what column to put the \"this \" string. Sometimes it's entirely possible to have structured xml with mixed content xml fragments embedded in it (for example, Atom with included XHTML fragment). I think Refine should provide some guidance there, in case the user selects an element as key that contains mixed content directly.\nIn any case, don't let the number of comments spoil the fact that this is an amazing job.\n. From jamesh...@google.com on December 15, 2010 20:45:48:\nQuotation Marks and Encoding: David had specific thoughts about why some formats shouldn't have these. they aren't clear enough to me to summarize, so let's wait and hear from him on this.  they're easy to add if that's the right thing.\nQuotation Mark Parameter: interesting idea.  easy to add from a UI standpoint, less sure about implementation.\nPagination: let's discuss with David how much complexity this adds - I'm not fundamentally opposed to it here.\nAuto-Detect Types warning: makes sense\nNext buttons: they do the same thing. let's see where we get to with pagination and we'll see if we want to tweak this somehow.\nShowing/Hiding Parse Formats: yeah, I'm of two minds about this.  always showing them felt like a lot of cognitive overhead to introduce when we expect in most cases to guess the parsing correctly.  how often do we expect to get it right?  either way, I'm open to always showing - what do other folks think?\nFixed Length Workflow: they aren't down arrows, they work like tab stops in a word processor, but agreed that it's problematic how similar those two kinds of elements are.  you drag arrows from the well horizontally into position.  it's hard to convey in a static image, but I think the down arrow confusion is a valid issue regardless and I'll look at some different icon for this.\nXML: happy to get more complex with this as necessary, and Gadget is super cool. this already seems pretty complex for a 2.1, so I'll wait for some guidance from whoever is implementing this, in terms of how much further we want to go here.\nthanks for the detailed feedback! \n. From dfhu...@gmail.com on December 24, 2010 18:31:29:\nRegarding quotation marks: they are only troublesome in delimiter-separated (text) files because they are used to escape the delimiters, e.g., consider\none,\"two,three\",four\nThere are 3 cells if \"ignore quotation marks\" is false, and 4 cells if true. Quotation marks are not an issue at all in other formats. For example, Excel files already have cells well separated.\nRegarding encoding: I think we need it for all except binary formats, like Excel.\nRegarding the fixed length, JSON, and XML formats: do we need the 2 step wizards? Or can record selection or column selection be done in any order with respect to setting the other options? It might be easier for implementation to not have the wizards.\nWill there be a way to select which file(s) inside an archive file to import?\n. From dfhu...@gmail.com on December 24, 2010 20:44:12:\nAlso, if the data is pasted from the clipboard and then sent in the HTTP POST body, then encoding is not an issue.\n. From techtonik@gmail.com on April 16, 2011 16:00:43:\nAwesome feature. Are there any plans to release it? \n. From dfhu...@gmail.com on April 17, 2011 03:40:36:\nThis is being worked on. No definite release date.\n. From dfhu...@gmail.com on September 01, 2011 18:38:06:\nRevised and implemented, to be in 2.5.\n. From tfmorris on December 15, 2010 06:34:49:\nre: #  2 The only reason it's like that is because I coded it to match the other spreadsheet importer I was using as a template.  I didn't understand why it was that way, but it seemed better to have them all consistent.\nI'd be happy to have them all drop the column numbers unless someone comes up with a good reason for them to be the way they are.\n. From dfhu...@gmail.com on December 20, 2010 00:35:50:\nre: #  2 - I'd be for dropping the column numbers as well. Note that the excel importer keeps each column name as-is unless it's a duplicate name, in which case it gets an index number starting from 2 appended to it.\n. From thadguidry on December 15, 2010 15:05:59:\nCould you not apply multiple Facets to ease exploration ? Even using a scatterplot facet on those few numeric columns you have would probably be useful.  Try exploring more with facets and let us know if it misses the point somewhere.\n. From galbith...@galbithink.org on December 16, 2010 03:22:49:\nMaybe I'm not understanding, but don't multiple facets just change which rows are visible?  With my 1920x1086 resolution screen, I can see a maximum of 14 (uncollapsed) columns. Suppose I have a table with many more columns than 14, with interesting facets containing many columns that have just blank or otherwise uniform contents.  In short, suppose I have a very badly designed table from a traditional data modeling perspective.  The proposed enhancement is meant to allow the user to focus on interesting (varying) data in such bad tables.\nGoogle Refine is great for cleaning up messy data-item tables.  Badly structured tables may be less common.  But, as mentioned previously, my approach to collecting and compiling human-generated data creates \"bad\" tables.  See related discussion at issue #286:\nhttp://code.google.com/p/google-refine/issues/detail?id=286&start=100\n. From galbith...@galbithink.org on December 19, 2010 19:09:27:\nSome related thoughts:\n(from http://purplemotes.net/2010/12/19/badly-structured-tables-have-a-bright-future/\nSee there for post with embedded links)\nbadly structured tables have a bright future\nWhich is a better, one big table, or two or more smaller tables?  The organization of the data sources, the number of smaller tables, the extent of the relationships between the smaller tables, and economies in table processing all affect the balance of advantage.  But cheaper storage, cheaper computing power, and fancier data tools probably favor the unified table.  At the limit of costless storage, costless processing, and tools that make huge masses of data transparent, you can handle a component of the data as easily as you can handle all the data.  Hence in those circumstances, using one big table is the dominant strategy.[*]\nUnified tables are likely to be badly structured from a traditional data modeling perspective.  With n disjoint components, the unified table has the form of a diagonal matrix of tables, where the diagonal elements are the disjoint components and the off-diagonal elements are empty matrices.  It's a huge waste of space.  But for the magnitudes of data that humans generate and curate by hand, storage costs are so small as to be irrelevant.   Organization, in contrast, is always a burden to action.  The simpler the organization, the greater the possibilities for decentralized, easily initiated action.\nConsider collecting data from company reports to investors.  Such data appear within text of reports, in tables embedded within text, and (sometimes) in spreadsheet files posted with presentations.  Here are some textual data from AT&T's 3Q 2010 report:\nMore than 8 million postpaid integrated devices were activated in the third quarter, the most quarterly activations ever. More than 80 percent of postpaid sales were integrated devices.\nThese data don't have a nice, regular, tabular form.  If you combine that data with data from the accompanying spreadsheets, the resulting table isn't pretty.  It gets even more badly structured when you add human-generated data from additional companies.\nHumans typically generate idiosyncratic data presentations.  More powerful data tools allow persons to create a greater number and variety of idiosyncratic data presentations from well-structured, well-defined datasets.   One might hope that norms of credibility evolve to encourage data presenters to release the underlying, machine-queryable dataset along with the idiosyncratic human-generated presentation.  But you can think of many reasons why that often won't happen.\nBroadly collecting and organizing human-generated data tends to produce badly structured tables.  No two persons generate exactly the same categories and items of data.  Data persons present change over time.   The result is a wide variety of small data items and tables. Combining that data into one badly structured table makes for more efficient querying and analysis.   As painful as this situation might be for thoughtful data modelers, badly structured tables have a bright future.\n\n[*] Of course the real world is finite.  A method with marginal cost that increases linearly with job size pushes against a finite world much sooner than a method with constant marginal cost.   The above thought experiment is meant to offer insight, not a proof of a real-world universal law.\n. From galbith...@galbithink.org on December 16, 2010 03:22:49:\nMaybe I'm not understanding, but don't multiple facets just change which rows are visible?  With my 1920x1086 resolution screen, I can see a maximum of 14 (uncollapsed) columns. Suppose I have a table with many more columns than 14, with interesting facets containing many columns that have just blank or otherwise uniform contents.  In short, suppose I have a very badly designed table from a traditional data modeling perspective.  The proposed enhancement is meant to allow the user to focus on interesting (varying) data in such bad tables.\nGoogle Refine is great for cleaning up messy data-item tables.  Badly structured tables may be less common.  But, as mentioned previously, my approach to collecting and compiling human-generated data creates \"bad\" tables.  See related discussion at issue #286:\nhttp://code.google.com/p/google-refine/issues/detail?id=286&start=100\n. From dfhu...@gmail.com on December 20, 2010 00:01:15:\nCamila, do you observe the same behavior on other browsers (Firefox, Chrome, Safari)? It'd be good to know if it's Internet Explorer-specific.\n. From sunny.y...@gmail.com on January 14, 2011 14:37:30:\nSame issue here.  I had permissioning issues launching the app, but now haev it working.  However, any faceting/clustering that result in more than a values within a given facet freezes the application.\n. From sunny.y...@gmail.com on January 14, 2011 14:37:59:\nP.S. At work where I don't have administrative permission to download other browsers.\n. From camila.s...@gmail.com on January 14, 2011 20:36:49:\nI don't have administrative permission to download other browsers, either.\n. From sunny.y...@gmail.com on January 14, 2011 22:17:45:\nFYI, I got our sys admin to permission me for Firefox 3.6 (vs. IE 8) and it no longer hangs up when faceting on large lists with many unique values.\n. From tfmorris on September 18, 2012 19:48:48:\nInternet Explorer specific problem.\n. From dfhu...@gmail.com on December 20, 2010 00:00:00:\nThis could be a tough one to fix since I'm not sure if the clustering process can be broken down easily into several chunks of work and canceled midway through.\n. @fadmaa - I've wrapped all the IDs with encodeURIComponent.  Does that meet your needs?\n. I've switched this to use encodeURI on the whole URI instead of encodeURIComponent on the ID because the latter trashes Freebase IDs with embedded slashes.\n. From thadguidry on December 22, 2010 16:27:49:\nThis is actually available now in the current trunk build. If you have the know how to build from trunk - see [DevelopersGuide] The Jsoup.org library that was integrated by Iain Sproat let's you take advantage of that.  You can also use Python as your language of choice in the expression editor and import lxml.html and lxml.etree , if you prefer that syntax and style over Jsoup or BeautifulSoup.  (See [Jython])  Feel free to reach out to me and others on our mailing list as well for additional help on any of the above.\n. From cv4...@gmail.com on December 23, 2010 09:11:34:\nThanks for the hint. I got the current trunk from SVN and got it running.\nBut it's still non-obvious to me how to me how to import data in the way I had in mind: When I open an XML file, I don't see how I can apply an XPath before importing. Instead, even for a moderately large (15MB) file, Refine takes ages (> 15 minutes) and loads of memory (> 700MB) to read the file. Presumably it needs so much memory because it creates a lot of records (> 100000), even though, if read correctly, the file only contains 2000 records.\nI don't see how I can recover my data which has been split up so much during the import. That's why I thought being able to explicitly point the software to the relevant XPaths might improve both efficiency and results considerably.\nI attach the data file I am seeing the problems with, so get a better idea what I'm talking about.\n. From thadguidry on December 23, 2010 14:37:53:\nDon't split it.  Don't do anything to it during import.  Uncheck everything and then let it load. (Should load everything into one cell, I think) Once loaded then perform your slicing and dicing and Xpath expressions, etc.  I think the team has only briefly talked about performing expressions prior to loading. But we had to get a consensus on how to handle the UX for it all.  James Home has given us some concepts to work with in issue #284 and issue #285 , Take a look at the screenshots 17 and 18 in there and you'll see how we intend to deal with XML specifically and giving you a nice UI selector.  And then let us know your thoughts in that issue #285\nIain, Tom, David can you recall if we perhaps left that off our [Roadmap] for good reason, since we're still in the planning & implementation for that feature ?\n. From cv4...@gmail.com on December 27, 2010 01:12:37:\nThanks for the hints and comments, @thadguidry.\nI unchecked all options on the import page but still get a very slow import with the splitting (using the current trunk version). My \u2013 possibly unqualified \u2013\u00a0impression from other experiments is that I can only suppress the splitting into columns but not that into lines.\nI took a look at the screenshots in issue #285 and like what I'm seeing there. Your idea of a more intuitive selector to determine which XML elements should become records in Refine, seems much more accessible than my ad-hoc idea using XPath.\n. From dfhu...@gmail.com on January 05, 2011 21:40:48:\nThad, yes, we purposely left the picking of XML xpath until the next version.\n. From geoffh...@gmail.com on December 23, 2010 00:00:45:\nI tried the export again and got a string representation of the date values.  I don't know whether this was a result of just restarting refine or when I tried to work around the issue by using Edit column -> Add a column based on this column and using value.toString() as my expression for the new column.\n. From GabrielS...@gmail.com on December 27, 2010 18:11:16:\nHere's a number of files related to this bug.\nFirst, there's patch that will fix the issue.  This change formats GregorianCalendar objects using the default date format provided by SimpleDateFormat.  I'm not sure if this is the correct format, but I had to choose something.\ndates.txt is a test file that can be used to recreate the problem (with the steps above).\ndates.csv is a sample of the bad behavior.\ndates_fixed.csv is the exported file after applying the patch.\n. From GabrielS...@gmail.com on December 27, 2010 20:41:15:\nI just noticed that I posted a bad patch file earlier, so here's an updated patch and a unit test.\n. From dfhu...@gmail.com on December 28, 2010 15:55:51:\nFixed by r1967.\nGabriel, I've changed your patch to output dates in ISO8601 format instead.\n. From GabrielS...@gmail.com on December 28, 2010 16:55:10:\nISO 8601 is a much better choice than using the default format.\nJust a quick nit: the updated unit test fails because the test grid isn't large enough.  I've attached another patch to fix this.\n. From tfmorris on March 21, 2011 15:12:47:\nissue #353 has been merged into this issue.\n. From tfmorris on March 21, 2011 15:12:47:\nissue #353 has been merged into this issue.\n. From stefa...@google.com on December 24, 2010 20:06:33:\nLanded in trunk at r1956.\n. From tfmorris on December 29, 2010 18:53:45:\nIt's more likely simple web masters than simple web servers that's the problem.  Even Python's SimpleHTTPServer takes a file extension->MIME type map. http://docs.python.org/library/simplehttpserver.html\nI'm not sure what the best solution is here.  One could go from file extensions, to trying to sniff the beginning bytes of the file, to ...  It seems like a slippery slope, but perhaps one worth venturing a little ways down.\n. From thadguidry on January 05, 2011 18:30:11:\nWould the proposed new UI screenshots in issue #285 make sense for your importing option needs regarding key:value ?\n. From chr...@gmail.com on January 05, 2011 18:47:53:\nThe UI looks great!\nI don't see a specific parser that would work but if I could figure out how to write an Extension that would be cool.\nAlso, I keep thinking there should be an easy way once importing it to convert it. The structure is similar to json but I don't see an easy way to make the conversion.\n. From thadguidry on January 05, 2011 19:00:01:\nI think just modifying the template during Export could help with your conversion needs ?  http://code.google.com/p/google-refine/wiki/Exporters\n. From tfmorris on September 18, 2012 19:08:12:\nUsing the text line importer and then using Transpose -> Key/Value transpose after the import.\n. From thadguidry on January 05, 2011 18:30:11:\nWould the proposed new UI screenshots in issue #285 make sense for your importing option needs regarding key:value ?\n. From dfhu...@gmail.com on January 07, 2011 05:11:58:\nAs long as you have Java installed on your machine, you can run Refine from your usb drive. Refine doesn't need to be \"installed\".\nBy the way, please direct questions to the mailing list. This issue tracking system is only for bugs and feature requests.\n. From GabrielS...@gmail.com on January 06, 2011 21:48:04:\nI tried with several .txt files and couldn't reproduce this issue on Windows XP (SP 3) with Internet Explorer 8.  Could you provide a masked copy of the file you are trying to import, please?\n. From sunny.y...@gmail.com on January 14, 2011 14:30:40:\nApologies, it was a permissioning issue on my end. We can close this out now.\n. From dfhu...@gmail.com on January 07, 2011 05:14:08:\nPlease direct questions to the mailing list. This issue tracking system is only for bugs and feature requests.\nYou can try to create a new column based on that column, using this expression\nvalue.split(/\\s+/).length()\n. From thadguidry on January 07, 2011 17:07:13:\nWorking with more than your brain could absorb at a given time is the whole idea behind simplifying views in Refine by using Facets and Clustering.  Any given screen resolution can only display so many rows as well, and showing 300 rows on the screen at one time doesn't resolve useful patterns the way you would like, trust us.  Please try to view the example [Screencasts] on the [GettingStarted] wiki page for examples of how to work with Facets and Clustering in general.\n. From Knight...@gmail.com on January 07, 2011 20:31:17:\nThank you for your reply.\nThe thing is, we have data sets in which we actually need to be able to see all of them to meaningfully clean them. I promise. Our brains can absorb it.\nBy limiting it to 50, this only increases the number of steps it will take to clean our data. If we could have the option to see all of the rows of data, no matter how many (even in the 1000s), it would really help us be able to clean it.\nA lot of it is trying to clean up text, like how countries have been spelled, or whether organizations have been abbreviated or spelled out. This isn't an issue of trying to clean numbers, really. \nSo, the degree of difficulty is actually harder by this row limitation, and would be much easier if we could see all rows without limit.\n. From dfhu...@gmail.com on January 07, 2011 20:40:54:\nAs a quick hack to get you unblocked, please dig into the source code directory and open this file: webapp/modules/core/scripts/view/data-table/data-table-view.js. Search for \"50\" in it, and add something like 1000. Be sure to have commas between the numbers. Then just shift-reload your browser showing Refine.\n. From thadguidry on January 07, 2011 20:42:44:\nYou REALLY need to watch the screencasts and see how the function http://code.google.com/p/google-refine/wiki/Clustering : CLUSTERING works.  You will be absolutely delighted in it's capability for your exact needs expressed, if you spend some time trying it out.\n. From Knight...@gmail.com on January 07, 2011 22:39:56:\n@dfhuynh - That's a great fix. Thanks!\n@thadguidry - I will definitely get to know clustering better. Thanks to you, too!\n. From tfmorris on June 14, 2011 06:39:04:\nObviously one can't see an arbitrarily large number of rows on a finite size screen, so it sounds like what you are actually requesting is navigation via the scroll bar vs paging.  Is that correct?  Are you still requesting this enhancement or have your needs been satisfied with Refine's currently available facilities.\n. From tfmorris on September 18, 2012 19:10:47:\nNo response from requester.  Assuming their needs are satisfied.\n. From tfmorris on February 04, 2011 04:55:48:\nThanks for the patch.  It's now applied.\n. From thadguidry on January 07, 2011 21:42:52:\nDan, I noticed that your screenshot shows your viewing by records rather than by rows  Can you try clicking on rows to see if it fixes the problem?\n. From dan.cao....@gmail.com on January 10, 2011 16:54:51:\nThanks Thad, you're correct, that fixes the problem...I've noticed the rows/records distinction but never used it until now. \nJust as a side question, what is the records-layout used for? And how does it determine if two rows make up a single record?\n. From thadguidry on January 10, 2011 17:46:39:\nSome of that is answered in the [ServerSideArchitecture] wiki page and describes the Data Model itself.  In general, the records view is used for data that would be more useful to view as a record, like XML and JSON data.  But it's completely up to you.  The difference is in the Data Model views themselves.  Either the Column Model (rows) or the Column Groups (records).  The underlying Data Model itself doesn't change however, only the views, for consistency across operations, transforms, etc, etc.\n. From tfmorris on June 14, 2011 06:34:03:\nUser misunderstanding per previous comments.\n. From tfmorris on June 14, 2011 06:26:02:\nCan you retest with the latest software?  I think Stefano made some changes in this area.\n. From iainsproat on June 14, 2011 08:10:33:\nah, I forgot that this was still open.  It works without cygwin :)\nJust run refine.bat from a normal Windows command prompt.\nI'm closing it as invalid.\n. From tfmorris on January 09, 2011 21:29:24:\nThe questions about operating system and browser versions aren't just to make you type more.  They really are necessary to understand what your configuration is.\nDo you have the JavaAccessBridge installed?  Is it installed correctly? (ie do other Java applications run?)\nThe first thing I would try to is uninstalling it or reinstalling, whichever is more appropriate.  From the little information that I can see here, I think it's unlikely to be a Refine-specific problem.\n. From maheshmo...@gmail.com on January 09, 2011 23:42:19:\nSorry about that...\nWhat operating system and browser are you using?\nOperating System : Windows 7\nBrowser - Chrome. I believe this is the default which opens up.\nInstallation wise, I don't have JavaAccessBridge installation, and I did try running the Refine installation but no luck. hope it helps..\n. From copsrepo...@gmail.com on March 02, 2011 17:36:01:\ni'm hitting the same issue, it seems. was at a nicar conference where david huynh, from google, demo'd google refine. wanted to use it on my machine, with win7 64-bit and google chrome (by default, but any browser would do). downloaded refine zip package, unzipped and tried to run google-refine.exe. it flashes open the command window, but that quickly closes and nothing else happens. i have java (32-bit and 64-bit versions) installed, and just added the java access bridge app, per \"project member\" above. still get the same activity -- i.e., command window flash and then nothing else. would really like to get this working, so any help would be appreciated.\n. From dfhu...@gmail.com on March 02, 2011 17:56:17:\nI'd suggest trying this\n- open a Command line window\n- change to the directory where you unzip'ed the zip file\n- run \"refine\" there\n- see if you get any error in the command line\n. From copsrepo...@gmail.com on March 02, 2011 18:09:38:\n(forehead slap!) i unzipped the package, but had unchecked the option to restore original folders. so, refine execute and batch files were telling refine to look for folders that didn't exist. argh. (forehead slap again!) thanks for the quick reply, tho. all is working fine, now.\n. [Copying the attachment over from Google Code to preserve the historical record]\n12:24:42.476 [            refine_server] Starting Server bound to '127.0.0.1:3333' (0ms)\n12:24:42.679 [            refine_server] Initializing context: '/' from 'D:\\Software Install\\Google\\google-refine-2.0-r1836\\webapp' (203ms)\n12:24:45.005 [                   refine] Starting Google Refine 2.0 [r1836]... (2326ms)\n12:24:45.076 [       FileProjectManager] Using workspace directory: C:\\Users\\msharma\\AppData\\Local\\Google\\Refine (71ms)\n12:24:45.082 [       FileProjectManager] Loading workspace: C:\\Users\\msharma\\AppData\\Local\\Google\\Refine\\workspace.json (6ms)\n12:24:45.520 [                butterfly] Starting butterfly ... (438ms)\n12:24:45.521 [                butterfly] Properties loaded from D:\\Software Install\\Google\\google-refine-2.0-r1836\\webapp\\WEB-INF\\butterfly.properties (1ms)\n12:24:45.684 [                butterfly] > process properties (163ms)\n12:24:45.684 [                butterfly]  Butterfly home: D:\\Software Install\\Google\\google-refine-2.0-r1836\\webapp (0ms)\n12:24:45.685 [                butterfly] < process properties (1ms)\n12:24:45.685 [                butterfly] > load modules (0ms)\n12:24:45.804 [                butterfly] < load modules (119ms)\n12:24:45.804 [                butterfly] > create modules (0ms)\n12:24:45.869 [                butterfly] < create modules (65ms)\n12:24:45.870 [                butterfly] > load module wirings (1ms)\n12:24:45.870 [                butterfly]  Loaded module wirings from: D:\\Software Install\\Google\\google-refine-2.0-r1836\\webapp\\WEB-INF\\modules.properties (0ms)\n12:24:45.871 [                butterfly] < load module wirings (1ms)\n12:24:45.872 [                butterfly] > wire modules (1ms)\n12:24:45.872 [                butterfly]  mounting modules (0ms)\n12:24:45.873 [                butterfly]  No mount point defined for module 'jython', mounting to '/extension/jython' (1ms)\n12:24:45.875 [                butterfly]  No mount point defined for module 'freebase', mounting to '/extension/freebase' (2ms)\n12:24:45.875 [                butterfly]  No mount point defined for module 'gdata', mounting to '/extension/gdata' (0ms)\n12:24:45.877 [                butterfly]  No mount point defined for module 'sample', mounting to '/extension/sample' (2ms)\n12:24:45.878 [                butterfly] < wire modules (1ms)\n12:24:45.879 [                butterfly] > configure modules (1ms)\n12:24:46.628 [                butterfly] < configure modules (749ms)\n12:24:46.628 [                butterfly] > initialize modules (0ms)\nsys-package-mgr: processing modified jar, 'C:\\Program Files (x86)\\Java\\jre6\\lib\\ext\\QTJava.zip'\n12:24:50.220 [                butterfly] < initialize modules (3592ms)\nException in thread \"main\" java.lang.UnsatisfiedLinkError: no JavaAccessBridge in java.library.path\n        at java.lang.ClassLoader.loadLibrary(Unknown Source)\n        at java.lang.Runtime.loadLibrary0(Unknown Source)\n        at java.lang.System.loadLibrary(Unknown Source)\n        at com.sun.java.accessibility.AccessBridge$1.run(AccessBridge.java:45)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at com.sun.java.accessibility.AccessBridge.(AccessBridge.java:42)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\n        at java.lang.reflect.Constructor.newInstance(Unknown Source)\n        at java.lang.Class.newInstance0(Unknown Source)\n        at java.lang.Class.newInstance(Unknown Source)\n        at java.awt.Toolkit.loadAssistiveTechnologies(Unknown Source)\n        at java.awt.Toolkit.getDefaultToolkit(Unknown Source)\n        at java.awt.Window.getToolkit(Unknown Source)\n        at java.awt.Window.init(Unknown Source)\n        at java.awt.Window.(Unknown Source)\n        at java.awt.Frame.(Unknown Source)\n        at java.awt.Frame.(Unknown Source)\n        at javax.swing.JFrame.(Unknown Source)\n        at com.google.refine.RefineClient.(Refine.java:424)\n        at com.google.refine.Refine.init(Refine.java:122)\n        at com.google.refine.Refine.main(Refine.java:111) \n. Both of these cases appear to have been corrupted installations.  Closing.\n. From malc...@notyourhomework.net on January 15, 2011 11:20:42:\nDiscovered \"auto-detect value types\" check box is selected by default. When importing without that option selected the data is imported without conversion. \n. From tfmorris on January 15, 2011 16:15:10:\nI think we should consider changing the default to unchecked.  It's both slow and often produces undesirable results.  Anything that can be done at import time can be done later (when it's more easily reversible too).\n. From dfhu...@gmail.com on January 15, 2011 16:40:50:\ntfmorris: hopefully the new importer UI will also help here, as you can toggle options and preview their effects right away.\n. From malc...@notyourhomework.net on January 17, 2011 05:21:55:\nImporting from an XML source, with auto-convert turned off, changes the data as if auto-convert was turned on. \n. From tfmorris on June 14, 2011 07:38:56:\nCan you attach an example file that demonstrates the problem of XML files not getting imported correctly when auto-detect types is turned off?\n. The default setting for auto-detect types in all importers is OFF for 2.6.\n. From tfmorris on June 14, 2011 07:36:22:\nCan you please retest with the Refine 2.1 RC1 release candidate?  I can't reproduce the error there.  A custom text facet works with your expression, while a custom numeric facet gives an error for non-numeric values (since it's a boolean expression).\n. From PaulMake...@gmail.com on June 14, 2011 07:44:58:\nI think this is a misunderstanding about a numeric facet. The expression value > 0 && value < 0.5 is not a number, it's a true/false value so it's not able to be (directly) mapped onto a number scale for plotting a histogram. As Tom says, for a boolean expression a Text Facet (aka list facet) is needed.\nYou can make a custom numeric facet by doing something like, if(value > 0 && value < 0.5, 1, 0)\n. From PaulMake...@gmail.com on June 14, 2011 08:56:26:\n\nYou can make a custom numeric facet by doing something like, if(value > 0 && value < 0.5, 1, 0)\n\nI spoke too soon - apparently this doesn't work; \"Parsing error at offset 13: Missing )\". David?\n(If you want to see what it looks like at least select Jython and,\nreturn 1 if value > 0 and value < 0.5 else 0\nI tested this this time :-))\n. From tfmorris on October 14, 2011 20:53:17:\nI don't know whether this was fixed after the original report or was just a misunderstanding, but either way I'm satisfied that the current implementation provides this functionality, so I'm closing this out.\np.s. Paul's numeric facet proposal can be modified as follows to work:\nif(and(value > 0,value < 0.5), 1, 0)\n. From thadguidry on January 19, 2011 15:42:34:\nWorks fine for me.  Perhaps you have a download manager or a Firefox addon for managing downloads turned on ?  Try to disable or uninstall those.  On my connection, I am able to download the entire .zip file in about 1 min.  How long is your download taking to get to 12.8 MB ?  How fast is your internet connection ?  If it's taking say 10 mins or longer, then you probably DO want to use a download manager with resuming capabilities.  Hope this helps !\n. From marcelov...@gmail.com on January 19, 2011 16:23:49:\nThe problem is due to my organization network security. It found out that there was a .exe file inside and just let me download around 13 MB. I tried 10 times in three different computers.\nI talked to a friend of mine who knew about this and he managed to put the download link in a special place in our network and in a minute he managed to download the complete file. Hope this information helps if someone of you has such a problem.\n. From iainsproat on January 19, 2011 16:26:34:\nThanks for the update, glad you got the issue resolved.  I'm now closing the issue.\n. From tfmorris on January 22, 2011 18:01:01:\nThat's a simple change, but in the mean time, can't you just use the id?  They're different between the two fields.\n. From dfhu...@gmail.com on January 24, 2011 02:42:18:\nI believe this has been fixed in the trunk version. Would you mind trying the trunk version?\nhttp://code.google.com/p/google-refine/wiki/GetDevelopmentVersion\n. From eduardocereto on January 24, 2011 06:54:32:\nOh my. I'll need to setup all this java env thing I'm not used to. \nI'll try to do that when I have some time. \nBut for now I got a ugly workaround that worked for me, but may not work for others with the same issues and in the need of a more complex date format.\nvalue.datePart('year') + '-' + value.datePart('month') + '- ' + value.datePart('day')\nIt won't give you the same results, but may work for others with same issue till a new stable is released.\n. From tfmorris on June 14, 2011 06:41:27:\nThere is a 2.1 release candidate available now that you can use to test whether this has been fixed.\nPlease let us know your results.\n. From ecer...@cardinalpath.com on June 14, 2011 07:23:20:\nJust tried it out, and it seems fine to me. feel free to close the ticket down.\n. From eduardocereto on June 14, 2011 07:24:50:\nBy the way, I just noticed that I replied from my corp mail. But I'm the same person. \n. From tfmorris on June 14, 2011 07:42:31:\nThanks for retesting.  The final 2.1 release should be available shortly.\n. From tfmorris on October 14, 2011 20:29:11:\nFixed in r2334.  Dates will now be exported in ISO 8601 format (2011-01-25).\n. From tfmorris on June 06, 2011 21:48:44:\nThe display issues appear to have been fixed with the earlier round of character encoding fixes, but reconciliation is still failing to return any results, despite the fact there is a perfect match, so let's focus this bug on that piece of things.\nI'm not sure at this point whether the problem is with Refine or with the Freebase reconciliation service.\n. From tfmorris on July 03, 2011 23:27:45:\nAs I've done more reconciliation recently I've noticed a large number of misses for things which are in Freebase due, apparently, to a stale reconciliation service index, so it's possible the reconciliation miss was due to this rather than the diacritic.\nMore data/investigation needed...\n. From tfmorris on November 19, 2011 00:39:16:\nThe reconciliation service sucks, but as far as I can tell it doesn't suck any worse for international characters.  I downloaded a spreadsheet of 90 buildings with names beginning \"Casa\" to test with.\nFor 8 with accented names, it automatched 2, got no candidate at all for 3, got the correct candidate as the top scorer for the other three, but the score wasn't high enough to automatch.\nFor non-accented 82 names: 14 automatched, 16 no candidate, 52 with scores too low to automatch.\nNote that since these all came from Freebase to start with, they should all be guaranteed to match.  My conclusion - it sucks, but doesn't suck any worse for accented characters.\n. From thadguidry on November 19, 2011 01:05:33:\nTom, did the spreadsheet have any other additional columns that contained metadata that could have also been matched as a disambiguator with the additional checkbox for a property on buildings or structures ?  (the use of additional columns to reconcile uses Collin's older recon service, which sucks sometimes, sometimes not, depending on domains)  Curious, if that would have changed the scoring.  Can you test ?\n. From tfmorris on June 06, 2011 22:17:44:\nAdding @Override annotations is a good thing to do as people come across places where they're missing, but issues and patches are a way to high and overhead mechanism for  doing this.\nA more efficient way is to just adjust our Eclipse settings appropriately so that they get flagged as warnings.  That way they'll improve over time (and we'll keep them clean).\nHaving said that, I did this class and a bunch of others in surrounding packages.\n. From Mer...@gmail.com on March 12, 2011 21:34:49:\nAre you using NetBeans IDE?\nI had the same issue. Found the warning under:\nOptions > Editor > Hints > Code Maturity Issues > Print Stack Trace\nIt says \"This inspection reports any uses Throwable.printStackTrace() without arguments. These are often temporary debugging statements, and should probably be either removed from production code, or replaced by a more robust logging facility.\"\n. From tfmorris on June 06, 2011 22:13:46:\nI'm going to defer this for now.  If it's going to be fixed, it should be as part of a general review of the error handling.  It's better to have the information for debugging, even if it's only in the log.\n. From kstur...@gmail.com on February 02, 2011 21:19:57:\nThere is a simple work-around to fix this behavior.\nAll you need to do is edit the file 'google-refine.l4j.ini' at the end of the file place the following line with your proxy server information.\n-Dhttp.proxyHost=webproxy.mycompany.com\nThis will configure to java to use the proxy server when downloading data from any webservice.  Depending on proxy configuration you may need to also set http.proxyPort and http.nonProxyHosts.\nMaybe this work around could be added to the wiki to make it easier to find.\n. From kstur...@gmail.com on February 02, 2011 21:49:01:\nIt is also possible to configure java to load the proxy information from the system settings. \nEdit the file 'google-refine.l4j.ini' and include the following line\n-Djava.net.useSystemProxies=true\n. From tfmorris on February 03, 2011 01:03:35:\nIs there any reason that useSystemProxies=true shouldn't be the default?\n. From kstur...@gmail.com on February 04, 2011 15:49:38:\nI think it should be the default.  I have never seen a situation where I wanted to disable the system default proxies.\n. From thadguidry on February 04, 2011 16:16:08:\n+1 agreed that it should be defaulted.\n. From tfmorris on February 04, 2011 20:20:21:\nFixed as described in conf/google-refine.l4j.ini.\nThanks for investigating and providing a patch.\n. From tfmorris on February 03, 2011 05:14:01:\nI'm not sure I'm going to have time to finish tracking this down, so here's where I got to.  In ReconConfig.reconstruct():\nReconConfig.reconstruct(JSONObject) line: 90    \nReconOperation.<init>(JSONObject, String, ReconConfig) line: 93 \nReconcileCommand.createOperation(Project, HttpServletRequest, JSONObject) line: 59\nthe mode is \"freebase/strict\" but the Map s_opNameToClass only contains an entry for for \"core/standard-service\" and nothing else.\nNot sure where it is supposed to be initialized yet...\nmatch = \"key\"\nnamespace = {\"id\":\"/base/ourairports/ourairports_id\",\n             \"name\":\"OurAirports namespace\"\n            }\nStrictReconConfig\n. From tfmorris on February 03, 2011 23:50:34:\nTurns out that I was missing the Freebase extension.  The Freebase extension is probably always going to be there in a normal installation, but I wonder if there's a need for checking what extensions an exported project depends on when it is re-imported.\nOpinions on whether I should close this or re-target it at better checking/reporting?\n. From tfmorris on September 18, 2012 19:18:28:\nWhen you moved the partially empty column to the beginning, Refine interpreted the empty cells as marking \"sub-records\" which were indented below the main record.\nIf the data isn't really record oriented, you can switch to using the \"row\" display mode instead of the \"record\" display mode.  Alternatively, you can undo the operation or move the column to a different location.\nThe performance slowdown is due to the fact that in record display mode you're now displaying records with many many rows making the grid much bigger and tougher for your browser to handle.\n. From tfmorris on February 04, 2011 02:02:36:\nFixed for key, guid, & id reconciliation.  If no result is returned, a non-match Recon object is created so that it won't be retried as it when the result is null.\n. From tfmorris on February 04, 2011 17:59:48:\nThanks for the bug report.  This is already fixed in source and the fix will appear in the next release.\n. From randy123...@gmail.com on February 07, 2011 03:42:42:\nTo note: I think I have fixed the issue by setting header lines to 0. The default header lines is 1. I think that showing a preview of the file on project creation of the header and a couple of data rows will provide ease of use. \nHowever, I think this still may be a bug. \n. From tfmorris on October 21, 2011 15:56:42:\nThe new importer UI guesses that the column delimiter is '(' but allows you to easily switch to a different delimiter or none at all.\n. From tfmorris on February 08, 2011 16:23:53:\nWhat errors, if any, are being reported on the console that the Refine server was started from?\nThere are no hardcoded limits.  It's much more likely that you're hitting an error which isn't being handled and reported properly.\n. From vinnygof...@gmail.com on February 08, 2011 17:13:02:\nDoes the console keep an error log by chance or do I need to scroll through it immediately when it fails?\n. From vinnygof...@gmail.com on February 09, 2011 16:11:30:\nI don't know yet if Refine keeps any kind of console log history but I was watching the console when it failed during my last test. At line \"16:24:18.429\" is when the reconcile stopped, the percentage completed window disappeared and no error messages popped up. \nConsole window contents attached.\n. From tfmorris on February 09, 2011 16:22:11:\nI don't think any logging is done to a file, although a little log4j configuration magic could probably change that pretty easily.\nAt first glance, it looks to me like it thinks things completed successfully.  Anything special about the data near where the operation stops?\nUnless David has some other ideas, it'll probably take getting a copy of the data file and the operation(s) that you are attempting to be able to debug this.\n. From vinnygof...@gmail.com on February 09, 2011 16:49:05:\nHow can I tell what row the operation stopped at?\n. From tfmorris on February 09, 2011 17:36:39:\nI don't see anything in the log which would indicate, but I thought you had an idea since you said \"about halfway through.\"\n. From vinnygof...@gmail.com on February 09, 2011 18:45:54:\nI was just going according to the status box at the top of the page during the reconcile, it was at 53% completed when it suddenly disappeared. The file I was working with at that time was about 20,000 rows and was hoping it would have logged where it stopped for debugging purposes.\nI'm currently trying to apply the exact same JSON code to a project containing 141,160 rows. This is the largest file I have so I'm curious to see if it successfully completes.\nThanks for your help with this.\n. From thadguidry on February 09, 2011 19:12:27:\nHmm....I'm wondering if he's not having a div problem with Chrome 9 as in issue #102 ? And that it actually was continuing the reconcile process, but he didn't know it ?  I typically have just looked and followed along watching in the Command window to be extra sure.\n. From vinnygof...@gmail.com on February 10, 2011 17:05:23:\nWhen the reconciles fail the command window stays open and there isn't any activity.\nI needed to stop the process on my biggest file (141,160). It got to about 24% complete however.\nI've decided to be more methodical with my approach. I have 15 files (projects) that range from 123 rows to 141,160 rows. I'm going to apply the JSON code to the smallest first and work my way up. I've attached the JSON code I've been working with.\nBelow is what I've done so far, how long it took to complete and if completed successfully:\nProject containing 123 rows - successful in 4 minutes \nProject containing 1,171 rows - successful in 15 minutes\nProject containing 4,797 rows - successful in 1 hour and 5 minutes\nProject containing 5,403 rows - successful in 1 hour and 14 minutes\nProject containing 9,716 rows - FAILED at 2 hours and 5 minutes (console message attached)\nProject containing 12,493 row - currently running (started at 11:54 am - 2/10/11)\nSo far the project size really has had a noticeable difference on the resources used. CPU usage peaks at about 20% and memory usage at about 1.5 GB.\nThanks again guys.\n. From tfmorris on February 10, 2011 19:06:18:\nThanks for the additional info.  The failure looks to be related to HTTP 500 Server Errors on the reconcile operation.  I wonder if a) they're getting retried by Refine and b) this is a transient or hard error.\nThe processing times strike me as being very long.  I'm guessing that most of this is network/reconciliation latency, but still an hour to reconcile 10,000 items (5,000 rows x 2 columns) seems like a lot.\nWhat Java heap size are you using?  If you've got the memory available, you should make it generous enough that it's not an issue.\n. From vinnygof...@gmail.com on February 10, 2011 19:20:44:\nThe project I was running (12,493 rows) just stopped at about 2hours and 3minutes into it. I've attached the contents of the console window.\nThe Java heap size is whatever default was for the install. I've got 4GB of memory  and only about 1.5 GB are being used during the processing, so what do you suggest I raise it to? Also, where is that setting changed?\nThanks.\n. From tfmorris on February 10, 2011 20:04:42:\nThe default heap size is 1024M (1 GB).  Try going up to 1536 or 2048, depending on what other memory demands you have on the system.\nYou can find instructions here\nhttp://code.google.com/p/google-refine/wiki/FaqAllocateMoreMemory\nIt definitely should be reporting the out of memory error though (presuming that's what's happening), so we'll need to look into that more.\n. From vinnygof...@gmail.com on February 12, 2011 02:16:14:\nI installed the java development kit and increased the heap size to 2048M. I tried to apply the saved operation to a file that been failing for me containing 9,716 rows and the reconciliation stopped again about 2 hours in on the \"Title\" column. I've attached the contents of the console window, the time of failure was 17:01:58.144.\nThanks.\n. From vinnygof...@gmail.com on February 12, 2011 02:19:34:\nAre there any know characters within a data set that can cause issues?\n. From dfhu...@gmail.com on February 12, 2011 04:58:06:\ntfmorris, looking back at my code, the run() method doesn't catch and log any exception\nhttp://code.google.com/p/google-refine/source/browse/trunk/main/src/com/google/refine/operations/recon/ReconOperation.java\nSo memory exceptions would go unnoticed.\nvinnygoffin, if you're comfortable with Java code, would you mind trying to modify that file locally, wrap the run() method's content in a try catch and see if any exception shows up? Thank you for your patience.\n. From vinnygof...@gmail.com on February 15, 2011 20:52:57:\nHi,\nI'm not versed in Java so unfortunately i'm not sure how to do what your asking, sorry.\nI have continued with some other testing using a file named(RED425 with 9,716 rows)this consistently stops and doesn't complete. I've attached the most recent console window contents of this failure.\nTrying to see if it was a piece of data within the file that is giving me the headaches, I split the file into 2 pieces (RED425-1 with 4,858 rows and RED425-2 also with 4,858 rows).\nBoth completed fine, so I'm not thinking it's a weird character or bad data or something like that. Would you agree? I've attached these files as well, in case you interested.\n. From dfhu...@gmail.com on February 15, 2011 22:53:24:\nThank you for trying again. I think you're right: it's not a weird character that's causing problems. It looks to me--and I'm not surprised--that the reconciliation service just got overloaded. So for now, is it OK for you to tend the reconciliation process somewhat manually (e.g., splitting it up into batches)? By the way, you could do this in a single project by creating a custom facet on any column with the expression\nceil(row.index / 4000)\nSubstitute 4000 for however large a batch you want. Select one value at a time in the facet and invoke the reconcile command.\n. From vinnygof...@gmail.com on February 17, 2011 03:37:29:\nYup, that's what I'm going to do. I'll split the files into 5000 row segments.\nThanks for all your help with this.\n. From vinnygof...@gmail.com on March 01, 2011 15:09:17:\nHi again,\nOne more quick question.\nIs there a way to automatically invoke the ceil(row.index / 4000) command in succession?\nI was thinking of seeing if I could have it somehow run it on the first 4000 rows, stop, then continue on the next 4000 and so on until complete.\nThanks.\n. From tfmorris on October 21, 2011 16:09:44:\nIt's not really possible to say for sure from the information provided, but this could be related to issue #440 which would effectively prevent completion of any single operation (one step in the undo history) which took longer than one hour.\nThe fix for that is in SVN and will be included in the upcoming v2.5 release.\n. From vinnygof...@gmail.com on February 09, 2011 18:45:54:\nI was just going according to the status box at the top of the page during the reconcile, it was at 53% completed when it suddenly disappeared. The file I was working with at that time was about 20,000 rows and was hoping it would have logged where it stopped for debugging purposes.\nI'm currently trying to apply the exact same JSON code to a project containing 141,160 rows. This is the largest file I have so I'm curious to see if it successfully completes.\nThanks for your help with this.\n. From thadguidry on February 09, 2011 19:12:27:\nHmm....I'm wondering if he's not having a div problem with Chrome 9 as in issue #102 ? And that it actually was continuing the reconcile process, but he didn't know it ?  I typically have just looked and followed along watching in the Command window to be extra sure.\n. From vinnygof...@gmail.com on February 10, 2011 17:05:23:\nWhen the reconciles fail the command window stays open and there isn't any activity.\nI needed to stop the process on my biggest file (141,160). It got to about 24% complete however.\nI've decided to be more methodical with my approach. I have 15 files (projects) that range from 123 rows to 141,160 rows. I'm going to apply the JSON code to the smallest first and work my way up. I've attached the JSON code I've been working with.\nBelow is what I've done so far, how long it took to complete and if completed successfully:\nProject containing 123 rows - successful in 4 minutes \nProject containing 1,171 rows - successful in 15 minutes\nProject containing 4,797 rows - successful in 1 hour and 5 minutes\nProject containing 5,403 rows - successful in 1 hour and 14 minutes\nProject containing 9,716 rows - FAILED at 2 hours and 5 minutes (console message attached)\nProject containing 12,493 row - currently running (started at 11:54 am - 2/10/11)\nSo far the project size really has had a noticeable difference on the resources used. CPU usage peaks at about 20% and memory usage at about 1.5 GB.\nThanks again guys.\n. From tfmorris on October 21, 2011 16:09:44:\nIt's not really possible to say for sure from the information provided, but this could be related to issue #440 which would effectively prevent completion of any single operation (one step in the undo history) which took longer than one hour.\nThe fix for that is in SVN and will be included in the upcoming v2.5 release.\n. From thadguidry on February 08, 2011 20:50:48:\nActually, I think this issue or question could go even higher level.  Would it be useful for the user to be presented with an option during import to automatically promote a null to a blank ?(typically a null = NOTHING found between a pair of commas in a CSV file or a pair of \\t tabs in a TSV file)  Earth,,New York City  vs Earth \\t\\t New York City\n. From benjamin...@gmail.com on February 08, 2011 21:55:42:\nI think an \"import empty values as nulls vs. blanks\" would be useful, as would a means to do this after import. As of now I can't transform any null cells to blanks, and have resorted to using exception handling to merge columns together.\nIn any case, it looks like issue #252 is related. When a cell is null, accessing the value attribute throws an error. They used GREL, I encountered the same through Jython.\n. From dfhu...@gmail.com on February 10, 2011 01:14:35:\nI'll be sure to add that option to the new importer UI.\n. From benjamin...@gmail.com on February 08, 2011 21:55:42:\nI think an \"import empty values as nulls vs. blanks\" would be useful, as would a means to do this after import. As of now I can't transform any null cells to blanks, and have resorted to using exception handling to merge columns together.\nIn any case, it looks like issue #252 is related. When a cell is null, accessing the value attribute throws an error. They used GREL, I encountered the same through Jython.\n. Just to clarify, none of these posts are from me. I just did the import from Google Code. The original author of each message is in the first line of the message (look for italics and From ...). From vincent....@gmail.com on February 14, 2011 18:09:30:\nHere is my output (short) when starting Refine.\nStarting Google Refine at 'http://0.0.0.0:3333/'\nJava HotSpot(TM) 64-Bit Server VM warning: Failed to reserve shared memory (errno = 28).\n18:07:18.993 [            refine_server] Starting Server bound to '0.0.0.0:3333' (0ms)\n18:07:18.994 [            refine_server] Max memory size: 15000M (1ms)\n18:07:19.005 [            refine_server] Initializing context: '/' from '/mnt/grefine/google-refine-2.0/webapp' (11ms)\n. From tfmorris on February 14, 2011 18:26:58:\nThe issue database is for bug reports and enhancement requests.  Please use the  mailing list for questions about tuning advice.  (Although, honestly, you may get quicker/better advice in a Java forum with expertise in running JVMs with large heap sizes)\nYour two postings talk about two different heap sizes 15g and 12g.  Which is it?  15g definitely seems too big for a 16g machine.\n. From vincent....@gmail.com on February 14, 2011 21:40:47:\nok, thanks for your suggestion.\nIll ask in a Java forum, ur right.\n. From tfmorris on February 14, 2011 22:31:35:\nI've tightened up the URL pattern matching so the Google Spreadsheets/Fusion Table importer doesn't get invoked, but that URL fails in a different way now in the generic importer getting an HTTP 400 Bad Request error even though the same URL (http://www.google.com/fusiontables/api/query?sql=SELECT * FROM 274408) works in the browser.  A quoting issue perhaps?  If someone else wants to take a look, feel free, otherwise I'll try to circle back at some point in the future.\nThread pool-1-thread-3\n    HttpURLConnection.getInputStream() line: 1313 [local variables unavailable] \n    CreateProjectCommand.internalImportURL(HttpServletRequest, Project, ProjectMetadata, Properties, String) line: 415\n    CreateProjectCommand.internalImport(HttpServletRequest, Project, ProjectMetadata, Properties) line: 187 \n. From tfmorris on February 15, 2011 23:15:48:\nFixed in CreateProjectCommand by making sure URLs are encoded before using them.\n. From thadguidry on February 17, 2011 03:55:31:\nOn the wiki, http://code.google.com/p/google-refine/wiki/Reconciliation\nPlease feel free to ask questions on our mailing list, rather than opening an issue. Thanks.\n. From tfmorris on November 19, 2011 00:04:09:\nThat page has files in three different formats, so \"any\" isn't very specific.  I examined ipg050712.xml which is in XML format and found that it has no root element.  In other words, it looks like\n\n\nrather than \n\n\n\n\nGoogle Refine will only handle XML files with a single root element, so you'll need to modify the files.  You'll end up with very large grids of cells which is likely to make your browser quite sluggish, so even after all this Refine might not be the best tool of choice, but it will import the files.\n. From thadguidry on February 23, 2011 17:30:42:\nAlso attached corresponding new OwnText.java file\nVerified as working correctly.\nScreenshots:\nwhen htmlText() is used - http://awesomescreenshot.com/05a836s29\nwhen ownText() is used - http://awesomescreenshot.com/017836z10 \n. From tfmorris on February 23, 2011 19:29:07:\nI'll apply this.  A couple of suggestions for the future:\n- create a single patch file containing both the modified and new module, that way it can all be applied at once.\n- use a test case that shows a difference in the output of the two functions (because there are no contained elements in your example, both functions return the same text)\n. From tfmorris on February 23, 2011 19:41:49:\nPatch applied in r2025\n. From thadguidry on February 23, 2011 19:50:53:\nre: single patch file.  Do you mean a .zip that has the files I modified in it ?\nre: test case.  I thought both screenshots showed the difference well between htmlText() and ownText() ?  Can you explain more ?\n. From tfmorris on February 23, 2011 20:15:19:\nMy bad on the test.  I looked at the screenshots four times, but I guess I should have opened them side by side rather than looking at them alternately.\nFor the patch, just diff the whole project or directory, the new module will get included in the patch as one giant addition.  That way the patch can be applied as a unit without any hand fiddling and we'll be sure everything is just the way you intended.  It's both more reliable and less work.\n. From thadguidry on February 23, 2011 20:35:34:\n1. No idea how to diff the whole project (although I understand the intent), use a tool for diff'ing ?  Use SVN diff ?  To me, a quick patch was exactly how I handled it, but welcome your comments, Tom.  Perhaps in a separate email thread.\n2. I'm a novice altogether with programming, if you recall, but try where I can.  This was the best I could do in a pinch.  I think it's safer to just email stuff directly to you or David later on. (Although I like the edit ability within Google Code)\n. From thadguidry on February 23, 2011 22:47:39:\nNot sure how to wire up for an additional error here, but I think it is needed ?\nhttp://awesomescreenshot.com/08683st19\nCan you help with that Tom ?\n. From vinnygof...@gmail.com on March 01, 2011 14:57:33:\nPlease let me know if additional information is needed.\nThanks.\n. From tfmorris on October 08, 2011 19:20:38:\nThis sounds like a known problem with the Freebase reconciliation service using a stale index.  Unfortunately, it's something which is out of Refine's control.  Google is in the process of re-implementing the reconciliation service, but until that's complete we just have to live with the stale index.\n. From tfmorris on February 28, 2011 18:02:15:\nWhat operations would it be most valuable to you to have parallelized?\n. From stefa...@google.com on February 28, 2011 19:18:35:\nBTW, clustering already uses all the cores it can find.\n. From vincent....@gmail.com on March 01, 2011 09:32:29:\nQUOTE : What operations would it be most valuable to you to have parallelized?\nANSWER : when creating a project based on a big flat file (big for us is starting at 4 go). So I mean when importing big files.\nToday we create project by calling files stored on AWS S3 / Cloudfront. Works fine, except for really big file. In that case, Refine \"works\" (from what we can see using iostat, mpstat, top, etc ... the data is loaded) but nothing is updated on the browser side.\nAnyway, adding multi core / proc usage would be fine for \"extreme\" users like us (dataminers) and adding interactivity / information on the browser side would also be nice in order to be aware of what's going on (ram feeding, freebase inserts, etc ...).\nMany thanks\n. From thadguidry on March 02, 2011 18:57:57:\nWould \"Flag Selected Records\" perform anything else? Other than flagging the records being displayed in the Cluster view.  Or is that the feature your asking for only ?\n. From sachdev...@gmail.com on March 02, 2011 19:03:24:\nWell the point is to select a group of records or creating a subset out of total records based on clustering information so that these records can be acted on later.\nMy specific requirement was to export the clustered records out of refine to perform some manipulation on a external database.\n. From sachdev...@gmail.com on March 02, 2011 19:11:44:\nI think the word 'Instead' in this issue's title is misleading. I didn't realize it while creating the issue.\nAny way I can change the title now ?\n. From tfmorris on June 07, 2011 06:29:50:\nTitle changed per original posters request.  Hopefully it's close to what you desire, but if not, please give us the text of the title you'd like.\n. From PaulMake...@gmail.com on June 07, 2011 10:30:13:\n\"\"\"\nWell the point is to select a group of records or creating a subset out of total records based on clustering information so that these records can be acted on later.\n\"\"\"\nIt's possible to do this without much work with version 2.0: If you mouseover each cluster a link appears \"Browse this cluster\". Clicking this link will open a new window with that cluster selected. At that point it's straightforward to select All -> Edit rows -> Flag rows, or Export right there & then, etc.\n. From thadguidry on June 07, 2011 13:54:19:\nThe problem grows however with larger messier files.  In my case, I have a 20,000 row file that gives me about 42 clusters in one column, which is cool.  I would like this proposed feature to just Flag all the rows across all 42 clusters, instead of having to click \"Browse this cluster\" and Edit rows -> Flag rows 42 TIMES.\n. From dfhu...@gmail.com on March 08, 2011 21:53:26:\nWhen the icon stops bouncing, could you open your browser and enter\n127.0.0.1:3333\nI'm not sure why that doesn't happen automatically.\n. From m.mara...@gmail.com on July 02, 2011 12:38:43:\nHey,\nI have 10.5.8 too. In my case, the icon appears in the task bar for a moment & then disappears. I think problem is the lack of Java 1.6 on Leopard x-(  \nHere is the stack trace i get on running it via the command line:\nlocalhost:MacOS username$ ./JavaApplicationStub \n[JavaAppLauncher] Requested [1.6+], launching in [1.5] instead.\n[JavaAppLauncher Error] CallStaticVoidMethod() threw an exception\nException in thread \"main\" java.lang.UnsupportedClassVersionError: Bad version number in .class file\n    at java.lang.ClassLoader.defineClass1(Native Method)\n    at java.lang.ClassLoader.defineClass(ClassLoader.java:676)\n    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:124)\n    at java.net.URLClassLoader.defineClass(URLClassLoader.java:260)\n    at java.net.URLClassLoader.access$100(URLClassLoader.java:56)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:195)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:188)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:317)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:280)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:252)\n    at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:375)\n    at java.lang.Class.forName0(Native Method)\n    at java.lang.Class.forName(Class.java:242)\n    at apple.launcher.LaunchRunner.loadMainMethod(LaunchRunner.java:55)\n    at apple.launcher.LaunchRunner.run(LaunchRunner.java:111)\n    at apple.launcher.LaunchRunner.callMain(LaunchRunner.java:50)\n    at apple.launcher.JavaApplicationLauncher.launch(JavaApplicationLauncher.java:52)\nAny ideas?\nI have been able to run it without any issues on Linux.\nThanks,\n..m\n. From tfmorris on July 02, 2011 14:58:07:\nDo you have a Core 2 Duo based machine? If so you should be able to run Java 6. If it's an older core duo machine you may be stuck. http://stackoverflow.com/questions/331777/what-are-my-options-for-running-java-6-on-os-x\n. From m.mara...@gmail.com on July 03, 2011 03:18:47:\nAh i see :)  Thanks for the link, will try it out. I have a Core 2 Duo, but I've been relying on Apple's Java all this time...  Will try it out :)\n. From dfhu...@gmail.com on March 11, 2011 07:48:21:\nThanks for reporting this, but the class TsvCsvImporter currently also handles the case where you don't have the \"split into columns\" option checked. As you phrased it, \"the name of this class is not indicative of its use\". I'm going to close this issue as it's the same as issue #242.\n. From tfmorris on March 21, 2011 15:00:10:\nCan you provide the rest of the requested information please?  In particular, what version of Refine are you using and when do you get this error?\nThis information lives at http://google-refine.googlecode.com/svn/support/releases.js  Are you connected to a network which has access to that URL?\n. From tfmorris on June 07, 2011 06:32:55:\nClosing as unreproduceable.  Feel free to reopen if you can provide more information.\n. Looks like only dates have the type declared explicitly (since they're stored as strings), but the rest can be done using Javascript types.  DataTableCellUI.cell.t=\"date\"  DataTableCellUI._cell.v contains value as either a string, number, or boolean.\n. _From matt.mac...@gmail.com on March 15, 2011 00:41:49:\nI think I may have found my own answer: http://groups.google.com/group/google-refine/browse_thread/thread/18e64a3e4eedfb09/dc94dbc0d3106441?lnk=gst&q=cluster#dc94dbc0d3106441\nI was thinking refine could do this but I'm probably using it as a golden hammer for this.\n. From matt.mac...@gmail.com on March 15, 2011 00:59:51:\nSimple ruby script solved my problem\nrequire 'rubygems'\nrequire 'csv'\nCreate the output file\nCSV.open(\"courts-deduped.csv\", \"wb\") do |csv|\ndeduped_courts = Hash.new\n  CSV.foreach(\"tennis-courts.csv\") do |row|\n    deduped_courts[row[1]] = row\n  end\ndeduped_courts.each do |key, value|\n    csv << value\n  end\nend\n. From tfmorris on October 08, 2011 19:24:06:\nRefine now has a separate facet which can be used for identical duplicates.\n. From tfmorris on December 12, 2011 20:23:39:\nThis was added by the patch in issue #398 and appeared in Refine 2.1.\n. From erjo...@gmail.com on March 17, 2011 09:37:52:\nChoose Jython as expression language; and write:\nreturn value/float(1000)\nOr if you have a column named \"2000\".\nreturn cells['2000']['value']/float(value)\n. From dfhu...@gmail.com on March 19, 2011 22:26:04:\nSusan, could you also try:\nvalue/1000.0\n. From tfmorris on March 21, 2011 15:10:48:\nDid those suggestions work for you?  If not, perhaps you could continue the conversation on the mailing list.\nI don't think it's wise to change the default arithmetic data types at this point in the game (ie automatically promote integers to floating point numbers), so I'm going to close the bug report, but obviously you've got at least a couple of people who are willing to help.\n. From chima...@gmail.com on March 19, 2011 02:50:25:\nThe Export Excel problem with IE (mentioned towards the end of the bug report) is that IE does not allow arbitrary values for the second argument of window.open (e.g., see discussion at http://stackoverflow.com/questions/710756/ie8-var-w-window-open-message-invalid-argument).  Changing the value from \"refine-export\" to \"blank\" should in theory fix this problem for IE.\n. _From chima...@gmail.com on March 19, 2011 03:53:44:\nSounds the same as issue #  122 \nhttp://code.google.com/p/google-refine/issues/detail?id=122\n. From chima...@gmail.com on March 19, 2011 04:03:57:\nMaybe not the same as issue #  122; the SVN trunk already has the fix for #  122.\nCannot reproduce this problem, tried on Windows XP with Chrome 10 (and Firefox 3.6).\n. From chima...@gmail.com on March 19, 2011 04:43:36:\nAn exception is being thrown, but the error handling logic (respondException) has a bug; as a result, the stack trace shows the secondary exception, which does not provide any information on the cause of the original problem.\nPlease look at the console window and see if there are two stack traces there; if so, please post the first one.\n. From jma...@gmail.com on March 19, 2011 16:48:59:\nHere is the output of the console:\n12:45:08.343 [                  command] Exception caught (609ms)\njava.lang.IllegalArgumentException: Invalid column index (256).  Allowable colum\nn range for BIFF8 is (0..255) or ('A'..'IV')\n        at org.apache.poi.hssf.usermodel.HSSFCell.checkBounds(HSSFCell.java:926)\nat org.apache.poi.hssf.usermodel.HSSFCell.<init>(HSSFCell.java:162)\n    at org.apache.poi.hssf.usermodel.HSSFRow.createCell(HSSFRow.java:141)\n    at org.apache.poi.hssf.usermodel.HSSFRow.createCell(HSSFRow.java:119)\n    at org.apache.poi.hssf.usermodel.HSSFRow.createCell(HSSFRow.java:38)\n    at com.google.refine.exporters.XlsExporter.export(XlsExporter.java:78)\n    at com.google.refine.commands.project.ExportRowsCommand.doPost(ExportRow\nsCommand.java:95)\n        at com.google.refine.RefineServlet.service(RefineServlet.java:171)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511\n)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet\nHandler.java:1166)\n        at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\nat org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet\nHandler.java:1157)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:3\n88)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.jav\na:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:1\n82)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:7\n65)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\nat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:1\n52)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:54\n2)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnectio\nn.java:938)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.\njava:228)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec\nutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor\n.java:908)\n        at java.lang.Thread.run(Thread.java:662)\n12:45:08.359 [          org.mortbay.log] /command/core/export-rows/2010-NS-PERIM\nIETER-OPS-Incidents.xls (16ms)\njava.lang.IllegalStateException: STREAM\n        at org.mortbay.jetty.Response.getWriter(Response.java:616)\n        at org.mortbay.servlet.GzipFilter$GZIPResponseWrapper.getWriter(GzipFilt\ner.java:358)\n        at com.google.refine.commands.Command.respond(Command.java:237)\n        at com.google.refine.commands.Command.respondException(Command.java:305)\nat com.google.refine.commands.project.ExportRowsCommand.doPost(ExportRow\nsCommand.java:103)\n        at com.google.refine.RefineServlet.service(RefineServlet.java:171)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511\n)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet\nHandler.java:1166)\n        at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\nat org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet\nHandler.java:1157)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:3\n88)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.jav\na:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:1\n82)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:7\n65)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\nat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:1\n52)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:54\n2)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnectio\nn.java:938)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.\njava:228)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec\nutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor\n.java:908)\n        at java.lang.Thread.run(Thread.java:662)\n12:45:08.718 [                butterfly] GET /favicon.ico main\nLooks like a problem with a maximum of 256 columns?\nI confirmed that my data has 287, but that is not a limitation of Excel 2007.  I did an XML import of my data into Excel and all 287 columns imported without issue.\n. From chima...@gmail.com on March 19, 2011 17:29:35:\nExcel 2007 can support more than 256 columns, but XLS format cannot:\nhttp://msdn.microsoft.com/en-us/library/aa730921%28v=office.12%29.aspx\nhttp://en.wikipedia.org/wiki/Microsoft_Excel\n. From jma...@gmail.com on March 19, 2011 17:37:25:\nI see.  Truncating the columns will have to suffice then.\nI noticed that the patch for the similar issue #  122 merely truncated large cell contents.  May I recommend displaying a notice anytime content is truncated (such as issue #  122 and whatever patch ultimately fixes this bug)?\n. From chima...@gmail.com on March 19, 2011 17:52:05:\nThat would seem reasonable.  It might be best to file this as a separate issue.\n. From jma...@gmail.com on March 19, 2011 18:00:52:\nOkay, I also filed issue #  352.  Thanks.\n. From tfmorris on March 21, 2011 15:08:16:\nSo, to summarize, the Refine's Excel .xls format exporter needs to limit the export to the first 256 columns?\nHas someone filed a bug report for the error reporting problem mentioned on Mar. 18?\n. From tfmorris on June 07, 2011 23:56:45:\nFixed in r2094.  We simply truncate without warning the user.  Better error reporting is the subject of issue #352.\nThe nested exception problem mentioned in comment 5 didn't appear to have an open issue for it, so I created issue #401 (now also fixed).\n. From chima...@gmail.com on March 21, 2011 00:06:51:\nSee issue #  294\n. From iainsproat on March 21, 2011 14:34:45:\nYour issue seems to be with the Google Maps geocoding API rather than Google Refine.  (i.e. The same occurs when using the API without Google Refine)\nYou would be better asking this query directly to the Google maps forum: http://code.google.com/apis/maps/documentation/webservices/forum.html\n. From tfmorris on March 21, 2011 14:40:16:\nThis is a Google Maps API question and should be directed to that team/product (although I'd suggest asking a question in one of their support forums instead of filing a bug report).\n. From techtonik@gmail.com on March 24, 2011 02:51:26:\nSeems like missing images are attached here - http://code.google.com/p/google-refine/issues/detail?id=244\n. From techtonik@gmail.com on April 03, 2011 00:57:08:\nThanks.\n. From tfmorris on June 14, 2011 06:23:23:\nSounds like this could be related to issue #404.\n. From PaulMake...@gmail.com on June 14, 2011 14:17:55:\nI'm not sure - if the file were interpreted as UTF-32LE it'd be completely hosed.\nThis bug was reported on v2.0 which doesn't have the encoding-guessing heuristics, as far as I know.\n. From tfmorris on June 14, 2011 16:41:27:\nCharacter encoding guessing has been in since well before 2.0.\nIf you can come up with a way to reproduce this, it'd be a big help in tracking it down.\nDoes it only effect column names or all data fields?\n. From tfmorris on March 29, 2011 18:55:17:\nThis sounds very similar to issue #107\n. From tfmorris on June 07, 2011 06:37:18:\nClosing as a duplicate.  It should be fixed in the upcoming release, but please feel free to reopen if you find it's not (or it's not a duplicate).\n. From tfmorris on June 06, 2011 21:32:09:\nPatch applied.  Thanks Tomaz!  (and thanks for including a test)\nLook for it in the 2.1 release candidate which will be out soon.\n. From dfhu...@gmail.com on April 12, 2011 18:47:37:\nThank you for your feedback. May I encourage you to use the mailing list instead of this issue tracking system? This is because you are not really filing a specific bug, or a specific feature request, but you're starting a discussion. The mailing list is appropriate for such a discussion.\nThe mailing list is also a place for you to ask how to do something. We will gladly answer your questions there.\n. From jmangr...@gmail.com on April 14, 2011 13:14:04:\ndfhu...@gmail.com,\nThanks for steering me in the right direction.\nAs you can probably tell I'm not a veteran of OpenSource projects and not very familiar with the rules and customs of the community.  I apologize for that and will file my message in the mailing list, as you recommended.\n. From tfmorris on April 17, 2011 16:10:09:\nOr conversely, if you've got a specific feature requests or problem reports, create a separate entry per problem/request so that they can be tracked appropriately.  \nProviding the requested information (software versions, operating system, etc) will also help provide more useful responses.  The performance issues sound you've probably got your system underconfigured for what you're trying to do.\nI'm going to close this.  Feel free to create new entries for specific problems -- or ask questions on the mailing list.\n. From tfmorris on June 10, 2011 22:39:56:\nI suspect that the fixes for issue #325 and issue #262 will help here.  Please test with the 2.1 release candidate and let us know if it resolves your problem.\n. From tfmorris on September 22, 2011 10:33:26:\nSince there's been no feedback in three months, I'm closing this.  Feel free to reopen it if the problem is reproduceable with the current v2.1 release.\n. From XULRunne...@gmail.com on April 20, 2012 18:39:27:\nDoes anyone use Google Refine on mac?  Can you say if this problem still exists?\nWho knows exactly what headless mode does?  I expect that mostly, it doesn't try to run the web browser.  Anything else?\nWe are trying to automate Google Refine through command-line and scripting environment, and we are not finding a lot of instances where anyone else has done this.  I have successfully built a Selenium test case that executes the actions that we want to have done, but my boss has the idea that if any part of a desktop OS is needed to do ETL with Google Refine, then it's not happening for us.\nAnyone out there using headless mode now?\n. From tfmorris on April 20, 2012 23:00:14:\nThe relevant code is in /home/tfmorris/workspace/grefine-all/server/src/com/google/refine/Refine.java \nIn addition to not using the Java Desktop APIs to open a new browser window, headless mode also skips some Mac-specific code to create a menu item.\nThe developer list is the best place to ask questions.  The issue tracker is for bug reports and enhancement requests. (But please rephrase your bosses objection in a way that a software engineer would understand when reposting to the dev list).\n. From tfmorris on April 20, 2012 23:12:43:\nr2492 includes a fix which I believe will make this problem go away, but I don't have a Mac to test with.\nPaul (or anyone else with a Mac) - please verify and reopen if this doesn't cure the problem.\n. From dfhu...@gmail.com on April 15, 2011 16:57:55:\nWe're going to re-work that screen cast. Thank you for your patience.\n. From techtonik@gmail.com on April 16, 2011 15:17:49:\nThere is one more video missing from screencasts page - http://www.youtube.com/watch?v=LovigIDl634\n. From tfmorris on October 08, 2011 19:27:52:\nI think all the screencasts have been redone now.\n. From dfhu...@gmail.com on April 17, 2011 03:39:35:\nChanged from \"Defect\" to \"Enhancement\".\n. From tfmorris on April 16, 2011 19:25:21:\nThis is likely a platform-specific problem for platforms which don't have UTF-8 as their default character encoding.  The current build assumes, but doesn't specify, UTF-8.\n. From techtonik@gmail.com on April 16, 2011 19:30:01:\nI am on Windows.\n. From techtonik@gmail.com on April 16, 2011 19:40:43:\nVista Home edition, JDK 1.6.0_24. Attached patch fixes the issue.\n. From dfhu...@gmail.com on April 17, 2011 03:44:10:\nIt was working on the Mac, but broken on Linux. I \"fixed\" it for Linux, but re-encoding that file. But then it's broken on the Mac now. I'm guessing that we have to explicitly escape those international characters.\n. From tfmorris on April 17, 2011 04:42:55:\nDepending on a platform's default character encoding doesn't really work since they all differ.\nI had already changed the Eclipse project definition and now I've changed the Ant build files to specify utf-8 as the source file character encoding.  So, for now, UTF-8 is the mandated character encoding for all source files. We can change this to something else, but it should be the same for all platforms.\n. From tfmorris on April 18, 2011 17:37:51:\nHappens with released Refine 2.0 as well.  Appears to be something to do with missing recon config data (perhaps not getting saved/restored?).\n. From phil.sal...@gmail.com on May 08, 2011 02:46:41:\nIs there a workaround?  It seems like this feature is straight up broken.\n. From gryanwha...@gmail.com on July 24, 2011 04:13:51:\nI'm having the same issue: A continually spinning dialog box saying \"Working\"  I was able to add columns from freebase yesterday, but then I reconciled more of my data and can no longer add columns.\n. From dfhu...@gmail.com on July 26, 2011 17:52:07:\nr2168 should fix the problem tfmorris reported. I'm not sure about the subsequent reports though.\n. From Ong.Jose...@gmail.com on April 22, 2011 07:08:13:\nWhoops, my mistake, not a bug -- thought I had checked ignore quotes, but hadn't.\n. From tfmorris on April 22, 2011 15:01:46:\nThanks for the update.  Glad you got it working.\n. From dfhu...@gmail.com on April 26, 2011 17:38:29:\nInteresting idea. These could just be \"no-op\" operations.\n. From tfmorris on October 08, 2011 19:31:27:\nI believe the current mode of operation is intentional since there's no easy way of knowing whether the change made by the user invalidates later operations in the history.\nIf your sure that the change you want to make is isolated, you could export the operation history, make the change in an editor, and then re-import the history.\nI'll leave this open for now, but I wouldn't expect any changes in this area in the near future.\n. From dfhu...@gmail.com on April 27, 2011 00:05:27:\nHave you looked through these instructions?\nhttp://code.google.com/p/google-refine/wiki/InstallationInstructions\nHave you tried pointing your browser to http://127.0.0.1:3333/ ?\nBy the way, this issue tracking system is for bugs and feature requests. Questions on how to use the software are better directed to the mailing list google-refine@googlegroups.com. Thanks.\n. From thadguidry on April 27, 2011 14:00:02:\nThere are many ways to group similar cells, names, columnar data in Refine.  Please email our mailing list (mentioned in the wiki) when you have questions, and we'll be glad to answer them there.  And you'll probably learn a few tricks from the community in doing so !, instead of hearing back only from the developers.  Closing this issue as INVALID for now.\n. From tfmorris on April 27, 2011 16:43:41:\nGrouping by alias cluster or name normalization don't seem like completely out of bounds requests to me, but they would require an external database (culture specific in the case of personal names) of aliases and nicknames.\nI'll leave this open pending further discussion, but I would second Thad's suggestion that you'll get better feedback by using the email list where a larger community can contribute ideas.\n. Refine uses the first column as the \"key\" column for the record, so if it is fully populated, Refine assumes each row is an independent record.  The workaround, as you discovered is to move the column someplace other than the leftmost position.  You can see what Refine things makes up a record by switching between Record and Row display modes.\n. From thadguidry on May 03, 2011 13:36:20:\nIt looks like the site itself is very slow to respond, with a query such as http://www.uk-postcodes.com/postcode/CA143YJ.json taking about 4-5 secs to return data. Given that, it would take about 50 hours to retrieve 45000 rows of constituency data by my calculations.  4*45000/60/60  Perhaps the data can be scraped better by going to government sources here ? http://www.direct.gov.uk/en/Dl1/Directories/Localcouncils/AToZOfLocalCouncils/DG_A-Z_LG\nhttp://www.direct.gov.uk/en/Dl1/Directories/index.htm\nAlternatively, perhaps you could ask someone on the ScraperWiki email list to see if someone could finish up and add more data to their scraper which seems to be a start of some of the data your looking for: http://scraperwiki.com/scrapers/council_postcodes/\n. From phi...@yahoo.com on May 04, 2011 09:51:24:\nThanks for your help. It looks like the ScraperWiki is a direct scrape from the government link you posted, but unfortunately they only seem to have postcodes of the council's physical offices, and not all postcodes covered by any one office.\nThe only other source I could find which could give a local authority name for any given postcode is http://mapit.mysociety.org/postcode/CA143YJ.html and this gives the same type of data, but again I had a 1-hour wait before it gave up on 0% yesterday.\nIt's good to know that the hold-up isn't with Refine, it's with the third party websites. My next attempt at a solution will be downloading the open source data from Ordnance Survey and looking the info up straight into Excel (http://www.ordnancesurvey.co.uk/oswebsite/products/os-opendata.html) - thanks again and wish me luck.\n. From tfmorris on October 08, 2011 19:33:11:\nClosing, not a Refine problem.\n. From dfhu...@gmail.com on May 04, 2011 17:09:03:\nI believe this problem has been fixed in trunk/ by r1982:\nhttp://code.google.com/p/google-refine/source/diff?spec=svn2052&r=1982&format=side&path=/trunk/main/src/com/google/refine/expr/functions/date/Inc.java&old_path=/trunk/main/src/com/google/refine/expr/functions/date/Inc.java&old=1613\nIt'll be available in the next release.\n. From tfmorris on May 04, 2011 15:03:59:\nI'll take a look.  As a workaround you can hit that URL with a browser, save the results, and then load the resulting \"data.csv\" file.\n. From tfmorris on September 22, 2011 11:20:31:\nThis works correctly in David's new v2.5 implementation.\n. From thadguidry on May 05, 2011 14:11:36:\nDid you try typing http://127.0.0.1:3333 into the Chrome browser address bar ?  Did Chrome work better for you ?\n. From dfhu...@gmail.com on May 10, 2011 20:59:12:\nPaul, in the Transform dialog box, at the bottom, what choice did you specify for \"On error\"?\n. From PaulMake...@gmail.com on May 10, 2011 21:22:24:\nI'm actually looking at the value in the preview pane, even before submitting it.\nInterestingly, when submitting it, I see \"Text transform on 0 cells in column number: grel:value/value\" -- it's not applied to that cell. (This happens regardless of the \"On error\" setting.)\nI can induce a NaN to appear with value.toString().toNumber()/value\n. From dfhu...@gmail.com on May 12, 2011 17:48:37:\nI'm seeing an exception thrown, too.\n. From PaulMake...@gmail.com on May 10, 2011 17:54:53:\nSorry, the transform there should've been \"value / value\"\n. From dfhu...@gmail.com on September 19, 2011 10:40:29:\nHi Paul, I can't reproduce this in trunk/. Could you check?\n. From tfmorris on November 29, 2011 07:25:47:\nThis still exists in 2.5 RC2.  I'll take a look.\n. From tfmorris on November 29, 2011 07:58:49:\nFixed in r2391.\n. From thadguidry on May 11, 2011 18:23:13:\nthat last line made my head spin a bit, Paul.  What are you trying to do again ?  Put it in layman's terms for me, I'm an idiot.\n. From PaulMake...@gmail.com on May 11, 2011 18:46:18:\nIt's possible to have links in XLS documents, so if you single-click a cell, it'll open in a web browser. To do this, enter a formula in the cell like,\n=HYPERLINK(\"http://10.0.0.22:8000/gallery/:507\";\"view 507\")\nRefine doesn't show either http://10.0.0.22:8000/gallery/:507 or view 507 :(\nNow, in my appserver's that's producing the XLS: if I could look at the HTTP headers and look at the user-agent string,\nif 'Java' in request.META.get('HTTP_USER_AGENT', ''):\n        self.use_hyperlinks = False\nRefine uses Jetty as its appserver and its user agent string is simply \"Java/1.6.0_24\"\nHope that makes more sense :)\n. From PaulMake...@gmail.com on May 11, 2011 18:50:38:\nJust for extra clarification, where Refine is making HTTP requests for XLS is via the Create Project -> \"or data file URL:\"\n. From thadguidry on May 11, 2011 19:41:57:\nHmm, Tom your thoughts on the data file URL method ?\nAs a related or separate issue, I THINK that the export function To Excel that uses POI should be OK for Hyperlink cells btw, but I am not sure if we even tested that, probably need to now also: http://poi.apache.org/apidocs/org/apache/poi/ss/formula/functions/Hyperlink.html\n. From tfmorris on May 11, 2011 21:31:52:\nThat seems like a reasonable request.  I suspect there are other cases where it would be useful to have the server's requests (when it's acting as a user agent/broker) be identifiable.\np.s. dropping links (or any content) on import is definitely a bug.  Please open a separate bug report for it.\n. This was fixed some time ago (before Refine 2.5).  The user agent string for current release is \"Google Refine/2.5 [r2407]\" and the next release will be \"OpenRefine/2.6\"\n. ps. @PaulMakepeace note that the name in the user agent string is changing if you are depending on it.\n. From tfmorris on June 07, 2011 06:49:16:\nStill an issue after upgrade to Apache POI 3.7\n. From PaulMake...@gmail.com on June 07, 2011 09:42:55:\nMy understanding is that this is caused by python-excel's xlwt not writing out a cached formula value. Excel will perform a formula evaluation when it loads the worksheet so this isn't an issue normally, but for situations like this where only the cached value is used it'll show up as a blank.\nIt's possible to either special case this in Refine or have POI do that formula evaluation. Ongoing conversation on python-excel,\nhttp://groups.google.com/group/python-excel/browse_thread/thread/162ff7517d660e68?pli=1 (I'm waiting on the sample UNCALC file)\n. From tfmorris on June 07, 2011 16:08:49:\nThanks for the update.  Please let us know how the discussion turns out.  It it's an xlwt bug, we'll close this.  If it's a POI bug, we'll keep it open to track the new version of POI with the fix.\n. From tfmorris on March 08, 2012 15:05:18:\nAny update on the investigation?  POI is about to close out their 3.8 release.\n. From tfmorris on September 18, 2012 19:56:00:\nIt sounds like the POI and python-excel teams are on this.  If and when they come up with a solution, Refine will pick it up as part of the next POI update, so I don't think we need to track this explicitly.  Closing.\n. From thadguidry on May 16, 2011 15:18:44:\nYou should be able to point refine.ini to the JRE that you want Refine to use.\nJAVA_HOME=C:\\Program Files\\Java\\jdk1.6.0_25\nPaul Makepeace just submitted a patch file for refine.ini and refine.bat via our mailing list.  Which should make the above work correctly.\nI don't think we have JAVA_HOME available as a command line parameter however like we do memory, perhaps we should or could ?\n. From PaulMake...@gmail.com on May 16, 2011 15:37:55:\nI find setting Windows paths a lot of typing hence getting the refine.ini to work :)\nBTW, If you just have a JRE the path is likely to be C:\\Program Files\\Java\\jre6\n. From JasonMVi...@gmail.com on May 17, 2011 15:31:30:\nThanks, guys,  I actually have that already set to \"C:\\Program Files\\Java\\jdk1.6.0_22\"\nSometimes it works; sometimes it fails (when I get it to fail again, I'll post the error).\nBut really I wanted to use this as a request to have Google host refine instead of having to download it and install it locally.  It's a great tool and I would love to easily point my colleagues to it and share it much like I do a google spreadsheet.\n. From dfhu...@gmail.com on May 18, 2011 05:01:30:\nJason, your request makes a lot of sense! It is a challenge to implement, though, as Google Refine started out as and continues to be a desktop tool, very different in architecture than the rest of Google products. I'll mark this issue as \"accepted\" for now, but it'll take sometime to figure out if it's feasible.\n. From tfmorris on May 19, 2011 03:58:51:\nI shouldn't try to speak for David, but my working assumption is that, until he says the branch is ready for alpha or pre-alpha or developer testing, it's basically a private sandbox.\n. From thadguidry on May 19, 2011 13:26:13:\nHey Tom, I've been working with David offline testing it out with him.  Just beginning some early documentation on issues, so we don't lose track of them.  He knows already.  We just need to decide how we are going to track/label 2.5 issues in the long run.\n. From tfmorris on October 08, 2011 19:35:51:\nDoes this bug still exist on the current trunk?\n. From thadguidry on October 10, 2011 14:16:17:\nFixed in Trunk.  Bug no longer exists.\n. From tfmorris on May 25, 2011 05:29:51:\nI'm having trouble understanding what form this would take.  Is it some user defined set of rules (and associated rule language) or ...  \nIs the % a probability that an value is correct or is it the percentage of values which are valid (implying that Refine would have to be able to discern valid/invalid with 100% accuracy).\nIf you could expand on what your envisioning, that might help developers figure out how hard it would be to implement and whether it fits with the goals of Refine.\n. From thadguidry on May 26, 2011 04:01:44:\nEHOPstore, you might be further interested or have an investment interest in using some of the USPS Address Verification & Address Quality solutions at http://www.usps.com/business/addressverification/welcome.htm  or contact them directly like I have done in the past: http://www.usps.com/ncsc/ziplookup/contactinfo.htm  At my job we have a Talend ETL process at night that scrubs one of our databases against one of those vendor software packages (CASS Certified) and we review using our AEC and even clean sometimes manually with simple tools, including Refine at times.  You might also look at Orange http://orange.biolab.si and perhaps try a learner and classifier solution to the problem, if the data set sample is large enough to support predictions. (just ask them for help on their forum).  Your probably looking for something like a custom reconciliation service to use in Refine that would utilize a CASS certified vendor address verification (or if you don't need a solution to be CASS certified, then perhaps alternatively a Google Maps API Premier license or another web api factory out there)  Outside of what I just mentioned, you can certainly do a lot with just Faceting, Splitting, and Crossing between project data sets as demonstrated here: http://feedproxy.google.com/~r/ouseful/~3/yCUHpNJghxo/\n. From justin.w...@gmail.com on May 20, 2011 15:10:18:\nI meant to flag this as an enhancement, sorry for the confusion\n. From thadguidry on May 20, 2011 15:46:35:\nConcerning your first question, exporting a list of the transformations...\nLook at Video 2 here http://code.google.com/p/google-refine/  at around 7:40 time frame which shows how.\n. From justin.w...@gmail.com on May 20, 2011 16:11:42:\nNice, I had missed that part of the video.\nI think it would be handy if in addition to exporting a JSON snapshot of transformations, you could export code that would execute those operations.  I'm imagining the ability to dump out a shell script, so that you could\ncat similar_data_set | refine_generated_parser.rb --use refine_generated_transforms.json > reformated_textfile_csv_etc.txt\nIf the underlying data format chances, you could throw the json back into Refine as shown in the video to update.\nSome way of sharing around transformations (not sure if Freebase does this) would also be cool.  So you could throw in a --use http://google-refine/transforms/wiki-table-actors.json\nJustin\n. From tfmorris on May 20, 2011 17:22:15:\nYou might want to review the discussion archives for past conversations about batch/headless running of Refine operation histories:\nhttp://groups.google.com/group/google-refine/browse_thread/thread/ae6ed4a829f0ece9/edbe458ad1070de9\nhttp://groups.google.com/group/google-refine/browse_thread/thread/19f4b56df081268c/f6fee660946b80d0\nhttps://groups.google.com/forum/#!topic/google-refine/RAqB4rY_3GE/discussion\n. From justin.w...@gmail.com on May 20, 2011 17:26:01:\nWill do, thanks.  Feel free to close the issue if redundant.\n. From PaulMake...@gmail.com on May 20, 2011 17:27:10:\nTake a look more specifically at https://github.com/PaulMakepeace/refine-client-py/blob/master/google/refine/refine.py#L294\nThat method will directly take exported transforms etc.\nIf you ask on the list I can provide some more help & improve the docs. \n. From tfmorris on May 20, 2011 17:55:50:\nI'm going to close this.  If, after reviewing currently available options, you find that you still have an unmet need which falls within the scope of the Refine, feel free to create a new enhancement request.  If you have multiple requests (e.g. export + batch ops), please create a separate enhancement request for each.\nIf you're unsure about whether something might be possible with the current software, the email list (Google Group) is a great place to get help.\n. From tfmorris on May 20, 2011 23:39:33:\nI suspect that this is a duplicate of issue #237.  Unfortunately the fix for that just missed the 2.0 release.\nYou might be able to workaround this by editing the metadata.json file associated with the project and changing \"encoding\":\"null\" to \"encoding\":\"ISO8859_1\" or whatever the default character encoding for your system (or, perhaps, to the character encoding that you think was used for the file).\n. From francisc...@gmail.com on May 21, 2011 00:26:08:\ncool, will try that on monday when i get back to the office. Also, i looked at issue #237 and yes, this seems to be a duplicate, as after i filed this i tried again and got the same error message as that report.\n. From dfhu...@gmail.com on May 23, 2011 13:55:50:\nI think this problem occurs because there isn't enough content in the file for Refine to guess where the record elements are. The next version of Refine will require you to pick the record elements yourself and hopefully won't have this problem.\n. From dfhu...@gmail.com on May 23, 2011 14:07:51:\nJeff, that error message is produced by the JVM. You're right that it would confuse some users. But I think lowering the memory to below 256M is a really rare case. I'd expect people to use the default of 1024M, and then increase it to handle larger data sets. I'm inclined to close this as \"Won't Fix\", unless there's any objection.\n. From jeff.al...@gmail.com on May 23, 2011 15:26:02:\nI had to lower the memory to even get it to start at all, and there was no useful warning message explaining why it was failing. Here's what would have helped me get started with refine without having to read the shell script and file this bug:\nmem=`awk '/MemTotal/ {print $2}' /proc/meminfo \nif [ \"$mem\" -lt 768000 ]; then\n  echo \"Warning: your machine has less than 768 megs of RAM. Starting Refine in low memory mode (-m 256M). You probably won't be able to edit larger projects without out of memory errors.\"\n  REFINE_MEMORY=256M\nfi\nThanks,\n  -jeff\nPS: It does work ok with less memory, at least to work through tutorial-sized experiments.\n. From jeff.al...@gmail.com on May 23, 2011 15:29:08:\nAlso, just before running the jvm, there should me a check that REFINE_MEMORY is not below 256M, or else you get the confusing error message.\n. From tfmorris on May 25, 2011 05:25:07:\nI agree with David that this is an exceedingly rare use case and would support closing the bug report, but I'll leave it to him to make the final decision.\n. From jeff.al...@gmail.com on May 25, 2011 07:21:36:\nI don't think it is rare. Here's how to reproduce it:\na. use a laptop from 2005 with 500 megs of RAM and no swap configured\nb. download refine\nc. run it\nd. it won't work with a strange error message about memory\ne. discover -m argument, try with 128M because there's no help suggesting another number\nf. get a different error that's also misleading\nI REALLY REALLY had to want to run Refine to overcome that out of box experience. Others won't.\nHere's a simpler proposed fix: make the help message for -m say, \"values less than 256M are not allowed\", then add the code to prevent them.\n'Nuff said.\n. From PaulMake...@gmail.com on May 25, 2011 10:01:47:\nOne problem is parsing & detecting memory, in a portable way (OS X, Linux, etc): the JVM supports suffixes like \"2G\" in the -Xm[sx] parameters etc; OS X doesn't have /proc/meminfo; etc.\nOne compromise might be -m '' which turns off all the heap settings. There's some argument to say that that's a useful thing in any case. Setting the heap sizes doesn't do what a lot of people expect it to...\nCertainly, a comment in the wiki page and usage message seems worthwhile.\n. From n...@apache.org on May 24, 2011 15:20:35:\nThere should be another 3.8 beta release along shortly (vote is expected next week), and 3.8 final is likely to be a month or so (there are two new features relating to writing to be finished first). \nIf you can, I'd suggest you try with a 3.8 beta - there have been several fixes that you may benefit from since 3.7 final, see http://poi.apache.org/changes.html\n. From tfmorris on May 25, 2011 06:39:17:\nI've updated to Apache POI 3.7.  We can update to 3.8 once it's released.\n. From tfmorris on January 27, 2012 22:38:41:\nI just checked and POI is now collecting changes for 3.8 beta 6 (!), so there's still no stable release available.  I'll continue to check periodically.\n. From tfmorris on March 08, 2012 15:03:53:\nThe POI team is forecasting the 3.8 release for March or April (ie soon).\n. From tfmorris on March 29, 2012 18:53:23:\nI've updated use to the recently released POI 3.8\n. From dfhu...@gmail.com on May 25, 2011 21:50:31:\nThanks, Paul. Checked in r2070.\n. From tfmorris on June 14, 2011 16:34:05:\nissue #408 has been merged into this issue.\n. From tfmorris on May 25, 2011 05:21:21:\nHi.  The issue tracker is reserved for bug reports and feature requests.\nPlease ask questions on the mailing list/group http://groups.google.com/group/google-refine\n. From tfmorris on October 08, 2011 19:43:13:\nDoes this problem still exist when using Refine built from the SVN trunk?  If so, can you provide a copy of the data file?\n. From raoulwis...@gmail.com on November 06, 2011 16:26:33:\nI'm experiencing the same issue here (with SVN r2362). I have sent you the XML file through e-mail. Only 8 out of 10 records are correctly identified, the other 2 do appear but have no record ID assigned.\n. From thadguidry on November 06, 2011 17:28:59:\nMy original issue with the index is resolved now in r2363 for my Italian Opera data test file (attached).  Aubrey Da Venne is now index 4 and Tamar is 5, correctly so.\n. From tfmorris on November 06, 2011 21:26:19:\nThad - Thanks for the file.  At a glance, it appears that there may still be a problem since I got 1341 records, but a text facet on the topic ID says there are 1937 choices.  As an example, record 819 has four topic Ids: Type, son-pochi-fiori, roberto, and dorotea.\nRaoul - I received your file and it looks like your file is also affected by the bug in issue #137.\nI'm going to close this as a duplicate.  Please follow issue #137 for updates.\n. From tfmorris on October 08, 2011 18:56:48:\nThanks for the patch.  The new UI has a reset button for the text filter.\nConcerning the white space changes, my preference is for patches to be the minimum size necessary to achieve what's needed.  I know that means turning off automatic cleanups, style fixers, etc, but it makes the patches much easier to review.\n. From thadguidry on September 27, 2011 15:05:07:\nVerified Fixed in Firefox 6 and Google Chrome 14.0.835.186 m\n. From dfhu...@gmail.com on September 27, 2011 09:43:34:\nCould you test again for whatever data you used? I think it's fixed now.\n. From thadguidry on September 27, 2011 15:01:02:\nVerified as fixed with my same TSV test files.\n. From dfhu...@gmail.com on September 27, 2011 09:42:52:\nIt seems to be working for me now.\n. From thadguidry on September 27, 2011 14:10:02:\nNOT FIXED in Trunk using Chrome 14.0.835.186 m on Windows 7 64bit.\n1. I tried another JSON file from ScraperWiki.com downloaded from https://scraperwiki.com/docs/api?name=us_news_top_400_universities#sqlite (see attached JSON file itself)\n2. Create Project\n3. Choose Files.. us_news_top_400_universities.json\n4. Click Next, and Preview shows. Great.\n5. Click on Record field \"school\".  (In other words, I want ONLY the school column)\n6. Preview then changes to blank preview window with no columns in dataPanel except a small light blue square in upper left.  No js errors in console shown.\n. From dfhu...@google.com on September 28, 2011 03:38:27:\nThis issue was closed by revision r2258.\n. From thadguidry on September 28, 2011 18:03:58:\nVerified as Fixed with previous attached universities file.\n. From thadguidry on May 30, 2011 13:14:54:\nTested and useful.  I was able to add a text facet along with this duplicates facet to quickly show the values of my duplicates (which were sorted by count, but a long list!)  Nice, useful patch, Paul!\n. From PaulMake...@gmail.com on June 02, 2011 13:22:52:\nThanks Thad.\nAny chance we could get this in for 2.1? I assume it's pretty innocuous. It would help us a lot here & we can get a bunch of folks testing 2.1 full time.\n. From dfhu...@gmail.com on June 06, 2011 20:54:54:\nPatch applied in r2084.\n. From tfmorris on June 08, 2011 00:22:38:\nThis has a dependency on Apache commons-codec 1.5.  We are currently at 1.4.\n. From tfmorris on June 09, 2011 19:45:02:\nWe've upgrade to the 1.5 release of the Apache codecs and support for this new codec has been added.  In addition to the clustering dialog, it's also available for use in GREL the same as the other coders.\n. From tfmorris on June 07, 2011 23:52:57:\nFixed in r2093.\n. From tfmorris on June 10, 2011 22:30:27:\nThe import and export sides sound like two independent problems, but I'll have a look at both.\n. From bad...@gmail.com on June 10, 2011 23:45:53:\nDavid told me on the mailing-list that the export problem is actually not a problem but a feature of CSV/TSV files. See http://en.wikipedia.org/wiki/Comma-separated_values#Basic_rules.\n. From tfmorris on June 11, 2011 22:31:50:\nReassigning to Iain for review since he did the original \"ignore quotes\" implementation.  He should be more familiar with the intended behavior.\nWe're apparently running a private patched version of OpenCSV 2.2.  OpenCSV 2.3 has been released since then, but didn't include the patch.\n. From iainsproat on June 13, 2011 14:21:04:\nIgnore quotes affects how the parser treats the separator character only, it doesn't stop the parser chomping the quotation marks.\ne.g.: For the line:\nhello\", world\"\nWith Ignore Quotation Marks set to false the parser would return one token:\nhello, world\nWith Ignore Quotation Marks set to true the parser would return two tokens:\nhello\nworld\nIn both cases the quotations are chomped which is the correct behaviour.\nThe preservation of quotation marks is a separate feature.\nI'm not sure if OpenCSV has a way to preserve quotation marks.  (I'll take a look).  If so (or it's something I can add easily), would the preservation of quotations be a feature that we would like to see on the importer page?\nWith regards to our private patched version of OpenCSV 2.2; the patch is now in the OpenCSV trunk and will be included in version 2.4 (I'm not sure of the release date of that though) https://sourceforge.net/tracker/?func=detail&aid=3018599&group_id=148905&atid=773543\nUntil OpenCSV 2.4 is released I think it may be better practice to build openCSV from its current trunk (as it will have all the improvements in the 2.3 release as well - what those are, I'm not too sure I can't find a changelog!) and use that, rather than using our branched 2.2.  Any objections to doing this?\n. From PaulMake...@gmail.com on June 13, 2011 14:31:03:\n\nwould the preservation of quotations be a feature that we would like to see on the importer page?\n\nI think it'd be better that whatever is generating the files to begin with are fixed to conform to the CSV \"standard\" (pick one). That is, wrap fields in quotes and double up quotes in the data.\n. From tfmorris on June 13, 2011 16:40:04:\nThe more I learn about this, the less I'm inclined to continue down this convoluted path of special casing things.  I think we'd be better off just adding better documentation about what \"ignore quotes\" does and pointing people at documentation on how to create well-formed CSV files.\nAs for using the OpenCSV trunk, that seems risky to me because there's no telling how stable they keep their trunk.  We could reapply Iain's patch to 2.3, but unless 2.3 has bug fixes we need, that may be more trouble than it's worth.  (Whatever path we choose, we should make sure opencsv-sources.jar matches what we use -- I got very confused during debug when it didn't contain the constructor we were calling)\n. From iainsproat on June 13, 2011 17:57:31:\n\nAs for using the OpenCSV trunk, that seems risky to me because there's no telling how stable they keep their trunk.\n\nThinking about it, the patched \"2.2\" version was (if I recollect correctly) built off of the trunk HEAD revision at the time (June 2010)....\n. From bad...@gmail.com on June 14, 2011 14:40:56:\nMy opinion on this issue is that the import mechanism of Refine should be made similar to the export mechanism, ie allow importing formats and not what appears to be a self-written splitting method (which I understood it isn't). Using formats allow to point to the documentation of [CTP]SV and require the import files to be compatible with the standard.\n. From iainsproat on June 14, 2011 14:53:19:\n\nrequire the import files to be compatible with the standard.\n\nI think we should expect import files to be for some part non-compatible with standards as part of the definition of \"messy data\".  (obviously the xml or json parsers will be more likely to choke on non-compatible formats than with csv)\nThere's some very large changes on the way with the importer UI, which I hope will help greatly with importing data.  I like the idea of having small bits of inline documentation though, perhaps as a tooltip.  I'm not totally sure what David has in his revised importer UI though. (David?)\n. From thadguidry on June 14, 2011 15:11:52:\nRefine is a clean up tool.  Built for cleaning even non-standard formats.  That includes CSV and it's variations outside of the pseudo standard http://tools.ietf.org/html/rfc4180  There is strict rfc4180 and non-strict and Refine allows for both, any, and all text formats to be dealt with.  The method of handling quoted strings and separated data fields by various ways is handled quite well now actually in my opinion.  Sometimes the separation and cleanup can be handled after the import.  But, I do agree with Paul that we should adhere to whatever agreed upon standard for handling double-quotes, single quotes or what someprograms just call \"text-qualified\" fields as noted in all the variations of CSV formatting here: http://www.csvreader.com/csv_format.php  Pick the method handling for quotes, stick to it throughout, and document the hell out of it so users are not confused.\n. From dfhu...@gmail.com on June 15, 2011 17:42:19:\nHi Iain, I've been mostly working on the plumbing and not so much on the details like tooltips. Perhaps you could check out the branch new-importer-ui and see what hints are missing? From chatting offline with Thad, he and I think we have all the importer levers now for at least the formats TSV/CSV/*SV, JSON, XML.\nI'm attaching some screenshots to show the development so far.\n. From tfmorris on June 13, 2011 21:09:52:\nJust to recap Paul's analysis from the email so that we have all the info in one place, he suspects that short reads from the input stream can leave the buffer only partially filled, leaving a large number of NULs which corrupt the character set analysis.\n. From tfmorris on June 14, 2011 05:58:32:\nFixed in r2102.  Thanks for the patch.  It was committed mostly as given except I conditionalized the wrapping of the input stream to only do it if the input stream doesn't support mark/reset.\np.s. It's a nit, but it'll make your patch less noisy if you turn off automatic \"cleanup\" of formatting in your editor/IDE.  Not a big deal for a little patch like this, but could complicate things for a larger patch.\n. From dfhu...@gmail.com on September 20, 2011 10:05:21:\nSorry this escaped my attention before. Have you resolved this issue? By the way, for this kind of emergency problems, it's best to email the mailing list.\n. From helenadeus on September 27, 2011 09:51:57:\nYes, sorry - it was a problem with an extension. I removed the folder and it started working again.\n. From thadguidry on June 13, 2011 20:57:32:\nPaul, does the label change from rows mode back to records mode ever ?  or does it stay in row based ? Does it perform differently using either mode as the starting point, I wonder ? \n. From PaulMake...@gmail.com on June 13, 2011 21:23:38:\nThad - am home now; maybe you could try it - it shouldn't be more than a minute to repro. Good so have a 2nd pair of eyes! Section 5, \"Transpose Fixed Number of Rows into Columns\" http://davidhuynh.net/spaces/nicar2011/tutorial.pdf\n. From elder...@gmail.com on August 29, 2012 10:41:13:\nI have also encountered this problem after using the \"Transpose rows into columns\" function. The \"Add column\" function adds a column but overwrites the data in a second column. If undoes the \"Transform and Add column\" and  tries Transform and Add column again the data overwritten in the column to the right of the one first affected. \nThe problem disappears one the Undo, Redo Transform / Add Column until all of the original columns have been affected.\nThe bug is addressed better by exporting the data after the initial transformation and then reimporting to Google Refine. Add column then works as it should.\nMy original data was a simple text file with no field delimiters.\n. I'll put the input data from attachment fixed-rows.csv in the next comment so that it doesn't get lost.\n. Tom Dalton sent $3700 to Betty Whitehead on 01/17/2009\n377 El Camino Real\n\"San Jose, CA\"\nStatus: received\nMorgan Lawless received $10500 from Bob Henselman on 02/05/2009\n2798 Lancaster Dr.\n\"New York, NY\"\nStatus: deposited\nEric Bateman sent $22000 to Liz Benedict on 03/02/2009\n89 Deerfield Cr.\n\"Springfield, WA\"\nStatus: received\nRobert Hartfort received $20000 from Ron Ingleman on 03/28/2009 \n198 Broadway Ave.\n\"Saratoga, CA\"\nStatus: unknown\n. The operation history provided by Paul doesn't work in the current version, but reproducing the steps by hand shows that the bug still exists.  I'll put the updated operation history in the next comment.  Additionally, attempting to undo all the way back to the beginning (Step 0), gives an alert with just \"2\" as the message and the following stack trace on the server console:\n17:38:25.743 [                   refine] POST /command/core/undo-redo (11810ms)\n17:38:25.744 [                  command] Exception caught (1ms)\njava.lang.ArrayIndexOutOfBoundsException: 2\n        at com.google.refine.model.RecordModel.addRootKeyedGroup(RecordModel.java:276)\n        at com.google.refine.model.RecordModel.computeKeyedGroups(RecordModel.java:217)\n        at com.google.refine.model.RecordModel.update(RecordModel.java:140)\n        at com.google.refine.model.Project.update(Project.java:256)\n        at com.google.refine.model.changes.MassRowColumnChange.revert(MassRowColumnChange.java:99)\n        at com.google.refine.history.HistoryEntry.revert(HistoryEntry.java:155)\n        at com.google.refine.history.History.undo(History.java:236)\n        at com.google.refine.history.History.undoRedo(History.java:172)\n        at com.google.refine.history.HistoryProcess.performImmediate(HistoryProcess.java:82)\n        at com.google.refine.process.ProcessManager.queueProcess(ProcessManager.java:94)\n        at com.google.refine.commands.history.UndoRedoCommand.doPost(UndoRedoCommand.java:68)\n        at com.google.refine.RefineServlet.service(RefineServlet.java:179)\n...\n17:38:31.749 [                   refine] POST /command/core/undo-redo (6005ms)\n. Simplified operation history:\n1. Transpose rows to columns - 4 rows\n2. Add column based on 1st column with value of \"foo\"\nUndoing the second operation deletes data from the 2nd column.  Redoing it puts \"foo\" in both cols 2 & 3.\n[\n  {\n    \"op\": \"core/transpose-rows-into-columns\",\n    \"description\": \"Transpose every 4 cells in column Column 1 into separate columns\",\n    \"columnName\": \"Column 1\",\n    \"rowCount\": 4\n  },\n  {\n    \"op\": \"core/column-addition\",\n    \"description\": \"Create column Foo at index 1 based on column Column 1 1 using expression grel:\\\"foo\\\"\",\n    \"engineConfig\": {\n      \"facets\": [],\n      \"mode\": \"row-based\"\n    },\n    \"newColumnName\": \"Foo\",\n    \"columnInsertIndex\": 1,\n    \"baseColumnName\": \"Column 1 1\",\n    \"expression\": \"grel:\\\"foo\\\"\",\n    \"onError\": \"set-to-blank\"\n  }\n]\n. Bumping priority due to data loss and adding @PaulMakepeace so OP can find bug's new home.\nI suspect what's happening is that the Transpose command isn't leaving the columns set up correctly.\n. From tfmorris on June 14, 2011 17:08:30:\nThanks for the suggestion.  There's so much work to do on the core code, that I think it's very unlikely the Refine team would branch into web browser development, particularly when there are already so many teams focused on that problem.\nA much simpler solution to the IE problem would be for the Refine server to refuse to run with an IE client, but then those browser checks would need to be removed when the IE team (hopefully) fixes their bugs.\n. From iainsproat on June 14, 2011 08:43:07:\nAre you talking about this file?\nhttp://code.google.com/p/google-refine/source/browse/trunk/refine.ini\nI don't see the word \"REFINES\" in it.\n. From PaulMake...@gmail.com on June 14, 2011 08:57:39:\nThis is fixed already in 2.1rc1 which you can try out here,\nhttp://code.google.com/p/google-refine/downloads/list\n. From chen...@gmail.com on June 14, 2011 11:05:57:\nOkay then.. I believe I was using something like google-refi-2.0-r1836.tar.gz\n. From iainsproat on June 14, 2011 12:06:43:\nThanks for reporting the issue Chen.\nI've now closed this issue.\n. From tfmorris on June 14, 2011 16:34:05:\nWell, it wasn't \"over a year\" and it was already fixed, but it was actually a real bug, so let's not mark it invalid.\n. From tfmorris on June 14, 2011 16:58:55:\nI'm not sure we can use them directly, but what we probably want to do is something equivalent to the Java CollationKeys (http://download.oracle.com/javase/1.4.2/docs/api/java/text/CollationKey.html) that we use for sorting.  \nBy allowing the user to specify the \"strength\" we could let them control whether letter case, accents, or language specific things like the eszett are normalized.\nThis would be a general solution for all languages rather than something specific to a single character or language.\n. From PaulMake...@gmail.com on June 14, 2011 09:11:31:\nLet's try that again,\nPurpose of code changes:\nConvert UNIX line-endings to DOS as a courtesy to Windows users (esp those using Notepad)\nWhen reviewing my code changes, please focus on:\nsvn diff doesn't produce any output, hence attaching whole file. It was produced with 'unix2dos refine.ini'\n(refine.bat has DOS line endings already)\n. From tfmorris on June 14, 2011 17:02:51:\nThere's an interaction with SVN since it does line ending conversions as well as the kit build since it may be built on a different platform than the target, but I'll have a look.\n. From tfmorris on October 08, 2011 19:57:14:\nI've switched the SVN eol-style property from Native to CRLF for this file, which I think should have the desired effect.\n. From tfmorris on August 23, 2012 16:21:04:\nMartin - please create a new issue with your problem.  It's different than this one and this is marked as fixed, so there's a good chance your problem will get lost.\n. From tfmorris on June 14, 2011 17:04:05:\nAs discussed on the mailing list, the fact that the importer is single threaded is unlikely to have anything to do with your problem.\n. From PaulMake...@gmail.com on June 18, 2011 07:19:48:\nTL;DR but looks like same as http://code.google.com/p/google-refine/issues/detail?id=38\n. From dfhu...@gmail.com on September 27, 2011 09:39:32:\nThe column headers now freeze vertically (and scroll horizontally). But the row headers don't freeze. Is this good enough?\n. From dfhu...@gmail.com on September 27, 2011 09:40:43:\nMarked as Post 2.5 for making row headers freeze as well.\n. From tfmorris on September 18, 2012 17:58:49:\nSince issue #38 covers column headers specifically we'll focus this on the remaining piece.\n. From tfmorris on October 08, 2011 20:05:31:\nWe've fixed a bug (issue #440) which would cause the project to get purged from memory (and then immediately reloaded), interrupting the fetching of URLs if it ran longer than one hour, which may have been part of your problem.\nIt's an inherent artifact of the current implementation that the actual Add Column doesn't happen until all the data has been collected.  Because of this, you should plan to operate on data sets which are small enough that you have a high probability of completing the data collection without being interrupted.  With the bug fix in place, we've had reports of users running fetches for over 24 hours successfully, so as long as your infrastructure and the service that you're hitting is reliable, there's no inherent limit in how long you can run.\nWe'll be producing a release candidate containing this bug fix shortly, so please retest when that's available.\n. From tfmorris on October 29, 2011 20:17:56:\nRefine 2.5 RC1 is available here: http://code.google.com/p/google-refine/downloads/list?can=1\nLet us know if it fixes your problem.\n. From dfhu...@gmail.com on September 20, 2011 09:40:17:\nThis seems to be more of an issue with Windows 7 Enterprise 64-bit rather than with IE9. Can you reproduce the problem on another machine with Windows 7?\n. From tfmorris on June 23, 2011 20:58:07:\nFor your first example, I think your values are backwards.  GREL gives 1 when it should give -1.\nI'm not sure if the evaluation order is just backwards (ie right to left instead of left to right) or if it's more complicated than that.\n. From tfmorris on June 23, 2011 23:44:27:\nFixed in r2123. Both - and + were evaluating the right-hand side as a sub-expression instead of a simple term.\n. From thadguidry on September 19, 2011 17:57:32:\nFor me, this issue only seems to happen with Firefox.  I have not experienced it on Google Chrome. (attached)\n. From tfmorris on October 08, 2011 18:02:26:\nStefano reports that this is a random non-fatal event which can be cleared by reloading the page.  Lowering priority...\n. From dfhu...@gmail.com on September 27, 2011 09:23:28:\nThe re-order column command under the All menu allows you to do this. I'm going to mark this as WontFix and change \"Reorder Columns\" to \"Re-order / Remove Columns\".\n. From thadguidry on September 27, 2011 13:37:52:\nCool. That works very well.\n. From thadguidry on July 14, 2011 13:13:56:\nI ran your attached file through Refine 2.1 and it looks much better, without incorrect splitting on some unicode characters.  Refine 2.1, which fixes several bugs, is being released today or tomorrow, so look forward to that update.  Great dataset on SKOS by the way !\n. From tfmorris on July 14, 2011 16:12:33:\nAlthough the release hasn't been officially announced, the final kits are available here: http://code.google.com/p/google-refine/downloads/list\n. From dfhu...@gmail.com on September 27, 2011 09:26:29:\nThis seems to work in trunk/ (upcoming 2.5), too. Marked as fixed.\n. From tfmorris on November 05, 2011 16:23:09:\nI think that's actually a bug.  We shouldn't be stripping leading and trailing quotes unless they're the only quotes in the field (and arguably not even then).\n. From thadguidry on November 05, 2011 17:06:17:\nAgree.  I was asking to make the \"Line-based importer\" also have the checkbox option to strip or keep outer quotes for fields.  It appears that this isn't even the case as you stated!  The function \"Quotation marks are used to enclose cells containing column separators\" is a question we are asking the user, instead of describing the function we are going to apply: Strip outer quotes (Yes/No).  Regardless, we need to have a checkbox option on both Line-based and CSV/TSV importers, where the user has a choice to keep outer quotes or not.  Does everyone agree on that issue ?\n. From dfhu...@gmail.com on November 05, 2011 19:35:39:\nSo, from what I understand, we need 2 different checkboxes:\n1. Quotation marks are used to enclose cells containing column separators (default: on)\n2. (Only available if checkbox 1 is on) Strip out such quotation marks (default: on)\nIs that correct?\n. From thadguidry on November 06, 2011 03:07:11:\nYes, I think having both those options would make Refine very flexible and comparable to other tools that have similar options.  Granted, it's something that you also need to have the option of putting the Quotes back into place upon export operations...which I think we have already, correct?\n. \"Apache Creek Golf Club\",\"3401 S Ironwood Dr\",\n\"Apache Junction\",\" AZ\",\"http://www.apachecreekgc.com\"\n\"Arizona City Club Inc\",\"13939 S Cleator Rd\",\n\"Arizona City\",\" AZ\",\"http://www.golfwebsite.ws/14527/\"\n\"Casa Grande Golf Course & Rv Resort\",\"3290 S Montgomery Rd\",\n\"Casa Grande\",\" AZ\",\"http://www.casagrandegolfandrv.com/\"\n\"Dave White Municipal Golf Course\",\"2121 N Thornton Rd\",\n\"Casa Grande\",\" AZ\",\"http://www.casagrandeaz.gov/web/guest/parks\"\n\"Francisco Grande Hotel & Golf Resort\",\"2684 W Gila Bend Hwy\",\n\"Casa Grande\",\" AZ\",\"http://www.franciscogrande.com/\"\n\"Gold Canyon Golf & Resort\",\"6100 S Kings Ranch Rd\",\n\"Gold Canyon\",\" AZ\",\"http://www.gcgr.com/sites/courses/custom.asp?id=985&page=57236\"\n\"Gold Canyon RV & Golf Resort\",\"7151 E Hwy 60\",\n\"Gold Canyon\",\" AZ\",\"http://www.robertsresorts.com/gold_canyon_az/active_community/gold_canyon_community.html\"\n\"Grande Valley Golf Club\",\"1505 S Toltec Rd\",\n\"Eloy\",\" AZ\",\"http://www.gvrgolfclub.com/\"\n\"Kearny Golf Club\",\"301 Airport Rd\",\n\"Kearny\",\" AZ\",\"http://www.golflink.com/golf-courses/course.aspx?course=39134\"\n\"Mountain View Golf Club\",\"38759 S Mountain View Blvd\",\n\"Saddlebrooke\",\" AZ\",\"http://www.robson.com/page.cfm?name=SaddleBrk_Golf\"\n\"Mountainbrook Village Golf Club\",\"5783 S Mountainbrook Dr\",\n\"Gold Canyon\",\" AZ\",\"http://www.mountainbrookgolf.com/mountainbrookgolf/start_page.php\"\n\"Mission Royale Golf Course\",\"11 Mission Royale South #1\",\n\"Casa Grande\",\" AZ\",\"http://www.missionroyalegolfclub.com\"\n\"Oasis Golf Club\",\"5764 E Hunt Hwy \",\n\"Florence\",\" AZ\",\"http://www.golflink.com/golf-courses/course.aspx?course=2053256\"\n\"Palm Creek RV Resort\",\"1110 N Henness Rd\",\n\"Casa Grande\",\" AZ\",\"http://www.palmcreekgolf.com/\"\n\"Poston Butte Golf Course\",\"6100 W Merrill Parkway\",\n\"Florence\",\" AZ\",\"http://www.postonbutte.com/index.php\"\n\"Superstition Mountain\",\"8000 E Club Village Dr\",\n\"Apache Junction\",\" AZ\",\"http://www.superstitionmountain.com/sites/courses/layout9.asp?id=917&page=51355\"\n\"Queen Valley Golf Assoc\",\"600 N Fairway Dr\",\n\"Queen Valley\",\" AZ\",\"http://www.queenvalleygolfcourse.com\"\n\"Roadhaven Golf Course\",\"1000 S Idaho Rd\",\n\"Apache Junction \",\" AZ\",\"http://www.roadhaven.com\"\n\"Saddlebrooke Golf Club\",\"64500 E Saddlebrooke Blvd\",\n\"Saddlebrooke\",\" AZ\",\"http://www.saddlebrooke.org/\"\n\"San Manuel Golf Club\",\"26950 N Reddington Rd\",\n\"San Manuel\",\" AZ\",\"http://www.golflink.com/golf-courses/course.aspx?course=44734\"\n\"Southern Dunes\",\"48456 W Hwy 238\",\n\"Maricopa\",\" AZ\",\"http://www.golfsoutherndunes.com/index.php\"\n\"Duke At Rancho El Dorado\",\"42660 W Rancho El Dorado Pkwy \",\n\"Maricopa\",\" AZ\",\"http://www.thedukegolf.com/sites/courses/layout9.asp?id=530&page=27167\"\n\"Link's Golf Club at Queen Creek\",\"445 E Ocotillo Rd\",\n\"Queen Creek \",\" AZ\",\"http://www.linksqueencreekgolfclub.com/index.htm\"\n\"Three Parks Fairways\",\"3831 N Florence Blvd\",\n\"Florence\",\" AZ\",\"http://www.golflink.com/golf-courses/course.aspx?course=37734\"\n\"Tierra Grande Country Club\",\"813 W Calle Rosa\",\n\"Casa Grande\",\" AZ\",\"http://www.thegolfcourses.net/golfcourses/AZ/14647.htm\"\n\"Hohokam Golf Course\",\"8324 E Hwy 287\",\n\"Coolidge\",\" AZ\",\"http://www.worldgolf.com/courses/usa/arizona/coolidge/hohokam-golf-course-public.html\"\n\"Robson Ranch Golf Club\",\"5750 N Robson Blvd\",\n\"Eloy\",\" AZ\",\"http://www.robson.com/page.cfm?name=RobsonRanchCG_Golf\"\n\"Saddlebrooke Ranch Golf Club\",\"62493 E Robson Ci\",\n\"Oracle\",\" AZ\",\"http://www.robson.com/page.cfm?name=SBRanch_Golf\"\n\"Golf Club at Johnson Ranch\",\"30761 Golf Club Dr\",\n\"San Tan Valley\",\" AZ\",\"http://www.johnsonranchgc.com/homepage/start_page.php\"\n. Previous comment contains the test data from the Google Code issue attachment.\n@thadguidry - this appears to have been fixed as part of the importer rework that I did for 2.6.  We put a much heavier emphasis on fidelity, so most automatic conversions are off by default and they're all optional.\n. From dfhu...@gmail.com on September 27, 2011 09:12:15:\nMarked as Post 2.5 since this requires resizable dialog boxes, which needs a lot more work overall.\n. From dfhu...@gmail.com on September 27, 2011 09:10:03:\nVersion 2.5 will have the front page completely revamped.\n. From PaulMake...@gmail.com on July 23, 2011 10:19:39:\nThis is actually expected behavior. The 'e means 'exponent ' in scientific notation and indicates \"times ten to the power of\" hence your results.\nWhen you import, turn off the default checkbox for \"auto-detect types\" and Refine won't attempt this conversion and your data will come on as is.\n. From tfmorris on July 23, 2011 13:23:53:\nPaul's solution is correct.  Additionally, please upgrade to the current release version 2.1.\n. From l...@talis.com on July 25, 2011 08:45:56:\nHi,\nYes I suspected there was some automatic inference happening. Presumably if I turn off auto-detect types then this will stop Refine detecting types on another of my other columns (my actual worksheet is much larger with dates and numbers). That means I'll need to do some manual coercion to fix.\nI still think its an issue that after having imported those values I can't seem to fix them using the bulk edit options (\"Apply to all Identical Cells\"). Surely that's an actual bug?\nIf that worked then I could simply import by data, auto-detecting types for the bulk of the columns and then fix up that single column. Probably less work than manually coercing types.\n. From PaulMake...@gmail.com on July 25, 2011 09:41:12:\n5E2 maps to 500, and if you have '500' in your dataset in that column you are hoping to be able to tell which is which which isn't possible.\nIf your dataset won't contain powers-of-ten numbers then you could conceivably map them back but you could run into issues with 50E1 v 5E2 etc\nIf you think you've found an actual bug I'd suggest discussing & characterizing it in detail first on the user mailing list, google-refine\n. From dfhu...@google.com on September 29, 2011 03:20:57:\nThis issue was closed by revision r2259.\n. From iainsproat on July 26, 2011 13:22:30:\nThis sounds a bit like a previous issue, #  262 http://code.google.com/p/google-refine/issues/detail?id=262 .\nCould you take a look at that issue and let us know if that solves the problem?\n. From tiagompc...@gmail.com on July 27, 2011 20:00:12:\nI have the same problem in the same platform (linux amd64, chromium and openjdk 1.06, ubuntu 11.04).\nNo proxy, refine says port is in use but that is false (netstat | grep 3333).\nTested with the daily svn version and with the last download.\ntiago@gbox:~/\u00c1rea de Trabalho/desktop/banco/google-refine-read-only$ ./refine \n[: 816: /tmp/refine.aVbZfDx: unexpected operator\nSomething is already running on http://127.0.0.1:44444/ but doesn't seem to be Google Refine\n. From tiagompc...@gmail.com on July 27, 2011 22:16:12:\nAlso google-refine-2.0-r1836.tar.gz works well in the exactly same enviroment.\nSo, this is really a bug.\nThanks,\nTiago\n. From emilio.i2e@gmail.com on August 23, 2011 11:21:16:\ngoogle-refine-2.1-r2136\nSame problem in same enviroment\nany solution?\n. From pa...@paulm.com on August 23, 2011 12:11:15:\nI can't reproduce this with a clean download of 2.1\n\"Something is already running on http://127.0.0.1:44444/ but doesn't seem to be Google Refine\"\n\"(netstat | grep 3333)\"\nport 44444 isn't the default 3333  -- can you try this looking for the port you're apparently trying to run Refine on? netstat | grep 44444\nIf in doubt about host & port you can run this to see the URL it's attempting to serve on,\necho \"http://${REFINE_HOST}:${REFINE_PORT}/\"\n. From dfhu...@gmail.com on September 27, 2011 09:09:09:\nNo info for more than a month. Closed as CantRepro.\n. From dfhu...@google.com on September 20, 2011 01:36:54:\nThis issue was closed by revision r2251.\n. From tfmorris on October 08, 2011 18:03:42:\nOne possible explanation for this is if the name is sourced from the (stale) reconciliation index.\n. This is a problem with the old recon service which is gone.  Closing.\n. From tfmorris on August 04, 2011 07:26:49:\nThanks for the report.  Although you reference v2.1, it sounds like you're actually testing with the trunk which isn't stable.  Does your test case work with the v2.1 release?\nWe still need to fix the problem, but it would help to know whether it's a new problem or one which has been around for a while.\n. From rodrod.s...@gmail.com on August 04, 2011 20:22:31:\nI can confirm that it does work in the v2.1 RELEASE (without the new importing UI). So, it's a new problem.\n-Rodrigo Salazar\n. From dfhu...@gmail.com on August 06, 2011 17:30:08:\nI'll take a look at this soon.\n. From dfhu...@gmail.com on August 06, 2011 19:38:04:\nFixed by r2189.\n. From dfhu...@gmail.com on August 06, 2011 17:24:27:\nCould you attach the data file that you started out with as well as the operation history you've done to it? We need both to debug this issue. Thanks.\n. From dfhu...@gmail.com on September 29, 2011 03:12:17:\nClosing because there is no more information from reporter.\n. From ldegroot...@gmail.com on November 17, 2011 21:59:58:\nSame behavior in Mac, Chrome\n. From tfmorris on November 17, 2011 23:53:28:\nFixed in r2379\n. From ldegroot...@gmail.com on November 22, 2011 21:10:11:\nWorks perfectly. Thanks!\n. From pa...@paulm.com on August 11, 2011 12:27:17:\nThis is the exact expression that worked for me,\n\"http://access.alchemyapi.com/calls/text/TextGetRankedConcepts?apikey=1062&text=\" + escape(value, 'url')\nObviously instead of 1062 is the API KEY.\nIn my experience, errors in making the expression are quite hard to spot!\nOne thing you can do to help with that is make a column that contains the URL so you can inspect/paste it into a browser. Then, \"Add column by fetching URLs\" from that URL column using just 'value' (no quotes)\n. From ocean.ch...@gmail.com on August 12, 2011 03:56:46:\nYou are a saint! Thank you! \n. From ocean.ch...@gmail.com on August 12, 2011 04:00:51:\nOh, and this issue is resolved - the Alchemy API works perfect. \n. From sean.gil...@gmail.com on August 11, 2011 20:34:25:\nI neglected to report that I'm trying to get data from a second cross against a second project. I'm attaching the project history where the success of one cross can be seen early on. The last operation is the failing one.\nDoes Refine only support one cross per project, or only one other project in crosses?\n. From thadguidry on August 26, 2011 03:46:05:\ncross() does indeed work in 2.1 (verified).\nDavid will have to answer on whether it can support more than 1 cross expression.\nI personally have run into the problem for some reason in that the expression editor window parsing kinda bugs out with cross() expressions.  I cannot simply copy and paste into the editor, instead, I have to manually type them out, and IT WORKS !  Weird.  Give it a shot and let us know.\n. From sean.gil...@gmail.com on August 26, 2011 04:11:25:\nI stumbled onto a anecdote in the mailing list about a limit of ~6 crosses per project, but am unable to find that post right now.\n. From thadguidry on August 26, 2011 04:20:01:\nYes, I was the one who did those 6 crosses one time, but I am not sure if that can still work now in release 2.1 or /trunk. At the time, David was even surprised I had strung that many all together!\n. From tfmorris on August 26, 2011 16:25:44:\nYou were definitely right to create a dedicated bug report for this.  I guess that other random commenter wasn't really interested in seeing the problem fixed.\nDo you see the time-dependent behavior that was reported in the other comment?\nOne thing which could be a potential issue is that projects are cached in memory and only written out periodically (every 5 minutes, I think).  If you've got data in memory which isn't represented on disk and the cross() is working off disk, it could get confused -- but that's entirely speculation on my part without actually looking at the code.\n. From pol...@multipagos.com.mx on October 07, 2011 20:59:50:\nHello all,\nI can also confirm that Version 2.1 [r2136] stops doing cross's after a few of them (3 in my case). I've tried rebooting; killing the process and waiting; exporting to csv and creating a new project and it still refuses to work.\nAny new suggestions or fixes?\n. From tfmorris on October 07, 2011 23:18:42:\nre: Comment 6\nDo you see any errors on the console/terminal that the Refine server was started from?  Have you tried exporting and recreating both projects?  Can you make the projects available for debugging?\n. From j...@tekii.com.ar on November 04, 2011 20:23:06:\nDid you change the file your crossing with? Ex:\nFila A i perform a cross op with File B applying this json AND after that, you changed your File B COLUMN B and make the join again and does not work. \n[  {\n    \"op\": \"core/column-addition\",\n    \"description\": \"Create column cross_result at index 1 based on column operacion using expression grel:cell.cross(\\\"FILE B\\\", \\\"COLUMN B\\\").cells[\\\"COLUMN A\\\"].value[0]\",\n    \"engineConfig\": {\n      \"facets\": [],\n      \"mode\": \"row-based\"\n    },\n    \"newColumnName\": \"cross_result\",\n    \"columnInsertIndex\": 1,\n    \"baseColumnName\": \"operacion\",\n    \"expression\": \"grel:cell.cross(\\\"FILE B\\\", \\\"COLUMN B\\\").cells[\\\"COLUMN A\\\"].value[0]\",\n    \"onError\": \"set-to-blank\"\n  }\n]\n. From craig55...@gmail.com on June 21, 2012 12:13:31:\nI've also noticed inconsistencies in the cross function. I've found that it works when matching against some columns, but returns an empty array for others. The failing columns were numeric data, the successful column (in this instance) was text. I converted the numeric columns to text (value.toString()), but this did not get around the problem.\n. From domoritz on August 11, 2012 14:21:26:\nI can confirm this problem for Version 2.5 [r2407]. I have two separate tables and wanted to join them. In the preview I can see the right results but as soon as I run the command (which takes a relatively long time for about 16 rows). \nI tried to join two lists of Harry Potter professors and their subjects from http://en.wikipedia.org/wiki/List_of_fictional_professors but ended up joining them in Excel instead. Let me know if you need more details. \n. From tfmorris on August 25, 2012 20:11:47:\nOK, I'm pretty sure I know what the problem is here.  It happened to me, so I actually had a test case that I could debug with. :-)\nJoins are cached to save on computational expense.  Unfortunately, although we have cache flush methods implemented, they're never called.  This basically means that if you do a cross once, the join for that pair of columns is frozen for all time (well, until you restart Refine anyway).  Note that the obvious case is that you notice it not working, but it can also fail in a much more subtle way if you update the values in the source or target column and attempt to redo the cross(), you'll get results based on the original values.\n@craig552uk - I'm pretty sure that cross needs the values to match exactly, including type. ie string('123')!=number(123) This is slightly counter intuitive since the vast majority of things in Refine work on a string equivalence basis.   Unfortunately, due to the caching bug, once it failed once, it was going to work until you restarted Refine.\nThere have been conflicting reports about what's needed to clear the condition, but I'm pretty sure that a) no amount of time will clear the condition and b) restarting the Refine server should clear the condition immediately.\n. From tfmorris on August 30, 2012 16:36:49:\nFixed in r2539.  Hopefully I found all the places that the cache needs to be flushed.\n. The fix will be included in OpenRefine 2.6.  Being based entirely on\ndonated time and effort, there's no real schedule.  We had hoped to have it\nout by now.\n. From tfmorris on August 26, 2011 16:53:32:\nThanks for the suggestion\n. From dfhu...@google.com on September 29, 2011 05:58:20:\nThis issue was closed by revision r2260.\n. From ocean.ch...@gmail.com on August 14, 2011 00:53:51:\nI've learned that adding .join(\", \") to the end of the expression remedies this issue - in case anyone else has run into this problem. But it would be helpful if Google Refine did not allow a preview of results that will not be reflected when an expression is applied. Thank you to everyone who helped me solve this problem. \n. From dfhu...@gmail.com on September 20, 2011 09:15:27:\nThe challenge here is that the result of the expression is not a single serializable value (that can be stored into a single cell). But we still want to preview it so that the user knows how to extend the expression further to convert that value into something storable.\nMaybe the preview UI should indicate which values are storable and which are not. I'm not sure if that's understandable.\n. From dfhu...@gmail.com on September 20, 2011 09:28:05:\nPost 2.5.\n. From thadguidry on September 20, 2011 14:00:36:\nI still say we use color coding here (like the example I emailed you David), & also label the column header in the expression editor to tell the user what value type will be output into the cells... if nothing, then say \"NOTHING WILL OUTPUT\" in big bold red flashing letters, lolol.  Or perhaps just tool tip a label \"OUTPUT PROBLEM, HOVER HERE\" to breakdown the scenario with different value types and give more useful explainations.  OR make that Red label \"OUTPUT PROBLEM, CLICK HERE\", be a clickable link that takes them to helpful pages on the wiki ??\n. From thadguidry on September 18, 2012 19:11:47:\nI've updated our function docs to show how to combine forEach() with parseJson() to get all named object instances in a JSON array.\nhttp://code.google.com/p/google-refine/wiki/GRELOtherFunctions#parseJson(string_s)\nThere might be a case to make it easier to do with better sugar syntax however, such as\nvalue.parseJson().keywords.text.join(\":::\")\n. From stefa...@google.com on August 16, 2011 16:49:01:\nFixed the java version issue (I used a more compact regexp) in trunk.\nJAVA is already defined.\n. From dfhu...@gmail.com on September 20, 2011 09:24:10:\nIs it possible that your Debian is disallowing anything to bind to port 3333?  Try another port, e.g.,\n./refine -p 80\n. From ste.steko@gmail.com on November 15, 2011 15:32:15:\nThe problem is with shell locale settings IMHO. At line 160 the script greps for the English \"Connection refused\" message, but this is what I would get from running the same command in my shell (it_IT):\nsteko@gibreel:~/Scaricati/google-refine-2.5$ wget -O - http://127.0.0.1:4444/\n--2011-11-15 17:26:11--  http://127.0.0.1:4444/\nConnessione a 127.0.0.1:4444...fallito: Connessione rifiutata.\nRunning\n$ LC_ALL=C ./refine\nworks just fine. Probably the same applies for all similar issues (e.g. 425, 262)\n. From stefa...@google.com on November 16, 2011 00:33:34:\nI just committed a fix for the locale issues. Not sure it fixes the problem reported in this issue, but it should fix the problem reported by ste.steko above. If not, yell.\n. From tfmorris on January 27, 2012 22:14:11:\nIf it's not a locale issue, it's likely to be a proxy issue.  Try adding \nno_proxy=\"localhost,127.0.0.1\"\nto your refine.ini file.\n. From dfhu...@gmail.com on September 01, 2011 18:29:20:\nr2228 removes the .5 second status pings.\n. From tfmorris on August 25, 2011 16:45:25:\nGood suggestion.  Progress is not always deterministic, but a better progress indicator would also help separate \"really slow\" from \"crashed but the spinner is still up.\"\n. From tfmorris on August 25, 2011 16:50:45:\nissue #294 concerns the CSV exporter.  The RDF exporter is distributed by DERI Galway and you can report issues at https://github.com/fadmaa/grefine-rdf-extension/issues\nIf you can reproduce the problem with one of the bundled exporters, please feel free to reopen this bug report, but for now I'm closing it since it's not our code.\n. From tfmorris on August 28, 2011 22:02:17:\nI believe this is being triggered by the cache purge (which is done as part of the autosave processing).  Autosave is 5 min. by default and main memory is purged of any projects which haven't been modified in the last hour.  The modification time isn't updated until the operation completes and is posted to the history, so any long running operation has the potential to have its Project object (which it's in the process of using) deleted from memory in the middle of the operation.\nI've applied a fix in r2222 which I think should cure the problem (not only for this, but all long running operations).  Please give it a try and let me know how you make out.\n. From thadguidry on August 29, 2011 13:18:45:\nSo, does this turn off ANY writes to disk, until a Fetch URLs operation completes ?  If not, what types of writes to disk would still happen, even after this patch (logging, anything else) ?  Asking so I can test further, later on.\n. From thadguidry on August 29, 2011 17:17:16:\nPulled new code via SVN, cleaned, build, ran Fetch URLs operation on 2400 MySpace urls, which ran for about 1.5 hours (unattended) but I did watch it up to 90%, walked away, when I returned to look at the Ubuntu terminal (20 mins later), I saw the Fetch1 column created, but no data for any rows?  I quickly captured the Log (attached here).  I then did a Shift F5 refresh, and no data in column showed.  I stopped Refine and restarted with same parameters and opened Firefox again, still no data showed in Fetch1 and a isBlanks facet showed True on all 2400 rows.  (sigh) Back to the drawing board? \n. From tfmorris on August 30, 2011 20:02:21:\nI could have sworn a replied to comment 3, but it looks like it never posted.  Writes aren't affected at all by the change, the only thing that has changed is that projects with long running processes aren't purged from memory.\nHaving said that, I don't think the project is marked as modified until the operation completes and it won't get autosaved unless its modified.\nre: comment 4 were there any errors in the UI?  You mentioned something about using facets before.  Was this test done with a facet or facets active?  If so, can you try  a smaller test with and without facets to see if that changes the behavior?  I'll look to see if there is a path where the operation can terminate without making the error visible.\n. From thadguidry on August 30, 2011 20:25:26:\nNo facets used on the Comment 4 testing.  I am strictly testing the Fetch operation to reach a 100% completion and also see the \"CellAtRow fetch(CellAtRow urlData)\" at line 253 of ColumnAdditionByFetchingURLsOperation.java actually create \"new Cell\" in the column.  It definitely creates the column on 100% completion... however it doesn't appear to be inserting the stream into each new Cell...for some weird reason.  That is the general failure that I am seeing.\nI would categorize this as 2 issues, I think?\n1. Fetch doesn't always reach 100% completion, No error shown, No new column of data.\n2. Fetch DOES reach 100% completion, DOES Create a new column, however no stream data appears in the column's new cells (such as raw HTML).\n. From thadguidry on September 07, 2011 18:35:32:\nI have tried 2 more rounds of testing with Fetch URLs.\n1. Fetch 2000 rows of unique URIs from \"test A\" domain. (about 2 hours)\n2. Fetch 4500 rows of unique URIs from \"test A\" domain. (about 3 hours)\nBoth tests completed and full HTML content was added into a new column.\nBoth tests used 2500ms delay. (quite harsh 2.5 sec expectation on returning a page result)\nBoth tests used multiple facets to filter down to those particular rows.\nMy conclusion is that this seems now fixed, and that my previous concern seems isolated to some sort of MySpace problem, or perhaps just too aggressive with the delay during testing against the MySpace domain.  The different \"test A\" domain is also well known, and exhibited no such problems at all with using current Trunk r2234 (after Tom and David's patches).\nWe may want to leave open until after the 2.5 public beta testing. And if no further issues, then close.\n. From tfmorris on September 22, 2011 10:34:52:\nBased on Thad's additional testing and my analysis of the problem, I'm pretty convinced this is fixed.\n. From tfmorris on September 13, 2011 20:54:44:\nPlease include the version of Refine that you are using (2.1, 2.0, trunk) with all bug reports.\nI spotted some anomalies in this area recently, so I'll have a look.\n. From tfmorris on September 13, 2011 21:02:05:\nI've committed some improvements in r2237, but will try to take a closer look later.\n. From tfmorris on September 22, 2011 11:16:50:\nI've confirmed that this is definitely broken in 2.1 and the patch I made fixes the problem described.  Since it works with the default setup, I'm not going to worry about testing with alternate onError parameters.\n. From dfhu...@google.com on September 01, 2011 22:11:55:\nThis issue was closed by revision r2230.\n. From thadguidry on August 27, 2011 21:56:26:\nHere is a Screenshot of my suggestion.\n. From dfhu...@gmail.com on September 27, 2011 09:05:26:\nThanks, Thad. Originally, because the preview updates as you types and changes the expression, the intention of showing the expression in the preview column header is for you to know which expression (editing version) it corresponds to. I do understand that this design has its problems and will think about it.\n. From thadguidry on September 27, 2011 13:21:19:\nBut do you think a user will simply know their expression anyway, since it will be in the larger text area above ?  My thought was YES, and so my feeling of feeling redundant, and the header better used as a hinting mechanism.  Hmm, maybe a 2 header system would work, the 2nd header being for errors and hints, or perhaps errors and hints should always be handled generically with jquery popup overlays ?\n. From dfhu...@gmail.com on September 28, 2011 03:45:34:\nThe scenario is this: the user is typing in some rather long expression, and the preview is lagging behind (especially if Refine is not run on the local machine). When the user stops typing and looks at the preview, we want them to know whether or not the preview is displaying the results of the full expression, rather than some intermediate, half-baked version that likely contains syntax errors.\nThen there's also the issue of what text to display for various types of intermediate result objects. Full Java class names seem a bit too low-leveled.\n. From dfhu...@gmail.com on September 29, 2011 03:07:52:\nRenamed issue. Marked as Enhancement, Post 2.5.\n. From dfhu...@google.com on September 01, 2011 17:32:16:\nThis issue was closed by revision r2227.\n. From dfhu...@gmail.com on September 01, 2011 17:32:53:\nFixed by r2227.\n. From dfhu...@google.com on September 02, 2011 03:31:10:\nThis issue was closed by revision r2232.\n. From tfmorris on September 13, 2011 20:50:25:\nIs this problem reproduceable with your small file?  If so, can you attach it here for debug purposes?\nNot that memory is shared across all projects and recently opened projects are cached in memory, so it is theoretically possible that your new small project pushed you over the edge.\n. From dfhu...@gmail.com on September 27, 2011 08:30:19:\nAndrew, could you please provide more info? (We'll wait for a few more days and close this as \"can't repro\".) Thanks.\n. From tfmorris on October 21, 2011 16:05:45:\nClosing due to lack of response from reporter.  Feel free to reopen if you can provide additional information.\n. From dfhu...@google.com on September 19, 2011 09:48:40:\nThis issue was closed by revision r2247.\n. From tfmorris on September 19, 2011 15:21:34:\nI had started working on this (oops, should have grabbed it as owner), but had assumed that Thad wanted to be able to specify the delimiters as an argument in GREL.\nThad - can you clarify what you were asking for?\n. From thadguidry on September 19, 2011 15:29:52:\nI had not initially thought about specifying the delimiters, but on second thought, this might be only special casing my own needs, hmmm, perhaps beyond the English language there might be a need to specify the delimiters.  Will that have to be in UTF8 format, I wonder ?  I only know a few punctuation marks in Chinese (Mandarin), so not sure about other languages.  I would suggest not special casing this, and instead make the user specify the delimiters (in UTF8) in the argument.\n. From tfmorris on September 13, 2011 20:47:12:\nWhat version of Refine are you using? Are there any errors logged on the console/terminal that you started Refine from?\nSince this is likely a data-dependent issue, a cut down test case would be a great help.\nAnother thing which might provide a hint is to try turning off the \"auto detect data types\" option on input and see whether the behavior changes.\n. From dfhu...@gmail.com on September 27, 2011 08:29:14:\nRyan, could you please provide more info? (We'll wait for a few more days and close this bug as \"can't repro\".) Thanks.\n. From tfmorris on December 09, 2011 22:41:43:\nNo response from reporter.\n. From dfhu...@google.com on September 19, 2011 08:49:40:\nThis issue was closed by revision r2245.\n. From dfhu...@gmail.com on September 27, 2011 08:27:12:\nHi Tom, please post the data and I'll take a look! Thanks.\n. From tfmorris on September 29, 2011 21:29:27:\nIt was single quote (') characters which were causing the problem.  I've fixed that (r2265) and also added an explicit error dialog if the upload fails (r2264).\n. From dfhu...@gmail.com on September 26, 2011 01:06:12:\nThis is serious. I'll take a look.\n. From dfhu...@gmail.com on September 26, 2011 01:14:49:\nTom, could you attach your data and operations for which this issue happens? I have not been able to reproduce it on first try. Thanks.\n. From tfmorris on September 26, 2011 18:08:08:\nUnfortunately I forgot to export the project in the state it was in.  I've attempted to recreate a subset of the operations and it looks like things actually get messed during the add column.  If you look at the attached project \"155 Views\" from Column 1 3 was duplicated to the Conference column during the Add Column operation.\nI had two facets on the YouTube Title column - startsWith('Strata') and contains('interviewed') - and had True selected for the first of those when I did the column add (ie not all rows were selected).\nWhen I was working before, I was playing with different combinations of the facet selections while adding several different columns, so it's possible that the data was getting messier and messier during the operations and I later misinterpreted that as Undo not working.  I'll have to play with it some more to see if I can reproduce exactly what happened (although Add Column shouldn't be adding random data, of course).\n. From dfhu...@gmail.com on September 27, 2011 00:34:45:\nThanks, Tom. It's not obvious to me why the undo wouldn't work in this case, so I'll wait for you to consistently repro the problem before digging into the problem.\n. From tfmorris on December 12, 2011 19:56:00:\nBump all unfinished 2.5 Milestone tags to next release\n. I'm closing this as unreproducable.\n. From dfhu...@gmail.com on September 29, 2011 02:29:09:\nThis seems dependent on the actual text you paste (e.g., how long it is). I pasted in XML and JSON text and it got detected properly. For example, the output of this Freebase query http://tinyurl.com/6x3hunt gets detected as JSON. Could you provide the text you used?\n. From thadguidry on September 29, 2011 13:29:10:\nThe universities top 10 json test file from Scraperwiki.  I attached again.\nI just noticed that TextFormatGuesser.java does not have an elseif for open/closed Square Brackets, just Angle Brackets.  Don't we also need that for json files that are one big array [] rather than an object {} ?\n. From dfhu...@google.com on September 29, 2011 14:02:16:\nThis issue was closed by revision r2263.\n. From tfmorris on September 29, 2011 22:23:18:\nIt doesn't look like any of the importer tests have been updated for the new importer architecture, so that will need to be done first before the patch is usable.\n. From tfmorris on October 07, 2011 16:55:22:\nCommitted in r2287\n. From thadguidry on September 29, 2011 19:36:01:\nFile Attached now (stupid google code)\n. From dfhu...@google.com on September 30, 2011 03:18:24:\nThis issue was closed by revision r2267.\n. From tfmorris on September 30, 2011 18:57:13:\nJust discovered that this function takes an optional \"preserve all tokens\" parameter which defaults to false.  This seems backwards to me, but it's too late to change it.\n. From dfhu...@gmail.com on November 04, 2011 21:52:15:\nThis issue was closed by revision r2357.\n. From thadguidry on October 12, 2011 13:27:59:\nThere is a feature in Undo/Redo that you can use to Export operations to a JSON text file and then paste them in for another dataset and Apply them.  This is shown in the tutorial videos.  Does that suit your needs ? or is your request something more ?\n. From tfmorris on October 12, 2011 20:48:48:\nIt would be useful to understand the \"messing\" that you're trying to avoid and/or the UI flow for what you're proposing.  Pretend we have no clue what context you're operating in or what your assumptions are and make it nice and simple.\n. From mccus...@gmail.com on October 13, 2011 19:49:43:\nExporting and re-importing operations sounds promising, but a \"re-import\" command might be clearer to users.\n. From frodesev...@gmail.com on September 26, 2012 06:53:53:\nI have the same issue. Let me summarize my workflow:\nManipulate some data:\n\nLoad some data into project \"foo\"\nDo some work\nLoad some data into project \"bar\"\nDo some work\nLoad some data into project \"fobar\"\nDo some work\n\nNow I want to use the 'cross()' function to do calculations on \"foobar\" relative to \"foo\" and \"bar\", as explained in\nhttp://code.google.com/p/google-refine/wiki/GRELOtherFunctions\nThis is a very powerful tool for data manipulation, and it works great.\nProblem situation:\nI now want to redo the process for a fresh dataset with the exact same data layout. If I export the JSON and apply to new projects, I am left with new project names \"foo1\", \"bar1\" and \"fobar1\". As the 'cross()' function, and presumably other functions too, depend on the name of the referenced projects, and hence it does not work well with the new names. It does even not play well with looking up cell contents from within the same project, as there is no parameter \"project.name\" available either.\nCurrently available solution\nThe solution available to me at present is this:\n- Load new datasets into new projects with new names\n- Extract JSON history from old projects\n- Replace all references to \"foo\", \"bar\" and \"foobar\" with \"foo1\", \"bar1\" and \"foobar1\" in the JSON histories\n  - This is error prone\n- Replay the JSON histories on the new projects\nWhile I can cope with this, being a wizard with regexp and understanding programming syntaxes quite well, it is not very handy, and is quite time consuming.\nProposed solution\nA simple \"Reload data and replay all operations\" function would solve this in a snap.\n;)Frode\n. From tfmorris on November 16, 2011 07:41:12:\nr2375 adds basic support for public Google spreadsheets (including worksheet selection) as well as the ability to sign out (ie deauthorize Refine) when you're logged in.\nThe following URL forms work:\nhttps://docs.google.com/spreadsheet/ccc?key=0AlD_6iEb8Ed9dGs3clVJYi0yYVBka181Z0ZKRW9kQ0E#gid=0\nhttp://spreadsheets.google.com/ccc?key=pc6ppXdYxYkRPmnSBmFDRUg\nhttps://spreadsheets.google.com/spreadsheet/ccc?key=0AsaDhyyXNaFSdDJ2VUxtVGVWN1Yza1loU1RPVVU3OFE&hl=en_US&authkey=CLXm3-ML\nhttp://spreadsheets0.google.com/ccc?key=pmEMxYRcQzzATwbOb71BmGA\nhttps://spreadsheets.google.com/pub?key=0Ah0xU81penP1dDNwSFROSU5KVlFRbmo5cERsTElKTGc&hl=en&output=html\nThese two don't yet:\nhttps://docs.google.com/spreadsheet/ccc?key=0AuCc2KRWCBN7dDlSSVBpcU5IZnVTWW5TVzhqY3V0WGc#gid=0\nhttp://docs.google.com/spreadsheet/ccc?key=0AuCc2KRWCBN7dDlSSVBpcU5IZnVTWW5TVzhqY3V0WGc\nThere may also be cases where some things with public spreadsheets only work if you're signed out for reasons I haven't fully investigated.\nI'm not crazy about the UI design, so I'm open to suggestions (or patches/fixes).\nI'll leave this open, but the current implementation is probably good enough for people to beat on (ie beta test).\n. From thadguidry on November 17, 2011 04:00:43:\nTom,\nI'm just going to string the issues I see so far.\n1. Part of the Error gets cutoff and probably would be nice to have the full error given to the user in the popup... notice my command window has the full error.  Capture.PNG\n2. There seems to be an EDIT checkbox available, but I didn't check it.  But what I did do was use File - Publish to the Web.  There is also the SHARE button option with settings such as shown in Maine-Lighthouses-XLSX.png\nI think that for # 2 the issue is basically that you have to really use File -> Publish to the Web in GDocs, in order for Refine to have access with the current implementation you have.  The SHARE button settings did not matter which radio button it was set to during my testing.\nFor # 1, I think that changing the error message to give the user a hint that they can use File -> Publish to the Web in GDocs to really make the file \"Public\" for Refine to access it properly would might work ?\n. From thadguidry on November 17, 2011 04:21:54:\nTom, \nIt looks like for Google FusionTables you have to make a view of an uploaded spreadsheet before Refine can access.  See attached screenshot.\nThe view that I created here: https://www.google.com/fusiontables/DataSource?docid=1HcVjgUbpAYRb3OIElW-vdnaq_udaAitKOvEeJpA&hl=en_US  does work with the Refine public url input now.\nBut my original table upload here: https://www.google.com/fusiontables/DataSource?docid=1pykVFIFBBGRTNOSEs12uThsvztzQmTN4ceiVoTU&hl=en_US even though it is made public, cannot be accessed for some reason.\n. From thadguidry on November 17, 2011 04:29:21:\nScreenshot of from About menu on Google FusionTables View made from Table.\n. From thadguidry on November 18, 2011 03:03:53:\nWith r2378 , I cannot import any Google docs spreadsheet if it is NOT PUBLISHED.  See comments of details covering 3 sharing scenarios with results in attached file.  If I instead choose, File -> Publish to the Web, then it every spreadsheet I have works and also works in 3 formats, csv, txt, xls.\n. From tfmorris on December 12, 2011 19:56:00:\nBump all unfinished 2.5 Milestone tags to next release\n. From tfmorris on December 20, 2011 18:06:25:\nIt looks like the requirement that a spreadsheet be published is a restriction in the current API (according to the Google Spreadsheet team), so there's nothing we can do about it for the time being.\nhttp://groups.google.com/group/google-spreadsheets-api/browse_thread/thread/c6beca8610eb653c?hl=en&tvc=2\nAs a workaround, users can access the public spreadsheet URL in a browser and download an Excel or CSV version which they can then import.  That's the best we can do for right now.\n. From tfmorris on October 12, 2011 23:47:41:\nFixed in r2330.  Note that this is an incompatible behavior change for the Excel importer.\nThere is regression test for this included in the recently added Excel import test stub.\n. From tfmorris on December 12, 2011 19:56:00:\nBump all unfinished 2.5 Milestone tags to next release\n. From dfhu...@google.com on November 04, 2011 22:33:49:\nThis issue was closed by revision r2358.\n. From tfmorris on October 21, 2011 17:48:48:\nIn r2341 I've added a first cut at this which will show the used JVM heap vs total available as part of the create project progress indicator.\n. From tfmorris on September 08, 2012 06:37:45:\nWith the improvements from issue #607, I'm going to consider the initial pass complete.\nWe could probably do more in terms of flushing cached projects when we're low on memory, but that can be a separate task.\n. From tfmorris on October 26, 2011 07:54:47:\nThanks for the suggestions.  These sound like two independent requests, unless I'm missing something.  Could you create a separate issue for the 2nd one, please?\n. From tfmorris on October 26, 2011 07:57:52:\nI think this has been reported before, but I don't know what issue number off the top of my head.\nAs a workaround, I'd suggest using the reconciliation facets (matched vs none, score) to narrow the range of what you're working on.  I typically choose None for the match and a high score range for the reconciliation candidate and work my way down through the scores as I reconcile things.\n. This is a duplicate of issue #33.\n. From tfmorris on December 12, 2011 19:56:00:\nBump all unfinished 2.5 Milestone tags to next release\n. From tfmorris on September 18, 2012 18:06:55:\nNot currently scheduled\n. The new recon service always returns MIDs.\n. Actually the new recon service is either not returning MIDs consistently or it;s returning both and the client is choosing the wrong one.  Needs to be investigated...\n. From tfmorris on October 29, 2011 22:07:00:\nFixed in r2350.\n. From tfmorris on November 05, 2011 16:40:06:\nFixed in r2359.\n. From thadguidry on November 05, 2011 17:23:17:\nWorking correctly now, even with nested records, it handles them.  Thanks Tom!\n. From thadguidry on November 04, 2011 15:49:21:\nUpdate: I imported the attached test file as-is.  Then used Edit Column -> Split into several columns.  Used Split by Separator with Regular Expression checked and input the \"Unicode code point\" http://www.regular-expressions.info/unicode.html of \\u0001 and clicked OK.  That worked.\nHowever, I still would like us to support any valid \"Unicode code points\" as separator characters during the importer preview stage.\n. From thadguidry on November 04, 2011 15:53:42:\nBounty $100 from me, via Paypal to anyone who can fix this and add enhancement of supporting any valid \"Unicode code points\" as separator characters during the initial importer preview stage.\n. From tfmorris on November 04, 2011 16:00:35:\nDoes cutting the character from someplace like a Character Map utility and pasting it into the field work?\nIt sounds like what you're really after is the ability to use some type of quoting/escaping notation for your separator characters.\n. From thadguidry on November 04, 2011 16:12:54:\nNO, cutting and pasting the character within Ubuntu did not work for me, nor did it on Windows 7, I even tried Alt - numeric keypad 0 1 with no luck on the separator char input box on the importer preview for CSV/TSV/separator.  Yes, agreed, ideally we really need to support the backslash escaping during the initial importer preview.  This would ease data entry when using VNC connections to Refine instances, instead of having to send over special keyboard command/control chars if a user is on Windows while remoting through a VNC connection to Refine running on Ubuntu.\n. From thadguidry on November 04, 2011 16:56:46:\nFYI Tom, since you've started (thanks!), David says you'll need to do proper Javascript unescaping for this.\n. From tfmorris on November 04, 2011 19:08:58:\nFixed in r2355.  Escaping and unescaping is now done server-side with escaped strings going over the wire instead of raw characters.  Escaping syntax used is Java's, but it's easy to switch to Javascript if people prefer that.  For the vast majority of stuff, they're probably identical.\nNote that for your example, Refine correctly guesses the separator character, so you shouldn't actually even have to type it in.\n. From tfmorris on December 12, 2011 19:56:00:\nBump all unfinished 2.5 Milestone tags to next release\n. Yes, please put any proposed fix on its own feature branch and submit a PR for it.\nThere are other examples around that you should be able to copy.  One that comes to mind is the \"Add column by fetching URL\" command.  The LongRunningProcess class might be useful, but it's been a while since I looked at the sorting code.\n. From dfhu...@gmail.com on November 06, 2011 05:17:30:\nI added that option just because it was parallel to the column separator option. But I don't know if it's really needed in practice. Maybe someone who has dealt with more data can chime in.\nIf it turns out that we need it, then I can just implement something similar to LineNumberReader.\n. From tfmorris on November 06, 2011 14:24:10:\nIt looks like old-skool ASCII files (binary MARC21 is one) which use the ASCII control characters might need this, but I'm not sure how common it would be for folks to need to process this type of file.\nYou're right that overriding BufferedReader.readline() (which is what it's subclass LineNumberReader uses) would be more efficient than implementing a FilterReader.\n. From tfmorris on November 06, 2011 14:25:58:\nhttp://ronaldduncan.wordpress.com/category/software/file-formats/\nhttp://www.loc.gov/marc/specifications/specrecstruc.html\n. From thadguidry on November 06, 2011 15:53:05:\nAgreed with Tom, this comes up typically only in old data sets like MARC, circa early 1980's and earlier, probably about the time that Tape storage died (Linear and used Control Characters to give position info) and where Hard Disks (Non-Linear) became cheaper.  I have not had to deal with control char hysteria within ANY data set during any of my enterprise data migrations in the last 15 years.  Safe to remove I think.\n. From dfhu...@gmail.com on November 06, 2011 18:28:31:\nI'll remove those options then.\n. From dfhu...@google.com on November 06, 2011 20:13:12:\nThis issue was closed by revision r2364.\n. From tfmorris on December 20, 2011 15:44:31:\nissue #511 has been merged into this issue.\n. From tfmorris on September 07, 2012 21:24:09:\nIn r2451 TabularImportingParserBase now defaults guessCellValueTypes to False so that importers which don't specify it don't get it turned on automatically.  This was adversely affecting Excel, Open Office Calc, and Google Spreadsheets because they had no control to turn off the default since they have data types built in.\n. From tfmorris on September 18, 2012 17:52:36:\nI think most of the work is done for this and I'd like to clean up any loose ends and get it included in Refine 2.6.\n. From tfmorris on December 12, 2011 19:56:00:\nBump all unfinished 2.5 Milestone tags to next release\n. From tfmorris on December 12, 2011 19:56:00:\nBump all unfinished 2.5 Milestone tags to next release\n. This has been fixed some time in the past and the UI now puts up an error dialog telling the user that they have to select a record path.\n. I don't have a proxy setup to test against, so please test this when released.\n. From tfmorris on November 12, 2011 20:33:31:\nr2371 makes the sorting order case insensitive, but Javascript doesn't appear to have a built-in diacritic folding method, so that'll be a little more work.\nAfter I committed the \"fix\" I discovered that this may actually be a browser-specific bug/difference, but it doesn't appear that there's been much progress in fixing it, so we probably should assume that the current state is going to exist for a while.\nhttp://code.google.com/p/v8/issues/detail?id=459\nThere's a code snippet here that can be used to scrub diacritics: http://lehelk.com/2011/05/06/script-to-remove-diacritics/\n. From tfmorris on November 15, 2011 19:27:04:\nIt sounds like you've got a specific change to propose, so please provide a patch which implements the change and explain why the current accessor getCustomMetadata(String key) isn't adequate for your needs.\n. From gofranshukair@gmail.com on November 17, 2011 12:41:46:\nMy suggestion is not related to the getCustomMetadata(String key).\nI propose just to remove the  line that add project custom metadata outside the if statement because that restrict saving custom metadata to the save mode only \nI attach the patch with the suggested changes \n. From tfmorris on December 09, 2011 23:07:55:\nThanks for the patch.  The relevant piece has been committed in r2404.  If you submit patches in the future, please try to make sure they only include the relevant change (this one had changes to project settings).\n. From tfmorris on November 18, 2011 00:15:47:\nRefine interprets leading blank cells as a poor man's record format (so you could have a column with multiple values per \"record\").\nIf below where it says \"4 records\", you click on \"Show as: rows\" the display will switch to row mode and you'll see it say \"5 rows\" and update the display accordingly.\nIf I misinterpreted what you need, feel free to reopen the bug report.\n. From tfmorris on November 18, 2011 21:02:52:\nAre you in Row mode or Record mode?  It sounds like the latter.  Google Refine interprets spreadsheet rows with empty leading cells as part of a pseudo-record with the row(s) above so that you can do things like:\nName    Roles\nSmith   Partner\n        Judge\nIf you switch to Row mode, the facet will work the way you expect.\n. From tfmorris on November 18, 2011 19:47:47:\nIt also looks like the character set encoding guessing is no longer being done.\nDavid - was this removed intentionally or is it just an oversight?\n. From tfmorris on November 18, 2011 20:57:06:\nr2381 makes sure that the project encoding doesn't get set to \"\" (which is probably the state Thad's project is in), but more work is required\n. From dfhu...@google.com on November 21, 2011 00:10:35:\nHi Tom, it was partially an oversight: I didn't connect the encoding guessing code back in the new UI framework, but then it didn't seem to cause any problem that I knew of, so I didn't think it was worth it. Also, when several files or an archive file get uploaded, it's not clear how to do the guessing.\n. From tfmorris on December 12, 2011 19:56:00:\nBump all unfinished 2.5 Milestone tags to next release\n. Moving back to 2.6.  We shouldn't be doing a new release with critical bugs pending.  The reason this was given a critical priority is that, if the user doesn't notice that the encoding is messed up at import time, there's no way to recover later.\n. From tfmorris on November 18, 2011 22:10:42:\nFixed in r2383.  There's a separate problem with the single cell edit not supporting ISO 8601 (because Javascript's Date.parse()) doesn't, but I'll open a new bug report for that.\n. From dfhu...@google.com on November 27, 2011 20:23:07:\nThis issue was closed by revision r2388.\n. From dfhu...@gmail.com on November 21, 2011 19:31:17:\nFixed by r2386.\n. From thadguidry on November 23, 2011 14:35:23:\nDan, you forgot to mention which version of Refine are you using ?\n. From danpaulsmith on November 23, 2011 14:37:42:\nSorry, latest version - 2386.\n. From tfmorris on November 26, 2011 06:10:37:\nThe problem is due to the variance/std deviation being calculated wrong (although there may also be an issue with properly ignoring quoted separators).  I've got a fix for the fist part.\n. From danpaulsmith on March 06, 2012 15:25:46:\nHi Tom,\nAny news on this bug?\nDan\n. From tfmorris on March 08, 2012 00:19:09:\nMy emphasis was backwards.  The std dev fix isn't the important one.  The separator guesser needs to ignore separators which are in quoted fields if quoting is turned on (information it doesn't currently have).\nI've been focusing on revenue generating activities recently, but I'll see if I can find some time to finish this up.\n. From tfmorris on March 08, 2012 15:55:20:\nFixed in r2458\n. From danpaulsmith on November 23, 2011 14:38:19:\nVersion 2386\n. From tfmorris on November 26, 2011 06:15:56:\nTimes can't be \"left\" in their original format, because their native format in Excel is just a number.  It's the format code that turns it into a time (which Refine doesn't support natively) or a date.  Perhaps we could convert it back to a string (but not sure how hard that would be).\nWe'll check into the added column.\n. From danpaulsmith on November 28, 2011 11:38:10:\nDoes Refine pick up on some sort of metadata in the Excel file that tells it to format a time? A time value is apparently a decimal number (0.75 for 18:00:00) in Excel.\nIf a time can be distinguished from a date, could it be as easy as using a GREL transform on import? i.e.\nvalue.toDate().toString(\"HH-mm-ss\")\n. From tfmorris on December 09, 2011 23:54:59:\nI've fixed the off-by-one error causing the extra column in r2405\nExcel doesn't actually have a time datatype.  It just has datetimes with a formatting string which causes the date to not be shown.  Your example of 0.75 is actually December 30, 1899 at 18:00 which you can see by changing the format string to \"M/D/YYYY H:MM\".  You can postprocess anything that occurs on this date if that's the right thing to do for your app.\n. From danpaulsmith on November 23, 2011 14:38:24:\nVersion 2386\n. From tfmorris on November 26, 2011 06:18:44:\nI've seen this recently when adding columns where it seems like the size of the column name and the size of the column contents aren't maximized correctly to size both the header and data cells.\n. From dfhu...@google.com on November 27, 2011 20:53:35:\nThis issue was closed by revision r2389.\n. From dfhu...@gmail.com on November 27, 2011 20:54:20:\nI've fixed this at least for the starring/flagging case. Please test for other cases.\n. From dfhu...@gmail.com on November 29, 2011 20:00:40:\nissue #497 has been merged into this issue.\n. From danpaulsmith on November 23, 2011 14:38:37:\nVersion 2386\n. From dfhu...@gmail.com on November 27, 2011 22:54:59:\nDan, I believe this is working as designed. Cells are not stored in the order in which they are displayed. Rather, the column model specifies which column corresponds to which cell index. There is a variable per project to keep track of essentially the longest cell array among all rows, so that the next time you need to create a new column, we know which cell index it would correspond to. As you perform operations that create columns, the cell array for each row grows. What we need is an operation for compacting the project.\n. From danpaulsmith on November 28, 2011 11:03:47:\nOK, I've changed our code to handle null values/columns.\nI think a compacting operation would make sense - either exclusively for the columnize operation or as a command that could be manually initiated at any point.\nThanks\n. From dfhu...@gmail.com on November 29, 2011 19:58:53:\nPlease see issue #498.\n. From danpaulsmith on November 23, 2011 14:38:45:\nVersion 2386\n. From dfhu...@gmail.com on November 27, 2011 21:19:21:\nDan, could you use the preference store instead? That has a command for saving, too.\n. From danpaulsmith on November 28, 2011 11:10:17:\nAh yes, I've seen that.\nDoes that save preferences specific to a single project or just for Refine in general? We need the former.\nWe'll be running a single instance of Refine on a remote server for multiple users - so we need to store metadata per dataset - and during \"create/import project\" (on the /index page as opposed to saving the data on the /project page).\n. From dfhu...@gmail.com on November 29, 2011 20:09:56:\nEach project has a preference store and the whole workspace also has one.\n(I'm marking this as WontFix.)\n. From danpaulsmith on November 30, 2011 11:08:12:\nOh right, how can we get & save to a project's preference store?\n. From dfhu...@gmail.com on November 30, 2011 17:41:28:\nhttp://code.google.com/p/google-refine/source/browse/trunk/main/src/com/google/refine/commands/SetPreferenceCommand.java uses a particular project's preference store if there is a \"project\" parameter specifying its ID.\n. From danpaulsmith on December 01, 2011 10:52:41:\nOk, I can use that instead now.\nAs a side note, there's still the issue of accessing the project ID on the index page moments before being redirected to the project page.\nI'm overriding pollImportJob() for now:\n/\n- Override Refine's 'pollImportJob' function\n- \n- This is necessesary so that we can capture the project's ID (not the import job ID), \n- which must be used to access and write to the metadata.json file.\n- \n- We place our own \"LinkedGov.saveMetadata()\" function inside the block of \n- code that is able to see the project ID, just before sending the user to the \n- \"project\" page.\n  /\n  /\n- Store the original 'pollImportJob' before we overwrite Refine's version.\n  /\n  LinkedGov.pollImportJob = Refine.CreateProjectUI.prototype.pollImportJob;\nRefine.CreateProjectUI.prototype.pollImportJob = function(start, jobID, timerID, checkDone, callback, onError) {\n```\n/\n * Create our own \"callback\" function\n /\nlgCallback = function(jobID,job) {\n/*\n * This function is accessed twice by Refine, the first time to \n * send the user to the \"preview\" panel when they can modify the \n * import options, and the second time, with the projectID to the \n * \"project\" page.\n * \n * The second time round is when the projectID is present, so we \n * intercept it to make a call to save our custom metadata.\n */\nif(typeof job.config.projectID != 'undefined'){\n    LinkedGov.saveMetadata(jobID, job.config.projectID, function(jobID, projectID){\n        Refine.CreateProjectUI.cancelImportinJob(jobID);\n        document.location = \"project?project=\" + projectID;\n    });\n} else {\n    callback(jobID,job);\n}\n\n};\n/\n * Call the original 'pollImportJob' function that was stored at the end of our new \n * 'pollImportJob' function.\n /\nLinkedGov.pollImportJob(start, jobID, timerID, checkDone, lgCallback, onError);\n```\n}\n. From danpaulsmith on November 23, 2011 14:38:55:\nVersion 2386\n. From dfhu...@gmail.com on November 27, 2011 22:26:19:\nThat's a good idea, Dan. I have been thinking that we'll need several \"layers\" (like map layers) to store metadata. One \"layer\" already exists for reconciliation, but it should be generalized. So another layer could be for original values. But we need to think about what to do with it when yet another transformation is performed on cells that already contain errors.\n. From danpaulsmith on November 28, 2011 11:32:44:\n+1 for generalising layers.\nI might be missing something, but I'd expect a transform on an error to always be an error?\nWe only have a small use case for this issue, so we'll workaround it using \"keep-original\" and a makeshift error flag.\nThanks!\n. From dfhu...@gmail.com on November 29, 2011 20:02:30:\nA transform on an error is usually an error, unless the expression tests for error and returns a normal value. But the issue is whether the original value of the first error is kept, or the \"original value\" is now the first error itself.\n. From guang...@structuredcommons.com on November 29, 2011 06:08:06:\nIt seems something wrong with load_configs.\n. From tatadel...@gmail.com on December 06, 2011 15:17:56:\nI can confirm this bug.\nWhat is the expected output? What do you see instead?\n$ ./refine\n[: 826: /tmp/refine.An5ligw: unexpected operator\nWhat version of Google Refine are you using?\nfrom svn r2398 and google-refi-2.1-r2136.tar.gz\nWhat operating system and browser are you using?\nLinux server 2.6.32-36-server #  79-Ubuntu SMP Tue Nov 8 22:44:38 UTC 2011 x86_64 GNU/Linux\n. From tfmorris on December 27, 2011 23:41:43:\nYes, this definitely seems to be broken.  I suspect that it was the change for issue #410 that is the culprit.\nAs a workaround you can specify the port on the command line using:\n./refine -p \nA slightly more permanent workaround is to change the line endings on the refine.ini file useing the command:\nfromdos refine.ini\n. From tfmorris on January 27, 2012 22:16:11:\nFixed in r2438.  If anyone doesn't have the fromdos utility and needs to fix this by hand, you can just delete the distributed refine.ini file and recreate it with any Linux text editor.\n. From dfhu...@gmail.com on November 30, 2011 23:58:27:\nFixed by r2394. Could you please test?\n. From dfhu...@gmail.com on December 03, 2011 22:19:26:\nThe bug still exists and will need a non-trivial fix. I'm triaging this issue to post 2.5.\n. From dfhu...@google.com on December 02, 2011 20:44:17:\nThis issue was closed by revision r2398.\n. From tfmorris on September 19, 2012 23:34:03:\nissue #296 has been merged into this issue.\n. From tfmorris on September 20, 2012 17:19:07:\nThanks for the patch!  Patch applied in r2567\n. From tfmorris on December 09, 2011 22:38:55:\nAdded in r2403\n. From danpaulsmith on December 16, 2011 12:39:59:\n(Pressed enter by accident!)\nPlease delete.\n. From tfmorris on December 27, 2011 21:09:46:\nAre you talking about during project import or using toNumber() ?  \nThe problem with a string like 32,645 is that it's ambiguous.  Commas are used as both thousands separators and decimal points, depending on the culture.  We could switch to using DecimalFormat.parse() http://docs.oracle.com/javase/6/docs/api/java/text/DecimalFormat.html#parse%28java.lang.String,%20java.text.ParsePosition%29 to get locale-aware parsing, but then we'd need extra knobs and levers to control it, adding complexity.\nBTW, the transform grel:value.replace(',','').toNumber() will convert numbers in this format if you're sure the comma is a thousands separator.\n. From dfhu...@gmail.com on December 27, 2011 22:23:32:\nIt seems dangerous to parse anything on import anyway, as, for example, I might be in the U.S. processing data files generated by people in Europe, and our thousand separators are different. I think Thad's suggestion for multi-column operation might solve this issue. After import, you can choose one or more columns to convert using the same transform expression. And you can leave out numerically-looking columns like zip code.\n. From dfhu...@gmail.com on December 18, 2011 20:40:30:\nCould you be more specific about \"make some operation on some files\"? Please export your project and attach it if possible.\n. From christia...@gmail.com on December 19, 2011 14:52:57:\nHi, \nthanks for your reply.\nI mean, you work on a csv by doing some operations (like batch edit, add columns, etc...), than you go to undo/redo and click \"extract\", and there is a bug as the JSON script doesn't show up on the right window panel. The popup window is clearly messed up as only the left side (with the list of operations to select) appears. \nSo it is not possible to copy the JSON script and apply it to another project (which would be very nice for me to do). The bug is not linked to the operations you apply as I had it on several different projects. I will send you a screen shoot by email as I cannot attach it here for some reason.\n. From christia...@gmail.com on December 19, 2011 14:54:38:\nHi, I cannot send you an email neither (your email adress is partially hidden). if you try to extract the JSON in the undo/redo section you should see the bug (windows 7; Chrome, refine latest version V2.5)\n. From dfhu...@gmail.com on December 19, 2011 17:26:39:\nI've requested for an increase in the attachment quota. But for now, could you export your project as a tar.gz file (use the Export button at the top right corner) and post it somewhere? Perhaps dropbox?\n. From christia...@gmail.com on December 27, 2011 21:00:12:\nHi dfhu,\nupdate : i found what is causing the bug : see new steps to reproduce:\nenter a very long expression (see example below)\ngo to undo/redo, then extract\nthe right window is not showing as the left windows spans to accomodate the length of the long expression.\nsee self explanatory screen shot.\nexample of long expression : Create column Class at index 1 based on column LOCO using expression rel:if(contains(value,\"BB\"),\"BB4800\",if(contains(value,\"G1000\"),\"G1000\",if(contains(value,\"G1206\"),\"G1206\",if(contains(value,\"G2000\"),\"G2000\",if(contains(toLowercase(value),\"euro\"),\"E4000\",\"\")))))\n. From tfmorris on December 27, 2011 21:31:25:\nThanks for the clarification.  That makes it clearer what the problem is.\n. Fixed by pull request #731 \n. From tfmorris on December 19, 2011 13:49:34:\nThe RDF extension is provided separately from Refine and is supported by a different team.  The page that you downloaded the software from (http://lab.linkeddata.deri.ie/2010/grefine-rdf-extension/) has a link to the issue tracker (http://github.com/fadmaa/grefine-rdf-extension/issues)\np.s. I'm sure they'd appreciate an actual version number (e.g. 2.5, 2.1, etc) instead of the ambiguous \"last one\"\n. From perrone...@gmail.com on December 19, 2011 14:14:39:\nIs Freebase extension supported by the same team? Because I have some problem also with Freebase!  Thanks\n. From tfmorris on December 19, 2011 17:54:11:\nIt looks like perhaps your network is configured incorrectly.  When you paste the address 4.standard-reconcile.dfhuynh.user.dev.freebaseapps.com in your browser's address bar, does it bring up a page titled \"Reconciliation Service API Test Dashboard\"?\nDoes your network have a proxy or firewall which needs to be configured to allow Refine to access the external network?\n. From perrone...@gmail.com on December 20, 2011 08:07:09:\nThanks for your support!\nAs regards the first question I tried to open the mentioned service and is possible to do it both in Explorer and in Chrome.\nI also checked the proxy settings and actually I'm using a proxy server for my LAN..what do you mean with configuring the proxy to allow Refine to access external network?\n. From perrone...@gmail.com on December 20, 2011 10:17:58:\nI have some news...I changed the proxy settings and now at least Freebase works fine but I have some problems with the RDF extension yet trying to use DBpedia so I have opened an issue in the proper place asking for a solution to the problem.\nThanks\n. From tfmorris on December 20, 2011 15:39:42:\nGlad to hear you got things fixed.  Thanks for lettings us know.\n. From thadguidry on December 20, 2011 14:20:14:\nDid you have a checkmark for the option to parse cells containing numbers during the project creation preview ?  That probably needed to be unchecked.\n. From danpaulsmith on December 20, 2011 14:31:05:\nYes, probably, but if I have 5 or 10 other columns containing numbers, wouldn't I want to leave the box checked?\n. From thadguidry on December 20, 2011 14:38:44:\nNOT on the import.  Easier to change the columns type AFTER the import with Edit cells -> Common tranforms -> to number.  Or with a GREL expression toNumber(value)\n. From tfmorris on December 20, 2011 15:44:02:\nUnfortunately, not all importers have the ability to turn off data type guessing/conversion.  There are two good reasons to have it off (and we'll probably be changing the default as well making sure all importers support this): 1) it's slow - the algorithm is try to convert to date, try to convert to float, try to convert to integer, try to ..., well, you get the idea and 2) it's irreversible.\nThe flip side of this is that it's tedious to convert lots of columns after import, so we probably need a better way to convert all columns or convert a bunch of columns.\n. Duplicate of #478 \n. From tfmorris on December 27, 2011 20:57:39:\nFrom a quick glance at the code, it would appear that this option is only implemented for tabular importers, not tree (ie XML & JSON) importers.\n. From tfmorris on December 27, 2011 20:34:22:\nI've seen that in the past, but didn't think it was fatal.  Is it causing a problem on the import or is it a cosmetic issue?\n. From thadguidry on December 27, 2011 21:36:47:\nI do not think I am losing any information, as least from my eyes.\n. From dfhu...@gmail.com on December 27, 2011 22:25:35:\nI was aware of this exception but couldn't figure out an easy way to fix it for 2.5.\n. From tfmorris on January 27, 2012 17:06:23:\nFixed in r2435.\n. From thadguidry on January 27, 2012 19:41:23:\nVerified fixed.\n. From tfmorris on December 28, 2011 19:01:50:\nThanks for the suggestion.  You don't say what clustering algorithm you are using, but you can probably work around this by adjusting the clustering parameters to exclude the undesirable result.\n. From kyawnain...@gmail.com on December 28, 2011 19:34:28:\n@tfmorris\nThanks for your comments.\nI use all the algorithms because they produced different clusters. Metaphone3 and Clonge-Phonetic yeild the most promising clusters. They produce different clusters and thus I run one after another. These algorithms do not have parameter to adjust.\nFrom these algorithms, some clusters contain right members to merge while other cluster(s) from the same algorithm has members mixed with desirable and undesirable ones. For the latter case, E.g six right members are not able to be merge because of unwanted one member and then I have to go back to text facet and edit 5 times. Since I am using multiple text facets as multiple filters to edit specific records (E.g group of villages under specific district under specific province due to very similarity of village names across the different district and provinces). Therefore this kind of editing is a lot time consuming compare to merging through clustering.\n. From thadguidry on December 28, 2011 20:29:09:\nPerhaps an easier way might be to apply your various filters and then STAR those rows... then do clustering ?  Use the Flag and Star to hold onto your filtered items for further analysis with Refine's other tools such as clustering ?\n. From tfmorris on January 12, 2012 06:35:01:\nThanks for the bug report. Fixed in r2421.\n. From dfhu...@gmail.com on January 11, 2012 19:13:25:\nLindsay, we're seeing some potential bugs with certain kinds of Excel files. If your data is not sensitive/private, then please email google-refine@googlegroups.com (with the file attached) and we'll try to figure out what went wrong with it. Reply to this thread\nhttp://groups.google.com/group/google-refine/browse_thread/thread/c939dd20436658eb\n. From elksie2...@gmail.com on July 03, 2012 12:55:07:\nI'm finding that I'm completely unable to import any spreadsheets, CSV files or indeed copy any clipboard information into Google Refine without it hanging. Can you help? This is an example of the sort of data I've been trying to import.\n. From elksie2...@gmail.com on July 03, 2012 12:57:34:\nUpdate: It appears to be a problem with Google Chrome. Great. Works with Firefox.\n. From tfmorris on July 04, 2012 18:08:31:\nElksie - That's surprising since Chrome is one of the primary browsers we recommend.  I just tried that file with Refine 2.5 using both Chrome 20.0.1132.47 m and Firefox 13.0.1 on Windows 7 without problems.  If you'd like us to look into the your problem, please create a new issue with the version numbers for Refine, Chrome, and your operating system.\nLindsay - Did you resolve your problem?  Feel free to reopen this issue with additional information (e.g. the file that fails for you) if you're still having problems.\n. From tfmorris on January 12, 2012 06:21:48:\nThe current implementation is limited to a maximum initial parameter value of 12 (the max factorial that it can compute without overflowing).\nI've added some error checking/reporting and slightly increased the dynamic range (it'll go up to 20 now) as a stopgap, but it needs a new algorithm.  Something along the lines of http://introcs.cs.princeton.edu/java/96optimization/Binomial.java.html\n. From tfmorris on March 08, 2012 22:56:34:\nFixed in r2459.  Refine now knows that combin(60,6) is 50063860.  It should successfully compute any result which fits in a long.  The upper limit is near combin(66,33) =  7219428434016265740\nThe new limit is based on the size of the result, not solely the factorial of the first parameter (which it never computes, because it's using a different algorithm).\n. From tfmorris on January 17, 2012 19:00:47:\nPatch applied.  Thank you!  If you'd like wiki editing privileges so that you can make improvements directly, please send email to David.\n. From tfmorris on September 18, 2012 20:00:58:\nPlease answer the questions in the template.  What version of Refine?  What browser?  What operating system?\nDoes it happen with all files or just one?  If just one, can you provide us with a copy of the file for testing?\n. No response from original reporter.  Closing as unreproduceable.\n. From tfmorris on January 18, 2012 00:32:05:\nFixed in r2423\n. From tfmorris on January 26, 2012 21:07:12:\nDid you mean to attach a file which can be used to reproduce the problem?  Did your import fail?\nissue #416 and issue #513 cover the two exceptions that you saw, both of which were non-fatal in other contexts, which is why I'm wondering if the data actually got imported.  Note also that the initial exception occurred 15 minutes before the others, likely at session startup.\n. From tfmorris on September 18, 2012 20:02:19:\nNo response from user.  Closing, but feel free to reopen if you can provide additional information.\n. From tfmorris on January 26, 2012 18:48:37:\nFixed in r2429.\n. From tfmorris on January 26, 2012 22:40:07:\nFixed in r2432.  A single underscore () is now used for anonymous field names instead of anonymous\n. _From thadguidry on January 31, 2012 16:50:55:\nI wonder if it came from SuggestTerm command extension at one time ? Here's some other areas that a search brings up: http://code.google.com/p/google-refine/source/search?q=suggestT&origq=suggestT&btnG=Search+Trunk\n. From danpaulsmith on January 31, 2012 16:56:53:\nwow, the search is good - I'll use that in future. Seeing as there are so many suggestT's - I guess it's not a bug?\n. From thadguidry on January 31, 2012 17:11:07:\nWhat problem are you having in the recon dialog specifically ?  that lead you to searching the JS code to begin with ?\n. From danpaulsmith on February 14, 2012 14:59:29:\nI'm doing some development around reconciliation - lead me through that file and thus onto the supposed typo.\n. From tfmorris on March 13, 2012 15:12:27:\nFrom the discussion, this doesn't sound like a bug to me.  Feel free to reopen if you believe otherwise.\n. From tfmorris on February 08, 2012 16:35:02:\nPlease use the mailing list for questions.  Refine supports multiple input files.\n. From danpaulsmith on February 14, 2012 14:58:29:\nWe've experienced the same thing! I'm also unsure what happened, but I think I had to force-quit Eclipse when running Refine through Eclipse. Like you said - all projects became corrupt - a bit of a scare!\n. From tfmorris on September 18, 2012 18:52:18:\nLow frequency, but high impact bug, so bumping up the priority.\n. From iae...@gmail.com on February 13, 2012 21:06:40:\nI have the same issue. \nname: name1\nemail: email@1.com\nname: name2\nemail: email@2.com\nname: name3\nemail: email@3.com\nends up showing just the last data pair as a single row (with name and email column headings as expected):\nname3  email@3.com\n. From tfmorris on September 18, 2012 18:45:53:\nWe could do a better job of this out of the box, but there is a way to work around the issue.\n1. Facet on something which marks the first row of each \"record\" (e.g. %0 in my example or \"name\" in the iaeonh's example)\n2. Add column using the expression: row.index\n3. Move the resulting column to the beginning\n4. Fill down on the column so that all rows in the \"record\" have the same index value.\n5. Finally, do the key/value transpose and you'll get the desired result.\n. From tfmorris on September 18, 2012 18:51:18:\np.s. As a breadcrumb for anyone specifically searching for how to do this, my example file is an EndNote citation file.\n. From thadguidry on October 03, 2012 15:16:41:\nNoting the feature name to find this issue easier: Columnize by key/value columns\n. From thadguidry on October 03, 2012 16:17:13:\nAttached another use case for this issue to try and resolve it.\nThe Note field could be choosen with the RECORDNUM column, but that causes only the last record row to be output.  Not choosing the Note Field and only selecting the Key and Value columns will only extract and columnize against the first Key value, in this case the \"Title\" values, but other values in the outer row processing FOR loop sequence do seem to be populated.\nConversely, removing the RECORDNUM column and starting a Columnize by key/value columns on only 2 columns, the Key and Value columns, will only return 1 row, the last record row with all Key fields populated correctly.\n. From tfmorris on October 03, 2012 20:47:27:\nMy workaround (or the last piece of it anyway) should work for Thad's example.\nFill Down on the RECORDNUM column, then transpose on Key + Value.  I get 86 records with reasonable looking columns.\n. From tfmorris on October 05, 2012 23:39:13:\nFixed in r2574 (ca2e959957a53015e9fce178c28ef619cbad1534).  I also added support for proper handling of repeated fields e.g. an EndNote record with multiple author keys.  This only works when there are key, value, and optionally note columns.  When there are more columns the previous/existing algorithm uses a key based on the values of all the remaining cell values and assumes they they all represent a single record.  This allows the k/v records to be non-continuous but has the downside that it doesn't allow multiple records with the same non k/v values.\n. From tfmorris on February 09, 2012 14:41:52:\nSorry to hear you're having trouble.  It looks like you're trying to create a project using a Google Spreadsheet as input.  Are you able to create projects based on any other input types (CSV, Excel, OpenOffice, etc)?  What happens when you close the window that says \"Close this window and return to Google Refine,\" do things continue normally?\nAnother thing to try would be a better web browser such as Firefox or Chrome (or perhaps a later version of IE, you don't say what version you're using).\n. From gerhard....@zww.uni-augsburg.de on February 20, 2012 11:04:56:\nHad the same issue. Worked when using the Chrome plugin for IE\n. From tfmorris on September 18, 2012 20:03:31:\nNo response from user.  Closing.  Refine 2.5 includes better support for Internet Explorer, so may help.\n. From tfmorris on September 18, 2012 18:14:05:\nThanks for the suggestion.  2.5.3 has been released now and we're currently on 2.5.1, so I'll see if we can get this in for the 2.6 release.\n. Fixed by commit 317bf35445f0e860697f84d3da3735f168c67bc8\n. From techtonik@gmail.com on February 09, 2012 20:15:30:\ns/in bundled/is bundled/\n. From tfmorris on April 11, 2012 14:24:11:\nThanks for the patch!  The wiki has been updated.\n. From dfhu...@gmail.com on February 20, 2012 23:05:12:\nCould you elaborate on where that date came from? Did it start out as a string \"2011-05-07\" in a text file, or a Date cell in an Excel file?... And what function is used to format the date? Or is this only what's displayed in the data table?\n. From danpaulsmith on February 21, 2012 09:32:40:\nIt can come from anywhere - I'm not talking about the import stage. This happens after I have transformed a column's cells \"To Date\" (Edit Cells > Common Transforms > To Date) on the project page.\n. From dfhu...@gmail.com on February 21, 2012 17:36:22:\nSo this is about the level of granularity being finer than it is. I'm not sure what the fix would be. If I'm not mistaken, in most if not all programming languages, unless with special date/time modeling, whenever you convert a string to a date/time representation, the level of granularity is lost and you end up with millisecond \"accuracy\". So ... is this feature request asking for special date/time modeling that would retain the level of granularity?\n. From danpaulsmith on February 22, 2012 09:35:04:\nI think it's just a case of using a different date format when only a date is present.\nIf only a date (\"12/10/2010\") is being transformed using toDate(), it should use the xsd:date format (\"YYYY-MM-DD\") and not the xsd:dateTime format (\"YYYY-MM-DDThh:mm:ss\") which incorrectly adds \"00:00:00Z\".\nCan there not be a test in the toDate() function to check whether a date is only a date and not a date-time?\n. From thadguidry on February 22, 2012 14:22:10:\nI have a different need from Dan, to have a PREFERENCE setting to keep dateTime formatting versus date formatting.  Hmm, maybe a checkbox on the importer wizard could be added to handle this ?\nForce dateTime formatting to \"YYY-MM-DDThh:mm:ss\" TRUE/FALSE\n. From ericjarv...@gmail.com on February 26, 2012 14:34:02:\nI second Dan's request... this has also been an issue for me/my data within Refine. \nThanks,\nEric Jarvies\n. From tfmorris on February 26, 2012 16:35:35:\nThe problem is that, although the UI always refers to it as a \"date\" it's really a date/time construct.  When a date string without a time is parsed, the time is set to midnight (it has to be set to something).\nOne possible solution would be to switch to Joda-Time http://joda-time.sourceforge.net/ which does have the concept of a date without a time.  It's relatively small (500KB) and has a compatible license.\nNote that this won't solve 100% of the problem since some of our import formats (e.g. Excel) have a similar problem in that they always assume that there's a time associated with a date (as David mentioned, this isn't uncommon in software).  One thing we could potentially do with Excel is look at the format string for cell and use that to determine whether the value should be a Date or DateTime based on whether the time is being displayed or not.\n@danpaulsmith - There's no reliable way to distinguish a date only from a date/time with a time of midnight after the fact.\n. From dfhu...@gmail.com on February 26, 2012 22:37:56:\nTo carry this thinking further, if the original text is \"2010\" and it gets parsed into date/time or just date or whatever, will we want it to be displayed as \"2010\" or \"2010/01/01\" thereafter?\n. From danpaulsmith on February 26, 2012 22:48:27:\n\"2010\". Adding \"01/01\" raises the same issue as adding \"00:00:00Z\" to a date.\n. From danpaulsmith on March 28, 2012 10:19:35:\nAnother example I noticed with some recent data (XLS import), which might contribute to this:\nOriginal values (Excel file):\nJun-10\nJun-11\nJul-10\nJul-11\nAug-10\nAug-11\nImported values:\n2010-06-01T00:00:00Z\n2011-06-01T00:00:00Z\n2010-07-01T00:00:00Z\n2011-07-01T00:00:00Z\n2010-08-01T00:00:00Z\n2011-08-01T00:00:00Z\nThe imported values suggest the events occurred on the 1st of each month.\nDan\n. From tfmorris on March 28, 2012 17:49:21:\nThose are the display renderings of the original values, not the original values themselves.  It's no different than if you imported a cell with a value of 3.1111 which had a format string that caused it to display as \"3.1\".  If you import that into Refine, you'll get 3.1111, not 3.10.\nThe actual value stored by Excel is 2010-06-01T00:00:00Z (well, actually a floating point value which is equivalent to that).  If you ask Excel what the day of the month is for the date formatted as \"Jun-10\" it'll tell you it's the first.  If you ask for the year, it'll give you 2010, not 10.\nBy changing the format string, you can get this single value to display as 6/1/10, 1/6/10, 1-Jun-2010, or a myriad of other formats.  None of this changes the underlying value itself and all calculations will treat the value as full precision, even if not all the precision is displayed.\nI'm clearly not explaining this adequately, but the rendered value and the value itself are two different things and both Excel and Refine operate on the values, not the renderings (although Excel has more flexibility in changing formats/renderings than Refine does).\n. From danpaulsmith on March 30, 2012 09:00:32:\nSorry, I understand the problem you have now. Excel is creating false information for dates as a result of the way it stores them. Ok, I'll mull this one over on our side. Cheers for the explanation!\n. From thadguidry on March 30, 2012 14:38:42:\nDoes the date/time output enhance to your benefit if you instead, from Excel export to CSV first...and then import that CSV into Refine ?  Curious.\n. From danpaulsmith on April 02, 2012 13:38:48:\nYes, if the data is saved in CSV format and then imported into Refine, the information is correct - unfortunately we're not the ones importing data into Refine.\n. From wdsno...@gmail.com on February 17, 2012 03:06:52:\nHi Tom,\nMore info: I upgraded from Gridworks Version 1.1-r878 .\nI did not export a tar file from Gridworks so I don't have a clean file from before the upgrade, sorry.  Attached is the earliest i have for this one, but I had to retrieve it via G-Refine so it is probably modified.\nRefine is great, I'm up and running with your help, thanks!\n-Will\n. From danpaulsmith on February 26, 2012 22:58:16:\nI also have dead hyperlinks in my cells as a result of undoing and redoing reconciliation in the Undo/Redo panel.\n. From tfmorris on February 26, 2012 23:27:50:\nDan - do you have a copy of a project which shows this that we could look at?\n. From tfmorris on March 08, 2012 00:38:51:\nA patch which repairs the damage after the fact is included in r2454, but I still don't know what the root cause during the upgrade process.\n. This is related to a version upgrade from many versions ago, so closing.\n. From tfmorris on February 20, 2012 17:53:13:\nThanks for the report.  That file can't be handled by Apache POI (which we use for reading Excel files) for some reason.  If you can save it in a different format you should be able to work with it.  What tool (and what version) was used to create the file?\nThe error reporting is broken in this case.  The error which gets reported on the console is:\njava.lang.IllegalStateException: A sheet hyperlink must either have a location, or a relationship. Found:\n\n    at org.apache.poi.xssf.usermodel.XSSFHyperlink.(XSSFHyperlink.java:72)\n. From tfmorris on February 20, 2012 18:42:18:\nThis looks like an Apache POI bug to me and I've filed this bug: https://issues.apache.org/bugzilla/show_bug.cgi?id=52716\n. From gerhard....@zww.uni-augsburg.de on February 21, 2012 06:43:58:\nThe file was created by dumping an mSQL database (by Hughes) to an HTML page using a simple table. The HTML table was dragged and dropped to Excel 2010.\n. From tfmorris on March 08, 2012 00:42:18:\nThe Apache POI team has fixed the bug and I've verified the fix in their current development tree, but we'll need to wait for them to release 3.8 before we can pull in the fix.\nAs a workaround, you can open the file in OpenOffice and resave it or save it from Excel in a different format.\n. From tfmorris on March 08, 2012 15:09:57:\nThe POI team is forecasting that the next release will be in either March or April (ie soon)\n. From tfmorris on March 29, 2012 19:03:42:\nPOI 3.8 has been released and incorporated into Refine.\n. From tfmorris on February 26, 2012 16:54:59:\nThis sounds like it could be related to issue #469.  \nYour suggestion to support direct page navigation is a good one, but please create a separate issue for it.\n. From tfmorris on February 26, 2012 17:09:54:\nI think the problem is actually on the conversion to number.  We're handling number parsing inconsistently between the importers where we attempt to convert to an integer first and only if that fails convert to floating point vs in the toNumber() function where we always convert to floating point.\n. From tfmorris on February 26, 2012 17:27:31:\nFixed in r2446.\n. From tfmorris on February 26, 2012 23:35:39:\nIt won't work for extreme cases, but in your example there's just enough of the pane visible to work with.  You can slide the whole pane left by clicking and holding in the top title bar and then drag the little corner handle at the bottom of the little white sliver on the extreme right to show more of the text pane.  All you need is enough of it to be visible to be able to select all and copy the text.\nStill needs to be fixed, but that workaround might help anyone who runs into this and has enough of the pane showing to work with...\n. This is a duplicate of #508 \n. Fusion Tables has since deprecated the entire API that we use and announced that it will be turned off January 15, 2013 (ie it should already be off, although it doesn't appear to be).\nAnnouncement: https://groups.google.com/forum/?fromgroups=#!topic/fusion-tables-users-group/NrrQcnFBoo0\nMigration Guide: https://developers.google.com/fusiontables/docs/v1/migration_guide\n. The new Fusion Tables API is supported on the fusiontables-migration branch.  There seems to be some flakiness with the upload code, which uses importRows, getting 503 BackEnd unavailable errors, but it may just be instability in the Google API infrastructure.\nWe now use OAuth2 for the Google Spreadsheets API even though it's still GData based just so that we don't have to deal with two different authentication frameworks.\n. From tfmorris on March 02, 2012 22:45:29:\nFixed in r2449.\n. From tfmorris on March 03, 2012 21:37:41:\nFixed in r2450.\n. From tfmorris on March 16, 2012 20:12:41:\nFixed in r2480.\n. From tfmorris on March 16, 2012 20:08:15:\nFixed in r2479\n. From tfmorris on September 22, 2012 18:15:18:\nI've reverted r2479 and provided a more complete fix.  See issue #600 for a description.\n. From danpaulsmith on March 05, 2012 10:33:24:\nversion 2442\n. From danpaulsmith on April 24, 2012 15:37:28:\nHi David,\nAny progress on this?\nThanks,\nDan\n. From dfhu...@gmail.com on May 02, 2012 07:44:36:\nSorry, Dan, I haven't had a chance to start working on it.\n. From tfmorris on March 08, 2012 15:13:09:\nTry running google-refine.exe from a command box so that you can see what error that it's getting.  You can also try running refine.bat.  Let us know what error you are getting.\n. From stroll...@gmail.com on March 08, 2012 22:54:20:\nI ran it through the command prompt and this is what I get:\nError occurred during initialization of VM\nCould not reserve enough space for object heap\nCould not create the Java virtual machine\n. From tfmorris on March 08, 2012 23:13:28:\nDo other Java apps run on your system without problems?\nIt sounds like your system is low on memory or on page file space.  By default Refine uses a heap size of 1 GB (1024 MB).  You could try reducing this to 512 MB to see if you can get it to start, but you'll only be able to work on small data files.  A better solution is to add memory and/or increase the size of your page file (real memory is best, but more virtual memory will help in a pinch).\nYou can change the size of the heap by editing refine.ini and changing the line:\nREFINE_MEMORY=1024M\nto \nREFINE_MEMORY=512M\n(or whatever value you want to use).  Start Refine by running refine.bat so that the .ini file gets read.\n. From stroll...@gmail.com on March 08, 2012 23:41:29:\nThe machine has 2gig of Ram and the page file is set to 2046 - 4092. Should I change the paging file or leave it as is?\n. From stroll...@gmail.com on March 08, 2012 23:44:01:\nI also tried what you suggested and still got this error.\n. From tfmorris on March 08, 2012 00:30:53:\nThe KMZ specific fix has been committed in r2453.\n. From tfmorris on March 13, 2012 15:05:56:\nIs XML the only encoding affected?  Looking at your sample values, I'm a little suspicious that all the failing ones are numbers.  Does the same thing happen with strings which don't need encoding?\n. From danpaulsmith on March 13, 2012 15:20:37:\nAll encodings are affected. You're right about the failing ones being numbers.\nThe escape() function appears to return \"null\" for numbers and dates that have been typed within Refine. Attached another screenshot with the column in view.\n. From tfmorris on March 14, 2012 03:09:47:\nFixed in r2463.  As a workaround, you can use value.toString().escape(\"xml\") instead of value.escape(\"xml\")\n. From danpaulsmith on March 20, 2012 11:48:44:\nHi, \nI've updated to r2482 and escape('xml', value) is still returning \"null\" for integers.\nDo I need to convert the value to a string first, or did you mean you've fixed it so it handles dates and integers?\nThanks,\nDan\n. From tfmorris on March 20, 2012 19:28:18:\nI changed it so that any non-string gets run through ToString() first.  If you're running from Eclipse, you may need to run the Ant build first ./refine build since I've sometimes old classfiles picked up otherwise.\nIf you're definitely running the latest code and it's still not working, please provide some example data and I'll check to see if there's a corner case I missed.\n. From danpaulsmith on March 21, 2012 09:56:41:\nHi Tom,\nAll good, thanks. Used \"ant build\" and \"./refine\".\nCheers!\n. From tfmorris on March 13, 2012 15:08:38:\nSince you're creating them programmatically, deleting the projects programmatically sounds like a reasonable approach.\nAre you looking for a convenience function to make it easier to delete them in a batch or a UI enhancement which would allow the user to delete a large number (all?) projects.  Since it is a destructive operation, I'm not sure having the user have to spend a little time and confirm the deletions individually is such a bad thing.\n. From tfmorris on March 15, 2012 17:50:18:\nGrrr, this is apparently how it's documented to work.  I'm not thrilled about the default (I think it should return all pieces by default), but it's probably too late to change it now.\n. From tfmorris on March 16, 2012 16:25:49:\nFixed in r2475.  There's currently a restriction that you can't use a Freebase topic as the root of the protograph and have this work, but it's always possible to restructure the graph to an acceptable shape.\n. From tfmorris on March 16, 2012 16:38:41:\nThanks for the patch.  It's applied in r2477.\n. From thadguidry on March 16, 2012 13:44:51:\nWhat operating system and Refine version ?\n. From tom.szek...@gmail.com on March 16, 2012 15:47:53:\nSorry, forgot to mention. I'm on OS X 10.7.3, and Google Refine Version 2.5 [r2407].\n. From tfmorris on March 16, 2012 19:49:17:\nI'm fixing some other stuff in this area, so I'll take a look at this as well.\n. From tfmorris on March 16, 2012 20:16:45:\nFixed in r2481\n. From tfmorris on March 16, 2012 20:22:15:\nFixed in r2482\n. From thadguidry on April 05, 2012 16:16:42:\n+1 but I would also like to see this use a new GREL function to do the split/join automatically to createRecords() rows and not just an importer function.  Attached example project that shows the kind of manual labor that has to happen currently because we do not have a GREL function to createRecords(array, OptionalUserDefinedDelimiterStringToPerformTheSplitAndJoinThatCreatesRecrods\"splitme\",BooleanToKeepOrDiscardDelmiterString)\n. From thadguidry on April 05, 2012 16:17:43:\nExample Project attached\n. From tony.hi...@gmail.com on July 09, 2012 15:14:25:\n+1  The =importHTML() formula in Google Spreadsheets is a great intro to simple screenscraping for many users, and the functionality would complement Google Refine Import well. Having loaded the URL, it might make sense to then offer an HTML option with a further refinement to select (at least initially) Table or List type along with the number of the table or list in the page (maybe even autodetecting the number of tables or lists available in the page?)?\n. From tfmorris on September 18, 2012 20:06:34:\nRefine isn't really meant for arbitrary editing of spreadsheets, but one hack that you can use to do this within Refine is:\n1. Add a cell value of \",,,,,,,,,,,,,,,,,,,,,,,,,,,\" (or something similar)\n2. Use the Edit Cells -> Split multivalued cells to create a bunch of empty rows\nCan you explain a little bit more about why you need blank rows which aren't based on the data that you are working with?\n. From steve.mo...@gmail.com on September 18, 2012 20:19:44:\nThis is because when your cleansing data you come across obvious entries that are missing from data sets and want to add them then and there.  A more common case in one project that you see a single row of data that should be split into two or more rows of data.\nExample: When working on a new potential taxonomy you find entries of a number of column the the key is a value like \n  New York/New Jersey Value2 Value3 Value4\nand I want to quickly fix this by inserting a new row with and fix the old row. \n  New York Value2 Value3 Value4\n  New Jersey Value2 Value3 Value4\nThere are a number of other cases like this when working the datasets your cleansing.   If you would like me to clarify further or answer other questions please let me know I would love to help.\n. From steve.mo...@gmail.com on September 18, 2012 20:20:23:\nDoes this explanation make sense?\n. From tfmorris on April 11, 2012 14:17:12:\nThanks for the problem report and fix.  Any chance you could provide the fix in the form of a patch to minimize the chances that we screw it up when applying it?\n. From kaspar.f...@dreizak.com on April 11, 2012 14:28:21:\nHere you go. Cheers!\n. From tfmorris on April 11, 2012 16:56:12:\nFixed in r2490.  I added a conditional to the patch so that it only executes on MacOS.  I also updated Launch4J from 3.0.1 to 3.0.2.\n. From tfmorris on August 30, 2012 16:24:41:\nIn r2538 locks are now acquired in the same order in both pieces of code so that competing threads won't deadlock.\n. From tfmorris on April 20, 2012 22:17:08:\nFacets don't even persist across project editing sessions, let alone export/import cycles, so that would be the first thing to do before being able to export them.\nOne possible workaround, depending on what you're trying to do is, if you've got a set of rows selected with facets, you can flag or star the rows so that you have a permanent record of which rows were selected. \n. The one way that you can preserve facets and their state (selections, etc) is to click the \"Permalink\" link at the top of the project page.  This will fill in the browser address bar with a big long URL that contains the browser information encoded in it.\n. From tfmorris on September 18, 2012 18:32:35:\nThis appears to be due to a bug in the Simile project's ButterflyScriptWatcher class which doesn't synchronize access to a HashMap.\nI've filed this bug http://code.google.com/p/simile-butterfly/issues/detail?id=8\n. I've seen this a few times and I think that it kills the timer thread, so we probably need to either fix Butterfly or, at a minimum, make sure the thread gets restarted.\n. Should be fixed by b3bae764403deea8924a1bda2c4a6535de2662b4 in our fork of the Simile Butterfly project.  The new Butterfly version is labelled v1.0.1 and 644019465819b8e7ced4c49afecfb536ac40656a updates our JAR.\n. From tfmorris on May 01, 2012 20:43:38:\nHi Eric.  The reduction in record count is intentional.  Google Refine considers \"indented\" rows (ie empty leading cells) to be part of the \"record\" they're indented under.  This can be useful for certain types of processing, but can definitely cause unexpected effects if you don't know it's happening.\nSide effects from this may be contributing to the other problems.  Refine (or browsers, depending on your point of view) isn't very good about dealing with records which have large numbers of rows because the default paging is by # of records, so 10 or 25 records could have hundreds or thousands of rows, depending on the organization of the data.\nNot sure about the single column display, but it's almost certainly related.  You could try switching to the row display mode (instead of the default record display mode) to see if it shows the other columns.\nThe error dialog that you're getting is strange.  That happens when you attempt to undo?  Any chance you are running out of memory on your computer?  Does the same thing happen when you restart Refine and/or your computer?\nYour project is almost certainly recoverable.  If you'd like one of us to take a look at it, attach it here and we'll see what we can do.\n. From ericjarv...@gmail.com on May 01, 2012 20:56:40:\nTom,\nI understand moving a column to the first position can be used for created rows within records, but what I am referring to is that Refine does not handle things gracefully should one accidentally place a column in that first slot, for example.  It causes corruption that cannot be recovered, because the 'Undo' becomes non-functional, and because the Columns start disappearing when you attempt to perform the undo process, there is no way to get back to normal, and no way to output/export.  Switching between Row/Record has no effect/benefit when the above mentioned occurs.  So there is no way for me to output/save the project to send it you.\nI have a top of the line Apple towerwith 64GB of RAM, fast RAID drives, etc., so it is not hardware performance related.\n. From thadguidry on May 01, 2012 22:40:15:\nFYI, The \"switching\" from Records back to Rows...might take quite a while, depending on how much data Refine has to churn through... I have seen it take over 10 mins in a few of my datasets...but it did eventually return back to Rows mode.\n. From tfmorris on May 02, 2012 00:37:36:\nOK, I thought you were describing three problems.  Sounds like it's just two (probably related) problems.\nHere are a couple of other things to try:\n- Start Refine from a terminal window.  Retry the steps that produce the error and report any errors logged on the terminal\n- Use the following steps to create a project export:\n  - Click \"Browse workspace directory\" at bottom of main Refine screen\n    (it's ~/Library/Application Support/Google/Refine/ on Mac)\n  - Hover over desired project or right-click and use \"copy link\" to get project #\n  - In a terminal window, do the following:\ncd <refine workspace directory>\ncd <project #>.project\ntar cvzf /tmp/<project name>.google-refine.tar.gz *\n\nFor example, the steps for one of my projects were:\n      cd ~/.local/share/google/refine\n      cd cd 1978234000731.project\n      tar cvzf /tmp/myproject.google-refine.tar.gz *\n- Attach the .google-refine.tar.gz and we can see if it's recoverable.\n. From tfmorris on May 02, 2012 00:38:52:\nTry opening a command window and running the refine.bat file by hand.  That way you'll be able to see the error that is causing it to stop immediately.\n. From tfmorris on September 18, 2012 20:08:59:\nClosing due to lack of response from user.  Feel free to reopen with additional information if you're still having the problem.\n. @mjy if you have an example of a file demonstrating this behavior and it's not confidential, please provide it.\n. Definitely.  Google Refine has been rebranded OpenRefine, but there's been no public release yet, so you're using the latest!\n. Hmmm, looks Github doesn't support attachments to issues.  How primitive!\nIf you have someplace you can host it (e.g. Google Drive), you can post a\nURL.  Otherwise you mail me the file and I'll take a look. (same name on\ngmail)\n. Thanks for the example data.  That helps a lot.  If I import the file as\ntext lines and then use a text filter with the regex \\t{63}$ I get 64 lines\nwhich match i.e. 64 lines end with a string of at least 63 tabs.  I think\nthis is the source of your extra columns.\nAdditionally, the file has character encoding issues.  If I had to guess,\nI'd say that perhaps to different encodings were mixed together or the\ncharacter encoding was declared wrong at some point.  When I edited in\nemacs, it couldn't find a character encoding which would work for all of\nthe characters in the file.\nIf you've got more questions or need additional help, let's move the\ndiscussion to the mailing list since it's not related to this issue.\n. From tfmorris on May 03, 2012 15:44:41:\nThanks for the patch.  Applied in r2494.\n. From ericjarv...@gmail.com on May 03, 2012 12:14:41:\nI had failed to mention above, that this is a cumulative effect, so if you have multiple Refine projects opened that you are working on, and are doing a lot of cross project data transferring(e.g.- cell.cross('someRefineProject','id')[0].cells['name'].value), those other projects will be effected by the aforementioned described bug in the other Refine project.  So until the bug is fixed, I suggest when anyone does a removal of duplicates, they stop and restart Refine, reload their project, and verify the duplicates facet is reporting '0' as it should be after one deleted/removes duplicate records.\n. From tfmorris on May 03, 2012 18:48:44:\nThanks for the bug report.  Do you have a small test data set that we could use to reproduce the problem?  That would help make sure we're doing exactly the same things you are.\n. From ericjarv...@gmail.com on May 03, 2012 19:03:51:\nTom,\nYou can do this with 'any' data set that has a handful of duplicate records in it... no matter how big or small, if you use the duplicates facet, remove the duplicate records, and remove all facets, and create a new duplicates facet, you'll get the error, meaning you'll get a duplicates facet showing you there is duplicates... but of course there isn't any because you just deleted them a few steps earlier.\nGive it a try and you'll see what I mean.  Now then, if you introduce large data sets with duplicates, you then get the above problem, plus a nasty caching problem.\n. I was never able to reproduce this interactively, but believe c961bb64 should fix it.  I think it would only occur if the facet was on a column that wasn't modified and there was subsequently row removal operation.\n. From dfhu...@gmail.com on May 16, 2012 13:15:12:\nEric, I understand the problem you're describing. I actually never intended for anyone using Refine to be paging through much of their data. The premise is that by using facets, you can actually isolate problems in the data and then fix them all at once. So, I'm curious about the data and tasks you're performing, and wondering if there are better ways than paging through it; or if a cell-editing tool like spreadsheet software is what you need. Could you describe your data and tasks? Thanks.\n. From j...@tekii.com.ar on May 08, 2012 13:26:29:\nCould you please name the Google Refine version + iOS version + an example of a name that crashes?\n. From j...@tekii.com.ar on May 08, 2012 14:17:15:\nIt's confirmed also for refine 2.5 in Linux.\nIt seems that spetial characters are not replaced before saving the file in XlsExporter.java: 83 \nAnd poi, does not support this in the filename:\nnever null\nminimum length is 1\nmaximum length is 31\ndoesn't contain special chars: : 0x0000, 0x0003, / \\ ? * ] [\nSheet names must not begin or end with ' (apostrophe)\nhttp://poi.apache.org/apidocs/org/apache/poi/ss/util/WorkbookUtil.html\nI upload a posible patch for this.\n. From tfmorris on October 12, 2012 23:06:31:\nThanks for the patch!  Patch applied in r2582.  Look for the fix in the 2.6 release.\n. From tfmorris on May 11, 2012 14:45:45:\nGood suggestion.\n. From Martin.M...@gmail.com on May 14, 2012 21:31:09:\na workaround is to click on change and close the transform windows without doing any modification. The grel expression is now displayed in the facet choice column. \nThis apply to any facet. \n. From ericjarv...@gmail.com on May 16, 2012 12:40:21:\nThanks... that certainly helps!\nEric Jarvies\n. From tfmorris on May 14, 2012 19:24:05:\nThanks for the patch.  We'll try to get it reviewed and applied.\n. From sergio@wikier.org on May 14, 2012 20:10:57:\nWell, actually I'm not sure if this patch applies only under Java containers, or could be also useful under other deployment environments, such as inverse http proxies.\nIn case it could help, I also point a related issue detected in an extension: https://github.com/fadmaa/grefine-rdf-extension/pull/51#issuecomment-5691585\n. From sergio@wikier.org on May 18, 2012 07:30:37:\nHere also another patch for solving some non null-save operations.\n. From sergio@wikier.org on June 28, 2012 14:48:47:\nAttached a clean version of the first patch (I was used some fake paths which may introduce issues applying it).\n. From sergio@wikier.org on June 28, 2012 14:51:23:\nAttached another small patch for fixing the core module's controller.\n. From sergio@wikier.org on June 29, 2012 10:18:26:\nI'm thinking that all invocations to \"../command/foo\" actually must be to  \"command/foo\"\n. From sergio@wikier.org on August 31, 2012 07:50:20:\nDo you plan to take care of this issue someday?\nAttached the last version of the patch including all changes working, that could be retrieved from the mercurial where we worked on: http://bitbucket.org/wikier/google-refine \nI don't want to convert such temporal repository in a fork of the project, but with latest changes it starts to be quite hard to keep it synchronized with upstream. \n. From tfmorris on September 19, 2012 23:24:16:\nI'm a little bit concerned about testing, but I'll see if we can get this included in the next release.\n. From sergio@wikier.org on September 20, 2012 06:12:18:\nCool! I can support you on testing if needed.\n. From tfmorris on October 13, 2012 17:50:14:\nOK, the patch has been applied in r2584. It was missing some /command references in the Freebase extension, clustering, etc.  Hopefully I caught them all.  The majority of the risk of things not working is when you're not running at the root, so normal operation should be unaffected.  Let us know if you find any references which were missed.  Although you mentioned changing \"../command\" to just \"command\" I still found \"../command\" references so those were changed before committing.\nThe patch also included a couple of unrelated things like the temporary directory fix.  For future reference, please keep each patch focused on a single issue.\nThe file main/webapp/WEB-INF/web.xml had some changes that I didn't include:\n- I restored the DTD reference\n- I didn't include the refine.data definition (but left it there commented out for now)\n- I restored some of the Jetty parameters that had been commented out\n. From tfmorris on September 18, 2012 20:13:38:\nPlease ask questions on the mailing list (Google Groups).  I'm not sure off hand what you're problem is, but I'm sure we can get it sorted out.\n. From tfmorris on May 23, 2012 21:52:44:\nThanks for the bug report.  Here's a slightly more complete set of sample data which shows the discrepancy:\n[{\"id\":\"39766\",\"cell\":[\"39766\",\"199\",\"T1009\",\"Ad\",\"An\",\"foo\",\"DEU\",\"19\",\"01:49\"]},\n{\"id\":\"39766\",\"cell\":[\"39766\",\"199\",\"T1009\",\"Ad\",\"An\",\"\",\"DEU\",\"19\",\"01:49\"]}]\n. From tfmorris on September 08, 2012 01:32:44:\nThis is fixed in r2543.  See the description in issue #596 for more detail on the fix, but all empty strings (and nulls) will now be imported by default.  There's an option to turn this off and restore the old behavior, but the default is to do it correctly.\n. From tfmorris on October 12, 2012 19:51:05:\nLooks good to me.  It brings add & delete into alignment with each other.  Patch applied in r2578.\n. From tfmorris on May 30, 2012 19:30:39:\nSorry about the problem.  You don't happen to have the server console log for this run do you?  Refine logs the actual amount of heap that it thinks it's using when it starts up.  I'd just like to make sure that it's actually recognizing the VM options that you're giving it.\nAlso, does the amount of time that it runs vary with the amount of memory that you give it?  If not, that would seem to indicate some other problem (lack of memory and/or page file?).\nAs an aside, if all you want is the descriptions, the blob API should give you that without all the other stuff (although I can't imagine that it's increasing the memory requirements all that much).\nResumable operations for something long running like this is a good suggestion, but would require significant infrastructure support (undo, etc).\n. From demonsteam on May 31, 2012 12:27:20:\nHi and thank you for your fast response. I don't have the server log anymore, but there was nothing abnormal except of the exception posted above. I have attached jvisualvm during the execution last time and I can confirm that the VM options were correct recognized (you could see the redundand options of Xms and Xms (default settings and custom settings), but they were successful overriden with my custom settings). I can reproduce this exception any time again.\nI couldn't find any dependency between the time running and the memory given, but between the memory and the time between the requests. My last run was interrupted after about 48 hours by 69% progress(time between the requests was set to 600 millis and with the memory options listed above).\nI wasn't aware with the blob api, but this is good suggestion and I'll look into as well. The descriptions were only example of the data, that I'm trying to parse, I need more of the details contaned in the json response.\nIn my opinion, the implementation of resumable operations wouldn't be that hard and time consuming, you need only to store the last operation executed with the given parameters and the last successful processed index of your data, so you can continue in case of server crash or some other disaster case. I would be interested to know what happens with the already downloaded temporal data and where it is stored (hopefully not in the memory ?). Thank you in advance.\n. From tfmorris on May 31, 2012 15:00:08:\nRefine operates with all data in memory, so you need to have enough heap & virtual memory to hold your entire project data, plus any working storage.  Looking at your numbers makes me think that your heap is underconfigured.  The example query that you give returns 18KB of data.  Multiplying that by 200K rows is going to use up your entire 4GB heap before taking into account object overhead, working storage, etc.\nI would at least double the size of the heap or splitting the operation into chunks.  Attempting a 3 day long operation without checkpoint/restart is probably optimistic, so splitting things up is probably the best choice.\nBTW, I don't mean to discourage you from implementing resumable operations.  We're always happy to receive patches from folk.  I think it may be more complex than you realize, but if your up to the task we'll be happy to review the results.\n. From demonsteam on May 31, 2012 15:20:20:\nI understand know why I become OutOfMemoryError, because my data size is too large to be kept in the heap. But when you restart google refine, you are still able to open existing projects, so data is stored somewhere, doesn't it? \nBut you are right, I'm not aware with the architecture of the project and I cannot say for sure if such feature is easy to implement or not. I had thought already for strategy of processing chunks of the data, but hoped for easier solution.\nI don't think that I have the time to work in the project and the develop a patch for resumable operations, but if I do find, I will certainly let you know. Thank you for your time!\n. From tfmorris on June 01, 2012 14:18:51:\nSorry to hear about the problem.  On the surface, that file sounds like it shouldn't be too big, although it depends to some degree on the number of cells and type of data.\nIt almost sounds as if the browser itself is crashing which makes me wonder if there's something unexpected about the data.  Are there a large number of rows with blank cells in the first column?  That could make Refine think they form pseudo-records causing it to attempt to display a large number of rows at once in the browser.  If this is the case, you should be able to import the file as plain text, switch to row display mode (record mode is the default), split the text lines into columns and move a different column to the first (leftmost) position to prevent this from happening.\nIf that's not the problem, can you try opening a Terminal window and running the Refine server there to see if there are any error messages on the console when the import fails?\n. From glit...@perpetuales.com on June 01, 2012 15:47:50:\nThe first column does not have blanks... there are a lot of blanks throughout the table... How do I run using Terminal?\n. From libo...@gmail.com on June 11, 2012 12:41:10:\nExcuse the typo - it takes 15 seconds on the screenshot, but I noticed also times around 30 seconds in similar cases - probably dependent on the real number of rows in each record.\n. From tfmorris on September 08, 2012 02:58:29:\nI've tracked this down to the column width resizing code in data-table-view.js.  DataTableView._adjustDataTables() is causing an enormous amount of time to be spent in jQuery's cssHooks.get() method, but I'm not sure what the fix is yet.  \nQuitting for now, so recording for posterity...\n. From sergio@wikier.org on June 18, 2012 15:10:41:\nSome people are already tried to add timeout support to $.getJSON, for instance: http://maheshbokkisam.blogspot.co.at/2012/03/jquery-getjson-with-timeout.html\n. From sergio@wikier.org on June 29, 2012 07:18:42:\nThe attached patch will handle such problem. Maybe it'd nice to report the user about such problem, but at least this solves the issue.\n. From tfmorris on October 12, 2012 22:31:34:\nFixed in r2580.  I added error reporting as well as a 10 second timeout for reconciliation service to reply within.  Thanks for investigating and getting the patch started.\n. From tfmorris on September 18, 2012 20:24:41:\nWhen I try this in Chrome on Windows, I just get new file with \"(1)\" added to the filename as I'd expect.  I'll see if I can reproduce it on Linux.\n. @Busa78 Are you using the DERI RDF Extension?  If so, that's probably a separate issue which should be reported to the developers of that extension.\n. From tfmorris on August 03, 2012 18:01:49:\nFixed in r2520.\n. From sergio.fernandez@fundacionctic.org on July 02, 2012 13:50:18:\ntypo at the title: s/directoty/directory\n. From tfmorris on October 13, 2012 16:00:45:\nFixed in r2583.\n. Another one which is likely related (perhaps caused by trying to use strip() on a null?)\nGREL: with(value.split(','),w,w[1].strip()+' '+w[0].strip())\n13:28:00.952 [          org.mortbay.log] Error for /command/core/text-transform (316ms)\njava.lang.AbstractMethodError: java.lang.Exception.toString()Ljava/lang/String;\n        at com.google.refine.grel.ast.FunctionCallExpr.evaluate(FunctionCallExpr.java:71)\n        at com.google.refine.grel.ast.FunctionCallExpr.evaluate(FunctionCallExpr.java:62)\n        at com.google.refine.grel.ast.OperatorCallExpr.evaluate(OperatorCallExpr.java:57)\n        at com.google.refine.grel.ast.OperatorCallExpr.evaluate(OperatorCallExpr.java:57)\n        at com.google.refine.grel.controls.With.call(With.java:71)\n        at com.google.refine.grel.ast.ControlCallExpr.evaluate(ControlCallExpr.java:57)\n        at com.google.refine.operations.cell.TextTransformOperation$1.visit(TextTransformOperation.java:181)\n        at com.google.refine.browsing.util.RowVisitorAsRecordVisitor.visit(RowVisitorAsRecordVisitor.java:61)\n        at com.google.refine.browsing.util.ConjunctiveFilteredRecords.accept(ConjunctiveFilteredRecords.java:64)\n        at com.google.refine.browsing.util.FilteredRecordsAsFilteredRows.accept(FilteredRecordsAsFilteredRows.java:50)\n        at com.google.refine.operations.EngineDependentMassCellOperation.createHistoryEntry(EngineDependentMassCellOpera\ntion.java:75)\n        at com.google.refine.model.AbstractOperation$1.createHistoryEntry(AbstractOperation.java:52)\n        at com.google.refine.process.QuickHistoryEntryProcess.performImmediate(QuickHistoryEntryProcess.java:73)\n        at com.google.refine.process.ProcessManager.queueProcess(ProcessManager.java:82)\n. This could also cause errors when creating custom text facets using array indexing.  In that case, and I suspect the others, the missing method was ArrayIndexOutOfBoundsException.toString().\n. From edouard....@gmail.com on August 06, 2012 15:46:17:\nHi,\nI'm having exactly the same issue running Mountain Lion...\nSomeone can help ?\n. From erin.s...@gmail.com on August 07, 2012 19:33:44:\nHaving the same issue, running Mountain Lion\n. From bodna...@gmail.com on August 08, 2012 03:42:17:\nI was able to fix this on my wife's laptop by taking the following steps:\n1) Open System Preferences\n2) Open Security & Privacy\n3) Go to the General Tab\n4) Change the \"Allow applications downloaded from:\" setting to \"Anywhere\"\nThis appears to be a security issue with Mountain Lion, but the above steps provide a workaround until it is \"fixed\" by Google.\n. From tfmorris on August 08, 2012 16:42:25:\nissue #602 has been merged into this issue.\n. From asid...@gmail.com on August 09, 2012 07:20:04:\nI fixed this issue on Mountain Lion with changing \"Allow applications downloaded from:\" setting to \"Anywhere\" too.\n. From tfmorris on August 09, 2012 14:59:11:\n@bodnarbm - Thanks for diagnosing the problem and reporting back on your fix.  Much appreciated!\nApple broke the installer with introduction of the Mountain Lion upgrade and many Java applications were affected.  If you search for the misleading error message, you can see the number of people this is affecting as Mountain Lion upgrades are done.\nhttps://www.google.com/search?hl=en&q=%22is+damaged+and+can%27t+be+opened.+You+should+move+it+to+the+Trash.%22\nThere's a thread on Apple's java-dev list about this misleading error message (which is big part of the problem since it directs attention away from the real error) http://lists.apple.com/archives/java-dev/2012/Jul/msg00106.html\nThis Apple support document describes Gatekeeper and how to configure it so Refine works (the procedure described in Comment 3 http://support.apple.com/kb/HT5290\nThis Oracle article discusses Mountain Lion and Java problems in general https://blogs.oracle.com/talkingjavadeployment/entry/java_applications_and_gatekeeper\nLastly, a potential solution is described here http://lists.apple.com/archives/java-dev/2012/Jul/msg00136.html\nI don't have a Mac to test it on, but it might change the misleading \"damaged\" error message to a more reasonable \"your app is unsigned, do you want to run it.\"\nAll in all, I'm pretty pissed off at the amount of time and money Apple is costing both developers and users.\n. From tfmorris on September 25, 2012 13:26:44:\nissue #622 has been merged into this issue.\n. From tfmorris on September 25, 2012 13:59:39:\n@dfhuynh - Do you think you can test this fix for 2.6?\n. From tfmorris on September 26, 2012 14:47:28:\nissue #621 has been merged into this issue.\n. @emgee3 Thanks for testing.  We'll go with some variant of this for 2.6.\nRuby unsigning script referenced above - http://snipt.org/kto/ (code copied below)\nC unsigning program - http://www.woodmann.com/collaborative/tools/index.php/Unsign\n``` ruby\n!/usr/bin/env ruby\nDeactivates any embedded code signatures in a Mach-O binary.\nmodule MachO\n  class Unsign\n    def self.unsign(filename)\n      File.open(filename, \"r+\") do |f|\n        Unsign.new(f).headers\n      end\n    end\nattr_accessor :headers\n\nprotected\n\nFatHeader = Struct.new(:cpu_type, :cpu_subtype, :offset, :size, :align, :mach)\nMachHeader = Struct.new(:cpu_type, :cpu_subtype, :filetype, :ncmds, :sizeofcmds, :flags, :reserved, :cmds)\nLoadCommand = Struct.new(:cmd, :cmdsize)\n\ndef initialize(f)\n  @f = f\n  @headers = process\nend\n\ndef debug(message)\n  puts message if ENV[\"DEBUG\"]\nend\n\ndef word_type\n  @big_endian ? 'N' : 'V'\nend\n\ndef patch_code_signature(lc)\n  # just change LC_CODE_SIGNATURE to a high value that will be ignored by the loader\n  debug \"PATCHING LC_CODE_SIGNATURE\"\n  @f.seek(-8, IO::SEEK_CUR)\n  @f.write([0xff, lc.cmdsize].pack(\"#{word_type}2\"))\n  lc\nend\n\ndef process_mach\n  len = @x86_64 ? 7 : 6\n  header = MachHeader.new(*@f.read(len*4).unpack(\"#{word_type}#{len}\"))\n  debug \"MACH HEADER: #{header.inspect}\"\n  header.cmds = (1..(header.ncmds)).collect do\n    lc = LoadCommand.new(*@f.read(8).unpack(\"#{word_type}2\"))\n    debug \"LOAD COMMAND: #{lc.inspect}\"\n\n    lc = case lc.cmd\n    when 0x1d then patch_code_signature(lc)\n    else lc\n    end\n\n    @f.seek(lc.cmdsize - 8, IO::SEEK_CUR)\n\n    lc\n  end\n  header\nend\n\ndef process_fat\n  num_arches, = @f.read(4).unpack(\"N\")\n  arches = (1..num_arches).collect do\n    FatHeader.new(*@f.read(20).unpack(\"N5\"))\n  end\n  debug \"FAT HEADER: #{arches.inspect}\"\n  arches.each do |arch|\n    @f.seek(arch.offset)\n    arch.mach = process\n  end\n  arches\nend\n\ndef process\n  magic, = @f.read(4).unpack(\"N\")\n  debug \"MAGIC: 0x%08x\" % magic\n  case magic\n  when 0xcafebabe then @big_endian, @x86_64 = false, false; process_fat\n  when 0xfeedface then @big_endian, @x86_64 = true,  false; process_mach\n  when 0xcffaedfe then @big_endian, @x86_64 = false, true;  process_mach\n  when 0xcefaedfe then @big_endian, @x86_64 = false, false; process_mach\n  else raise \"unknown magic: 0x%08x\" % magic\n  end\nend\n\nend\nend\ncommand line driver\nif FILE == $0\n  if ARGV.empty?\n    $stderr.puts \"usage:  #{$0} filename ...\"\n    exit 1\n  end\nARGV.each do |filename|\n    puts \"removing signatures from: #{filename}\"\n    MachO::Unsign::unsign(filename)\n  end\nend\n```\n. Starting with 2.6-alpha1 kits are signed with a self-signed certificate.  The OS X Gatekeeper will ask if you want to trust an application from an unknown developer, but it won't whine about things being \"damaged.\"\n. The 2.6 beta kit is signed https://github.com/OpenRefine/OpenRefine/releases/tag/2.6-beta.1\n. @felixrabe I just double checked the v2.6-beta1 kit on a fresh Mac and didn't have any issue with it.  I got a message saying that my security preferences only allow apps from identified developers, but after going to Security Preferences, I was given the option to open it anyway.  Nothing about the app being \"damaged\" as it used to do with 2.5.\nHas anyone else had problems with that kit?  We're about to generate the final v2.6 kits and I want to make sure that this problem is actually solved.\n. @herrernst Thanks very much for the feedback.  I wanted to make sure I wasn't imagining things. :-)\nIf anyone sees this problem with a recent kit (perhaps on a OS X version), please feel free to re-open, but I'm closing for now.\n. @janemnichols Your comment is a little confusing because you mention OpenRefine, but screen shot shows Google Refine, implying that you were running the older Google Refine 2.5 kit.  The problem definitely still exists in 2.5.  It's only OpenRefine 2.6 which will have the fix.\n. From tfmorris on August 02, 2012 15:53:52:\nThe issue appears to be that facetCount isn't converting the cell values to strings first, the way the text facet does.  You can work around this by transforming the ids to text first.\n. From tony.hi...@gmail.com on August 02, 2012 18:42:02:\n@tom thanks - I cast a new column w/ toString and went that way....\n. @Downchuck I don't consider 150K rows \"large.\"  If you can reproduce this and provide more details than it does \"not work well,\" please create a new issue and include as much information as possible.\n. From tfmorris on August 02, 2012 21:43:07:\nFixed in r2519.  The function also now does better error checking and reporting on the arguments and there are some rudimentary tests for it all.\n. From tfmorris on August 03, 2012 13:31:59:\nissue #595 has been merged into this issue.\n. From tfmorris on August 03, 2012 13:31:59:\nI actually fixed this just yesterday.  The code was converting the second date to a string and then attempting to re-parse it as a date which was failing because the internal toString() format looks more like debug output than a parseable date.\nThe fix is available now in the source repository and will be included in the next release.  As a workaround with 2.5, you should be able to do something like:\nvalue.diff(now().toString(),\"days\")\n. From iain.dil...@gmail.com on August 03, 2012 14:49:46:\nThank you! Good to know.\n. From tfmorris on September 08, 2012 01:30:28:\nThe JSON importer was handling all values as strings and then attempting reparse them itself.  In r2543 I've extended the tree parse framework (used by JSON & XML) to support any Serializable data type that Refine supports.  All parsing is now handled by the JSON parser and no strings are touched by default (before it was trimming whitespace and skipping empty strings - issue #578).\nAll JSON keywords are supported including null, true, & false.  Each is imported as the appropriate type directly.\nAll number parsing is done by the JSON parser and range has been extended to support Doubles (only Floats were supported before).\nOptions are available to trim white space, skip empty strings and attempt to parse strings as numbers which can be used to restore the previous behavior, but they're all off by default.\n. From tfmorris on August 03, 2012 22:12:40:\nFixed in r2521\n. From tfmorris on August 04, 2012 23:17:44:\nSorry didn't see that this had already been entered, so I added issue #599.  The problem has been fixed in the source repository and the fix will appear in the next release.\n. Duplicate issue is actually #598 here on Github (skew introduced during move from Google Code)\n. From tfmorris on August 04, 2012 18:05:03:\nFixed in r2523.  The parsing preview panel displays now, but the RDF/XML parse isn't returning anything to display.\n. From tfmorris on August 04, 2012 23:17:44:\nissue #598 has been merged into this issue.\n. From tfmorris on September 22, 2012 18:09:45:\nThis is due to an incomplete fix for issue #544.  I've reverted the changes from r2479 (which also caused introduced the bug in issue #618) and provided a more complete fix in r2569.\n. From tfmorris on August 09, 2012 14:19:21:\nFixed in r2527.  You can now select the root object if it's not an array (arrays are never selectable).\n. From tfmorris on August 14, 2012 19:33:00:\nThanks for the detailed step-by-step.  Sorry about the problem.  Increasing priority since it represents potential data loss.\n. From tfmorris on August 18, 2012 21:59:51:\nWhere in the GUI did you do the manipulation which resulted in that JSON for the operation?  I'm not seeing anything which would allow you to move a column out of range.\n. This problem can be triggered from the UI by using \"Move column left\" on the first column or \"Move column right\" on the last column.  Because of the order of operations is remove then add and it fails before a history entry is created, the operation is not undoable or recoverable.\nThis has been fixed with better range checking by commit 12a61b6ec62719ac19098e2eb7fb52c85986051e but even better would be to just disable the commands for the end columns.\n. From tfmorris on August 14, 2012 20:03:04:\nThis function implements an \"old-school\" definition of whitespace as described here: http://docs.oracle.com/javase/1.4.2/docs/api/java/lang/String.html#trim()\nThere are lots of Unicode whitespace characters (em-space, en-space, thin space, nbsp, etc) which aren't included.\n. From tfmorris on August 14, 2012 20:11:04:\nUpdated to use the Guava method CharMatcher.WHITESPACE.trimFrom(s) in r2528.  Still needs tests.\n. From tfmorris on August 14, 2012 23:03:00:\nI've added some basic tests.  The two characters that the Guava method doesn't appear to handle are 0-width NBSP \"\\uFEFF\" and Word Join \"\\u2060\".  I'm not going rely on Guava for now rather than trying to work around this.\n. From tfmorris on September 18, 2012 20:29:16:\nThat's a very brief description, but if I understand what you're asking for, you can go to the transformations history when you're editing a transformation and \"star\" any transformations in the history that you want to save for reuse.  Of course, you also have all of the transformations saved in the history as well until they roll off the end.\n. From tfmorris on September 18, 2012 20:32:41:\nThis sounds very similar to issue #585.  What version of Chrome?  Does the same thing happen in other browsers?\n. From tfmorris on September 07, 2012 21:50:09:\nFixed in r2542.\n. From tfmorris on September 11, 2012 23:06:55:\nThanks for the patch!  Applied in r2545.\n. From s...@google.com on September 12, 2012 18:48:36:\nCan you also remove the file extensions/gdata/module/MOD-INF/lib/google-collect-1.0-rc1.jar since it is no longer needed.\n. From tfmorris on September 13, 2012 18:06:46:\nGood point.  It's gone now.\n. From runuppat...@gmail.com on September 23, 2012 21:37:45:\nHi, I'm getting this error while trying to import from googledocs:\njava.lang.NoSuchMethodError: com.google.common.collect.ImmutableSet.of([Ljava/lang/Object;)Lcom/google/common/collect/ImmutableSet;\n    at com.google.gdata.wireformats.AltFormat$Builder.setAcceptableTypes(AltFormat.java:399)\n    at com.google.gdata.wireformats.AltFormat$Builder.setAcceptableXmlTypes(AltFormat.java:387)\n    at com.google.gdata.wireformats.AltFormat.(AltFormat.java:49)\n    at com.google.gdata.client.Service.(Service.java:558)\n    at com.google.refine.extension.gdata.GDataExtension.getDocsService(GDataExtension.java:97)\n    at com.google.refine.extension.gdata.GDataImportingController.doListDocuments(GDataImportingController.java:127)\nI'm launching from within Eclipse and Windows, I'm synced with the current version (2569), from what I see it seems gdata-core-1.0 is still calling the old library. Is it possible that the jar in the SVN is not up to date or am I missing something in the setup?\n. From s...@google.com on September 23, 2012 22:42:24:\nThis can be fixed by updating the following gdata client libs from the latest version at http://gdata-java-client.googlecode.com/files/gdata-src.java-1.47.1.zip\nM       extensions/gdata/module/MOD-INF/lib/gdata-core-1.0.jar\nM       extensions/gdata/module/MOD-INF/lib/gdata-spreadsheet-3.0.jar\nM       extensions/gdata/module/MOD-INF/lib/gdata-docs-3.0.jar\nM       extensions/gdata/module/MOD-INF/lib/gdata-base-1.0.jar\nM       extensions/gdata/module/MOD-INF/lib/gdata-spreadsheet-meta-3.0.jar\nM       extensions/gdata/module/MOD-INF/lib/gdata-docs-meta-3.0.jar\nM       extensions/gdata/module/MOD-INF/lib/gdata-client-1.0.jar\nM       extensions/gdata/module/MOD-INF/lib/gdata-media-1.0.jar\nM       extensions/gdata/module/MOD-INF/lib/gdata-client-meta-1.0.jar\n. From tfmorris on September 18, 2012 15:31:52:\nCan you provide a reference for the definition of this function?\nSince we already provide a variety of date formatting functions, I'm guessing that what you really need is to be able to convert from a microsecond Unix time (ie Unix time multiplied by 1,000,000) to a Refine DateTime data type.  From there you would be able to use any of the standard date functions.\n. From to...@benmoshe.com on September 18, 2012 19:53:24:\nYes, it is very common to get the timestamp in microsecond unix time, a google example can be seen at rbig query functions https://developers.google.com/bigquery/docs/query-reference#timestampfunctions, but there are many other places you use it... \nAxtuwlly, I wanted to use refine as part of automatic pre processing of data I am getting, and was wondering on size of input, and do it all in thbwckgrounf before loading csv to big query...\n. From tfmorris on September 18, 2012 15:11:01:\nYou need to have Java installed and your JAVA_HOME environment variable set to point to it to be able to use Refine.\nPlease use the mailing list/group for questions.\n. From tfmorris on September 17, 2012 22:28:42:\nissue #612 has been merged into this issue.\n. From tfmorris on September 18, 2012 15:33:37:\nDoes this happen independent of whether you ask for strings to be quoted on output?  Are the cell values that you're attempting to output nulls or empty strings (\"\")?\n. From Austen....@gmail.com on September 18, 2012 15:40:23:\nSo it turns out the person who was inspecting the CSV files I sent her was getting them backwards. Google Refine was outputting them with just ,, for a blank cell. The data we imported to GR before had the ,\"\",  But GR turned them into ,,\nSo I think this is a non-issue at this point.\nBut can you tell me where I would ask for strings for to be quoted on output like you mentioned? I can't find settings like that anywhere.\n. From tfmorris on September 18, 2012 16:06:59:\nThanks for the update.  I'll close this.\nAs far as the output option, I was apparently misremembering the input option which controls quote handling.  On the output side we always follow the spec.\n. From tfmorris on September 18, 2012 16:05:07:\nThanks for the suggestion.  Another approach might be to combine this with choosing the data type of the input column/field (since I think we're going to turn off \"guess data types\" by default) and, similar to Excel or OO Calc, have a \"Skip this column\" option in addition to the supported data types.\n. From tfmorris on September 19, 2012 23:18:52:\nThanks for the patch.  Patch applied in r2566.\n. From runuppat...@gmail.com on September 19, 2012 22:09:29:\nWell, installing Google Chrome Frame plugin for IE fixes the problem, but I'm not sure if, from the Google Refine developers point of view, this configuration has to be considered mandatory or just a workaround.\nYou might just mention it in the installation doc.\n. From tfmorris on September 19, 2012 22:52:49:\nDid you not get prompted to install the Chrome Frame plugin the first time Google Refine opened in IE?  That's what happened with my IE 9 installation.\n. From runuppat...@gmail.com on September 20, 2012 07:17:07:\nSure it prompted, but I didn't install it on the first time because I didn't find anywhere a statement saying I should install a Google Chrome related software upon Internet Explorer to make Refine work.\nI think the shown message is misleading because tells about how to improve your web experience with Chrome technology and not that you actually need it to use the page.\nMaybe this is obvious for the developer who added the tag, but not for the common user.\nSecondly I don't see the point in the Google strategy piggybacking Internet Explorer deficiencies with Chrome technology.\nI looked at the installation directories of Chrome Frame and it seems that the occupation is roughly equivalent to a standard Chrome installation, and this probably means that you when installing Chrome Frame are actually installing Chrome, even though not fully enabled.\nFrom the Chrome Frame FAQ I understand that it slips in replacing the rendering and the js engines when directed by a tag.\nI think among many other people, especially web developers, that IE is inferior compared to the other browsers, and in fact I don't use it, but if the IE support means using Chrome in the shadow, well, why should I use IE at all?\n. From tfmorris on September 20, 2012 15:34:22:\nOK, we'll look at making it clearer to the user that the Chrome Frame plugin is mandatory for Refine. It's also possible that IE 9 has improved enough over IE 7 & 8 that the Chrome Frame isn't really needed for that release.\nThe reason for not just telling people to use a different browser is that in many corporate environments users don't have a choice.  The use of Chrome Frame is a low cost way for the Refine team to support these users without having to implement extensive workarounds.\n. Chrome Frame is being retired, so I think we can consider the web improved enough to make this moot.\n. From tfmorris on September 22, 2012 16:02:59:\nFixed in r2568.  This was actually already supposed to be supported using the decompressor from the Apache Ant Tools (so no need for Commons Compress), but we weren't stripping the \"BZ\" prefix that gets appended by most command line tools.\n. From tfmorris on September 27, 2012 20:09:46:\nIt's probably something trivial, so we'll see if we can get it in 2.6\n. From tfmorris on September 22, 2012 18:10:28:\nFixed in r2569.\n. From tfmorris on October 12, 2012 19:43:44:\nFixed in r2577\n. From tfmorris on September 27, 2012 20:07:31:\nI can't reproduce this in my installation, but it sounds like it potentially could be related to issue #620.\n. From tfmorris on October 13, 2012 15:47:02:\nThad - is this reproduceable for you?  I want to start closing out all Milestone 2.6 bugs, so it'd help to know if this is something we need to track down.\n. @thadguidry Can you reproduce this?\n. From cso...@gmail.com on September 26, 2012 14:43:56:\nWhy was this closed?? I don't see tfmorris' comment. HELP.\nMike\n. From tfmorris on September 26, 2012 14:47:27:\nI'm not sure how much clearer \nStatus: Duplicate\nMergedInto: issue #591 \ncan be made.  Please continue the discussion in that issue.\n. From cso...@gmail.com on September 26, 2012 17:45:33:\nNow, I got it. Trying the rec from 622.\nMike Corso\nCoolSite.com\n. From thadguidry on September 27, 2012 13:23:00:\nSeems to work just fine for me in Google Chrome latest on WinXP 32bit using latest trunk version r2569.\nI created a new NameBranchMerge column for you to continue with in the attached project.\nDoes your terminal or console report any Java errors of any kind when performing that GREL expression ?  What browser version are you using ?\n. From tfmorris on September 27, 2012 20:03:42:\nThanks for providing the data to reproduce the problem.  That's a big help.  Don't forget to provide versions for Refine and your browser too.\nI can confirm that the problem exists in Refine 2.5 and is fixed in the current development sources, I'm going to mark this as fixed.  Please retest when the 2.6 beta is released.\nIf I had to guess, I'd say the problem has something to do with escaping of HTML entities or tags (it as embedded  tags, but since it seems to work now, I'm not going to spend time figuring it out.\n. From tfmorris on September 28, 2012 22:34:06:\nPlease ask questions on the mailing list.  This system is for bug reports and enhancement requests.\nThe class Row is the one which represents a row.\n. The original thread is now at https://groups.google.com/forum/?fromgroups=#!topic/openrefine/pimNnOExyjU with the renaming of the Google Group.  \nTo repeat the workaround here, you can use diff(\"10/12/2012\".toDate(),\"10/14/2012\", \"days\").  In other words, \nmake the second argument a string instead of a date. \n. This is a duplicate of #594 which was fixed a year ago.\n. ## Copying the code submitted by mihili from Google Code so that we don't lose it.  \n==== File name: main/src/com/google/refine/browsing/filters/ExpressionEqualRowFilter.java    ====\nIn the method: public boolean internalFilterRow(Project project, int rowIndex, Row row)\n[Change 1] Replace the content of:\nif (value.getClass().isArray()) {...}\nso that it looks like this:\n```\n        if (value.getClass().isArray()) {\n            Object[] a = (Object[]) value;\n        // call the new method\n        return testValueAnded(a);\n    }\n\n```\n[Change 2] In the same file, insert the method below:\n```\nprotected boolean testValueAnded(Object[] a) {\nboolean result = false;\n\n// for each facet...\nfor (Object match : _matches) {\n    // ...we want to have a match with at least one subitem of the field\n    // let's go through all the subitems\n    result = false;\n    for (Object v : a) {\n        if (ExpressionUtils.isError(v)) {\n            return _selectError;\n        } else if (ExpressionUtils.isNonBlankData(v)) {\n            // at the first positive, we are done with the facet\n            if (testValue(v, match)) {\n                result = true;\n                break;\n            }\n        } else {\n            return _selectBlank;\n        }\n    }\n    // if we are here and we didn't get at least a positive, then this is a negative\n    if(!result)\n        return false;\n}\n\nreturn true;\n\n}\n```\n==== File name: main/src/com/google/refine/browsing/Engine.java ====\n[Change 3] Make the following changes:\n```\n            //FilteredRows filteredRows = getFilteredRows(facet);\n            FilteredRows filteredRows = getFilteredRows(null);\n        //FilteredRecords filteredRecords = getFilteredRecords(facet);\n        FilteredRecords filteredRecords = getFilteredRecords(null);\n\n```\n. Reported as https://code.google.com/p/simile-butterfly/issues/detail?id=10 but since Butterfly is unmaintained, we'll probably need to fix it ourselves.\nPushing to post-2.6 since it's a low frequency event and doesn't endanger any data.\n. Sounds like it could be related to the deadlock scenario described in this article about Java 7 multithreaded class loader improvements: http://docs.oracle.com/javase/7/docs/technotes/guides/lang/cl-mt.html \n. From s...@google.com on October 03, 2012 22:33:54:\nThis needs to be backward compatible with custom suggest services. I have deleted the patch for now, I'll upload a new one when I have that working.\n. From s...@google.com on October 08, 2012 20:53:49:\nOk, here's the patch file. The suggest widget is now backwards compatible with custom suggest services that emulate the old suggest API.\n. From tfmorris on October 08, 2012 22:44:08:\nThanks for the patch.  Is this stock Freebase Suggest 4.0 as served from gstatic.com or has it been modified in some way?\nIt looks like we're currently running something 1.2.1 based which is pretty out of date considering how long 3.1 was available before 4.0.\n. From s...@google.com on October 08, 2012 22:57:25:\nIt is the stock suggest as served from the links below, no modifications whatsoever. All the customization is done in custom-suggest.js\nhttps://www.gstatic.com/freebase/suggest/4_0/suggest.min.css\nhttps://www.gstatic.com/freebase/suggest/4_0/suggest.min.js\n. From tfmorris on October 12, 2012 17:06:00:\nIt appears that the new version of the Suggest widget no longer returns Freebase object types for matches which we're currently referencing.  \nhttp://code.google.com/p/google-refine/source/browse/trunk/main/webapp/modules/core/scripts/views/data-table/cell-ui.js#333\nI'll need to dig further to see if we actually use this information.\nHas this patch been deployed internally at Google?\n. From tfmorris on October 12, 2012 18:53:58:\nFixed in r2575.  Thanks for the patch!\nThe last remaining problem was an incompatible change in the returned data from Suggest.  They changed \"n:type\" to \"notable\" (and \"r:score\" to \"score\", but we don't use that).\n. We've actually switched to http://reconcile.freebaseapps.com instead of standard-reconcile.freebaseapps.com as part of the change in backend reconciliation services\n. I understand.  Staying with the current service isn't an option.  It is\ngoing away.  If the backend services that we are currently using don't\nimprove (which I'm confident they will, by the way), we'll switch to using\nthe Search API.\n. From tfmorris on October 12, 2012 22:41:41:\nFixed in r2581\n. The first pass of changes is complete.  The main work which remains to be done is incorporate the new logo when it's available and test things related to kit building.\n. The new logo is incorporated and we've updated all (hopefully) the names and links.\n. As a workaround, you can split this into two separate \"Add column from Freebase\" operations:\n1. On your reconciled city column, add the Population using the constraint.  Because it's a CVT with no name, you'll get entire column of hyperlinks with the label \"null\" but they'll all be linked to the correct Freebase topic.\n2. On the newly added column, use Add from Freebase again, but this time select the \"number\" property of the CVT.  It might be useful to add the Year too, so you know what date the population is associated with.\n. We need to add /type/content to the blacklist too so that we don't pick up captioned photos that match search.\n. Updated to Clojure 1.4.\n. Thanks for the patch.  I noticed this the other day and hadn't gotten around to investigating yet.  I've also noticed that Freebase reconciliation doesn't allow the selection of topics which don't have the expected type (ie if you're reconciling against /book/author you can't select a topic which only has /common/topic and not /book/author).  Does your patch fix that as well or is that a separate problem?\n. Never mind.  It was an entirely separate problem with Search/Suggest not always returning notable types and the Javascript crashing when trying to access them.  It's covered in #636 (and fixed).\n. Can you provide the exact text of the error message so we can see if it's coming from our code or somewhere else?  Does it appear in the terminal attached to the Refine server or in your browser?\nAs for your questions, please use the mailing list for questions, not the issue reporting system.  That sounds like a very large number of clusters.  You definitely don't want to be using a text facet with that many entries (use Edit Cells->Cluster & Edit), but let's continue the discussion on the mailing list.\n. Thanks for the additional detail Brian - particularly the console messages.  We'll track down where the error is coming from and, at a minimum, get it documented.\nThe fingerprint(value) or value.fingreprint() should give you the same results as the clustering, so you're safe on that front.\n. Jetty apparently does limit the size of POSTed forms by default: http://wiki.eclipse.org/Jetty/Howto/Configure_Form_Size\nAlso we don't really handle large numbers of things in the UI very well.  Too much stuff has a tendency to choke the browsers. See issue #695 \n. Please use the mailing list for questions.  You can use \"Split multi-valued cells\" to split these out and then cluster them.\n. @kiminoa I'm not sure whether you are requesting detection of transliterations or just making sure it doesn't inappropriately try to detect a transliteration as a language.  We can do the latter with a confidence threshold, but the language models are all pre-built for a fixed set of languages.\n. @kiminoa I'm pretty sure Apache's implementation is different from the Chrome implementation.  We'll try to make sure that the confidence thresholds get set appropriately, but I think Linear B transliterations might be considered an edge case.\n@magdmartin That page is available in the current wiki as well \n. Thanks for catching that.  Once we get DNS sorted out, the link will actually be to http://openrefine.org\n. I've merged #666 instead removing the FileCleaningTracker altogether.  Files are now deleted explicitly.\n. Yes, but we don't provide free, guaranteed response time support, if that's your assumption.  Bugs get triaged as developers have time to do it.\nI've confirmed that the bug still exists in the current development version.\n. Currently we use a homegrown table to remove accents from characters.  It has a number of limitations: 1) it only deals with Latin-1 Extended and Latin-1 Supplement code charts and 2) it only deals with single accented characters, not ligatures.\nThe best solution is probably to switch to Java's text Normalizer and normalize the text to NFKD form, then throw away the characters which are in the Unicode combining character class (diacritics, basically).\nIf someone would like to do the Unicode/Java research to confirm that this is the best approach, it should be pretty easy to implement.\n. So I've looked into this a little bit further and I can't find anyone who splits the character \u00e6 (and Unicode calls it a character, not a ligature).  As a matter of fact, this search engine mapping table (which is doing the same thing that we're trying to do) explicitly maps other things like the Cyrrilic ae ligature to this character. http://sphinxsearch.com/wiki/doku.php?id=charset_tables#external_resources\nCan you provide some references for text processing algorithms which handle this differently?\n. Thank you for the additional info.  I guess I'm confused as to the correct course of action here.  Even though it's considered a valid letter (and apparently not a decomposable one according to Unicode), do you still think the best course of action is to split it into the two character sequence \"ae\"?\nI'm not a purist and am happy to do something which is pragmatically useful, but don't want to create a worse problem than I'm solving.\n. A lot of places in Refine treat cells which are null, contain the empty string, and contain a string of whitespace equivalently, although I'm not sure such a loose interpretation is always desirable.\nAnyone have opinions on whether we should change the behavior or the description here?\n. Not sure why this didn't get closed automatically by my commit message.\n. A similar exception emanating from the timer thread:\nException in thread \"autosave\" java.util.ConcurrentModificationException\n        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:806)\n        at java.util.HashMap$EntryIterator.next(HashMap.java:847)\n        at java.util.HashMap$EntryIterator.next(HashMap.java:845)\n        at com.google.refine.InterProjectModel.flushJoinsInvolvingProject(InterProjectModel.java:109)\n        at com.google.refine.model.Project.dispose(Project.java:109)\n        at com.google.refine.ProjectManager.saveProjects(ProjectManager.java:265)\n        at com.google.refine.ProjectManager.save(ProjectManager.java:206)\n        at com.google.refine.RefineServlet$AutoSaveTimerTask.run(RefineServlet.java:93)\n        at java.util.TimerThread.mainLoop(Timer.java:555)\n        at java.util.TimerThread.run(Timer.java:505)\n. Just got this with the latest sources, so the fix is apparently inadequate:\n15:53:20.375 [ importing] Disposed 1368989413682 (10ms)\nException in thread \"autosave\" java.util.ConcurrentModificationException\nat java.util.HashMap$HashIterator.nextEntry(HashMap.java:806)\nat java.util.HashMap$EntryIterator.next(HashMap.java:847)\nat java.util.HashMap$EntryIterator.next(HashMap.java:845)\nat com.google.refine.InterProjectModel.flushJoinsInvolvingProject(InterProjectModel.java:112)\nat com.google.refine.model.Project.dispose(Project.java:109)\nat com.google.refine.ProjectManager.saveProjects(ProjectManager.java:265)\nat com.google.refine.ProjectManager.save(ProjectManager.java:206)\nat com.google.refine.RefineServlet$AutoSaveTimerTask.run(RefineServlet.java:93)\nat java.util.TimerThread.mainLoop(Timer.java:555)\nat java.util.TimerThread.run(Timer.java:505)\n. I switched the default from NUL to DEL, but I'm not sure that ESC, BEL, or some other unusual control character wouldn't be better.\n. In looking at this again I concluded that OpenCSV wasn't behaving as documented where it says that using NUL for any of the escape or quote characters disables that processing.  I fixed OpenCSV and reverted the change that I made to Refine.\n. If you create a pull request with your solution, we'll make sure it gets merged in.\n. Here's a readable version of the javascript:\nfunction () {\n    $.getJSON(\"/command/core/get-operations?\" + $.param({\n        project: theProject.id\n    }), null, function (data) {\n        if (\"entries\" in data) {\n            var a = [];\n            for (var i = 0; i < data.entries.length; i++) {\n                var entry = data.entries[i];\n                if (\"operation\" in entry) {\n                    a.push(entry.operation);\n                }\n            }\n            Refine.postCoreProcess(\"undo-redo\", {\n                lastDoneID: 0\n            }, null, {\n                everythingChanged: true\n            }, {\n                onDone: function (od) {\n                    Refine.postCoreProcess(\"apply-operations\", {}, {\n                        operations: JSON.stringify(a)\n                    }, {\n                        everythingChanged: true\n                    }, {\n                        onDone: function (o) {\n                            if (o.code == \"pending\") {\n                                Refine.update({\n                                    everythingChanged: true\n                                });\n                            }\n                        }\n                    })\n                }\n            });\n        }\n    }, \"jsonp\");\n})();\n. Reopening. The VIBS extension is either incompatibly licensed (they say GPL) or closed source (I don't see any evidence that they actually publish sources), so it's not an appropriate solution for all OpenRefine customers.\n. Thanks for the clarification about the VIBS extension.\n. Pretty-printed version (may not be 100% accurate):\n$.getJSON(\"/command/core/get-all-project-metadata\", null, function (data) {\n        if (\"projects\" in data) {\n            var frame = DialogSystem.createDialog();\n            frame.width(\"500px\");\n            var header = $('<div></div>').addClass(\"dialog-header\").text(\"Select project\").appendTo(frame);\n            var body = $('<div></div>').addClass(\"dialog-body\").appendTo(frame);\n            var footer = $('<div></div>').addClass(\"dialog-footer\").appendTo(frame);\n            $('<p></p>').text(\"Select project to use history from:\").appendTo(body);\n            var sel = $('<select />').appendTo($('<p></p>').appendTo(body));\n            for (var project in data.projects) {\n                $('<option>').val(project).text(data.projects[project].name).appendTo(sel);\n            }\n            var selSorted = sel.find(\"option\").toArray().sort(function (a, b) {\n                return (a.innerHTML.toLowerCase() > b.innerHTML.toLowerCase()) ? 1 : -1;\n            });\n            sel.empty();\n            $.each(selSorted, function (key, val) {\n                sel.append(val);\n            });\n            $('<button class=\"button\"></button>').text(\"Cancel\").click(function () {\n                DialogSystem.dismissUntil(level - 1);\n            }).appendTo(footer);\n            $('<button class=\"button\"></button>').text(\"OK\").click(function () {\n                $.getJSON(\"/command/core/get-operations?\" + $.param({\n                    project: $(sel, \"option[selected]\").val()\n                }), null, function (data) {\n                    if (\"entries\" in data) {\n                        var a = [];\n                        for (var i = 0; i < data.entries.length; i++) {\n                            var entry = data.entries[i];\n                            if (\"operation\" in entry) {\n                                a.push(entry.operation);\n                            }\n                        }\n                        Refine.postCoreProcess(\"apply-operations\", {}, {\n                            operations: JSON.stringify(a)\n                        }, {\n                            everythingChanged: true\n                        }, {\n                            onDone: function (o) {\n                                if (o.code == \"pending\") {\n                                    Refine.update({\n                                        everythingChanged: true\n                                    });\n                                }\n                            }\n                        });\n                    }\n                }, \"jsonp\");\n                DialogSystem.dismissUntil(level - 1);\n            }).appendTo(footer);\n            var level = DialogSystem.showDialog(frame);\n        }\n    })\n})();\n. OK, I've updated the pretty printed version to match.  If you insert things using the Markdown code block markup (four backticks) you shouldn't have to worry about escaping things or your code being reformatted.\nI'm not really expecting the code to be reused.  I just want it to be readable to support your textual description of the enhancement request.\n. This should potentially include any additional metadata from #1045 as well, especially description\n. Thanks for the pull request.  We've got our own fork of OpenCSV at https://github.com/OpenRefine/opencsv where we'll be apply the changes from that patch, but it's currently stalled because it bundles multiple things together and I haven't had time to untangle them.\n. Thanks for the bug report and the fix.  I've applied the fix.  Please retest when the next release comes out.\n. I think a new tutorial would be great, but that's really above and beyond\nthe work required for the migration.  Feel free to tackle it if you want\nthough.\nOne candidate API might be Twitter.  It's JSON, fairly simple, and pretty\npopular.  I don't think the example needs to be as elaborate as the current\none, although more than one step might be good (e.g. getting your followers\nand then counting the followers of the followers).  Bonus points using a\nsecond API for the followup step. :-)\n. This has been done for a while.  Thanks for everyone's help!\n. @magdmartin If this is intended to show up at openrefine.org, it needs to go in a different repo.  The one you want is https://github.com/OpenRefine/openrefine.github.com\n. This has changed considerably since the 2.5 release.  We've made quote removal optional for all importers and change the default so that values are left unchanged rather than being parsed.  I'll need to go back through and see if this still needs to be modified in some way.\n. How does this relate to pull request #644?\n. Can you describe what problem this is fixing?  Is this intended to allow multiple import jobs at the same time or something else?\n. Multiple concurrent jobs isn't a use case which is supported or anywhere near the current design center for Refine.  There's certainly nothing to prevent us from moving in that direction if there's sufficient demand, but you should expect to find more stuff which doesn't work in this environment.\n. It would be helpful to put independent pieces in separate pull requests.  I've taken a different approach with this, but hopefully it will provide you with equivalent functionality.\n1. The jobs table ImportingManager has been switched to a ConcurrentHashMap.  This doesn't throw ConcurrentModificationExceptions and should be more appropriate for this usage.\n2. ImportingJob has been refactored to central most access to the config object rather than allowing external consumers to know its internal format.  A number of new ImportingJob methods were created which provide synchronized access to setting and getting fields in this object.\n. Thanks!\n. What was the type of the source file? ie was it plain text or in some type of zip or archive? Local or URL?\nThere were a few problems fixed with various things in this area. See #587, #544, #599 (found by searching for import-temp in bug database)\nDoes %TEMP% point somewhere reasonable?  Do you have write access to it?\n. Thanks for following up to let us know.  Feel free to send email to the list if you have any more questions.\n. If you'd like to make this a pull request, we can merge it right in for you.\n. Jesus and Martin - thanks for taking this.\nJesus -  I know it seems like a lot of work for such a trivial fix, but we're trying to build a community and infrastructure which scales easily to greater things.\n. Pushing to post-2.6\n. I've done some optimization work on this (and would appreciate additional help from any jQuery pros), but have come to the conclusion that a fundamentally different approach is required.  It's been a topic of interest with a couple different clients, but hasn't reached the threshold of pain to actually get implemented, but I've got an approach in mind that I'll implement when I've got a few spare cycles.\n. I haven't had a chance to review the patch yet, but I'll try to get to it\nsoon.\n. I've merged your pull request including the XOR operation.  Thanks for the contribution!\n. It's actually MacOS which is damaged.  Please see #590 (or the various blog posts around the web when Apple broke this with Mountain Lion 10.8).\n. I believe the quota exceeded message is actually a weird way of saying that the API has been retired.  Closing as a duplicate of #539 \n. Is the browser window still responsive?  If so, it sounds like the Refine server is still running, but there was perhaps an unhandled error in the Javascript code running in the browser.\nThe other possibility is that there's an enormous \"record\" (which probably isn't a real record, but Refine getting confused) which lies off the first page, but is displayed when you use your filter or increase the page size.  If this is the case, switching from the default Record Mode to Row Mode should make the problem go away.\nIf it's neither of those things, let us know what else you find out and we can look for some other possibilities...\n. To explain a little bit more about what's happening, Refine interprets indented table rows as being part of a \"record\" where the master is the previous row that had the leftmost cell filled.  If you're data has missing entries in the first column, but all the rows are independent, you can rearrage the columns so that a more fully populated column is on the left.  Switching to Row Mode works too, but only if the first screen isn't effected (because then you run into the problem before you have a chance to switch modes).\n. On Fri, Feb 15, 2013 at 9:51 AM, Ben Welsh notifications@github.com wrote:\n\nI work on a small team of data journalists at the L.A. Times. It would be\nhelpful, I think, to have one version of Refine on a dedicated machine that\nis hosted on our intranet. Before I try to wire it up to Apache, could you\nplease let me know if this a totally stupid idea.\nPlease use the mailing list for questions, not the bug tracker.\n\nPeople have set up Refine in the configuration you describe, but you'll\nneed to manage security and coordination entirely on your own.  Anyone who\ncan access the server will be able to see all and edit all projects stored\nthere.  If two people edit the same project, they will potentially\noverwrite each other's work.\nTom\n. The web site is actually stored in a separate repository, so that's the best place to report problems (or submit pull requests with fixes), but here is OKtoo.  I've just turned (back) on issues for the web site repo and transferred this over to https://github.com/OpenRefine/openrefine.github.com/issues/3\n. An initial cut is live at http://reconcile.freebaseapps.com/reconcile.\n. Fixed by commit https://github.com/OpenRefine/OpenRefine/commit/7b3379afc70d2836b294c7865c870e3c0ab3fe47\n. Fixed by 4a5d3d4\n. Fixed by commit a2711e4f59e7fc49d3d0a814e404ae31acf39ea7\n. Yes, I just noticed that yesterday and have a fix, but it hasn't been\npushed to Github yet.  In the process I discovered that we don't properly\nreport any Python errors, so that'll improve too.\n. I'm going to call this good enough for the small snippets of Python that most people will use.\n. Most Freebase references, including the query based reconciliation have been fixed.  The latest version of Suggest removed other dependencies on the old infrastructure.\nThe main thing remaining to be done ia a replacement for TopicBlocks in view/data-table/cell-ui.js (perhaps using the code in Suggest).\nOther miscellaneous stuff:\nRefineBroker (unused)\nFreebase extension:\n schema-alignment search\n Refinery stuff\n. I've added a replacement for Freebase Topic Blocks based on the flyout that Freebase Suggest creates for its use.  We repurpose it for our own uses.\n. I'm declaring victory on this one.  If anyone finds something missing, please open a new issue.\n. That service definitely no longer works.  Please make a backup of your workspace directory and give the OpenRefine 2.6 beta 1 release a try.\nhttps://github.com/OpenRefine/OpenRefine/releases\n. Sounds like an upgrade issue.  We should have removed the old recon service since it's broken (ie it's the service itself that doesn't work).  If you don't have it listed, you can add the new service using the endpoint: http://reconcile.freebaseapps.com.\nCan you open an new issue for the upgrade issue and provide a copy of your list of reconciliation services?  Thanks!\n. Let's focus this issue on status reporting.  I've split the performance problem to #699.\n. This affects all tree shaped importers (JSON & XML).  The record processing logic uses a temporary object to hold the record before copying it to the project, but it was adding a row for every row imported so far, then scanning them all to find them empty, copying the valid row(s) and deallocating the entire structure making the whole process O(n^2) -- with a high constant as well because the lists were all linked lists instead of ArrayLists.\nThe new implementation lowers the import time for the example above (330K records, 123 MB) from 58 minutes to under 15 seconds ie a 200-250x speedup.\n. Which test is failing? (Don't have a development environment convenient right now)  The performance problem is pretty severe, so I'm not sure just abandoning the attempt to fix it is a solution.\n. Thanks for sticking with this, despite the lack of feedback.  I've been buried on another project.\nThe server tests should all pass (and do on my system).  If there's a problem that is JVM or environment specific (or just random), we should fix it, but I don't have time to look into it in detail right now.\nWhat test is failing?  If it's crashing, can you provide a stack trace?\n. The defaults for the 2.6 release are Guess datatypes - OFF and Preserve Empty Cells - ON.  Importing with the defaults yields:\n\nThis is still going to require some manual cleanup, but provides a better starting point (and there are no lossy transformations).\nI don't know what your starting point is, but most browsers copy tables to the clipboard in TSV format which Refine handles easily.  Alternatively, you can use parseHtml().select('tr td') (or whatever selector you want) to customize handling of the data.\nThe handling won't improve much using the current XML parser.  We'd need to layer additional HTML semantics on top (or implement a separate HTML parser).  You can leave this open as a request for a full HTML parser if you want, but the situation should be much improved when 2.6 is out.\n. p.s. you can recover the table structure by a) moving or deleting the headers and b) using the Transpose command to unwrap every 3 lines into columns.\n. Fixed by d1b2dc3.\n. This is a duplicate of #612.\n. I think you should be able to reproduce it by just clicking on the different datatypes in the import preview dialog.  In this case, it looks like I clicked PC Axis, then XML, then OpenOffice, then RDF, then Excel, probably with a CSV file or something similar loaded.\n. What exception are you getting?  What version are you using? (OpenRefine implies that you're building yourself from the source repository rather than using Google Refine 2.5)\n. Literals in N-Triples are required to be printable ASCII http://www.w3.org/TR/rdf-testcases/#ntrip_strings so that isn't a valid triple.  It needs to be escaped.\nWe should give a better error message, but unfortunately a) there's a bug in the JRDF error reporting and b) it's an inactive project so it's unlikely to get fixed.  We should probably switch to Jena for our RDF parsing, but I don't know how much work that'll be.\n. I'm reopening this because the error reporting really needs to be improved, but, as I said above, it may require switching to a different RDF library, so it may not happen immediately.\n. Well, if they put them in a list, we'd treat it as a list.   It's a little weird because they keys aren't really adding any value since they're duplicating the same data which is stored under the \"id\" key.\nThis may be an attempt on MapIt's part to avoid the security issue with top level arrays: http://flask.pocoo.org/docs/security/#json-security\nInstead of returning {1:{foo object}, 2: {bar object}} if they'd chose instead to return {'items':[{foo obj},{bar obj}] this would all work seamlessly, but that's probably all water under the bridge.\nWe'll see what we can do to handle cases like this.  It's relatively rare, but it's not the first time I've seen it.\n. What platform (operating system) and what version of Chrome?  What page?\nChrome is my default browser and I just checked the home page at openrefine.org and it looked fine to me.\n. I'm also running 26.0.1410.64 m and everything renders fine here.  I think\nthere would be quite an outcry if this were a general problem, so I suspect\nit's something specific to your browser installation and/or settings, but\nI'm not sure exactly what.  If you use any browser extensions, you might\ntry disabling them to see if that helps.\nOn Fri, Apr 26, 2013 at 11:33 AM, prairieskygal notifications@github.comwrote:\n\nI'm currently using Windows 7 and Version 26.0.1410.64 m of Chrome.\nEvery page was rendering this way, but just the paragraph text. Not the\nheaders.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/711#issuecomment-17081693\n.\n. @sckucera I'm not seeing the connection to OpenRefine.  As an aside, Microsoft stopped supporting Windows XP in 2009, so it's unlikely you'll get much help with it.\n\n@prairieskygal I'm assuming the lack of response means you were able to get your problem sorted out.  Feel free to reopen this if that's not the case.\n. The operation merges rows with the same non-key/value column cell values, apparently intentionally.  I'm not entirely convinced this is ever desirable, but certainly in the presence of records, it would make sense to fold the rows of the record into a single row.\nWith the current scheme a sequence like this:\nID KEY VAL   FIRST LAST\nid1 key1 val1 John Smith\n    key2 val2\n    key3 val3\nwill probably end up as\nID  FIRST LAST   KEY1 KEY2 KEY3\nid1 John Smith val1\n                      val2 val3\nparticularly in this case when the \"blank\" cells contain random whitespace left over from the XML parse.\n. What format is the spreadsheet in? (e.g. Excel 97, OpenOffice, etc) What\nversion of Refine are you using?\nIf you can put the spreadsheet somewhere accessible (or email it to me on\ngmail - same name), I'll take a look.\nOn Mon, Apr 29, 2013 at 9:22 PM, biofool notifications@github.com wrote:\n\nI have a spreadsheet that has a couple thousadn rows spread out over maybe\n20 worksheets. If I just try to import one or two it works fine but if I\ntry to import all of them the import always hangs (twice I let it run for\nover an hour).\nI can provide the spreadsheet. Also I used some simple perl code (yeah I'm\nthat old :^) to convert the mutli worksheet thing into a single worksheet\nand that imports into refine just fine.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/713\n.\n. Need the spreadsheet to be able to debug this.\n. I'm closing this due to lack of data to reproduce the problem.  Feel free to reopen it if you can provide the spreadsheet.\n. There are two things that you can do with the current version:\n- Use one of the operations from the common transformations menu to change\n  the data to lower/upper/title case\n- Use a custom text facet with the expression value.toLowercase() (for\n  example) to force all the values to the desired case when computing facet\n  values\n\nOn Wed, May 1, 2013 at 10:50 AM, KartikaGarg notifications@github.comwrote:\n\nHi,\nI used google-refine to clean up some messy data . Its a very good tool\nand quite easy to use. Since I did not have a lot of time, My data had a\nlot of entries which differed only in case. I would have liked GARY and\ngary to be counted under one , when text facet was used. However I had to\nmanually add up the counts.\nAdding a global property which says case sensitive or insensitive would\nhelp in this case.\nThanks in advance\nKartika Garg\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/714\n.\n. Thanks for letting us know.  Glad to hear you found a solution.\n\nJust to clear up a possible misconception though - if you use a custom text facet, you can transform the data just for the facet without having to change it in the spreadsheet itself.  So, for example, you could make the facet expression value.toTitlecase() or value.toLowercase().contains('hyd') or any other expression that you want, which will transform the values for the purposes of the facet computation, but not effect anything else.\n. This is closely related to #556\n. Duplicate of #652 which I've just reopened\n. The Mac OS installation problem was introduced by Apple in their last OS\nupdate.  You need to temporarily relax the security settings to do the\ninstallation.\n https://github.com/OpenRefine/OpenRefine/issues/590\nOn Wed, May 22, 2013 at 9:48 AM, jmccu notifications@github.com wrote:\n\nRunning OS 10.8.3. It will not let me download. Yesterday I could but it\nrefused to upload any data. So I trashed the application and re-uploaded.\nNow it says that it is corrupted and I have to move it to trash. wth?\nReally, really need this. Any suggestions?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/719\n.\n. Duplicate of #590\n. It's not really the size of the data set, but the size of the resulting HTML table which is the issue.  Usually it's a combination of a bad import creating weird records and the front end attempting to display them without clipping.  The paging in the UI is intended to prevent this problem, but it breaks down in records mode (vs rows mode) because it's assumed that records are a relatively small multiple of rows.\n. Continuous Integration is set up on Travis CI now.  Tests are run after every commit and the test status is visible at the top of the README on the project page.\n. Sorry to hear that you are having problems.  You can cut & paste the error message here.  Without seeing it, it's hard to guess what the problem might be.  The N-gram fingerprint operates on the column where you selected the operation from the menu.\n. This may be an interaction between the default Record mode display and the fact that sort, I think, works solely on rows.  If you switch your display to Row mode, does the final sort match what you see on the screen?\n\nCan you provide a simple example with a few rows/columns and a step-by-step to make sure that we can reproduce the behavior that you're seeing?\n. Thanks.  When you describe the steps to reproduce, please use a few sample data rows (even if it's only dummy data).  I'm having a hard time visualizing what the (a)s and (b)s contain/represent.\n. Thanks for the suggestion.  Since table extraction from PDF is a complex and relatively unique task, it'd probably be better done using a GUI specifically dedicated to that task.  Of course the results could then be processed in Refine.\n. Tabula is a promising looking project for those interested in this: https://github.com/jazzido/tabula\nApache PDFbox is much too level an API to be useful to Refine.  We'd have to implement a table extraction algorithm on top of it.\n. I think there are three different things here:\n1. multiple workspaces per user\n2. multiple users\n3. multiple tenants (each with their own set of independent users)\nWe could improve organization, even within a single workspace, through the use of tags and associated filters for tags, names, words in project descriptions, etc.\n@Downchuck could you expand on the project set import/export use case?  I'm not sure I understand how this is related to using cross()\n. It doesn't appear to be documented, but you can specify the refine data directory by adding something like\nJAVA_OPTIONS=-Drefine.data_dir=c:\\users\\tfmorris\\AppData\\Roaming\\OpenRefine\nto your refine.ini file  By changing the refine.data_dir parameter, you can have multiple directories, but limited to one per Refine server.  (You could open different servers on different ports though).\n. Thanks for the pull request (and thanks to @thadguidry for reviewing).  I'm just slightly hesitant to add XOR since it can be created from AND, OR, and NOT, but it's more straightforward to have it as a convenience to users.\nI'm going to change the implementation slightly to: a) use autoboxing and b) use Java's XOR operator\nFor future pull requests, I'd like to see test coverage too (I know we aren't very good about that ourselves, but we need to improve the test coverage).\n. OK.  I've added test coverage for all the boolean functions and include the new XOR function in it.\nThe new implementation is simply\nreturn (Boolean) args[0] ^ (Boolean) args[1];\n. You may be able to work around this by setting your locale to one which uses EN as the language in the shell that you start the Refine server from.  The date format that the parser uses gets created using the default locale. http://docs.oracle.com/javase/6/docs/api/java/text/SimpleDateFormat.html#SimpleDateFormat(java.lang.String)  We probably need to extend this to allow the user to specify a locale other than the default so that dates from a different locale can be processed.\n. Which field(s)?  If you've got a patch which incorporates the simple fix, please open a pull request with it.\n. The 2.6 release should be more robust with respect to the way that it saves data to disk.  Documenting how to recover corrupted projects is a pretty open ended task.  If we could foresee the sources of corruption well enough to document them, I'd rather have us put the effort into fixing the bugs rather than documenting how to recover from their effects.\nIncluding the initial import command & options as part of the undo history so that it's a complete record of the project history is a good idea and I think we've got an issue for it already.  Basically it would mean defining a new ImportData operation so that the initial state can be an empty project rather than one which is already populated with the initial data.\n. When you say \"view\" are you talking about an open project with some facets selected or something else?  ie is this meant to represent a filtered subset of the rows?\n. Thanks for tracking this down.  We need to be computing CollationKeys and using CollationKey.compareTo() instead of recomputing the CollationKey each time as is done in RuleBasedCollator.compare()\n. This is fixed by b91fc8a2b16f735109fefaadac52a7e4ccb9194e\n. Hmm, that's not very useful is it!  It looks like the problem may actually be in filter (and forEach), not the isNotNull test because \nfilter([ null, \"this is a string\", null ], v, true)\nand \nforEach([ null, \"this is a string\", null ], v, v)\nfail as well.\n. Record mode is only partially supported in Refine and is generally not used at all by transformations.  Having per record variants of these functions would be useful, but I don't think we want to get rid of the current behavior.\n. Thanks for the patches!  I'll have a look through them.  For future stuff, it'd be great if you could use topic branches with a fix (or set of related fixes) per branch.  That a) allows us to review them individually of each other and b) isolates any other unrelated stuff that you might do on your master branch.\n. I've merged 1c3cfc9 and d1044c9.  \nI've fixed issue #730 with a general change to every place that currently binds keyup to instead bind keyup, change, and input events.\nTwo changes need more work:\n- 9a1ffc1 needs to catch a narrower exception than Exception\n- 371240d needs something different than a hard coded limit.  One possibility would be to have it count rows instead of records in record mode.  Whatever the solution, the user needs to be notified that they are not seeing all the data.\nThanks again for patches.  I'm going to close this, but please open new pull requests if you come up with revised versions of the remaining two patches.\n. There's isn't a suggested media type right now.  Do you think this would be useful?  What type(s) of use case(s) would it support?  The OpenSearch case seems somewhat different to me in that you encounter the links in the same context where you'd use them (ie your generic web browser).\n. Sounds reasonable.  Want to make a proposal by writing something up on our wiki (or here)?\nRiffing off the opensearch mediatype, perhaps something along the lines of \nThe \"type\" attribute must contain the value \"application/openrefinereonciliation+json\".\nThe \"rel\" attribute must contain the value \"reconcile\".\nThe \"href\" attribute must contain a URI that resolves to an OpenRefine reconciliation endpoint description document.\nThe \"title\" attribute may contain a human-readable plain text string describing the reconciliation service\n. This is the same as issue #591.\n. I think we do follow redirects generally (that's the Java default).  The issue appears to be that the Java engineers and browser engineers disagree on what's safe.  Java removed the capability to follow redirects across protocols (e.g. http->https as in this case) because they felt it was a potential security/privacy leak.  Chrome on the other hand happily follows the direct without user intervention.\nSince we don't do authentication, I think the privacy leak concern isn't relevant, so I can add code to manually follow the redirect.\n. Actually, the extensions weren't even getting built because they weren't included in the test build dependencies.  They're being built (with the correct version of Java) now, but there's still no test coverage.\n. Thanks for the pull request.  It looks like there are merge conflicts with the current repository.  Can you update from this repo and resolve the merge conflicts please?  Update the pull request when it's done and I'll merge it in.  Thanks!\n. Thanks @magdmartin.\n@Blakko - if you could add some basic starting documentation for both 1)\ntranslators and 2) developers, that would be great.  We still need to\ntranslate the server side of things as well, of course, so I'll have to\nupdate it with information on that.\nOn Fri, Jul 26, 2013 at 9:55 AM, magdmartin notifications@github.comwrote:\n\nThanks for this contribution Blakko!\nBased on your email I open a wiki page to documenthttps://github.com/OpenRefine/OpenRefine/wiki/Translate-OpenRefinethis new functionality for people willing to translate Refine in other\nlanguage. Feel free to edit it.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/pull/755#issuecomment-21622008\n.\n. OK, so I clearly didn't do a very good job reviewing that.  Serves me right for trying to review a big patch while in a hurry.  The hardcoded paths worked with the development version, but didn't work in the kit build.  I've made the following modifications (mostly in commit f4ff22734071d0d3ecc01451ff96409b72cc6d8d, line endings in 3ff4dac81f7d0dd4dd4400482e6a38b71ed2aa8c)\n- normalized line endings for all files\n- made all file loading relative to module base\n- moved core language files into appropriate place relative to core module root\n- eliminated all SetLanguage commands and now use SetPreference instead\n- eliminated all module LoadLanguage commands except for core's and added module to it as a parameter\n- remove BOM from JSON language files which kept them from parsing\n- fixed duplicate keys in JSON language files which kept them from parsing\n\nI don't have time for an extensive review now, but I suspect we might be able to make better use of common translations.  I know it's tempting to go overboard in this regard, but basic strings like \"Help\", \"Edit\" etc should need only be translated once.  I'll open a separate issue for that.\nAnyone who's tracking this pull request please double check the changes I made.  They're relatively extensive.\n. Thanks for the explanation.  Modularity is good and I appreciate you translating the extensions as examples.  Modules include a dependency graph and all extensions depend on 'core' so they can assume that they core module is always available.  I just extended the LoadLanguage command to include a module name and provided extensibility that way from the core command.\nThanks again for all your work!\n. Sorry to hear about the problem.  If you send me (gmail) the files or put them somewhere accessible, I'll see what I can do to recover the data.\n. This sounds the same as #602 which we, unfortunately, never got clarification on.  The Move Left at the leftmost column sounds like it's the key.\n. This problem is fixed in the current development version and the fix will be included in OpenRefine 2.6.  I fixed it a while ago, but never had confirmation of the exact cause, so couldn't test the fix.  I've tested this scenario and made a small cosmetic improvement so that invalid moves at the beginning/end result in noops instead of errors.\nYou should be able recreate your project by extracting the undo history from the old project, creating a new project with the same input file and project creation options, then applying the undo history to rerun all the recorded operations.\nFeel free to contact me if you need any help with this.\n. Did you mean to make this a pull request?  Github doesn't seem to think that any of the files have changed.  It looks like perhaps it was an internal merge for you going the other direction.\n. OK, thanks.  I thought that might have been the case, but wanted to make sure.\n. Could you try opening a command window and running refine.bat from the\ndirectory where it is installed then copy and paste any error messages so\nwe can get an idea of what's going on?\nOn Aug 1, 2013 3:37 PM, \"jadel041\" notifications@github.com wrote:\n\nUnable to open google-refine.exe or refine.bat in Windows 7.\nWhen I double-click on either, the window flashes open for a second and\nthen disappears. Installation is not successful.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/761\n.\n. @sparkica - The Google Refine 2.5 memory setting was 1024M.  It didn't get increased until after the release, but I've backed it back down from 3000M to 1400M for the 2.6 release based on your comment.\n\n@jadel041 - we need the error messages to be able to tell what the issue is on your system.\n. No response from OP - closing.\n. Fixed by d7531bbbd80a1ad9852c58571a600b8dc9701c85\n. It always included the line break.  The only difference is that the\nseparator is guessed correctly saving the user having to specify it\nmanually.\nOn Sat, Aug 3, 2013 at 12:22 PM, magdmartin notifications@github.comwrote:\n\nWhat is the new behavior now? Does Refine include the line break in the\ncell?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/764#issuecomment-22057233\n.\n. The Freebase reconciliation service has gone away.\n. This was first reported by @thadguidry on the openrefine-dev mailing list.  Thanks for reporting!  Bug fix will be in the next kit.\n. Despite the referenced fix, the revision still isn't getting set, at least on Mac with the 2.6-rc1 kit.  This will need more investigation.\n. This needs at least a minimal fix before the 2.6 release.  One possibility would be to remove the \"TRUNK\" nomenclature and depend on developer discipline to always create useful version numbers.\n. The Freebase Reconciliation service only returns the notable_for info, but I've added a separate MQLread call to our backend service freebaseapps.com to get the type information.  This should work with both 2.5 and 2.6\n. Batch format is described here: https://developers.google.com/coordinate/v1/batch\nEndpoint is https://www.googleapis.com/batch\n. Thanks for reporting Thad.  I had a few free minutes over the weekend, so went ahead and fixed this (see #767).  I should have added you so you'd see it.\n. Facets are applied as part of the \"engine\" processing for every command.  They have no permanent existence in Refine, but are instead part of the viewing experience.  If you click on the \"permalink\" link on the project page, you'll get a URL which encodes all the current facets which are active.\n\nWe don't have an \"export API\" or any other type of supported/documented API, but the JSON describing the facets is passed in our internal client/server protocol and used as part of the export process.\n. The best way is probably to create the facets that you want in the\nbrowsers, click \"Permalink\", and then examine the URL in the browser\naddress bar to see what it contains.\nOn Mon, Aug 5, 2013 at 3:16 PM, Michael Bianco notifications@github.comwrote:\n\nGotcha, so facets are parsed right herehttps://github.com/OpenRefine/OpenRefine/blob/f1387bdb24070b847f9a80be2b68b4c134efefa9/main/src/com/google/refine/browsing/Engine.java#L180.\nIs there any docs on the facets array? Or is the best way to look through\nthe java source? Thanks\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/772#issuecomment-22131778\n.\n. This was caused by the fix for #290.  I've modified the fix to address the problem.\n. The Freebase reconciliation service is going away.  Unless this is replicated with another reconciliation service, we're not going to bother fixing it.\n. Migration will be automatic.\nOn Aug 8, 2013 9:29 PM, \"magdmartin\" notifications@github.com wrote:\nShould we add some doc for user to migrate their existing project then?\nOn 2013-08-08 6:41 PM, \"Tom Morris\" notifications@github.com wrote:\n\nOne last bit of branding cleanup. It's not strictly necessary, but it'll\ncause less confusion in the future.\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/OpenRefine/OpenRefine/issues/777>\n.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/777#issuecomment-22370383\n.\n. Here's a link to the specification: http://www.dataprotocols.org/en/latest/simple-data-format.html\n. Thanks for finding (and fixing) this!  As far as I can tell, this should fix #781 as well.\n. Looks like this got broken by the patch to support relative paths for the servlet 03d997be8ef8a58cef13f92e18a61cf0be5a4d92 (one of the problems with having releases so infrequently is that things affecting the kit build don't show up when they get broken)\n. Is this with the 2.6-alpha1 kit or something else?  The reference to LODRefine makes me wonder whether it was a private build or just a repackaging of the 2.6-alpha1 kit.  I'm not sure either is a good idea to distribute to customers at this stage, but if it's a private build, I've seen problems like this if you build on Java 7 and attempt to run on Java 6.\n. The 2.6-alpha1 kit on Mac has the same problem as all the other platforms in that it has the extension directory set wrong, so I don't really understand how it even got as far as finding file:/Applications/OpenRefine.app/Contents/Resource/webapp/extensions/gdata/module/MOD-INF/controller.js  I'll have to look closer at how this is wired up.\n\nThe talk about Java versions, changing build parameters, etc makes this sound like a private build.  Just to triple check when I say \"2.6-alpha1 kit,\" I'm talking about this binary build: https://github.com/OpenRefine/OpenRefine/releases/download/2.6-alpha1/openrefine-mac-2.6-alpha1.dmg\nWhen you say \"Running JavaAppLauncher OpenRefine.app/Contents/MacOS\" are you doing something other than just doubleclicking the Refine icon? (Just trying to make sure I understand which cases work and which don't).\nThis is blocking the next kit build, so I'll look at it today.\n. Thanks for the detailed narrative.  That's a HUGE help.\n1 is known problem with refine.ini in alpha1 (fixed)\n3 is a known problem with the Ant build procedure in alpha1 (fixed)\nA newly built kit that I can't test with is an unknown quantity.  The bundled JRE included by Oracle is supposed to eliminate the variability of the Java versions.\nI suspect the crux of the problem is the error: OpenRefine_fork/clean/OpenRefine/build.xml:237: java.lang.UnsupportedClassVersionError:   com/oracle/appbundler/AppBundlerTask : Unsupported major.minor version 51.0\nbut I'll need to investigate further.\nThanks again for the detailed report.\n. The .class files for the bundled extensions aren't included in the Mac kit at all, thus the error when attempting to load them (despite the weird error message).  It doesn't have anything to do with incompatible Java versions, they just aren't there.\nI haven't investigated to see if it's there in 2.5 as well, but I'm guessing not. I'll get if fixed before 2.6-alpha2\n. Isn't this fixed by your pull request #779?  I manually edited the butterfly.properties file in the 2.6-alpha1 kit to match what that patch would produce and it was able to find all the bundled extensions.\n. I'm going to assume this is the same problem.  Please reopen the issue if you disagree.\n. I can't reproduce this in the current code.  Can you provide an example project which has the values that you're trying to convert?  I suspect that perhaps they're floating point numbers (which will get formatted as you describe), not integers.\n. Fixed by 3828139ed0bcfb540f7b7b82df145fd5d3d6e14c and 844b8182a405446db0038b17811212b8c361cd79 \n. Actually I think this was some kind of timing or ordering issue that only occurred in certain environments, rather than something that was specific the data directory not existing.\n. We're going to have the same need for both Knowledge Graph and Wikidata APIs, but I guess we can just create new issues for those.\n. The main reason the options exist is for backward compatibility because several of the importers used to always strip whitespace and there was no way to import a file as is.\nOne of the things that I'm thinking of which might mitigate the need for this is a way to apply operations to multiple columns.\nAnother thing I should mention is that we now support multicharacter separators, so if the problem is a program that's writing the files as value1 , value , ... or value1, value2, ... you could just make your separator \" , \" or \", \" respectively.\n. I cut about 11 MB from the kits which brings the Mac kit down to 92 MB which gives us a little headroom under the 100 MB Github limit (and the others to 40 MB), so I'm going to declare victory for this release.\n. The importer class is com.google.refine.importers.MarcImporter  It looks like it just reads MARC (in any format that marc4j's permissive reader can handle) and writes a temp MARC/XML file that it then parses using the XML importer.\nSo MARC/XML is already supported and this is just intended to provide support for binary MARC.\nIn terms of wiring it up, I'd look at something like the RDF triples importer to see what needs to be done.  This registration probably needs to be enhanced: https://github.com/OpenRefine/OpenRefine/blob/master/main/webapp/modules/core/MOD-INF/controller.js#L211\nand these should be verified/tested:\nhttps://github.com/OpenRefine/OpenRefine/blob/master/main/webapp/modules/core/MOD-INF/controller.js#L239\nhttps://github.com/OpenRefine/OpenRefine/blob/master/main/webapp/modules/core/MOD-INF/controller.js#L269\nI don't think it'd be a ton of work.  It may have just gotten lost in the project creation redesign that was done a couple of versions ago.\nIf you have funding available and don't want to tackle it yourself, I could take care of doing the work on a contract basis.\n. Is this the 2.6 beta 1 kit or a private build?  If the latter, the first thing I'd try is to do a \nrefine clean\nrefine build\nrefine\ncyle to see if you've got stale files hanging around.\nIf it's the beta kit, what version of Java are you running?  We require Java 6 now (since Java 5 is EOL), but I'm not 100% sure that we check for it.\n. By way of background, the two likely causes of this error are:\n1. The classes that the Javascript is trying to load haven't been compiled (the one in the error message is a brand new class for the I18N stuff, so if you updated without rebuilding, it could be that)\n2. You compiled the classes with a later version of Java than you're trying to run with (e.g. built with Java 6, but trying to run with Java 5)\n. This regression was apparently introduced by ca2e959957a53015e9fce178c28ef619cbad1534 when fixing issue #529\n. Are there any error messages output on the console/terminal? Do you have a Freebase pulldown menu in the Extensions: bar (underneath the Export & Help buttons in the upper right)?\n. Can you let me know what URL you downloaded the kit from?  This almost sounds like it's one of the old alpha kits.\n. Now that I look more closely, I can see there there is a space in the menu where the Add Columns from Freebase command would go, so your theory about it being a rendering issue fits.\nWhat browser(s) are you using?  Are there any errors on the browser's Javascript console?\nAs you may be able to tell, I'm starting to run out of ideas here...\n. I've replicated the issue with Chromium on Linux, so I should be able figure out what's going on.\n. As a workaround, if you run into this problem, use the Language Settings option on the main Refine screen to select either English or Italian.\n. The version of Java 7 that we bundle has a minimum OS X version requirement of 10.7.3. \nhttp://www.java.com/en/download/mac_sysreq-sm.jsp \nGoogle Refine 2.5 may not have bundled Java in which case it would have used whatever version you have installed on the system, but Apple is apparently now requiring a bundled JRE (and the Oracle app bundler includes the JRE too).\nIt may be possible to jury rig things to work by using Software Update to make sure you've got the latest version of Java 6 and using it to run the Refine main class by hand, but basically you've got an unsupported configuration.\nI'll update the readme with the minimum OS X version to make this clearer for others.\n. Thanks Pablo!\nI've merged the pull request from Pablo (@ultraklon) but haven't had a chance to retest the bug.\n. The reconciliation service is gone.\n. The standard style of Freebase reconciliation is still supported, but I'm guessing that we're not correctly upgrading existing installations.  This was an upgrade from Google Refine 2.5, correct?\nThe URL listed:\n4.standard-reconcile.dfhuynh.user.dev.freebaseapps.com./reconcile\nis the old reconciliation service.  One clue is the \"Could not fetch URL: http://api.freebase.com/api/service/search?read=15000\" error message.  That's the previous API endpoint which Google has decommissioned.\nThe new reconciliation services lives at\nhttp://reconcile.freebaseapps.com/reconcile\nAs a workaround, you should be able to add it by hand (Add Standard Service button at bottom left of reconciliation dialog).\nI'll have a look at what needs to be done to make the upgrade smoother.\n. @cldwalker Thanks for the confirmation.  We'll have it fixed before the next kit goes out.\n. There are a couple of different problems describe in this thread.  The most recent problem is that the entire freebaseapps.com domain has been retired, so anything that lives on it, including our new Freebase reconciliation service, is gone.\n@thadguidry The new reconciliation APIs have been supported since they were introduced.  Unfortunately they were proxied through a service which has been shut down by Google.  We can either:: a) host the service somewhere else or b) special case Freebase support differently from all the other reconciliation services and self-host it in the Refine server.\n. When I attempt to use a Sindice reconciliation service configured to run against DBpedia (although I presume it happens with anything), I get the following in the stack trace:\nCaused by: java.lang.ClassNotFoundException: com.ibm.icu.text.StringPrepParseException\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)\nIt looks to me like they are depending on a JAR that we jused to bundle, but no longer do.  It was a 10MB+ file that we were only using for character set guessing.  We may re-introduce a heavily tailored and smaller version, but the RDF extension needs to bundle whatever files they require and not depend on us to provide them.\nPlease report this bug against the RDF Extension.\n. p.s. you might be able to work around the problem by placing an ICU4J jar of the appropriate version in:\n./OpenRefine/extensions/rdf-extension/MOD-INF/lib\n. The RDF Extension was created by DERI at NUI Galway and it is not currently bundled with OpenRefine.  The web site is here: http://refine.deri.ie/support-dev\n. Actually, the client can't access that directory directly, so we'll need to add a new API call to get the available languages.\n. I wish the OP had provided more detail (!), but it looks like the or-proj prefix for IDs equates to core-project in the translation files and these IDs exist in all languages as core-project, open/export/help\nClosing for now.  If there's evidence that this is an actual current problem, please reopen.\n. Please use the mailing list/google group for questions.  That way the whole community will see it and whoever's available can answer it, rather than having to wait for a developer.  We only use this issue system for bug reports and enhancement requests.\n. Sorry about that!  It looks like the group settings were messed up and set\nto disallow posting by new members rather than the intended moderation of\nnew members.\nPlease try again.  Your first message will be moderated, but I'll look for\nit and approve it right away.\n. Please use the Google Group https://groups.google.com/forum/#!forum/openrefine for support requests and questions.  If your Refine server is running, the most likely cause is some sort of firewall issue.\n. Thanks for reporting the problem.  This is the same as #797 which we've already fixed for the next kit.\nThe workaround for the current beta kit is to use the Language Settings option on the main page and choose either English or Italian as your language.  These strings are missing from the pseudo language \"default\" which every starts with.\n. Sounds like #652 (depending on what vintage of source code you were testing with)\n. I'm going to close this as a duplicate of #652 which has been fixed.  Please reopen if it's seen again with more recent code.\n. Thanks for looking.  I do most of my Mac testing using a virtual machine,\nwhich, unfortunately, triggers a JVM bug in the graphics handling, so my\nJVM is patched to disable the very path that I'd need to be able to test\nthis.  I'll need to find an actual Mac to test on.\n. Thanks @ultraklon.  So you're saying, that OS X can use the same logic that everyone else does?  I don't have a real Mac to test on, but I'll merge the pull request so others can test.\n. The pull request is (much belatedly) merged.  If other Mac users could test and let us know if it's an improvement, that would be a great help.\n. @herrernst Thanks for testing.  I'll close this issue based on your results.  Thanks again to @ultraklon for the fix.\n. This snippet will work, but after playing with it, I think it's too confusing for the user to deal with our multiple internal number representations, especially since we transparently coerce them as necessary everywhere else.  We'll need a better solution.\nif (args.length == 2 && args[1] instanceof String) {\n            Object o2 = args[1];\n            if (o1 instanceof Calendar || o1 instanceof Date) {\n                DateFormat formatter = new SimpleDateFormat((String) o2);\n                return formatter.format(o1 instanceof Date ? ((Date) o1) : ((Calendar) o1).getTime());\n            } else if (o1 instanceof Number) {\n                return String.format((String) o2, (Number) o1);\n            }\n. Thanks for the bug report.  I'm pretty sure that's the Freebase extension.  As as workaround you can go to the Language Settings on the main screen and choose English (or Italian) as your language.  The strings are only missing from the \"Default\" language (which I think we'll get rid of in favor of just having English as the default)\n. I'm closing this as a duplicate of #797.  Feel free to reopen it if you believe that's not the case or if you find the workaround there doesn't work for you.\n. The mailing list https://groups.google.com/forum/#!forum/openrefine is a better place to ask questions.  In addition to the configuration information Martin asked for, we also need to know what version of OpenRefine you are using.\nThe most likely cause of the problem 1 is that the separator isn't what you think it is (e.g. an em dash instead of a hyphen).  Try cutting and pasting the single separator character to make sure you've got it right.\n. Glad you got it working.  We recommend using Chrome or Firefox rather than IE.\n. When did you download the sources?  Can you try doing a ./refine clean before the ./refine build to make sure that there is nothing stale/partially built left around?  \nI just rebuilt from the latest sources and didn't have a problem.\n. There are no external dependencies other than a JDK.  The particular dependency that the compiler is complaining about is satisfied by the file\nOpenRefine/main/webapp/WEB-INF/lib/commons-codec-1.6.jar\nwhich is included with the sources.\nOne strange thing that I notice is that when I a clean build, I get a message saying that it's compiling 427 source files, rather than the much smaller number (372) that you quoted.  That makes me wonder if you've got an incomplete set of sources.\n. I'm closing this as unreproducable.  I just did some more builds on a brand new OS X machine without any problem, so I'm pretty convinced that it was a problem with the local environment.\n. Prototyping would provide useful information, but perhaps you could outline your process so far and the decision criteria that led to MontageJS being the answer.  There are lots of potential JQuery replacements out there (AngularJS is another that comes to mind), but there would have to be a pretty big payoff for such a large engineering investment.\n. Yes, doing large numbers of live incremental updates is bad because of continuous relayouts and a whole host of other issues.\n. \"The version on the website\" is 2.5 or one of the 2.6 alpha/beta releases or ... ?\nFor the source build, are you running ./refine build before running `..refine' ?\nFor either/both, does the file ./server/classes/com/google/refine/Refine.class exist?\n. Glad you got it working.  If I had to guess at the problem with 2.5, my first guess would be that it got unpacked without preserving the directory structure.  The kit's been available for over a year without reported problems.\n. Could you expand on what you see the use case being?\n. Thanks.  Sorry for the delay in merging.\n. Thanks for the suggestion, but I'd rather not introduce a new dependency into the project just for this.\n. There hasn't been any activity on this in a long time and we never got a file to reproduce it with.  If you can reproduce it and provide us with a file, please reopen the issue.\n. How many columns does your project have?  It looks like these formats are limited to 256 columns and you're trying to output 257 or more.\nWe should handle the error better though and give the user explicit feedback about what the problem is.\n. I'm going to optimistically assume that the pull request from @abhillman that I recently merged has fixed this problem.\n. Thanks for the fix! Be aware, however, that the UI tests are a lot more broken than a typo or two.  They haven't been maintained in years and depend on a very out-of-date test framwork (Windmill).\n. I'd lean towards using Selenium at Sauce Labs as described here http://about.travis-ci.org/docs/user/gui-and-headless-browsers/ I started investigating a while back and set up an account, but haven't done much more.\n. There is not a Chinese translation and a pull request with one would be very welcome.\nI'm not sure we have everything in place to support right-to-left languages though. There are probably some places with code like \"there are \" + 3 + \" items in your list.  The I18N support that was contributed does string literal lookups, but it doesn't support sophisticated string formatting.\n. Pull request #869 included a contributed Chinese translation.  Can those of you who are interested in this test the current translation and let us know if there any problems?\n. I'm going to assume that the current translation is adequate.  Feel free to create new bug reports if you find problems with it.\n. This appears to be a data file, not a patch for OpenRefine.  If I've misunderstood your intent, please re-open and explain.\n. Pablo & Martin - Thanks for testing.\nPablo - could you clarify whether you tested 2.6 beta 1 or the current development head?\nI need to double check, but one possibility is that I may have unintentionally pushed a jQuery upgrade to Github before it was ready.  That could explain issues like this.\n. @ultraklon Did you ever get any more information on this?\nAll - I haven't seen this, but we should explicitly test it for the 2.6 release.\n. Thanks for the bug report.  This is the same as #804 which has already been fixed, but we don't yet have a new kit available with the fix.\n. Please use the mailing list / Google group for questions:\nhttps://groups.google.com/forum/#!forum/openrefine\n. I think a number of things are broken on master due to an ill-conceived attempt to upgrade the version of jQuery.  I plan to move that to a separate branch and revert to the previous version of jQuery.\n. Thanks Frank (@FWennerdahl).  I've merged Pablo's ( @ultraklon) PR since it was here first, be appreciated both of your efforts.\n. Yup, totally my fault.  Thanks for fixing Frank and thanks Pablo for verifying.\n. @annolangen Thanks for the contributions.  I apologize for the delay in reviewing and merging your pull request.  Generally, I'd prefer one fix per pull request and, in particular, not mixing infrastructure stuff like Eclipse vs. IntelliJ or code reformatting, with functional changes.\nI'll probably pull in your changes individually, so the pull request will stay open a little while longer while I do that.\n. I've manually merged the three fixes from d04750f0bdc42bc4f7adadfcae930df2a8147938 , including the improvement to #837.  Thanks for the contributions and apologies again for the delay in getting it all merged in.  Individual clean pull requests will get merged quicker in the future.\n. Sorry for the delay in reviewing this.  Some of the APIs that you've changed are not only public, but are actively used by 3rd-party importers (e.g. setProgress) which means they really can't be changed.  I took a cut at a more minimal solution, but I need more time to review and compare the two approaches.\nYou can see an initial pass at my version here: https://github.com/tfmorris/OpenRefine/tree/import-sync-cleanup\n. The bug described in #480 was fixed on the web client side, but having a default record path may make it easier for programmers using the 3rd party client libraries to drive the OpenRefine server in certain circumstances.\n. Fixed by c4b6f824f0676e51ae73dc3fb7fed8a16f5ce375 from @annolangen in #853.\n. Thanks for the pull request.  This is a pretty big change to a very critical part of the system.  Do you have a test which demonstrates the failure that you are attempting to address?  Race conditions are always bad, but given that these operations are all being invoked by a single human clicking a mouse, the likelihood of the race occurring seems, on the surface, small.\nI've updated the TestNG instructions in the wiki.  Feel free to edit it yourself if you find problems -- that's the whole point of it being a wiki.\nFor stats (e.g. 15% errors, 40% faster), it would help to interpret them if some context about the test conditions were provided.    Otherwise they're basically just random factoids.\n. The OpenRefine server is just a web server implemented in Java.  It has no GUI at all, let alone an OpenGL GUI.  The client is a vanilla web browser.\nThis SO answer http://stackoverflow.com/questions/8870304/dont-automatically-switch-to-the-higher-end-discrete-gpu claims that you need to add that key with the boolean value of true.\nIf you come up with a solution that works, we'd be happy to integrate it, but I can't justify wasting more time chasing OS X-specific bugs.  None of the other operating systems we support give us the kind of headaches that Apple products do.  OS X is the only one with these problems.\n. Submitting this in the form of a pull request from a specific feature branch would make it easier to review and integrate.\n. Glad to hear you got it figured out.\n. Please use the openrefine-dev for questions and requests for assistance.  We use the issue tracker for bug reports (in our code, not yours).\nIn addition to the good suggestions from @jadient, you might want to consider invoking Java directly since the .exe is just a wrapper on the java -jar command.  It will remove one more level of indirection and variability.\n. Thanks for the fix!\n. @plhyc1216 Thanks for your contribution!\n. You should report this on the LODRefine web site.  The issue tracker here is for OpenRefine.  They'll probably want to know what browser you were using.\n. That's the stereotypical manifestation of a missing translation.  You could try setting your language to English to see if it's an issue with the English \"translation\" or the default language translation.\n. Fixed in 2.6-rc1\n. We currently check for\nSystem.getProperty(\"os.name\").toLowerCase().startsWith(\"mac os x\")\nbut the recommended form is\nString osName = System.getProperty(\"os.name\");\nreturn osName.contains(\"OS X\");\nso perhaps the contents of os.name changed in way that is messing us up.\n. Fixed by c7aa25ccd3f2685bfe655ee3b689c3c47b5ae5cc from PR #1074\n. I don't suppose you got a copy of the stack trace for the exception, did you?  It's going to be pretty hard to track this down given that we don't have the steps to reproduce it or the stack trace.\n. @felixrabe were you able to find any more details on how to reproduce this?  Without additional info, there's probably not much we can do.\n. I'm closing this for lack of additional information.  Feel free to reopen if you can reproduce the problem.\n. Can you give us a pointer to the file that you are attempting to use?  Since you mention OpenRefine, I'm assuming that you are using the 2.6 beta.  Is that correct?\nIf the data is in a zip file, you might try unzipping it by hand and then importing the contents.\n. Thanks for the pull request!  I'll add it to the .gitignore as well to prevent future problems\n. Hi.  Please use the mailing list/group for general questions: https://groups.google.com/forum/#!forum/openrefine\nIf you've got a specific feature request, feel free to create an enhancement request for it, but it needs to be a little more specific than just \"out of core algorithms\"\n. Please use the mailing lists for questions.  The issue tracker is for bug reporting and enhancement requests.\n. Please use the mailing list for questions.  The issue tracker is for bug reports and feature requests.\n. Is this with 2.6 beta 1 or the current version in source control?\n. I've ripped out Chrome Frame so OpenRefine will at least start up in Internet Explorer in the future, but almost none of our users use IE, so it gets very little testing.  We recommend either Chrome or Firefox for use with OpenRefine.\n. Please use the mailing list for questions.  2 GB is the absolute bare minimum for Windows 7, so you shouldn't expect good performance. http://windows.microsoft.com/en-us/windows7/products/system-requirements\n. Thanks for the suggestion.  Do you have thoughts on how the user would specify these various filters and facets?  Currently OpenRefine only has a single set of facets which are active at a time.\n. @sutt Missed your question earlier, but the Custom Tabular Export dialog sounds like a reasonable place to put controls for this.\n. We really need the example data (or URL to generate it) to be able to provide any help at all with this.\n. Thanks for the update.  You might want to double check your refine.ini file to make sure that you migrated any important stuff like increased heap allocation to the new installation.  There aren't any known performance regressions in 2.6.\n. Thanks for the pull request. I should be able to cherry-pick the fix, but in the future, please use a dedicated branch for pull requests to they don't get polluted with unintentional changes (e.g. your local Kew extension changes)\n. Commit aa7f34248b0d9039c77aa8b5034b6dae72f7aebb has been merged in.  Thanks for the fix! \n. Hi @aliciavc.  Sorry for the slow response, but the mailing list is a much better place to ask questions like this.  Hope you got your problem resolved already, but if not, please ask for advice there.\nhttps://groups.google.com/forum/#!forum/openrefine\n. The Export Excel menu exports to XLS.  To export to XLSX, use the Custom Tabular Exporter and select \"Excel in XML (.xlsx)\"\nI'll leave this open as a potential enhancement request for a quick Excel 2007+ menu option\n. Add either horizontal scroll bars or the ability to resize the dialog to allow users to see long column names effectively.\n. I'd recommend partitioning your data using facets and clustering a subset.\nWe should provide a better error message (and perhaps provide a way to do this in chunks automatically), but the maximum size for our internal protocol messages is 1MB and the operation that you're attempting is almost 10MB which isn't even close.\n. The enhancement in #876 allows the maximum form size to be configured if that helps.  This will be included in the next release of OpenRefine.\n. Thanks for the pull request!\n. I'm not sure I understand the question, but please use either the openrefine list for user questions or openrefine-dev for developer questions (sounds like this might be the latter, but I'm not really sure).\n. What version are you using?  The line-based text importer shouldn't be doing any interpretation/conversion of the text at all.  Does this happen immediately after project creation or after you've applied other transformations?  Can you provide a small sample project that demonstrates the issue?\n. Thanks for the enhancement!\n. What type of data resides at the URL that you are using?  Can you provide an example URL for us to test with?\nThe \"Malformed reply from SOCKS server\" seems suspicious to me.  Is there a proxy or firewall involved in the communication path?\n. Glad to hear you found a solution to your proxy problem.  Problems with the RDF Extension should be reported to that project, not here, but it sounds like a known compatibility issue between OpenRefine 2.6 and 0.8 of the RDF Extension.  They have produced a 0.9 release which addresses it.\nhttps://github.com/fadmaa/grefine-rdf-extension/issues/85\n. Thanks for the suggestion.  We would welcome a pull request which implemented this functionality.\n. GREL doesn't have the concept of functions/methods - everything has to be in a single nested/chained set of functions calls.  For the other two languages though, you should be able to use the standard language support for module reuse.  Also, the command history is shared across projects, so things can be easily reused within a single user's projects.\nWe don't have any type of external code sharing or collaboration platform.  We'd be interested in hearing more about how people might use such a capability.\n. This sounds like some type of installation or configuration problem.  Please use the group / mailing list to ask for assistance.\nhttps://groups.google.com/forum/#!forum/openrefine\nIt has more eyes and more people people available to help.\n. Thanks for the fix.  Is there an open bug report for this or is it something you found on your own?\nIt would be great if you could add a test to verify the fix -- and make sure that we don't break it again at some point in the future.\n. Thanks very much.  We appreciate the contribution -- and especially the quick turnaround in adding the test case.  You'll see this in a 2.6 release soon.\n. I think this is only used for debugging, but appreciate the improvement.  I've tweaked it slightly to use the StringUtils convenience function so it knows how to do date formatting as well.\n. Thanks.  As you may have noticed, there are a number of things available not available in the .bat file including kit builds, etc, but it certainly makes sense to include at least the easy stuff.\n. Thanks for the fix!\n. Thanks for the pull request.  Normally I wouldn't accept binaries (ie the JARs), but since they're just source JARs, I've merged them in to save me the trouble of downloading them myself -- because I'm lazy. :-)\n. Thanks for the fix!\n. Your proposed change is self-contradictory and doesn't add any useful information.  Sorry.\n. Thanks!  We love new language support.  Have you seen any major issues with RTL presentation?\nAny ETA on when this would be complete?  I hope to get the 2.6 release out the door over the holidays.  I'm not looking to integrate any major code changes (ie better RTL support), but would be happy to include Hebrew in the supported languages.\n. Could you perhaps include a word or two about what you were trying to do (ie sequence of steps) and what the expected result was?\n. Closing due to lack of information on how to reproduce.\nFeel free to reopen if you can provide information on when this happens.\n. If you have a chance to describe your workaround or misunderstanding, that might help the next user with the same issue...\n. @patd0000 We update the issue tracker whenever there's a change in status, so there is no new news.  It's likely to be an easy fix, but someone needs to find the time to look at it.  Pull requests accepted, of course. :-)\n. What functions and/or menu selections were you using?  What Pandas functions were you using?  Where did you find documentation that the two were expected to perform equivalently?\n. I'm pretty sure there's nothing that says OpenRefine is intended to work the same as pandas, so your use of \"correctly\" and \"incorrectly\" isn't correct.  OpenRefine works as it is documented to work.\nThe tutorial you reference says:\n\nIn the above mentioned step, we assume the dataset has a field with unique values, \nindicating that the entire row represents a duplicate. This is not necessarily the case, \nand great caution should be taken to manually verify wether the entire row represents \na duplicate or not.\n\nThat sounds pretty clear in saying that they only expect it to work on a single column (ie the unique ID column).\nIf you'd like to facet things by the values in multiple columns, one way to do it would be to create a key that is the concatenation of the values in those columns.  You could either create a new column with this key or you could just use a custom facet that creates the key on the fly.\nIf you think a feature similar to what pandas has is important, you could add your support to the enhancement request #592 that is open for it.\n. Please use the mailing list for help requests and installation issues:\nhttps://groups.google.com/forum/#!forum/openrefine\nOne likely cause of this is not preserving the directory structure when unzipping the files.\n. Please use the mailing list/group to discuss questions that you need help with or problems that you aren't sure are bugs.  https://groups.google.com/forum/?hl=en#!forum/openrefine\n(but first see my StackOverflow answer http://stackoverflow.com/a/27892017/167425)\n. Thanks for fixing the things that @maniksurtani pointed out.  I agree with all his comments.\nThe pull request also needs to be done from a feature branch, not from your forked master, because otherwise it's susceptible to getting polluted by unintended later changes (e.g. your removal of openjdk6 from the Travis config which might have been intended to be a purely local change).  Each feature branch/pull request should include a single coherent set of changes without any unrelated changes.\nWe won't be updating any dependencies until after the 2.6 release, but that should happen very soon.  At that point I'll be doing a major dependency update and probably another relatively quick release, so these code changes will come in handy then.\n. Thanks for the fix.  It probably would be better to remove the bashisms from the script, but this is a simple solution for now.\n. Thanks for the contribution. \n. Hi Qi Cui.  I'm not ignoring you.  I just haven't had a chance to review your PR between snowstorms, the Superbowl (winning!) and real work.  I'll get to it tomorrow.\n. Fixed by PR #937\n. @martingraham Thanks (belatedly) for all the analysis.  I'm glad you found a workaround.  \nAny chance we could get a copy of the spreadsheet for debugging purposes?\n. @martingraham If you can attach the file here if you are comfortable with its level of anonymization or you can email it to me.\n. This seems to straddle the line between a feature request and a question.  Questions of this ilk are best suited for the developer's mailing list (where they'll be seen by more people and probably answered quicker).\nSome off-the-cuff thoughts:\n- the VIBS plugins are all non-obfuscated Javascript, so not much reverse engineering is required (although they're GPL licensed, so no derivatives can be contributed to OpenRefine or anything else where GPL is an incompatible license)\n- although the Add column by fetching URL operation is a long running background operation, you should be able to queue the following operations (a series of Add column based on this column operations?) and have them wait until it's done\n- you can generate the necessary URL in a new column, use it for your fetch, then delete it afterwards if you want things tidy.\nUnless you want to turn this into a feature request for something that's generic enough to be generally useful, why don't we move this discussion to the mailing list to clarify the best approach.\n. What version did you see this with?  There were significant changes made in 2.6 specifically to improve robustness of the save code in the face of heap exhaustion.\nThe periodic automatic project save uses exactly the same code as the user initiated save, so any problems seen in one will be seen in the other.\nWhat evidence do you have that autosave, specifically, is the root cause?\nNote that the alternative, forcing the user to manually save, will cause them to lose all work since their last manually initiated save.  In my mind, fixing the save function (if it still needs fixing) is vastly preferable to disabling autosave altogether.\n. As Martin mentions, this functionality is already available in OpenRefine.  You can specify a URL as the input on the Create Project screen and if the URL specifies a zip file (or tar file) it will explode it and let you choose one of the contained files as your data source.\nI'm going to close this, but feel free to reopen it with additional details if we've misunderstood your requirements.\n. I'm not sure I understand your question, but if the query is URL is an API that returns JSON or CSV (or HTML) and takes query parameters as parameters with the URL, then, sure, OpenRefine can do that too.\nThese type of questions are really better suited for the group/mailing list (presuming they aren't already covered in the FAQ).\n. This appears to be fixed in the current development version.\n. Hmm, and I was just getting ready to yank out all the Freebase stuff...  I suppose we could rename it and cut out all the Freebase-specific stuff related to Refinery (QA Data, Load, etc).\nI'll do a more thorough review, but some notes to myself in the meantime:\n- need to update French & Chinese translations (actually looks like they're missing)\n- double check operation/protocol compatibility\n. The Freebase extension that this depends on is currently out, but we need to decide if that's the right thing to do (the PR got merged without review). Yanking the whole thing is certainly the easiest approach, but it also removes useful functionality that is still available from Freebase (and, of course, prevents this proposed change).\n. The view is going to get refreshed on a per-character basis, so if it's slow initializing, it's going to be very slow as you try to use it.\n. Please upgrade to OpenRefine 2.6-beta1 and see if it helps.  If you're still seeing the problem, we'll need a test data set to help reproduce the problem.\nYou might also ask for help on the mailing list, because the cross() function is one of the most difficult to use and people often get it wrong.  For example, it's pretty much the only function that takes a cell instead of a value as an argument.\n. As Martin said, this discussion is better suited for the mailing list (and\ndon't hijack someone else's issue unless you're 110% convinced that it's\nthe same problem as you're seeing).\nOn Thu, Mar 26, 2015 at 12:05 AM, RUGANO notifications@github.com wrote:\n\nI'm on OpenRefine 2.6 beta and the cell.cross(\"results_export\",\n\"Values\") is returning me an empty array [] .\nIf you're getting an empty array (ie null result), Martin's proposed\nsolution won't help.  Are you getting empty arrays for all rows?\n\nThe first thing I'd do is triple check your project names, column names,\nkey values, etc.  Make sure that you've trimmed all whitespace and gotten\nrid of anything else which might be messing up the matching.  Cross is one\nof the trickiest functions to use because it has some many parameters and\nways to go wrong.\nIf that doesn't fix things, please post more details on the mailing list,\nincluding, if possible, some example projects that demonstrate the problem.\nTom\n. The standard reconciliation service is gone, thus the message.  It can be safely ignored, but it is annoying.  The current development version has this service removed, but I think it gets recreated automatically if you try to remove it in the released version.\nI think this error was originally mentioned in #805, but it has morphed beyond recognition and no longer makes any mention of it.\nThe issue isn't platform-specific and affects all three platforms equally.\n. What version of OpenRefine is this?  Are there any error messages logged on the Javascript console?\n. Using Firefox 36.0.1 on Windows 7 with the current development version I, like @DavidLeoni,  can't reproduce this.  When it \"hangs\" is the browser spinning in a compute bound loop or is it idle?  What does Firebug show the browser doing?\nThere are definitely problems with 094562e54b5c6a9af0df151f28996e860081bd47 (typos in version numbers), but they are fixed by a later commit.\nIf there were problems with Firefox compatibility introduced in September, 2013, I'd be very surprised if it took 18 months for them to be discovered.\nAnother thing you could try is doing a hard browser refresh (or using a private browsing window) to make sure that you're not running stale cached code.\nLet us know if you find any additional clues.\n. OK, thanks for the additional info.  I hadn't noticed the repeated requests in your log.  It's almost like it thinks it's restarting the open.\nOf course, user installs always have the client and the server on the same machine, so they won't see this, but it'd still be good to track down since it probably hints at some underlying timing (or other) bug.  You may need to increase the logging level on the client and/or server to figure out why it thinks it needs to restart things.\n. @jackyq2015 I'm trying to understand what the situation is here.  You closed the issue, but your last comment indicated that you had to fall back to an older version of jQuery.  Does the problem still exist in the current development sources?  Can you summarize what configuration(s) don't work?\n. Thanks for offering to help.  I think a Spanish translation would be a great enhancement (and have tagged this issue as such).\nI'd actually suggest using translation-en, rather than translation-default, as the starting point, even though they should be equivalent.  I think we're going to get rid of the latter eventually anyway.  From a political point of view, I'd like all languages be equal, but, in actual fact, the working language of the project is English, so that's the really default and it just makes extra work to maintain to identical files.\nThe email list would be a good place to solicit help with the translation, but also feel free to submit a pull request with just a partial translation and point others at your branch which is included in the pull request as a focus for them to help with the translation.  Please reference this issue with the pull request and that'll allow everyone to track what's going on and keep in sync.\nThanks again for volunteering!\n. p.s. we don't have an incredibly robust I18N structure, so if you find something in the core that doesn't work or needs improvement, feel free to open new issues to address them.  For right now you pretty much need to restrict yourself to simple string substitutions, so there's no fancy plurals, embedded parameters, reordering of phrases, etc.\n. @nestorjal Any progress on this?  v2.6 will be released shortly, so if this is close it would be great to have it in the release.\n. The partial translation as been merged\n. The developer's mailing list is the best place for questions like this.\nIn addition to the Python library, there's also a Ruby library here: https://github.com/maxogden/refine-ruby\nNote though that both of these client libraries exploit an internal API which which is not officially documented for external use (ie it could change at any time)\n. Please use the mailing list for questions and requests for assistance.\nThe most likely cause of the browser crash is a very large table it's attempting to display.  This could be due to the data having a very large number of columns, Refine misinterpreting the CSV quoting (try changing the quote option on import), or Refine thinking that there are \"records\" with a very large number of rows.  If you provide more details on the mailing list, we can get it figured out.\n. This appears to be fixed in the current development version (and rc1). \n. I just double checked the 2.6-rc1 Windows kit and it's definitely fixed.  It seems unlikely that it's a Linux specific bug.  \n@magdmartin Can you make sure that you're not seeing cached artifacts by doing a force reload in your browser.  What browser are you using on Linux?\n. Very puzzling since the only browser specific code that we have is the old IE shim.\nHopefully someone else with a Linux system can help debug.\nJust so we have as much information as possible - what distribution and version of Linux? What version of Firefox & Chrome.\n. No need.  I have a fix in hand already.  Thanks for investigating.\nTom\n. Fixed by 75ec34ea93c6979b96d17e72ce17998c2178d940\n. Thanks for clarifying what you are actually requesting.  I've updated the title of the issue to reflect that.\n. This sounds like a (potential) issue with the Refine RDF extension.  Could you please report it to them? https://github.com/fadmaa/grefine-rdf-extension/issues\n. Hi Jan.  The email list / Google Group is the best place for questions which aren't actual bug reports.\nThe readers will probably want to see examples of the entries that you thought should have been clustered, but which weren't, along with the parameters that you used for the clustering.\n. On Tue, Apr 7, 2015 at 8:35 PM, jansalm notifications@github.com wrote:\n\nSorry Tom\nI am very new to this.\n\nNo problem!  We're just trying to direct you to where you'll get the best\nhelp.\nTom\n. Two things:\n1. The default limit for the number of choices in a text facet is much, MUCH, lower, so you must have increased it.  You can reset it lower by going to http://127.0.0.1:3333/preferences and changing ui.browsing.listFacet.limit to a lower value.\n2. You can access clustering without using a Text Facet by using Edit Cells -> Cluster and Edit.  This will take you directly to the clustering dialog.  If you have too many choices there, you could pre-filter using something like a text facet on the first character of the value.\nWe already have an issue regarding performance of facets, so I'm going to close this.\n. Please use the mailing list / Google group to ask questions.  The issue tracker is for bugs and enhancement requests.\nThere is a 2.6 beta available on the downloads page and the development team just had a discussion regarding what it would take to wrap up the final 2.6 kit.  Expect to see it soon.\n. Ignore AppVeyor.  It's not fully set up, but there's not an easy way to retroactively disable it.\nI think I've got the equivalent of this on a branch in my repo, but we'll use your version.  Thanks for the contribution!\n. Please ask questions on the appropriate mailing list.\n. I'm happy to accept low risk fixes for this for 2.6.  Not sure whether 2.6 or 2.7 is \"next\" in this context.\nA combination of making the menus size to their contents and keeping the translated text as short as possible is probably the correct solution for this, but without looking at it I have no idea how hard it would be to fix the flyout sizing.\n. The misleading message sounds like a bug to me, so I've created a separate issue for it. #976\n. First, you are an order of magnitude (ie 10x) over the design center of OpenRefine, so you're going to find lots of things which don't work smoothly at this scale.\nSecond, there's little to no multi-threaded code in Refine (other than what is inherited from standard Java library implementations), so if you N*100% being used by Refine, especially for a very long period of time, the most likely consumer is the JVM garbage collector, which has the unfortunate characteristic that it will try really, REALLY hard to make things work, even if it should just give up, since it's going to fail eventually anyway.\nSuggestions:\n- check JVM garbage collector stats to see if you're at 99% of available heap, thrashing the GC\n- allocate more memory\n- process the data set in pieces\n- split the database before importing it (or use the CSV importer which has a customizable delimiter)\nHow many cells are you trying to create?  You're starting with 16M rows x 1 column -- how many will you end up with?  There's overhead associated with each cell (and each row).\n. I suspect this is a version mismatch on a dependency such as Guava.\n. This happens with v2.6-rc1 and needs to be fixed before release.\n. This method was deprecated, then removed in Guava 15.  We had updated to Guava 18, from Guava 13, but I've reverted that with bfc39d9e8c22edc035929f24d6b34adf2d8c9335.\nWe are probably going to need to update to more recent versions google-api-client and related libs to get support for replacements for deprecated APIs (e.g. Google Spreadsheets creation API) We're using 1.13.1-beta and the current version is 1.19.1.  \nUnfortunately, there are a fairly intricate set of dependences for the Freebase and GData extensions which may be in conflict with each other, but we'll leave that for another day.\n. Thanks for the bug report.  If the code is correct, the next place to check is the French translation.\n. https://github.com/OpenRefine/OpenRefine/blob/master/main/webapp/modules/core/scripts/views/data-table/text-transform-dialog.html\n. Should I expect a pull request for this?\n. I don't understand why one would use a pull request for this (in either direction).  If it's a repo that you control, you can merge things directly.  PRs on repos that you have commit privileges to are only normally used to allow for review of the proposed change (in which case you wouldn't merge your own PR).\n. Welcome - and thanks for the offer.  We would love to have an additional high performance clustering algorithm.  The existing simile-vicino algorithms are integrated here: https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/clustering/knn/kNNClusterer.java but that assumes that blocking is required.  Does the new algorithm use no blocking at all or just larger blocks? (I haven't had a chance to read the paper yet).\nIf you have a paper or blog post which describes the performance, that would be sufficient for the performance side of things.  For test coverage, we prefer to have test coverage for all new features (although we're not very good about enforcing that).\nNext steps:\n- fork the OpenRefine repo\n- create a feature branch\n- integrate your code and tests\n- submit a pull request for us to review\nIf you have any questions, please ask on the openrefine-dev list.  We look forward to your contribution!\n. @lispc The stated design center for OpenRefine is < 1 M rows, although in practice users push this higher.  I can see what sample data sets I have available, but something like 1 to 5 million author names would probably be a reasonably representative large data set.\n. There's an Apache-licensed implementation of PassJoin in Scala available here: https://github.com/sjyk/sampleclean-async/blob/master/src/main/scala/sampleclean/clean/deduplication/join/PassJoin.scala\nI haven't studied it closely enough to see if it implements exactly the same algorithm.\n. The issue tracker for the RDF Extension is here: https://github.com/fadmaa/grefine-rdf-extension/issues\n. Does this only happen with the text filter or all types of facets?\n. Access what documents where?\n. I'm guessing that you are using Google Refine 2.5.  This was fixed by bf2f775c0bf0e12595ad20637bc0ddfc8eefca04 over a year ago and is available in the OpenRefine v2.6 beta 1 kit.  It will be in the v2.6 kit out shortly.\nIf you are using a more recent version of Refine, please re-open this.\n. Please use the mailing lists for questions.\nText facets and numeric facets are handled completely differently.  One creates a list of choices and the other shows a histogram of counts vs numerical values.\n. I think we are talking past each other.\nA custom TEXT facet and a custom NUMERIC facet are, at the end of the day, just a text facet and numeric facet as I stated in my original reply.  Yes, they both open the expression preview window -- that's the \"custom\" part, but what they expect as a resulting value and what they do with it afterwards are completely different.\nWhy are we discussing this?  Why aren't we focused on getting 2.6 tested and released?  What part of \"use the mailing lists\" for things which aren't bugs or enhancements was unclear?\n. Oops.  This was a stale issue queued up months ago that just go synced.  It's low priority since Freebase reconciliation is gone, but I'll leave it open for now in case it's useful for other reconciliation services.\n. I think we got this for free when we updated our version of the Fusiontables client library.\n. I'm going to use the wording from July 29 version that I created in b296c29937b20bf3cf7f12b25fc94284615a11d1 as you can see in the history above.  I don't see the point of leaving Google Refine references lying around.\n. @jackyq2015 Thanks for the offer, but I've already got most of the work for #979 done (I'm assuming that's what you meant instead of 970).  It's a little bit complex because of the shared dependencies for Fusion Tables, Google Sheets, and Freebase, so I still need to do a little more testing.\nI think there is more work required than just upgrading the libraries, but it's a necessary first step.\nNote that Google Sheets import still works.  It's just the export which has stopped working.\np.s. As an aside, it's apparently not possible to assign an issue to a non-project member. I tried to assign another issue to you and wasn't able to do it, so we'll have to just rely on \"claiming\" issues using comments.\n. This is fixed, but it required removing/disabling the Freebase extension.  There's a Catch-22 because more recent versions of the Freebase client library don't include MQLRead and older versions don't work with current versions of the core Google client API libraries (and you can't have multiple versions of the dependent libraries easily).  We might be able to resovle this with fancy classloader tricks, but for the remaining few months of Freebase availability, it's not clear it's worth the effort.\n. Thanks @jackyq2015 ! I probably won't be able to fully verify until Sunday, but merging on the assumption that it's good.\n. Thanks for looking into this.  I had already fixed it with 75ec34ea93c6979b96d17e72ce17998c2178d940 by the time I saw your PR.\n. Thanks for the contribution.  I've merged what's been done so far.  Leaving the English strings for untranslated pieces is a good default so that users at least see something.  Please open a new pull request when you've made more progress on the translation.  We'd prefer that the pull requests be made from a feature branch (ie not \"master\") to reduce the chances of unintentional changes getting mixed in.  Thanks again!\n. Off the top of my head, I'd say the class org.python.core.PyException.\nWhat were you attempting to do when you got the error?\n. What version of Java are you running?  Are you using the refine shell script to start things?\nI just verified that the kit is OK and works fine for me (running the Linux kit on OS X, but that should be pretty close).  The only other things that come to mind off the top of my head are: 1) the classpath isn't getting set correctly because you're not using the shell script or 2) you've got an older version of Java which is incompatible with the distribute class files (although in that case, I'd expect to see a nested exception complaining about the incompatibility.\n. I'm running out of ideas.  I installed Ubuntu 15.04 on Virtualbox, along with the default-jre (OpenJDK 1.7.0_79) and the OpenRefine 2.6-rc1 kit works fine.  I also verified that the jython jar contains the PyException class.\nPerhaps your kit was corrupted somehow during download.  Could you try downloading and unpacking a fresh copy?\n. Sorry for the delay in reviewing this.  Where is the source for EditDistanceJoiner.jar?  We don't accept binary contributions without source that we can recreate them from.  Also, what is the license associated with the code.\nI'm going to revert this for the time being until we get these questions sorted out, but we're definitely interested in the contribution.\n. There is no OpenRefine 2.5.  There was a Google Refine 2.5 and we have a release candidate for OpenRefine 2.6 rc1.  The problem you describe is fixed, I think in RC1, but if not, definitely in 2.6 RC2.\n. The current behavior sounds inconsistent, so marking this as a bug.\n. There are probably also people (RTL folk) who would prefer the panel docked on the right vs the left.  There's lots that could be done here from a configuration point of view.  All that's needed is time/effort/money.\n. Unlike GREL, the language syntax is defined by Python, so this isn't an option.\n. We probably need to check to make sure that we're not using the default system encoding here and that we either always export in UTF-8 or allow the user to specify the encoding.  We should include a meta tag with the matching character set, no matter what character set is used.\n. I'll be building another release candidate shortly.\n. The underscore character is used as a placeholder for empty (ie null) names and the column name is constructed as a concatenation of the names for the path leading to the current object/field (since the names aren't necessarily unique by themselves).\nAre you asking for something different to be done?  We've discussed in the past (and I think there might be an issue for it already), removing the root object from the path.  Are you looking for something along those lines or something different?\nWhile brevity is appreciated, including enough words so that we can tell what you're requesting is helpful.\n. Any ones in particular?\n. It's also fixed in RC1 available on the downloads page https://github.com/OpenRefine/OpenRefine/releases/tag/v2.6-rc1\n. This will need to be added to the command line as well as the .ini file.\n@jackyq2015 What is the issue when running on a remote Linux server?  Doesn't it just fail to do anything?\n. The only place JSON has ordering is in arrays.  Iterating through JSON keys will always have an implementation specific order which depends on the hash used.\nCould you describe a little bit more about your use case and why you need the columns in a specific order during processing?  Do you know that you can easily reorder them on export by simply dragging column names around? (If export order is your end goal)\n. We fixed most (all? at the time) of the absolute path references a while ago.  If new hardcoded path references have crept in, please provide a list of them (it should be a short list) -- or better yet, a pull request with the necessary fixes.\nI don't understand what, if anything, running on a non-PLD has to do with this.  I don't know of any place where host name/address matters.  Can you clarify?\n. That's a good suggestion, but, just for clarification, all project state except for facets is saved automatically, so nothing should be lost.  If you want to save a particular facet setup, you can just click on the permalink at the top of the screen.  No plugins are required.\n. This is a duplicate of #133\n. While suggestions about implementation tactics are appreciated, much more valuable to the development team is a clear description of what you are doing, the problem(s) you are seeing, and how you'd like to see things improved.\nHaving said that:\n- 18M rows / 300M cells is far above the design center for Refine, as Martin said\n- scrolling vs pagination, while an interesting discussion, would be a significant UI change and development effort\n- as you scale up to that size, relying on manual inspection of all cells isn't really feasible, so what's needed is more powerful tools to understand the impact of your transform\n. Thanks for the suggestion.  We are open to reviewing pull requests which implement this.\n. The reason this was still pending is because it contained unverified binary files.  I'm not going to revert it, but all the files still need to be manually verified.\n. I've replaced the binaries with those for 3.13 and deleted the POI 3.8 source jar.\n. This is a duplicate of #348\n. Can you expand on this?  I can't tell what you are trying to say/ask.\n. Sounds like it might be a browser-specific problem.  @Opennat What browser (and operating system and version of Refine) are you using?\n. I'm going to assume that you've mistakenly selected the wrong repo.\n. The good news is that there have only been 20 commits (and fewer changes than that due to PR merges) since RC1.  The bad news is that I don't, at a glance, see anything which is a likely culprit.\nWhat operating system are you testing on?  Does it occur on more than one?  One possibility is that it's a build issue for one platform, but I'll need to dig in more to figure out what's going on.\n. I've tested both the Mac and Windows RC1 kits and can't reproduce the problem, so my assumption is that this is something that is Linux (or JVM) specific, not a difference in behavior between RC1 and the current development version.\n. p.s. just to be explicit, on both platforms the last column for the imported project is named DEMOLITION, the same that I see in Excel and there are no additional columns after EST_CONST_COST (ie the next column is named ASSEMBLY).\n. So, is this a real issue or not?  If it's real, it should be fixed for 2.6.  If it's not, we should close it.\n. There are a number of inconsistencies related to null handling.  There are places where nulls silently get promoted to the empty string (\"\") as well as this dichotomy.  I think you'll also sometimes see different results from value.operation() vs operation(value) when value is null.\nRight now we're halfway between a strict, consistent implementation and a totally permissive implementation.\n. Rather than double clicking, the typical UI affordance for this kind of thing is a small arrow/triangle icon on the vertical splitter.\n. Not a duplicate, I think, but definitely related.  This is a relatively simple request, as compared to the other.\n. Probably related to this jQuery issue (which will require upgrading to jQuery 3.0 to get the fix once it's released):\nhttps://github.com/jquery/jquery/issues/2060\nFor now we'll just ignore it.\n. Edit column -> Remove column from the column menu\n. Welcome back to Refine.  If you've got questions, the openrefine mailing list is a good place to get them answered. https://groups.google.com/forum/#!forum/openrefine\nInteger division works the same way it always has.  1 / 2 is equivalent to floor(1/2) ie it returns only whole number results.  If either operand is a floating point number, they both get promoted to floating point and a floating point result is returned.\nYou can use 1 / 2.0 or 1.0 / 2 or 1.0 * x / y (if you're working with variables of unknown contents) to achieve the results you desire.\n. Thanks Thad!\n. Good suggestion and one that we've certainly thought about, but I can't seem to find an existing issue for it, so thanks for adding it.\nIn addition to the original URL/filename, we should also add:\n- a free form description field that the user can populate with notes if they want\n- the import option settings which were used\nThe second is a poor man's workaround to not having the import operation included in our operation history.  Ideally, the project creation operation, along with all its settings and parameters would be just another operation like all the others.\np.s. speaking of workarounds, I just paste the URL at the end of the project name and use that to keep track of where data came from.  Not elegant, but it works with the current system.\n. Thanks for the contribution!\n. A silent error sounds like a bug, not an enhancement request.\n. I've incorporated fixes for most of these comments in be936a86eb77f664341f0dd70b2174f9d1fe8b7c.  I'll discuss with @magdmartin who he had review the changes before he agreed to commit the PR to see if we can improve the process.\n. The recommended way to do this is use Javascript style comments and run things through JSmin before doing the JSON decoding.\nThe one reservation that I have to doing this is that it encourages people to consider the JSON editable which is something that we haven't thought through the implications of.  There are a bunch of things like the client/server wire protocol, various internal data formats, etc which we've left undocumented on purpose so that we have the freedom to change them.\n. Your assertion that JSON arrays aren't ordered is incorrect. \nhttp://stackoverflow.com/questions/7214293/is-the-order-of-elements-in-a-json-list-maintained\nThe reconciliation service is already filtering and ranking the results.   It makes far more sense, at least to me, to order them correctly at that point, particularly since the reconciliation service presumably has the most information available as to what the correct order is.\n. Cool.  Always happy to hear about new reconciliation services!\n. Thank you @rudygt !  I'll review in more detail this weekend.\n. @blacksmoke26 Many of the file formats (those which don't declare character encoding explicitly) allow the user to select the character encoding to use.\nWhat was the original file format?  Did you select the character encoding or just accept the default?  If you provide us with additional information and, preferably, access to the original file, we can look into this further.\n. I'm going to use the wording from July 29 version that I created in b296c29937b20bf3cf7f12b25fc94284615a11d1. I don't see the point of leaving Google Refine references lying around.\n. Thanks for the contribution -- and for including a test!\nIn the future, please review the diff before you submit it and make sure it's clean of changes unrelated to its purpose (e.g. the import reorg that your IDE apparently decided was necessary).  Otherwise we just get noise injected from dueling editor/IDE setups.\n. This is showing the entire file as changed in the diff on Github.  Can you fix that please? (Presumably a line ending issue)\n. @joewiz Thanks for the tip, but we still need a clean patch.  Github is just reflecting what git sees in the repo.\n. Thanks for updating the patch -- and thanks for the fix!\n. The purpose of that check is to make sure that there's a good likelihood that the character being returned is actually a separator as opposed to just frequently occurring random character from the data stream.\nWe're not going to just remove the check, so I'm closing this PR.  Feel free to create an issue which describes what behavior you think is wrong.  I'd need to dig into your example more, but I'm guessing that there is some other underlying issue which is triggering this behavior since, at a glance, the file appears to have exactly 12 semicolons (;) per line which should easily pass the check.\nAs an aside, any time you have the data available in both Excel and CSV formats, you should choose the Excel format because it contains more information.  In this case it correctly distinguishes the strings which look like numbers from actual numbers.\n. I had a closer look at the file and the behavior is being triggered by the first line which contains no semicolons.  This makes the standard deviation just high enough to exceed the threshold.  The file is somewhat unusual in that, in addition to the first unformatted line, it also contains 5 lines of sheet header description and 5 lines of column headings.\nSome possible solutions:\n- always ignore the first line\n- increase the threshold allowed for the standard deviation of separator counts\n- redo the separator guessing when the user changes their import parameters (since the user is presumably going to ignore the first 5 lines anyway)\nFeel free to transfer this discussion to the issue you create.\n. Thanks for the update.  I'm not sure how familiar you are with git, but if you can create separate branches for your pull requests, it minimizes the chances that two unrelated changes on your master branch will interfere with each other.\nThanks again for all your help with translating! \n. @dbolser-ebi Glad you got your question answered, but please using the mailing list / Google Group for future questions so that we can reserve the issue tracker for actual problem reports.\n. @danpaulsmith What version of OpenRefine was your testing done with?\n. @kafran That doesn't sound like the same problem.  How do you know there are projects?  Do they appear when you use a different browser? What operating system are you running on?\nIf you're still having the problem, please post to the OpenRefine mailing list with details of your configuration and we'll see if we can get you sorted out.\n. @magdmartin Who reviewed this before you merged it?  I'm happy to have you merge pull requests for translations, READMEs, etc, but you don't have the technical knowledge to review code contributions.\n. @FigaCZ Can you please clarify what version of OpenRefine you are attempting to run and which OS X 10.11.1 Beta you are running (there have been four so far).\nMy first inclination, if OpenRefine runs on released versions of OS X, is to report this as an OS X bug to Apple.\n. Thanks for the PR.  Currently our build/run setup is a weird mix of command shell and Ant.  The run stuff is all currently in the refine / refine.bat scripts.  Is there a specific need to move this piece from the shell scripts to the Ant file?  Note also, that the shell script does a bunch of additional processing.\nWhile I'm not opposed to merging this, I'd prefer to make a single concerted cleanup pass through everything (which might also mean switching to Maven instead of Ant).\nAs an aside, it's preferable to have the PR on it's own branch (e.g. thadguidry-patch-1) as opposed to master, so that it doesn't accidentally get polluted with later changes that are unrelated.\n. Thanks!\n. Thanks for the contribution.  Can you explain a little bit more about the motivation/use case?  Does this match up with an enhancement request in the issue tracker?  Some use case of your own?\n. Thanks for the contribution.  I'm not sure we've got the headroom to include ICU4J and still make it under Github's 100MB file size limit for the Mac kit, but I'll have a look.\n. The issue is being caused by whitespace between tags and a code path that the \"trim whitespace\" flag doesn't effect.  With that fixed, turning off \"preserve empty strings\" (on by default) and turning on \"trim whitespace\" (off by default), the XML import generates a much more compact table.\nWe changed some of the import setting defaults in 2.6 to disable transformations which aren't reversible, but XML whitespace is kind of a special case, so I'll take a closer look to see if there's a better way to fix this.\n@ettorerizza Your file imports for me, but most of the populated data columns are off to the far right of the screen.  In other words, it behaves the same as Tony's example.\n. I revisited this and have a better solution, but there is an issue that I'm unsure how to deal with. Without a DTD or XML Schema, the parsing mode that XML parsers use is the \"Mixed\" mode where an element can have text, nested elements, or both. If there's a DTD/Schema and it says that an element is not a mixed element, then any text can be discarded by the parser. Without a schema, there's not way to distinguish pretty-print white space from element text.\nWe can drop all whitespace-only strings that occur in mixed elements, but I've got a nagging concern that this may cause other issues. Am I being overly paranoid or do we need yet another toggle to control this?\n. Enhancement requests are always welcome.\nOKFN (cc @rgrp) has suggested we support the Data Package standard (http://data.okfn.org/doc/data-package), which, as I understand it, the W3C work was based on before they diverged.  They have funding from some foundation to help support this.\nI know the W3C likes to make things all standardy and \"improved\" with their professional standards makers, which sometimes puts them in conflict with more pragmatic folks (vis WHATG), but in this particular case I haven't dug into it to see whether OpenRefine should be picking one to support, support both, or waiting until the pissing contest is over and there's a clear winner.\nI don't know the whole backstory, but when I read minutes of a meeting where the original author isn't in attendance and everyone else votes to dismiss all his suggestions, I wonder what's really going on behind the scenes.  http://www.w3.org/2015/09/23-csvw-minutes.html#res\n@psychemedia Have you seen anyone else consider supporting the W3C standard if it gets approved?\n. Please use the mailing list / Google Group for questions.  https://groups.google.com/forum/#!forum/openrefine\nBrowsers aren't a very good testing mechanism because they can have all kinds of cached cookies, authentication tokens, etc.  A utility like curl will give you a better indication of what's going on.  My first guess, since you say it's HTTPS, is that the API requires authentication, but if you ask on the mailing list with more details about what API it is, exactly what URL you're hitting, etc, we can get it figured out.\n. So Chromium works and Firefox doesn't with the same Refine installation and no other changes?  That's very weird.\nWhat version of Java do you have?  Can you cut & paste the text of the traceback?  The screen capture is nice visually, but it's much easier to work with the text in debugging tools.\nIf you're comfortable using the browser debugging tools, comparing the network traffic for the two browsers to see if they're the same would be interesting.  Also see if there are any warnings/errors on the Chromium debugging console.\nThe reason Firefox is unhappy is that it's expecting JSON and we're sending HTML, which we shouldn't be doing, but I suspect there's a lot more to it than that.\n. Sorry to hear you're having difficulty.  Can you provide more context on what you are doing and what the environment is?\nWhat version of OpenRefine? What version of Windows? What browser and version? Is this something you're trying to do interactively? If so, what menus, choices, etc are you selecting? If not, can you post the JSON for the script that you're pasting into OpenRefine's history window.\np.s. Anything you add to refine.bat has no effect if you don't actually run refine.bat\n. Additionally, the select() function returns an array even if there's only a single element, so you'll need an array index.\nvalue.parseHtml().select('body')[0].htmlText()\n. Do you have an example file that you can provide which demonstrates the problem? That would make it easier to debug.\n. @zacharynanfelt Welcome to the community! Thanks for validating that this works now.. Thanks for the contribution. It's always great to have a new translation!\n(I've disabled the AppVeyor build until the setup is complete).\n. Thanks for the pull request. Is there an issue associated with the bug that this fixes? What's the issue number?\n. Glad you figured it out!\n. There is so much missing here.\nWhat version of Refine?\nWhat operating system and what version of it?\nWhat were you doing when you got the error? (step by step)\nIs the error reproducible (ie it happened more than once)?. @felixlohmeier Thanks for the summary. Do you have links the the original sources of the quotes? Was it one of the mailing lists or another issue?\n@jackyq2015 The referenced commit appears to be to master, not a branch. Does it resolve this issue? Who reviewed it? (Apparently Thad has reviewed it post-facto based on his typo comment). \"Thad\" being @thadguidry, of course (so he gets notified). What's the issue # that this PR is addressing? Was it reviewed by whoever approved the original bad merge?. There's what looks like a copy of the original project here: https://github.com/joshmarinacci/AppBundler\nand a fork which has some improvements and actually looks like is maintained here: https://bitbucket.org/infinitekind/appbundler/. In my mind the ordering is: 1) creators, 2) current contributors, and 3) all past contributors. I think 1 trumps 3, and David and Stefano (certainly David, since this project builds on much of his previous work at CSAIL etc) deserve pride of place as creators.\nI'd probably reorganize this as two lists: 1) current contributors and 2) emeritus contributors with creators having special mention up front.. LGTM. The diff is massive (basically the entire file), but I reviewed the license text and it is unchanged -- just reformatted.. I'm a little surprised the fix doesn't include some additional unit tests to make sure that this type of regression doesn't happen in the future.. This PR doesn't appear to include any help or documentation for the users. That would both help review it and help the eventual users. Is it located somewhere else?. There's a whole pile of modules which, without reviewing them in detail, sound like they're generic Wikidata things (e.g. org.openrefine.wikidata.schema, etc.)\nIs there a reason that a generic Wikidata library couldn't be used instead? e.g. https://github.com/Wikidata/Wikidata-Toolkit. Butterfly isn't our project, so I'd recommend that the version numbering make it clear that it's a private fork, in case they ever do a 1.0.2 (unlikely, but still not a good practice to make a possibility -- and yes, looking back at the commit history, I realize I did it wrong 6-7 years ago).\nThe internal implementation leakage to extensions is a much bigger issue, but if most/all of them will be broken by this change, it'd be a good time to put a more formal interface in place so that doesn't happen in the future. The Butterfly class loader is one piece of that, but clearly it's not sufficient.\nIf an extension gets broken by the JSON serialization change, I'd argue that that indicates that it's not well enough isolated and the solution isn't to make the abstraction even more leaky, but, rather, to force the extensions to provide their own JSON serialization, unless it's a facility provided by OpenRefine through an opaque API.\nThere's not a lot of context on #1882. Is there an email thread or something else which has more detail?. @wetneb Sorry for the delay in replying. The changes that you've made to eliminate 3rd party (ie org.json) interfaces from the extension interface are definitely headed in the right direction and that's exactly what I was talking about. You certainly can't be faulted for interfaces which were defined years before your involvement in the project and I'm sorry if you got the impression I was blaming you.\nWas any investigation done into a clean room implementation of the critical pieces of the org.json APIs? I don't know how much work it'd be, but that'd be one way to finesse the problem if compatibility is paramount. It would mean hijacking their namespace and would still require a recompile, but might be worth considering depending on the number of extensions, how hard they are to update, etc.\nBreaking the API will make extensions incompatible, whether it's a big break or a little break, so it makes sense to take the time to get the new API right and do all the redesign at once, in my opinion. Making a series of smaller breaking changes, just makes life more difficult for the extension developers.\nAs long as you are breaking the API, it makes sense, I think, to try and tighten things up so that you're not leaking implementation details. Annotations defined by a 3rd party are effectively part of the API and represent an abstraction leakage.\nJacky's interpretation of the class loader structure is correct, as far as I understand it. Isolation of extensions from each other is one of its goals, and, I suspect, that reason that the class loader hierarchy is \"wrong\" (ie inverted).\nMy recommendations, which I, unfortunately, don't have time to test out right now are:\n- evaluate an API compatible org.json workalike if the surface area of the used API is small enough\n- if that's not feasible, make a clean break and don't expose Jackson through the API\n. @thadguidry Can you provide a pointer to where this consensus was built/documented? The link that Antonyn provided basically said \"I'm doing this\" without any discussion, agreement, or disagreement.\nI'll try and find time Thursday to send a note to the dev list where things are more visible. I'd hope that any casual extension developer could search for \"breaking extension incompatibility\" or something similar and find the relevant proposals, discussion, and decision.\nre \"breaking some compatibility with some extensions\" [emphasis added] - what extensions would not be broken? Are there any?\nre \"previous merit\" - I'm not familiar with that phrase. Definition please?. Yes, please always use blocks with conditionals.  Leaving bare statements is error-prone.\n. Consider using Guava's Lists.reverse here.  ie \nfor (Change change : Lists.reverse(_changes)) {\nWe use Guava already so its (very handy) convenience functions are available for your use.\n. Dropping support for an environment should be called out explicitly rather than hidden in the patch.  We'll probably replace this with openjdk7.\n. This PR was merged 4 days ago.  Please open a new issue (or better yet, pull request).\n. Why is this dismissing a single level partway down the stack instead of unwinding everything?\n. Is this needed in addition to the front end change to show the busy dialog or is it redundant?\nWhile it doesn't make sense to have multiple operations per client, there may be multiple clients, so we don't want this single threaded unless there are real synchronization issues to protect against.\n. This should call the constructor above (Line 11) rather than duplicating code.\n. missing new line\n. Removing deprecated methods shouldn't be included in this patch.\n. If this is used in both Tabular & Tree imports, is there a reason it can't be hoisted to ImportingParserBase?\n. Conditionalizing this doesn't fix the underlying issue which is that the internal state doesn't track the state of the checkbox.  Instead the fix should be applied at line 222 by using this.checked as the value instead of !cluster.edit.  The current implementation is too prone to errors like this.\n. Sorry for the delay, but I wanted to make sure I wasn't missing something since synchronization is something that is non-trivial to get right.\nIn this case, I don't think this is going to gain you any advantage.  The Engine objects are created per HTTP request using the serialized facet description which is passed in with the request.  Since synchronized methods use the object's default lock, it's only protecting against multiple invocations from within a single HTTP request which would only be as the result of a particularly pathological bug in our code.\nI can't ask you to revise the PR since @magdmartin already merged it, so I'll back out those changes by hand.\n. ",
    "thadguidry": "1277 Is being worked on to support this issue !. @wetneb Does this happen on your working branch any longer with Wikidata new parts ? If not, then let's close this out noting so.. @jackyq2015 @wetneb @ostephens @ettorerizza Read through this issue... I think David Huynh has a good point, and this would be a very useful new GREL function.\nImagine a value...\nYang,David,John,Charlie\nand doing GREL\nvalue.split(',')\nproduces\n[ \"Yang\", \"David\", \"John\", \"Charlie\" ]\nand by adding a new command\nvalue.split(',').toColumns()\nthe preview window will instead show\nCreating Columns:  [ \"Yang\", \"David\", \"John\", \"Charlie\" ] \nand then clicking OK on Transform window it executes the existing core/column-split command for those rows that have valid Arrays (non-error rows).\nAdditionally, the new toColumns() can also take parameters just like it does on the Edit columns -> Split into several columns\nThoughts ?\n. This is solved already quite easily now in 2017+.  Here's how...\n@ettorerizza Most folks just use a Apache Stanbol - Docker image and send a Fetch URLs command to the Stanbol Socket URL endpoint.  https://hub.docker.com/search/?isAutomated=0&isOfficial=0&page=1&pullCount=0&q=stanbol&starCount=0\nHere's one that works for your use case I would think quite well. Even though its Drupal mention, its the same use case...provide a string of tags or entities extracted from some larger text string.\nhttps://www.drupal.org/project/auto_recommended_tags\nHere's some docs to get started once you have your instance running from Docker... https://stanbol.apache.org/docs/trunk/enhancementusage.html\nIf someone really wants tighter integration with OpenRefine, then an Apache Stanbol extension could be made (running in same Jetty instance on different port than OpenRefine)  They could also provide a Wikidata enhancement chaining as a default package.  Actually, I think someone has already wrote that enhancement somewhere if I recall, perhaps a Wikidata dev already or some Github project.\n. @ettorerizza Ah yeah, that's the one :). @ostephens Yes there are extensions to OpenRefine that handle this.  Closing this issue.. This should be handled by an extension.. I've started on this and will fully convert the Ant build scripts to Gradle.  Those with Eclipse IDE can install the newest Buildship plugin as a precursor to my PR coming in the next few weeks.. @wetneb Actually, I'd rather baby step this... lets move to Maven first.   Can we use what is here and tweak to get the Maven build working properly ?  (that way, I can later convert from Maven to Gradle much easier)\nAnd the other worry I have is... I don't have a Mac. :( to test on that side of things.  Since we need our whole team involved on replacing our build strategy, I'd rather just use something we are all familiar with to start with...Maven.. This should be handled by an extension.. Hi @answerquest we'll be eventually changing our UI to a more modern framework, quite possibly based on React, and so extensions will have to be re-written.\nHaving said that, this need you have to make it easier to export to a map should be handled by an extension.  We won't add this to our core, mainly because there are many kinds of mapping applications and endpoints that could be utilized, so its best to have extensions handle that where the community that is interested in their particular Mapping need, can directly contribute to those extensions and enhance them, without disturbing our core.\nIf someone wants to work on this extension need, my suggestion would be to begin in 2019, since this fall/winter 2018 we plan to begin changing quite a few things.. @wetneb close and mark as duplicate.... its sorta a duplicate...well close enough :) so mark it as a duplicate :) :)  That's how we say \"related\" in Github :). @ettorerizza Ya found the issue ! haha. @wetneb Do you know if this functionality is working against Wikidata or any standard reconcile service, since Freebase is now deprecated and you removed some legacy Freebase stuff from our code ?. @wetneb awesome.  Thanks for your input. Closing it.. @eswright Hi Eric, yeah, we have a bug in our column counting strategy, as @tfmorris hinted at above, and its never been found yet because of our lack of time, sorry about that.  But for now, you could just MANUALLY parse the data by selecting the upper most outer record set element visually with the Json Importer...then you'll get some rows...and then manually parse those with Add new column using GREL parseJson() function as our Wiki has a few recipes and examples.  That will probably get you by for now with nice record rows.  Hopefully we can find time to work on this issue more by the end of the year.  Having more example data that causes the issue would be EXTREMELY HELPFUL !  If you can upload your JSON file somewhere and email me the link, then I can analyze more for you.. @eswright Thanks Eric.  We got the linked file you sent privately to us.  Yeah, I see a problem, where not all the objects in the large array get created in OpenRefine 2.7 release.  I've sent this to @jackyq2015 as well.  Stay tuned over the next few weeks while we work on the issue and a fix.\n@wetneb Thanks Antonin, that file will help us also !. @jackyq2015 I'd say this needs to be our highest priority issue at the moment, it looks like we're throwing away records of data during import. (or not parsing them out) Not good.. Closing as unreproducible. Data files needed for investigation.. We have this basic functionality with parseJson() now.  So closing.\n\n. REOPENING ISSUE:\nI'm an idiot.  Now I remember the use case... TO RECORDS !  So yeah, a new parseJsonToRecords() with an Add New Column.  And probably we want to have a sanity check and new data type for JSON, like we do with Date, Numbers, etc.. This issue can be handled by taking advantage of the existing Column Splitting command as @dfhuynh mentioned in #36 \nSo anything that produces Arrays ... can be automatically split into Columns / Records, including a new parseJsonToRecords(). @ostephens Yeap, your right !  startsWith and endsWith was for simplicity, and they are useful for quick matching without bogging folks down into learning regex.  Let's keep those two simple, agreed.\nSo closing this issue.. Yes, please close.\n. OpenLibre supports importing DBF files and then saving as CSV.\nClosing issue as will not support, since there is a pretty easy open source workaround.. Closing as unreproducible. @jackyq2015 has fixed #137 where the XML import was improperly merging records.. @ostephens yeah it is sufficient, that was @iainsproat use case many years ago, we solved it that way on the mailing list.  Closing this. And thanks for navigating through these old issues to help close them out.. @ettorerizza There is now a robust Java Bibtex parser https://github.com/jbibtex/jbibtex. @jackyq2015 @wetneb Don't we already handle this ?  Or only on import and we need to do more work to support giving options for changing encoding on export ?  This is an old issue and I feel we pretty much give most users what they most need regarding encoding support.  So I think we can close this, Yes ?. #1277 is being worked on to handle this issue now ! :) \n1277 will also handle the case of issue #12 MySQL Support . @stevevance we already have export as CSV which you can load into any database. This feature was to connect to a database and perform a CREATE TABLE with the selected columns in OpenRefine.  Is there something more than that feature you need ?  Let us know how you envision it working.. @stevevance Right, so you'd like a nice dialog that let's you set all those column options, AND even an option that says \"Set the selected Datatype on all columns\" :). @tcbuzor I think it makes sense to have the dialog expose a dropdown for the Database vendor, and then another showing the enumerated list of Datatypes supported by that Database vendor, along with the option to check to make ALL selected OpenRefine columns to be set as the users selected SQL Datatype.  We can reuse part of our existing export dialog that currently exposes the individual Columns options.. @tcbuzor CREATE statement, and truncated to first 30 bytes (whitespace collapsed) of the OpenRefine Project name.  . @tcbuzor Oracle < 12.1 has a table name limit of 30 bytes.. @tcbuzor Yeap, column name max length is 30 bytes if its Oracle < 12.1. @tcbuzor I would also add text somewhere explaining what goes into the input boxes.. @tcbuzor Should that say instead \"Include Schema\" ?  Wondering if \"Include Structure\" might confuse folks, because its kind of an Oracle naming convention, rather than universal.  Also re-read my comment above, I've updated it (don't use hover text, just text somewhere \"input max values into boxes as necessary\" or some such). We delivered the basic SQL Export functionality in OpenRefine 3.0 ( Thanks @tcbuzor ! )\nAdditional Features can be requested by opening a New Issue.. @wetneb Is this fixed now, where Freebase is not mentioned any longer ? If so, let's close this.. Closing as this is out of scope for our project. OS specific and other tooling exists to support running programs as a service.. Thanks for squeezing that in Tom.\n. @wetneb yes close, and besides, this was Gridworks days at that time and some of the importers like CSV , XML, and JSON were upgraded afterwards.. No longer need this fixed. Closing.. Tested with newest Internet Explorer Edge on Windows 10.  Export Excel XLS and Excel XLSX works with a new blank tab automatically opening up and OpenRefine tab blinking and highlighting that there is activity to see.  Clicked on OpenRefine tab and saw the download prompt at the bottom of browser window.\nHowever I did confirm that there is another subtle issue with Apache POI error that seems to come up sometimes #1185 when trying to export an XLSX file.. Yeap this is a duplicate.\nClosing this one since #68 describes the need very well and discusses options.. Not enough detail. Closing.. The idea of Macros plays here, which OpenRefine does not have, but basically begin recording a Macro (a set of operations against 1 column) that you can then Replay over any number of columns of your choosing)\n. @magdmartin Agreed, overall it needs to be highly intuitive and easily selectable for various rearrangements.  There's plenty of implementations of Jquery plugins that would help our cause with this issue. http://plugins.jquery.com/?s=table\n. Yes @ostephens , I think it does, so closing this one.\nSupporting Macros is in issue #109 . Fixed in #1539 and #1598 . This seems to be a muted Error now, after Thad and Jacky have looked into it.  We no longer have the forever spinner, but we probably should be throwing an Error, since it seems as if the Sort worked, but in fact, it does nothing with mixed text/number/date values and asking to Sort by Date.. Closing. We will no longer support IE8 (or its tricks). Browsers have been updated significantly now in 2018.  Closing.. We have a nice XML importer with UI selector capability.  Closing this issue.\nIf folks want/need an Xpath expression box on the importer itself, they can open a new issue for that.\n. @wetneb Close this one.  There's additional work that will eventually happen in the java 8 refactor branch along with the StringBuilder replacement.\nAs for #322 There are little things around general code cleanup that will be done, most of those come from hints from IDE's and code coverage tools like Jacoco, etc ... all of which we plan to do systematically in several phases.  Some of which is hinged on the performance enhancements first I think.\nI would prefer that we tag more issues into our 3.0 Milestone and mark them with several Parent issues that have a label or Milestone.   The individual tasks in the issues are marked as Github Task Lists.  Basically, just start organizing all these old issues and as well as the cleanup that your doing (and thanks for that!).  You can create more Labels as well.  Here's how to do task lists. (there's a button at the top of the issue editor) where you can then check the box to mark as done !\n- [] This is first task\n- [] This is second task\n- [ ] This is first task\n- [ ] This is second example task for #322\n. Fixed in #1539 and #1598 . Should we also support the one-line format for wiki tables ?\nhttps://en.wikipedia.org/wiki/Wikipedia:Advanced_table_formatting\nThe wikitext for any one row can be compressed onto a single line by joining columns with double-bars \n\"||\" between them and ending each row with \"<tr>\".\nExample row 1: |fmtspec|AA||fmtspec|CC||fmtspec|EE<tr>\nExample row 2: |fmtspec|BB||fmtspec|DD||fmtspec|FF<tr>. This is still open ?  Gosh, I need to start triaging our old backlog of things we (OpenRefine contributors) will NEVER DO BUT THANKS ANYWAYS.. Freebase is gone.  Closing.. @tfmorris Well coming back to this after a few years. :)  We definitely need 1 enhancement here on the Export -> Custom Tabular Exporter\n1. The issue is that we did not give the user an additional checkmark option to output With Quotes or Not.\nWe need to give the user this option.\nI ran across it today with a pure line input format... SQL  and just 1 column to hold all the lines of SQL text.\nI did a few transformations on a few faceted lines for each TABLESPACE and then went to use Custom Tabular Exporter to export just pure lines from Column 1.... but...sigh....\nGrid View:\nCREATE TABLE \"VT1\".\"ACCOUNT_PROCESS_MONITOR\" \n   (    \"OPERATION\" VARCHAR2(10 BYTE), \n    \"BATCH_NO\"\" NUMBER(7,0), \n    \"PROCESS_COUNT\" NUMBER(4,0), \n    \"CREATED\"\" DATE\n   ) SEGMENT CREATION IMMEDIATE\nCustom Tabular Exporter output:\nCREATE TABLE \"VT1\".\"ACCOUNT_PROCESS_MONITOR\" \n\"   (   \"\"OPERATION\"\" VARCHAR2(10 BYTE), \"\n\"   \"\"BATCH_NO\"\" NUMBER(7,0), \"\n\"   \"\"PROCESS_COUNT\"\" NUMBER(4,0), \"\n\"   \"\"CREATED\"\" DATE\"\n   ) SEGMENT CREATION IMMEDIATE\ntons of extra quotes that are not needed and no way to get rid of them with any of the options in Custom Tabular Exporter.\nSo then tried to use Templating Export as a work around with....\n\nbut get an extra null for blank rows.\n2. So even Templating Export needs a checkmark option for omitting blank or null rows as well.\n. Closing this issue as it is pretty much resolved now after move to OpenCSV 4.0 and the new Importer UI options now in place ... and so will open new issue for the Templating Export enhancement I mentioned to support conditional Quotes or not and excluding blank or null rows.. ICU4J could be used to provide normalize() and transliterate() commands The most recent bikeshedding discussion is here on our mailing list: https://groups.google.com/d/topic/openrefine/6hY1-GI0KAI/discussion. Agreed, I've tested what needs to be off by default in the Line importer, seems to work OK.  One area that I DID NOT test heavily for appropriate defaults was the JSON importer.  So we need to test that one, thoroughly.\n. Errors for storing Arrays in cells is troublesome and should notify the user that it is not possible #1088. @stevenqzhang The VIB-Bits plugin is what most folks use to solve this issue.  In fact, we could probably close this issue, since the general use case is handled nicely by the plugin.. We might eventually want to have OpenRefine directly support these VIB-Bits functionality for\n- executing history steps from other projects \n- re-execute history. #1870 is a request to exclude rows that blank values from the template.\n470 is a request to allow to choose the filename for export templates. #468 gathers together all Export Template enhancement requests.. @OpenRefine/core I'd like to see us finish off what David started on this old issue and connect the encoding guessing code and ensure it populates the Character encoding field in the UI during import preview.  We might want to increase our max rows read to 1000, instead of the default 100 for the guessing. ( I recall David and I talking about that because of a few real-world files I ran across, but don't remember specifics other than the guessing was sub-optimal with fewer rows read on those files. I no longer have the files.). This needs review to see if the bug indeed still does exist in 3.1 now.  Someone should just write a test case around this issue and then fix if the issue comes up.  @ostephens wanna hack on this and take your OpenRefine coding skills to the next level ? :)\n. @wetneb would you say this is resolved now ?. @wetneb I noticed this https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/model/Cell.java#L85 and is there anything more needed to suffice for Tom's need on this issue?  If so, can you add some implementation details of what is further needed here?\nUPDATE: Do we have more than two fields on cell now? Ah, no we don't, so that needs to be added.  Then our docs updated https://github.com/OpenRefine/OpenRefine/wiki/Variables#cell . @wetneb Can you perhaps provide a quick fix for this issue over the next 2 weeks ? . @wetneb Thanks, this is Google Sponsored actually via the Wikidata Reconciliation subproject we have for Phase 1, still ongoing (still money left), so track your time.\nAnd yes, a different screen is ideal if not to much trouble. That's what the Freebase Judgement screen did hosted by Freebase Apps back in the day.  Its too bad we didn't have the foresight to save that App before it got destroyed by Google, such as what Spencer did with his apps. @ettorerizza Yeah, we know... Jackson library does fine for parsing it out... the problem is the Record row rendering is slow.  I'll mark this has higher priority to look into and fix eventually.  Thanks Ettore !. For whomever works on this... Here's a good primer, since OpenRefine now uses java.time.offsetdatetime https://stackoverflow.com/questions/21242110/convert-java-util-date-to-java-time-localdate\nOur code for the GREL toDate() function is here: https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/expr/functions/ToDate.java. Summary:\nI think I would choose the format option to make it painless for our users (we do the hard things, to make it easier for them). \nErrata:\nWhile it is painful and complex within our toDate() function, this is necessary to keep GREL syntax nice and tidy. This is also how other programming languages handle this convention, like Javascript (Moment.js), Python, Pandas, Ruby.  Our toDate() already has an optional format argument.  Where I could see this leading to is possibly extending to handle a 3rd argument for timezone or utc.\nMoment.js does things so beautifully and should be our model https://momentjs.com/docs/#/parsing/unix-timestamp-milliseconds/\nRuby uses time.at  http://ruby-doc.org/core-1.9.3/Time.html#method-c-at\nPandas uses Pandas.Timestamp  https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.Timestamp.html#\nWe're not looking to completely incorporate all methods in Ruby and Pandas.Timestamp into our toDate().  Those can be handled by Python/Jython instead of GREL.  But for GREL, it makes sense to at least have a common Unix timestamp convertor built into our toDate() for our users.\n. @ostephens Then don't worry about timezone at this time.  Just use something like:\nDate date = Date.from( Instant.ofEpochSecond( timeStamp ) );\nand we should be good.  That will give us the UTC/Greenwich time.\n- [ ] Also ensure that we document on the Wiki this very important footnote:\n\nOpenRefine internally uses java.time which converts a Unix Timestamp using The Java Time-Scale.\nThe Java Time-Scale divides each calendar day into exactly 86400 subdivisions, known as seconds. These seconds may differ from the SI second. It closely matches the de facto international civil time scale, the definition of which changes from time to time.  More information here: https://docs.oracle.com/javase/8/docs/api/java/time/Instant.html#Time-scale\n. @jackyq2015 In that case, yes, we can close.\n. Not reproducible any longer.  This seems to be fixed in OpenRefine 2.6 trunk\n. HDF5 supports metadata ...  https://support.hdfgroup.org/HDF5/Tutor/HDF5Intro.pdf\n\n\"The Split File I/O driver enables HDF5 metadata to be stored in one file and\nthe data in another.\". @ostephens Yeap !  That use case is one of the reasons for us adding the new find() !\nI'll add this example to our Recipes.\nTo find Proper Names using the full Latin Character set would be:\nvalue.find(/([A-Z]([a-z]|[\\u00C0-\\u00FF])+)\\s([A-Z]([a-z]|[\\u00C0-\\u00FF])+)/).join(\" | \")\n\n. Recipe added:  https://github.com/OpenRefine/OpenRefine/wiki/Recipes#finding-proper-names-or-people-names-in-a-string\n. ICU4J could be used to provide normalize() and transliterate() commands The most recent bikeshedding discussion is here on our mailing list:  https://groups.google.com/d/topic/openrefine/6hY1-GI0KAI/discussion\n@frodeseverin Could you share your thoughts on that mailing list topic discussion please ?. The description should fit what the actual underlying code is doing.\nSo what is the code actually doing with the Output blank rows option ?\nOn Mon, Jan 14, 2013 at 11:27 AM, Tom Morris notifications@github.comwrote:\n\nA lot of places in Refine treat cells which are null, contain the empty\nstring, and contain a string of whitespace equivalently, although I'm not\nsure such a loose interpretation is always desirable.\nAnyone have opinions on whether we should change the behavior or the\ndescription here?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/651#issuecomment-12229239.\n\n\n-Thad\nhttp://www.freebase.com/view/en/thad_guidry\n. I like the wording of those 3 options.  Makes more sense to me.\n- Thad\n. Steps to reproduce:\n1. Increase Refine memory in order to large a large CSV file.\n2. Use a large CSV or JSON file and load into Refine.\n3. Save project.\n4. Decrease Refine memory to default 1400mb or less to cause error to occur on project loading.\n5. Restart Refine.\n6. Click to open the large project and notice large spinner and then error in console.\nEven now with version 3.2 , we still don't show to the user in the UI that a Memory Error has occurred on Project loading, and we could and should within the context of /command/core/get-project-metadata\nPerhaps add more robust error handling within Refine.reinitializeProjectData or onLoad() ?\n12:03:45.988 [                   refine] GET /command/core/get-project-metadata (11ms)\n12:04:11.538 [          org.mortbay.log] Error for /command/core/get-project-metadata (25550ms)\njava.lang.OutOfMemoryError: Java heap space\n        at com.fasterxml.jackson.core.sym.CharsToNameCanonicalizer.makeChild(CharsToNameCanonicalizer.java:315)\n        at com.fasterxml.jackson.core.JsonFactory._createParser(JsonFactory.java:1345)\n        at com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:899)\n        at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3004)\n        at com.google.refine.model.Row.loadStreaming(Row.java:225)\n        at com.google.refine.model.Row.load(Row.java:204)\n        at com.google.refine.model.Project.loadFromReader(Project.java:218)\n        at com.google.refine.model.Project.loadFromInputStream(Project.java:177)\n        at com.google.refine.io.ProjectUtilities.loadFromFile(ProjectUtilities.java:157)\n        at com.google.refine.io.ProjectUtilities.load(ProjectUtilities.java:118)\n        at com.google.refine.io.FileProjectManager.loadProject(FileProjectManager.java:249)\n        at com.google.refine.ProjectManager.getProject(ProjectManager.java:505)\n        at com.google.refine.commands.Command.getProject(Command.java:175)\n        at com.google.refine.commands.project.GetProjectMetadataCommand.doGet(GetProjectMetadataCommand.java:53)\n        at com.google.refine.RefineServlet.service(RefineServlet.java:182)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n        at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n        at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:923)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:547)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212). @biiip There has been discussion before from folks that work with this sort of workflow.  But the team really needs a better understanding of what this involves.  Here is our current understanding of this issue ...(what we do in OpenRefine to make things easier against the following needs, we can follow up on, but first an understanding of the workflow and needs) ...\n\nUser wants to add or append additional data to an existing OpenRefine project. (perhaps because the data was too large and was split into multiple CSVs or its new data for a project that came in because the data set grows over time for various reasons)\nThe User might like to perform on the newly added data rows, the Undo/Redo set of operations (set of batch operations) that they did on the previous existing rows in the OpenRefine project.\n\nNow the approach we might take to make things easier for the above...\n1. We could add an Append importer.  This new importer would add new rows (and tag them with a NEW flag, not the star or flag, to keep track of them for further operations if the user desired in 2.\n2. Have a new function or button option in the Undo/Redo dialog or elsewhere that allows to \"Redo all operations on only NEW rows\".  The operation by default will run all checked operations on the NEW rows.\n. @wetneb Just to be clear, Its a whole approach.  That is, 1 and 2 are considered a sequence of actions.  (1 and 2 are inclusive, we add an Appender AS WELL AS provide a new function to Redo operations on the tagged New or Appended rows.)  That OK ?\nUPDATE: and of course their might be other operations to perform or apply on those NEW rows.  The important part is when adding them we Mark them as NEW so that we have context to do whatever we want or an extension wants to do with those NEW rows.. @biiip There is other more important work that we have scoped out.  This would be a Medium priority...unless we get like 50 folks that +1 this issue :)  Which seems hasn't been the case in 4 years.  The other work surrounding this would be around our data model alterations we have planned (and some started with our metadata support and data package support), but as @wetneb says its better to wait and let us separate our backend and frontend 1st which we have planned for this year (hoping summer).\nThe work estimate is probably about 40-80 hours for a single developer once we have the backend separated out nicely and comfortable with operations (this would be an Update operation in CRUD essentially). A refresh of the browser page might make things useable again, but if a background operation is still pending then the state might not be valid anymore. \nImprovement Idea: Long-running operations are distinctly defined by OpenRefine and we don't currently have a default timeout (I think) for Long-running operations or a way to quickly let users know...\"Hey, this might take a while...ok?  If this takes too long, click CANCEL on the Spinner dialog that comes up to force cancel the operation\".. getField() is not intended to be used that way.\nIt is faster and easier if you instead select ONLY the row or rows you need from the Record array.\nThis gets only the 2nd row of the Record array\nrow.record.cells[\"Column 1\"].value[1]\n\nIf you want to get multiple rows then just use Array slicing\nThis gets the 1st through 4th row of the Record array\nrow.record.cells[\"Column 1\"].value[0,3]\nThis gets the 2nd through 4th row of the Record array\nrow.record.cells[\"Column 1\"].value[1,3]\n\n. @wetneb Can you work on a fix for this over the next 2 weeks ? (invoice-able work). @ultrageek please email our mailing list so we can help further.  https://groups.google.com/forum/#!forum/openrefine. @jackyq2015 is this fairly well RESOLVED now, in our present ways of releasing ?  Can this be closed now ? or still an issue for Mac Kits in particular ?. @jackyq2015 Can we close this ?. Hmm... going to have to add Tom Morris to my Google+ category of \"EXTREMELY COOL FOLKS THAT GIVE ME  - HAPPYNESS\"\n. @wetneb So what's left to deliver the features on this issue ?  I'd like to see a Task breakdown please with linked issue numbers on each task remaining.. It's the Trunk private build.  I'll try to clean... thought I did that, but then now that I think about it...perhaps not.  On work laptop.  Will try tomorrow.\n. My goof.  Clean and Build did not occur on the work laptop as I thought.  Once I did clean/build, it is working correctly now.  Closing this invalid issue.\n. Jacky\nCan you add those comments directly to #796 issue ?   Thanks.\nBTW, Markdown is supported on comments in Github if you want to prettify\nthat alignment - https://guides.github.com/features/mastering-markdown/\nThad\n+ThadGuidry https://www.google.com/+ThadGuidry\nOn Tue, Sep 22, 2015 at 6:10 PM, QI notifications@github.com wrote:\n\nThis is the result I got after applied the patch made by @ultraklon\nhttps://github.com/ultraklon\nId a b c d\n1\n2 1 1 3\n3 2 4 5\n4\n5 3 2 5 3\nBut when I export the csv, It's fine(only 3 lines). Maybe it's because the\nempty line were removed when exporting.\nSo It's not a completely fix me.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/796#issuecomment-142447735\n.\n. @jackyq2015  still not fixed completely. See #1098 \n. We just need to add support for the new Freebase /reconcile service on googleapis\nhttps://developers.google.com/freebase/v1/reconciliation-overview\n\nAnd ensure that a OpenRefine user has a dialog box where they can input their API key.\n. b) self host in Refine.  I already spoke with David H. today about that actually and he also thought it was a good idea.\n. Martin, The design should support the reconciliation service to be OFF by default, and enabled as an OpenRefine preference.\n. I have updated the bounty / issue to reflect the new needs for Wikidata Reconciliation.  (Given that Freebase is going away and will be absorbed into Wikidata this spring)\nThe starting point for those interested looks like this:\nhttps://www.wikidata.org/w/api.php?action=wbsearchentities&search=Valve&language=en&type=item\nHelp for the Wikidata API is here: https://www.wikidata.org/w/api.php\n. @magnusmanske \nIts a bit more than just your server side on Wikidata :) changes also need to be done on our client side in OpenRefine as well.  Essentially, the Freebase Standard Reconcile as a default needs to be replaced.  This requires working through and cleaning up most of the Java, JS, HTML, and JSON files here: https://github.com/OpenRefine/OpenRefine/search?utf8=%E2%9C%93&q=reconciliation+OR+reconcile+OR+recon&type=Code\nThe bounty gets awarded when both sides are done.  The bounty can be split between developers if they wish, for instance, if you want to have someone do the client side improvements in our code, while you take credit and a partial bounty for the server side.  Up to those involved, we don't care as long as it gets done properly and tests complete.  Good luck, get others involved, or finish it all yourself :)\nAlso @magnusmanske when OpenRefine performs the guess-types-of-columns command there's sometimes failures against the Wikidata reconcile.  I know that Wikidata doesn't really have Types, but instead of displaying Q numbers, our users are looking to see name values, like \"automobile\", \"animal\", etc... not Q1420 and Q729, etc.\ner'>2.1758</td><td bgcolor='#eeeeec' align='right'>411128</td><td bgcolor='#eeeeec'><a href='http://www.php.net/function.file-get-contents' target='_new'>file_get_contents</a>\n(  )</td><td title='/data/project/wikidata-reconcile/public_html/index.php' bgcolor='#eeeeec'>../index.php<b>:</b>147</td></tr>\n</table></font>\n{\"q0\":{\"result\":[{\"id\":\"Q2085381\",\"score\":0.5,\"match\":false,\"type\":[],\"name\":\"publisher\"},{\"id\":\"Q649953\",\"score\":0.33333333333333,\"match\":false,\"type\":[\"Q618779\"],\"name\":\"Pulitzer Prize for Editorial Cartooning\"},{\"id\":\"Q871232\",\"score\":0.5,\"match\":true,\"type\":[\"Q4894405\"],\"name\":\"editorial\"}],\"total_search_results\":818},\"q1\":{\"result\":[{\"id\":\"Q700750\",\"score\":0.5,\"match\":false,\"type\":[\"Q215380\"],\"name\":\"Blank & Jones\"},{\"id\":\"Q6529244\",\"score\":0.33333333333333,\"match\":false,\"type\":[\"Q5\"],\"name\":\"Les Blank\"},{\"id\":\"Q18441355\",\"score\":0.25,\"match\":false,\"type\":[\"Q134556\",\"Q7366\"],\"name\":\"Blank Space\"}],\"total_search_results\":540}} couldn't be parsed as JSON object\n        at com.google.refine.util.ParsingUtilities.evaluateJsonStringToObject(ParsingUtilities.java:131)\n        at com.google.refine.commands.recon.GuessTypesOfColumnCommand.guessTypes(GuessTypesOfColumnCommand.java:196)\n        at com.google.refine.commands.recon.GuessTypesOfColumnCommand.doPost(GuessTypesOfColumnCommand.java:89)\n        at com.google.refine.RefineServlet.service(RefineServlet.java:177)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n\n. @wetneb Yes, and you can claim the bounty on BountySource.com. The best code I have seen for dealing with number formatting is in Pentaho and its usage of java.text.NumberFormat to handle this with Format Masking and NumberFormat can use different Locale's\nNumberFormat nf = NumberFormat.getInstance(Locale.FRENCH);\nhttps://github.com/pentaho/pentaho-kettle/blob/master/core/src/main/java/org/pentaho/di/core/row/value/ValueMetaBase.java#L81\nhttps://github.com/pentaho/pentaho-kettle/blob/master/core/src/main/java/org/pentaho/di/core/row/value/ValueMetaBase.java#L957\nhttps://github.com/pentaho/pentaho-kettle/blob/master/core/src/main/java/org/pentaho/di/core/row/value/ValueMetaBase.java#L1339\nhttps://github.com/pentaho/pentaho-kettle/blob/master/core/src/main/java/org/pentaho/di/core/row/value/ValueMetaBase.java#L3815. Yet another possible way would be to use java.text.MessageFormat ?. UPDATED (I don't see we missed anything):  For conversion methods using our + operator (concat) , we check if both sides are NOT NULL\nif (args.length == 2) {\n            if (args[0] != null && args[1] != null) {   <-- The OP's problem\nhttps://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/grel/ast/OperatorCallExpr.java#L65\nand if both sides are not a number or integral then we convert both arguments to String and append them together.\nif (\"+\".equals(_op)) {\n                    return args[0].toString() + args[1].toString();\n                }\nhttps://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/grel/ast/OperatorCallExpr.java#L122\nAdditional test cases with both null arguments and both whitespace arguments, 1 null side, 1 whitespace side, etc could be added.  Codacy should show 100% test coverage for the OperatorCallExpr.java  which we don't currently.\nMy question to the OP is if he really does expect behavior that is consistent with the Java Language Spec for String conversion when using the binary + operator https://docs.oracle.com/javase/specs/jls/se7/html/jls-5.html#jls-5.4. Sidenotes from 2.7 current behavior:\n\"thad\" + null == \"thad\"  <-- false, and I'm OK with this behavior, its expected, AND won't change it\n\"thad\" + \" \" + null == null <-- true, and I'm OK with this behavior, its expected, AND won't change it\n\"thad\" + \" \" == \"thad \"  <-- true\n\"thad\" + null == null  <-- true\n\"thad\" + \"\" == \"thad\"  <-- true\n\"thad\" + \"\" == null  <-- false\n1 + null == null  <-- true\n1 + \"thad\" == null  <-- false\n\"thad\" + null == \"thad\"  <-- false\n\"\" + \"\" == \"\"  <-- true\n\" \" + \" \" == \"  \"  <-- true\n\"\" + \" \" == \"\"  <-- false\n\" \" + null == null  <-- true\n\" \" + null == \" \"  <-- false\nnull + null == null  <-- true\nisBlank(\" \")  <-- false\nisBlank(null) <-- true\nisBlank(\"\")  <-- true\nisNonBlank(\" \")  <-- true\nisNonBlank(null) <-- false\nisNonBlank(\"\") <-- false\nisNotNull(\" \")  <-- true\nisNotNull(null)  <-- false\nisNotNull(\"\")  <-- true. coalesce(\"thad\" + null) == \"thad\"  <-- true  ??\nno we don't. nope, we explicitly did not add any null conversion functions. Yes, I know those are quite common, and I use them in my daily database work all the time :)\nWe can certainly add that kind of function to https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/grel/ControlFunctionRegistry.java\nOh, and this GREL expression probably has some expectation as well after coalesce() is added I'd imagine:\ntype(\"thad\" + coalesce(null)) == \"string\" <-- true. funny how a 2000 year old issue still becomes an issue for folks :) https://en.wikipedia.org/wiki/Null_morpheme. Brainstormed a bit today and thinking about making it super easy with our existing GREL dot notation syntax for users like what Groovy and Koitlin do... For the coalesce operation, what if we introduced this need as a new safe navigation operator \"?.\" like what C# and Groovy and other languages do ?  http://www.oracle.com/technetwork/articles/java/java8-optional-2175753.html\nThis could be put as a new token in Parser.java\nLanguages such as Groovy have a safe navigation operator represented by \"?.\" to safely navigate through potential null references. (Note that it is soon to be included in C#, too, and it was proposed for Java SE 7 but didn't make it into that release.) It works as follows:\nString version = computer?.getSoundcard()?.getUSB()?.getVersion();\nIn this case, the variable version will be assigned to null if computer is null, or getSoundcard() returns null, or getUSB() returns null. You don't need to write complex nested conditions to check for null.\nIn addition, Groovy also includes the Elvis operator \"?:\" (if you look at it sideways, you'll recognize Elvis' famous hair), which can be used for simple cases when a default value is needed. In the following, if the expression that uses the safe navigation operator returns null, the default value \"UNKNOWN\" is returned; otherwise, the available version tag is returned.\nString version = \n    computer?.getSoundcard()?.getUSB()?.getVersion() ?: \"UNKNOWN\";\n. @eximius313 I was only talking about GREL convention.\n- [x] For 1, we would still need to add Common Transforms  (BTW, Blank means both null and empty...and we will not ever change that behavior since its useful to have a single operation that treats them equally)  But we also need \"Null all cells\" and \"Empty all cells\").\n- [x] For 2, we would need to add 2 more Customized Facets... \"Facet by null\" and \"Facet by empty\" (along with our existing \"Facet by blank\" and \"Facet by error\")\n. @ostephens\nThe safe navigation operator ?. is simply an inline sugary syntax to do \"if null, please proceed\", but does not replace null with anything, its just a check.\nassert value != null\nThe Elvis operator ?: is simply a sugary syntax to perform a ternary operator, which is a shortcut for if/else branch assigning some value to a variable.  The downside is that the user has to tell the value they want to assign, rather than just coalescing null into \"\".  But we could do that as a default for our Elvis operator syntax.\n(SIDENOTE: There is also the idea in other languages of collecting only non-null values and that can use the safe navigation operator ?.\nassert listOfMaps*.a == listOfMaps.collect { it?.a } //equivalent notation\n// But this will only collect non-null values\n)\nDavid implemented a Null Object Pattern for GREL (forgot where in code) that represents a recognized keyword of null, similar to \nhttp://docs.groovy-lang.org/latest/html/documentation/index.html#_null_object_pattern\nBUT, if we go with coalesce() , it would need to be a well defined function for our GREL usage.  Because... it performs differently in different languages.  And I am so used to COALESCE in the Database world where the first non-null expression is returned.  For example, https://docs.microsoft.com/en-us/sql/t-sql/language-elements/coalesce-transact-sql where for example, \nSELECT COALESCE(NULL, NULL, 'third_value', 'fourth_value');\nreturns the third value because the third value is the first value that is not null.\nThink through the need carefully.  WHAT problem do we actually have, and then HOW best to solve it.\nThe easiest way to solve the OP's immediate problem, IMHO ?\nSupport this:\nreplace(null,null,\"\")\n . @eximius313 \nIt would look like this\ncells[\"Column 3\"].value + cells[\"Column 1\"].value.replace(null,\"\")\ncells[\"Column 3\"].value + cells[\"Column 1\"].value.replace(null,\"ThisIsNull\")\nallowing you to replace null values with whatever you want,\ninstead of throwing our current error of Error: Cannot retrieve field from null even when doing the proper syntax in the above 2 examples.\nWould that work for you ?\n. @eximius313 Sure we can introduce a new coalesce() for you to use instead of replace().  And yes, its easier with coalesce() since null check is a builtin and it removes 1 layer that does not have to be specified by the user as it would be with replace().  \ncoalesce(null,\"\")\nreplace(null,null,\"\")\nI was just pointing out a minor flaw in our current replace() that we ALSO need to fix up to support null replacement handling to be a nice citizen all around.. @ostephens No, that's a null replacement function...different than coalesce().  That would be something like NVL() in Oracle. https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions105.htm\nAnd no, I am indifferent about replace() supporting NVL() like functionality, but others might.  So let them speak up later about it, if they need to.  For the OP's issue, only coalesce() is the wanted position and #1496 should do it.  Thanks for that Owen !. @ostephens Perhaps you could add the coding pattern of NullText in your coalesce() function?  org.apache.commons.text.StrBuilder has getNullText() and setNullText()\nThere are tons of examples online and in Github https://github.com/search?l=Java&q=getNullText&type=Code&utf8=%E2%9C%93\nBut perhaps also getNullText might not even be needed if we modified some in\nhttps://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/model/Cell.java\nand\nhttps://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/model/Row.java\nso that a null was treated as a silent field ?\nThis is probably something @jackyq2015 can answer, because I know he dealt with a bit of this while working with metadata https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/model/medadata/validator/checks/MinimumLengthConstraint.java\nStill, having shorter function of value.coalesce(\"\") to replace this would be nice:\nvalue.toString().replace(\"null\",\"\"). @ostephens ??? I'm confused also... are you telling me that when you have a null cell value... that a simple\nvalue.toString() does not convert that to a String representation of \"null\" in the preview ?  It does for me. https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/util/StringUtils.java. @ostephens oh boy, yeah, I see the issue now...we are not consistent with cells[ ] ... I wonder if something changed or broke ?  I'm using 2.8 here:\n\n\nand to GREL handling, null is a value, technically.  Our cells variable and even documentation says that value can be null https://github.com/OpenRefine/OpenRefine/wiki/Variables  but it seems that if value of the cells variable is null, then we are saying, Oops ! I cannot find the field from null...and that's stupid, it totally does have the field, its just not being exposed...so probably have to get the cells variable to expose it properly. @jackyq2015 could probably help there.  Then the 2 screenshots above will be consistent, and I think I would be OK with that, but do want others to chime in on this issue.. @ostephens its done here (I THINK) https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/model/Cell.java#L68  and do you have \"value\"... no you don't, so it breaks.  Probably just need to fix it there and put a null check there @jackyq2015 ?. @ostephens FYI @tfmorris is the one who added the toString() method for debugging the cells variable itself so you can do things like cells[\"Column 1\"].toString()\nLook at the history of things also in our Github and BLAME is very useful view to see timeline of who/where/why for any file in our codebase.  I've found it super useful over the years to track regressions, but I don't see one on Cell.java I think.  The impact of cells variable is at the row and column level. And our model folder is where everything comes together.\n\n. Ah, AngularJS is also embeddable as well (just noticed, had not before)\n.... hmm... that's a win for us, so I can mockup both and we can see the\ndifferences.... I guess I can begin with a separate fork on my own to begin\nexploring.\nOn Fri, Oct 4, 2013 at 12:47 PM, Tom Morris notifications@github.comwrote:\n\nPrototyping would provide useful information, but perhaps you could\noutline your process so far and the decision criteria that led to MontageJS\nbeing the answer. There are lots of potential JQuery replacements out there\n(AngularJS is another that comes to mind), but there would have to be a\npretty big payoff for such a large engineering investment.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/823#issuecomment-25717411\n.\n\n\n-Thad\nThad on Freebase.com http://www.freebase.com/view/en/thad_guidry\nThad on LinkedIn http://www.linkedin.com/in/thadguidry/\n. Paul Hammant seems to prefer Angular over others, and has some nice analysis earlier this year with earlier versions of all the client-side MVC using the TODOMVC project on github  http://paulhammant.com/2013/01/18/client-side-mvc-roundup/\n. A good place to start looking for performance problems I think is that we try to do the following with jQuery in a few places.\nCommon Pitfalls found under AngluarJS's FAQ's https://docs.angularjs.org/misc/faq:\n\nng-repeat\nng-repeat gets this a lot. People try to use jQuery (see above) to add more elements to some container as they're fetched from the server. No, bad dog. This is what ng-repeat is for, and it does its job very well. Store the data from the server in an array on your $scope, and bind it to the DOM with ng-repeat.\n. There is still a small issue with the dual licensing ExtJS has.\nI think AngularJS architecture overall is more comfortable for developers\nand that is the most important for the architecture porting work to be\ndone..\n\nThad\n+ThadGuidry https://www.google.com/+ThadGuidry\nOn Tue, Feb 3, 2015 at 10:35 AM, QI CUI notifications@github.com wrote:\n\nHow about ExtJS? In terms of widget support, it's better. I think it's\nimportant for the data presentation and visualization. But its footprint is\nheavier than AngularJS\nhttp://www.techferry.com/articles/ExtJS-vs-AngularJS.html\nit's GNU GPL license v3. Since openRefine is BSD, it is compatible.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/823#issuecomment-72683640\n.\n. Closing this issue as no longer needed.  We are going to plan for separating the frontend / backend and leverage a modern frontend framework that the community is comfortable in.. @tfmorris seems to be fixed now.  Tested on Windows 7 with refine clean build against latest Chrome.\n. Closing since now OpenRefine supports a system property refine.max_form_content_size thanks to @FWennerdahl years ago.. @jackyq2015 Perhaps we can harden this for MacOSX users ?  I wonder what the exact scenario is that causes this to occur for them ?\n\n@tcbuzor since you have a Mac also, any ideas about this issue that folks sometimes have, or ways to avoid it ?. @JobsiteSean Were you able to resolve this yourself ?  What does the \"hang\" actually look like , spinner forever, something else ?\n. checks pass and you manually verified it works via dialogs.   Merge is OK.  But I'm just wondering about client API's that might need a look over as well.  I don't think we have limits on the expression length elsewhere, but I could be wrong.  https://github.com/OpenRefine/OpenRefine/search?utf8=%E2%9C%93&q=expression&type=. This is duplicated in #1232 . @noamoss how's the Hebrew translation coming along ? :)  We'd like to get this in sometime in the future. Let us know.. @answerquest Oh really ?  If that is happening on Windows, then that is a bug on our part.  We should be defaulting to UTF-8 whenever the user does not pick the encoding on the exporter.  We fixed that and made UTF-8 default very long ago.\n@ostephens Thoughts on why UTF-8 is not being defaulted across all the OS's we support ? Can you find the original issue for that ?. (Dockerfile links in initial comment were broken, since Docker Hub made some slight changes....but updated them with good links now). Mine as example:\nDockerfile\nFROM maven:3.6.0-jdk-8-alpine\nMAINTAINER thadguidry@gmail.com\nRUN apk add --no-cache git\nRUN git clone https://github.com/OpenRefine/OpenRefine.git \nRUN OpenRefine/refine build\nRUN mkdir /mnt/refine\nVOLUME /mnt/refine\nEXPOSE 3333\nCMD [\"OpenRefine/refine\", \"-i\", \"0.0.0.0\", \"-d\", \"/mnt/refine\"]. @psychemedia Yes, extra RUN commands could be added to remove build folders/files and only leave the classes/libs.  @wetneb probably has that info that could be added.  OpenRefine when it is compiled (refine buildcommand) needs the JDK, but after compiled and to run it, only needs JRE.  \"Standard Java Web App Stuff\".  My Dockerfile is for active development usage.. :+1: \n. :+1: This looks like it is under the extension, so fine for merge I would say. We have other work to do later since the API will be deprecating as well later this year. (or rather, changing form to the Knowledge Graph API)\n. @tfmorris We have source control, so if there are bits and pieces we need later from the extension work, we can always refactor them in for the Knowledge Graph API.  I have actually been using @MattBlissett extension against his Plant List API on a couple of fun projects.  So I would rather just see a broken Freebase menu option and I can deal with the emails regarding that it is a work-in-progress to remove the Freebase guts in a 2.6 minor release later.  Lesser of 2 evils.  KISS principle.\n. @simon1tan if your still having some issue around the Text Facet resizing, please open a new issue.  This one is a bit dated now.  Thanks !. @ettorerizza Something like this would work that says \"exclude\" where the \"undefined\" is ?\n\nFor whomever wants to hack and learn... @ostephens !!! :) the code is here:\nhttps://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/browsing/facets/TextSearchFacet.java\nhttps://github.com/OpenRefine/OpenRefine/blob/master/main/webapp/modules/core/scripts/facets/text-search-facet.js\nelmts.caseSensitiveCheckbox.bind(\"change\", function() {\n    self._config.caseSensitive = this.checked;\n    if (self._query !== null && self._query.length > 0) {\n      self._scheduleUpdate();\n    }\n  });\n  elmts.regexCheckbox.bind(\"change\", function() {\n    self._config.mode = this.checked ? \"regex\" : \"text\";\n    if (self._query !== null && self._query.length > 0) {\n      self._scheduleUpdate();\n    }\n  });\nJust extend with something like this...and also wire it up properly in other places, just use the regexCheckbox as an example of what to add in other areas.\nelmts.excludeCheckbox.bind(\"change\", function() {\n    self._config.mode = this.checked ? \"exclude\" : \"text\";\n    if (self._query !== null && self._query.length > 0) {\n      self._scheduleUpdate();\n    }\n  });. :+1: for merge\n. The best way to handle this is with Tooltips as other applications do.  Mozilla provides a guide and good practice about Tooltips in trees, such as what we have here in our menu tree. Where our menu tree width can be locked down, but provide Tooltips to show the full label when mouse hovering. https://developer.mozilla.org/en-US/docs/Mozilla/Tech/XUL/PopupGuide/Tooltips\nWe'll just need to improve our interface with Tooltip option and this issue goes away.  We can even have a user preference for Tooltips On/Off for folks that don't want any annoyance.. @wetneb Hmm, We might want to refactor in a few of these mentioned.  I'll leave it up to you as we progress this year in 2019 and depending on our time and budgets, of course.. @jezcope you might try creating a shortcut and setting the \"Start in\" path as the network path.  Sorry, but we can't help more with this issue.  Try to ask Windows guru's on Stackoverflow, I'm sure they could help with a workaround somehow.. @kennedyoliveira To get this now... you can drop in the Jython 2.7 jar file yourself and it should work.  The path is something like this:\n \\openrefine-win-2.6-rc.2\\openrefine-2.6-rc.2\\webapp\\extensions\\jython\\module\\MOD-INF\\lib\nJust make sure to move the original jython-standalone-2.5.3.jarfile to another folder and not have both Jython jars in that folder path. . @kennedyoliveira ask that json parsing question again on our mailing list, or on StackExchange where the community can help and provide support. (our issues on Github are not a support queue). Verified as working as expected in 2.7  I am the one who originally tested this, way back when.  INVERT is a special mode and stays in that mode until you unselect it.  While in inverted mode, a click on a value itself will cause it to strikethrough and this is expected.  If you click on include next to it, it will be deselected.  You stay in inverted mode until you have clicked on INVERT again to change back to normal mode in the facet.\n. Closing as no response from OP. Support for infinite scrolling in our data grid will be added in next Phase 2 enhancements. See #1347 \n. 2 kinds of errors we have traditionally dealt with and handled.  David was semi-consistent with these 2.\nThose for GREL syntax / user error.  Those for Java.\nDavid considered null to be an error sometimes.  That was probably not the best way to deal with it for GREL usage(user error).  We know that null for GREL is not an error, but for Java it could be.\nI strongly feel we should return null to GREL(the user) when accessing a non-existent field.\nAn \"error\" in GREL syntax was always intended for the user as guidance (you can see that in how David originally wired some of those catch() parts.  The error should not be for inside GREL functionality (which is Java).\nThis particular error is actually just more information to the user...but we can provide that information later with new and improved way through GREL dialog helpers which I still have in my future vision plans with new UI.  The GREL window will be 3 parts in my future vision, maybe 4, and the 3rd part would display Helper text.\n\"this value is null, it is possible that you have asked for a non-existent field, or there is no value at all for your expression, you might want to double check things, or ensure your working with a valid cell/row object or tuple\". . @ostephens given the consensus from above -  Looks like we have all casually voted to return null for non-existent fields for non-null objects.  So Owen, can you work on implementing that ?. Added Tom's note to wiki:\nhttps://github.com/OpenRefine/OpenRefine/wiki/GREL-Math-Functions\nThad\n+ThadGuidry https://www.google.com/+ThadGuidry\nOn Wed, Jul 29, 2015 at 2:07 PM, Dino Scheidt notifications@github.com\nwrote:\n\nApologies. I did a prior google search before; checked the GREL math\noperations https://github.com/OpenRefine/OpenRefine/wiki/GREL-Functions\nand did not find any notable behaviour for devisions, float and automatic\ntyping.\nMaybe this could be added, as it is certainly an unexpected ux - as it\nseems to https://code.google.com/p/google-refine/issues/detail?id=350\nhttp://have%20popped%20up%20before\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1044#issuecomment-126061211\n.\n. No movement, but someone could just add jsmin (perhaps from jawr-core https://github.com/j-a-w-r/jawr-main-repo/tree/master/jawr/jawr-core or https://github.com/collegeman/htmlcompressor) to where it would strip out the comments prior to where we apply operation history here: https://github.com/OpenRefine/OpenRefine/blob/master/main/webapp/modules/core/scripts/project/history-panel.js#L292. @cameronstewart Comments are now supported via the VIB-BITS plugin and its History Tools that support comments in Undo/Redo history.  Read their manual. https://www.bits.vib.be/software-overview/openrefine\n\n@hvmarck Thanks for finally adding support for comments in Undo/Redo history and making it a part of your public plugin !\nClosing this issue, since feature is now handled via a public plugin that is available.. I agree except for 516.  Its more of a \"Retraso de tiempo\" or time delay.  Change and we can merge.\n. +1 to \"Tiempo de retraso\"\n. Jacky,\nant clean\nant build\nShift F5 on browser to force clear cache\n?\n. Dan,\nYou will need to first get the part of the array that you want to trim.\nRemember that split() outputs an Array, not a whole string.  It outputs a\nfirst and second part.  Our GREL String functions are documented a bit\nhere: https://github.com/OpenRefine/OpenRefine/wiki/GREL-String-Functions\nYou probably want to do something like:\nvalue.split(\";\")[0].trim()\nor\nvalue.split(\";\")[1].trim()\nto trim the first or second part\nThad\n+ThadGuidry https://www.google.com/+ThadGuidry\nOn Thu, Oct 1, 2015 at 10:16 AM, Dan Bolser notifications@github.com\nwrote:\n\ntrim(split(...)) doesn't seem to work.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1077.\n. Yes we have GREL Control functions.  Controls are bit different than\nfunctions.  And you can read more how they differ here:\n\nhttps://github.com/OpenRefine/OpenRefine/wiki/GREL-Controls\nforEach()\n forNonBlank()\n etc....\nTake a look more at all of our Reference material :\nhttps://github.com/OpenRefine/OpenRefine/wiki/Documentation-For-Users#reference\nthen dive into more docs and Recipes for a fuller picture  (Scroll down\nthis page before you click on any link, just to get a high level view of\nall the kinds of documentation we have)\nhttps://github.com/OpenRefine/OpenRefine/wiki/Documentation-For-Users\n.\nThad\n+ThadGuidry https://www.google.com/+ThadGuidry\nOn Fri, Oct 2, 2015 at 4:21 AM, Dan Bolser notifications@github.com wrote:\n\nThanks Thad,\nThe issue I'm having is that the list can be 1 to n terms, and I want to\ntrim them all... Actually the solution in this case is to split on ' ,'\nrather than ','.\nI'm guessing it isn't possible to run a function over a list in GREL?\nCheers,\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1077#issuecomment-144970396\n.\n. Looks like we do have a few recipes that showcase what you sort of need to\ndo.\nI like making folks think and work through problems, rather than directly\nshowing them the answers. :)  - I do the same for my kids :) :)\n\nhttps://github.com/OpenRefine/OpenRefine/wiki/Recipes#split--map--join\nIf you still need help after that, just reach back out to us on the mailing\nlist.\nThad\n+ThadGuidry https://www.google.com/+ThadGuidry\nOn Fri, Oct 2, 2015 at 8:51 AM, Thad Guidry thadguidry@gmail.com wrote:\n\nYes we have GREL Control functions.  Controls are bit different than\nfunctions.  And you can read more how they differ here:\nhttps://github.com/OpenRefine/OpenRefine/wiki/GREL-Controls\nforEach()\n forNonBlank()\n etc....\nTake a look more at all of our Reference material :\nhttps://github.com/OpenRefine/OpenRefine/wiki/Documentation-For-Users#reference\nthen dive into more docs and Recipes for a fuller picture  (Scroll down\nthis page before you click on any link, just to get a high level view of\nall the kinds of documentation we have)\nhttps://github.com/OpenRefine/OpenRefine/wiki/Documentation-For-Users\n.\nThad\n+ThadGuidry https://www.google.com/+ThadGuidry\nOn Fri, Oct 2, 2015 at 4:21 AM, Dan Bolser notifications@github.com\nwrote:\n\nThanks Thad,\nThe issue I'm having is that the list can be 1 to n terms, and I want to\ntrim them all... Actually the solution in this case is to split on ' ,'\nrather than ','.\nI'm guessing it isn't possible to run a function over a list in GREL?\nCheers,\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1077#issuecomment-144970396\n.\n. Dan,\n\n\nBut then you would not have learned those 10 shiny new things you now know\nyou can do in GREL besides your immediate problem.  And you have gained\nconfidence in yourself = PRICELESS\nThad\n+ThadGuidry https://www.google.com/+ThadGuidry\nOn Thu, Oct 8, 2015 at 5:54 AM, Dan Bolser notifications@github.com wrote:\n\nThanks thad,\nLooking here:\nhttps://github.com/OpenRefine/OpenRefine/wiki/GREL-Controls#filterexpression-a-variable-v-expression-test\nthis solution was pretty obvious:\nforEach(split(value, ','), v, trim(v))\nPersonally, I rather just try the answer than work it out for myself ;-)\nThanks for help,\nDan.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1077#issuecomment-146502767\n.\n. Use the Line-based text file selection instead of the\nCSV/TSV/separator-based.\n\nPlay around with the importing options, until you can better understand\nyour data.\nOptionally, look at your file in a regular Text Editor such as Notepad++ on\nWindows, etc, to determine the file type or patterns that you can see.\nThen adjust the importing options to your needs or liking.\nHowever, the Line-based importer can work for anything, even without\ndetermining a patter, it just depends on Line Feeds and Carriage Returns as\nits separator.\nLet us know how far you get, and if not far, then try to attach a sample of\nyour file if you can (privacy concerns and all that).\n. The specific reason was getting the Netbeans environment playing nicely.  I have some other PRs coming for that later.  Good point, I'll create my own branch and then ask for a PR there. Thanks Tom! and welcome back ! :)\n. @tfmorris @jackyq2015 @magdmartin @MattBlissett \nOK, I am now working on setting up our new Gradle build.  (and with the Gradle Wrapper, so you won't even have to install Gradle beforehand !)  I'm about halfway done with the new organizational changes, but from a high level it will be treated as a Gradle multi-project build - A multi-project build in gradle consists of one root project, and one or more subprojects that may also have subprojects.:\n- /openrefine\n  - /client\n    - build.gradle\n  - /server\n    - build.gradle\n  - /shared\n- build.gradle\n- settings.gradle\nI will maintain it.\nThis NEW build way will be pushed into my repo later for eveyone to review once I am done, Then later we can clone it under openrefine/gradle_build at some point in the future once everyone is happy. (this way I don't slow anyone down,and we get an easier build and even easier distributions !! in the future)\n(@tfmorris - And, we will not even have to lose the Eclipse .project settings etc. apply plugin: 'eclipse'\nI looked at the current state of affairs with Maven and SBT and I still was not pleased enough - most of the major players are now going Gradle and for good reason, its also defacto standard added now in Eclipse Mars !! - no plugin needed.  We also use it at Ericsson)\n. I don't need this anymore.. @charlie7691 Try to use a Facet on some column to limit the rows however you wish. THEN perform your Clustering.  OpenRefine is designed for operations near 1 million rows or less.  So you might have to batch up your operations into phases or slices at a time with Facets or Limit Rows during the import and project creation. (make 2 projects at 1 million rows each, then do clustering, etc)\nI am closing this and marking as a non-issue since this goes against our original design, but we understand that you want to do more work with Refine.  However, we have hit up against limits with the current design, which hopefully one day we will rework that and allow ~ 10 million rows.  Feel free to have open discussion with us on our mailing list and not this issue.\n. @charlie7691 Glad you got some work done Charlie ! :)  Yes, Facets are very powerful in OpenRefine, so use them judiciously. And you can apply more than 1, and there are so many kinds to choose from, or you can build your own custom facet with GREL, Clojure, or even Python language.\n. @charlie7691 You could also learn a bit of Python and use https://github.com/gfairchild/pyxDamerauLevenshtein\n. Uncheck the option for removing blank rows ?\n. Try line-based importer, then find the separator pattern for fields and split.\nOpenRefine has no problem with nulls in the first column, such as if your CSV file opened in Notepad looked like this:\nfieldA,fieldB,fieldC\n1,2,3\n1,2,3\n,2,3\n1,,3\n. You'll have to attach the CSV for me to find out...or if it is private then email to me and I can take a look.\n. thadguidry@gmail.com  - I thought it was publically available to see in my Github profile ? hmm, click on my Picture :)\n. @daviddou82 \nLooks like OpenRefine is handling your file just fine as a CSV import with default values.\nYou can verify this, by being in \"rows\" mode in the upper left once your looking at the data grid, and in the upper right corner clicking on \"last\" to see the last few rows, which might take a few seconds to actually display since you have a large amount of rows. (took 12 seconds on mine with default RAM allocation given to OpenRefine -Xmx 1024m)\nPlease reply and confirm if you see the same, and then we can close this issue out.\n. @daviddou82 if your in \"rows\" mode, then the count should be accurate.\n. Looks like we don't really have an option that \"keeps\" the quote characters if a user wants.\n\"this is somethng\",\"this is something else\"\nCurrently, on the preview, we remove the double quote characters, regardless of the checkbox for \"Quotation marks are used to enclose cells...\"\nI see 3 further enhancements also needed.\n1. A click option to keep the quotes but still separate on a separator char. (supporting single or double quotes)\n2. Changing the phrase \"Quotation marks are used to enclose cells containing column separators\" to the actual action that it will perform, similar to our other import options that use an actionable label.\n3. Give the user a clickable option to allow or not on single quotes.\n@stundzig your pull request does not address those, but actually hides to the user the data manipulation that OpenRefine is doing during import.  We try to avoid hiding to the user what is happening to their data and instead like to give them checkboxes, etc to apply actions and visually see the changes during preview.  Can you work on those further enhancements and the above and request again ?  We would absolutely love that !\n. @ostephens Could you get this done over the next 2 weeks ? (invoice-able work). @wetneb yeap... creating Records from Arrays.  Do we have an all encompassing Issue for that already ? I only know about my #152 \nFound it !  Yeap, an OLD issue...and it is still much needed... #36 [Wishlist] Seamless conversion of arrays into multiple columns. @ostephens Did you get a chance over the past weekend to look at this ?. @ostephens Let's ask @jackyq2015 and @wetneb what they think.  Guys ?. @kgrons The Export Cluster button barely shows up for me on latest Firefox and Chrome.  Can you fix that and request pull again ?  Then I'll be happy to merge if my functional tests pass.\n\n. @kgrons The download does work and is good Json representation.  Button text just needs to be fixed and then I can merge.. @corajr Works perfect. Thanks ! Merged.. @wetneb I'll defer to @jackyq2015 as to how we should handle this.  @jackyq2015 thoughts ?\nFYI, longterm, I also want to see ICU4J included so that we can tackle things like what I mentioned on the mailing list about smart replacement for diacritics inside expressions https://groups.google.com/d/topic/openrefine/6hY1-GI0KAI/discussion. @danbri Hasn't Gregg already put lots of work into it and all the tests ? to cover the uses cases that sort of originally inspired it all ? http://www.w3.org/2013/csvw/wiki/Use_Cases \n. @danbri yes Dan.  Don't worry, will support CSVW metadata... just gotta get some more contributor help on things like that.  But first things first... laying the groundwork for broader metadata support, then pretty much anything can be done at importer and exporter time.  Its always been in the 'plan' and I'll continue to push for this as well :)  and this is the issue and story for that.. Mailing List thread side discussion about if we support CSV on the Web AND Data Package family spec\nhttps://groups.google.com/d/topic/openrefine-dev/6UU_w98PcJs/discussion\nWaiting to hear back from @danbri and @rufuspollock on that thread.. I spoke to Jacky and the approach I think that makes sense is...\nThe 3 formats that we see in the wild currently used are Data Package, CKAN (simple JSON with 1 \"meta\" object and 1 \"data\" object), and CSVW.\n\nnew Importer for metadata that can be based on our JSON importer (this can handle ANY JSON-based metadata formats.  \nnew importer/exporter for CSVW since this supports the widest Linked Data efforts through JSON-LD format with Schema.org semantics.\n(optional) new importer/exporter for Data Package if time allowed or we feel the community is vocal to also want it.\n(optional) XML based metadata importer that can be based on our XML importer. On Data.gov and Data.gov.uk and other's I didn't come across many remaining XML based metadata sets... looks like most are getting converted to JSON based at a rapid pace.\nStoring the metadata should use CSVW format to allow the widest amount of metadata properties and Linked Data capabilities.  This will benefit everyone, Scientists, Researchers, Wikidata, Librarians, you name it and allow OpenRefine to be a great one-shot power tool to pull in regular CSVs and annotate.  The annotation of Columns (fields) can be done with a nice GUI later on, but for immediate Proof of Concept now we can just start with a raw input box for the JSON-LD format and use Gregg Kellogg, Manu Sporny's, Markus Lanthaler's work with jsonld.js processor which has an API \n\nAll in favor ?\n. @wetneb For 1. I was talking about a different case that comes up concerning the use of the generic JSON importer we have...There are cases where in the wild I have seen and demonstrated to Jacky that metadata is found in freeform Json records and XML (basically the datasets out there that are pre-2005).  I showed him one last night.  We'll want to support freeform Json metadata mapping as well, but that can be a longer term goal.  And that freeform JSON metadata mapping can be done similarly to our JSON importer selector.\nFor CSVW and Data Packages support... yes, those are new importers.  I have updated my comment above.\n. @rufuspollock the problem is that there's not good support for Linked Data within Data Package / Table Schema from what I see.  I.E. only the rdfType:. @danbri per factoid (individual provenance on an OpenRefine cell), seems way too heavy ?  OpenRefine works on mass edits against columnar operations.  I think our team was hoping to only have to deal with provenance at the Column level only, as your PublicToilets example JSON shows (a column of cells that share the same metadata/provenance).   For the qualifiers, yes those could be stored also (it could be treated as Parent/Child column structures (grouping columns), which our data model doesn't support yet but many datagrid/storage models support grouping columns and operations across them.  I have worked through previous designs and have thoughts about that, I'm just not sure on the best approach yet.  Columnar Grouping definitely handles that case however, but so does our Record mode in a way.  There's even Row Grouping options such as this example http://mleibman.github.io/SlickGrid/examples/example-grouping.html \nAntonin @wetneb had more thoughts on qualifiers, but my hunch is that Columnar Groups (sub data for a column) might also help there as well.  And then having a nice presentation for that in our UI, column folding, layered slide out panels, etc. all come to mind.\n. @danbri Righto.  Yeah, I think just the Parent/Child column structuring (sub data for a column) will work out well in the long term for this edit and mapping visualization need at individual cell levels.  Our current Edit for a cell which is a rather plain text box...will change to be much more feature rich with an optional docking panel...rather than the current limiting popup dialog overlay. \nIf you haven't heard me say it before...I want OpenRefine to be the Photoshop of data cleaning tools.  We're just not there yet :). @rufuspollock (at import/export) Right and that's part of the problem, data sharing takes a hit because sometimes the data is not well understood !  The idea is that later, they would have the ability to pull in metadata through our Wikidata Reconciling process and apply to columns or groups of cells to enrich their dataset.  @wetneb can also speak about some of this.  The process is described here https://github.com/OpenRefine/OpenRefine/wiki/Reconciliation. @wetneb What the community has been saying is that against any particular column...there is a need for\n1. Storing Data Types (Boolean, String, Number, etc) - Your case for Data Packages, which is unclear if it supports 2 very well.\n2. Storing Semantic Types (CreativeWork, Person, Event, PropertyValue, VideoGame, etc) - Our case for CSVW which supports both 1 and 2\nDoes Data Packages specification support both that our users want ?  I have yet to get a good answer.\nBut I'm flexible, if the community has suddenly said they don't care about 2, then I need names and numbers of those folks, just for posterity sake when they come back to us and say, oh sorry, yeah we need that also now. :)\n. @wetneb Breathe Antonin.  :) There's no battle here.  Not sure if you know, but everyone here @danbri @rufuspollock myself, etc are actually friends on the web.  All discussion is equally important and OpenRefine values all opinions.  Sorry, I thought the use case was clear in this discussion and by the OP and your initial thumbs up reaction that knowing and understanding more about a dataset was important and being able to share that knowledge.\nThe association of a Semantic Type to a column is just a simple mapping.  (the details of how is where Jacky and I and yourself will need to make a choice based on a format, but that format I think should be JSON based and both CSVW and Table Schema have us covered there)\nThe Semantic Type can be set manually or discovered and presented automatically.\nManually, by a user through a Add Metadata column option.  Jacky supported that idea and I think you did also.\nAutomatically, a few things can be done as well.  Here's one idea that I felt was useful based on other tools (like DBpedia's) that do something similar. After Reconciling, a user can be presented with a dialog that shows perhaps 2-3 of the top Semantic Types by percentage which can be based on the Wikidata, \"instance of\" property https://www.wikidata.org/wiki/Property:P31 and allowing the users to choose and apply the appropriate or most fitting one.  Say you have a data set with a bunch of Sports Governing bodies...and the auto meta dialog says that most of them in your OpenRefine column are also instances of nonprofit organization or just simply organization.  The user sees that in the dialog and then can apply that Semantic Type of \"Organization\" a  subclass of nonprofit organization.  Sometimes the discovery can even be reversed, where the user thought he had a generic dataset of Organizations and lo and behold he finds out through that dialog that 98% of them are actually Sports Governing bodies as well !\nThe Semantic Type can just be the Wikidata URL for the QID, for example, International Sport Governing Body https://www.wikidata.org/wiki/Q1346006\nApplying the \"mapping\" portion through a property can be done on both CSVW and Table Schema\n{\n      fields: [\n        {\n          \"name\": \"Organizations\",\n          \"type\": \"string\",\n          \"rdfType\": \"https://www.wikidata.org/wiki/Q1346006\"\n        }\n        ...\n      }\n    }\nAlso, discovery of an additional Schema.org mapped Type \"could\" also be done as well through the Wikidata \"equivalent class\" property.  As shown in this example of Organization https://www.wikidata.org/wiki/Q43229 where there is mapping to DBPedia, Schema.org, W3\n@rufuspollock I would hope that Table Schema and parsers out there support Multi Typing like CSVW does ?  Do you know ?\n{\n      fields: [\n        {\n          \"name\": \"Organizations\",\n          \"type\": \"string\",\n          \"rdfType\": [\"https://www.wikidata.org/wiki/Q1346006\",\"http://www.schema.org/Organization\"]\n        }\n        ...\n      }\n    }\nAs @danbri and @rufuspollock and I share the same concern, the importance of this Issue #1096 is actually more about adding metadata and exporting it to enhance knowledge of a dataset that didn't exist before.  And furthermore, nicely importing that knowledge back into Wikidata, as all of us agreed already \"that would be an awesome thing to be able to do\".\n@wetneb which parts above do I need to flesh out more that might still be unclear, just let me know.\n. @jackyq2015 @tfmorris  I tested using #796 use case given by user and using clipboard import, however, @jackyq2015 only partially fixes the bug introduced, and still cannot perform a complete UNDO after the Transpose change because the Cat and Val columns are blown away.\n. @LibrarPotter Match does work, but your probably just missing some knowledge on how it works.\nMatch can output a boolean or an array.  So it depends on how you wired it up.\nUse our documentation below to get a bit more familiar on how flexible it can be for your needs.\nhttps://github.com/OpenRefine/OpenRefine/wiki/GREL-String-Functions#matchstring-s-regexp-p\nhttps://github.com/OpenRefine/OpenRefine/wiki/Understanding-Regular-Expressions\nClose this issue if you find that those are helpful and get your further, otherwise, @joewiz is correct in asking you for more information for an example string that your trying to match, and what your GREL recipe looks like.  Let us know.\n. That reconcile service is no longer supported.  It is deprecated.\nSee https://developers.google.com/freebase/v1/reconciliation-overview?hl=en\n. this service:  http://reconcile.freebaseapps.com/\nIs in a state of \"no longer being actively maintained\".  The original maintainer was Andi Vajda who can be reached at email vajda@google.com\nYou can ask him directly.  That service is not run by the OpenRefine team, but was run by Andi when he was with Freebase/Google and it is his app and indexing service that you are having problems with.  Freebase/Google have already said that there are no guarantees of any service availability in the future after the deprecation time, which was last summer in June 2015.\nOpenRefine does provide a Reconciliation interface for any kind of reconciling service...and there are a few of those services still around on the internet... see a list here: https://github.com/OpenRefine/OpenRefine/wiki/Reconcilable-Data-Sources\n. #468 gathers together all Export Template enhancement requests.. The service (not provided by OpenRefine, but Google/Freebase) is now\ndeprecated.\nRead comments on this closed issue:\nhttps://github.com/OpenRefine/OpenRefine/issues/1101\nThad\n+ThadGuidry https://www.google.com/+ThadGuidry\nOn Sat, Jan 16, 2016 at 2:09 PM, Luis M Sanchez notifications@github.com\nwrote:\n\nI have test data with the names of 10 actors, that I would like to expand\nwith some additional data.\n[image: screen shot 2016-01-16 at 3 06 23 pm]\nhttps://cloud.githubusercontent.com/assets/3156487/12374221/bd58bcd6-bc62-11e5-9a73-83e5636eb195.png\nThis used to work fine using this reconciliation service:\nhttp://reconcile.freebaseapps.com/reconcile. However, now it is stuck\nthere. Is there any problem with the service?\n[image: screen shot 2016-01-16 at 3 07 36 pm]\nhttps://cloud.githubusercontent.com/assets/3156487/12374237/0bdc1c0e-bc63-11e5-9b69-b82ba5a13baf.png\n[image: screen shot 2016-01-16 at 3 04 55 pm]\nhttps://cloud.githubusercontent.com/assets/3156487/12374239/22edf872-bc63-11e5-9cea-aadd485b99a1.png\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1108.\n. We won't change the current behavior of defaulting to value since its a normal practice in other data tools.  Other OpenRefine extensions can certainly override this behavior if need be, or have a preference toggle.. @ettorerizza @ostephens Yeap... Hence the new issue I created so we can deal with that in a different and new and improved way, like other code editors, notepads, etc. deal with this...  #1286 and we can show those non-breaking spaces (nbsp) as well as regular spaces and carriage returns, etc ... basically anything less than char(32) ... the unprintable range.   The idea in #1286 is to showcase unprintable chars in all of OpenRefine's various dialogs, facets, edit areas to deal with those kinds of hidden characters... easier.\n\n@ettorerizza I'm thinking that having a simplier easier to see escaping view (similar to this javascript library's compact 3rd form) is probably well suited to your needs... we just need to get our OpenRefine displays to present that nicely to you so you can work more fluidly with data like that... https://www.npmjs.com/package/string-escape#compact. @ostephens can you do an ad-hoc restyle, using browser dev tools and perhaps take a screenshot to show me how that might look like ?  \nUPDATE:  actually Owen, the css pre-wrap won't work, it changes the data value for the user.... and that's something we avoid.  We give users the tools and easy functions to change the data values as they need.  I think mainly this is just a display issue for the users data....and showing the real data values as I suggested in #1286 in various places in OpenRefine where it is important....is where we need to focus on.\nBTW, we are trying to get new advanced funding and contributors towards lots of issues and features that we want and need to improve in OpenRefine like #1286 ....stay tuned on that news :). @ostephens Hi Owen, so sorry, earlier I actually was testing pre-wrap in another grid technology, not OpenRefine's that resulted in my jumping to conclusions.\nThis is only a display issue and I can confirm in newest Firefox that white-space: pre-wrap works effectively\ndiv.data-table-cell-content {\n  line-height: 1.2;\n  color: #222;\n  position: relative;\n  white-space: pre-wrap;\n}. @ostephens well, with current human-computer interaction via mouse, there are 3+ buttons now, and some folks COULD use the middle mouse in various ways, but that is like 1000 support issues waiting to be written for us to triage :)  We'll keep our plans to simple 1 button processing for now and the future UI work.. @ettorerizza what about using Line-based ?  That's sorta OK ?  Just trying to know where in our code the issue really lies.\nUPDATE: Ah, you have weird enclosing quotation marks.  Set encoding to UTF-8 and also uncheck \"Quotation marks are used to enclose cells containing column separators\" and \"Parse next 1 lines as column headers\". The reference for the functions is here on our wiki:\nhttps://github.com/OpenRefine/OpenRefine/wiki/GREL-Other-Functions#jsoup-html-parsing-functions\nuse htmlText()\nvalue.parseHtml().select('body').htmlText()\n. @wetneb Do you have thoughts on this ?  or do we close this as a \"won't fix\" citing \"such and such reasoning\" ?. This is a general UI issue that affects many programs, not just OpenRefine.\nThe issue is really \"How do we handle displaying long strings of information to a user?\",  \"Do we autowrap?\", \"Do not autowrap and instead create a horizontal scrollbar for the user to pan left or right to review long strings?\"  \"How far should the pan left or right be allowed, should their ever be a max limit?\"\nSome of these challenges are answered with a few Jquery plugins.\nHere is one that comes to mind that could help with a horizontal scroll that auto updates and recalculates scrollbar sizes on javascript/ajax changes such as what happens during Clustering.\nhttps://gromo.github.io/jquery.scrollbar\n. This isn't a OpenRefine issue, but a local environment issue.  The proof is in our code here:\nhttps://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/importing/ImportingJob.java#L70\nBut to help you out, Check your TEMP and TMP environment variables for possible corruption, or just simply change them to another folder path like E:\\TEMP or whatever you want.\nOn my Windows 7 spare laptop, I have my User Environment Variables for TEMP and TMP pointing to\n%USERPROFILE%\\AppData\\Local\\Temp\n\n . @wetneb Considering your comment in #1496 about this issue,\nDo we consider this issue as not fixable/not warranted ?  I need a use case for this issue presented so that it shows where something with GREL Eval causes tremendous grief for folks...otherwise \ud83d\udc4e Can you put that use case together for me to understand the impact of this issue ?. @viktordickmm I'm not sure what your facet represents... can you show me the data that forms the cluster ?  Did you choose UTF-8 for the character encoding when you originally imported into OpenRefine ?\n\n. @wetneb see referenced issue #1228 , This might be slightly related ? The hidden 1234.0 value not matching the shown value 1234 ?  dunno, but ...\nNeed to dive in and verify this with a test and perhaps somehow make a test case for this also if we can.. @ettorerizza yeah, ok...  I think we really should not hide the fact that there is precision with an amount of scale to you as a user.  A number of 123.45 has a precision of 5 and a scale of 2 (two digits to the right of the decimal)  But 123.0 also has precision and scale...just because a 0 is there, you could say its a whole number, but technically, to scientists, its not a whole number yet until its been converted to 123.  And I don't think we should ever auto convert things during import, but let the users make the choice afterwards.\n@wetneb @jackyq2015 Now I am convinced we ALWAYS need to display precision/scale to users that have imported numbers with any amount of scale.  Even if the scale is only 1 digit and holds a 0 as in @ettorerizza case.  Other tools always default to display precision and scale, and I think we should make that change as well.  We can call it the \"Truthful data feature\" :)\n. @wetneb yeap, absolutely, its non-trivial and requires a full review at multiple points and our tests would have to be much more rugged as well :). @jackyq2015 @wetneb Right, cross() is based on string, but thats where it becomes a usability issue for folks unaware like @ettorerizza who would expect then an ERROR or hint that crossing a Number column against a Text column is not going to work out well :)  OK, so....\n\nImprove the test cases for this. Ensuring that 2 Numbers can cross and 2 Strings can cross, but that trying to cross a Number and a String properly throws an error to the user.\n\nAll agreed ?. Thanks for this fix !  Tested against English to ensure no breakage of functionality of Clustering methods.. We have not compiled against Java 9 yet, or tested against it yet.  Only Java 8 so far...but sure, we'll eventually release an updated OpenRefine that supports Java 9 and perhaps utilize a few new features and slightly improve performance in a few areas of OpenRefine.\nIf you feel that Java 9 support is worthwhile, and potentially encourage our developers here to get Java 9 supported sooner... feel free to place a bounty of whatever $$$ against this issue you have opened at https://www.bountysource.com/teams/openrefine/issues\n. You can get that by changing your importer options for the header row.\nBy not selecting for a header row (0) then you will get OpenRefine default column naming convention:\nColumn 1 , Column 2 , Column 3 ...\n@griii2 Does that help your use case ?\n. I think 3 would be most appropriate given OpenRefine is a data cleansing tool.  And we should try to cleanse a bit of that.  Folks expect it not to fail for a String value and we can just escape for those cases.. Looks like we might be able to just expand on the CASE logic for VALUE_EMBEDDED_OBJECT\nhttps://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/importers/JsonImporter.java#L125\nusing JsonToken\nhttp://fasterxml.github.io/jackson-core/javadoc/2.2.0/index.html?com/fasterxml/jackson/core/JsonParseException.html\nor perhaps using JsonParser.Feature methods might do it\nhttp://fasterxml.github.io/jackson-core/javadoc/2.2.0/index.html?com/fasterxml/jackson/core/JsonParseException.html. @VitoLiao Did you solve this somehow Vito ?. @VitoLiao No problem. OK thanks anyway.. @wetneb Let me try this out locally and review tonight. And thanks for working on those issues I brought up about the Wikidata recon service interface.. @wetneb ok, let me try the simpler version.. This works well Antonin.  Good Job. Minor issues for the fuzzy matching scoring on the service itself that I will address on your project. @wetneb Do you want or need the full bounty ?  Should it be split among others ? Magnus ?  Thoughts ?. @joewiz Already asked Tom Morris and Jacky on our OpenRefine developers mailing list.  I'd like to see a formal release of 2.6 with Wikidata Recon.\nI'll work with @wetneb on adding documentation on his project Wiki here....then later we can also add that same information to our official OpenRefine Wiki on Github.. @wetneb Looks good.  And tested OK.  But just to be sure... please check on this issue https://github.com/wetneb/openrefine-wikidata/issues/6 before we go further with a merge.  I just want to make sure.. Thanks ! Merged.. With Chrome and Firefox you can right click on an element on a webpage and\nget lots of details on elements through an Inspector window.  The inspector\nwindow highlights elements and you can move your mouse around in the\ninspector window of the HTML code.\nIn Chrome or Firefox -\n1. Right click on the element you are interested in in the page itself and\nchoose Inspect Element.\n2. The inspector window appears with HTML code.\n3. In the inspector window move your mouse around and hover over elements\nto highlight them.\n4. Right click on the  or whatever element that you are really\ninterested in extracting then choose Copy -> Copy Selector (or Css Selector)\n5. You can then use that which is copied to the clipboard and paste into\nthe GREL select() itself, since that just uses Jsoup under the covers.\nmw-content-text > div.bandeau-article.bandeau-niveau-ebauche.plainlinks\nbecomes\nvalue.parseHtml().select(\"\ndiv.bandeau-article.bandeau-niveau-ebauche.plainlinks \")\nMore details here in these links and read the important tip about refering\nto Jsoup's specific Selector command (which is what the GREL select()\nactually exposes:\nhttps://github.com/OpenRefine/OpenRefine/wiki/StrippingHTML\nhttps://jsoup.org/apidocs/org/jsoup/select/Selector.html\n-Thad\n+ThadGuidry https://www.google.com/+ThadGuidry\n. Good Catch Scotty !\nI'll take a look later tonight and let you know what I find out.\nThanks for this report.. Looks like your operation history is not exactly aligned with the dataset... or you missed an operation (or more) somehow...\nIn the below section...I get a ERROR for column not found for \"264$c 2\" removal.\nThe column-split operation before it however, makes a \"264$c 1\" column.\n{\n    \"op\": \"core/column-split\",\n    \"description\": \"Split column 264$c by separator\",\n    \"engineConfig\": {\n      \"mode\": \"record-based\",\n      \"facets\": []\n    },\n    \"columnName\": \"264$c\",\n    \"guessCellType\": false,\n    \"removeOriginalColumn\": true,\n    \"mode\": \"separator\",\n    \"separator\": \"||\",\n    \"regex\": false,\n    \"maxColumns\": 2\n  },\n  {\n    \"op\": \"core/column-removal\",\n    \"description\": \"Remove column 264$c 2\",\n    \"columnName\": \"264$c 2\"\n  },. OK understood.  Errors are to be expected then with your JSON operations....OK.\nSo, Looks like real issue with null is with the simple operation at step 66 that uses\nvalue.toString()\non column 959$a\nAnother thing I'm wondering is if your intent might have been to do a blankdown on that column instead ?  There are 2 record rows under 10706 ... in the 700 column for the Added Entry-Personal Name in Marc 21 format I notice :)\nBut yes, I am able to reproduce the null.  Let me dig in now.. @scottythered found it.  We weren't passing a valid test case in the past that would throw an error.  Tom Morris @tfmorris  (and @DavidLeoni ) fixed this and now doing value.toString() will actually give you the null as a real String literal of \"null\"\nDetails here: https://github.com/OpenRefine/OpenRefine/pull/915\nThe code change itself is here: https://github.com/OpenRefine/OpenRefine/commit/4eb6eb6eda50f4cbf81c937219c02227d9c4fb29\nTest is here: https://github.com/OpenRefine/OpenRefine/blob/master/main/tests/server/src/com/google/refine/tests/expr/functions/strings/ToFromConversionTests.java#L117\n. @jackyq2015 Can you take a look into this ?  Did you build with the right parameters ? See https://github.com/OpenRefine/OpenRefine/blob/master/refine#L540. Hi @Shamanou !  Thanks for working on an extension to OpenRefine !\nWe wouldn't want to merge this into our project directly of course.  Although we appreciate the request.\nIts more likely that folks would download OpenRefine and then could install your extension. Or could just download your Fairifier from your Github repo or website and install and run it.\nAnything else we can help you with ?. BTW @Shamanou , once you provide a distribution, we would be happy to also list it here http://openrefine.org/download.html with the other distributions.. @adehner If you look at your TSV file in Notepad... what is the Character or Control Char that tells you and me that the line has ended ?  Things like this are usually a data problem in the original file when it was created.\nIf you have mixed CR/LF and LF without always enclosing quotes for all your data fields, then OpenRefine will not be able to figure out when a line ends or begins either.\nTry to export from your original data source and select to \"always enclose columns/fields with quotes\" or similar.  Then try importing into OpenRefine.\nHowever, if your stuck with this really bad format of a file...then your only recourse is to try to use the Line-based importer in OpenRefine... but even that might be wrong, if your original data is so bad that even you can't see some kind of pattern of when a line starts or ends in Notepad.  If you can see a true pattern that signifies when a line should end, then you can use external scripting tools (Python, SED, etc) to try to clean it up, wrap with enclosing quotes, and then output a TSV or CSV file with meaningful lines.\nLet us know.\n. Oh do you mean that you wanted a way to quickly just re-read your original input file ?\nUnfortunately we don't have that feature, because OpenRefine was purposely built to deal with static files typically. (those that are not changing)\nIts sounds like your looking for a file monitoring solution that other ETL programs have.  Or you can create a new Project automatically with scripting using one of the OpenRefine clients like Paul Makepeaces Python client listed here: https://github.com/OpenRefine/OpenRefine/wiki/Documentation-For-Developers\nThere might also be extensions written by others that are linked to on the same Wiki page above that might accomplish what your looking for, but I'm not certain.\n. This is looking better, however, there will need to be a \"Use Cache\" checkbox option on the Fetch URLs dialog.\nI'm sorry, but this is a hard requirement because many of our folks use OpenRefine for querying (polling) different kinds of networks, devices, services, sensors, etc. for research and fun and sometimes expect to get different data returned upon each successive request to the same URL.  Things like IoT sensor measurement reading, etc.  (and yes, I know OpenRefine is not a good fit for doing things like this, but alas, a lot of our users are non-programmers)\nPlease add a \"Use Cache\" checkbox option with some hover text that briefly describes what it will/won't do.   I'm OK with it defaulting to ON.  You can store this option in our /Preferences as noted here: OpenRefine/main/webapp/modules/core/scripts/preferences.js and here OpenRefine/main/src/com/google/refine/commands/GetAllPreferencesCommand.java\nYou can also store the cache size and invalidation delay in our Preferences also.\nThanks Antonin, coming along great !\n. @wetneb Looks good and works well I think. Just tested the functionality against your random.org test urls.\n@jackyq2015 any more thoughts ? good to merge ?\n. @jackyq2015 @wetneb I think we still have an issue lurking somewhere... on repeated builds on Travis we get a failing test because of this error https://travis-ci.org/OpenRefine/OpenRefine/jobs/308507072#L1314\n. @wetneb awesomeness. Thank you !. @wetneb yes, option 2.  Although, I'm not sure if some of the older recon services, like VIAF, still provide the service or not (but it shouldn't stop our wanted position to extend a users data with additional data.  https://github.com/OpenRefine/OpenRefine/wiki/Reconciliation-Service-API#examples\nJust to make sure your looking at the right places also Antonin...\n2.6-rc1 was the last time we had freebase extension fully included here: https://github.com/OpenRefine/OpenRefine/tree/v2.6-rc1/extensions\nand the start of the operation was called ExtendDataOperation.java btw\nI snipped the following from our wiki as a bit of provenance as well... https://github.com/OpenRefine/OpenRefine/wiki/Reconciliation-Service-API\n-- snip --\nBecause the reconciliation service only returns a single ID, getting additional column values could be done one of two ways:\n\n\nThe Freebase way using a separate command a la \"Add column by fetching from Freebase\" (perhaps just using \"Add column by fetching URL\" against a local REST service)\n\n\nBy defining multiple \"types\" in the reconciliation service mapped to the same table, but returning different columns as the ID. In Mr/Ms 9er's example, you could have:\n\n\nCA Corporation ID returning the id column value as the ID\n\nCA Corporation CorpID returning the corpid column value as the ID\n\nboth mapped to the same table. The user would then choose the \"type\" that they want based which ID they want returned.. @wetneb Another option for standardizing would be not using REST and instead work on adding support for GraphQL to eliminate extra round trips and this also aligns with how MQL query engine worked in Freebase and stills does with Facebook and Financial Times.  Query for the exact data that you need.  These articles explain it nicely https://medium.freecodecamp.org/rest-apis-are-rest-in-peace-apis-long-live-graphql-d412e559d8e4 and https://code.facebook.com/posts/1691455094417024/graphql-a-data-query-language/. @xiaomj Ah, your right !  Undo Redo of Flags and Stars does not work well at all !  Thanks for this fix !. @thatbudakguy Curious, can you try again with OpenRefine 2.7rc2 beta release ?. @joewiz Can you assist @Archaeo-Programmer with this issue ?. @sheva605 sorry but this doesn't make a lot of sense to me.  If you want to exclude parts of the Undo/Redo history when you do Extract...just uncheck the operation box.  Can you explain further what you mean by merge everyone based on those two methods ? which methods ?. @sheva605 Currently we don't support merging data by applying String Similarity functions directly.  Instead our Clustering functions create \"mass edit\" operations with From / To as shown in the below example export, and what your describing as not wanting OpenRefine to peform.  We could add the feature that your wanting, but this would need to be a feature request and we might never get to it, or you could post a bounty $$$ to get someone to hack on it for you.\nFor your wants and needs, there are other String Similarity tools like Python scripts and libraries, here's one , even Office 365 has a feature or plugin for merging similar strings.  Do some web searches, post on forums, and you'll find lots of other tools and ways to accomplish this without using OpenRefine. Best of luck to you.\njson\n[\n  {\n    \"op\": \"core/mass-edit\",\n    \"description\": \"Mass edit cells in column Column 1\",\n    \"engineConfig\": {\n      \"mode\": \"row-based\",\n      \"facets\": []\n    },\n    \"columnName\": \"Column 1\",\n    \"expression\": \"value\",\n    \"edits\": [\n      {\n        \"fromBlank\": false,\n        \"fromError\": false,\n        \"from\": [\n          \"Michael\",\n          \"Micheal\"\n        ],\n        \"to\": \"Michael\"\n      },\n      {\n        \"fromBlank\": false,\n        \"fromError\": false,\n        \"from\": [\n          \"George\",\n          \"Gorge\"\n        ],\n        \"to\": \"George\"\n      }\n    ]\n  }\n]. @ostephens Nope.  Can't recreate now.  Seems like from the log that Apache POI had a marshalling problem probably thrown up from some bit/byte that it didn't like while Zipping.  I'll close this.  We can reopen when I or someone else comes across the issue again on export creating large-ish .XLSX files. @wetneb You gonna hack on this ? :)  Perhaps some of the extensions might have some reusable code ?. @wetneb I tried :)  Thanks for helping out where you can, nonetheless.. This is a question.  Not an issue.  Please ask questions on our mailing list.\n(Yes OpenRefine can read in CSV files or just Lines of text using Line-based importer)\nPlease ask further questions on our helpful mailing list and do not open issues on Github which are used for opening bug reports or feature requests. Thanks.\n. We use these that support English and German.\norg.apache.commons.codec.language.Metaphone;\norg.apache.commons.codec.language.DoubleMetaphone;\norg.apache.commons.codec.language.ColognePhonetic;\norg.apache.commons.codec.language.Soundex;\nand Metaphone3 ... there are some good code comments from developer Lawrence Philips in here about how Metaphone3 works with English and its keying system https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/clustering/binning/Metaphone3.java\nGerman language support uses... \nhttps://commons.apache.org/proper/commons-codec/apidocs/index.html?org/apache/commons/codec/language/ColognePhonetic.html\n(we could add others or you or someone else could with a bit of Java and a pull request)\n(for instance Beider-Morse could be added to support any language   https://commons.apache.org/proper/commons-codec/archives/1.10/apidocs/index.html?org/apache/commons/codec/language/bm/BeiderMorseEncoder.html)\n(how Apache Solr deals with it, is similar to how OpenRefine does... but we did not implement BM and a few others https://cwiki.apache.org/confluence/display/solr/Phonetic+Matching)\n. There are many syntactical changes that can be applied to our source code since our base was Java 1.5 at the time of our first release.  However, we'd rather not refactor this quite yet against our master branch, but these kind of changes will happen in a future refactoring branch.  We'll close this issue for now but appreciate your effort to help.  In fact, if your a Java developer, please have a look at our issues listing that are tagged as a bug or enhancement and work to resolve those.  That kind of effort would be very appreciated.  Thanks again.. @desmorto in fact, if you want I can begin to create a Java8 branch for you to request a pull against.  That would give us a base to begin most of the refactoring work and you could gladly contribute to that branch.  Again, we don't want to introduce major code changes in master directly just yet...but do that work in another branch that we could call Java8-refactor.  Thanks again.  Let me create that branch now.. @desmorto Done.  Go ahead with a new pull request against 'java8-refactor'.  And you can continue your contributions to that branch.. Thanks ! @desmorto . This would need to be a fork of OpenRefine that someone else can certainly maintain.  As it is now, our developer resources are already constrained enough for existing feature requests and user bases on Linux, PC, and Mac.  But asking us to build and support another platform is stretching us too thinly.\nPersonally, I think getting OpenRefine to run on alternate platforms would be a lofty but educational goal for someone to hack on, if not limiting.. @Yuutakasan hmm, that's weird.  What is interesting is that it is showing 4 bytes (4 question marks) to hold just 1 character.  I see that the code for that last character is actually 6 bytes however (which is the maximum that UTF-8 can hold per character.  \n\ud844\udf1b  =  \\x0A\\xF0\\xA1\\x8C\\x9B\\x0A\nFurther interesting is that when I copy and paste your last character into a single OpenRefine cell, I actually get a different character...\n\u131b = \\xE1\\x8C\\x9B\ninstead of\n\ud844\udf1b = \\xF0\\xA1\\x8C\\x9B\n@jackyq2015 Can you debug this ?. @Yuutakasan When I export your import.txt file... I get\n\n\u6709\u9650\u4f1a\u793e\u306a\u3079\u8336\u5c4b\u3042\u3055\ud844\udf1b\nYou are probably not using a viewer like Notepad++ or similar that can show that last character as being \\xED\\xA1\\x84\\xED\\xBC\\x9B  ?\nBut regardless... its a bug somewhere because somehow during export we change the bytes...\nfrom\n\\xF0\\xA1\\x8C\\x9B\nto\n\\xED\\xA1\\x84\\xED\\xBC\\x9B\n. @Yuutakasan Thanks, we'll have to let @jackyq2015 look into this specifically.  My hunch is that we might not actually be storing it correctly in cell and so this https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/exporters/CsvExporter.java#L108 might be giving back the wrong data in the first place.  Otherwise its an issue in csvwriter itself here https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/exporters/CsvExporter.java#L114. @Yuutakasan Yes, but you can also test it by adding it to the refine.ini file and starting refine.bat or refine.sh if your on Linux.  Just uncomment the JAVA_OPTIONS= line. You might not be running OpenRefine with enough memory...especially if you have a really long string that your trying to parse out.  https://github.com/OpenRefine/OpenRefine/wiki/FAQ%3A-Allocate-More-Memory\nAn alternative is to use Jython as your expression engine instead of GREL.\nimport re\nm = re.search('(?<=abc)def', 'abcdef')\nreturn m.group(0)\nMore info on our wiki here: https://github.com/OpenRefine/OpenRefine/wiki/Jython\nI'll close this issue out as its a known handling case (probably the lookahead in the Java Regex Engine as always) that we can assist with on our mailing list.  It's not a bug.. @felixlohmeier This has introduced a bug for Windows users.  It seems the default REFINE_MIN_MEMORY is not getting set in refine.ini (you left it commented and it needs to be uncommented) otherwise it doesn't get set and the following happens:\nC:\\Users\\eguitha\\git\\OpenRefine>refine\nInvalid initial heap size: -Xms\nError: Could not create the Java Virtual Machine.\nError: A fatal exception has occurred. Program will exit.. @felixlohmeier I've pushed the fix and better comment for REFINE_AUTOSAVE_PERIOD.  Just wanted you to be aware @felixlohmeier . @hpiedcoq Can you give us 1 row of your data and the regex your trying on which column ?. @felixlohmeier That would be nice Felix.  Jacky and I chatted about a few other things on Sunday, like perhaps adding 1 or 2 more unit tests for the cross() work...other than that, its currently shippable and I'm using the trunk build now.  Any bugs or issues we can probably resolve down the line, so I'd say go ahead and close the issue so that Jacky can withdraw his funds :)  He did a great job on it, and now as a bonus, because of this work, we now have a way to easily generate summed up column values now as part of a cross() function, a much requested feature that Excel does in Pivot Tables but we never did https://groups.google.com/forum/#!topic/openrefine/CfNurLR5CII :)  I'll post about how to do that later on the wiki... we could probably even add that function as a column operation later on perhaps if folks want it easier.. The visualizations provided by the open source Facets project is built upon Polymer components and Typescript.\nI do not see our team re-architecting our existing facets that our users are accustomed to now.\nBut there is no reason why someone else could not produce a new extension that could provide the Facets project or something like it for enhanced visualizations within OpenRefine or outside of it, even in a new browser tab or with a different server, or even fork OpenRefine and produce better facet visualization based on some libraries or newer web components.\nThanks for the suggestion.  I do hope someone works on some part of the above that I mentioned.  But our team won't for the foreseeable future.. OpenRefine's mission is to provide users a nice interface for cleaning messy data and light analysis towards the goal of cleaning messy data.  We didn't design it for Statistical Analysis, but there are some Extensions that provide a bit of this.  You might be better off looking at real Stat tools or Machine Learning tools.. Looks good enough for now and to let folks build and play and test more.  I'll merge this in knowing that the API can change and you'll have more PR's :)  Great Job again @wetneb . @fmaali The build broke... Hmm, I wonder if its an env issue ?  Can you take a look  https://travis-ci.org/OpenRefine/OpenRefine/jobs/255029967. @fmaali I'm checking that now.... @fmaali OK, there's some weirdness happening on Travis for me... its just spinning when I try to do a sync....that doesn't seem right.  I'll try again tomorrow (although their status light shows green like all systems are OK)\nUPDATE:  Ah... its because of their move to Trusty Ubuntu from Precise.  See their blog https://blog.travis-ci.com/2017-07-11-trusty-as-default-linux-is-coming\nWhich happened TODAY. July 18\nSo, yeah...lets try again tomorrow. :)  or the next day :) :). @fmaali\nbuild 364 failed https://travis-ci.org/OpenRefine/OpenRefine/builds/255029966\nbuild 365 passed https://travis-ci.org/OpenRefine/OpenRefine/builds/255062631\nI think we're good now.. 1. Support any SPARQL endpoint, but maybe we actually want to just put effort into Wikidata.\n2. Mapping of Prefixes.. @belm104 your asking for something that doesn't apply to normal patterns.  There's no notation or pattern or hint or key that could tell OpenRefine that the values need to be aligned a certain way other than the key column.  The function only takes 1 KEY column and 1 VALUE column and converts the KEYs into columns... and then bins the values under each column.  That's it.  Its pretty dumb and simple and extremely useful.....but not for your case that you describe.  And the expectation you described is an antipattern to my eyes.  Its possible that your use case was a constant Ordinal value of 3 ?  Since you have 3 rows for every key ?  If that's your use case, then the correct function is actually Transpose.  Choose your Column 2 to Transpose with a value of 3.  But again, I still don't know if that's the pattern your actually trying to work with or not.\n@wetneb is correct.  We designed the Columnize and Transpose functions for simple cases.  I.E. David and I never intended to have full Excel like Pivot Table functionality with them...although we wanted to get there by perhaps having a separate matrix dialog show on a new browser tab...and that did come up during conversations that would provide Pivot Table like functionality with more advanced features ... but it was on our long range plans that we discussed privately and never got worked on.  But that has nothing to do with the expectaton of @belm104 and their use case which I still don't understand.\nI would document the current behavior of Columnize (keys will be converted to columns and values for each will be binned under each key) and its intended uses on our wiki, but also note to users that its limited in functionality and not intended for full Pivot Table like functionality as found in Excel, and ETL tools.. ALL - Often we have this idea of transposing our messy data from denormalized fashion to another more normalized form where that form is of the concept of Record Rows or \"relatedness\".  When I say antipattern, I am talking not about the dataset we have in Column1 and Column2...but the dataset we WANT from it like a row -1;6;1 \n@belm104 has some expectation of what their Row or Record Row needs to be for some reason, and I am trying to understand that need, but do not see it clearly.  Let me explain further...\nTaking @belm104 's original use case data of those 2 columns:\nColumn1;Column2\nSourceFile1;2\nSourceFile1;3\nSourceFile1;-1\nSourceFile2;3\nSourceFile2;4\nSourceFile2;6\nSourceFile3;-3\nSourceFile3;4\nSourceFile3;1\nIn other words, I have yet to hear WHY they need a row in the form of\n2;3;-3\nor\n-1;6;1\nHOW are 2 and 3 and -3 ... related ?\nHOW are -1 and 6 and 1 ... related ?\nHOW do I tell a machine like OpenRefine that they are related ?  Do I use a Key ?  Which Key ?  Do I use Ordinal numbering ?  If so, then which columns and how should we sort that prior to transpose ?\n. @wetneb  Look again.\nColumn1;Column2\nSourceFile1;2\nSourceFile1;3\nSourceFile2;3\nSourceFile3;-3\n2 and 3 are \"related\"... -3 is not. @wetneb we welcome users always.  But me saying antipattern does not reflect any negativity.\nIf you take the original 2 column dataset provided at the top of the issue...\nand in OpenRefine edit the Column2 cell of \"-1\" and change to \"1\"\nand then edit the Column2 cell of \"1\" and change to \"-1\"\nThen you can see how the function works.\nBut I am still wondering if any of you have an expectation of flattening somehow when Column 1 is sorted as @ostephens example forms ?  That's what I am trying to discover here.  We might be able to solve that also with another option\n. We walk subsequent Rows in the KEY column.  So...the order of the KEY column will have an affect.  Is that what all the confusion was about with the 4 of you ?  I thought that was clear from our code ?  Doesn't anyone read our code ? :)\nOn the serious side, we could probably add more comments in the function, but its already pretty clear just reading all the comments that we have now in KeyValueColumnizeOperation.java doesn't everyone think ?\nhttps://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/operations/cell/KeyValueColumnizeOperation.java#L142\nAlso I just noticed this :)\nhttps://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/operations/cell/KeyValueColumnizeOperation.java#L219\nand what do we do when there's no KEY for a row ? We just copy the row https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/operations/cell/KeyValueColumnizeOperation.java#L154. I elect.... Owen to work on this :). @wetneb liking Codacy so far ?. @wetneb OK.  For this one, as long as we get the additional test and things pass and Travis is happy and no errors (warnings OK for now) on Codacy, then you can accept commit.  We'll probably want to tweak Codacy for our project eventually (ignoring a few things, etc)... we should probably schedule a hangout with Jacky after summer break..September-ish.  Sound good ?. Breathe everyone. :)\nDon't worry about compatibility with external projects.  Others outside of OpenRefine can take that task because its just simple JSON with our existing metadata format.\n@fpompermaier We have a plan, long term for bigger data under Performance Improvements 2017 \nAll -\n1. What you want at the end of the day is reusing our simple key/value pair storage per project.  We have that in place and it's stored like so in metadata.json \n\n\nFor those doing the work, this search actually shows quite well (using \"last modified\") all the parts of OpenRefine that get touched or will need to be touched or reviewed. https://github.com/OpenRefine/OpenRefine/search?utf8=%E2%9C%93&q=Last+Modified&type=\n\n\nI'd prefer that the design allow the user with a button and simple dialog (reuse our existing dialogs in many places already to allow checkbox choice) to choose which keys are important to show from that metadata on the project open ui and show them as columns as @ettorerizza wants.  Don't pick which keys for them, but having the initial default of created and notes would be a very wise choice, the other key/values should be allowed to be filled in by the author.  Similar to the UI experience we have for our preferences.vt   You could have a button next to each project name that opens the metadata table reading in the metadata.json and allowing editing in table form, like we do for preferences.vt\nSee [1] https://github.com/OpenRefine/OpenRefine/blob/master/main/webapp/modules/core/scripts/preferences.js\n[2] https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/ProjectMetadata.java#L108 , etc\n. @ettorerizza Yeap, that's the idea, an ABOUT or ? question mark button next to each project name that can open up a table of KEY / VALUE columns similar to our preferences.vt  Also, being able to click checkboxes (up to perhaps max 4, or whatever works) those KEY's Values that you want to \"SHOW ON PROJECT PAGE\" as an additional column (space available) for easier instant viewing when the project page shows.. I agree with @magdmartin Let's have a \"predefined\" set of fields that the preferences.vt would have and from those, select few of the fields that are exposed and displayed on the Project homepage by default out-of-the-box installation of OpenRefine.\n\n\nI have no opinion on what those \"predefined\" fields should be, but defer to the community to decide.  After all, they will be able to change those fields at anytime with the \"About\" link or \"Metadata\" link, or whatever we decide to call the link.\nRegarding the implementation, the preferences.vt dataset can be arrayed.  The \"preferences\" for OpenRefine program itself and the \"metadata\" for a ProjectID.  We want to ensure we keep those separate views or tabs or whatever.  \"preferences\" affect ALL Projects.  \"metadata\" is individual per ProjectID.  However we want a single link or button for each ProjectID in the Project homepage listing to click on and takes our users to that ProjectID's \"metadata\" to edit.  I do not see a need for a \"preferences\" for each ProjectID, but that could also be explored or warranted if needed.  (By the way, Extensions have their own preferences setup already and its separate from this discussion.)\nHope that is clear and makes sense @wetneb @magdmartin @jackyq2015 . @jackyq2015 what appears or happens when the users clicks on all those EDIT buttons ?  Why have an EDIT button , when clickable input fields could be used and then a single cancel and save button ?. @jackyq2015 ok understood.  1 step at a time :). @jackyq2015 @ettorerizza The idea was to have each column as optional to display on the project page.  We just need a checkbox column (Show/Hide) next to each metadata column name on the edit dialog to handle this.  The Project Name column should always show and not allow to hide.  We'll provide resizeable columns throughout the UI's later on in further milestone after December 2017.. @wetneb @jackyq2015 My Comments are in italics above.. Jacky just need to make the fields selectable to display or not.  That probably will solve your problem better.. @ettorerizza Yes, these need to be cleaned up a bit in our CSS files.\n@jackyq2015 It annoys me A LOT.  Let's open additional issues to note the problems and then fix them.  I also would like to see the Transform window back to the way it was as not automatically stretched 100% in width of window.\nNext Release is blocked until these UI issues can be cleaned up.. Let us know why its failing.  I'm very busy this week at work, no time to dig into issues.  Thanks for looking at this @wetneb .. @wetneb The forward plan was to drop oraclejdk7 and go to 8.  Help there is appreciated.. @wetneb I think we'll need to edit our travis.yml file then.... see https://github.com/travis-ci/travis-ci/issues/5897\nbasically add\n\"addons\": {\n    \"apt\": {\n      \"packages\": [\n        \"oracle-java8-installer\"\n      ]\n    }\n  },\nlike this guy did https://travis-ci.org/gsmet/hibernate-validator/jobs/181629047/config\notherwise we get the Travis error \n$ jdk_switcher use oraclejdk7\nSwitching to Oracle JDK7 (java-7-oracle), JAVA_HOME will be set to /usr/lib/jvm/java-7-oracle\nupdate-java-alternatives: directory does not exist: /usr/lib/jvm/java-7-oracle. @wetneb The issue could be fixed with that addon package, but doesn't matter now, and oraclejdk7 is end of life and why it was removed in Ubuntu trusty.  Looks like your good now against oraclejdk8 and openjdk7 as evidenced here https://travis-ci.org/OpenRefine/OpenRefine/builds/260572450. I'd prefer we use Codecov...or Codacy ... both free for open source projects\nhttps://github.com/marketplace/codecov\nhttps://github.com/marketplace/codacy\nCodacy has a nicer view even for pull requests.. @wetneb Just use Gradle instead.  I've done the work already.  You can add anything else you need into the build.gradle file on our work in progress branch.  Then pull over what we need into master branch from it.\nhttps://github.com/OpenRefine/OpenRefine/tree/java8-refactor. @wetneb yeah, so we are already \"gradleized\" in the java8-refactor branch...  I'm a Gradle expert as well, btw, but still don't know enough shit :)  ping me on dev list if you need assist or want to schedule a hangout.. @wetneb sweet ! and looks like we now have the 2 new badge icons that are clickable on the main Github project page https://github.com/OpenRefine/OpenRefine\nbuild passing Codacy B coverage 20% Bountysource $150 in 1 bounty. Better I think for us long term would be Renjin http://docs.renjin.org/en/latest/introduction.html\nI also really like the fact that it uses javax.scripting interfaces http://docs.renjin.org/en/latest/library/evaluating.html\nI think just having R within OpenRefine would REALLY expand our user base.. UPDATE: So I posted a nice email to the R Lang users group to let them know we'd love to collaborate and explore some use cases, and not just this one.  Who knows, maybe later after our UI Refresh, they could help build some cool extensions for data exploration.  So we'll see some folks coming into this issue and also asking about things on our mailing list.  Expect it to get busy around here !  Like Alex @akbertram coming into the picture to take a look at things and see where he can help out with Renjin ! Thanks Alex !!!. @psychemedia Do you know R Lang well enough ?  Would you like to have a Hangout this weekend to show me ideas ?. @psychemedia Thanks Tony.  We are now documenting ideas and more technical details into a Wiki page so that its more readable. But feel free to continue discussion in this issue or on our mailing list.. So the world has certainly changed since this issue was first opened in just 2 years time.\nThere is now growing support for Polygot Applications through various tooling.\nOne of those is GraalVM\nGraal can help and do many things actually, like make Java programs faster or make applications extensible.\nCurrently GraalVM has limited support for Ruby, R, Python.\nLots of cool things, like guest language functions can be eval'ed and used as Java Values\nI allow others to have full rights on experimentation or ideas :-)\n. @wetneb yea exactly my feelings for same reasons.. @wetneb I did install the webhook for Codacy manually.  Looks like their auto-install ON/OFF switch for it is a bit buggy even for an admin.  no one online in their chat at this hour to talk to about it.  Regardless, it looks like its setup for PUSH monitoring now.  I've given you admin access, and your now part of the CORE team.  We know where you sleep. :)  Welcome to CORE !. @narendravardi Go for it.  Submit pull request when done.. @narendravardi it all starts here https://github.com/OpenRefine/OpenRefine/blob/master/main/webapp/modules/core/scripts/views/data-table/menu-edit-cells.js#L206\nBUT you cannot just change that existing \"to Text\" operation because many folks already depend on its current functionality as its expected when you convert Java Numbers to Strings (and we don't want to throw away the original value that Apache POI found in the Excel cell either) https://docs.oracle.com/javase/tutorial/java/data/converting.html\nAlso, when we display a Number in the Cells in our datagrid, we are not displaying the .0 for whole numbers.  That's a design choice we made long ago.  And want to keep that. (and keep them green colored :)  )\nSo...\nI think we might want to have a new operation for this use case ?\n@ettorerizza what do you think ?  how about a \"chomp Decimal Text\" or \"strip Fraction Text\" or ?\nUPDATE:  Oh boy, I just thought of a wrinkle in this as well... not all users have a locale that uses a period or dot as the decimal separator... it all depends on their locale setting in their OS.  It could be a comma if they are from Sweden :)  So that proposed new operation will need to know their locale setting and that's pretty easy to find with a Java system call and then you'll know.  https://stackoverflow.com/a/2123168/1100717. Hmm... I'm wondering if this issue is also showing itself in subtle ways even for cross() problems as in this guy's problem #1158 . @narendravardi How's your progress coming on this issue ?. @narendravardi How's your progress on this ?. @jackyq2015 this needs to be a display option then, I would say.  So let's make an enhancement around the need to SHOW FULL PRECISION or somesuch....it has been asked for before on the mailing list in the past.  Do you agree with adding a new display option, Jacky ?. @jackyq2015 cool. glad we agree. @wetneb Dude, don't even ask... just get that done !  It would be super awesome.  Yes anything that can help folks translate OpenRefine to more languages easily = A GOOD THING.  In fact, your Wikipedia experience, etc is most helpful for driving localization efforts and I give you free reign on that, since I have no opinion or much expertise in those areas.. @wetneb the pushes are fine.  I like seeing those fixed fast...not worried about abuse.. @wetneb The approach is different for many different use cases, and it is not always about metadata.  This shift approach is just 1 operation of many that are needed in OpenRefine.  When and where we allow folks to perform them for different reasons will need to be explored more, sure.  What we are trying to do now is \"being better at capturing use cases for discussion/review later for enhancements however they may eventually land as functions, operations, importing features, etc.\"  Having said that...let's review !\n\n\n[ ]  we can also have an improvement on the XLS importer, but not only there, but on any of our row based importers.  The OP wanted a way to attach his metadata rows to each and every data row...that could be easily allowed by a simple checkbox option during import called \"add header rows as additional columns to every row\" ?  Design mocks needed.\n\n\n[ ] Certainly the dialogs for the existing Transpose operations can be enhanced.  What we need are design mocks.. @wetneb If I recall, I think I asked David H. to consider Fetch URLs and Reconcile as long running operations and hence no automatic project save occurs.  Honestly, I cannot recall...so...We need to\n\n\n[ ]  check the code on that \n\n\nand could change the logic so that project saves are configurable and could occur more rapidly than default (ex. every 4 mins).. @wetneb your probably right, like I said...I cannot recall, and I am not very intimate anymore with our code in a lot of areas. But I can give lots of pointers to folks that have more time so they can get intimate with our code :). You just need to custom package them so Jython and Butterfly can see them.\nJust put your packages in Jython folder. You can ask Jython folks more how\nto do this. Then when our Butterfly inits Jython your set.\n. @ettorerizza Can we close this ?  or do you need something more from us on this issue ?. @ettorerizza Ah, you want a way to share Expressions...like perhaps having a window in OpenRefine to search by categories and then scrolling in that category to see the community Expressions, etc. and then you can click on one to use it.  Do you ever use the \"Starred\" feature ?  Anyways, similar to our own Starred but allow a more flexible, larger, community sharing window for searching for Expressions.  OK understood.  Let's keep this open and rename the title to reflect the exact need.. @tentonsofhours Your username on Windows is Ukrainian or Russian or a close cousin :) that has UTF-8 characters...so that's the reason.  We have seen this happen before with other users.  Your only choice is to create a different username in Windows using basic ASCII letters and then run OpenRefine under that user.  Sorry, but this is not a OpenRefine issue but more of a system / Jython issue that will be fixed by the Jython team in the near future.  Let us know how things go, so that we can close this issue once you have it fixed.. @wetneb I had been tracking it.  The original bug is here http://bugs.jython.org/issue2331 and was fixed here https://github.com/jythontools/jython/commit/c61eaefb774cdb174472265d3c27fc5b87febd34\nJython 2.7.1 has just been released last month (also the reason why I asked if we could release a OpenRefine 2.7.1 - but we are not tied directly to Jython version, its just a coincidence) http://fwierzbicki.blogspot.com/2017/07/jython-271-final-released.html\nI had asked Jacky to just build Jython 2.7.1 SNAPSHOT and then include it...but didn't happen.\nIf you want you can pull into our master branch that standalone jar now and update our dependencies, etc.   We can then cut a OpenRefine 2.7.1 release which I would like you to handle to get experience with it.\n. @wetneb Sweble Wikitext Parser is ASL 2.0 license so its fine to include...but you have to also include a jar that contains the source code as well, like we do for the others here https://github.com/OpenRefine/OpenRefine/tree/master/main/webapp/WEB-INF/lib-src\nand you also need to update any text files like README, etc...that mention our dependencies and include Sweble Wikitext Parser to that list.. @wetneb Looks like there are still conflicting files that need to be addressed ?. Ah ok. I'm still getting used to the new PR views with Codacy.\n. Well your welcome to switch out to something else that works well. Try one\nof the other integrations from the Github Marketplace. Many to try out and\nsee if they fit better for us.\n. @wetneb Codacy does show coverage btw.  Its under View All link on the right side of dashboard under Coverage https://www.codacy.com/app/OpenRefine/OpenRefine/dashboard  But perhaps other tools show this a bit better or more towards your taste.  Just try a few other coverage tools to get a feel for what you prefer.  I have no preference, they all are similar, just some are more denser than others in the views. (easier for old eyes or not :)  )\nCodecov seems to be the 2nd or 3rd most utilized in Github projects from my cursory filter search on Github.. @wetneb Codacy already gives us the badge on our main page...and we are at 20%.  The code smells can be tweaked in the Codacy admin pages... we probably should just set some time this weekend between the 3-4 of us and just come to agreements on all this... would be much easier I think.  I don't even have a good email address for you, btw... can you email me ?  I'll setup a Google Hangout for this weekend.. @wetneb OK, I tweaked Codacy for our project, should be better now that jquery libs are excluded.  I couldn't get Codecov to do the easy setup...it requires a project change with Gradle and Jacoco.  So that's a no-go.. No delete for the Jython 2.7.0 jar file ? or am I missing something or my git index is freaking out ?. @wetneb yeah, its my git index... fixed it... so this is good.  MERGE PLEASE. @wetneb OK, I'll try to re-enable it.  I tried before in the past and didn't seem to help, but who knows.. @eximius313 Did OpenRefine guess the correct encoding in the importer options at the bottom of the Preview as UTF-8 with Bom ?  or just UTF-8 ?  Did you adjust or change the encoding options at the bottom of the importer Preview ?. I don't think there is much we can do here other than escaping a column name.\nPerhaps something like\n\\u00A0UNID\nWould that help you catch something next time @eximius313 ?. @ostephens \nRE 1 - No it won't overlap with #1286 since that will be a CSS style applied, not a value replacement.  For this issue, I would rather do a value replacement by just escaping hidden characters on a created column name as in my example and then the user can see clearly and can rename it.\nRE 2 No, that can cause a few other problems.  We try really hard to give the users all their data that they import, even on Column names.  They might want to clean up their source generation or whatever, so let's be a good citizen and just inform them visually, casually.  So let's just give them their data...but slightly tweak for hidden characters in Column names.  Once #1286 lands then they also will see hidden characters in cell values as well.  But for column names, let's just escape text, instead of CSS styling.\n. @eximius313 \"invalid\" means different things to different people and machines.  That's why.\nMaybe someone before thought it was nice to have a black heart because they LOVE something about a column name.  http://graphemica.com/%E2%9D%A4\nBut that black heart will actually display.  The issue is when things are hidden and do not display.\nI am suggesting that it is a bad practice to hide things from users...when they are hidden...you don't know or cannot see if it has an impact.  So this is more about showing users those things that are hidden.\nThe best way to do that in any programming language is un-hide those characters...and that's always done with escaping into displayable character sequences.  Like /u00A0 or whatever.. @tmmcub You might be causing OpenRefine to run out of memory.  Try allocating more memory to OpenRefine when you work with large record files.  Follow this guide and then let us know if it fixes the issue for you https://github.com/OpenRefine/OpenRefine/wiki/FAQ#out-of-memory-errors---feels-slow---could-not-reserve-enough-space-for-object-heap\nTo specify a record path for a JSON file, you just click with your mouse in the preview to highlight the JSON object that you wish to work with.  It will pull out all those same objects from the JSON array and load them into record rows so you can work with the data.\nIf your doing the above and your still having problems, then my hunch is that there is some data corruption in the JSON file itself.  Use an online Json file validator to ensure the file is not borked.. @tmmcub sure thing.  glad to help.. @ettorerizza The free memory detection is being worked on in #1295. @jackyq2015 @ettorerizza @wetneb I think this makes sense actually.  Downsides ?\n-                    return d >= _from && d < _to;\n +                    return d >= _from && d <= _to;. @wetneb Do you know where in the Facet UI code we are doing it wrong ? Is it just the JSON or the Facet range handler as well ?. @wetneb OK, let's open a new issue for this bug then, and we can investigate further.. @wetneb This is probably an Apache POI bug again which we use for loading XLSX files\nMy suggestion @bluntmagic would be to Save as CSV in Excel or LibreOffice Calc and then open the CSV file in OpenRefine.  If privacy is not a concern you could upload somewhere for us to grab the XLSX file and try to provide a fix or at least let the Apache POI team know about where the issue is at with your file and hopefully improve OpenRefine in the future.. @wetneb I need a visual on this to understand it better.  Just scribble on some paper and snap it with your phone and upload is fine. :)  Would they be indexed somehow ?  part of a JSON blob ?. @wetneb AH, ok. @wetneb lol, crazy guy... yeah this looks good.  Thanks for the fix man, this bug has also upset me for a while.. @ettorerizza Curious, How well does smartSplit() address the same problem ?\nYou may want to partition and keep the separator value ?  split() is not the only weapon.\npartition(/a/, true)\nIts not a bug, by the way, just how it works and why we have smartSplit() and partition()\n. @ettorerizza Glad you like the other functions. :)  Yeah, its best to just think of split() as instead a cut()...because internally that's about what its doing, nothing fancy.  We put the fancy and smarts into the other 2 functions :) \nClosing as resolved. @wetneb It was @stefanom as the original author of fingerprint() and he would tell you that normalization is a tricky thing, even within a single language because of differences in dialects, writing habits, and then there's history where things change, i.e., the NEW normal :) \nHaving said that, its probably because he was looking for efficiency and doing the sort and remove duplicate tokens so that he didn't have to convert all of them to their ASCII equivalent.. @wetneb I'm just saying that you will find that some folks (including me) might treat this as a false positive in some other languages.  I'm fine with the changes since they take care of the 90% common cases anyways.  David's tests did not account for @ettorerizza case and you have fixed that. Thanks.. @wetneb I am not a complete language expert so I don't know every use case.\n@ettorerizza So the idea of fingerprint() is to generalize language string tokens.  Some folks might not want to generalize, which is your use case I think... so you would not want to use fingerprint() in that case.  fingerprint() will remove duplicates to perform the generalization and normalization and I would say that is not conservative...not all users want that to happen and that's fine... we have other algorithms to use.  What makes 2 strings different is opinionated and is why we have many algorithms available for clustering.  But perhaps there's a need for one more ?  flatFingerprint() or somesuch that does not remove duplicates ?  Dunno up to you and what you want.  But changing the functionality of fingerprint() to fit every use case is not something I want to do, bear that in mind.\n. @ettorerizza it can be 100% of the cases, if it only deals with whitespace and nothing else...but then you have cases like mine and other scientists that deal with Unicode(160) non-breaking spaces .... non-breaking means don't breakup the string on this space because its significant, I even put a recipe towards this on year 1 of OpenRefine https://github.com/OpenRefine/OpenRefine/wiki/Recipes#question-marks--showing-in-your-data\nCurious, @ettorerizza So which rules would you consider 'safe' to allow for your 'safe clustering' ?\nTry to capture what your thinking about exactly in a new issue so we can track it.. @wetneb FYI, Most of the open source world has aligned to GAVC convention borrowed from Maven.\nGroup:Artifact,:Version:Classifier.\norg.openrefine:OpenRefine:2.7.1:SNAPSHOT\nand filenames look like\nopenrefine-2.7.1-SNAPSHOT.zip\nopenrefine-2.7.1-rc1.zip\nTypically, tag names are just the VC parts ... version and classifier\n2.7.1-SNAPSHOT\n2.7.1-rc1\n. Good Job Guys !  :). @ostephens I gotta ask... How the hell do other Java applications that run on Mac handle all this ? What do they do now compared to how things were when this article was written by Oracle https://docs.oracle.com/javase/7/docs/technotes/guides/jweb/packagingAppsForMac.html  ?\nMy thoughts are that we don't bundle...but instead provide nice scripts for Mac users to run, to make @paregorios problem... the Cert installation or whatever on Mac for OpenRefine usage... EASIER.  Let do that instead.  (decouple the issue from our build process, so we don't have to alter our build process). @ostephens At the end of the day, I'd like you to just close this issue and then provide some good documentation, avoidance, best practice, etc... on our Wiki to help navigate this kind of problem with Server side security when using our Fetch URLs function against a server using HTTPS.   We can then point folks to that page to try different options, and advise of security concerns as well.  A Wiki page is a much better way to deal with this very niche \"area\" (its not an issue) in particular.  Which really OpenRefine dev / support team should not be getting into the weeds with.   Let's call that Wiki page \"Fetching URLs with HTTPS\" or whatever.\nThanks All.\n. @ettorerizza The problem still might exist.  Users in constrained IT environments might not see OpenRefine being allowed to start because JRE or JDK is caught as suspicious by a corporate security program running on their computer.  We cannot help that however, so its a non-issue for us.\nI think I would be OK with a bundled JRE, and allowing folks to still override the bundled JRE to that of their $JAVA_HOME\nThe problem that I don't know about on OSX is about Signing ?  Is it a requirement at the OSX level or is it just a requirement for Apple Store ?  In other words, does the OpenRefine+JRE bundled app on OSX need to be signed somehow or not ?\n. @tonero101 Has a Mac and might be able to throw together a script to make this easier.  He's working on the JDBC enhancement also.  Tony what do you say, can you tackle a script for Mac users to make cacerts installation and management easier ?\n@ostephens @jackyq2015 We need to drop Java7 now and go head first into Java8+  Do we have a issue labeled as \"task\" for that yet ?. The Expression input box itself (inside the Transform dialog) is resizeable. And works well in Firefox as originally intended and designed.\nAgreed that Dialogs should be resizeable primarily so that users can see longer value strings previewed by stretching things out.\n. @jackyq2015 yes on 2.7.0.0 openrefine.exe , the expression input box (inside Transform dialog) is resizeable on both Chrome and Firefox...but not IE. @richyvk Just FYI on the design we're looking towards and in our future plans are to have the expression box a dockable element that can be snapped to the left, right, bottom, etc (same for facets) and that can remain open and allow you to quickly click on a column and then move your mouse to that left, bottom, or right expression box and start transforming with Cloure, Python, GREL, etc.  Even optionally moving windows and panels into a separate browser tab or window if you have 2 or more screens.  We have big plans to make a lot of the edit menus disappear and instead move a lot of operations that you do in OpenRefine into more flow based work.  Think Photoshop for data :). @richyvk A long ways off.. I'm fine with \"item\" or \"thing\".... \"topic\" is broader than either one of those, but that's how Freebase was...you could have either an \"item\" or a \"subject\" or \"category\" and were equal footing, all represented as a general \"topic\".  This doesn't align with SKOS or Schema.org's Thing or WIkidata's Item.  So agree to change this to a narrower term which makes it easier for alignment with any Ontology system or Vocabulary.\n. @ettorerizza  Your not mistaken, Split multi-valued cells is String only currently.\nBut the enhancement could easily be done by someone. Like @ostephens :) \nTake the code that is here: https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/expr/functions/strings/Split.java#L69\nand implement it similarly here: https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/expr/functions/strings/SplitByCharType.java#L53. @ostephens my preference would be \"never trim\" because I'm a purist and because I work with UTF-8 quite a bit and unprintable control characters that are significant to my work.\nEventually, what will help with all that is ...\nI want to see in OpenRefine a toggle mode for DISPLAY UNPRINTABLE CHARS for our users (like the ACK and CR/LF below)...so that they show up.  This is work to be done in the future in part of our UI Enhancement project(s). \n\n. @ostephens Awesome job on contributions Owen !  Your Rocking !  Keep 'em coming ! . @wetneb Is there export functionality on Wikimedia Commons for the .tab to CSV or supporting CSV on the Web ?. @jackyq2015 Right understood.. When can we add typing for cell values ?  Does the 1st task of changing the model need to be done in order for this to happen ?  I am wondering of a timeline for this.. we marked as 3.5 milestone (separation front/back) , but does it need to stretch to even 4.0 (UI changes begin for next August 2019) ? . @wetneb uh... yeah...THAT was not there before... lololol. @jackyq2015 @ValentinChCloud\nI should probably put this into our Wiki somewhere but... here's additional info for both of you to be aware of.  Something that I deal with often in the enterprise with Java apps running in containers.\nA few things to be aware of when dealing with memory usage with Java Apps running in Containers\u2026\n\n\nThe kernel can sometimes kill the Java process if JVM memory usage grows over the cgroups limit defined for a Docker container.  Deep details are here: https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt\n\n\nThe default GC algorithm uses 1/4th of all memory from the host OS, if Xmx option is not defined explicitly.\nhttp://blog.jelastic.com/2017/04/13/java-ram-usage-in-containers-top-5-tips-not-to-lose-your-memory/\n\n\nIn case you run into performance memory issues with Java Apps running in Containers,\nyou have the above as a general guide.. It's in our future plans to support database connectivity options. This\nwill happen next year. Stay tuned.\nOn Sat, Oct 21, 2017, 3:38 AM aadrian notifications@github.com wrote:\n\n@wetneb https://github.com/wetneb thank you for the information.\nI don't think OpenRefine supports JDBC natively.\nThis looks quite unusual to me, since the server is based on Java, and\nJava's main advantage, purpose and selling point with businesses is mostly\nJDBC .\nYou can try exporting your dataset from JDBC to a format that OpenRefine\nsupports.\nHmm, using those formats would also mean to loose information (metadata,\nconstraints, relationships etc), so it would pretty defy the whole purpose\nof \"refining\" the data :( .\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1277#issuecomment-338375685,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA8NRpcQKIu_UazIDvMMAwssakIXRuZcks5sua1ngaJpZM4QBegF\n.\n. There are additional advanced parameters that are also useful to have.\nWe can add an Advanced Tab later on however.  For now I like just going with a simple enumerated list or dropdown for the DB Type\n\nA Validation Query field is also useful. Pentaho, NiFi and others have that.\nAnd perhaps a maximum wait time just in case and others parameters.\nAlso a JDBC Property Name / Value array is also useful.  Pentaho's UI for that is just another dialog 2 columns: Property Name | Value \nand the user fills in and they are added to the JDBC connection URL as parameters.\nhttps://help.pentaho.com/Documentation/5.1/0P0/0U0/010/000\nhttps://nifi.apache.org/docs/nifi-docs/components/org.apache.nifi/nifi-dbcp-service-nar/1.4.0/org.apache.nifi.dbcp.DBCPConnectionPool/index.html. @kidehen Its implemented now as \"database\" extension, available in OpenRefine 3.0\nSee release notes https://github.com/openrefine/openrefine/releases. @kidehen We're open source.  So hack on the inner of the \"database\" extension  Maybe you could hack on it to create a GenericConnectionManager and GenericDatabaseService classes that could be used...\"generically\" ?\n@tcbuzor @jackyq2015  Can you assist Kingsley on what he needs to do , other than providing the ConnectionManager and DatabaseService classes ?\nOh, you'll need potentially a JDBC driver for the type of DB even if \"generic\".  JDBC drivers are aware of the full command set of a particular DB.  Here's a list of popular ones: http://www.sql-workbench.net/manual/jdbc-setup.html. @wetneb @ostephens @jackyq2015 I'd rather not disable that particular PMD rule... its a fairly best practice : Method names should always begin with a lower case character, and should not contain underscores.\nI understand that our current quality with existing code / tests is lackluster and violates many of the quality rules in Codacy, but that is also something that each one of us can take responsibility towards and begin cleaning up as we touch our aging code base.\nWhen time is of the essence... ignore...but then clean up later as we can.  It helps other contributors coming into the project to see understandable clean best practice syntax :). @weblate Coveralls ? or you meant Codacy right ?. @weblate Coveralls is cleaner because Checkstyle is OFF by default.  So perhaps your asking to just turn off Checkstyle... that I can agree on I guess... we (the other devs that care about this) can always work on style improvements on as needed basis directly with Codacy analysis through their site itself https://www.codacy.com/app/OpenRefine/OpenRefine/dashboard \nTurning off Checkstyle now in Codacy .... @weblate OK, I've turned off all the \"non-sensical\" style checks.  There might be more within PMD that might cause you some grief... just let me know on those as they come up.. @weblate actually, even better I think for everyone, I turned off the Pull Request Commenting from Codacy... it should only now provide the summary checking and then you can click on Details to see more.. @remram44 yeah, on it now :)  5 mins :). @remram44 Ignored all under /main/webapp/modules/core/externals/. @remram44 Hey Remi ! Just checking in on your progress.. @ostephens @ettorerizza @wetneb Do you guys also like that orange colored example ?  Does it have a good fit and finish for cell styling in OpenRefine ?  Of course we will work with a Google Designer for some of this later on, but for now I wanted to just show a better example of how the look could be.  Both Jacky and I prefer the coloring and style using graphic symbols, like example 2 shows, rather than the text based control character symbols as shown in example 1 and typically used in other programs such as \"SP\", \"ACK\", \"CR\", \"DEL\", \"TAB\", etc. \nVOTE: Agree that example 2 is more towards our liking and preference for the future UI toggle of \"display all chars\" ?\n. @ostephens Why do you like them more ?  because they stand out more ?  they are a bit larger ?\n\n\n. @ostephens interesting you mention you have to learn what the icons/graphics stand for.  The icons and graphics are actually international and well defined.  You will see them on Chinese and Ukrainian keyboards for instance.  Look at your Tab key.  But I think I understand...we're a bit of an internet society now and the historical ways of labeling don't want to budge. :) \nIts a bit 'in your face'... but that's a pretty useful thing for things that were hidden before and make you want to beat your head into a wall when things don't work as intended :)\nOK.  We will go with just simple text highlighted labels instead of the graphic symbols.. @ostephens No, I agree with you now.  The \\n\\l for instance will be better represented, its actually 2 separate control chars, and not just 1.  Despite their being Unicode graphic forms that represent some of these control characters, not all of the char(0) - char(31) are well represented in those graphic forms.\nWe'll use the Unicode abbreviations (NUL, DEL, etc) https://en.wikipedia.org/wiki/Control_character but not for HT and VT.. @ostephens Personally not a fan of the newer diagonal flows on post-2003 Unicode Control Pictures.  I prefer the original horizontal flows just as they were from Bigelow 1993 and just scaling smaller with good fonts as I gave in the examples before. Which is in LucidaSansUnicode font.\nhttps://pdfs.semanticscholar.org/4a8e/2fa49b13a2bcabf7d0ebdf2b55b0816abaf8.pdf\nBut perhaps on second thought...to support those that are younger aged :) the diagonal flows might be better overall since they won't confuse with regular text, but regardless my idea was to enforce some different CSS coloring for the control chars/pics.. @jackyq2015 I've renamed the issue to where I think the problem really lies, in the datePart() and not toDate().  Specifically I'm asking for the datePart(\"S\") or datePart(\"milliseconds\") and I expected that datePart(\"milliseconds\") should have given only the millisecond part as a result.  Instead what happens is that OpenRefine chomps off 7 seconds from the requested millisecond part or 7000 millis in other words...and instead only returns me the remainder, which is 380 millis.\nTo a data scientist... they lost 7 seconds (7000 very important milliseconds) and OpenRefine only returns the fractional 380 millis and not the full 7380 millis, which I know, when converted is 7 seconds and 380 millis, but that's not the part that I asked for is it ? I didn't ask OpenRefine for the seconds part, I asked for part milliseconds.  I also know that SSSS is not typical and instead that SSS is more typical output for millisecond from time systems, but it does happen in the wild with ultra precise timing instruments, I've actually used some of those very instruments myself in the US Air Force some that go down to a millionth of a second https://en.wikipedia.org/wiki/Orders_of_magnitude_(time)\ndatePart() should be wired so that there is no loss of value to our users.  But what seems to be happening is that part of the value, the 7000 millis is moved into the seconds part...and only returns the 380 in the millisecond part.  I.E.  datePart() is shifting and converting parts.  I don't think this is wise to our users, many of whom expect it to work like R lang does and moment.js and its get() method and many other libraries that are strict when you request the millisecond part and it would return the 7380 or just 738.\nPlease take a look into how moment.js handles this\nhttps://momentjs.com/docs/#/parsing/\nhttps://momentjs.com/docs/#/get-set/minute/\nhttps://momentjs.com/docs/#/get-set/get/\nLet's do some research here, but I think that long term, we might be better served to switch out to using java.time for our time handling needs in OpenRefine.  \"Joda-Time is the de facto standard date and time library for Java prior to Java SE 8. Users are now asked to migrate to java.time (JSR-310).\"\n. Referenced Mailing list discussion for this issue https://groups.google.com/d/topic/openrefine/uU6O3OG4eng/discussion. @jackyq2015 \n1. Correct.  datePart() is just like moment.get() ... where we are GETTING a particular Part of a DateTime.  Our docs actually say \"Returns part of a date\".  So if we ask to return \"ms\", or \"milliseconds\", it should only return the \"ms\" part....NOT convert the whole Date to milliseconds.  That's a completely different function below.\n\nWe also still need a Unix Epoch Time conversion function as asked in #608.\nBecause we can do this... \n\"2014-03-14T05:30:04\".toDate().datePart(\"time\") -> 1394775004000\nbut no way to convert it back\n\"1394775004000\".toDate('d/M/y') -> ERROR\nhttps://www.mkyong.com/java/how-do-get-time-in-milliseconds-in-java/ \n\nUseful (use Instant):\nhttps://stackoverflow.com/questions/3371326/java-date-from-unix-timestamp#comment71374164_24703644\n. Team,  S L O W   D O  W    N  here. :)\nOverall there are several ways to approach the use case and making @felixlohmeier happy with essentially a faster \"split or join operations that are based on keys\".\nBut what I need to understand and what confuses me is a mixing of discussions all over the place and I want to know which one that @felixlohmeier is after.  Is it...\n\nWe are trying to attempt to use cross() in order to handle a key lookup.  And cross() does that very well now.  When the keys match, @felixlohmeier wants to perform further operation(s).\nWe are trying to perform a \"split function who's output goes into a key lookup argument in cross()\"\n\nThe problem with 2 is that it complicates matter because it means more memory is needed since data copying and cache storage is needed to hold the split key while lookups are performed.\nSo @felixlohmeier perhaps the problem your facing is that the split() is taking a really long time on your 1.5 million rows in Project A when you do only that operation ?  How long does it take if you do a Edit column -> Split into several columns ?\n. @wetneb the PR made a bit of sense to me, but now we have @felixlohmeier last 2 hours of comments.  So that changes things Antonin.  But yeah, I can admit that perhaps I was hasty and the use case was not quite so clear.  So let's clear it up now that I'm here on the issue.. @felixlohmeier Why are you using Split Multi-Valued cells instead of 'Edit column -> Split into several columns' ?  Is it because you need record rows made from Split Multi-Valued cells ?. @felixlohmeier well I was thinking also of you playing with our Transpose while your doing that.  OpenRefine's data model prefers 'more rows, less columns'.  Honestly however, if you've got that much data to manipulate, I'd be more than happy to have a Google hangout with you this weekend and just train you and show you tips in Pentaho ETL or any other tool designed better for this.  We can certainly put more into OpenRefine (and we will eventually I promise) to make this sort of thing easier, but honestly, and I am being dead serious, OpenRefine's core itself is not well designed to handle this use case.  However, tons of free off-the-shelf tools handle it with ease and in seconds.  I can do what your asking to do in Pentaho on 1 billion rows in about 22 secs.. @felixlohmeier AH... and now I have the proper context. :)  Ok, then your fine.  I'm sure other folks WILL find a splitCross() function useful, but I am actually against having additional argument based approaches.  Early on we developed GREL and its syntax to be more functional oriented and support the wonderful dot notation chaining.  If we overload our existing cross() with tons of arguments, then I fear the complexity becomes a tedious headache rather than GREL being a breath of fresh air.  Which is the full picture why its troublesome actually, its not about functionality as @jackyq2015 or @wetneb might worry about, but for me its about our user base and keeping GREL's ease of use.\nIncidentally, one of the ideas that is yet to be implemented is making it easier to write GREL to be \"a walk in the park\" (yeah, I actually hate our Expression Window and it could be done much more slicker if I had a UI expert latched to my designer brain) and also having CodeMirror support to make our GREL syntax even more vivid and appealing like autocompletion, autoclosing brackets, tons more.  Just need to write a tokenizer for GREL language, but before we do, I want to actually improve GREL syntax a bit, perhaps next year. https://codemirror.net/doc/manual.html#modeapi\n. @felixlohmeier Sure thing!  Nice working with you Felix on your interesting use case!. @jackyq2015 @felixlohmeier @wetneb For the big picture, I've begun describing the future state I'd like to see with cross() here https://github.com/OpenRefine/OpenRefine/projects/1#card-5380183. @claussni fix the 3 test failures... such as [testng] FAILED: crossFunctionCaseSensitiveTest. @wetneb Reverted for you.  Can you comment on any changed lines (using Github line commenting) where you have concerns so that everyone can share and hear your thoughts ?  We can certainly make our review policy more strict and adhere to a +2 +2 policy (2 reviewers saying good to merge) as well.. @ostephens same look and feel in Firefox and Chrome ?. @ostephens Just a minor training point.  In Java and Javascript, there are standard equality operands and STRICT equality operands.  One does the type conversion AND THEN does the equality check, and the other does not do type conversion, I.E. it is STRICT.  If your comparing identity, you want to ensure your using STRICT operands (also known as the identity compare operands) (===, !==) Always use the bible reference with a handy chart at the bottom https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Comparison_Operators\nIn Java, its common practice for String comparisons to use .equals() rather than === , but its a preference and the later only available in later JDK's.\nYour doing GREAT however, Owen, and really like these constant commits from you !  Keep up the learning !. @felixlohmeier Thanks for understanding Felix and great working with you on your interesting use case in #1289 !. \"Instead of auto set it, a responsive warning message when Out of Mem occurs could helpful\"\n@jackyq2015 Indeed, that also is needed.. @magdmartin on point 2 we do know what the FREE memory is...and taking 50% of that the first time they run OpenRefine shouldn't cause too many problems.  Yeah, we can do preference setup.. @jackyq2015 Can you confirm that the OSX options above work well enough for us to parse out the FREE memory ?. @jackyq2015 hmm... there's a Perl dependency... can you confirm if this works also ?\ntop -l 1 | grep ^PhysMem gets us closer to that ?. @jackyq2015 I thought of that also.  Let's change the need of this issue then to a notification only.\nWe inspect (using the 3 OS memory lookups that found to work well above) and then advise the user on startup... \"Hey ! We noticed that you have 6 Gigs of Free Memory available....OpenRefine can run better when given more memory.  Read our FAQ on how to allocate more memory here: blahURL\". This can even be a Javascript popup. @jackyq2015 Beautiful.  Good Job.\nD:\\git\\OpenRefine>refine\nYou have 12219M of free memory.\nYou current configuration will allow to use 1400M of memory.\nOpenRefine can run better when given more memory. Read our FAQ on how to allocate more memory here:\nhttps://github.com/OpenRefine/OpenRefine/wiki/FAQ:-Allocate-More-Memory\n.\n14:47:49.668 [            refine_server] Starting Server bound to '127.0.0.1:3333' (0ms)\n14:47:49.670 [            refine_server] refine.memory size: 1400M JVM Max heap: 1407188992 (2ms)\n14:47:49.682 [            refine_server] Initializing context: '/' from 'D:\\git\\OpenRefine\\main\\webapp' (12ms). @ettorerizza Matcher has much more that folks are probably not aware of, I myself have used Matcher.lookingAt() back in the day.  Better to link to the Official Java docs https://docs.oracle.com/javase/8/docs/api/java/util/regex/Matcher.html\n@jackyq2015 @ostephens Google Guava had nice helpers back in the day, but no longer in it, since Java has advanced. But this might help (they have interesting notes) https://github.com/google/guava/search?p=2&q=matcher+find%28%29&type=&utf8=%E2%9C%93 and in particular what their CommonMatcher provides https://github.com/google/guava/blob/master/guava/src/com/google/common/base/CommonMatcher.java\n@jackyq2015 @ostephens I also don't think we need to worry about NFA... but who knows what kind of gigantic strings that users might try to put into OpenRefine https://github.com/google/re2j  Let's skip worrying about that particular corner case.. @ostephens Yeap, but it should not throw an exception... we don't want it to...its a bad user experience....let me explain so it makes more sense and you can see the alternative way to handle this...\nOK, so...\nThe \\ is a valid regex character technically, in other words lets not treat it as a compile pattern error YET, because users are still typing to put something after it like \\d or \\b etc :)  We just need to pretty up an info message in the case of a single trailing slash \\ (not throw an error) I would say.  We need to tell the user that their expression is not understood YET and that a slash \\ has to have additional syntax after it.  We probably need to just catch that specific case and override with a better message to the user.  And luckily the slash \\ character is actually the only special character that we have to special case here and deal with btw.  Haven't looked how you did it, but perhaps it also could be approached another way with just escaping under the covers for the pattern ?  See the stackoverflow link in my initial comment.  \\ becomes \\\\ or \\\\\\\\ under the covers, etc might be option, but I'd prefer we see a message always for an incomplete \\ that is missing an additional parameter after it , such as \\d or \\w , etc.\nLook at how Python re does it for some inspiration as to the possible message to the user ... https://github.com/python/cpython/blob/master/Lib/re.py#L286\nUPDATE: actually Python does have special casing mapping as well \nhttps://github.com/python/cpython/blob/master/Lib/re.py#L251\nI've updated the initial comment to reflect the current state of things against our trunk.. @wetneb right.  its just the slash \\ really that will cause a bit of grief for users.  This is a common known thing to pretty up sometimes in tools that use java.util.regex. @ostephens Hi Owen, I think as a programmer your caught up on the error, which is expected, but I want you to approach this as you are showing an 8th grader a demo of text filtering.  How would you make this a better experience for them while also helping them to learn more?  Look at how other tools beautify this error.  Try it on https://regexr.com/  You can hover over the error and see a friendly message ... \"dangling backslash\".  Yeah, that means extra work for us and we also have to have a translatable mapping file, but so what, it brings value to our users everywhere we are using java.util.regex \nI create these issues so that even folks outside our community might sometimes tiptoe in and see something interesting they can fix and work on and improve our users experience as well.  This isn't something that necessarily needs to be worked on this week or next, but in my prioritizing as High we are noting that \"this is important to our users\".  Pattern matching and finding is not a niche area, as noted by Ettore's recent issue created to have a find() operation to help our basic match().  Looking for patterns is key to OpenRefine and anything we can do to help our users with that should be deemed very important.\nA reminder of WHO our users really are.  They are not programmers, we are.  As design lead for our community, I want to challenge and remind our contributors that \"we\" need to do the hard stuff to make our users lives easier and more pleasant.  David Huynh did back-flips for me as a user...and all of us benefit from the work he and others did to make our lives easier.\n(incidentally, its the same philosophy that Schema.org has, when we have to, we make it easier for consumers at the cost of it being harder for publishers). @ostephens You might also find a way to de-couple and reuse that effort in GREL in our expression dialog for match() and partition(), etc , i.e. all the places that use java.util.regex\nRegarding the lookbehind... hmm, the text filter still uses java here which does support lookbehind, although not the blazing fastest https://www.javaworld.com/article/2077757/core-java/optimizing-regular-expressions-in-java.html  Also, lookbehind had bugs back in Java 4 and 5 days but has been fixed.  Here's a primer that details important notes about lookbehind: https://www.regular-expressions.info/lookaround.html\n. @ostephens How's work progressing on this Owen ?. @ostephens of course its OK.  We appreciate any effort.. @ostephens Are we good on closing this issue out and triaging it into Milestone 2.9 as done ?  or does the team think there is more to do ??. @ettorerizza :) And guess what else David and I had envisioned and planned for back in the day but never implemented ?  Yeap, macros (That enhancement was here #109 , to basically creating your OWN menu items based on a GREL expression) and then having customizable menus to your liking (stored preferences, and UI is do-able with the right javascript libraries...Jquery had 4 plugins just for that primary purpose)  Feel free to find those issues and reference them here.. It's a feature and works on single column. I think we already have an issue somewhere to provide a unique rows facet that would do what your asking.  If not, please create one.. @eswright\nLet's look at the GREL expression in the B facet picture above.  It is asking for this in layman's terms:\n(Show me a count of rows in Column B where the value ) is greater than 1\nfacetCount(value, 'value', 'B') > 1\nNow let's look at the data rows that are shown with the A facet selected on 3 (clicked and shows orange color)\nOpenRefine's data structure grid looks like this in that picture:\ncolumn B\nrow value 2\nrow value 2\nrow value 7\nNow run the layman terms sentence in your head... what is the count or number of rows in Column B that has a value greater than 1 ?\nIf you arrived at a result of 3, then the Facet on B column showing a true 3 seems to be correct and accurate.  3 rows are matching that expression.\nBut I love pictures to explain better of what facetCount() really does and how it works.  facetCount() by the way is what we use for the Duplicates Facet counting and it simply takes 'value' in the normal case as shown in the above picture in this issue, but you could override this to say \"Only show me duplicate rows that have a value of 7\" like I have done so in this picture (notice how many rows are true that have a value of 7?  then what do you think the facetCount() result would be for \"true\" when I clicked OK button ?  If you guessed 2, you are correct !!! , there are 2 rows that equally have a value of 7 :\n\nNow let's really see what happens when I change back to just 'value' : \n\nThere we see that 5 total rows have duplicated values (5 true rows in the preview dialog).  Its not 3, because I have not selected on my A Facet on the 3.\nBut when I then select on 3 in my A Facet...\n\nI then expose an anomaly...\n\nor did I ?  Which row of value 7 is being presented here, you wonder ? (its row 5.) Wait, didn't we have 2 rows that had a value of 7 in B column , and not just one ?  Yes we did, and still do  (rows 5. and 6.) ...but then why isn't the other 7 row shown (row 6.)?   Because we filtered on A Facet for only the rows that match a value of 3 and not value of 1 !\nIn Summary:\nWe designed the Duplicates Facet to work \"alone\", and not really in tangent with other Facets at the same time.  In other words, the Duplicates Facet always looks at ALL ROWS in a column, until you change 'value' to something other than 'value'.  But out of the box, the Duplicates Facet has the GREL expression of : facetCount(value, 'value', 'B') > 1 where 'value' means all the values in a column, and its special in that way within the expression.  (Technically its an iterator variable, if you will, in our Java code for the Duplicates Facet functionality)\nIn other other words... a Duplicate value row...is always a Duplicate value row...no matter the other Facets that expose to show it, or hide it.\n. @eswright If you have a need for a new Duplicates Facet that is , a Filtered Duplicates Facet, then we could certainly provide that new Filtered Duplicates Facet that takes into account all the other facets.  But the Duplicates Facet will always work on ALL ROWS as designed and tested.  Feel free to create a new enhancement request if you need that additional Filtered Duplicates Facet.\n(by the way, the reason we would not want to change the logic of the existing Duplicates Facet, is because many thousands of folks already know how it works generally and are aware of the logic of it and have already built workflows with that existing logic, so we wouldn't want to change it...but doing a new or different Facet for some other logic is doable). @wetneb yes, but we could probably put some controls around that... for instance, always making filtered duplicates facet a LAST OUT facet.  In a hypothetical new UI, the \"filtered duplicates facet\" may not even be part of the regular facets views, where we might want to highlight to users that this is always a FINAL LAYER that is applied (David H. and I talked about final layers in the past, just like Photoshop does layer ordering with a last layer).  How we present these always final layers...could be a UI experiment or Proof of Concept for a budding UI intern or Google Summer Project.. @wetneb Can you add/squeeze this into 3.1 ?. All software produced in DERI is BSD licensed.  Even if @fmaali doesn't recall :)  He and Richard mentioned it once in a talk http://refine.deri.ie/  \nFork it and update the license text so it uses the 3 clause BSD https://opensource.org/licenses/BSD-3-Clause . It will work like this...\ncan take a String form:\nrange(\"2-4\") --> [2,3,4]\nor a String form in a value\nrange(value) --> [2,3,4]\nor 2 numbers\nrange(2, 4) --> [2,3,4]\nand doing join(\",\") will convert the Array to a String\nrange(2,4).join(\",\"). --> 2,3,4. @joanneong Thanks for working so diligently on this ! Great job !. Being test framework agnostic is a \"good thing\".\nOwen, it probably would be wise to go headless with PhantomJS.   I like\nthat Karma can support it.\nI myself have used Robot framework and also Sahi in the past.\n+1 Karma seems to take a much easier route from my quick inspection.\n-Thad\n+ThadGuidry https://plus.google.com/+ThadGuidry\n. @wetneb yup, no problem, thanks for advising.. Thanks for the temp fix on this Jacky.  Hoping we can get much better dialog handling in later milestones.  We'll close this now.. @jackyq2015 Oops, I think we should also ensure that there are no duplicate keys somehow as well ?\nIt seems sensible to NOT allow duplicate \"name\" pairs since its not useful to users.\ntest1 and test1 show on the homepage, but then About shows only the single test1 Key\n\nTry to Reproduce with...\nKey:\nuserMetadata\nValue:\n[{\"name\":\"client name\",\"display\":true},{\"name\":\"test1\",\"display\":true},{\"name\":\"test1\",\"display\":true}]\n\n. @wetneb Jacky and I would like to talk with you offline in a chat sometime.  Do you have time this weekend ?. @ostephens With #1342 this is now fixed, and the Row Count metadata column is updated immediately whenever our datagrid view is updated through /core/get-rows command https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/commands/row/GetRowsCommand.java\n. OpenRefine has traditionally stayed away from providing Statistics and will\nremain that way (yes, I myself am making a hard line, here's why).  There\nis such a rich ecosystem of great opensource tools that already provide\ntons of value for users.  And for OpenRefine users, its just an export away\nto get that value.\nWe can certainly make it a bit easier to integrate with other tools, and in\nfact the JDBC connector option that we are working on is one of those\nways.  We're working on others as well.  Keep the ideas coming on what\nother tools / processes that you would like OpenRefine to somehow\nintegrate.  That's what we want to hear about, (and not discussions about\nadding everything including the kitchen sink into OpenRefine) :). @ettorerizza The slippery slope.  Your asking questions about data, and more advanced questions.  Working with Arrays seems to be a growing need for our OpenRefine users and that question falls into that Category.  I'm worried and wondering about the presentation to the user as well as further operations needed or wanted.  Let's walk through some scenarios...\n\nYou ask OpenRefine that question with a new menu option on ColumnA \"UNION against other Column\" or through a Transform with a new GREL function.\n\ncells[\"ColumnA\"].union(ColumnB)\nIt produces what is called an Intersection in Mathematics ( A \u2229 B ) and its output could be represented as an array \n[\"Jean\",\"Albert\"] \nor held in heap memory, or not outputting anything at all, until we have the full command or operation that you want to perform with that Intersecting Set.\nDoes OpenRefine display that array ? always ? what if its 3.5 million elements in that Set ?\n\nYou ask OpenRefine to produce an Array on ColumnA with Transform menu option.\nWe give you a way to produce an Array from the Column's list of values.  We can hold it as a variable, call it \n\ncells[\"ColumnA\"].asArray() \n(similar to numpy.asarray)\nand it internally produces\n[\"Jean\",\"Albert\",\"Pierre\",\"Antonin\"]\nHow do you intend to further use it ?  What other commands would be useful before or after or with \ncells[\"ColumnA\"].asArray() ?\n\nExtending and overload join() seems hackish since it currently expects an Array, but we could make it produce one with a special case if the input was a Column list ? (I don't like special casing when there are alternative approaches, but...)\n\ncells.ColumnA.join(\",\") \n\n???. @ostephens Yeah, my scenarios are just to get you guys thinking , really thinking , about how you ideally would like to handle various things in OpenRefine.  Write them up together in a Google Doc and share it, link it here.  We need clarity from our Users on a lot of these advanced topics, then we can come to agreement.  I won't shut the door on this issue now, until we get more clarity from you guys. :)  But don't expect us to turn OpenRefine into a Stat tool, but sure some better advanced functions or viz or integrations with other tools we can work on.\n\nOK, now go collaborate and then get back to us in this issue. :) . @ostephens This should fix the issue in #1335 . . Update the download text showing only 2.8 ... but just point to the /releases page.  not directly to the tag for 2.8\n. @ettorerizza I'm curious however WHY it means several subsets.  Is it because we do a crappy job on hiding columns ?   Tucking things away that are no longer important as you refine and polish the data through additional column creation ?  What / where exactly are the pain points.  I'd like to discover those, rather than just giving you a new hammer.  Think through the problem carefully.  Imagine anything is possible, and just tell us or draw it on paper and take a photo.. @ettorerizza Perfect.  That's exactly the response I was hoping you would say.  And agree entirely.\nHowever, one thought that came up long ago from David and I was the undo/redo of Facets to store a state and we chatted about this long ago in Gridworks days.  But we never got around to researching more the complexity in our code or what would have to be refactored or changed.  The idea was generally to equate to the idea of Photoshop's Layers history...pretty powerful stuff...providing a snapshot button that stores the state of all your current Facets.\nThoughts on that idea ?  I mean in addition to this issue's immediate need.. @ettorerizza we would totally like to include vib-bits like features into OpenRefine, but that's always a time/expense decision.  What we can do and hope to do is after our UI refresh, help extension authors get things working again.\nFor Bountysource, I would advise to hold off.  In fact, we are soon to register as a non-profit and take donations directly.  We might want to actually turn off the BountySource system, but that needs to be put to vote on our mailing list. (feel free to start that email or @magdmartin can)\nAnyways...\nBut we agree on this particular issue's direct need:\nOne click new project creation from a state of facets.  Correct ?. @jackyq2015 Yes it falls back to English, @wetneb and I already confirmed this.. @wetneb ah!. @jackyq2015 and then how does the existing Pull Request affect this or that ? Move to jquery.i18. @jackyq2015 lolol, I KNOW that.  What's the effect of the changes in that pull request ?  How do they relate to this issue is my wonder as well.. Upgrade your curl ?  compare versions ?. @wetneb I disagree with @ettorerizza and purely because of semantics. :)  I'd rather NOT replace the \"subject\" field.  Here's why:\nIn Schema.org, DCMI, and even Wikidata itself these are different properties.\nTags can be of the form \"lamborghini time trials\" or even #michaeljacksonconcerts.\nWhich is different than the subjects of Automobile Racing and Pop Star.\nhttp://schema.org/keywords\nhttp://pending.schema.org/about\nLet's keep these concepts separate, even scientists treat them separate at times.. @wetneb Careful.  Just because \"subject\" is not useful to you, doesn't mean that @ostephens and others in OpenRefine's bibliographic world will not appreciate it.  :)\nThere's no harm in keeping \"subject\" and it provides value by NOT squashing 2 different concepts, A Tag, and A Subject into one thing.  It is best to keep these 2 concepts separate, especially in regards to any metadata discussion in OpenRefine.  This also helps when having to integrate data with Knowledge Based System (KBS) tools outside of OpenRefine used in libraries and research centers.  Our JDBC export feature will need an option to publish the metadata as well eventually, but can work on that later.  Many Database Schemas have 2 columns for this, one for Subject Matter and one for Tags.\n(specifically, for instance in MODS metadata http://www.loc.gov/standards/mods/mods.xsd the\n<xs:element ref=\"subject\"/> \n versus the\n<xs:element ref=\"relatedItem\"/>). @jackyq2015 yeap, I agree. @jackyq2015 feel free to merge when ready.  You can tackle remaining updates or bug fixes with another pull. Let's merge this as you say to avoid a 3 way merge.  Thanks team !. @fpompermaier Most folks just use PMD and its what Codacy uses behind the scenes for Code Style checking  https://marketplace.eclipse.org/content/eclipse-pmd\nFeel free to submit pull requests on all 365 issues it currently finds here :) \nhttps://www.codacy.com/app/OpenRefine/OpenRefine/issues?&filters=W3siaWQiOiJMYW5ndWFnZSIsInZhbHVlcyI6W251bGxdfSx7ImlkIjoiQ2F0ZWdvcnkiLCJ2YWx1ZXMiOlsiQ29kZSBTdHlsZSJdfSx7ImlkIjoiTGV2ZWwiLCJ2YWx1ZXMiOltudWxsXX0seyJpZCI6IlBhdHRlcm4iLCJ2YWx1ZXMiOltudWxsXX0seyJpZCI6IkF1dGhvciIsInZhbHVlcyI6W251bGxdfV0=. @tfmorris but David and Stefano wanted to specifically say \"emeritus\" to note they are no longer active, and not just say creators.  \"emeritus creators\" ???. @ostephens yeah, I thought of that and thought ... \"good enough\" but OK, let me edit and make the \"Emeritus Creators\". @wetneb OK ready for merge now also :). @magdmartin I'd prefer to remove the list of contributors from our website actually.  Maintainability and all that.. @wetneb OK, ready for merge now :). @magdmartin It was decided long ago that it would be \"nice to have\".  I freaking hate it and want to remove the .txt file :)  All in favor... Say Pickles !!. You guys are no fun. :)  No one said \"Pickles\" !!. @emoron Thanks for the issue!  However, we know we need to update our code base to work with the newest Google API's to get this working again.  We have this in our plans.\nSimilarly, I do get a slightly different error on a 36634 row by 13 column data set.\n21:17:40.844 [                   refine] POST /command/gdata/upload (167256ms)\nException in thread \"Thread-5\" java.lang.OutOfMemoryError: GC overhead limit exceeded\n        at com.sun.crypto.provider.CipherCore.doFinal(CipherCore.java:958)\n        at com.sun.crypto.provider.AESCipher.engineDoFinal(AESCipher.java:491)\n        at javax.crypto.Cipher.doFinal(Cipher.java:2377)\n        at sun.security.ssl.CipherBox.decrypt(Unknown Source)\n        at sun.security.ssl.InputRecord.decrypt(Unknown Source)\n        at sun.security.ssl.SSLSocketImpl.readRecord(Unknown Source)\n        at sun.security.ssl.SSLSocketImpl.readDataRecord(Unknown Source)\n        at sun.security.ssl.AppInputStream.read(Unknown Source)\n        at java.io.BufferedInputStream.fill(Unknown Source)\n        at java.io.BufferedInputStream.read1(Unknown Source)\n        at java.io.BufferedInputStream.read(Unknown Source)\n        at sun.net.www.http.ChunkedInputStream.readAheadBlocking(Unknown Source)\n        at sun.net.www.http.ChunkedInputStream.readAhead(Unknown Source)\n        at sun.net.www.http.ChunkedInputStream.read(Unknown Source)\n        at java.io.FilterInputStream.read(Unknown Source)\n        at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(Unknown Source)\n        at java.util.zip.InflaterInputStream.fill(Unknown Source)\n        at java.util.zip.InflaterInputStream.read(Unknown Source)\n        at java.util.zip.GZIPInputStream.read(Unknown Source)\n        at java.io.FilterInputStream.read(Unknown Source)\n        at java.io.PushbackInputStream.read(Unknown Source)\n        at sun.nio.cs.StreamDecoder.readBytes(Unknown Source)\n        at sun.nio.cs.StreamDecoder.implRead(Unknown Source)\n        at sun.nio.cs.StreamDecoder.read(Unknown Source)\n        at java.io.InputStreamReader.read(Unknown Source)\n        at com.google.gdata.util.io.base.UnicodeReader.read(UnicodeReader.java:110)\n        at org.apache.xerces.impl.XMLEntityScanner.load(Unknown Source)\n        at org.apache.xerces.impl.XMLEntityScanner.scanContent(Unknown Source)\n        at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanContent(Unknown Source)\n        at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown Source)\n        at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)\n        at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source). Tested and this is now resolved on Windows 10 with latest version from master branch (currently 3.1 beta).\nClosing.. @ostephens However Codacy doesn't seem to detect that falling through.  Click on the yellow banner on this for details  https://www.codacy.com/app/jackyq2015/OpenRefine/file/12515874477/issues/source?fileBranchId=5930252#l@result.result.startLine\nhmm, perhaps PMD is seeing that * and + cases as questionable?. @ostephens yes please. @Museumdatacleaner Can you provide a screenshot ?\nAre you importing a previous OpenRefine project (a .tar.gz file) ?\nAny errors you see in the Console or Terminal that OpenRefine is running in ?. Looks like your using IE, some older version...I'm wondering if there are critical Javascript errors in its console ?\nhttps://msdn.microsoft.com/en-us/library/dd565630(v=vs.85).aspx\nThere could also be an IE Addon that is causing some issue here.\nCan you also try newest Firefox or Chrome latest ?. @Museumdatacleaner Edge is pretty awesome actually on Windows 10 and OpenRefine works fine on it.. @wetneb qaResult displayed the gathered results from Freebase staff and expert judgments once a client recon data set was uploaded to Freebase.  \nFrom CHANGES.txt also\n```\n2.0 Release  (November 10, 2010 - first release as Google Refine)\nMajor Changes:\n- New extension architecture.\n- Generalized reconciliation framework that allows plugging in standard reconciliation services.\n- Support for QA on data loads into Freebase.\n- Timeline Facet (Issue 40 and 95)\n```. @xseris We had issues with earlier versions of Jackson, but that was nearly 7 years ago (@tfmorris can speak about that).  Jackson's CSV parser is considered production ready now since 2.3 and latest is now 2.9.2  Jackson also gives us the flexibility with CSVMapper or Factory...which will be useful to setSchema prior to reading/writing (via previous metadata inspection once we decide on approach in #1096 ) \nThe other choice would be https://commons.apache.org/proper/commons-csv/\nI'm not altogether worried about import performance since at the end of the day its dependent heavily on https://docs.oracle.com/javase/8/docs/api/java/io/File.html\nI am more worried about schema alignment and ease of use programmatically.. @ostephens Thanks for cleaning this up Owen ! . I think I fixed it.  I checked the option under Settings -> Branches to \"Restrict who can push to this branch\" and then added Weblate user.\n@wetneb Can you confirm ?\n. @jackyq2015 what do you mean ? They already provide that, we are just restricting direct pushes to Master branch and requiring pull request reviews before merging.. @wetneb Ah, approved review required.  let me see.... @wetneb Weird, it says that he should be allowed to push. See ?  Click the EDIT button at bottom next to Master branch on here:  https://github.com/OpenRefine/OpenRefine/settings/branches/master\n. @wetneb Hmm, shouldn't be hard.  PostCoreProcess supports updateOptions and we seem to already store and keep the expression of the bound Facet even after a Column rename, we just don't seem to update dynamically a particular Facet id.\n<a href=\"javascript:{}\" class=\"facet-choice-link\" bind=\"changeButton\" title=\"Current Expression: grel:value +12\">change</a>\n<span bind=\"titleSpan\">Source</span>. @wetneb Ah, yes, and that.. I have no problem with this file when I selected with my mouse on the whole document element \n<Documento version='1'>\n\nThe import and record view will be very slow to use because of technical details that we plan to fix later next year, but for now, its still usable, just slow and sluggish.\n. Some background history on the need...\nWe had heard the need for both Project notes and Column notes... I think the best way to handle that is with a new Menu option called Add Notes ... that does a \"no-op\" like David Huynh suggested in #368.\n\nSupport a Add Notes edit button that opens an freeform input box for Project level from the ALL menu.\nSupport a Add Notes edit button that opens an freeform input box for a Column menu (where the \"no-op\" holds the column name as well so that the user doesn't have to type the Column name, but can just say \"this column\" in the text anywhere).\n\nI am not worried about sequencing... I would let the user Add or Modify any text blob they want to the Notes input dialog box at both Project and Column level...and let them deal with the sequencing of any text by themselves by numbering or whatever characters or text they want to use to organize.  For example in a Column level...\n\"1. First I need to facet on this column to find all unique types.  2. Then I force uppercase all the unique types to make things look better.  3. Then I can work more with Column \"values_2017\" and see if there are duplicates\n\"\n. @tcbuzor @jackyq2015 Any idea why all of those failed \"loading workspace\" under /tmp on Travis ???  I wonder if there's a setting in Travis we need to enable/disable ?\n[testng] 04:58:35.883 [       FileProjectManager] Loading workspace: /tmp/OR_DBExtension_Test_WorkspaceDir5488033601648434206/workspace.old.json (0ms)\n   [testng] 04:58:35.883 [       FileProjectManager] Failed to load workspace from any attempted alternatives. (0ms)\n   [testng] .04:58:35.929 [       FileProjectManager] Overwriting singleton already set: com.google.refine.io.FileProjectManager@217abcc9 (46ms). @wetneb stop lying. :)  https://www.appveyor.com/docs/services-databases/. @wetneb because I'm retiring soon. :)  And your going to run it all. :) :). Actually, upon waiting and waiting.. eventually I see another error...\nallocation size overflow\nproject-bundle.js:32795:33\nListFacet.prototype._update\nhttp://127.0.0.1:3333/project-bundle.js:32795:33\nListFacet.prototype.updateState\nhttp://127.0.0.1:3333/project-bundle.js:32524:3\nBrowsingEngine.prototype.update/<\nhttp://127.0.0.1:3333/project-bundle.js:31356:9\nfire\nhttp://127.0.0.1:3333/project-bundle.js:3121:10\nfireWith\nhttp://127.0.0.1:3333/project-bundle.js:3233:7\ndone\nhttp://127.0.0.1:3333/project-bundle.js:9277:5\ncallback\nhttp://127.0.0.1:3333/project-bundle.js:9687:8. We know we don't deal with large facets very well already in #672 \nAnd this is because of how / what we are doing with Jquery to populate any List Facets.. @wetneb we can now if you want.  Whats your ID on Skype ?. @wetneb The Type's are that of more Semantic types and less of rigid types or data types.  For instance, we want folks to be able to type a column against the concept of http://schema.org/funder or for that matter a http://schema.org/CreativeWork  In the real world, sometimes Types are uplifted as \"property names\" with expected rules / restrictions against domains and hard Types like http://pending.schema.org/maximumAttendeeCapacity having an expected type of http://schema.org/Integer .  In our model, we want to allow BOTH, and let a Semantic overlay or extension or further data package managers apply further Semantic operations for Type casting, refinement, reconciling, etc.  By not constraining the Types in our model, we allow the most flexibility for others to expand on our core.. @wetneb OK, so when you said Type you were saying Data Type in a programming language like our Java.  And Yes, I was talking about BOTH.  Yeah, the Data Package standard has a field called Type (which is really Data Type ) and it could be enumerated and controlled.  The problem that I have is that of ... where's the fucking controlled list ? https://frictionlessdata.io/specs/table-schema/#types-and-formats  that doesn't really spell it out and says its based on json-schema but with modifications and then where are those?  What does the spec say is the final controlled list of allowed Types ?  I couldn't find it or see it through the weeds.  Did you ?\nAh wait, I guess its on the page itself further down and inclusive of geopoint and geojson for instance.  Ok that makes more sense now that I see it.\nAnd yes, the Table Schema spec has Rich Types (Semantic Types) which are in field \"rdfType\" , but I don't think we had an answer from Rufus Pollock about if it could take an array of multiple rdfTypes ... I hope it can... it will need to.\n . Cannot launch OpenRefine on Windows with Jacky's branch (although Jetty runs but the app doesn't start and gives 503 Service Unavailable.  He will debug later today or tomorrow when he has time.  Sent him email with some details.. @jackyq2015 Cleanup on all of these \"blue colored\" issues (skip the yellow), since now you have reverted the code mostly.  See https://www.codacy.com/app/OpenRefine/OpenRefine/pullRequest?prid=1186996. @jackyq2015 Can we change to using randomX or https://github.com/coddec/random-required or something else instead of the random.org , in order to fix the constantly failing testUrlCaching ? https://ci.appveyor.com/project/openrefine/openrefine/build/1.0.136#L345  \na few others are listed here: https://alternativeto.net/software/randomorg/?license=free\n. @jackyq2015 On export of the Data Package... if we could, lets sort the Object list to something sensible the first time... like Name, Title, Description, Version, License, etc..  if its not too much trouble (yeah, I know JSON is unordered)...otherwise just use that default Sort of Ascending in that Object grid control the UI has on the Object level.\nThe \"keywords\" array should be allowed to be empty if a user wants to not type any keywords or tags there.  Currently the UI says it requires at least 1 item...let's change that.\nThe \"name\" should be allowed to be anything and any pattern (treat it as pure string)... Currently the UI says that it cannot have whitespace.  \"prices csv\" is not valid.  \"prices_csv\" is valid.  Let's fix that also to allow whitespaces if a user desires on the \"name\".\nIf I then export the .zip file... and then try to import it as a data package with Import then it throws error saying it must be gzip format. :)  Is that not supported yet to import an archive file, but it must only be created via a URL ? or am I missing something on our Create project GUI that has changed somewhere else ?\n  . @jackyq2015\nBUG for Autosaving is fixed now on your latest commits. Thanks.\n\nHmm, then don't worry about the JSON ordering.\nThen don't worry about if Data Package spec wants to be strict about \"name\", it is fine (but I disagree with their validation on some of the properties)  So let's leave validation ON as it currently defaults to.\nGreat that zip file Data Packages will be able to be imported later.\n\n. @wetneb I've added notes about those merge points and PR expectations to our Developer Wiki as well.. https://github.com/OpenRefine/OpenRefine/blob/master/CONTRIBUTING.md#how-to-submit-prs-pull-requests-patches-and-bug-fixes  Feel free to improve.. @wetneb A lot of the changes are IDE changes for Eclipse (to improve tooling support for others pulling our project into Eclipse).  But I'm wondering if some of this is really necessary in our repo ?  Typically .settings , .project , and others are not pushed into Git repos, but we have historically added config and preferences for Eclipse and other IDE's under https://github.com/OpenRefine/OpenRefine/tree/master/IDEs/eclipse \n@jackyq2015 Can the .settings and .classpath and others be moved under that path to avoid the mixing of source versus IDE config and preferences ?  Did some of these get pulled into your branch from Tony's DB work ?. @wetneb Right, and I agree. I actually want you guys to work on this and add .settings and .classpath and even .project to our gitignore file to prevent these things from happening.  Look at how other big java projects on Github handle these same things related to IDE setups and ignored files.  Like this one https://github.com/randoop/randoop/blob/master/.gitignore. @jackyq2015 I gave you a headstart on it over 1 year ago !  Its in this branch https://github.com/OpenRefine/OpenRefine/tree/java8-refactor\n/gradle/wrapper\nbuild.gradle\ngradlew\ngradlew.bat\nStarting usage docs I put inside the https://github.com/OpenRefine/OpenRefine/blob/java8-refactor/build.gradle\nBut yeah, there's more that can be done to fully support Eclipse config within the build.gradle file. I just haven't done much with it myself, but have at it !\nI also use BuildShip plugin in Eclipse at work. Its fantastic.\n. My preference is to go Gradle only.  Its an easier setup for developers. (once I do the work of getting our Ant converted over to a nicer build.gradle file). @jackyq2015 bugs in displaying added and removed lines in code ?  dunno.  But shows correctly in IntelliJ and gitkraken. @wetneb @ettorerizza Yeah, we already have some issues for this at #1017 and #898 \nBut I'll let you guys investigate those and come up with a broader brush in this issue to address all of these needs / issues.. Also #901 Rename multiple column names at once. @jackyq2015 no concerns on the mapping between the 2 ideas.  Yes, even Schema.org uses \"keywords\" to represent the idea of \"tags\" or \"tagwords\".  Henceforth, CSVW has \"keywords\" also.. @wetneb Can you work on this over the next 2 weeks ? (invoice-able work). Merge this whenever your ready Antonin.  Looks like your still working out the Appveyor failures, that's fine.\nOh and thanks for this work !!!. @wetneb @jackyq2015 Why can't we just drop random.org and instead just mock this cache test instead with Mockito ?  I see tons of examples on the web such as this one https://reliablesoftwareblog.wordpress.com/2016/04/19/testing-a-cacheable-method-with-mockito/. @wetneb my point is that this specific test could use behavior verification rather than state verification. And mocking is a great way to get there.  This is from 2007, but explains the concepts that we have learned and often  https://martinfowler.com/articles/mocksArentStubs.html. I remembered what it does...\nany - At least 1 type in query must be present on the entity in the graph for it to be returned\nall - All types in query must be present on the entity in the graph for it to be returned\nshould - i think this was a 50% rule where at least 50% or more of the types must be present on the entity in the graph for it to be returned.  (say an entity has 4 types in graph, and query is type_strict providing 2 of the types, but entity will not be returned since not 50% of types, but of providing 3 of the types present on the entity, then it would be returned - 75% of types matching)\n. @wetneb right, its support was for multiple types.. @wetneb Yeah, this will help us test Tony's DB work.  Also, Appveyor has the services for various DB's that can be configured in it's yml file so that we can test against Windows as well.. ALL ... our overall architecture plans are to offer alternative processing into BEAM transforms (which opens up every possibility including Raw file manipulations if you have to, all the way up to cloud run transforms on current tech stacks).  But as @magdmartin has stated, we have plans to 1st work on separating out our backend completely and aligning for BEAM.  That's our high-level plan.\nAs far as replay of operations, we have to research a bit more , but regardless any sequence of operations will be using runners within BEAM and following its current Execution Model. https://beam.apache.org/documentation/execution-model/\nAs far as OTHER work that we plan to do... is to get the local desktop experience of OpenRefine to use much less resources in order to handle MORE data and quicker operations within the bounds of local heap or even native heap.  (FYI, Blazegraph and others did similar work in maximizing performance by using Native Heap as much as possible, wherever possible https://jira.blazegraph.com/browse/BLZG-533 and using more Native Heap where it makes sense is something we will eventually explore as well).  \nAnd one more side note.... moving to Java 9 will help us drop a few bits per String in our data model.\nif (COMPACT_STRINGS) {\n    byte[] val = StringUTF16.compress(value);\nhttps://www.sitepoint.com/inside-java-9-part-ii/#compactstrings. \"we\" includes \"you\". :)  This is open source.  A community of volunteers.  But Yes we have teams https://github.com/orgs/OpenRefine/teams and there is a technical steering committee (which includes me, Jacky, Antonin, and Owen...which we refer to as CORE https://github.com/orgs/OpenRefine/teams/core ) to basically help guide a bit.  But OpenRefine can take many forms in the case of extensions and where the community wants to go with it.\nAnyone can request access to a team.  Being added to a team depends on a few important considerations, for addition to CORE of which is knowing our history, goals, and codebase and previous contributions to our codebase.  But NOT being in CORE team, doesn't mean you have no voice in matters of OpenRefine, quite the contrary actually...\nTechnical discussions and online meetings can happen anytime the community desires or has the time.  Our CORE team has had multiple online meetings and these are announced in our OpenRefine Dev Mailing list.\nI and the CORE team put together the Milestones for review and have historically set the agenda for OpenRefine features, but we are not the only voice...the entire community has the voice...which includes you. :)\nSo at this point, your probably asking... \"Hey Thad, how can I get more involved with OpenRefine and technical discussions? \"  , then I would reply ... \"Right here, as well as on our Dev mailing list, as well as on our community hangout sessions\".  And as soon as we setup the non-profit organization we intend to have a big community online hangout session discussing... \"the future\".\nUPDATE:  I've added the \"Google Sponsored Projects\" https://github.com/OpenRefine/OpenRefine/projects to our Github Projects page to make things more clear for you \nUPDATE2: And the Milestones are not really strict.  We're all just a bunch of volunteers with limited time and now getting some reimbursement from Google for some of that time on certain features.. @fpompermaier Hmm... yeah seems like teams are not publicly shown even though I have all of them being set to Visible. https://help.github.com/articles/about-teams/  But teams are not for discussion but actually for permissions and project communication with \"@mentions\".  If anyone wants to be part of \"technical discussions\" then you simply subscribe to our DEV mailing list. Easy.. Does it happen in Firefox ?  Can you try to disable ALL Chrome extensions and try again ?. This sounds like a preference that we should set...and then if users want to override it...they update the preference.\n@wetneb   So... any thoughts on what a reasonable cap that we can setup in preferences.vt ???. @jackyq2015 I'd say that's exactly the issue the OP has here.  Hmm, Jython team says this fix is going into Jython 2.7.2 and I would say we probably could just build our own Jython 2.7.2-SNAPSHOT from latest Jython and incorporate the last 6 months of their fixes from Stefan and Jeff https://hg.python.org/jython/shortlog/default and use that SNAPSHOT version for now when we produce our next release.. @jackyq2015 I reviewed the last 5 months of commits.  There's only fixes, and nothing I'd be worried about.  There's an alpha version floating on the mailing list. But I'd say even a beta version would be fine for us if your that worried, but I'm not as I stated.  Problem historically has been that Jython are slow on releases...but fast on bug fixes in master as they are found, only pushing new features once Python gets them.  But up to the community voices on this one.  I.E. if its just this guy (original poster), then yeah, we can wait.. @wetneb yeah it was, but we might see still some corner cases come up, but we can deal with them individually and update the Jython version.. @tcbuzor @wetneb @jackyq2015 I guess we are only testing MySQL on Appveyor from the looks of it...also on Appveyor, it seems there's a Postgres setup error here, why do we use the same travis-SQL file for Appveyor, wouldn't it be better to have one just for Appveyor also ?  https://ci.appveyor.com/project/openrefine/openrefine/build/1.0.300#L29 . @wetneb hmm, for sanity's sake....can you try with OpenRefine 2.7 release as well on your end ?  (I'm using Firefox browser latest)\nNevermind, I just found the issue.  This was a regression introduced in 2.7 and its now fixed in 2.8 and trunk. :)  Thanks to whomever fixed it.\n. @wetneb #1286 is the issue discussing the HOW-TO proposal for the display of Unprintable Characters, which could tie into @ettorerizza need.  Formatting or changing the display of data so you can understand or work with it better, while keeping the raw data in the backend storage.  A toggle mode is best for this, like other data tools do.  Regardless, I would say its not \"distorting\" that we are after here, but the opposite by giving hints, highlighting, applying certain formats, etc. (with colors other than black and white, which sucks sometimes) to make visualization easier.  And yeah, that work will be cumbersome for sure, but worth it. (BTW, Where's our UI Gods when we need them !?!? lolol). @ostephens Ah yeah, this issue was because I normally just need RPAD, but sure, why not both !  And the terms lpad() and rpad() are nice and GREL like already :) short and sweet.. @wetneb I can't even checkout his branch properly. WTF Github !?!?\nTHAD@DESKTOP-TJUMQT0 MINGW64 ~/OpenRefine (master)\n$ git checkout -b stundzig-develop/1086-quotecharacter origin/stundzig-develop/1086-quotecharacter\nfatal: 'origin/stundzig-develop/1086-quotecharacter' is not a commit and a branch 'stundzig-develop/1086-quotecharacter' cannot be created from it\nIt looks like the stupid Github web editor for merge conflict resolve grabbed the wrong line I told it for prepareOptions.  I of course WANTED to handle this in IDE, but the Git branch error above is preventing me !\nI attempted to resolve the merge conflict, its resolved now(reversed via master into his branch via the web editor compare/resolve conflict), but we still have to fix the missing quoteCharacter field on prepareOptions some in TsvCsvImporterTests.java\nCan you checkout his branch or do you get a similar error as me ?. @wetneb found my issue... I forgot I had forced origin to Jacky's branch during the holidays for easier review !\nremote.origin.url=git://github.com/jackyq2015/OpenRefine. @wetneb @jackyq2015 Here's a clipboard text that I used for testing... looks like it works...where line 4 in preview has the column 3 with cell text or not depending on the \"Use character\" checkbox.\n\"this string has double quotes around it\",\"this string also has double quotes around it<NEWLINE>\"\nthis string has no quotes, \"but this string does!\", 'and this string has single quotes around it<NEWLINE>'\n'\"and this string is wrapped first with double quotes and then single quotes <NEWLINE>\"'\n\"this string has double quotes around it and single quotes around this: 'TERM'\", \"and this string has double quotes around it and a comma is here, <-- COMMA<NEWLINE>\"\nWe should probably also allow the quoteCharacter to be changed by the user ?\nUPDATE: nevermind, it works.. Works great !  Thanks to @wetneb and @stundzig !. And we need to work on the website refresh, well, at least attempt to \"freshen it up\" :)  This would be an awesome portfolio opportunity for some teenager somewhere in the world :). @joanneong Yes we have new logos designed for us by Google.  I'll email the package to you Joanne, check your email.. From our internal email where I should probably make my comment public about the reasoning...\nFrom THAD on private list-\n\"To summarize for others that might not understand what Denis is artistically telling us in the cues layout...let me explain...\nSo he is saying as a designer he can play more with Serif fonts (those with character edges like Swift) and cannot with Sans Serif fonts (those like Calibri that have no fancy edges).\nI get that.  I understand that from an artists perspective and it is an important element to consider when you want to have places to \"embellish\" with your creative artistry.  Serif fonts give you a \"bigger canvas\" and Sans Serif fonts do not.  Yes, I know Denis and understand what your trying to say there.  Hmm...\nIn that case, so be it, Swift it is... embroidery, knits, materials will cost more, but it is very negligible I guess, and by choosing a Serif font like Swift, we will not lose out on a great opportunity for artistic embellishments on the Serifs and playing with our Diamond facets or any pattern we desire.\n\"\n@ettorerizza So, the artist who designed it gave the reasoning behind the choosing of the \"Swift\" font with serifs (embellishments), so that artistic variations could be done and the serifs broaden the capability, etc. with patterns, motifs, etc.\nThe PDF design doc  that was supplied with it shows the usage of the serifs for embellishment.\n@joanneong I would just enlarge the font for now (maybe un-bold and provide screenshots again for us to see and decide?)  Joanne you should have the original vector files in that ZIP file I emailed you, to work with and increase the font size a bit.. @joanneong I think its the perfect font sizing now for our current UI on those pages. Good Job !. @joanneong You can delete them Joanne, we have them backed up in Git and elsewhere.. @joanneong Yes, its great ! Good job Joanne !  Are you ready for final merge ? or do you have more work to do and for us to review on this pull request (PR) ?. After a year of thought and recon service reworkings... have the 2 of you come to some final conclusion on the link URL handling ?  on the recon object or recomputed ?  Have folks within Wikidata expressed some preference during workshops, conferences, meetups, etc ?. Agree, it is also an opportunity to update our code and reduce object counts for ReconEntries ... like using map more, for instance on https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/operations/recon/ReconOperation.java#L307  we could use map.replaceAll() \nIt also seems like all the old cell recon entries are held in memory until after the run() is complete, or am I misreading the code flow ?. We might also just leverage a wider surface than Apache Arrow itself and instead incorporate Apache Drill (Apache Arrow + Hadoop, NoSQL, and others support)\nhttps://drill.apache.org/\nIn-fact, Arrow was spun off as a new project using code donated from Drill [1] and has now grown to include a lot more.  [1] https://drill.apache.org/docs/value-vectors/\nI've taken the Docker Drill embedded version out for a spin and its really fast and performant, even when only given 4gigs Ram.  I was impressed with the ability to connect a few JSON files along with a large 500meg CSV file...all 3 being joined and compared with SQL operations at the same time.\nI'd like to see more research by others to look into Apache Drill as a complete data backend for OpenRefine.\nI thought this was also interesting... someone connected Drill to Spark via Spark's dataframe. @michaelizer We already have OpenRefine translated in Spanish, but you can review and possibly provide better translations or missing ones by visiting our Translate OpenRefine wiki page. @ostephens #443 Improve Expression Preview dialog to show type of results\nI have big ideas for a new and improved Expression Preview dialog for us once get front/back separation and new UI work started.. @ostephens Ah, that was yours and mine your thinking of from our mailing list and now an enhancement issue #1088 Give the user an Error when they try to populate a cell with an array. @joanneong good work so far !  My idea was to handle the 80% of the use cases that folks would use the GREL range() function for.  The design should mimic what Jython/Python does and it's range() https://docs.python.org/3/library/stdtypes.html?highlight=range#range\n\nOnly Integers supported, I would say, just like Python's range()\nYes, return it as an array.  We have other issues regarding enhancing our displays (via coloring, syntax, etc) that will let folks know that their output is not of a String type that can be written to a cell.  So for range() just handle as we do with other functions that output an Array.\nYes, negative numbers should be supported, just as they are with Python's range()\n\nOne more argument that I think would be valuable would be a STEP, just as Python's range() has.\nrange(start, stop[, step])\nAlso testing for equality should mimic what Python does and treat the range objects as sequences. See notes on the range() class in Python's docs url above.. @joanneong I revoke my previous comment that STOP should also be output.  The range STOP and the output is inconsistent once I played with your changes.  I strongly agree with @wetneb that we keep our 0-based indexing that we have throughout OpenRefine.  So a 5-element range call looks like\nrange(5) --> [\"0\", \"1\", \"2\", \"3\", \"4\"]\nand not\nrange(5) --> [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]. @joanneong the expectation is that it works like Python 3.3+ range() does. @joanneong @wetneb What if we just added a Range Type as well ?  We could look into using existing libraries to gain benefits for future expansion and also add a Range Type so we can do map-reducing Range overlap/compare/etc once we implement Beam support later this year ?\n1. https://github.com/google/guava/wiki/RangesExplained\n2. https://commons.apache.org/proper/commons-lang/javadocs/api-release/org/apache/commons/lang3/Range.html\n3. Guava and Commons test example https://github.com/andygulin/sample/blob/master/src/main/java/examples/showcase/RangeTest.java\nI think (in a new issue someone can create) we probably want to additionally have users tell us from a new drop-down menu option that a column is full of Range type values. \nEdit cells -> Common transforms -> To range \nBecause I'm seeing this kind of data about Ranges in the wild in scientific data sets both old and new... So to make @joanneong life even harder on this issue .... :)\n```\n1-9      <-- The OP's original format and why we have this issue \n[1-9]\n1..9\n(1..9)\n[1..9]    <-- Not seeing this one in any datasets however\n```\n. @wetneb Yes of course, but what we have historically tried to do with GREL has always been giving easy power commands to help our users.  I feel strongly that having a Range type will benefit our users and allowing them to work with Range types and specifically in scientific data sets, Interval Notation, a bit easier.  By they way, I did find a generator, http://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math4/util/IntegerSequence.Range.html. @joanneong that sucks, I cannot actually see how it will look, until we merge this in. Thanks Github! :) lolol.  OK, if it doesn't work well or looks crappy, we can just hot fix it :)  Let me merge.. @joanneong OK, I like it now that I've fixed it up.  I've used the underlines to provide bold to differ with text once displayed.   Edit mode is still normal text, and that's fine.\nLet's leave this for now and see how things work out over the coming days/weeks.. @ettorerizza sorry, I don't quite understand your complaint.  Can you be more specific about what you like or don't like ?  Is something formatted or showing up weird for you because you view it in email ? something else ?. @ettorerizza DONE !. @ostephens Yeap, the background for treating them the same was to make simplifying the GREL handling characteristics.  We assumed that most users of OpenRefine did not care about NULL and just wanted Blanks or ZERO LENGTH STRING.  However, we now know, 8 years later, that these should actually be handled distinctly, since that has risen to be the expectation from most journalists, data scientists, etc.  Basically, we assumed too early that the world didn't care much about NULL as much as we did.  We were wrong.  They care :)  So we can now go back and change this assumption so that NULL and ZERO LENGTH STRING will be treated distinctly in OpenRefine.  Its a minor change to how OpenRefine treats the semantic meaning of NOTHING.  For purists and data scientists, its NULL.  For journalists and the vast majority of our users its ZERO LENGTH STRING.  This will take care of Export value issues automatically, since users can now explicitly convert NULL references to BLANK, vice-versa,  or whatever String value they want (like what I do sometimes <NULL>).\nFor our users, they will have to do more work to take care of NULL themselves, and that's fine I think and the expectation it seems and in our philosophy of giving users a power tool to decide for themselves, instead of us doing things magically for them and hiding that magic.\nFurther background...just in case you didn't know or read much into my comments in #820.  NULL can be interpreted as 2 things.... INVALID, or ZERO VALUE.  (this has nothing to do with programming, btw, I'm talking about semantics) Some purists in various fields, will assume BLANK means ZERO VALUE and NULL means INVALID.  Non-Purists might view things differently.  Everyone knows that INVALID and ZERO VALUE are semantically different.  So different OpenRefine users might opt to treat NULL as either one, or both.  Our job is to let THEM decide for themselves how they wish to treat it.  Hope that makes sense, but that's really the crux of the issue for most OpenRefine users that have brought up issues about this, but probably never explained to you precisely why they were complaining :)  I deal with this everyday as a Data Architect providing data for Business Users who want to know when things are INVALID and when things are ZERO, BLANK, or NOT GREEN. :)\n1,,5  <-- Could be INVALID or ZERO or BLANK, depends on how user wants to treat it, but \"user knows best\"\n1,0,5 <-- Definitely ZERO, but user could opt to treat as INVALID, NULL, EMPTY, or OPENREFINE ROCKS!\n\"1\",\"\",\"5\" <-- Who knows ?  Not OpenRefine, but \"user knows best\".. @wetneb it makes comparisons between branches with tooling easier (1 repo, just branches...instead of 2 repos, their branches).  It's also easier from a Github project level because Github provides visualization for comparing branches.  Do you want me to allow Flavio to push directly into beamusupscotty ? or require pull request reviews before merging ?  See screenshot options:\n\n. @fpompermaier You should have received an Invite email to be added as an \"External Collaborator\" to our project.  Please accept that invite and then I can add your username to this new branch, so that you can push directly.. B.  Good point.  I did filter and somehow this 'null' came through, not sure what was going on. This unfortunately might have to be reproducible in order to fix it I think.  I'll try today.\nUPDATE: No need for B.  Human error. Removed it from issue.. @fpompermaier I agree with @wetneb and also thought you were going to just be leveraging against our existing model and adapting and improving in this new branch.  I then wanted to look at this new branch under the eyes of Structure101 to begin to see the new forms and where we can refactor, and also ask David Huynh or Google Dataflow team their thoughts or pitch them questions in email as we go along.. @fpompermaier In that case, just push this again directly to the branch OpenRefine/OpenRefine:beamusupscotty where I have already given you push rights.  I'll close this PR since we don't need Review, and the expectation is you will do your work directly in beamusupscotty where you have full control and ownership of that branch under our Repo. (You are now officially a collaborator for that branch)  If you need me to send the collaborator email invite again because you didn't receive it for some reason, then just let me know.. @ostephens as stated on your pull request... I'm backtracking now and thinking that the last one\ncoalesce(null) -> null\nmight or should perhaps be\ncoalesce(null) -> <blank>\nThis needs wider discussion and just feeling around with it I think from the community. I should not be the sole decider on this one.. @ostephens good point.. @ostephens \n1. Hmm, OK, now I'm remembering WHY the conglomeration of Blank and Null a bit more.... lolol.   What is the plan for the Text Facet, Numeric Facet, etc however ?  I think we should additionally support showing a label for (null) as well, since we show (blank).  If its not in your plans on any of these issues you created, then can you please add 1 more issue for that ? Thanks :)\n\n\nThe other thing to note is that in our other Common transforms -> \"to somethings\" , we are converting (blank) to (null) , although I haven't checked our code.   I just tried this with having a column of numbers, blanks, and nulls and using \"to number\" on 2.8 release and it converted my (blank) cells to (null) explicitly.\n. @ostephens right, and not convert the users data silently, as its currently doing with this bug.. Screenshot of test\n\n. @wetneb Agreed, revert the PR.  Then @jackyq2015 can fix, and we can put more robust tests in place. And I'm glad that org.json actually made their end more robust on type checking, even though it sucks for us at this point.. @jackyq2015 this will fix for any importer ?. @wetneb Besides reverting the PR... what are you proposing to improve the PR ? More tests ?  An alternate form of testing this somehow or seeing the impacts ?  How can we bring confidence to the importers and Data Package / Metadata enhancements ? . @jackyq2015 I don't think anyone is displaying an attitude.  Antonin is trying to help improve things and had legitimate concerns as did I, but most of those concerns are removed now given your explanation.  Let's not be too harsh, after all, its only the few of us that I can count on 1 hand. :) right ?  And thanks to both of you for even caring about this little project called OpenRefine that we all work so hard on !\n\nOK, so, let's get to work as the awesome team we are ... we know we need to improve the tests here, around Project Creation and Importers... @jackyq2015 any other areas that you feel would help to assure quality in future PR's around Data Package and Metadata that you can think of ?  Also, Antonin brought up a good point about the Data Package java library...should we invest some time to help fix that up, anything within that concerned you while working with it Jacky ?. @jackyq2015 Awesome, thanks Jacky. that will help.. @magdmartin We just need to also update our README.md file to include the URL to the Github folder path, then I think we are good.  Notice this sentence in our README.md \"See that file also for information on open source libraries that OpenRefine depends on.\"  which should be replaced with something like \"See <url>(text) for license information on other open source libraries that OpenRefine depends on.\". @fokky I think a much better way to handle this is not actually provide Hover Cards because they acutally limit us quite a bit, but instead to have a complete Recon Panel that can be docked on the right/left side of OpenRefine so that you can have a fully scroll-able panel for showing complete information from Wikidata or another recon service.  That's basically our longterm plan for this UI feature.. @ostephens P1963 is a good 1st pass for disambiguation properties in Wikidata.  Unfortunately, often there are not enough P1963 statements on Types in Wikidata, in fact, I spend too much of my minutes of spare time often filling them in... like this one just now https://www.wikidata.org/wiki/Q12518  But yeah, calling properties that help describe uniqueness for entities... \"disambiguating properties\" is fine.. @wetneb Oh, its just a panel that we can expose for an IFRAME for the Recon Entity endpoint that the Recon service provides.  It would save the user from having to buy another computer monitor, or installing a Smart Split Tab extension in their browser, just to see the details of a particular Recon Entity endpoint ... allowing them to stay focused on one browser tab in OpenRefine as they work down their column of entities.. @ettorerizza yes, that's the idea Ettore, however, not a popup dialog as we have now, but actually 2 panels, one where a constant View Recon Entity panel shown on the right of the browser window, that can be toggled to display or not, and who's content simply refreshes when an entity is hovered mouse on in the recon column and perhaps that can be data limited to the user's chosen set of disambiguating properties.  This is basically what the Freebase Recon UI had that was hosted on Freebase.com long ago for Judgements (somewhat similar to Primary Statements), but we never got around to moving that panel functionality into OpenRefine itself.\nThe View Recon Entity panel itself really should be dockable, sizeable, and showing a user's chosen set of disambiguating properties by a user to customize their recon service workflow.  Some Recon services show a lot of info, some not so much.  We want to give choice to the user themselves, to show more or less data for each entity, and also our new interface should be much more streamlined to allow quicker judgements by perhaps just hover mouse over entities where the View Recon Entity panel just auto refreshes to show details of the entity and then on the Match Recon Entity panel, you click on much smaller buttons in the column when you want to match.  Less clicking, more Auto Show magic, and customizable views for the View Recon Entity panel.  The Match Recon Entity panel can thus be very small, with only buttons for Match, Match All, Cancel.\nPretend, we're Microsoft or Ubuntu, designing a brand new style and flow of Recon matching.. @wetneb Again, I say let the users make the choice of what data and how much or little to see rather than hardcoding a Recon workflow that might not work for them.  Let's just give them the power tools to work better with Recon.\n@ettorerizza The hover is a different mode than your thinking.  My idea is that it also has background cell highlighting for easier visibility with 3 buttons (Checkmark, 2 Checkmarks, X).  The set of entities shown on the current grid would be background loaded in batches, not loaded upon each hover, probably using using something like HTML5 and React's iframe ref with callback . @wetneb yes, absolutely baby steps.  I just wanted to ensure that the team understands my opinion here, since you asked for clarity earlier.  Agreed, those quick changes can help considerably for now.. @Stolyassa Hi, it looks like you might have some non-English characters in your username perhaps ?  Also, try to extract the contents of the .zip file again to a more basic path like just C:\\OpenRefine2.8\\ \nMy hunch is that %sign in the path of \\openrefine-win-2.8%20(1) is what is causing this issue for you, so try to avoid creating that when you extract the zip file.\nLet us know if the new path works for you.. @Stolyassa is it possible you have Python already installed on a weird path ?  Do a cmd.exe and then type\npython -V\nwhat is the output ?\n@ettorerizza any ideas also what might be the issue ?\n. @Stolyassa Ah !  can you uninstall that particular Python installation?  or if it is for other uses, then perhaps you can temporarily remove that Python from your Path in System or User environment variables on your Windows system ?  The reason is that I suspect it is getting in the way and Python 3.6.3 is being used as the interpreter instead of the Python 2.7.x already supplied in the Jython JAR file that we ship with OpenRefine.\nThis should explain a few things how Jython itself locates python.path and python.home\nhttp://www.jython.org/archive/22/userfaq.html#where-s-the-registry-file\nand\n http://www.jython.org/archive/22/userfaq.html#my-modules-can-not-be-found-when-imported-from-an-embedded-application\nBut first let me know if the above temporary remove or temporary path remove works for you.\nThen we can see about how we can possibly add detection of this and give users a Warning about it in the future and save others some grief.\n. @jackyq2015 awesomeness !  keep those fingers going !. Good grief.  OK.  I need some clarity here.\n@jackyq2015 in one sentence, what are you worried about.\n@wetneb in one sentence, what are you worried about.. @wetneb @jackyq2015 Thanks both.\nMy opinion then is this... \"it seems trivial enough that we can generate a new token and add this to our release binaries, and in doing so we help our users by not potentially making ourselves outright outlaws of Google's policies\".  (soft gavel blow on my desk). @wetneb I just checked and looks like older versions did actually perform a stringify and set the limit param as a String value and not as an Int.\n\nI honestly thought it was an Int.... as our other limit values like in Fetch Delay and get-rows are Integers.  Hmm, wonder why Recon config was done differently , dunno, that's a David question I guess.\n\nAh, the Form data is always pure JSON objects that we create, I see now.  And those objects are always stringified ... well used to be.\n. @wetneb thanks for keeping us honest. :)  our old memories are fading, but your young blood keeps the team FRESH :) :). @wetneb No one breaks things on purpose.  Let's please be patient with everyone here and just try to tweak the defensive attitude a bit, just some personal advice from me to you. :) Both Jacky and I approve of your passion for OpenRefine and of course all of us appreciate everyone's efforts.  Both of you are extremely talented well beyond my capacity and would probably buy each other a beer if you met up in a bar! :)  Jacky's not retorting actually to this PR, his mannerism is different than yours and mine.  He's more factual in communication than you and I, and we both should appreciate his style as it balances with the rest of our team.  If you re-read, he's just thinking out loud and even acknowledging your effort to keep the UI healthy and avoid service interruption and also acknowledging to support clients that send a string.  He's agreeing, in his own style of communication. \n(and we all agree that the printed word sucks for real communication of nuances and feeling, so what we read might not be what the author was feeling). @wetneb Perhaps this label could be improved with \"type item or drag reconciled column here\"\n\n. @wetneb Moving the Alignment dialog around, seems like containment isn't made for the Hover Tips ?\nhtml body div.fbs-reset.fbs-pane.fbs-pane-property div.fbs-status\n\n. @wetneb of course.  I was only commenting, I didn't block you. :). @wetneb scroll just past halfway down that page...then you will begin to see the Java files and issues.  And click the line numbers on left to open the file issue... then click on yellow bar areas to expand and see details of issues.  Some are minor quality preference issues, skip fixing those, just address the real issues.\n\n  . @wetneb thanks for reviewing. your done then.  yes sometimes the issues are debatable. Jacky has his quality level. and I have mine.. @wetneb and its not always the Yellow ones... quality is subjective and I hate seeing unused imports and bloat... like this one you have... https://app.codacy.com/app/jackyq2015/OpenRefine/file/15226927129/issues/source?fileBranchId=6695671#l@result.result.startLine\nJust basically see that the Codacy review is not so good.. click on Details link...scan through briefly to see where issues are really at that can improve our code quality.  Its a subjective science, and the team pretty much trusts that we at least do a quick visual scan through Codacy review and it only takes usually 1-2 minutes.  At my work, we use Sonarqube and have it highly tweaked and try to maintain an 75% test coverage rate and 80% custom quality rate.. @wetneb LGTM.  Can you get them to update the API org.wikidata.wdtk.wikibaseapi.ApiConnection ???\n09:26:18.799 [                   refine] POST /command/wikidata/login (12151ms)\n09:26:20.314 [..kibaseapi.ApiConnection] Login Error: Failed (1515ms)\n09:26:27.894 [                   refine] POST /command/wikidata/login (7580ms)\n09:26:29.206 [..kibaseapi.ApiConnection] Login Error: Failed (1312ms)\n09:26:34.532 [                   refine] POST /command/wikidata/login (5326ms)\n09:26:35.970 [..kibaseapi.ApiConnection] API warning [main]: Subscribe to the mediawiki-api-announce mailing list at <https://lists.wikimedia.org/mailman/listinfo/mediawiki-api-announce> for notice of API deprecations and breaking changes. Use [[Special:ApiFeatureUsage]] to see usage of deprecated features by your application. (1438ms)\n09:26:35.970 [..kibaseapi.ApiConnection] API warning [login]: Main-account login via \"action=login\" is deprecated and may stop working without warning. To continue login with \"action=login\", see [[Special:BotPasswords]]. To safely continue using main-account login, see \"action=clientlogin\". (0ms). @wetneb They are not spurious.  You are deliberately confusing the JS runtime engine.  For instance, Why do you need to add \"toolbar\" in L218 again , my preference would be to rename with \"grouptoolbar\" or remove L218 altogether ?\nYour also deliberately saying you want to make it harder for others coming in by introducing a variable hoisting pattern.  I would rather avoid those.. @wetneb lol, no you'll never offend me.  I'm a very happy go lucky kinda guy.  I'm just asking you to improve the quality a bit, in areas that I know have caused grief for JS programmers in the past.  Runtime errors are the bane of all JS programmers and those 2 anti-patterns that I pointed out cause 90% of runtime errors in JS.\nYour correct in thinking that this possibly cannot happen.  I am taking the stance that IT DOES HAPPEN, WHEN WE LEAST EXPECT IT. :)  So let's try to avoid it when we can.\nI'm also not taking a hardline here.  You can leave things as is Antonin.  But I just want to help you improve your coding skills in these particular areas, since I've had to chase down runtime errors in the past with UI teams more often than not.. @wetneb Thanks for the tutorial.  My thorough testing will come this weekend when I finally have dedicated time.. @wetneb I always test with proper merge master into the PR and then test functionality.  This PR https://github.com/OpenRefine/OpenRefine/pull/1527 got reverted so getting the \"not an int\" error and cannot test.. @wetneb Is a full column of data sent to determine the Column Type ?\nhttp://127.0.0.1:3333/command/core/guess-types-of-column?project=2242467183748&columnName=recordrow+4&service=https://tools.wmflabs.org/openrefine-wikidata/en/api\nThis would not be aligned with how Freebase Recon did things.  As we worked through a dataset with Facets... I could filter on a particular subset, recordrow 3, i.e., Country=France in my Facet, and then only rows from my recordrow 4 of wine regions in France (selected in Facet) would be sent as JSON, not all of the rows from recordrow 4 which included wine regions from other parts of the world.  Here, I'm only focusing on France for now, yet, I get returned other region types...\n. @wetneb \n1. Any idea what's going on with the reference ?\n\n\nI clicked save and yet Issues still shows that I didn't Edit yet ?\n\n\n. @wetneb your right, I just checked BLAME and that post config for Recon has not changed.  My memory faded, but I now recall, the expectation was that a column should be constructed for Recon against a single Type.  So I guess we did just send over X first values of the column to guess-types-of-column. (I wonder where the X first values is however, I didn't see it in the code, only @tfmorris addition of max-candidates ? Anyways, yeah, you can ignore that as an issue.\nYeah, if you can make the UI clearer that would be nice.  \"Statement is missing property\".\nTo help even more... show \"Statement X\" on the left side (plenty of space) of each one.  The coloring boxes is not enough.. Can I screenshare with you for 10 mins , and show few things ?  Google Hangout ?. @wetneb hop on to your gmail for Hangouts...just tried calling ya.. @wetneb I think just adding a trashcan on the empty Property input boxes would work nicely. I noticed that even qualifiers need a way to remove when the input box is empty.  So having a trashcan icon for empty input boxes makes sense.  . @wetneb Awesome !  Then in that case, your free to use whatever UI javascript and libraries you want or need.  Get Creative !. @DavidFichtmueller Thanks for the PR !  So, you have seen some of these incorrect header strings on public endpoints ?  Curious which ones ?. @wetneb works as advertised.  can't wait to have the new interface for this down the road !  There's so much more data properties that I need to see for some of these corner cases in reconciling that I do...the long tail domains.. @wetneb yes, there's a lot that is needed.  I'll get around to them sometime this year. no worries.. should we also support purity of our GREL null string ?\ncoalesce(null)  ->  <blank cell>\nbut then that might be more of an expansion of string handling for GREL, and I think David avoided that previously, but this seems logical if not useful ... since the purpose of this function was to also provide an easy GREL way to replace nulls with blanks ? Thoughts ?\nAlso this ? Do we expect the null value to be coalesced to <blank cell> ?\n\n. @joanneong tested just now... works great !\n@jackyq2015 Wondering why my local merge of Joanne's branch causes a temp file missing?\nC:\\Users\\THAD\\OpenRefine>\"C:\\Program Files\\Java\\jdk1.8.0_151\\bin\\java.exe\" -cp \"server\\classes;server\\lib\\*\"   -Xms1400M -Xmx1400M -Drefine.memory=1400M -Drefine.max_form_content_size=1048576 -Drefine.port=3333 -Drefine.host=127.0.0.1 -Drefine.webapp=main\\webapp -Djava.library.path=server\\lib/native/windows com.google.refine.Refine\n10:53:58.968 [            refine_server] Starting Server bound to '127.0.0.1:3333' (0ms)\n10:53:58.984 [            refine_server] refine.memory size: 1400M JVM Max heap: 1407188992 (16ms)\n10:53:58.984 [            refine_server] Initializing context: '/' from 'C:\\Users\\THAD\\OpenRefine\\main\\webapp' (0ms)\n10:53:59.285 [            refine_server] Failed to use jdatapath to detect user data path: resorting to environment variables (301ms)\n10:53:59.285 [            refine_server] Failed to use jdatapath to detect user data path: resorting to environment variables (0ms)\n10:53:59.285 [                   refine] Starting OpenRefine 2.8 [TRUNK]... (0ms)\n10:53:59.285 [                   refine] initializing FileProjectManager with dir (0ms)\n10:53:59.285 [                   refine] C:\\Users\\THAD\\AppData\\Roaming\\OpenRefine (0ms)\n10:53:59.379 [         project_metadata] java.io.FileNotFoundException: File 'C:\\Users\\THAD\\AppData\\Roaming\\OpenRefine\\1825969120998.project\\metadata.temp.json' does not exist\n        at org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:137)\n        at com.google.refine.model.medadata.ProjectMetadata.loadFromFile(ProjectMetadata.java:551)\n        at com.google.refine.io.ProjectMetadataUtilities.loadFromFile(ProjectMetadataUtilities.java:153)\n        at com.google.refine.io.ProjectMetadataUtilities.load(ProjectMetadataUtilities.java:98)\n        at com.google.refine.io.FileProjectManager.loadFromFile(FileProjectManager.java:396)\n        at com.google.refine.io.FileProjectManager.load(FileProjectManager.java:361)\n        at com.google.refine.io.FileProjectManager.<init>(FileProjectManager.java:94)\n        at com.google.refine.io.FileProjectManager.initialize(FileProjectManager.java:81)\n        at com.google.refine.RefineServlet.init(RefineServlet.java:130)\n        at javax.servlet.GenericServlet.init(GenericServlet.java:241)\n        at edu.mit.simile.butterfly.Butterfly.init(Butterfly.java:180)\n        at org.mortbay.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:440)\n        at org.mortbay.jetty.servlet.ServletHolder.doStart(ServletHolder.java:263)\n        at com.google.refine.RefineServer.configure(Refine.java:296)\n        at com.google.refine.RefineServer.init(Refine.java:208)\n        at com.google.refine.Refine.init(Refine.java:114)\n        at com.google.refine.Refine.main(Refine.java:108)\n (94ms)\n10:53:59.394 [         project_metadata] java.io.FileNotFoundException: File 'C:\\Users\\THAD\\AppData\\Roaming\\OpenRefine\\1791016523934.project\\metadata.temp.json' does not exist\n        at org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:137)\n        at com.google.refine.model.medadata.ProjectMetadata.loadFromFile(ProjectMetadata.java:551)\n        at com.google.refine.io.ProjectMetadataUtilities.loadFromFile(ProjectMetadataUtilities.java:153)\n        at com.google.refine.io.ProjectMetadataUtilities.load(ProjectMetadataUtilities.java:98)\n        at com.google.refine.io.FileProjectManager.loadFromFile(FileProjectManager.java:396)\n        at com.google.refine.io.FileProjectManager.load(FileProjectManager.java:361)\n        at com.google.refine.io.FileProjectManager.<init>(FileProjectManager.java:94)\n        at com.google.refine.io.FileProjectManager.initialize(FileProjectManager.java:81)\n        at com.google.refine.RefineServlet.init(RefineServlet.java:130)\n        at javax.servlet.GenericServlet.init(GenericServlet.java:241)\n        at edu.mit.simile.butterfly.Butterfly.init(Butterfly.java:180)\n        at org.mortbay.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:440)\n        at org.mortbay.jetty.servlet.ServletHolder.doStart(ServletHolder.java:263)\n        at com.google.refine.RefineServer.configure(Refine.java:296)\n        at com.google.refine.RefineServer.init(Refine.java:208)\n        at com.google.refine.Refine.init(Refine.java:114)\n        at com.google.refine.Refine.main(Refine.java:108)\n (15ms)\n10:53:59.879 [       database-extension] Initializing OpenRefine Database... (485ms)\n10:53:59.879 [       database-extension] Database Extension Mount point /extension/database/ [*] (0ms). @joanneong Yes, Joanne, that last design is intuitive enough.  Just change to \"or\" instead of \"&\" as @ostephens suggests.  Great job again !. @jackyq2015 Ah, good detective work.  Yes, when I am testing PR's I am always merging to master.  For checking the state of master, I always reset to head and then perform my testing.  If you were able to reproduce and see the issue then great !\nWere you able to reproduce it with our current master state ?. CLOSING ISSUE: After chatting with Jacky, this is a non-issue and just a fact that I had 2 older projects that were created from a bug in previous master version around the metadata.  The bug has been fixed in master according to Jacky and so I'll just delete those 2 previous projects and create new one's.\n. 59\n. @ostephens whoa, wait a sec... this was supposed to be an option, as @wetneb said.  We don't want to make this default...its visually too busy for regular users.  Put more work into this and make it an option in our preferences.vt for now.. @jackyq2015 automatically switching to records mode when record-like rows were detected...has always been a feature.. @jackyq2015 @wetneb I would have to agree I guess with Jacky, that if a user chooses the \"Line-based Importer\"... then it should stay in Rows mode.  For other importers, I think its fine for us to continue to detect records and move into Records mode.  Once we get a new UI, we can have a Guide dialog that says \"We've put you in Records mode, since we detected records based formatting of your data file\".\nI think we can close this issue if there is not objection ?. Your detective work can start here :)  https://github.com/OpenRefine/OpenRefine/blob/master/main/webapp/modules/core/MOD-INF/controller.js#L129\nhttps://github.com/OpenRefine/OpenRefine/blob/master/main/webapp/modules/core/MOD-INF/controller.js#L190\nhttps://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/commands/recon/ReconCopyAcrossColumnsCommand.java\nhttps://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/operations/recon/ReconCopyAcrossColumnsOperation.java\nInterestingly, in some translations we have \"copy-recon\" and \"copy-recon2\" ? weird. Oh wait, that's just the hover text on copy-recon2.\nhttps://github.com/OpenRefine/OpenRefine/blame/da513589491c7df99a1f22de1592197fc5cd68b3/main/webapp/modules/core/scripts/views/data-table/menu-reconcile.js#L127\nAH !  Here:\nhttps://github.com/OpenRefine/OpenRefine/blame/da513589491c7df99a1f22de1592197fc5cd68b3/main/webapp/modules/core/scripts/views/data-table/menu-reconcile.js#L399\n. @wetneb It still works just fine for me in OpenRefine 2.8 ... just tested with\nAbraham Lincoln\nGeorge Washington\nBarack Obama\nWhere all 3 were Recon'd and matched.\nI made a new column based on \"copy value from original column\"\nThen selected \"Copy reconciliation data...\" and the dialog came up and I clicked Copy...\n\n. @jackyq2015 on my work laptop...will get for you on Monday.. @jackyq2015 emailed you the 60MB zipped project file.  Was too big to attach here at 10MB limit.\n. @jackyq2015 How much memory did you give it ?. @jackyq2015 its the release version of 2.8  (there's no Ant tasks to perform) :)\nI'll see if its in one of the other projects in the morning and let you know.. So this happens right when I started OpenRefine 2.8 on my work laptop using Firefox latest and OpenRefine loads and shows its starting homepage...\n```\n10:18:17.831 [            refine_server] Starting Server bound to '127.0.0.1:333\n3' (0ms)\n10:18:17.862 [            refine_server] Initializing context: '/' from 'C:\\User\ns\\eguitha\\Downloads\\openrefine-2.8\\webapp' (31ms)\n10:18:22.011 [            refine_server] Failed to use jdatapath to detect user\ndata path: resorting to environment variables (4149ms)\n10:18:22.011 [            refine_server] Failed to use jdatapath to detect user\ndata path: resorting to environment variables (0ms)\n10:18:22.121 [                   refine] Starting OpenRefine 2.8 [TRUNK]... (110\nms)\n10:18:36.939 [                   refine] POST /command/core/load-language (14818\nms)\n10:18:36.990 [                   refine] GET /command/core/get-preference (51ms)\n10:18:37.019 [                   refine] POST /command/core/load-language (29ms)\n10:18:37.242 [                   refine] POST /command/core/get-importing-config\nuration (223ms)\n10:18:37.413 [                   refine] GET /command/core/get-all-project-metad\nata (171ms)\n10:18:37.594 [                   refine] GET /command/core/get-languages (181ms)\n10:18:37.648 [                   refine] GET /command/core/get-version (54ms)\n10:18:37.748 [          org.mortbay.log] /images/favicon.png (100ms)\njava.lang.IllegalStateException: Committed\n        at org.mortbay.jetty.Response.resetBuffer(Response.java:1024)\n        at javax.servlet.ServletResponseWrapper.resetBuffer(ServletResponseWrapp\ner.java:202)\n        at org.mortbay.servlet.GzipFilter$GZIPResponseWrapper.resetBuffer(GzipFi\nlter.java:310)\n        at org.mortbay.servlet.GzipFilter$GZIPResponseWrapper.sendError(GzipFilt\ner.java:319)\n        at edu.mit.simile.butterfly.Butterfly.error(Butterfly.java:1020)\n        at edu.mit.simile.butterfly.Butterfly.service(Butterfly.java:528)\n        at com.google.refine.RefineServlet.service(RefineServlet.java:200)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511\n)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet\nHandler.java:1166)\n        at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n    at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet\n\nHandler.java:1157)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:3\n88)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.jav\na:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:1\n82)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:7\n65)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:1\n\n52)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:54\n2)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpCo\nnnection.java:923)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:547)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.\njava:228)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n        at java.lang.Thread.run(Unknown Source)\n```\nThen I click on the OpenBuildings project on my OpenRefine and it begins to load and I hear the laptop fan kick into high gear and then just a spinner with this additional output...\n```\n10:22:00.626 [                   refine] POST /command/core/load-language (20287\n8ms)\n10:22:00.667 [                   refine] GET /command/core/get-preference (41ms)\n10:22:00.703 [                   refine] POST /command/core/load-language (36ms)\n10:22:00.755 [                   refine] GET /command/core/get-project-metadata\n(52ms)\n```\nand eventually the fan goes silent and then I get this additional information in the console log ...\n```\n10:25:49.663 [          org.mortbay.log] Error for /command/core/get-project-met\nadata (228908ms)\njava.lang.OutOfMemoryError: GC overhead limit exceeded\n        at java.util.Arrays.copyOfRange(Unknown Source)\n        at java.lang.String.(Unknown Source)\n        at com.fasterxml.jackson.core.sym.CharsToNameCanonicalizer._addSymbol(Ch\narsToNameCanonicalizer.java:490)\n        at com.fasterxml.jackson.core.sym.CharsToNameCanonicalizer.findSymbol(Ch\narsToNameCanonicalizer.java:463)\n        at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._parseName(Read\nerBasedJsonParser.java:1677)\n        at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(Reade\nrBasedJsonParser.java:682)\n        at com.google.refine.model.Cell.loadStreaming(Cell.java:151)\n        at com.google.refine.model.Row.loadStreaming(Row.java:247)\n        at com.google.refine.model.Row.load(Row.java:218)\n        at com.google.refine.model.Project.loadFromReader(Project.java:224)\n        at com.google.refine.model.Project.loadFromInputStream(Project.java:187)\n    at com.google.refine.io.ProjectUtilities.loadFromFile(ProjectUtilities.j\n\nava:157)\n        at com.google.refine.io.ProjectUtilities.load(ProjectUtilities.java:118)\n    at com.google.refine.io.FileProjectManager.loadProject(FileProjectManage\n\nr.java:231)\n        at com.google.refine.ProjectManager.getProject(ProjectManager.java:482)\n        at com.google.refine.commands.Command.getProject(Command.java:181)\n        at com.google.refine.commands.project.GetProjectMetadataCommand.doGet(Ge\ntProjectMetadataCommand.java:56)\n        at com.google.refine.RefineServlet.service(RefineServlet.java:170)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511\n)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet\nHandler.java:1166)\n        at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n    at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet\n\nHandler.java:1157)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:3\n88)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.jav\na:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:1\n82)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:7\n65)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:1\n\n52)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:54\n2)\n``\n. @jackyq2015 nope. @quan17partition()is not about regex matching, nor is it intended for Zero-length assertions, but instead can use a pattern to partition against.  Your assuming to much with thepartition()` function.  If there's a no match, then nothing is returned (in this case an empty String), and that's what we want.\nIF you need to see the differences between rows 3,4,5 ... then use our correct function... match().. @tcbuzor What's it look like with 10 columns ?. @tcbuzor 1 more option on the bottom please... \"remove whitespace from column names\", the Column 1 should be \"Column 1\" VARCHAR(1000), etc.  so I think having that quick option instead of forcing users to perform a rename on each column (painful) would be a \"good thing\".\nCREATE TABLE clipboard (\nrecord VARCHAR(1000),\nColumn 1 VARCHAR(1000),\nvalues 2 VARCHAR(1000),\nvalues 3 VARCHAR(1000),\nvalues 4 VARCHAR(1000),\nvalues 5 VARCHAR(1000),\nvalues 6 VARCHAR(1000),\nvalues 7 VARCHAR(1000),\nvalues 8 VARCHAR(1000)\n);\nINSERT INTO clipboard (record,Column 1,values 2,values 3,values 4,values 5,values 6,values 7,values 8) VALUES \n( 'Aldeaso\u00f1a 2010' ),\n( 'Spain' ),\n( 'Castilla-y-Le\u00f3n' ),\n( 'Vino de la Tierra    ' ),\n( 'Bodega Convento San Francisco, S.L. (Producer)' ),\n( '+39 983 87 80 52' ),\n( 'www.bodegaconvento.com' ),\n( 'record','record','Alfredo Santamaria 2013','Spain','Castilla-y-Le\u00f3n','Cigales    ','Alfredo Santamaria Arias, S.L. (Producer)','+34 983 58 50 06','www.bodega-santamaria.com' ),\n( 'Alfredo Santamaria 2013','Alfredo Santamaria 2013','Spain','Castilla-y-Le\u00f3n','Cigales    ','Alfredo Santamaria Arias, S.L. (Producer)','+34 983 58 50 06','www.bodega-santamaria.com' ),\n( 'Spain','Alfredo Santamaria 2013','Spain','Castilla-y-Le\u00f3n','Cigales  ','Alfredo Santamaria Arias, S.L. (Producer)','+34 983 58 50 06','www.bodega-santamaria.com' )\nAlso, another nice feature to have... if I facet on only 24 rows of my 100 rows... I expect that the Export to SQL will only preview those 24 rows that I have a Text facet on.  When I tried this, it was previewing all rows, no matter my Facet options...otherwise the preview ends up looking like above, where there is a schema alignment issue because it is previewing the first 10 rows of my 100 rows, where some are not in the right alignment and I knew that and why I applied a facet on only the ones rows I wanted to export to SQL and preview.. @tcbuzor umm... I wasn't saying to ignore the facets, but to respect them ?. @tcbuzor ok, the Facet respecting works now.  But not the trim on column names for preview or download..\n\nCREATE TABLE clipboard (\nrecord VARCHAR(255),\nColumn 1 VARCHAR(255),\nvalues 2 VARCHAR(255),\nvalues 3 VARCHAR(255),\nvalues 4 VARCHAR(255),\nvalues 5 VARCHAR(255),\nvalues 6 VARCHAR(255),\nvalues 7 VARCHAR(255),\nvalues 8 VARCHAR(255)\n);\nINSERT INTO clipboard (record,Column 1,values 2,values 3,values 4,values 5,values 6,values 7,values 8) VALUES \n( 'record','record','Alfredo Santamaria 2013','Spain','Castilla-y-Le\u00f3n','Cigales    ','Alfredo Santamaria Arias, S.L. (Producer)','+34 983 58 50 06','www.bodega-santamaria.com' ),\n( 'record','record','Al-Ria Reserva 2015','Portugal','Algarve','Vinho Regional  ','Casa Santos Lima (Producer)','+351 2 63 76 90 93;+351 2 63 76 06 21','www.casasantoslima.com' ),\n( 'record','record','Amaren Tempranillo Reserva 2009','Spain','Rioja','Rioja Alavesa    ','Araex Rioja Alavesa SL (Producer)','+34 945 15 05 88','www.araex.com' ),\n( 'record','record','Aponte Reserva 2008','Spain','Castilla-y-Le\u00f3n','Toro   ','Bodegas Frontaura SLU (Producer)','+34 983 88 04 88','www.bodegasfrontaura.com' ),\n( 'record','record','Arzuaga Reserva Especial 2011','Spain','Castilla-y-Le\u00f3n','Ribera del Duero ','Bodegas Arzuaga Navarro SL (Producer)','+34 983 68 11 46','www.arzuaganavarro.com' ),\n( 'record','record','Baigorri Belus 2013','Spain','Rioja','Rioja Alavesa    ','Bodegas Baigorri (Producer)','+34 945 60 94 20','www.bodegasbaigorri.com' ),\n( 'record','record','Bottega Il Vino degli Dei 2012','Italy','Veneto','Amarone della Valpolicella DOCG  ','Bottega SPA (Producer)','+39 04 38 40 67','www.bottegaspa.com' ),\n( 'record','record','Burro Loco Rosado 2016','Spain','Castilla-y-Le\u00f3n','Vino de la Tierra   ','Concejo Bodegas SL (Producer)','+34 983 50 22 63','www.concejobodegas.com' ),\n( 'record','record','Cancellaia 2012','Italy','Toscana','Toscana IGT    ','Az. Agr. Pakravan-Papi (Producer)','+39 3356 00 44 46','www.pakravan-papi.it' ),\n( 'record','record','Cardela 2014','Spain','Castilla-y-Le\u00f3n','Ribera del Duero  ','Bodegas Boh\u00f3rquez, S.L. (Producer)','+34 915 64 37 51','www.bodegasbohorquez.com' ),\n( 'record','record','Casa Ferreirinha Vinha Grande Red 2014','Portugal','Porto E Douro','Douro  ','Sogrape Vinhos SA (Producer)','+351 227 86 80 08;+351 227 85 03 00','www.sograpevinhos.com' ),\n( 'record','record','Casa Mayor Old Vines Cabernet Sauvignon 2016','Chile','Valle de Colchagua','Vi\u00f1a Casa Silva SA (Producer)','+56 7 22 71 65 19','www.casasilva.cl' ),\n( 'record','record','Castello di Vicarello 2012','Italy','Toscana','Toscana IGT ','Castello di Vicarello (Producer)','+39 0564 99 04 47','www.castellodivicarellovini.com' ),\n( 'record','record','Castillo Labastida Ermita Santa Lucia 2016','Spain','Rioja','Rioja Alavesa ','Araex Rioja Alavesa SL (Producer)','+34 945 15 05 88','www.araex.com' ),\n( 'record','record','Castroviejo 2013','Spain','Rioja','Rioja DOCa  ','Bodegas Pastor Diaz Sl (Producer)','+34 941 14 23 90','www.pastordiaz.com' ),\n( 'record','record','Cava Amorany Brut Gran Cuv\u00e9e','Spain','Catalu\u00f1a','Cava ','Vid Vica SL (Producer)','+34 933 10 22 62','www.vidvica.com' ),\n( 'record','record','Champagne Bauchet Mill\u00e9sime Premier Cru 2009','France','Champagne','Champagne  ','Champagne Bauchet (Producer)','+33 3 26 28 17 72; +33 3 26 58 17 72','www.champagne-bauchet.com' ),\n( 'record','record','Champagne Fresne Ducret La Grande Hermine 2009','France','Champagne','Champagne    ','Champagne Fresne Ducret/SCEV JA MILAUR (Producer)','+33 3 26 49 24 60','www.champagne-fresne-ducret.com' ),\n( 'record','record','Champagne Jean Dumangin Brut Mill\u00e9sime 2009','France','Champagne','Champagne   ','EARL Jean Dumangin (Producer)','+33 3 26 03 42 17','www.champagne-jean-dumangin.fr' ),\n( 'record','record','Champagne Marc Perla-Rosa 2012','France','Champagne','Champagne    ','Champagne Marc (Producer)','+33 3 26 58 46 88','www.champagne-marc.fr' ),\n( 'record','record','Champagne Vincent Lamoureux Ros\u00e9','France','Champagne','Champagne  ','EARL Lamoureux Vincent (Producer)','+33 3 25 29 39 32','www.champagne-lamoureux-vincent.fr' ),\n( 'record','record','Ch\u00e2teau La Fleur Peyrabon 2014','France','Bordeaux','Pauillac  ','Ch\u00e2teau Peyrabon (Producer)','+33 5 56 59 57 10','www.chateau-peyrabon.com' ),\n( 'record','record','Ch\u00e2teau Moulin du Terrier 2016','France','Bordeaux','Bordeaux Rouge    ','GAEC Forcato (Producer)','+33 6 71 21 57 71','www.chateau-lary.com' ),\n( 'record','record','Ch\u00e2teau Ventenac Carla Ros\u00e9 2016','France','Languedoc-Roussillon','Cabard\u00e8s    ','Maison Ventenac (Producer)','+33 4 68 24 93 42','www.maisonventenac.fr' ). @tcbuzor I think this is feature complete now from my side.\n@jackyq2015 @ettorerizza could you test locally also and give it a review also ?. @ettorerizza I would rather not force opinion but think we should instead give the user an option to \"Possible solution: transform all null into empty strings just before export.\"\nThe \"handle records as relationships\" for SQL export would be an enhancement, and THAT would have a multitude of options since users differ on their needs.  I speak from experience with this.\n@ettorerizza So... Go ahead and open a new enhancement issue for your design thoughts on \"handle records as relationships\" and provide drawings even if you can about how you ideally would see an interface to deal with that and we can have @tcbuzor Tony work on that in a future enhancement...and ideally once we get our front / backend separated.. @tcbuzor Just insert a null, as discussed above. We all upvoted.. @jackyq2015 I'll need your eyes on this as final verify, please.  Take a look. Thanks.. Not against this issue, but... this issue has come up before where there's automation needs for scripting especially around batch processing.  Most data architects do this already in other tools.  I personally use Apache NiFi (it has a Variable Registry now for that), where my users can actually modify the data flow even at runtime via Python scripts and change environment variables on a whim.\nI'm curious about your less technical users, since that's exactly the user base we tried to design for with OpenRefine.  When you say \"cleanse a file\", I guess you mean there's an already developed Python script that the team shares and it takes arguments, and you want to store those arguments as a preference or environment variable ?  So that they can just choose the custom Python script and quickly configure it ? . All regexp functions should work like replace() and partition().  Where the regex is enclosed in / /\nIf that is not the case in all functions, then that was/is an oversight and should be fixed, and ideally upon importing older projects we should seamlessly upgrade older JSON operations to use the regex enclosed in / /\n@gobertm is it only our match() that uses \" \" for enclosing regex and all other regex style functions use / / ?  Did you scan our code base ?. @ettorerizza Yes there is a Java package for Reading Writing RDF serializations now called Riot from the Jena framework\noaj.riot Package Summary\n. @ettorerizza give us a dump of the console please.. @ettorerizza I mean both consoles... including javascript ( from your Chrome browser CTRL + Shift + J ). @ettorerizza yeah, but keep it open when you start OpenRefine and then perform your import test...does anything else show up in the console , any errors ?  Also...disable all your browser extensions when your testing stuff like this :). @jackyq2015 please fix :)  I also reproduced this same issue just now on the wikidata-extension branch I'm testing other things with.  And same error as @ettorerizza \n```\nJQMIGRATE: Logging is active\nindex-bundle.js:10335:2\nwindow.controllers/Controllers is deprecated. Do not use it for UA detection.\nindex-bundle.js:44477:1\nSynchronous XMLHttpRequest on the main thread is deprecated because of its detrimental effects to the end user\u2019s experience. For more help http://xhr.spec.whatwg.org/\nindex-bundle.js:9594:5\nThe character encoding of a framed document was not declared. The document may appear different if viewed without the document framing it.\nimporting-controller\nXML Parsing Error: not well-formed\nLocation: http://127.0.0.1:3333/command/core/get-importing-job-status?jobID=1\nLine Number 1, Column 1:\nget-importing-job-status:1:1\nXML Parsing Error: not well-formed\nLocation: http://127.0.0.1:3333/command/core/importing-controller?controller=core%2Fdefault-importing-controller&jobID=1&subCommand=initialize-parser-ui&format=text%2Frdf%2Bn3\nLine Number 1, Column 1:\nimporting-controller:1:1\n```. @jackyq2015 is this an exposed option or not ?\nTo aid in checking for errors in UTF8-encoded files, there is a utility which reads a file of bytes as UTF8 and checks the encoding.\nutf8 -- read bytes as UTF8\n\n. @jackyq2015 forget it.  we can deal with that option later.. @jackyq2015 my test5 workspace is also screwed up by this.  Although it is a test workspace and I backup using iterative \"testX\" workspace paths.  I guess Antonin doesn't know about our best practices of creating backups of the workspaces while developing with OpenRefine :)  @wetneb Now you know.  But seriously Jacky, yes it would be nice to have my test5 workspace fixed up.  Hmm.. perhaps just a preferences array for crap that we break and change along the way, so we can fix without disruptions sometimes ?  [\"dateAddZ\":\"True\",\"newGrelContainsCommandFallback\":\"True\"]. @ostephens yeah, or the simple fix... find/replace in the metadata.json :)  Good enough for now and works.. Yes, just choose Python or Clojure as your scripting language...rather than GREL.  It will be easier for you.\n. Lots of examples on our Wiki of IF statements with Python... https://github.com/OpenRefine/OpenRefine/wiki/Jython. @wetneb Works as advertised.  Good Job !. @wetneb LGTM\n@jackyq2015 Yeah, @paulhoule has his Base KB http://basekb.com/gold/  (and why, you wanna query all my cool writes I did into it since 2007 ? \ud83d\ude03 ). +1 agree on updating httpclient.  Besides Java 9/10/11 gets a new shiny HttpClient soon anyways. http://openjdk.java.net/groups/net/httpclient/intro.html. @ostephens Its a smartSplit() issue.  Looks like David Huynh never went in and added the multichar support (I checked the code and history in link below) and so it is only looking at a single char for the sep String instead of any number of characters for sep.  This was added as an enhancement for split() and also on the menu for \"Split into several columns...\" and the reason I know this is because I asked @dfhuynh to change that functionality back in 2010, which he did.  BUT...doesn't look like we did that for smartSplit() .... so yeah... smartSplit() only takes the 1st character in the sep argument supplied and ignores any other characters in the argument.\nNotice here it splits on everything with \"r\" ... but not my supplied \"rd\"\nI'd say that's a enhancement from 2010 that I asked for that never happened in smartSplit()  \ud83d\ude00 and not a classic bug \ud83d\ude04 \n\nSo your Store Array functionality is fine.  And works great from my testing on all sides, so far.\nIssue is our inaccurate documentation and description of smartSplit() saying it works with a sep String, when it fact it only works with a sep Char ... and make a change to have it work like the other split functions and allow sep to be a String and not just charAt(0) , and keeping its \"smarts\" with special handling of quotes and guessing tabs or commas of sep String is not supplied.\n@ostephens @ettorerizza @wetneb If you agree on above enhancement with smartSplit() then create new issue with above info copied in.. I say we go forward.\nAnd its not hard to go forward... it just means everyone putting better Tests in place where we are lacking them.\nLike here especially lacking:\nhttps://github.com/OpenRefine/OpenRefine/blob/master/main/tests/server/src/com/google/refine/tests/expr/functions/ExpressionUtilsTests.java\nBut basically adding good tests for these 44 source files that involve Date:\nhttps://github.com/search?utf8=%E2%9C%93&q=Date+repo%3Aopenrefine%2Fopenrefine+path%3A%2Fmain%2Fsrc+language%3AJava+language%3AJava&type=Code&ref=advsearch&l=Java&l=Java. Thanks for this David!  Do you care to update our Wiki FAQ ?  I'll give you write access to fix this up and then let us know in this issue and we'll close it out.\n. @griii2 Actually, you should already have EDIT rights for our Wiki.  Look for the Edit button in the top right corner of any Wiki page.. @griii2 if you have further problems just let us know on our mailing list.. I already stated my design decision to get past the differing opinions.\nThat is to go forward...take the extra effort and time to go to Java 8\nclasses and using java.time as necessary. We will not use Java 7 classes.\nOn Thu, May 3, 2018, 1:31 AM Antonin Delpeuch notifications@github.com\nwrote:\n\n@wetneb commented on this pull request.\nI think we should either migrate all dates to the Java 8 classes or stick\nwith Java 7 instances. So the new part to handle OffsetDateTime is not\nnecessary - or it should replace entirely the Calendar and Date classes\n(but in that case there are a lot of other things that should be migrated\ntoo).\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/1592#pullrequestreview-117147070,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA8NRvm2L6kEQlJ-oLfOqWLK_0SK8hOCks5tuqQwgaJpZM4TwZ1H\n.\n. Can't. On vacation. Do not rush. We have time to do this right is what I am\nsaying.\n\nOn Thu, May 3, 2018, 8:46 AM Antonin Delpeuch notifications@github.com\nwrote:\n\n@thadguidry https://github.com/thadguidry dear \"Lead Designer\", let's\nhave a call maybe? I value your feedback as a user but this is about a\ntechnical decision.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/1592#issuecomment-386300765,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA8NRklfqmNzfxb5Eo1_wEcecq2JD9SCks5tuwoWgaJpZM4TwZ1H\n.\n. @ostephens Thad describes it as The Most Preferred Behavior By The Majority Our Users :). @lucaswerkmeister are you trying to figure out how to import your data into an OpenRefine project and limit by a certain number of rows ? or limit by only a few columns ?\n\nOne trick to do systematic subsetting of the data set is to create a\ncustom text facet on any column with this expression\n mod(row.index, 10)\n\nThen select one facet choice at a time.\nIt is hard to figure out exactly what it is your having difficulty with.  Reconciling produces a result...that is called candidates and you can then match on one of them.  You can create new additional columns with anything from a Recon object.  See our Variables doc about the recon object \nDoes any of the above help you ?  . @ostephens is correct.  The right way to do it is not skip the reconciling.  And instead just speed up the reconciling.  One way this was done in the past was that with Freebase service (prior to Wikidata), you could tell Freebase that you in fact had MID identifiers already in a column...then Freebase would use a much faster index lookup that OpenRefine could send over 1000 MID lookups in a batch request and return all the matches in about 10 seconds.. @wetneb Lucas and others just replied to my question on the wikidata-tech mailing list about fastest query to verify a QID...\nhttps://lists.wikimedia.org/pipermail/wikidata-tech/2018-May/001251.html. @jackyq2015 you forgot to add Nano support to datePart()  or is that going into another PR / issue ?. Tested as fixed !  Thanks Owen !. @wetneb Can you check the Project version and if 2.7 or earlier run original, otherwise run newer ?\nPerhaps yet another way is when we load the history file itself...you can inspect the file date and if earlier than \"2017-11-18\" (the release date of 2.8) consider it original...and when we write back, ensure it becomes newer ?\n2.8\ncom.google.refine.model.changes.CellChange\nrow=0\ncell=0\nold={\"v\":\"Autonomous single-door controller with built-in Proximity card reader and 3x4 keyboard for programming.\"}\nnew={\"v\":\"{\\n    \\\"id\\\": 1,\\n    \\\"name\\\": \\\"A green door\\\",\\n    \\\"price\\\": 12.50,\\n    \\\"tags\\\": [\\\"home\\\", \\\"green\\\"]\\n}\"}\n/ec/\n2.6-alpha.2\ncom.google.refine.model.changes.ReconChange\nnewReconConfig=\nnewReconStats={\"nonBlanks\":1,\"newTopics\":1,\"matchedTopics\":0}\noldReconConfig=\noldReconStats=\ncommonColumnName=Column 1\nupdateRowContextDependencies=false\ncellChangeCount=1\nrow=0\ncell=0\nold={\"v\":\"2018-02-11T00:00:00Z\",\"t\":\"date\"}\nnew={\"v\":\"2018-02-11T00:00:00Z\",\"t\":\"date\",\"r\":\"1526819887401036191\"}\n/ec/\n/ec/. @ostephens That's 24 hour time format as the default.  I don't think we should change this.  Our tests can check for various time formats, certainly.  But 24 hour time is the default time format for nearly everything in tech and programming.  And besides... we already know that and documented as such in the code comment already for CalendarParser https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/expr/util/CalendarParser.java#L82. @ostephens We have always conformed to ISO8601 since Google Refine 2.1 days and we added localization of dates during 2.6 also.... In fact, I've tested this many times before, and see Tom's mention here: https://github.com/OpenRefine/OpenRefine/issues/294#issuecomment-9432067\nYour doing it wrong.\nWhat your doing is your asking for 12 hour format....instead of 24 hour format.\nvalue.toDate().toString(\"yyyy-MM-dd HH:mm:ss\")\nnotice my capital HH's\n. @ostephens @jackyq2015 Guys... please ensure we have testing for 12/24 Hour formats as well !!!!  And converting back and forth between them with\ntoString(\"yyyy-MM-dd HH:mm:ss\")\nand\ntoString(\"yyyy-MM-dd hh:mm:ss\"). @jackyq2015 and just to remind you, recall we added support for locale with toDate() in 2.6 and various internationalization bits https://github.com/OpenRefine/OpenRefine/releases/tag/2.6-alpha.2  Here is Tom's commit for the toDate() changes https://github.com/OpenRefine/OpenRefine/commit/fa072df85c27ce601f867a7f39c5a5d1bf288dec. @martinjrobins Our current testing platform on Travis and Appveyor do not test against Java 9 or 10 or 11.  We traditionally setup our testing frameworks with the latest release Java and run tests against those and fix issues that come up.  I am not against this PR, but it might expose bugs/issues for users that we have not yet discovered.  Then again, those users NOT using the latest Java are probably very advanced users to begin with, so this PR is in fact...harmless. :) \nWe could certainly also improve our testing framework scripts, so that we test on current Java and the next version upcoming.  Feel free to submit a pull request against our Travis or Appveyor files as well for that.\n. @martinjrobins I'd be fine with a warning, and that is probably the best way to handle this.. Its fine.  You guys can make a CONTRIBUTORS file and move things over to that as necessary. @ostephens on your fork...do\n git reset @{u} --hard\n and this will ensure that it does a hard reset against the upstream.  Then \"refine clean\" and test again.. @jackyq2015 is this reproducible with what we have in master now ?. @ostephens The Bug: regardless of FF or Chrome, \n1. if you try my example 2 line dataset above by importing with clipboard\n2. and then simply check and uncheck the\n\n[ ] Use character \" to enclose cells containing column separators\n\nyou will see that the quotes do not appear when it is unchecked.\nWe added this option for those users that wanted to keep quotes when imported or not.\nIn FF, I noticed that extra XML Parsing error in the Web Console and there's a little View Details link in the Web Console for FF right after the error, so I clicked it, and then it seems to cause the error stack starting with\njava.lang.IllegalArgumentException: parameter 's' should not be null ...\n. @jackyq2015 umm, how does a user tell OpenRefine to keep the single quotes and import them into each cell, for them to handle manually later on with GREL, Facets, etc.\nI think, especially for CSV/TSV importer, an additional import option is needed, called \"keep quotes during import\".  The newer versions of OpenCSV library has support for many more options that we are not taking advantage of within CSVParser unfortunately.\nI would advise you to look through the BLAME view (left side of this view of what David, Tom, others have added support for and where.\nMy take is that Tom didn't take the fix far enough along with #490 and expand user options during import.  We want that now, give the user more power and control by exposing what is available within OpenCSV.\nhttps://sourceforge.net/p/opencsv/source/ci/master/tree/src/main/java/com/opencsv/CSVParser.java#l192\n. @jackyq2015 Sure.  Jackson also provides many options now https://github.com/FasterXML/jackson-dataformats-text/blob/master/csv/src/main/java/com/fasterxml/jackson/dataformat/csv/CsvParser.java#L27\nhttps://github.com/FasterXML/jackson-dataformats-text/blob/master/csv/src/main/java/com/fasterxml/jackson/dataformat/csv/CsvSchema.java. Awesome. Let's Merge it then.\nI set mine to 12000 also ,and it timed out.  I removed the preference and then it worked fine with your new defaults in code.\n23:00:52.359 [                   refine] POST /command/core/importing-controller (2ms)\n23:01:04.403 [          org.mortbay.log] /command/core/importing-controller: java.net.SocketTimeoutException: Read timed out (12044ms)\nSo good to merge I would say.. Looking at the Git commit history and see the impacts of org.json.* it is highly likely it came from either\nhttps://github.com/OpenRefine/OpenRefine/commit/19f98b7ea2a012065a9e21999a9f9dc30347daf7\nor\nhttps://github.com/OpenRefine/OpenRefine/commit/f58d963dbd459036bbe3cbf505ba38b093e14266\nTurn on Debug and Step through with a breakpoint on com.google.refine.operations.cell.MassEditOperation.reconstructEdits(MassEditOperation.java:124)\nto see what the JSON Array's index 0 value really is.\nIts also interesting to note that the description on JSONArray itself...which to me provides the clue\nhttps://stleary.github.io/JSON-java/org/json/JSONArray.html\n. the generic JSONArray.get() might be useful for 1st pass inspection.\nAlternatively, you possibly could use https://stleary.github.io/JSON-java/org/json/JSONArray.html#optJSONObject-int- for that 1st pass inspection, since the opt methods do not throw an error and instead return null.\nGet the optional JSONObject associated with an index. Null is returned if the key is not found, or null if the index has no value, or if the value is not a JSONObject.\nUp to you @ostephens how you think best to handle it.  Some ways are easier for test setup, but there might be more thorough tests needed in those cases.. @ostephens I am so impressed with your digging in and solving real problems and coming up with great solutions to our issues.  Thank you so much for stepping up and learning beyond what you originally intended of helping out on \"easy fixes where you can\" !  Keep up the great work , we really appreciate it !. Hi !  We just released 3.0 Beta of OpenRefine.  Please try it out, as we have added several new features to help you with NULL values, such as new Facets and a new Toggle mode for displaying NULL cells.  You can read about the new features and download the newest version on our releases page https://github.com/OpenRefine/OpenRefine/releases\nWe would appreciate any feedback you can give us on how we can make things even easier for your needs !. Sorry, WHAT are you trying to perform ?  So you have some blank Zip Codes on 891 rows... they basically look like this \"\"  ... so what do you want to do with that empty string (also called a blank) ?. Owen,\nGreat writeup actually !  I've added this info to our Wiki under 2 pages:\nhttps://github.com/OpenRefine/OpenRefine/wiki/Cell-Editing#what-is-a-cell-\nhttps://github.com/OpenRefine/OpenRefine/wiki/GREL-String-Functions  <--we\nwere missing the toString() function, and was found only on Date page\nhttps://github.com/OpenRefine/OpenRefine/wiki/GREL-Date-Functions\nFeel free to edit more and improve.\n-Thad\n. @ostephens We wanted toString() to be able to convert any Data Type to a string, that included null.  Sorry, but how does this hurt ?  Can you give me a scenario where this causes some grief ?. Oh yes, I completely agree on those points.  And I'm not sure why Tom did that...other than visibility of null's.  But I always had the idea of simply handling that via display...and not Data Type conversions as he did. (and this is how other programs deal with it also, as I have demonstrated in other issues)\nBesides which...for those that truly want NULL in the value of a cell...they can use replace(\"\",\"null\")  Yes ?\n+1  Please revert toString to old style.  Knowing that we have new ways to make null visible now.. @jackyq2015 perhaps have @yaeln zip up his complete OpenRefine workspace folder and email to you privately and you can troubleshoot more since you have a macOS ?  Or is this really something OS specific ?. Butterfly loads things from the Classpath.  Classpath is figured out through the start script.  Maybe the Classpath is munged for some reason ?\n@jackyq2015 has he tried to just run using the Linux start script.... ./refine ?  Because I keep seeing JavaAppLauncher being used in his replies.\nDebug the entire start flow with\nbash -x ./refine\nand then let's see what it looks like.\n@jackyq2015 It also could be a corner case with reflection ? https://developer.jboss.org/thread/251978?_sscc=t\n  . @yaeln Can you move your OpenRefine install outside of a DropBox path ?  I'm wondering if something with a Dropbox client might also be causing some conflict ?  dunno....\n/Users/yaeln/Dropbox/OpenRefine3/openrefine-3.0-beta/webapp. Hi @susannaanas !  Thanks for the report on 3.0 Beta !\nHow exactly did you perform the remove for \"Even after removing the items marked \"don't reconcile\" ?. @susannaanas Is it a super urgent need ?  Curious, Do you know of how many other folks that would benefit from this enhancement other than you ?. @wetneb However not sure what to do about the occasional Wikibaseapi issues.  I re-ran the Travis builds and then those were sorted out as well.. @praveene04 that is fine....its running as you can see it said \"Started OpenRefine 2.8..\"\nDon't worry about the Failed to use jdatapath... that's just our user data path detection and it resorted to env variables.\nNow with that running.. just open your web browser and go to http://127.0.0.1:3333\n. @jackyq2015 Hey...hmm, should we default to localhost for Macs ?  Wondering if that makes things easier, or if it was just @praveene04 weird setup ?. Confirmed by me also...with projects with or without tags, doesn't matter, still get same error in console.\n@jackyq2015 you have a typo !  not \"meda\" but needs to be \"meta\"\nat com.google.refine.model.medadata.MetadataFactory.buildDataPackageMetadata(MetadataFactory.java:57)\noh wait, perhaps you named the package that way for isolation reasons.. @magdmartin No, I ALREADY looked at Jackson and it's usable, but quite a bit of work for @jackyq2015 or whomever, like... A LOT of work actually :-)\nAnd @wetneb has mentioned within https://github.com/OpenRefine/OpenRefine/blob/master/extensions/wikidata/src/org/openrefine/wikidata/utils/JacksonJsonizable.java#L41. @ostephens I would probably begin looking at this through an Interface lens.  So that TDD (Test Driven Development) is approached from the very beginning.  We actually have an Interface: \ncom.google.refine.Jsonizable\nwho's consumers implement it, and would be the first test points :\ncom.google.refine.model.AbstractOperation\ncom.google.refine.model.Cell\ncom.google.refine.clustering.Clusterer\ncom.google.refine.model.Column\ncom.google.refine.model.ColumnGroup\ncom.google.refine.model.ColumnModel\ncom.google.refine.commands.Command\ncom.google.refine.commands.browsing.ComputeClustersCommand\ncom.google.refine.commands.browsing.ComputeFacetsCommand\ncom.google.refine.grel.Control\ncom.google.refine.browsing.DecoratedValue\ncom.google.refine.browsing.Engine\ncom.google.refine.expr.EvalError\ncom.google.refine.browsing.facets.Facet\ncom.google.refine.grel.Function\ncom.google.refine.commands.history.GetHistoryCommand\ncom.google.refine.commands.history.GetProcessesCommand\ncom.google.refine.commands.project.GetProjectMetadataCommand\ncom.google.refine.history.History\ncom.google.refine.history.HistoryEntry\ncom.google.refine.commands.HttpUtilities\ncom.google.refine.model.medadata.IMetadata\ncom.google.refine.importing.ImportingJob\ncom.google.refine.operations.cell.MassEditOperation\ncom.google.refine.browsing.facets.NominalFacetChoice\ncom.google.refine.model.OverlayModel\ncom.google.refine.util.Pool\ncom.google.refine.preference.PreferenceStore\ncom.google.refine.process.Process\ncom.google.refine.process.ProcessManager\ncom.google.refine.model.Recon\ncom.google.refine.model.ReconCandidate\ncom.google.refine.model.recon.ReconConfig\ncom.google.refine.model.ReconStats\ncom.google.refine.model.ReconType\ncom.google.refine.model.RecordModel\ncom.google.refine.model.Row\ncom.google.refine.preference.TopList\n\nWe'd want to start with Org.JSON 's serialization/deserialization feature set and compare and map those to Jackson's\nhttps://github.com/FasterXML/jackson-databind to build a utility class to replace org.json.JSONObject\nWith that utility class that can mimic existing JSONObject and object manipulation with POJO's and affords the same options as Org.Json did begin conducting test cases across \"the work set\": https://github.com/OpenRefine/OpenRefine/search?l=Java&q=%22JSONObject%22\n??? - @jackyq2015 what next ? probably Writer and Exception ?  Maybe Exception should even be first, since we want to do TDD\n. @wetneb You came to similar conclusions that I did when I first looked at Jackson... that most of our classes should not need any custom code for de/serialization since annotations should be sufficient, and we ensure the serialization format is kept identical.\n\nGood Job.  I like the approach.\nAnd agree about a migration guide for developers and extension authors.\n. Let's continue to make small incremental changes, as you are doing, and not get bogged down, and as you say we can do more refactoring later.  After all, with all the work you are putting in graciously, we want some Christmas presents this year, not next ;). @wetneb hey on this paragraph on the migration guide, can you add a line link to one of our tests as an example of this, so that authors are shown a good way to frame this kind of testing.  I'm worried that some might not fully know :\n\nBefore undertaking the migration, we recommend that you write some tests which serialize and deserialize your objects. This will help you make sure that the JSON format is preserved during the migration.. @wetneb LOL, We're preparing the user for the \"Enterprise world\" where they will have a love/hate relationship later on when the only language available to them will be SQL !. @tcbuzor Please fix this VERY SOON.. prefer the widest distro packaging...if that is .deb or whatever nowadays,  ppa seems limiting?. @ostephens Right...and CONSISTENCY MATTERS.  So, I'm glad that your making it so.. @ostephens \n\nTL:DR; \"trying to treat objects as equivalent to some strings in this situation is probably a bad idea.\"\nCorrect @ostephens , it is a bad idea.\nYou probably need to know this history....\nFacets were always designed to be Datatype specific....the exception to the rule was the Text Facet.\nThat behavior goes way back to pre 1.0 days actually.  And your scenarios I just tested on 2.6 alpha 2\nThe Datatypes other than Text, those Facets always excluded rows that were not of the given Datatype...so yeah, folks would miss rows of data, and that was expected.  In fact, that's why David added for me the value.type()  I.E., it wasn't a bug, but a design choice to restrict Facet handling, otherwise, as you see, things get unwieldy trying to deal with different Datatypes within one process or operation, its not impossible, but we always thought that folks would like to work in layers, even with deducing the Datatype.  For those folks with less precise needs, the Text Facet worked as it flattened and converted everything toString and presented what you had, even null/blank cells.\nIn OpenRefine, my opinion would be to continue to keep Facets as being Datatype specific in their handling or evaluation of data except for Text Facet.  Just as we always have.\nIt may also be better to just add more Datatype Facets to account for various Datatypes or new one's we introduce, for instance my Json Datatype suggestion.\nWe are definitely missing a real Boolean facet and also the Type evaluation for a Boolean value.type() https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/expr/functions/Type.java\nYes, we could enhance Facets to have mini-filters (datatype buttons underneath,etc.) themselves to expose more/less rows by using value.type() against them.  But that kind of UI work, might be better served by focusing first on \"the Beast\", going to a new UI and separating Front/Backend ops and then making beautiful Facets.  But don't let me stop you from enhancing the current state of things.\nHope that helps ?\n. @ostephens \"I think we need the text/list facet to treat non strings in a consistent way\"\n...and that consistent way is done by tackling \"The Beast\" , where a column index for Values As Text, is kept and where the Text Facet looks to that column index.\nDidn't our current data structure already keep internally or have that kind of index when the Text Facet is built ?  I've forgotten, perhaps its just an index of Objects.  so might need an index of Strings.  but I really thought we were already doing that internally somewhere...\"making a copy\".  @jackyq2015 might know more.\n. @ostephens No, you misunderstood.  Do not do any fancy conversions for the Text Facet.  We never did that before.  When I said that Text Facet was the exception to the rule... I meant that the Text Facet back then would only deal with String Datatypes.  It would actually still display in the Facet, a Datetype and its Count correctly, along with String Datatypes, but the rows would never show if you selected the Date Datatype in that Text Facet...since they were not a Text Datatype.  Also, back in Google Refine 2.0 r1836, it would still show Dates in a column grid as Black and White color, not Green color.   We eventually improved things as time went on.  Here is how it looked and worked in Google Refine 2.0 r1836...where in the Text Facet I have selected 1 String Datatype (whose row is showing) and 2 Date Datatype's (whose rows do not show in grid, but only the String Datatype)\n\nSo for the current state we are in now...I would prefer that we have users using different Facets to deal with different data types.  For the Date Datatypes still showing in a Text Facet...I would probably not even show them any longer, we did that back in the day as a convenience.  But it often confused and we dealt with it on the mailing list.\nThe Text Facet does not have to show every non-Text Datatype value... only String Datatype values, its a TEXT Facet after all.  And I think that is what you are asking us.  So my answer is, Change the Text Facet, so that Users can only see String Datatype values.  Then you are left wondering...\"but what about non-Text Datatype values\" ?  I would say that you show at the top of the Text Facet something like \"non Text Rows\" (like we do with Blank and Null) and a Count next to it, which should be warning enough to users that \"hey you have some rows in this column that are not of a Text type!  Go deal with them somehow with other Facets, etc\".  And then also add more Facets to deal with those other Datatypes, like Boolean, etc.\nFor the future, when we go to a new UI, we can take the same backend code and eventually refactor into a smarter multi-Datatype Facet, similar to how Data Wrangler does, and deal with the \"non Text Rows\" in a much more elegant fashion in a single Facet dialog.  \nAsk if anything is unclear from above.  I want to ensure you get the vision here.... I.E.  don't disturb the current Text Facet handling too much...we'll do that later on with more funding and help.. Why do we keep forgetting our design agenda?  Perhaps because some of it is always caught up inside issues I guess.  But nearly every \"enhancement\" is a \"design discussion\".  I'm all ears as to where / how we want to keep our design agenda going forward, so that we don't forget these things.  Right now it's a bit of Projects / Issues / Google Docs\nRegardless, @ostephens and I and @ettorerizza had already agreed on a design going forward, where we would provide many more facets, such as Boolean.  We just haven't got there yet.  Owen's first pass was to make it clear to users that they are dealing with different TYPES of data.  We have that now and it clearly made Ettore think more about his data :-) and that's exactly what we wanted in the first place. (this is in keeping with OpenRefine as a power tool and not doing to many automatic things that hide data nuances to our users) Now we just need more facets or operations to deal with OTHER TYPES of data BESIDES TEXT.  (yes Owen, our design forward is to keep Text facet as Text)\nFor 2019, we need to begin to start on:\ndesign sketches for our new UI around Facets would be most welcome to be put up somewhere.  React-based live Facet experiments would be even more welcome so we could play with the fit and feel (and we did this back in the Gridworks/Google Refine days with Google web developers).  I just don't know where... Heroku ? (https://github.com/mars/heroku-cra-node)\nSince @wetneb has already stated that we need to begin to nail down the UI, to help with the needs of the API and decoupling.. Actually, I do not see why in 2018, that a new tab has to be opened at all for a download from a webapp.  We should look into handling all that seamlessly and in a better fashion across all 3 browsers.. @visch so you said you don't want a javascript alert.  So you want no alerting at all and just silently timeout and leave you on the datagrid ?  How is that useful to you ?  Give us more details on how you imagine what OpenRefine would show you, or do, if the Wikidata API times out.  Tell us how you want this designed, with step by step.  Tell us how \"fail more gracefully\" would look like to you in OpenRefine ?. @ostephens Nice.  The functionality of it is fine for me.  Like I said before in issue, this can be enhanced further later on once we get a new UI, but for now its functions as expected.  Where later you might want to catch when they are about to mass edit a 'true' or 'false' and throw up a pop-up warning \"This Edit will also update rows of boolean values True or False\", since it is a bit unexpected when they see 2 groupings and select 1 to mass edit and lo-and-behold the other grouping also changes even when they didn't select it, but that's exactly what we have been doing since 2.0 days.\nSo up to you on warning dialogs, now or later.. One small question:  Are we OK with having \"undefined\" showing for null cells when using a Custom Facet with grel:value.type()  ?  Or should those cells show up in the facet as \"null\" ? hmmm...\nUPDATE: my preference would be to show them as \"null\" instead of \"undefined\" in the Facet with grel:value.type() as a user it makes more sense.  But as a developer, the dance of Java type coercion is well understood.  And for OpenRefine Facets, they are first and foremost designed as visual aids for our Users, and not Developers. \n\n. No need to worry.  Folks that want to continue to have surprises in their data and OpenRefine hiding away those surprises can continue to use 2.8 and previous.  If they want less surprises and better Facets that show data inconsistencies, then they can move forward with OpenRefine 3.0+  Those users that move forward will have minimal changes to adapt their workflows.\nThis is one of those features where it will actually help our users, rather than surprise them.  Since we will no longer hide their data inconsistency, but expose that inconsistency fully with these Facet changes...so that they can deal with them and clean it up.\nMerge it.\n(and besides, you could hack together a Python script that updates any older History operations JSON so that it is 3.0 release compatible and offer that script to the world should they need to use it for one of their OpenRefine projects). @ostephens Here's one I found on the net... Look at \"Environmental Area\" or even \"Country and Governance Level\" on the Inventory worksheet.\nWe need to support the \"Column Groups\" selection idea, even during our Importer Preview.  Its on our TODO list in Projects.  Wes' team did extend the data grid to allow \"Column Groups\" he said...we would just need to put work into our Importers. \nETR Instruments Inventory - for publication.xlsx\n. @wetneb just tried.  Has the same error.\n\nInterestingly, in my Git Bash if I do curl -v http://api.crossref.org/works it seems to retrieve.\n. @wetneb refine clean FTW !  ( lolol... I gotta get more sleep...was long night with India team)\nYes it works. Good Job.. @whanley \nAre you able to use the Mac terminal/command line?\nIf you are confident with this, you can do the following:\nOpen a terminal\nChange directory to\n/path/to/OpenRefine.app/Contents/MacOS/\nfor example - if you installed in Applications, the command will be:\ncd /Applications/OpenRefine.app/Contents/MacOS/\nRun OpenRefine by using the command:\n/JavaAppLauncher\nThis should start OpenRefine and in the terminal Window you should see the console/log\nCopy Paste the error in the log that you see into a reply on this issue.. @whanley by chance do you have any other external extensions installed for OpenRefine ?. Ah, that might be because of the older RDF extension that is not 3.0 compatible ?\nCheck out newer one: https://github.com/DTL-FAIRData/orefine-rdf-extension\nUPDATE: Opps ! wrong url, EDITED. @omkarnix Directly in SmartSplitTest.  You are welcome to improve SmartSplitTests.  Take a look at how CoalesceTests.java is performed, where @ostephens did a great job in the test setup.. @wetneb @omkarnix great job guys ! thanks so much for this !. Pages and Pages of existing code and libraries to help already with this:  https://github.com/search?q=smart+quotes. @ostephens This issue would be nice for you to research and hack on when you get bored over the holidays this year. ;-). JVM preferences are many... refer to official Java documentation for those.\nFor OpenRefine preferences...\nUnfortunately, we were not consistent in how we wired up the PreferenceStore.  But if you look in each of these files from this search, you will see many of them, like the listFacet.limit you found:\nhttps://github.com/OpenRefine/OpenRefine/search?p=2&q=getPreferenceStore%28%29&unscoped_q=getPreferenceStore%28%29\nTODO:\n\n[ ] Update our code so that its easy for us to know what so we can pre-populate the PreferencesStore and UI grid and always show the user the preferences available that can be set in the UI. Probably extend this along with updates where PreferenceStore is used. https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/commands/GetAllPreferencesCommand.java\n\nExpressions (Json Array)\n- scripting.starred-expressions (https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/commands/expr/ToggleStarredExpressionCommand.java#L23)\nExample: \"expressions\":[\"code\":\"value.toString()\"]\n- scripting.expressions (https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/commands/expr/LogExpressionCommand.java#L57)\n- globalExpressions (https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/commands/expr/GetExpressionHistoryCommand.java#L81)\nLanguage\n- userLang (Default is UTF-8)\nFacets\n- ui.browsing.listFacet.limit\nGoogle Drive Timeouts\n- googleConnectTimeOut\n- googleReadTimeOut\nExport Template\n- exporters.templating.template\nReconciliation (Json Array)\n- reconciliation.standardServices\nMetadata (Json Array)\n- USER_NAME\n- projectName\n- projectTags\n- title\n- homepage\n- image\n- license\n- encoding\n- userMetadata\n- customMetadataColumns\n- ??? @jackyq2015 will have to document these.\nWikidata (Json Array)\n- \"wikidata_credentials\":[\"username\", \"password\"]\n. Another added today with #1959 \nShow reconciliation previews on hover of reconciliation candidates\n* cell-ui.previewMatchedCells=false. fixed above. @pachamaltese Here's what we were given for SVG files \n_SVG.zip\n. @pachamaltese awesome.  Let us know when you think its fairly \"ready to ship\" and we'll review again, maybe another week or 2 to give you time to play and see if any more improvements can be made?. @Gautamshahi Our documentation for Reconciliation https://github.com/OpenRefine/OpenRefine/wiki/Reconciliation\nI've added a special NOTE into our Wiki page above to highlight this now:\n\"NOTE: OpenRefine Reconciliation does NOT work against a SPARQL endpoint at this time (https://github.com/OpenRefine/OpenRefine/issues/1212), but instead only a Reconciliation Service API.\"\n. @ettorerizza No, we don't want empty cells to behave as null cells in the general case.  We changed that behavior with Owen's work.\nThere is now an expectation by OpenRefine after Owen's change that when you \"Sort by Number\", that the column is full of numbers only.  How do you wish to highlight to the user that \"Hey dude, actually, your column that you asked me to sort is not only numbers...you need to fix that first\" ?\nMy vote would be to have a popup warning and suggest to the user to FIRST run Facet by Null and Facet by Blank to inspect those rows in the column.\nRecall that we also agreed that later we will add an import option (perhaps preference) to \"treat blank cells as null cells\" instead of ramming it down a users throat as we did prior to 3.0 all the way back to 1.0.  But before we add that option, we are waiting for Antonin's work and a nicer UI.  For now, we are kinda just getting by as much as we can...knowing that larger, cleaner, smarter ways of working in OpenRefine are coming.. @wetneb It is useful to have the reasons when classifying a bug or not.  Can you further state your reasoning why you think this is a bug?  Can you refer to my reasons why I classify this as NOT A BUG and state your counter positions ?  This will help us have a fuller understanding of expectations from different points of view.  Jacky, Owen, and Martin might bring further expectations, so let's see where we all stand on this.  Thanks.. I also don't see any changes from Jacky or Owen on isNonBlankData , which the process uses\nhttps://github.com/OpenRefine/OpenRefine/blame/master/main/src/com/google/refine/expr/ExpressionUtils.java#L111a\n@wetneb @ettorerizza I'm also wondering Where this bug was introduced ?\nSince I have no java errors when sorting by number with your given test case in OpenRefine 2.0-r1836\nThe general argument as I see it is this:\n* Do we make a key for sort comparisons or not...when ExpressionUtils.isNonBlankData(value) is false ?\nFrom my eyes, that key was being made just fine in OpenRefine 2.0-r1836 when value was an empty string.. @magdmartin any errors in javascript ?  Can you check the Browser console ?\nWorks fine for me on OpenRefine Win 3.0 Beta and RC1 and Master(trunk) in Firefox and Chrome latest.  Must be something with your project, special special :). It works fine for me on Trunk with Firefox latest.  You might have had an error happening server side ?  do the fetch again and choose the radio button for \"on error\" to \"store error\" instead of the default of \"set to blank\".. @tcbuzor I don't see any changed files on this pull request.  Did you pull from master first ?  This PR looks weird from Github view.  Perhaps you should delete it and create a fresh one ?. You forgot to ask for the value that you want from the cells object.\ncells[\"Serial\"].value\nhttps://github.com/OpenRefine/OpenRefine/wiki/Variables#cells. @wetneb The recon null check is what we wanted, so LGTM for merge. You are probably only looking at the 1st preview stage.... look to the top right corner for the \"Create Project\" button on the preview grid.....then it will take you create the Refine Project and leave you with the main data grid with all your rows (not the preview stage of only 100 rows). Can you provide a small screenshot , if some of the data is not private ?  This is otherwise hard to figure out or see what you are trying to do.  If your data needs to be in some custom format you might look at our Wiki page on Templating Exporter ... https://github.com/OpenRefine/OpenRefine/wiki/Exporters\nWe also have a Custom Templating Exporter.. Ah, you are doing webscraping, and these are webscraping questions actually.  I know the answers but our Github issues are for specific OpenRefine questions.  The right place to ask general web scraping questions is probably on Stackoverflow with tag \"web-scraping\".  https://stackoverflow.com/questions/tagged/web-scraping  Someone there should be able to help you with all your webscraping questions.  (and I would not suggest OpenRefine for doing advanced webscraping like this, its better to use a programming language designed for it, like these https://elitedatascience.com/python-web-scraping-libraries.  Since you are scraping a highly dynamic website that heavily uses Javascript, my suggestion is to use Scrapy with Selenium.  If you want to use a visual tool, then you can run a Docker version of Portia https://github.com/scrapinghub/portia and there are many videos on the internet on using all these tools.)\nBest of luck ! And keep learning !. @bohy Yes, there is a quite a bit of learning for you to do before you can scrape dynamic web pages.  If you are impatient, or unwilling to learn, then I suggest hiring someone through https://www.scrapinghub.com \nI'm sorry but we cannot help you further here.  Good Luck !. @wentianq For Windows users, I've added 2 more things to check in our FAQ about this https://github.com/OpenRefine/OpenRefine/wiki/FAQ#i-am-having-trouble-connecting-to-openrefine-with-my-browser\nBut first, try http://localhost:3333. @wentianq How to change the IP address and Port , as well as many other things , is described in the FAQ, can you scroll up and down on the page and read it a bit ?. @wentianq Anything that works !  Try 8888 or 8181 or 3334\nNote you have to remove the # comment character\nBut the defaults you have are already working... since OpenRefine and the Jetty servlet are serving out the beginnings of a webpage to you from what we see on your screenshot.\nSo the problem will lie somewhere else...\nTry Firefox in Private Incognito mode perhaps ? (eliminating any add-ons). I give up.\nWait, maybe Javascript etc is completely blocked from running on this machine somehow ?\nDoes this page https://jsfiddle.net/boilerplate/jquery show a white HELLO WORLD box in lower right corner ?\n\n. I officially give up.  Perhaps others on the team can help discover the solution for you.. Which library is that @wetneb ?. Done !  This was released as part of OpenRefine 2.7 !  Use the latest OpenRefine version.  Closing this issue now.\n. @ostephens I see.  Then we probably want some good feature name for this, because we will need a menu item name for it.  Hmm.. @wetneb you mean for 3rd party jars right ? https://maven.apache.org/guides/mini/guide-3rd-party-jars-local.html. @wetneb Yes, I will be the tester for Windows.  Right now a few issues in refine.bat file... give me a few minutes.. Hmm, interesting...\nDouble Double Quotes seem to cause various issues depending on style used either on key side or value side ...  double double quotes are only needed when you have to escape special characters.\nbut...\nRunning refine build causes a syntax error:\nC:\\Users\\THAD\\OpenRefine>call \"E:\\Downloads\\apache-maven-3.5.4-bin\\apache-maven-3.5.4\\bin\\mvn.cmd\" \"\"process-resources compile dependency:build-classpath\"\"\nThe syntax of the command is incorrect.\nbut if I replace with single double quotes... this error happens:\n```\nC:\\Users\\THAD\\OpenRefine>call \"E:\\Downloads\\apache-maven-3.5.4-bin\\apache-maven-3.5.4\\bin\\mvn.cmd\" \"process-resources compile dependency:build-classpath\"\n[ERROR] No plugin found for prefix 'process-resources compile dependency' in the current project and in the plugin groups [org.apache.maven.plugins, org.codehaus.mojo] available from the repositories [local (C:\\Users\\THAD.m2\\repository), central (https://repo.maven.apache.org/maven2)] -> [Help 1]\n``. Yeah, the updatedrefine.bat` fixes it... but...\n```\nC:\\Users\\THAD\\OpenRefine>call \"E:\\Downloads\\apache-maven-3.5.4-bin\\apache-maven-3.5.4\\bin\\mvn.cmd\" process-resources compile dependency:build-classpath\n[INFO] Scanning for projects...\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Build Order:\n[INFO]\n[INFO] OpenRefine                                                         [pom]\n[INFO] OpenRefine - main                                                  [jar]\n[INFO] OpenRefine - server                                                [jar]\n[INFO] OpenRefine - extensions                                            [pom]\n[INFO] OpenRefine - Jython extension                                      [jar]\n[INFO] OpenRefine - Wikidata extension                                    [jar]\n[INFO] OpenRefine - Database extension                                    [jar]\n[INFO] OpenRefine - Gdata extension                                       [jar]\n[INFO] OpenRefine - PC-axis extension                                     [jar]\n[INFO]\n[INFO] ---------------------< org.openrefine:openrefine >----------------------\n[INFO] Building OpenRefine 3.0-SNAPSHOT                                   [1/9]\n[INFO] --------------------------------[ pom ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ openrefine ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\server\\src added.\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ openrefine ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\server\\src added.\n[INFO]\n[INFO] --- maven-dependency-plugin:3.1.1:build-classpath (default-cli) @ openrefine ---\n[INFO] No dependencies found.\n[INFO] Dependencies classpath:\n[INFO]\n[INFO] ------------------------< org.openrefine:main >-------------------------\n[INFO] Building OpenRefine - main 3.0-SNAPSHOT                            [2/9]\n[INFO] --------------------------------[ jar ]---------------------------------\n[WARNING] The POM for edu.mit.simile:butterfly:jar:1.0.1 is missing, no dependency information available\n[WARNING] The POM for marc4j:marc4j:jar:2.4 is missing, no dependency information available\n[WARNING] The POM for net.sf.opencsv:opencsv:jar:2.4-SNAPSHOT is missing, no dependency information available\n[WARNING] The POM for edu.mit.simile:vicino:jar:1.1 is missing, no dependency information available\n[WARNING] The POM for io.frictionlessdata:datapackage-java:jar:1.0-SNAPSHOT is missing, no dependency information available\n[WARNING] The POM for io.frictionlessdata:tableschema-java:jar:1.0-SNAPSHOT is missing, no dependency information available\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO]\n[INFO] OpenRefine 3.0-SNAPSHOT ............................ SUCCESS [  0.639 s]\n[INFO] OpenRefine - main .................................. FAILURE [  0.261 s]\n[INFO] OpenRefine - server ................................ SKIPPED\n[INFO] OpenRefine - extensions ............................ SKIPPED\n[INFO] OpenRefine - Jython extension ...................... SKIPPED\n[INFO] OpenRefine - Wikidata extension .................... SKIPPED\n[INFO] OpenRefine - Database extension .................... SKIPPED\n[INFO] OpenRefine - Gdata extension ....................... SKIPPED\n[INFO] OpenRefine - PC-axis extension 3.0-SNAPSHOT ........ SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 1.104 s\n[INFO] Finished at: 2018-09-10T10:15:47-05:00\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal on project main: Could not resolve dependencies for project org.openrefine:main:jar:3.0-SNAPSHOT: The following artifacts could not be resolved: edu.mit.simile:butterfly:jar:1.0.1, marc4j:marc4j:jar:2.4, net.sf.opencsv:opencsv:jar:2.4-SNAPSHOT, edu.mit.simile:vicino:jar:1.1, io.frictionlessdata:datapackage-java:jar:1.0-SNAPSHOT, io.frictionlessdata:tableschema-java:jar:1.0-SNAPSHOT: Failure to find edu.mit.simile:butterfly:jar:1.0.1 in https://repo.maven.apache.org/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced -> [Help 1]\n[ERROR]\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR]\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException\n[ERROR]\n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn  -rf :main\nC:\\Users\\THAD\\OpenRefine>\n.\nC:\\Users\\THAD\\OpenRefine>mvn --version\nApache Maven 3.5.4 (1edded0938998edf8bf061f1ceb3cfdeccf443fe; 2018-06-17T13:33:14-05:00)\nMaven home: E:\\Downloads\\apache-maven-3.5.4-bin\\apache-maven-3.5.4\\bin..\nJava version: 1.8.0_151, vendor: Oracle Corporation, runtime: C:\\Program Files\\Java\\jdk1.8.0_151\\jre\nDefault locale: en_US, platform encoding: Cp1252\nOS name: \"windows 10\", version: \"10.0\", arch: \"amd64\", family: \"windows\"\n```\n\n```\nC:\\Users\\THAD\\OpenRefine>mvn process-resources\n[INFO] Scanning for projects...\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Build Order:\n[INFO]\n[INFO] OpenRefine                                                         [pom]\n[INFO] OpenRefine - main                                                  [jar]\n[INFO] OpenRefine - server                                                [jar]\n[INFO] OpenRefine - extensions                                            [pom]\n[INFO] OpenRefine - Jython extension                                      [jar]\n[INFO] OpenRefine - Wikidata extension                                    [jar]\n[INFO] OpenRefine - Database extension                                    [jar]\n[INFO] OpenRefine - Gdata extension                                       [jar]\n[INFO] OpenRefine - PC-axis extension                                     [jar]\n[INFO]\n[INFO] ---------------------< org.openrefine:openrefine >----------------------\n[INFO] Building OpenRefine 3.0-SNAPSHOT                                   [1/9]\n[INFO] --------------------------------[ pom ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ openrefine ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\server\\src added.\n[INFO]\n[INFO] ------------------------< org.openrefine:main >-------------------------\n[INFO] Building OpenRefine - main 3.0-SNAPSHOT                            [2/9]\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ main ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\main\\src added.\n[INFO]\n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ main ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 489 resources\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-marc4j) @ main ---\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/shared/maven-shared-utils/0.4/maven-shared-utils-0.4.pom\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/shared/maven-shared-utils/0.4/maven-shared-utils-0.4.pom (4.0 kB at 22 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-plugin-api/2.2.1/maven-plugin-api-2.2.1.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-settings/2.2.1/maven-settings-2.2.1.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-profile/2.2.1/maven-profile-2.2.1.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-plugin-registry/2.2.1/maven-plugin-registry-2.2.1.jar\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-project/2.2.1/maven-project-2.2.1.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-plugin-api/2.2.1/maven-plugin-api-2.2.1.jar (12 kB at 134 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-interpolation/1.11/plexus-interpolation-1.11.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-profile/2.2.1/maven-profile-2.2.1.jar (35 kB at 172 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/classworlds/classworlds/1.1-alpha-2/classworlds-1.1-alpha-2.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-interpolation/1.11/plexus-interpolation-1.11.jar (51 kB at 233 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-model/2.2.1/maven-model-2.2.1.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-plugin-registry/2.2.1/maven-plugin-registry-2.2.1.jar (30 kB at 120 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-artifact-manager/2.2.1/maven-artifact-manager-2.2.1.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-project/2.2.1/maven-project-2.2.1.jar (156 kB at 556 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-repository-metadata/2.2.1/maven-repository-metadata-2.2.1.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-settings/2.2.1/maven-settings-2.2.1.jar (49 kB at 148 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/backport-util-concurrent/backport-util-concurrent/3.1/backport-util-concurrent-3.1.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-artifact-manager/2.2.1/maven-artifact-manager-2.2.1.jar (68 kB at 191 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-artifact/2.2.1/maven-artifact-2.2.1.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-model/2.2.1/maven-model-2.2.1.jar (88 kB at 239 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/apache/maven/shared/maven-shared-utils/0.4/maven-shared-utils-0.4.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-repository-metadata/2.2.1/maven-repository-metadata-2.2.1.jar (26 kB at 68 kB/s)\nDownloading from central: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-utils/3.0.15/plexus-utils-3.0.15.jar\nDownloaded from central: https://repo.maven.apache.org/maven2/classworlds/classworlds/1.1-alpha-2/classworlds-1.1-alpha-2.jar (38 kB at 94 kB/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/maven-artifact/2.2.1/maven-artifact-2.2.1.jar (80 kB at 167 kB/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/shared/maven-shared-utils/0.4/maven-shared-utils-0.4.jar (155 kB at 297 kB/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/org/codehaus/plexus/plexus-utils/3.0.15/plexus-utils-3.0.15.jar (239 kB at 411 kB/s)\nDownloaded from central: https://repo.maven.apache.org/maven2/backport-util-concurrent/backport-util-concurrent/3.1/backport-util-concurrent-3.1.jar (332 kB at 562 kB/s)\n[INFO] pom.xml not found in marc4j-2.4.jar\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\main\\webapp\\WEB-INF\\lib\\marc4j-2.4.jar to C:\\Users\\THAD.m2\\repository\\marc4j\\marc4j\\2.4\\marc4j-2.4.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall5772026773249538762.pom to C:\\Users\\THAD.m2\\repository\\marc4j\\marc4j\\2.4\\marc4j-2.4.pom\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-tableschema) @ main ---\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\main\\webapp\\WEB-INF\\lib\\tableschema-java-1.0-SNAPSHOT.jar to C:\\Users\\THAD.m2\\repository\\io\\frictionlessdata\\tableschema-java\\1.0-SNAPSHOT\\tableschema-java-1.0-SNAPSHOT.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall3607150643136041684.pom to C:\\Users\\THAD.m2\\repository\\io\\frictionlessdata\\tableschema-java\\1.0-SNAPSHOT\\tableschema-java-1.0-SNAPSHOT.pom\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-datapackage) @ main ---\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\main\\webapp\\WEB-INF\\lib\\datapackage-java-1.0-SNAPSHOT.jar to C:\\Users\\THAD.m2\\repository\\io\\frictionlessdata\\datapackage-java\\1.0-SNAPSHOT\\datapackage-java-1.0-SNAPSHOT.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall2298769318123207533.pom to C:\\Users\\THAD.m2\\repository\\io\\frictionlessdata\\datapackage-java\\1.0-SNAPSHOT\\datapackage-java-1.0-SNAPSHOT.pom\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-opencsv) @ main ---\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\main\\webapp\\WEB-INF\\lib\\opencsv-2.4-SNAPSHOT.jar to C:\\Users\\THAD.m2\\repository\\net\\sf\\opencsv\\opencsv\\2.4-SNAPSHOT\\opencsv-2.4-SNAPSHOT.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall8574416347435339149.pom to C:\\Users\\THAD.m2\\repository\\net\\sf\\opencsv\\opencsv\\2.4-SNAPSHOT\\opencsv-2.4-SNAPSHOT.pom\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-butterfly) @ main ---\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\main\\webapp\\WEB-INF\\lib\\butterfly-1.0.1.jar to C:\\Users\\THAD.m2\\repository\\edu\\mit\\simile\\butterfly\\1.0.1\\butterfly-1.0.1.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall5703142553009508439.pom to C:\\Users\\THAD.m2\\repository\\edu\\mit\\simile\\butterfly\\1.0.1\\butterfly-1.0.1.pom\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-vicino) @ main ---\n[INFO] pom.xml not found in vicino-1.1.jar\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\main\\webapp\\WEB-INF\\lib\\vicino-1.1.jar to C:\\Users\\THAD.m2\\repository\\edu\\mit\\simile\\vicino\\1.1\\vicino-1.1.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall5006581299332227391.pom to C:\\Users\\THAD.m2\\repository\\edu\\mit\\simile\\vicino\\1.1\\vicino-1.1.pom\n[INFO]\n[INFO] -----------------------< org.openrefine:server >------------------------\n[INFO] Building OpenRefine - server 3.0-SNAPSHOT                          [3/9]\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ server ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\server\\src added.\n[INFO]\n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ server ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 5 resources\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-jdapath) @ server ---\n[INFO] pom.xml not found in jdatapath-alpha2.jar\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\server\\lib\\jdatapath-alpha2.jar to C:\\Users\\THAD.m2\\repository\\com\\codeberry\\jdatapath\\jdatapath\\alpha2\\jdatapath-alpha2.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall724503436912821139.pom to C:\\Users\\THAD.m2\\repository\\com\\codeberry\\jdatapath\\jdatapath\\alpha2\\jdatapath-alpha2.pom\n[INFO]\n[INFO] ---------------------< org.openrefine:extensions >----------------------\n[INFO] Building OpenRefine - extensions 3.0-SNAPSHOT                      [4/9]\n[INFO] --------------------------------[ pom ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ extensions ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\extensions\\server\\src added.\n[INFO]\n[INFO] -----------------------< org.openrefine:jython >------------------------\n[INFO] Building OpenRefine - Jython extension 3.0-SNAPSHOT                [5/9]\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ jython ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\extensions\\jython\\src added.\n[INFO]\n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ jython ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 3 resources\n[INFO]\n[INFO] ----------------------< org.openrefine:wikidata >-----------------------\n[INFO] Building OpenRefine - Wikidata extension 3.0-SNAPSHOT              [6/9]\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ wikidata ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\extensions\\wikidata\\src added.\n[INFO]\n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ wikidata ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 80 resources\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-wdtk-datamodel) @ wikidata ---\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\extensions\\wikidata\\module\\MOD-INF\\lib\\wdtk-datamodel-0.9.0-SNAPSHOT-jar-with-dependencies.jar to C:\\Users\\THAD.m2\\repository\\org\\wikidata\\wdtk\\wdtk-datamodel\\0.9.0-SNAPSHOT\\wdtk-datamodel-0.9.0-SNAPSHOT.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall2542638287304281113.pom to C:\\Users\\THAD.m2\\repository\\org\\wikidata\\wdtk\\wdtk-datamodel\\0.9.0-SNAPSHOT\\wdtk-datamodel-0.9.0-SNAPSHOT.pom\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-wdtk-wikibaseapi) @ wikidata ---\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\extensions\\wikidata\\module\\MOD-INF\\lib\\wdtk-wikibaseapi-0.9.0-SNAPSHOT.jar to C:\\Users\\THAD.m2\\repository\\org\\wikidata\\wdtk\\wdtk-wikibaseapi\\0.9.0-SNAPSHOT\\wdtk-wikibaseapi-0.9.0-SNAPSHOT.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall415162618468242020.pom to C:\\Users\\THAD.m2\\repository\\org\\wikidata\\wdtk\\wdtk-wikibaseapi\\0.9.0-SNAPSHOT\\wdtk-wikibaseapi-0.9.0-SNAPSHOT.pom\n[INFO]\n[INFO] ----------------------< org.openrefine:database >-----------------------\n[INFO] Building OpenRefine - Database extension 3.0-SNAPSHOT              [7/9]\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ database ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\extensions\\database\\src added.\n[INFO]\n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ database ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 29 resources\n[INFO]\n[INFO] ------------------------< org.openrefine:gdata >------------------------\n[INFO] Building OpenRefine - Gdata extension 3.0-SNAPSHOT                 [8/9]\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ gdata ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\extensions\\gdata\\src added.\n[INFO]\n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ gdata ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 10 resources\n[INFO]\n[INFO] -----------------------< org.openrefine:pc-axis >-----------------------\n[INFO] Building OpenRefine - PC-axis extension 3.0-SNAPSHOT               [9/9]\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ pc-axis ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\extensions\\pc-axis\\src added.\n[INFO]\n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ pc-axis ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 2 resources\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO]\n[INFO] OpenRefine 3.0-SNAPSHOT ............................ SUCCESS [  0.162 s]\n[INFO] OpenRefine - main .................................. SUCCESS [  5.272 s]\n[INFO] OpenRefine - server ................................ SUCCESS [  0.081 s]\n[INFO] OpenRefine - extensions ............................ SUCCESS [  0.011 s]\n[INFO] OpenRefine - Jython extension ...................... SUCCESS [  0.038 s]\n[INFO] OpenRefine - Wikidata extension .................... SUCCESS [  0.537 s]\n[INFO] OpenRefine - Database extension .................... SUCCESS [  0.207 s]\n[INFO] OpenRefine - Gdata extension ....................... SUCCESS [  0.085 s]\n[INFO] OpenRefine - PC-axis extension 3.0-SNAPSHOT ........ SUCCESS [  0.031 s]\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 6.554 s\n[INFO] Finished at: 2018-09-10T10:41:55-05:00\n[INFO] ------------------------------------------------------------------------\nC:\\Users\\THAD\\OpenRefine>\n```. Closer !!! \n```\nC:\\Users\\THAD\\OpenRefine>git pull\nremote: Counting objects: 3, done.\nremote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0\nUnpacking objects: 100% (3/3), done.\nFrom git://github.com/OpenRefine/OpenRefine\n   78058911..4bb57952  maven      -> origin/maven\nUpdating 78058911..4bb57952\nFast-forward\n refine.bat | 9 +++++----\n 1 file changed, 5 insertions(+), 4 deletions(-)\nC:\\Users\\THAD\\OpenRefine>refine build\nC:\\Users\\THAD\\OpenRefine>rem Changing this for debugging on Appveyor\nC:\\Users\\THAD\\OpenRefine>rem @echo off\nC:\\Users\\THAD\\OpenRefine>rem\nC:\\Users\\THAD\\OpenRefine>rem Configuration variables\nC:\\Users\\THAD\\OpenRefine>rem\nC:\\Users\\THAD\\OpenRefine>rem MAVEN_HOME\nC:\\Users\\THAD\\OpenRefine>rem   Home of Maven installation; copy is in the source as tools\\apache-ant-*\nC:\\Users\\THAD\\OpenRefine>rem\nC:\\Users\\THAD\\OpenRefine>rem JAVA_HOME\nC:\\Users\\THAD\\OpenRefine>rem   Home of Java installation.\nC:\\Users\\THAD\\OpenRefine>rem\nC:\\Users\\THAD\\OpenRefine>rem JAVA_OPTIONS\nC:\\Users\\THAD\\OpenRefine>rem   Extra options to pass to the JVM\nC:\\Users\\THAD\\OpenRefine>rem\nC:\\Users\\THAD\\OpenRefine>if \"Windows_NT\" == \"Windows_NT\"\nC:\\Users\\THAD\\OpenRefine>if \"Windows_NT\" == \"WINNT\"\nC:\\Users\\THAD\\OpenRefine>rem --- First two utilities for exiting --------------------------------------------\nC:\\Users\\THAD\\OpenRefine>goto endUtils\nC:\\Users\\THAD\\OpenRefine>rem --- Read ini file -----------------------------------------------\nC:\\Users\\THAD\\OpenRefine>set OPTS=\nC:\\Users\\THAD\\OpenRefine>for /F \"tokens=1,* delims==\" %a in (refine.ini) do (set %a=%b )\nC:\\Users\\THAD\\OpenRefine>(set # NOTE: This file is not read if you run the Refine executable directly= )\nC:\\Users\\THAD\\OpenRefine>(set # It is only read of you use the refine shell script or refine.bat= )\nC:\\Users\\THAD\\OpenRefine>(set no_proxy=\"localhost,127.0.0.1\" )\nC:\\Users\\THAD\\OpenRefine>(set #REFINE_PORT=3334 )\nC:\\Users\\THAD\\OpenRefine>(set #REFINE_HOST=127.0.0.1 )\nC:\\Users\\THAD\\OpenRefine>(set #REFINE_WEBAPP=main\\webapp )\nC:\\Users\\THAD\\OpenRefine>(set # Memory and max form size allocations= )\nC:\\Users\\THAD\\OpenRefine>(set #REFINE_MAX_FORM_CONTENT_SIZE=1048576 )\nC:\\Users\\THAD\\OpenRefine>(set REFINE_MEMORY=1400M )\nC:\\Users\\THAD\\OpenRefine>(set # Set initial java heap space (default: 256M) for better performance with large datasets= )\nC:\\Users\\THAD\\OpenRefine>(set REFINE_MIN_MEMORY=1400M )\nC:\\Users\\THAD\\OpenRefine>(set # Some sample configurations. These have no defaults.= )\nC:\\Users\\THAD\\OpenRefine>(set #ANT_HOME=C:\\grefine\\tools\\apache-ant-1.8.1 )\nC:\\Users\\THAD\\OpenRefine>(set #JAVA_HOME=C:\\Program Files\\Java\\jdk1.8.0_151 )\nC:\\Users\\THAD\\OpenRefine>(set #JAVA_OPTIONS=-XX:+UseParallelGC -verbose:gc -Drefine.headless=true )\nC:\\Users\\THAD\\OpenRefine>(set #JAVA_OPTIONS=-Drefine.data_dir=C:\\Users\\user\\AppData\\Roaming\\OpenRefine )\nC:\\Users\\THAD\\OpenRefine>(set # Uncomment to increase autosave period to 60 mins (default: 5 minutes) for better performance of long-lasting transformations= )\nC:\\Users\\THAD\\OpenRefine>(set #REFINE_AUTOSAVE_PERIOD=60 )\nC:\\Users\\THAD\\OpenRefine>rem --- Check JAVA_HOME ---------------------------------------------\nC:\\Users\\THAD\\OpenRefine>if not \"C:\\Program Files\\Java\\jdk1.8.0_151\" == \"\" goto gotJavaHome\nC:\\Users\\THAD\\OpenRefine>rem --- Argument parsing --------------------------------------------\nC:\\Users\\THAD\\OpenRefine>if \"\"build\"\" == \"\"\"\" goto endArgumentParsing\nC:\\Users\\THAD\\OpenRefine>if \"\"build\"\" == \"\"/h\"\" goto usage\nC:\\Users\\THAD\\OpenRefine>if \"\"build\"\" == \"\"/?\"\" goto usage\nC:\\Users\\THAD\\OpenRefine>if \"\"build\"\" == \"\"/p\"\" goto arg-p\nC:\\Users\\THAD\\OpenRefine>if \"\"build\"\" == \"\"/i\"\" goto arg-i\nC:\\Users\\THAD\\OpenRefine>if \"\"build\"\" == \"\"/w\"\" goto arg-w\nC:\\Users\\THAD\\OpenRefine>if \"\"build\"\" == \"\"/d\"\" goto arg-d\nC:\\Users\\THAD\\OpenRefine>if \"\"build\"\" == \"\"/m\"\" goto arg-m\nC:\\Users\\THAD\\OpenRefine>if \"\"build\"\" == \"\"/x\"\" goto arg-x\nC:\\Users\\THAD\\OpenRefine>goto endArgumentParsing\nC:\\Users\\THAD\\OpenRefine>rem --- Fold in Environment Vars --------------------------------------------\nC:\\Users\\THAD\\OpenRefine>if not \"\" == \"\" goto gotJavaOptions\nC:\\Users\\THAD\\OpenRefine>set JAVA_OPTIONS=\nC:\\Users\\THAD\\OpenRefine>set OPTS=\nC:\\Users\\THAD\\OpenRefine>if not \"1400M\" == \"\" goto gotMemory\nC:\\Users\\THAD\\OpenRefine>set OPTS=  -Xms1400M -Xmx1400M -Drefine.memory=1400M\nC:\\Users\\THAD\\OpenRefine>rem --- Check free memory ---------------------------------------------\nC:\\Users\\THAD\\OpenRefine>for /F \"usebackq skip=1 tokens=*\" %i in (wmic os get FreePhysicalMemory | findstr /r /v \"^$\") do @set /A freeRam=%i/1024\nC:\\Users\\THAD\\OpenRefine>echo You have 10202M of free memory.\nYou have 10202M of free memory.\nC:\\Users\\THAD\\OpenRefine>echo Your current configuration is set to use 1400M of memory.\nYour current configuration is set to use 1400M of memory.\nC:\\Users\\THAD\\OpenRefine>echo OpenRefine can run better when given more memory. Read our FAQ on how to allocate more memory here:\nOpenRefine can run better when given more memory. Read our FAQ on how to allocate more memory here:\nC:\\Users\\THAD\\OpenRefine>echo https://github.com/OpenRefine/OpenRefine/wiki/FAQ:-Allocate-More-Memory\nhttps://github.com/OpenRefine/OpenRefine/wiki/FAQ:-Allocate-More-Memory\nC:\\Users\\THAD\\OpenRefine>echo.\nC:\\Users\\THAD\\OpenRefine>if not \"\" == \"\" goto gotMaxFormContentSize\nC:\\Users\\THAD\\OpenRefine>set REFINE_MAX_FORM_CONTENT_SIZE=1048576\nC:\\Users\\THAD\\OpenRefine>set OPTS=  -Xms1400M -Xmx1400M -Drefine.memory=1400M -Drefine.max_form_content_size=1048576\nC:\\Users\\THAD\\OpenRefine>if not \"\" == \"\" goto gotPort\nC:\\Users\\THAD\\OpenRefine>set REFINE_PORT=3333\nC:\\Users\\THAD\\OpenRefine>set OPTS=  -Xms1400M -Xmx1400M -Drefine.memory=1400M -Drefine.max_form_content_size=1048576 -Drefine.port=3333\nC:\\Users\\THAD\\OpenRefine>if not \"\" == \"\" goto gotHost\nC:\\Users\\THAD\\OpenRefine>set REFINE_HOST=127.0.0.1\nC:\\Users\\THAD\\OpenRefine>set OPTS=  -Xms1400M -Xmx1400M -Drefine.memory=1400M -Drefine.max_form_content_size=1048576 -Drefine.port=3333 -Drefine.host=127.0.0.1\nC:\\Users\\THAD\\OpenRefine>if not \"\" == \"\" goto gotWebApp\nC:\\Users\\THAD\\OpenRefine>set REFINE_WEBAPP=main\\webapp\nC:\\Users\\THAD\\OpenRefine>set OPTS=  -Xms1400M -Xmx1400M -Drefine.memory=1400M -Drefine.max_form_content_size=1048576 -Drefine.port=3333 -Drefine.host=127.0.0.1 -Drefine.webapp=main\\webapp\nC:\\Users\\THAD\\OpenRefine>if not \"\" == \"\" goto gotClassesDir\nC:\\Users\\THAD\\OpenRefine>set REFINE_CLASSES_DIR=server\\classes\nC:\\Users\\THAD\\OpenRefine>if not \"\" == \"\" goto gotLibDir\nC:\\Users\\THAD\\OpenRefine>set REFINE_LIB_DIR=server\\lib\nC:\\Users\\THAD\\OpenRefine>rem ----- Respond to the action ----------------------------------------------------------\nC:\\Users\\THAD\\OpenRefine>set ACTION=build\nC:\\Users\\THAD\\OpenRefine>if \"\"build\"\" == \"\"build\"\" goto doMvn\nC:\\Users\\THAD\\OpenRefine>if not \"E:\\Downloads\\apache-maven-3.5.4-bin\\apache-maven-3.5.4\" == \"\" goto gotMvnHome\nC:\\Users\\THAD\\OpenRefine>set MVN_ACTION=\"\"build\"\"\nC:\\Users\\THAD\\OpenRefine>if \"\"build\"\" == \"\"build\"\" set MVN_ACTION=compile dependency:build-classpath\nC:\\Users\\THAD\\OpenRefine>if \"\"build\"\" == \"\"test\"\" set MVN_ACTION=compile test\nC:\\Users\\THAD\\OpenRefine>if \"\"build\"\" == \"\"server_test\"\" set MVN_ACTION=compile test -f main\nC:\\Users\\THAD\\OpenRefine>if \"\"build\"\" == \"\"extensions_test\"\" set MVN_ACTION=compile test -f extensions\nC:\\Users\\THAD\\OpenRefine>call \"E:\\Downloads\\apache-maven-3.5.4-bin\\apache-maven-3.5.4\\bin\\mvn.cmd\" process-resources\n[INFO] Scanning for projects...\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Build Order:\n[INFO]\n[INFO] OpenRefine                                                         [pom]\n[INFO] OpenRefine - main                                                  [jar]\n[INFO] OpenRefine - server                                                [jar]\n[INFO] OpenRefine - extensions                                            [pom]\n[INFO] OpenRefine - Jython extension                                      [jar]\n[INFO] OpenRefine - Wikidata extension                                    [jar]\n[INFO] OpenRefine - Database extension                                    [jar]\n[INFO] OpenRefine - Gdata extension                                       [jar]\n[INFO] OpenRefine - PC-axis extension                                     [jar]\n[INFO]\n[INFO] ---------------------< org.openrefine:openrefine >----------------------\n[INFO] Building OpenRefine 3.0-SNAPSHOT                                   [1/9]\n[INFO] --------------------------------[ pom ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ openrefine ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\server\\src added.\n[INFO]\n[INFO] ------------------------< org.openrefine:main >-------------------------\n[INFO] Building OpenRefine - main 3.0-SNAPSHOT                            [2/9]\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ main ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\main\\src added.\n[INFO]\n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ main ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 489 resources\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-marc4j) @ main ---\n[INFO] pom.xml not found in marc4j-2.4.jar\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\main\\webapp\\WEB-INF\\lib\\marc4j-2.4.jar to C:\\Users\\THAD.m2\\repository\\marc4j\\marc4j\\2.4\\marc4j-2.4.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall5706180299824558122.pom to C:\\Users\\THAD.m2\\repository\\marc4j\\marc4j\\2.4\\marc4j-2.4.pom\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-tableschema) @ main ---\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\main\\webapp\\WEB-INF\\lib\\tableschema-java-1.0-SNAPSHOT.jar to C:\\Users\\THAD.m2\\repository\\io\\frictionlessdata\\tableschema-java\\1.0-SNAPSHOT\\tableschema-java-1.0-SNAPSHOT.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall8664650477005446422.pom to C:\\Users\\THAD.m2\\repository\\io\\frictionlessdata\\tableschema-java\\1.0-SNAPSHOT\\tableschema-java-1.0-SNAPSHOT.pom\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-datapackage) @ main ---\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\main\\webapp\\WEB-INF\\lib\\datapackage-java-1.0-SNAPSHOT.jar to C:\\Users\\THAD.m2\\repository\\io\\frictionlessdata\\datapackage-java\\1.0-SNAPSHOT\\datapackage-java-1.0-SNAPSHOT.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall2994717947431738369.pom to C:\\Users\\THAD.m2\\repository\\io\\frictionlessdata\\datapackage-java\\1.0-SNAPSHOT\\datapackage-java-1.0-SNAPSHOT.pom\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-opencsv) @ main ---\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\main\\webapp\\WEB-INF\\lib\\opencsv-2.4-SNAPSHOT.jar to C:\\Users\\THAD.m2\\repository\\net\\sf\\opencsv\\opencsv\\2.4-SNAPSHOT\\opencsv-2.4-SNAPSHOT.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall290561056679042344.pom to C:\\Users\\THAD.m2\\repository\\net\\sf\\opencsv\\opencsv\\2.4-SNAPSHOT\\opencsv-2.4-SNAPSHOT.pom\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-butterfly) @ main ---\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\main\\webapp\\WEB-INF\\lib\\butterfly-1.0.1.jar to C:\\Users\\THAD.m2\\repository\\edu\\mit\\simile\\butterfly\\1.0.1\\butterfly-1.0.1.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall1228505345306003727.pom to C:\\Users\\THAD.m2\\repository\\edu\\mit\\simile\\butterfly\\1.0.1\\butterfly-1.0.1.pom\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-vicino) @ main ---\n[INFO] pom.xml not found in vicino-1.1.jar\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\main\\webapp\\WEB-INF\\lib\\vicino-1.1.jar to C:\\Users\\THAD.m2\\repository\\edu\\mit\\simile\\vicino\\1.1\\vicino-1.1.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall2774807677129874382.pom to C:\\Users\\THAD.m2\\repository\\edu\\mit\\simile\\vicino\\1.1\\vicino-1.1.pom\n[INFO]\n[INFO] -----------------------< org.openrefine:server >------------------------\n[INFO] Building OpenRefine - server 3.0-SNAPSHOT                          [3/9]\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ server ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\server\\src added.\n[INFO]\n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ server ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 5 resources\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-jdapath) @ server ---\n[INFO] pom.xml not found in jdatapath-alpha2.jar\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\server\\lib\\jdatapath-alpha2.jar to C:\\Users\\THAD.m2\\repository\\com\\codeberry\\jdatapath\\jdatapath\\alpha2\\jdatapath-alpha2.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall5549266587789948809.pom to C:\\Users\\THAD.m2\\repository\\com\\codeberry\\jdatapath\\jdatapath\\alpha2\\jdatapath-alpha2.pom\n[INFO]\n[INFO] ---------------------< org.openrefine:extensions >----------------------\n[INFO] Building OpenRefine - extensions 3.0-SNAPSHOT                      [4/9]\n[INFO] --------------------------------[ pom ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ extensions ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\extensions\\server\\src added.\n[INFO]\n[INFO] -----------------------< org.openrefine:jython >------------------------\n[INFO] Building OpenRefine - Jython extension 3.0-SNAPSHOT                [5/9]\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ jython ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\extensions\\jython\\src added.\n[INFO]\n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ jython ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 3 resources\n[INFO]\n[INFO] ----------------------< org.openrefine:wikidata >-----------------------\n[INFO] Building OpenRefine - Wikidata extension 3.0-SNAPSHOT              [6/9]\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ wikidata ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\extensions\\wikidata\\src added.\n[INFO]\n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ wikidata ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 80 resources\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-wdtk-datamodel) @ wikidata ---\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\extensions\\wikidata\\module\\MOD-INF\\lib\\wdtk-datamodel-0.9.0-SNAPSHOT-jar-with-dependencies.jar to C:\\Users\\THAD.m2\\repository\\org\\wikidata\\wdtk\\wdtk-datamodel\\0.9.0-SNAPSHOT\\wdtk-datamodel-0.9.0-SNAPSHOT.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall8393476059771208918.pom to C:\\Users\\THAD.m2\\repository\\org\\wikidata\\wdtk\\wdtk-datamodel\\0.9.0-SNAPSHOT\\wdtk-datamodel-0.9.0-SNAPSHOT.pom\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-wdtk-wikibaseapi) @ wikidata ---\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\extensions\\wikidata\\module\\MOD-INF\\lib\\wdtk-wikibaseapi-0.9.0-SNAPSHOT.jar to C:\\Users\\THAD.m2\\repository\\org\\wikidata\\wdtk\\wdtk-wikibaseapi\\0.9.0-SNAPSHOT\\wdtk-wikibaseapi-0.9.0-SNAPSHOT.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall2337773162740714664.pom to C:\\Users\\THAD.m2\\repository\\org\\wikidata\\wdtk\\wdtk-wikibaseapi\\0.9.0-SNAPSHOT\\wdtk-wikibaseapi-0.9.0-SNAPSHOT.pom\n[INFO]\n[INFO] ----------------------< org.openrefine:database >-----------------------\n[INFO] Building OpenRefine - Database extension 3.0-SNAPSHOT              [7/9]\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ database ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\extensions\\database\\src added.\n[INFO]\n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ database ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 29 resources\n[INFO]\n[INFO] ------------------------< org.openrefine:gdata >------------------------\n[INFO] Building OpenRefine - Gdata extension 3.0-SNAPSHOT                 [8/9]\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ gdata ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\extensions\\gdata\\src added.\n[INFO]\n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ gdata ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 10 resources\n[INFO]\n[INFO] -----------------------< org.openrefine:pc-axis >-----------------------\n[INFO] Building OpenRefine - PC-axis extension 3.0-SNAPSHOT               [9/9]\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ pc-axis ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\extensions\\pc-axis\\src added.\n[INFO]\n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ pc-axis ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 2 resources\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO]\n[INFO] OpenRefine 3.0-SNAPSHOT ............................ SUCCESS [  0.162 s]\n[INFO] OpenRefine - main .................................. SUCCESS [  0.476 s]\n[INFO] OpenRefine - server ................................ SUCCESS [  0.045 s]\n[INFO] OpenRefine - extensions ............................ SUCCESS [  0.012 s]\n[INFO] OpenRefine - Jython extension ...................... SUCCESS [  0.018 s]\n[INFO] OpenRefine - Wikidata extension .................... SUCCESS [  0.090 s]\n[INFO] OpenRefine - Database extension .................... SUCCESS [  0.022 s]\n[INFO] OpenRefine - Gdata extension ....................... SUCCESS [  0.021 s]\n[INFO] OpenRefine - PC-axis extension 3.0-SNAPSHOT ........ SUCCESS [  0.019 s]\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 0.999 s\n[INFO] Finished at: 2018-09-10T11:01:54-05:00\n[INFO] ------------------------------------------------------------------------\n[INFO] Scanning for projects...\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Build Order:\n[INFO]\n[INFO] OpenRefine                                                         [pom]\n[INFO] OpenRefine - main                                                  [jar]\n[INFO] OpenRefine - server                                                [jar]\n[INFO] OpenRefine - extensions                                            [pom]\n[INFO] OpenRefine - Jython extension                                      [jar]\n[INFO] OpenRefine - Wikidata extension                                    [jar]\n[INFO] OpenRefine - Database extension                                    [jar]\n[INFO] OpenRefine - Gdata extension                                       [jar]\n[INFO] OpenRefine - PC-axis extension                                     [jar]\n[INFO]\n[INFO] ---------------------< org.openrefine:openrefine >----------------------\n[INFO] Building OpenRefine 3.0-SNAPSHOT                                   [1/9]\n[INFO] --------------------------------[ pom ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ openrefine ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\server\\src added.\n[INFO]\n[INFO] --- maven-dependency-plugin:3.1.1:build-classpath (default-cli) @ openrefine ---\n[INFO] No dependencies found.\n[INFO] Dependencies classpath:\n[INFO]\n[INFO] ------------------------< org.openrefine:main >-------------------------\n[INFO] Building OpenRefine - main 3.0-SNAPSHOT                            [2/9]\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ main ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\main\\src added.\n[INFO]\n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ main ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 489 resources\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-marc4j) @ main ---\n[INFO] pom.xml not found in marc4j-2.4.jar\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\main\\webapp\\WEB-INF\\lib\\marc4j-2.4.jar to C:\\Users\\THAD.m2\\repository\\marc4j\\marc4j\\2.4\\marc4j-2.4.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall938561938859035536.pom to C:\\Users\\THAD.m2\\repository\\marc4j\\marc4j\\2.4\\marc4j-2.4.pom\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-tableschema) @ main ---\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\main\\webapp\\WEB-INF\\lib\\tableschema-java-1.0-SNAPSHOT.jar to C:\\Users\\THAD.m2\\repository\\io\\frictionlessdata\\tableschema-java\\1.0-SNAPSHOT\\tableschema-java-1.0-SNAPSHOT.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall7569250432286616716.pom to C:\\Users\\THAD.m2\\repository\\io\\frictionlessdata\\tableschema-java\\1.0-SNAPSHOT\\tableschema-java-1.0-SNAPSHOT.pom\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-datapackage) @ main ---\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\main\\webapp\\WEB-INF\\lib\\datapackage-java-1.0-SNAPSHOT.jar to C:\\Users\\THAD.m2\\repository\\io\\frictionlessdata\\datapackage-java\\1.0-SNAPSHOT\\datapackage-java-1.0-SNAPSHOT.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall3285036541263614878.pom to C:\\Users\\THAD.m2\\repository\\io\\frictionlessdata\\datapackage-java\\1.0-SNAPSHOT\\datapackage-java-1.0-SNAPSHOT.pom\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-opencsv) @ main ---\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\main\\webapp\\WEB-INF\\lib\\opencsv-2.4-SNAPSHOT.jar to C:\\Users\\THAD.m2\\repository\\net\\sf\\opencsv\\opencsv\\2.4-SNAPSHOT\\opencsv-2.4-SNAPSHOT.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall6265124905554513808.pom to C:\\Users\\THAD.m2\\repository\\net\\sf\\opencsv\\opencsv\\2.4-SNAPSHOT\\opencsv-2.4-SNAPSHOT.pom\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-butterfly) @ main ---\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\main\\webapp\\WEB-INF\\lib\\butterfly-1.0.1.jar to C:\\Users\\THAD.m2\\repository\\edu\\mit\\simile\\butterfly\\1.0.1\\butterfly-1.0.1.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall8312986223002378703.pom to C:\\Users\\THAD.m2\\repository\\edu\\mit\\simile\\butterfly\\1.0.1\\butterfly-1.0.1.pom\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-vicino) @ main ---\n[INFO] pom.xml not found in vicino-1.1.jar\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\main\\webapp\\WEB-INF\\lib\\vicino-1.1.jar to C:\\Users\\THAD.m2\\repository\\edu\\mit\\simile\\vicino\\1.1\\vicino-1.1.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall8288299714092558892.pom to C:\\Users\\THAD.m2\\repository\\edu\\mit\\simile\\vicino\\1.1\\vicino-1.1.pom\n[INFO]\n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ main ---\n[INFO] Changes detected - recompiling the module!\n[INFO] Compiling 487 source files to C:\\Users\\THAD\\OpenRefine\\main\\webapp\\WEB-INF\\classes\n[WARNING] /C:/Users/THAD/OpenRefine/main/src/com/google/refine/importing/ImportingUtilities.java: Some input files use or override a deprecated API.\n[WARNING] /C:/Users/THAD/OpenRefine/main/src/com/google/refine/importing/ImportingUtilities.java: Recompile with -Xlint:deprecation for details.\n[WARNING] /C:/Users/THAD/OpenRefine/main/src/com/google/refine/model/Column.java: Some input files use unchecked or unsafe operations.\n[WARNING] /C:/Users/THAD/OpenRefine/main/src/com/google/refine/model/Column.java: Recompile with -Xlint:unchecked for details.\n[INFO]\n[INFO] --- maven-dependency-plugin:3.1.1:build-classpath (default-cli) @ main ---\n[INFO] Skipping plugin execution\n[INFO]\n[INFO] -----------------------< org.openrefine:server >------------------------\n[INFO] Building OpenRefine - server 3.0-SNAPSHOT                          [3/9]\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ server ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\server\\src added.\n[INFO]\n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ server ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 5 resources\n[INFO]\n[INFO] --- maven-install-plugin:2.5.2:install-file (install-jdapath) @ server ---\n[INFO] pom.xml not found in jdatapath-alpha2.jar\n[INFO] Installing C:\\Users\\THAD\\OpenRefine\\server\\lib\\jdatapath-alpha2.jar to C:\\Users\\THAD.m2\\repository\\com\\codeberry\\jdatapath\\jdatapath\\alpha2\\jdatapath-alpha2.jar\n[INFO] Installing C:\\Users\\THAD\\AppData\\Local\\Temp\\mvninstall7990252780181174615.pom to C:\\Users\\THAD.m2\\repository\\com\\codeberry\\jdatapath\\jdatapath\\alpha2\\jdatapath-alpha2.pom\n[INFO]\n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ server ---\n[INFO] Changes detected - recompiling the module!\n[INFO] Compiling 4 source files to C:\\Users\\THAD\\OpenRefine\\server\\classes\n[INFO]\n[INFO] --- maven-dependency-plugin:3.1.1:build-classpath (default-cli) @ server ---\n[INFO] Wrote classpath file 'C:\\Users\\THAD\\OpenRefine\\server\\target\\classpath.txt'.\n[INFO]\n[INFO] ---------------------< org.openrefine:extensions >----------------------\n[INFO] Building OpenRefine - extensions 3.0-SNAPSHOT                      [4/9]\n[INFO] --------------------------------[ pom ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ extensions ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\extensions\\server\\src added.\n[INFO]\n[INFO] --- maven-dependency-plugin:3.1.1:build-classpath (default-cli) @ extensions ---\n[INFO] No dependencies found.\n[INFO] Dependencies classpath:\n[INFO]\n[INFO] -----------------------< org.openrefine:jython >------------------------\n[INFO] Building OpenRefine - Jython extension 3.0-SNAPSHOT                [5/9]\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO]\n[INFO] --- build-helper-maven-plugin:1.8:add-source (default) @ jython ---\n[INFO] Source directory: C:\\Users\\THAD\\OpenRefine\\extensions\\jython\\src added.\n[INFO]\n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ jython ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 3 resources\n[INFO]\n[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ jython ---\n[INFO] Changes detected - recompiling the module!\n[INFO] Compiling 3 source files to C:\\Users\\THAD\\OpenRefine\\extensions\\jython\\module\\MOD-INF\\classes\n[INFO]\n[INFO] --- maven-dependency-plugin:3.1.1:build-classpath (default-cli) @ jython ---\n[INFO] Skipping plugin execution\n[INFO]\n[INFO] ----------------------< org.openrefine:wikidata >-----------------------\n[INFO] Building OpenRefine - Wikidata extension 3.0-SNAPSHOT              [6/9]\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO]\n[INFO] OpenRefine 3.0-SNAPSHOT ............................ SUCCESS [  0.617 s]\n[INFO] OpenRefine - main .................................. SUCCESS [  7.988 s]\n[INFO] OpenRefine - server ................................ SUCCESS [  0.368 s]\n[INFO] OpenRefine - extensions ............................ SUCCESS [  0.019 s]\n[INFO] OpenRefine - Jython extension ...................... SUCCESS [  0.299 s]\n[INFO] OpenRefine - Wikidata extension .................... FAILURE [  0.031 s]\n[INFO] OpenRefine - Database extension .................... SKIPPED\n[INFO] OpenRefine - Gdata extension ....................... SKIPPED\n[INFO] OpenRefine - PC-axis extension 3.0-SNAPSHOT ........ SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 9.527 s\n[INFO] Finished at: 2018-09-10T11:02:05-05:00\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal on project wikidata: Could not resolve dependencies for project org.openrefine:wikidata:jar:3.0-SNAPSHOT: Could not find artifact org.openrefine:main:jar:tests:3.0-SNAPSHOT -> [Help 1]\n[ERROR]\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR]\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException\n[ERROR]\n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn  -rf :wikidata\nC:\\Users\\THAD\\OpenRefine>\n```. Latest Pull.... YEAH !  \n```\n[INFO] OpenRefine 3.0-SNAPSHOT ............................ SUCCESS [  0.643 s]\n[INFO] OpenRefine - main .................................. SUCCESS [  4.227 s]\n[INFO] OpenRefine - server ................................ SUCCESS [  0.173 s]\n[INFO] OpenRefine - extensions ............................ SUCCESS [  0.027 s]\n[INFO] OpenRefine - Jython extension ...................... SUCCESS [  0.457 s]\n[INFO] OpenRefine - Wikidata extension .................... SUCCESS [  2.621 s]\n[INFO] OpenRefine - Database extension .................... SUCCESS [  1.310 s]\n[INFO] OpenRefine - Gdata extension ....................... SUCCESS [  0.951 s]\n[INFO] OpenRefine - PC-axis extension 3.0-SNAPSHOT ........ SUCCESS [  0.309 s]\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 10.925 s\n[INFO] Finished at: 2018-09-10T12:12:54-05:00\n[INFO] ------------------------------------------------------------------------\nC:\\Users\\THAD\\OpenRefine>\n``. Butrefinerunning doesn't work afterrefine build`\n```\nC:\\Users\\THAD\\OpenRefine>for /F \"tokens=*\" %a in ('java -version 2>&1 | find \"version\"') do (set JVERSION=%a )\nC:\\Users\\THAD\\OpenRefine>(set JVERSION=java version \"10.0.2\" 2018-07-17 )\nC:\\Users\\THAD\\OpenRefine>echo Getting Free Ram...\nGetting Free Ram...\nC:\\Users\\THAD\\OpenRefine>wmic os get FreePhysicalMemory\nFreePhysicalMemory\n10484292\nC:\\Users\\THAD\\OpenRefine>for /F \"usebackq skip=1 tokens=*\" %i in (wmic os get FreePhysicalMemory | findstr /r /v \"^$\") do @set /A freeRam=%i/1024\nC:\\Users\\THAD\\OpenRefine>(\necho -----------------------\n echo PROCESSOR_ARCHITECTURE = AMD64\n echo JAVA_HOME = C:\\Program Files\\Java\\jdk1.8.0_151\n echo java -version = java version \"10.0.2\" 2018-07-17\n echo freeRam = 10239M\n echo REFINE_MEMORY = 1400M\n echo -----------------------\n) 1>support.log\nC:\\Users\\THAD\\OpenRefine>set LIB_CLASSPATH=('type server/target/classpath.txt')\nC:\\Users\\THAD\\OpenRefine>set CLASSPATH=\"server\\classes;('type server/target/classpath.txt')*\"\nC:\\Users\\THAD\\OpenRefine>\"C:\\Program Files\\Java\\jdk1.8.0_151\\bin\\java.exe\" -cp \"server\\classes;('type server/target/classpath.txt')*\"   -Xms1400M -Xmx1400M -Drefine.memory=1400M -Drefine.max_form_content_size=1048576 -Drefine.port=3333 -Drefine.host=127.0.0.1 -Drefine.webapp=main\\webapp -Djava.library.path=server\\lib/native/windows com.google.refine.Refine\nError: A JNI error has occurred, please check your installation and try again\nException in thread \"main\" java.lang.NoClassDefFoundError: org/mortbay/jetty/Server\n        at java.lang.Class.getDeclaredMethods0(Native Method)\n        at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)\n        at java.lang.Class.privateGetMethodRecursive(Class.java:3048)\n        at java.lang.Class.getMethod0(Class.java:3018)\n        at java.lang.Class.getMethod(Class.java:1784)\n        at sun.launcher.LauncherHelper.validateMainClass(LauncherHelper.java:544)\n        at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:526)\nCaused by: java.lang.ClassNotFoundException: org.mortbay.jetty.Server\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n        ... 7 more\nC:\\Users\\THAD\\OpenRefine>goto end\nC:\\Users\\THAD\\OpenRefine>\n```. @wetneb migrating to something other than butteryfly was on our long term plans.  Does anything about just doing the packaging into a jar first....make development for Extension authors any harder btw ?\n@jackyq2015 might have some insight into that since he did investigate this lightly at one point sometime last year.\n@weblate I wonder how Apache Sling and other projects deal with dynamic class loading ? https://mvnrepository.com/artifact/org.apache.sling/org.apache.sling.commons.classloader\nUPDATE: Ah, they use OSGi and Apache Felix...hmm, a bit too heavy for us.\nJBoss EAP deals with this via modules.  Butterfly basically inits a lightweight module-like loader.  I wonder if we could use the power of Java 9+ now with module support ?. @wetneb Oh, but we have multiple SLF4J bindings ?  Not needed I think.. @wetneb Also would be nice to config log4j or within Maven settings.xml to guide the location of log4j.properties and log4j.xml , or another method ?  Because I see an annoying WARN when I run test:\n```\n . . .\n[INFO] Running TestSuite\nlog4j:WARN No appenders could be found for logger (FileProjectManager).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\nvalue:4\n. . . \n``. I had the same thoughts about keepingrefine` script as @wetneb for the same reasons, so let's keep it, we'll need it later for various \"start\" things.. And that's why it's marked as Milestone 4.0 :-)\nWe use Milestone's now to show general timelines and priorities of when we think work can be complete by.  Milestones have due dates.  If you click on a Milestone number on the right next to an issue you can see that.  Sometimes we update the due dates on Milestones.  We probably want to push back the 4.0 Milestone due date from January 2019 to perhaps later next summer ?\n. WOW NICE ONE @wetneb  !  You get a virtual bonus cookie today ! \ud83e\udd47 . @xseris This doesn't work for me...\n\nUse clipboard to define data\nwith the following data:\na,b,c\nd,e,f\nChoose CSV/TSV importer\nUse the column names:\n1st,2nd,3rd\nPreview does not update with new column names.\nClick CREATE button and also no column names are renamed from defaults.\n\n. I am OK with having the feature and understand it from a users perspective.\nI also agree with @wetneb that this could be done better in the UI... BUT for now, this can work, and in the future, we can have a dialog to make this prettier.\nTODO:\n\n[ ] Shrink the length of the \"Column names\" input box 50% at least.\n[ ] remove the right side label of \"comma separated\" and instead just relabel above the input box with \"Column names (comma separated)\".\n[ ] Add checkbox before the \"Column names (comma separated)\" as @wetneb suggests that enables/disables against the existing option \"Parse next X lines\".\n. @xseris move the input box BELOW the label (instead of on the right side currently). @xseris does Update Preview not show the renamed columns ?  Could it ?. @xseris I mean, after I type in the comma separated column names that I want into the new input box, I do not see the column names in the preview change.\n\nBut I just did a clean checkout of your branch and tried again and it seems to work.\n( By the way, We are in the process of moving to Maven on our master branch now, and so I cannot locally merge and test against our master without getting a build error because of our /lib folder file duplication where older jars are not getting cleaned because the clean process changed from Ant to Maven.  But you do not have to concern yourself about that on this PR.  Its for us to deal with. )\nAnyways, testing this on your branch, and things are fine and I do see the column names change.  Well done, and Thanks Again !. FYI, yes Thad knew and understood that the Maven conversion would be merged AFTER the 3.0 final release.  But I guess Jacky didn't know or understand our roadmap.  This is for 3.1.  Let me make a milestone for 3.1  We should be using Milestones so that we don't trip up on these major feature releases and so that everyone is clear on what is going into a release or not.\nUPDATE:  I have updated all the Milestones and added approximate due dates that can always change in the future based on our velocity. . I am happy.  That's exactly what I do.. @wetneb On Windows ...\n1.  the refine.bat seems to work fine...but the openrefine.exe does not and throws this error:\nE:\\Downloads\\openrefine-win-3.1-SNAPSHOT\\openrefine-3.1-SNAPSHOT>openrefine.exe\nError: Unable to initialize main class com.google.refine.Refine\nCaused by: java.lang.NoClassDefFoundError: org/mortbay/jetty/Server\nThis seems to be caused by the extra target folder made during packaging ??? that should not have been created... only the \\server\\lib , instead of \\server\\target\\lib\nHere is where I see Jetty jar:\nE:\\Downloads\\openrefine-win-3.1-SNAPSHOT\\openrefine-3.1-SNAPSHOT\\server\\target\\lib\n\nClicking Create Project -> Google Data - \"sign in and authorize\" causes a stack trace:\n\n16:39:40.498 [          org.mortbay.log] Error for /extension/gdata/authorize (69826ms)\njava.lang.NoSuchMethodError: com.google.api.client.auth.oauth2.AuthorizationRequestUrl.<init>(Ljava/lang/String;Ljava/lang/String;Ljava/util/Collection;)V\n        at com.google.api.client.auth.oauth2.AuthorizationCodeRequestUrl.<init>(AuthorizationCodeRequestUrl.java:60)\n        at com.google.api.client.googleapis.auth.oauth2.GoogleAuthorizationCodeRequestUrl.<init>(GoogleAuthorizationCodeRequestUrl.java:100)\n        at com.google.api.client.googleapis.auth.oauth2.GoogleAuthorizationCodeRequestUrl.<init>(GoogleAuthorizationCodeRequestUrl.java:86)\n        at com.google.refine.extension.gdata.GoogleAPIExtension.getAuthorizationUrl(GoogleAPIExtension.java:63)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.mozilla.javascript.MemberBox.invoke(MemberBox.java:126)\n        at org.mozilla.javascript.NativeJavaMethod.call(NativeJavaMethod.java:225)\n        at org.mozilla.javascript.optimizer.OptRuntime.call2(OptRuntime.java:42)\n        at org.mozilla.javascript.gen.file__E__Downloads_openrefine_win_3_1_SNAPSHOT_openrefine_3_1_SNAPSHOT_webapp_extensions_gdata_module_MOD_INF_controller_js_10._c_process_2(file:/E:/Downloads/openrefine-win-3.1-SNAPSHOT/openrefine-3.1-SNAPSHOT/webapp/extensions/gdata/module/MOD-INF/controller.js:101)\n        at org.mozilla.javascript.gen.file__E__Downloads_openrefine_win_3_1_SNAPSHOT_openrefine_3_1_SNAPSHOT_webapp_extensions_gdata_module_MOD_INF_controller_js_10.call(file:/E:/Downloads/openrefine-win-3.1-SNAPSHOT/openrefine-3.1-SNAPSHOT/webapp/extensions/gdata/module/MOD-INF/controller.js)\n        at org.mozilla.javascript.ContextFactory.doTopCall(ContextFactory.java:405)\n        at org.mozilla.javascript.ScriptRuntime.doTopCall(ScriptRuntime.java:3508)\n        at org.mozilla.javascript.gen.file__E__Downloads_openrefine_win_3_1_SNAPSHOT_openrefine_3_1_SNAPSHOT_webapp_extensions_gdata_module_MOD_INF_controller_js_10.call(file:/E:/Downloads/openrefine-win-3.1-SNAPSHOT/openrefine-3.1-SNAPSHOT/webapp/extensions/gdata/module/MOD-INF/controller.js)\n        at edu.mit.simile.butterfly.ButterflyModuleImpl$Controller.process(ButterflyModuleImpl.java:399)\n        at edu.mit.simile.butterfly.ButterflyModuleImpl$Controller.run(ButterflyModuleImpl.java:377)\n        at org.mozilla.javascript.Context.call(Context.java:544)\n        at org.mozilla.javascript.ContextFactory.call(ContextFactory.java:515)\n        at edu.mit.simile.butterfly.ButterflyModuleImpl.processScript(ButterflyModuleImpl.java:650)\n        at edu.mit.simile.butterfly.ButterflyModuleImpl.process(ButterflyModuleImpl.java:427)\n        at edu.mit.simile.butterfly.Butterfly.service(Butterfly.java:516)\n        at com.google.refine.RefineServlet.service(RefineServlet.java:201)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n        at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n        at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:923)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:547)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n        at java.lang.Thread.run(Thread.java:748). @wetneb \nNew SNAPSHOT fixes the  2 issues !\nWhy are we keeping the \\target\\ path ?  That's just a Maven convention.  Why not distribute similarly as we have in repo layout ?  If you feel that keeping \\target\\ as part of distribution simplifies build/test cycles and other things for Developers , then I am OK with that.  I'm just so used to seeing the file layout as we had before, but that's just a personal quirk, and want to make sure it doesn't foul up anyone else.. @wetneb Great, yeah, that's what I figured you were after and the reasons why.  It makes sense to me as a Developer...but not as a user :)  Great job thus far man !. @wetneb Do we have an issue for this already, or is this new ?\n11:28:20.655 [                   refine] POST /command/core/compute-facets (58ms)\n11:28:20.655 [                   refine] GET /command/core/get-models (0ms)\n11:28:20.699 [                   refine] POST /command/core/get-rows (44ms)\n11:28:20.726 [                   refine] POST /command/core/compute-facets (27ms)\n11:28:31.196 [                   refine] GET /command/wikidata/login (10470ms)\n11:28:31.252 [          org.mortbay.log] Error for /command/wikidata/login (56ms)\njava.lang.NoSuchMethodError: com.fasterxml.jackson.core.JsonFactory.requiresPropertyOrdering()Z\n        at com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:571)\n        at com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:480)\n        at org.wikidata.wdtk.wikibaseapi.ApiConnection.<init>(ApiConnection.java:185)\n        at org.wikidata.wdtk.wikibaseapi.ApiConnection.getWikidataApiConnection(ApiConnection.java:205)\n        at org.openrefine.wikidata.editing.ConnectionManager.restoreSavedConnection(ConnectionManager.java:98)\n        at org.openrefine.wikidata.editing.ConnectionManager.<init>(ConnectionManager.java:70)\n        at org.openrefine.wikidata.editing.ConnectionManager.<clinit>(ConnectionManager.java:61)\n        at org.openrefine.wikidata.commands.LoginCommand.doPost(LoginCommand.java:47)\n        at org.openrefine.wikidata.commands.LoginCommand.doGet(LoginCommand.java:75)\n        at com.google.refine.RefineServlet.service(RefineServlet.java:171)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n        at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n        at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:923)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:547)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n        at java.base/java.lang.Thread.run(Unknown Source). Duplicate of #213 . @skyarchers Also, if you run the refine.bat (if you are on Windows) you can attach the support.log to this issue (new in 3.0). Duplicate of #213 (kind of)\nDuplicate of #1737 (kind of). > \n\nMerged ;-) Thanks folks!\nWhen a new feature is added, does someone update the documentation in the wiki ?\n\n@msaby Did you add this feature into our Wiki ? :-). @msaby Add it under this new section I made under Cell Editing, which is what Search and Replace does...edits cells: https://github.com/OpenRefine/OpenRefine/wiki/Cell-Editing#search-and-replace. Sure, thanks @msaby !\nYeah your right... \\d+ seems to be treated differently.\njson\n  {\n    \"op\": \"core/text-transform\",\n    \"description\": \"Text transform on cells in column Column 1 using expression value.replace(/\\\\bfish\\\\b/i,\\\"whale\\\")\",\n    \"engineConfig\": {\n      \"facets\": [],\n      \"mode\": \"row-based\"\n    },\n    \"columnName\": \"Column 1\",\n    \"expression\": \"value.replace(/\\\\bfish\\\\b/i,\\\"whale\\\")\",\n    \"onError\": \"keep-original\",\n    \"repeat\": false,\n    \"repeatCount\": 10\n  },\n  {\n    \"op\": \"core/text-transform\",\n    \"description\": \"Text transform on cells in column Column 1 using expression value.replace(/\\\\\\\\d+/,\\\"999\\\")\",\n    \"engineConfig\": {\n      \"facets\": [],\n      \"mode\": \"row-based\"\n    },\n    \"columnName\": \"Column 1\",\n    \"expression\": \"value.replace(/\\\\\\\\d+/,\\\"999\\\")\",\n    \"onError\": \"keep-original\",\n    \"repeat\": false,\n    \"repeatCount\": 10\n  }. @jackyq2015 interesting...how did you know WHAT to fix based on what was detailed in #1717 ?  We had never had issues before around this and so placing the blame on controller.js seems wrongly placed...but perhaps I am missing what your Root Cause Analysis has discovered... can you share those details ?. Is it only with Chrome or any browser on Windows 10 ?. Dissimilar to #1472 the Remove Rows with empty cells. @wetneb @ostephens could we squeeze this into 3.2 milestone?. @skyarchers We have Windows, Mac, and Linux instructions for giving more memory to OpenRefine on our FAQ page https://github.com/OpenRefine/OpenRefine/wiki/FAQ%3A-Allocate-More-Memory\nIf you have 64bit Java installed, then you can set more than 3072M.  The setting needed all depends on how much memory your computer has freely available after the OS and other applications steal memory away.  My advice is exit all applications except OpenRefine and increase the setting, while restarting OpenRefine and trying it out...if you get out of memory errors in our console log, then you have hit the limit.  Your only choice after trying to increase with settings is to get more RAM installed, or configure and start OpenRefine on compute cluster on a private cloud instance somewhere.\nAnother alternative is to work in batches with this file...perhaps only loading 1/2 of the rows and working with it in 2 OpenRefine project files, instead of 1.  But that might not be ideal for what you are trying to do with your data in OpenRefine.. This looks like its an issue in the DERI RDF extension version you are using.\nThis might be easily fixed if you use the newest version under here ? https://github.com/stkenny/grefine-rdf-extension/releases\nCorrect @stkenny ?. You can see the semi-complete list of extensions on our Downloads page and scroll down (notice we have both versions of RDF extension) http://openrefine.org/download.html#list-of-extensions. @jackyq2015 I don't see why not...that was the whole idea of the command/operations partitioning that David did...I further recall that plugins would be able to use and extend operations... that would be done through the AbstractOperations ?? and the idea was simple in practical terms... core + op  https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/operations/OperationRegistry.java#L74\nQuestion is:  if it is abstracted enough, and can even create/use custom metadata during a plugin's abstracted operations ?\n2nd Question:  Do we allow plugins to customize the Engine as well as the Model ?  I think they can and can provide Engine config, yes?. Well since you didn't ask me... :)  Here's our other TODO's for toDate() ...\ntoDate(\"1541895065\") -> 2018-11-11T00:11:05+00:00 in ISO 8601\n - Reference: #608 - Specifically https://github.com/OpenRefine/OpenRefine/issues/608#issuecomment-437331395\n - Reference: #1122 - documentation improvement (epoch mentioned). @wetneb sorry, yes, I assumed to much.  Closing.. @ostephens Correct, more correctly, Stefano added it for Windmill-based UI testing.  Command used on L259 within...\nhttps://github.com/OpenRefine/OpenRefine/blame/95d0cf546d37dc81c493c9ef09e946a7547cd1ba/refine#L256\nOriginal commit here: https://github.com/OpenRefine/OpenRefine/commit/6dbe794658feae7acda9e29a45315b42ece0fa47\n. @wetneb Seems like I've seen the build fail a few times because of this specific test failing...\nWhere it expects status:running but its getting status:done\nCan you fix that , or perhaps ignore the status check ?\n[ERROR] Tests run: 565, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 67.318 s <<< FAILURE! - in TestSuite\n[ERROR] serializeProcessManager(com.google.refine.tests.process.ProcessManagerTests)  Time elapsed: 0.016 s  <<< FAILURE!\njava.lang.AssertionError: expected:<{\"processes\":[{\"id\":4205572,\"description\":\"some description\",\"immediate\":false,\"status\":\"running\",\"progress\":0}]}> but was:<{\"processes\":[{\"id\":4205572,\"description\":\"some description\",\"immediate\":false,\"status\":\"done\",\"progress\":0}]}>\n    at com.google.refine.tests.process.ProcessManagerTests.serializeProcessManager(ProcessManagerTests.java:27). @vineetharkut Please perform the following and then reply with the output...\nWhile OpenRefine is running...\n1. If you are running the JRE and not the JDK, then stop here, install the JDK 8, and continue.\n2. Start ANOTHER New Command Prompt terminal window, in Cortana search box type cmd & hit ENTER\n3. First let's find the Pid number that OpenRefine is running on... so issue this command.\n    C:\\Users\\THAD>\"C:\\Program Files\\Java\\jdk1.8.0_191\\bin\\jcmd\"\nshould result in something like:\n\n```\nC:\\Users\\THAD>\"C:\\Program Files\\Java\\jdk1.8.0_191\\bin\\jcmd\"\n16064 com.google.refine.Refine\n6512 sun.tools.jcmd.JCmd\n\nC:\\Users\\THAD>\n```\n\n\n\nThen with that Pid number, issue the command again but with VM.system_properties added, for example:\nC:\\Users\\THAD>\"C:\\Program Files\\Java\\jdk1.8.0_191\\bin\\jcmd\" 16064 VM.system_properties\n\n\nCopy and paste the output into your reply to us\n\n\nMy hunch is that in the last week, something you updated or installed has caused the JVM or Python or Path to cause this issue.. Fixed now by merged PR #1814 . ...testing it now.... Oracle's driver also lets you set the cursor options, if you need examples (Coffeetable) https://docs.oracle.com/javase/tutorial/jdbc/basics/retrieving.html#rs_interface. When you fat finger something... \nC:\\Users\\THAD\\OpenRefine>refine m\nC:\\Users\\THAD\\OpenRefine>refine buil\nit will catch the unknown ACTION\nand not just go right to :doRun and start OpenRefine.\nYes tested and use the refine.bat file as shown above in commands, on Windows just the refine implies the .bat execution.  Similar to refine.sh can be launched with ./refine. @mlhale7 Is it 64bit Java on that staff computer ?  Do java -version\nCan you also attach the support.log file in the OpenRefine folder to this issue ?  Thanks.. @mlhale7 I tried your Voices dataset in 2.7, 2.8 and our Trunk version... all of them loaded without problems using Firefox & Chrome latest and refine /m 4096m using jdk1.8.0_151\nYou have 322 columns in this dataset and I can easily slide left to right in our datagrid.  Text Facets also are no problem, where I made 4 on 4 different columns and 2 on the same column.\nMy hunch is that there is not enough memory provided, for your Facets to enjoy a bit of speed.  I see your doing lots of mass edits and sometimes those involve a Facet, and other times you are reordering those 322 columns.. @mlhale7 It is located under the OpenRefine folder itself...where the refine.bat file is located.. @ostephens Shall I test ??. @ostephens You only have 4 digit year tests.  You should have at least this one, since year last is a default for the library, yes/no ?\n\"02-02-01\".toDate()  - > [date 2001-02-02T00:00:00Z]\n\"0002-0222-021\".toDate() -> [date 0222-02-21T00:00:00Z]  <--I think this is technically correct & valid\nI just wanna make sure that java.time doesn't change on us in the future, and those should help let us know.\nBut then this happened...\n\"0002-0222-021\".toDate(\"yyyy-MM-dd\") -> [date 0020-06-19T00:00:00Z]\n\"0002-02-1\".toDate(\"yyyy-MM-dd\") -> \"0002-02-1\".toDate(\"yyyy-MM-dd\")\nAh, because of Gregorian calendar start of 1582 !!\n\"15820211\".toDate(\"yyyyMMdd\",false) -> [date 1582-02-21T00:00:00Z]  <- yes probably\n\"15830211\".toDate(\"yyyyMMdd\",false) -> [date 1583-02-11T00:00:00Z]  <- definitely\nReference: https://docs.oracle.com/javase/8/docs/api/index.html?java/util/GregorianCalendar.html\nBut then there's this...\n\"2018AD\".toDate() -> \"Error: Unable to convert to a date\"\n\"2000 BCE\".toDate() -> \"Error: Unable to convert to a date\"\nReference: https://docs.oracle.com/javase/8/docs/api/java/util/GregorianCalendar.html#BC. @ostephens sorry I just blast everything in 1 comment.  super busy guys.. I just tried on fresh Ubuntu install... works fine.  (@wetneb but note the ${project.version} warning which might need a cleanup ?)\n```\nthad@DESKTOP-TJUMQT0:~/OpenRefine$ ./refine\nYou have 16308M of free memory.\nYour current configuration is set to use 1400M of memory.\nOpenRefine can run better when given more memory. Read our FAQ on how to allocate more memory here:\nhttps://github.com/OpenRefine/OpenRefine/wiki/FAQ:-Allocate-More-Memory\nls: cannot access 'server/target/lib': No such file or directory\n[INFO] Scanning for projects...\n[WARNING]\n[WARNING] Some problems were encountered while building the effective model for org.openrefine:packaging:jar:3.1-SNAPSHOT\n[WARNING] The expression ${version} is deprecated. Please use ${project.version} instead.\n[WARNING] The expression ${version} is deprecated. Please use ${project.version} instead.\n[WARNING] The expression ${version} is deprecated. Please use ${project.version} instead.\n[WARNING]\n[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.\n[WARNING]\n[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.\n[WARNING]\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Build Order:\n[INFO]\n[INFO] OpenRefine                                                         [pom]\n[INFO] OpenRefine - main                                                  [jar]\n[INFO] OpenRefine - server                                                [jar]\n[INFO] OpenRefine - extensions                                            [pom]\n[INFO] OpenRefine - Jython extension                                      [jar]\n[INFO] OpenRefine - Wikidata extension                                    [jar]\n[INFO] OpenRefine - Database extension                                    [jar]\n[INFO] OpenRefine - Gdata extension                                       [jar]\n[INFO] OpenRefine - PC-axis extension                                     [jar]\n[INFO] OpenRefine - packaging                                             [jar]\n[INFO]\n[INFO] ---------------------< org.openrefine:openrefine >----------------------\n[INFO] Building OpenRefine 3.1-SNAPSHOT                                  [1/10]\n[INFO] --------------------------------[ pom ]---------------------------------\nDownloading from central: https://repo.maven.apache.org/maven2/org/codehaus/mojo/build-helper-maven-plugin/1.8/build-helper-maven-plugin-1.8.pom\n[snip][snip]\n``. @ostephens if you are OK with losing work in there...git reset --hard @{u}or you could 1st try not losing workgit reset --soft @{u}`\nref: https://git-scm.com/docs/git-reset#git-reset-emgitresetemltmodegtltcommitgt\nOf course just manually copying your working changed files to another folder...doing the git reset --soft @{u} and then copying the changed files back on top, is usually a good strategy as well.\nMaybe we need to get you git trained. :) you might not be working in the best way with your branching, because you have me worried when you say \"...working from my Git repo - copying the content of the repo\"\nYou  using the Fork & Pull model correctly ? https://guides.github.com/introduction/flow/\nYour master branch also shows 88 commits behind our master, just FYI. https://github.com/ostephens/OpenRefine. To be more clear, this is what is happening...\n\n[\n  {\n    \"op\": \"core/text-transform\",\n    \"description\": \"Text transform on cells in column Column 1 using expression value.replace(\\\"x\\\\\\\\dx\\\",\\\"\\\")\",\n    \"engineConfig\": {\n      \"facets\": [],\n      \"mode\": \"row-based\"\n    },\n    \"columnName\": \"Column 1\",\n    \"expression\": \"value.replace(\\\"x\\\\\\\\dx\\\",\\\"\\\")\",\n    \"onError\": \"keep-original\",\n    \"repeat\": false,\n    \"repeatCount\": 10\n  }\n]\n. You have to put the issue # into the Comments somewhere... not the Title of the PR.  I've just updated your first comment on your PR and now it is linking just fine.. @itsacoderepo Do you recommend a fix such as what Sonarqube did ? https://github.com/SonarSource/sonarqube/commit/08438a2c47112f2fce1e512f6c843c908abed4c7#diff-6d8def68a00bf88a105528765f02fb95\nor another method / library ?. For the case of concatenation in GREL...\nI would expect users want to have more power and a say so in how null is treated in an expression.\nWhich is also why we have Facet option now that treats blanks and nulls individually.\nI think most users, in the case of concatenation in GREL, expect that null should be treated as a blank.\nBut, we could certainly give THEM the choice, and simply introduce a checkbox on the Expression dialog that says \"treat null as blank\" , that way we don't break existing handling.\nBut it is also just as easy to do themselves ?\nvalue + null.toString(). There's also Thad's way with Clojure as documented on our Wiki https://github.com/OpenRefine/OpenRefine/wiki/Recipes#11-clojure. So is the consensus between the two of you to:\n- add an Error field(s) to com.google.refine.model.Recon\n- use customized Facet (for now, which can be moved to a future Recon panel, See Issue #1520 (Thad's design comment) ) to expose Recon Error fields\n- Anything else ?\n. @wetneb I like the idea of separation of concerns.  Longterm, this could show in the new UI work as a view of those Errors and even faceting on them.  Multi-datagrids per project(might still have an issue for that) similar to Excel sheets, and having a split bar between the 2 sheets, ReconData|Errors, so you can Facet and view both sides at the same time.. @wetneb Since we have java.time now ... What about Era and Period ?  Is that important for Wikidata ?\nhttps://docs.oracle.com/javase/8/docs/api/java/time/chrono/Era.html\nhttps://docs.oracle.com/javase/8/docs/api/java/time/Period.html\n. @wetneb ah ok, good to know.. @ostephens This is resolved somehow in 3.1 trunk... I just followed your exact steps and things are fine in FF with Trunk... however you are correct that in 3.1 Beta release, the problem you observed can be reproduced by me as well.  Good news is that this is fixed in 3.1 trunk... but leaving it to you guys to figure out why and how. :-). #468 gathers together all Export Template enhancement requests.. @wetneb sure, then close this if that is the case.. ah the Metadata is also a date related issue.  OK.  I'll let you fix these bugs and test again once you've squashed them.. If we need to call in help here...let me know.  Although you and your fingers have taken us a very long way thus far ;-). > \n\ncan not see the butterfly change. where it is?\n\nHe commited it here: https://github.com/OpenRefine/simile-butterfly/commits/openrefine\nClick the hashes to see the changes.\nJacky, you gotta learn Github more :)\n\n\n. @wetneb Thoroughly impressed with your detective work !  And you didn't even have to ping Stefano !  Well done mate !. @wetneb It might be beneficial to ask the Jackson community if there is any potential workaround for the ObjectMapper, etc. requirements ?  I'm just pulling back and thinking what other communities might have done in this case...perhaps they might have further answers...dunno.\nRegarding Tom's comments, probably based around the concept of ISP, and Antonin's concerns around EngineConfig ... I strongly agree that OpenRefine be the entity that enforces the contract within interfaces, and that means a standard serialization library that we dictate. (and Spring gets it right here, fwiw, imho). In regards to breaking some compatibility with some extensions.  We had already discussed this as a team and voted to break compatibility and then help extension authors to align (even at one point Google mentioned they might help with funding for this).\nIn regards to migrating to a newer framework (maybe Spring), we also discussed this before in fact and even in this issue we see these votes:\n - Antonin \ud83d\udc4d \n - Jacky \ud83d\udc4d  (and he's mentioned this to me before and was OK with Spring, he just thinks maybe easier route to lift up Butterfly to help with compatibility, but forgets we already agree to break some and that he agreed was OK.)\n - Thad \ud83d\udc4d  (who feels Spring brings even more contributors to us through easier understanding of our project, but strongly feels we use it wisely where it makes sense, and avoid abstraction complexity where it does not make sense, keeping our codebase WELL COMMENTED as we go forward)\n - Tom \ud83d\udc4e \n - Owen ~\nSo its clear to me we already have enough consensus to move forward on this issue.\nAlso, as meritocracy means those with merit (the current doers) cast the greater vote, its also very clear we have contributors willing to help with a migration.  Although I note that those with previous merit (Tom) are worried about breaking things, we have already decided on this long ago as OK.\nLet's move forward now.\n. > @thadguidry Can you provide a pointer to where this consensus was built/documented? The link that Antonyn provided basically said \"I'm doing this\" without any discussion, agreement, or disagreement.\nWe have had team calls about this, impromptu chat with Jacky (who was on the fence with React and Spring, and thought React might still have some way to help expose java class to frontend, but needed research) and we documented the high level need based on that on our OpenRefine Phase 2 & 3 enhancement doc\n\nI'll try and find time Thursday to send a note to the dev list where things are more visible. I'd hope that any casual extension developer could search for \"breaking extension incompatibility\" or something similar and find the relevant proposals, discussion, and decision.\nre \"breaking some compatibility with some extensions\" [emphasis added] - what extensions would not be broken? Are there any?\n\nWe agreed that RDF extension was one that we might want to financially/contributory help.  Owen's as well, and he has started opening issues and begun work to move some of his extension functions into our core.  I don't think we agreed on any others however on our calls.\n\nre \"previous merit\" - I'm not familiar with that phrase. Definition please?\n\nYou have not been active, nor contributed in the last 3 years and were non-responsive when specifically sent emails or questions on our lists.  However this is fine, we understand that life sometimes happens and priorities come up, and we had to move on and make decisions at times without input from you.  And now thankfully, we have more contributors than just yourself.  Although it would be ideal if you could find time to contribute and participate more in discussions as you are doing now, which by the way is a contribution in its own right, i.e., taking the time to read issues and giving valuable experience-based evidence/opinion.\nI'd like to see you become an active contributor again, and participate in our team discussions and reviewing code and design proposals.\nDo you maintain an extension for Personal or Government purposes that require some attention ?\n. @wetneb Tom is still a watcher for OpenRefine, (we have 522 watchers shown at top of Github!) https://github.com/OpenRefine/OpenRefine/watchers, and gets all notifications to his email already, he just decided for personal reasons not to participate much in last 3 years.. @wetneb ??? There's no heat.  We're discussing openly, I'm just answering Tom's questions politely about my reasoning that he asked about, nothing more.  Tom and I have worked together on OpenRefine since the beginning, we're online friends, I've even paid him for valuable work.  He's just asking about my rationale, and I'm just responding about our current project status and where he can see the proposal doc and letting him know I'd like him to participate with us more.  I think your getting the wrong impression that there is some squabble when there is none.\n@tfmorris Let me know if there's something we need to change in Github to allow you to watch the project status better, but from my looking at things, you should be all set to get emails already for any issues/commits.  Perhaps you have an email filter on that might be mis-categorizing, dunno.  Let me know.. A little progress...is still some progress. :) Thanks Antonin for moving us forward.. This touches on 2 things also and where many folks often have the same needs... \n1. reusing a recipe for repeated cleaning of the same data structure, but different files with different values, but always same data structure. (CSV, JSON, XML, etc) (we already have but quirky and needs to be improved with our History apply/extract)  This is usually handled programmatically outside the OpenRefine community with things like Python, Pandas, etc.\n\n\nIn your case, you have a need to keep the original data structure but with values changed through data cleaning with OpenRefine.\n\n\nkeeping provenance of the original data file.\n\n\nWe can probably add this automatically by storing the original filepath/filename in the \"About\" link on Open Project page.. Can you give an example of \"internal path\".  This means a lot of different things to different folks.  A Visual example or sketch would help clarify what you are after. . @ostephens I know the current menu structures are awkward, but that's all we have now.\n\n\nGood news is that we hopefully can begin new UI work in 2019.  Patience.  My design idea is to mimic some of Google Spreadsheets, but rather than popup dialogs, make it even more interactive with multi-select columns and using a toggle-able Data Options Panel on the left or right.  I'm considering introducing in our new UI is to switch between Modes and introduce a Data Options Panel, like for Reconciling, Clustering, etc.  As well as a Ribbon of Modes icons, with hover text... similar to Excel and Google spreadsheets and Trifacta Data Wrangler.  A Data Options Panel can be completely customized by Extensions.  Anyways, the idea is that you HIGHLIGHT the columns you want to operate on, and then use the Data Options panel or Expression Editor or other tools to apply to those selected columns...rather than a dropdown button on each and every column as we have now.\nThe idea is to get away from a lot of dropdown menus.  I've tried it out a few times on some React panel examples and feel the flow of OpenRefine operations isn't broken at all but enhanced for the better.\nThe other idea is to get rid of popup dialogs in many places, even for Expression Editor and move it to the top and use our existing data grid for Live Previewing, similar to PGAdmin and other tools . @wetneb They weren't exposed before because Stefano made the decision that other algorithms were not much better for finding \"clusters\".  He mentioned this before on mailing list here. @ostephens what about other importer formats (.xls .ods) and not just .xslx?. @kidehen \nOK, so, my vote, SURE, let's have a way to hook in a Generic JDBC connector.\nOn other Enterprise applications (Pentaho, Talend, etc), I have seen this handled fairly easily with forms that let you specify the .jar file path of the driver you intend to use, along with configuring Options and Advanced settings.\nExample info & pics here: https://help.pentaho.com/Documentation/6.1/0P0/0U0/010/000\nBut we'd need a sponsor to pickup the bill for doing the work, or would welcome contributions !\n. @wetneb Nice one !  and it doesn't look like it took much effort !  Are you also thinking long term about supporting the need for different forms of UI for clustering (params, scopes, etc), as we discussed in other issues ?. > \n\n@thadguidry no, currently it is not possible to pass parameters to custom clustering methods, as the UI is not extensible.\n\nRight, I know, but that's what we want to support long term.  Regardless, I'm confident in that you understand the vision here so that's good, even if we can't do much currently with the UI for extensions yet without more design mocks and discussions.. I think you need to do a git reset --hard @{u} because it seems your local git index is not picking up the merged changes for whatever reason.  We provide the Butterfly 1.0.2 .jar now ... not 1.0.1 which your error is referring to.\nLet us know if the git reset fixes this up for you.. This is a duplicate of #468 \nPlease add any additional details of the features you want to issue #468 so we can track it in one place.. @cottrell Both @jackyq2015 and myself only did some minor bikeshedding of ideas and Parquet was a format that was deemed potentially useful to us.  But we had planned to incorporate support for it by using Apache Beam.  See #1433 and #1468 as well as our Performance Improvements project page https://github.com/OpenRefine/OpenRefine/projects/1\nOur Apache Beam research notes are kept here: https://docs.google.com/document/d/1WT8nCYdNUU14y39IJJlB9fqnxhO0XZnjgHZljEdw-60/edit?usp=sharing\nOf course, you don't have to use Apache Beam at all to import Parquet files, I'm just giving some background information here on what we had planned...to use Beam to handle a lot of this for us.  But it can be done in stages, you are welcome to work on a direct importer for Parquet files with Antonin's feedback.. @browncow5 Thanks, Ryan! BTW, how did you find out about OpenRefine and choosing this issue?. @browncow5 Awesome, \"Good First Issue\" is working!  Yeah, we added some, as suggested by Github.  So I am glad that new feature they added is working!  Thanks for helping and so glad you found us! ;-). hmm, this breaks it on Windows now.\nAh, refine /d doesn't work on Windows ?\nC:\\Users\\THAD\\OpenRefine>refine /d \"C:\\Users\\THAD\\test Workspace A\". I am using refine.bat on Windows within a Windows Command Prompt.  Using this as a test...\nC:\\Users\\THAD\\OpenRefine>refine /d \"C:\\Users\\THAD\\test Workspace A\"\nbut my old workspace still shows up in Refine.. @wetneb it is not a feature of the refine.bat that I personally use.  I was just testing it out.  Sure, you guys can open an issue for the refine.bat to fix the /d option not working as it does on Mac/Linux.  Up to you guys, but it doesn't affect me since I personally don't use that /d option on Windows, but others might want to see it fixed.. @stevevance I'd prefer that we just throw a Warning signal on the SQL Exporter dialog...and then stating a small message just beneath the Content|Download tabs stating something like \"Column names contain invalid chars or reserved words and need to be renamed before continuing\"\nThis is because there are more restrictions than just characters, but whole reserved word lists, such as in PostgreSQL and others. (cannot have a column named BETWEEN)  And for accuracy, you can have spaces and punctuation in Database objects, but often you have to quote the objects all the time for both creation as well as referring to the object.\nSELECT filling, topping, crust FROM \"3.14159\";\nAlso, if we wanted to support highlighting any problematic column names, we would have to find a SQL Rules Library that has all the rules/restrictions for all kinds of databases. Or implement them ourselves (not an option) after we pay ISO a lot of $$$ dollars just to see the rules https://www.iso.org/standard/63565.html\nDoes the warning message sound like a fair tradeoff?. Incidentally, we had decided during design time of the SQL Exporter that it was up to users to get their data in the right order prior to export (which includes names/data). @wetneb Well, then the only way would be going \"semi-safe\"... where someone can write the Identifers conversion routines by following PostgreSQL Lexical Structure documentation as a general guide. Since ISO locks up the standard behind a paywall. https://www.postgresql.org/docs/current/sql-syntax-lexical.html#SQL-SYNTAX-IDENTIFIERS\nFollowing whats in 4.1.1 of that Lexical Structure guide should make it compatible for all databases.. @AndreyR777 Can you attach as a file the full console log?. Andrey says he \"Already helped me run the program. Thank. \"\nSo closing this issue.. @wetneb Not true regarding the columns have to be the same name. We have this documented https://github.com/OpenRefine/OpenRefine/wiki/GREL-Other-Functions#crosscell-c-string-projectname-string-columnname. @psychemedia Care to hack on this?  Just look at the existing connection code in the extensions database folder and have a look at the tests as well.  Ooo, also just noticed the README.md under /database needs to be touched up a bit also since we use Maven now (don't use Ant anymore). @psychemedia You can just use Docker Compose (version 3) to handle this custom networking need (Docker Compose lets you create any number of virtual networks and handles the routing for you (via IP Table rules it automatically creates).\n1. Create 2 networks in the Docker-compose file\n2. Assign 1st service (OpenRefine) to frontend network  (dunno how MyBinder works, but this might also need to be on frontend network as well)\n3. Assign 2nd service (Recon Service) to backend network \nLots of good examples of folks using Docker custom networks floating around on Github to give you ideas also.\nhttps://docs.docker.com/compose/networking/\n. Let's clarify what the OpenRefine values are in the column after import in step 1 and what the import method is.\nthis ?\nwd:Q55830412\nor this ?\nQ55830412\nor something else ?\nBecause when I download from SPARQL query using the \"Download as TSV\", I already get URLs that look like this:\nitem\nhttp://www.wikidata.org/entity/Q55830412\nhttp://www.wikidata.org/entity/Q55830494\nhttp://www.wikidata.org/entity/Q55830502\netc.... I like the idea of a SPARQL importer.\nBut it wouldn't have all the nice features of WDQS online interface, right? (that isn't as powerful)\nHowever, I don't want a lot of overhead in OpenRefine for us to maintain. (that's a worry I have)\nI wonder if we could just use the existing WDQS browser interface in OpenRefine somehow...\nWhat if we flipped things around...and WDQS had a \"Download ID's\" option? or \"Download OpenRefine project\" option?. @wetneb We talked about this in #1666 here: https://github.com/OpenRefine/OpenRefine/pull/1666#issuecomment-421719191\nCan it be reverted and still show users their data inconsistency as I mentioned in my comment?  I don't think so, but open to discuss.  My worry and Owen's was that the previous facet being available does not give users a good view on their messy data.  That's very important to us, that we not hide data issues and I mentioned this in my comment.\nBut I guess your take is that the existing facets may hide data issues, but we'll provide new facets with new behavior.  My worry there is that we stay in the same rut with folks missing data issues by using the old facets and not new facets.  Anything you can do to make the new facets be named the same as old facets (from a display perspective to users) and in the same menu structure would certainly help there I guess.  They get the new facets, but for compatibility reasons we will still have the old facets in some submenu with a different label.\n. @wetneb OK thanks for confirming. Agree to revert and work towards better steps as you identified above.. @wetneb I was hoping that we had a Toggle Preference for \"hover.reconcilePreview\"=\"true\"?  Other than that, I agree the new tab feels natural, but popup reconcile preview feels unnecessary and necessary at different times of the data cleaning/reconciling phases.  So let's see if we can make this a preference to turn on or off as needed.  I wish it was a quick radio button or slider on/off in the interface itself somewhere in Reconcile setup?. We could just add it to the existing \"Project metadata\" dialog.\n//*[@id=\"metadata-body\"]\nBut instead of it being a Key/Value... put the new toggle option right above the Key/Value table.\nLabel the toggle option \"Hover Recon Preview\" and default it on and give it a tooltip to explain it.\nStore the setting as part of the Project metadata for now if we can to simplify.\nWe can always partition it later with\n\"Project metadata\"\n\"Project settings\"\n(or if not too much work, you can add a 2nd storage layer to Projects called \"Settings\" in addition to \"Metadata\"? but I think that's more work, and quick and dirty is fine for now). We do store the zip file name.\nMy zip file was \"texas_grasses.zip\".\nOpenRefine truncates the .zip part, then inserts into the File column.\n\n. Updated comment above.. My error... we don't....anymore ?  hmmm....I think this broke somewhere over the years.\nYeah, @nanobrad is right, we don't store the original Filename.  We used to I'm pretty sure, but perhaps not.  What we store is the folder structure inside the .zip file.\n. @nanobrad Got it.  Yeah, will need to add the Zip filename as a column value and get this working again, if it ever did, cannot recall 100%, I'm getting old :)  Thanks for clarifying!. Super big +1 and for the same reasons you gave.\nThere is also Sonarqube itself, which we use at Ericsson. And its available FREE for Open Source Projects in a hosted cloud platform that already integrates with Github with 1 click setup ... https://sonarcloud.io/about\nYou could try all 3 and then pick the one you most prefer... I am fine with any (since they all use the same backend plugins and db for Java/Javascript projects - cve, findsecbugs, etc.) So the only difference is in features and preferences.\n. @jfeng43 Thanks!  Don't break functionality however.  I'm not sure how or if refineHelperService is even something that's used for context with Reconciliation in general or not.  So you'll need to ask @wetneb about all that before you begin.. I'd also like to see a snippet \"cross lookup failed\" or whatever in the console logs when cross(\"my_project\", \"my_column\") fails. (this will help test/debug later on with new UI). I would aggregate >0 then throw error, but that's fine Antonin.  No log then.\nAlso, seems Project renaming affects this as well...I renamed my project and then now cross() lookups fail even after restarting Refine.  David, why did you hardcoded the tests so much and not use ProjectMetadata?  He probably was rushing to get this to me at the time, lolol.  I really needed cross() to add extra data on a large Freebase upload.. Hi @only1chunts , In addition to what Owen says to check, I wanted to make sure you understood the concepts happening there...we call them Record Rows... rows within a record, which sometimes folks don't understand depending on their background of working in various tools with data.  This is all detailed on our Wiki here: https://github.com/OpenRefine/OpenRefine/wiki/Variables#record. Yes sorry about that.  We fixed this.  Try 3.2 Beta https://github.com/OpenRefine/OpenRefine/releases. So @ostephens there's going to be another PR to address the \"column name not found\" scenario also since you mentioned both project and column in #1976 ?\nCurrently its sorta silent if the column name is not found in a project that is found.... it returns null instead of a nice error to the user like \"NonExistingTestColumn cannot be found in project clipboardCrossTest\".\ncell.cross(\"clipboardCrossTest\", \"NonExistingTestColumn\")\nreturns null\nThanks so much Owen for this so far and looking forward to additional PR's to address the missing column name scenario and more tests! . Why the default of 1 ?. We'd want to keep 127.0.0.1 as our default.  So I'd suspect you can just\nnohup ./refine > openrefine.log 2>&1 &. I saw that also after the revert and agree, let's fail with error on greater than 4 arguments as well as having a test for when there are missing arguments... in other words, when there are arguments 1,2,null,4 it should throw error as we currently do.. I would prefer this be representative of a real LONG.  I've often seen other projects test code use a max java long\n9,223,372,036,854,775,807\nto help uncover data type errors or conversions if needed or introduced in the future.  This will also help bulletproof ProjectManager a bit more as well in the hashmap.\n. Could any of these not also be a Servlet based error potentially as well ?\ncatch (IOException|ServletException e)\nJust trying to harden our tests.. hmm, wondering about maximums here for UserMetadata ... like if they use a client API for filling it instead ?\nComing from telecom, I'm always worried probably too much about maximums and seg faulting :)\n. \ud83d\udc4d . \ud83d\udc4d . It does not, I agree.  I just come from a background where tests are fairly extreme and hit system limits.  OpenRefine is not mil-spec :). Team, we know its best practice, but let's not focus too much on the frontend incidentals.  Our plan is for a UI overhaul with Google's help.  Honestly, we just need to work on the backend and just \"fast quick repair\" on the frontend to make things \"work good enough for now\", knowing that our UI re-work can happen with Google's assistance in a bit.  I already talked to Jacky about this during my hang out with him.  Simon@Google is working on getting a designer or two for helping us out.\nSpeaking of hangouts, we need to come to some regularly scheduled hangouts for the 4 of us.  Oh @ostephens if you didn't know, we kinda voted you as a core contributor in the past day :) and I'll send out an official email shortly, announcing.\n. @wetneb We know.  Jacky is just fixing some incidentals that I bring up in our hangouts.  There just minor typo fixes. I know, I know, it makes the changes look dirty. I know.  But for typos, I am OK with dirty changes of typos within feature pulls.  I know your probably not, but I and Jacky are OK with them.  We're moving fast, because we have an agenda to get this release done, moved out...so that we can actually begin to plan better and focus on \"the 1st quarter fun\".. typo on Display. fix typo. Why are you copyrighting as Google instead of yourself ?  BSD allows you as the author of a source file to retain the copyright.. Yup, this seems to output nicely, regardless.\nLook-behind group does not have an obvious maximum length near index 169 \\n(?<kiloCharacters>[^\\n]{1000})(?<=(?<newLine>\\n)(?<pairsAndText>[^'\\n]{0,1001}|[^\\n']{0,1001}'[^\\n']{0,1001}'[^\\n']{0,1001}){0,1001}(?<oddComa>')(?<text>[^\\n']{0,1001}))(?(?<=')(?!')) ^\n. What was your basis on translating these into friendly messages ?  Did you look to something online to assist with this set of PatternSyntaxExceptions , was it just Regexr.com ?  Are we complete on showing \"something useful\" for all kinds of PatternSyntaxExceptions in your mind ? Not missing any ?. Sure. OK, this is good enough for now.. Rename this with OpenRefine. Rename this with OpenRefine. uhh, you probably want to put your name here and not Google's ?  :). sames. sames. sames. sames. @wetneb He's just following the spec... Title is also typical of an extra field seen from news organizations as well.\nhttps://frictionlessdata.io/specs/table-schema/\n\"name\": \"name of field (e.g. column name)\",\n      \"title\": \"A nicer human readable label or title for the field\",\n. @wetneb I'd rather not impose restriction within our column model for now against Semantic Types ( I know I know, folks will whine and complain, but I'm against this directly in our modeling)  Later we can bring about rules and sets (and extensions can even impose them as well).  That was the long term plan back in the day and is the better way to handle enumerations or restrictions as well as casting.. why are Trys dangerous in Java ?  Sorry, I thought they avoid danger ? :). @wetneb I had thought about just starting with that for now, yes ?  And then expand or let your recon or extensions begin to change its form, say for instance to a drop down of enumerated values to pick from.  Or a changing enumerated list of values depending on the context switch in a particular extension or domain need.. @wetneb this could even be driven by an expression, no ?  Imagine providing some rules via a Python or GREL expression that controls the enumerated values available.  I feel that is very powerful and rather than having Semantic Types being baked in to our core, we allow for expanding domain usage.  This is reminiscent of Wikidata and Schema.org's evolving Types / Properties as well.  In fact, I am on the working group for IoT.Schema.org and there is MUCH debate on mapping between vocabularies regarding various Actions and Controls of Sensors and Systems.  So change here regarding Semantic Types will be a constant.. For some reason when I try to build on Windows it cannot find the /src folder on the tableschema-java path and complains.. @jackyq2015 change that... I don't like this setup... jars preferred.  So what if its a bit of maintenance.  This is more pain than its worth.. this isn't needed.. wait, why is this needed here ?. we could flip this around to avoid an NPE and it will just return false ... but maybe throwing that NPE is better for debugging for now ? https://www.codacy.com/app/OpenRefine/OpenRefine/file/13416576553/issues/source?bid=6129680&fileBranchId=6129680#l285. I don't see this used.. where is this used ?. yeah, my thoughts also, better to not hide in this case.  Also... isn't the name without hypen per the spec ?  \nA Data Package descriptor MUST be a valid JSON object. (JSON is defined in RFC 4627). When available as a file it MUST be named datapackage.json and it MUST be placed in the top-level directory (relative to any other resources provided as part of the data package).. Add a comment about the What.  For instance, something like what you said in the pull request \"This will execute extension tests directly from the main directory, with ./refine extensions_test ...blah\". why not the latest ?  6.10 ?. I'm glad we used Jacoco.  Cobertura SUCKS ASS.. What does this \"broken\" do exactly ? Oh it skips tests classified as broken ? I'm not all to familiar with how testng handles things.\n  . OK. how does it know they have failed before ? it keeps some history or xml in a folder , or ?. ah k. Only Basic Auth for now, right ?. hmm, wondering about Proxy-Authorization as well ? https://developer.mozilla.org/en-US/docs/Web/HTTP/Authentication perhaps a later pull if folks want it ?. I think we need to keep the full expanded URL, like we had before?  That way a local MD file reader and Github views will work just fine.. Ah, now I see, just the empty function call without args...but can we make these translatable ?\n\n. @jackyq2015 Perhaps this should say \"uploaded\" instead of \"exported\" ?   also capitalize \"Google Drive\" since it's a registered trademark.. @jackyq2015 Nice, so we know which method exactly failed.. @jackyq2015 nothing about encoding options ? needed / supported ?. @jackyq2015 hmm, where how are these tokens maintained ? oh wait, are these even tokens , or registered ids with Google Apps somewhere ? I'm just wondering if Tom maintained an account for this stuff or not , and if we should know about this more from him ?. @jackyq2015 Is this timeout value still what Google recommends ?  I know that Apache HTTP has a internal default of 3 minutes timeout that can be overridden in Apache's newer package version.. @jackyq2015 Hmm, should we care about thread safety here ?\nTheir https://developers.google.com/api-client-library/java/google-http-java-client/reference/1.20.0/com/google/api/client/http/apache/package-summary\nis thread safe based on Apache V4,\n versus\nhttps://developers.google.com/api-client-library/java/google-http-java-client/reference/1.20.0/com/google/api/client/http/HttpRequest. @jackyq2015 best to wrap things like this with URL encode as a best practice.. @jackyq2015 let's mention the method also while logging for easier debugging, like you did before. Nevermind, this is not an exception !  pffft, I'm getting sleepy.. @jackyq2015 Why JCommander and not just use Apache StringUtils methods for null conversion where you need it ?. @jackyq2015 Always capitalize proper nouns and registered trademarks, as a best practice. \"Google Drive\". @jackyq2015 maybe add some extra dots, \"uploading...\" ?. I'd like to see all the packages renamed eventually.  Its fine to start renaming with this one.  And it doesn't matter about funding sources, the community owns the packaging and maintenance now for OpenRefine for the past several years since it was given to the community.. The Freebase namespace is still referenced out in the wild.  keep this.. I wouldn't remove it, it useful for debugging.  Instead just log this as DEBUG rather than INFO on slf4j. Is the input always https ?  or can it fall back to http sometimes ?. Remove unused commented code please.. And this will throw an error for those that do not have 8 GB.  Although, its a moot point because the majority of our users do have 8 GB or more, so this new setting is probably fine.  And it probably will solve more user complaints about not loading their datasets which tend to be bigger nowadays by default.  So...\n+1 for this higher memory to help with the current trend and expected users having 8 GB at least nowadays.. https://commons.apache.org/proper/commons-validator/apidocs/org/apache/commons/validator/routines/UrlValidator.html. @ostephens but we could have PER PROJECT Preferences as well as GLOBAL Preferences.\n@jackyq2015 Didn't your model/metadata re-work actually afford that now for us ? or would there be more work needed to support those 2 preference levels ?. You need to perform a \"call\" such as:\ncall %MAVEN_HOME%\\bin\\mvn.cmd %MVN_ACTION%\nmvn.cmd is a Windows batch file and not an executable.\nIn Windows, if one batch file invokes another directly, when the invoked batch file finishes, script run ends. If you want the calling batch file to continue after the invoked batch file ends, you must use the \"call\" command.. Was there some problem with the newer jackson2 client ?  Can you explain the \"unifying\" reason ?. I'd remove the \"finding-options\" and \"replacement-options\" to save space on the dialog window.  It's self evident already they are options.. Also, I would put this under the \"Cluster and edit...\" on 1st menu, instead of under \"Common transforms\". I am thinking we should explain KeyerFactory here... help introduce the high level concepts and what they are used for.  Folks just stepping into this are looking for understanding as this will be our working example clustering extension and if they are not completely familiar with the whole part of our clustering ways of working, having comments will be useful.  Also perhaps have the link for our wiki, where Stefano explained the whole binning, etc.. Similarly, we should introduce with a comment the concept of Keyer and what it is and why its used.  The tests help somewhat, but useful to really explain here about binning keys.. You can drop the word \"Count\".  It's implied as other facets do.  Just say \"Non-Blank values per column\", etc.. ",
    "wetneb": "@thadguidry yes this is unfortunately still current (it is not specific to freebase).. OpenRefine is now available in multiple languages and uses jQuery's i18n features for that.. @tfmorris: Given the issues mentioned above, I went for a different route: parsing directly the source wikicode. It has pros and cons: parsing is easier, but template expansion is not supported so some reconciliation links will be lost.. @thadguidry any updates on this?. @ostephens yes, thanks! I am also closing this to reflect the fact that the migration was done.. I think it is pretty clear now why this should not be done. The ColumnGroup model is a notion that is independent from any knowledge base, while the protograph is specific (Freebase-specific, RDF-specific, or hopefully soon Wikidata-specific).\nPotentially, OverlayModels can be very different from trees of columns (and they will not always have the same metadata). So I propose to close this issue.. This was solved by f5fc44e24e8e5398b1de3cac4f41f2feb70b7924:\nhttps://github.com/OpenRefine/OpenRefine/commit/f5fc44e24e8e5398b1de3cac4f41f2feb70b7924#diff-a6c9ecf8444bfcf85e906bbe5961faeaR48. This notion of common types and properties seems specific to Freebase, closing.. This has been implemented, and the feature is available to standard reconciliation services with the preview endpoint. It is being used (for instance) by OpenCorporates and Wikidata.. Has anybody experienced that recently? I haven't seen this \"timing out\" problem but we definitely need a better error-reporting (if the service returns an HTTP error code, or an invalid payload) but there is already #1128 for that, so I propose to close this one.. Got it! Closing as a duplicate of #1128 then :). @thadguidry I don't see why this bug would still be present, because clearly for Wikidata the search works and does not fall back on Freebase\u2026 And there is nothing specific about Wikidata in the code (except that the service is configured by default). I think we can close this.. @eliohb please report any issue related to the RDF extension here: https://github.com/stkenny/grefine-rdf-extension/issues\nI will keep deleting your posts in this repository as they are off-topic.. This would require inspecting the GREL/Python/... expressions that define each facet, identify the columns it depends on, and produce a new expression with the updated columns. It's a significant amount of work but I can see it being useful down the line for some of our long term projects.. @eswright I suspect it might. I have run into the same problem today actually.. @thadguidry Here is an example dataset:\nhttps://figshare.com/articles/GRID_release_2017-07-12/5203408\n(the file where I observed the problem is \"grid.json\"). @jackyq2015 Here is a much smaller file where the same problem appears: the source file contains eight records, OpenRefine merges some of them and obtains only four.\nhttp://pintoch.ulminfo.fr/49a768ad87/grid_small.json. Hi @jackyq2015. In rows mode all is fine indeed, but when you switch to records mode you will see that some records are merged together.\n\n. @jackyq2015 that's fantastic! It works indeed for my example.\nIt'd be great to have a test for that (I can help if needed).. yes that would be a useful feature to have.. Closing as non-reproducible, this works fine in the current version as far as I can tell.. So, the facets are already stored in the permalink. What needs to be added is only the page size and offset, right?. I don't think this is in the scope of the project. And it's very old.. This is too specific and clearly outside the scope of this software, as argued above.. Related to #1221.. Agreed, closing.. This is currently not possible given the current standard reconciliation API: the type used for the original reconciliation process is not passed to the suggest APIs. This would be worth changing as we make changes to the API.. I believe this also connects with metadata support as we would need column types to export the project as a SQL table. #778 #1096. @tcbuzor the floor is yours!. This should be adapted so that Freebase is not mentioned at all.. \"Triple loader \"and \"MQL write\" export options should just be removed as they were specific to Freebase.. Pickle is not designed for that at all. It has poor interoperability and portability. This would require choosing a Python data structure to represent the tables.. By the way there are also requests for support of reconciliation services with API keys (from OpenCorporates). A PR about this was filed a while back but has not made it to master (#758).\n. I propose to close this as non-reproducible as no test file was provided.. @ostephens @thadguidry thanks for your work on cleaning up these old issues!. Refinery does not exist anymore, closing.. old. Closing just like #324. Closing just like #324. @thadguidry should this (#322) be re-submitted as pull requests?. Yep, I have used this syntax a few times already, for instance in #1213 .. This is solved by aa4517ba5.. @thadguidry Yes, this format is also supported.. I have used the URL fetching capabilities for more than 2000 rows and have not encountered this problem. It was most likely solved by the fix mentioned above. Should we close this?. @ultrageek it would help if you could provide OpenRefine's logs (the white text in the black window) when you stumble on the issue again - I suspect some exception must show up there.. This functionality was moved to standard services and the issue is still valid. However, this does not seem to be something that we should support. This just does not fit in OpenRefine's philosophy: operations modify the table by adding or changing rows and columns, but cannot will not take into account later changes to the data they are relying on. The way to obtain this sort of update is to replay the change (extracted from the history).. This still does not work because they have changed their API.. Solved by the revamp of the google extension by @jackyq2015 . Patch does not apply anymore, closing.. One example of possible generalization of the reconciliation metadata would be to add provenance information to cells (references). This would be useful for #1243.. @thadguidry I can't think of a situation where I encountered something like this - I think we would need a screenshot to see exactly what the issue is. Closing as non reproducible in doubt.. Similarly we could provide the option to un-delete projects which were erroneously deleted.. Sure, the method to change is https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/model/Cell.java#L71 (this is where GREL and Python fields are resolved). A new case for \"error\" must be added there, the value it returns will be available as cell.error for users.\nThe annotations you have noticed have nothing to do with GREL / Python, they are only there for JSON serialization.. @ettorerizza after thinking about @thadguidry's suggestions for reconciliation UI I\u00a0would actually be in favour of a completely different screen setup for reconciliation.\nThe task of going through a list of potential matches and choosing the right is extremely different from  what OR is designed for (apply operations on many rows and drill down with facets).\nDuring a manual reconciliation session:\n- the facets do not change and we do not look at them -> they should not be visible\n- we look at one row/record at a time -> the other rows should not be visible\n- we want to see as much information as possible about the potential matches -> their previews should be visible without any click\nSo, I would be interested in adding a button \"Start reconciliation session\", or something like that, which would take you to a completely different screen, tailored for this task. You would do your matches there, and when you get bored you click \"Back\" and you are back to the normal OR screen.. @thadguidry sure I will look into that. @ettorerizza sure, looking at other columns is super important - I only proposed to hide the other rows.. This seems to be a generic issue of dealing with large projects. Just get more memory and increase heap space.. @eliohb  This ticket is unrelated to your problem. Please use the latest version of OpenRefine (3.1) and the latest version of the RDF extension (1.1.0) from https://github.com/stkenny/grefine-rdf-extension/releases.. @eliohb again, this is not the right place to report issues about the RDF extension. Please use https://groups.google.com/forum/#!forum/openrefine or https://github.com/stkenny/grefine-rdf-extension/issues.\nAny further message that you post in this thread will be deleted.. Closing as a duplicate of #585 since no additional info was provided.. Indeed.. This was specific to Freebase's interface. The new data extension dialog for standard services does not support nested properties and does not handle constraints in the same way.. Duplicate of #926.. Closing as a duplicate of #1221.. This isn't actually a bug, just a performance issue, so removing the tag.. It looks solved, so closing.. Closing as a duplicate of #1214.. @tfmorris could you share the dataset so that any contributor could tackle this bug?. @biofool thanks! closing then.. I think adding rows to the project as an operation is against the philosophy of OpenRefine's data model, because that's not a row-wise (or record-wise) operation. However, the request makes a lot of sense from a practical perspective. It should be possible to apply OpenRefine's operations to a stream of data that is gradually discovered in chunks. That's something to consider in our long term architectural changes.\nSo I prefer approach 2 of Thad's alternative.. Oh okay I misread your proposal. Yeah, I think your two points make a good proposal, although I guess I would not even give the choice to the user to perform 2 or not: 2 should be performed without asking any questions. Otherwise the edit history is going to be quite messy.\nBut again a real solution would be to properly allow running OR operations on streams, and that requires a big backend change.. @stucki1984 I'm closing this as non-reproducible as you have not provided any example data - feel free to reopen if you can find that.. That would be very useful, I have run into this problem multiple times.. @Downchuck what do you mean by \"multi-column record key\"?. I think we should make sure Fill down works as expected in Records mode. I would be in favor of fixing that before 2.9.. @jackyq2015 is this still current?. This issue is about LODrefine, a fork of this project. Please reopen if you can reproduce this with OpenRefine 2.7.. This is fully supported by the Wikidata reconciliation service \\o/\nSee how it works for other languages and alphabets:\nhttp://qiita.com/yayamamo/items/eade3e5788e6f359bce7. Closing as a duplicate of #12.. It looks like this is solved (but the test coverage is still very low). I am proposing #1224 to help solve that.. Fixed by #1856.. So, facets are already taken into account for export, closing.. @rufuspollock We have a first working version of this, just merged. @jackyq2015 has implemented import and export for data packages distributed as Zip files, and also data packages hosted on the web as a JSON file.\nThe various properties of columns (types, constraints, descriptions, \u2026) are not currently exposed in the UI - this will require more work and coordination.\nI also plan to work on import/export for data packages with embedded tabular data. This might require PRs to the java library for data packages.\nBy the way, thanks for setting up https://github.com/frictionlessdata/test-data, this is very useful.. I don't think we have issues for these yet:\n expose column metadata in the UI and let the user edit it  #1726\n design how type validation and other constraint validation (uniqueness, format) should be handled in OpenRefine ; implement it #1727\n* add support for inline data packages (one JSON file storing both metadata and tabular data) #1728. Freebase reconciliation has been shut down.. I'm starting to work on this. I am building a more complete server here: https://github.com/wetneb/openrefine-wikidata. @thadguidry Can we close this issue?. Thanks a lot!!. Freebase suggest does not work anymore, so\u2026. I agree with conclusions 1, 2 and 4 but it is not clear that 3 should be the desired behavior.. @magdmartin yes I understand that this would be the desired result in some cases. But I think there should indeed be a \"coalesce\" method (that would translate all nulls to blanks) so that this is done explicitly.\nFor instance, consider a table where you would have email addresses split in two columns: one for the username (what is on the LHS of the at sign), one for the domain (RHS). If you want to recover email addresses from that with cells[\"username\"].value + \"@\" + cells[\"host\"].value, surely you don't want to have just \"@\" as value when both cells are null: you want that new cell to be null too, I think.. @thadguidry yeah that is fine! but do we have a coalesce function?. @thadguidry no, that syntax is clearly not possible as it would break compositionality. I think the op was thinking more of something like this: \"thad\" + coalesce(null) == \"thad\". In other words, a function that converts null to \"\" and leaves any string untouched.. Closing after years of inactivity and as no tests were provided. Feel free to resubmit once the comments on the PR are addressed.. Closing this stale pull request as the code base has diverged since. Closing stale pull request since the code base has diverged.. No answer from the initiator after three years, closing. That's a bug of OS X, not OpenRefine.. I propose to increase the default maximum form size (still letting users define their own in refine.ini) and close this.. No answer for two years, I think it is reasonable to close that.. I think this PR looks pretty good. It is old but still looks relevant. And it works well for me. Should we merge it?. @eliohb Again, this is not the appropriate place to report issues about the RDF extension. I am deleting your post as promised.. I think it would be simpler to merge this directly in master. This will let users contribute via Weblate. Having a half-finished translation in master does not seem too harmful - users who select this language in their preferences will have English strings when the Hebrew translation is undefined.. I have merged this, the translations can be edited directly there:\nhttps://hosted.weblate.org/projects/openrefine/translations/he/. @answerquest yes! It would be great if you could make a PR for this.. @answerquest Tom is not working on the project anymore.\nCSVWriter is provided by the OpenCSV package (see the import at the top of the file).\nBy the way the version of this library that we use would benefit from being updated to a newer version - that would be my first step, I think.. @Rots do the Dockerfiles proposed above work for you? If so and if you want them to be part of the official repo, it might just be a matter of submitting these as a PR.. @jackyq2015 what is the status of this PR?. I think it would be great to have it as an extension, that would not use CSV files but any OpenRefine project. You could just reconcile a column to any other project in OpenRefine. OpenRefine would expose a reconciliation API on all its projects, and that could also be used by external tools.\nThat is what #176 proposes.. @psychemedia the functionality you were trying to implement should be much easier to set up now that we have a data extension API. The issue for that is #1179 and the protocol draft is here:\nhttps://github.com/OpenRefine/OpenRefine/wiki/Data-Extension-API. It looks like this PR is pretty uncontroversial but still has not been merged after two years\u2026 If no one complains I will merge this in the coming days.. Closing this as it can no longer be merged after removal of the freebase extension.\nA new version of that PR would be welcome, either with MQL or maybe with its more recent successor GraphQL.\nReconciliation services can also expose this functionality directly with https://github.com/OpenRefine/OpenRefine/wiki/Data-Extension-API (see #1179).. @thadguidry you seem to be pretty close to a working prototype of that feature, why don't you propose a PR yourself? I'm sure you can do it!. Closing as duplicate of #612.. Closing as a duplicate of #1135.. For me this is very low priority.. This was solved!. I agree, order for JSON keys is unspecified so the user should not expect them in any particular order. Closing.. @ostephens I don't see any harm in returning null for non-existent fields of non-null objects. Returning null for fields of null objects is less standard. But it's not clear to me in which cases it would cause harm.. Hi @junwei-wang,\nI'd like to merge this but it seems that the repository where the PR was from has been deleted. If you still have the data somewhere, do you think you could file a new PR? The only solution I see for me is to download the files individually and commit them myself, but that would not preserve attribution and it's quite dirty.\nThanks a lot for your help!. @junwei-wang no problem! it should have been merged long ago. Closing then.. Closing as non-reproducible since the user did not provide any sample file.. We decided to just notify about the new version without auto-updating the local copy: #1258. Closing, see #1452.. In a dream world, I would like to be able to use expressions that return arrays, and OR would directly split the array into multiple cells (in records mode). Currently, we first need to join the array with a separator, and then use \"split multi-valued cells\", which is not very clean.. Looking at the latest releases, it seems that the aforementioned 100MB file size limit is no longer true (122MB for Mac). Should we merge this? I think it would be good to have a test or two to demonstrate the cases where that is useful. Maybe something based on #1197?. Closing because of failing build - feel free to rebase and resubmit, ideally with tests.. @danbri not that I'm aware of, but there is some momentum around adding more project metadata in #1221. It would be great to see more progress on both of these issues!. That's very interesting - I do see a lot of value for this in the context of Wikidata integration. This essentially means that we could create a Wikidata overlay model at import time, just based on the tabular metadata (if wikidata properties are provided).. @jackyq2015 as I explained earlier I don't think there is any discussion to be had about \"Data Package versus Table Schema\". These two things just do not do the same thing at all, and it does not make any sense to compare them: it's like discussing \"XML vs XSD\" or \"JSON vs JSON Schema\"\u2026\nSo:\n- OpenRefine's model should be updated to store metadata following the Table Schema specs\n- We need new importers and exporters following the Data Package specs\nIs it any clearer?. I am very confused - we don't seem to be talking about the same thing at all!\nI don't think an importer for Data Packages should be based on the generic JSON importer - they don't do the same thing at all, and basically no code can be shared between the two.. @thadguidry I think this debate between CSVW and Data Packages only makes sense if we come up with concrete integration plans.\n@danbri I've been working on schema alignment for Wikidata (a prototype is available in the wikidata-extension branch of this repository). As you know RDF isn't really a first-class citizen in Wikidata so I have taken the path of not reusing RDF schemas at all. There is currently no way to push some RDF triples directly to Wikidata, and even if there were such a mechanism you don't really want to ask users to do the RDF serialization manually (they know the Wikibase UI, not the RDF serialization specs).\nAs far as I can tell, the sort of RDF schemas CSVW can handle have a particular shape. Quoting https://www.w3.org/TR/2015/REC-csv2rdf-20151217/:\n\nThe CSV to RDF translation is limited to providing one statement, or triple, per column in the table\n\nVirtual columns make it possible to extend that a bit, but this is still far from the level of genericness that is allowed by the (unmaintained) RDF extension for OpenRefine, or the (fledgling) Wikidata extension. So I'm not sure how we can make these things interact. Given a CSVW, we can generate a RDF or Wikibase schema, but for the reverse direction it's less clear.\nIn theory, the \"one column = one property\" mantra would work quite well for the reconciliation API, as each column can be mapped to a property of the reconciliation service. But we need to step back and think about the scenarios in which this integration could work. Intuitively, the user would import in OpenRefine a CSV with CSVW metadata. OpenRefine would store the propertyUrls in its model. When opening the reconciliation dialog (for a reconciliation service compatible with the namespaces used in the CSV), the columns would be automatically matched against the target properties. There are multiple issues with that workflow:\n Reconciliation is only useful when the rows have not been matched by the data producer to the target database. Is it really plausible that the data producer provides a nice CSVW with Wikidata properties in it, without bothering to provide Qids in a column? Or at least some unique ids that Wikidata knows of?\n Reconciliation in OpenRefine is done for a specific column, and the property mappings of the other columns only make sense given that choice of column to reconcile. For instance, if I have a dataset of heritage sites, I might want to reconcile the sites themselves, their administrative location, their country, the organization that runs/owns them, and so on. The \"location\" column of my dataset can be matched to \"located in the administrative territorial entity (P131)\" if I want to reconcile the heritage sites themselves, but not if I want to reconcile the organization that runs them (they might not be headquartered on the site they run).\n* Very often I find myself not mapping a column to a property even if the mapping is legitimate, just because the values I have are unreliable or not suitable for fuzzy matching (e.g.: official website URLs of organizations): a choice of property mappings has to be made with scoring in mind.\nSo, I don't really see any compelling use case that would really motivate support for CSVW metadata in OpenRefine's model. I think it would make a lot of sense for extensions like the RDF extension or the Wikidata extension to provide a best-effort import/export of their schemas to CSVW metadata files, but that does not require any change in OpenRefine itself. I don't really see the case for CSVW import: if you have a CSVW at hand with nice metadata, then most likely you don't even need OpenRefine to clean it up: it is already clean and aligned.\nSo, if mapping columns to RDF properties is not that useful, what about validating columns against particular data types? We need to figure out how we want to integrate these type constraints in OR. One simple way to do that would be to let the user define these types for each column (via a new action in the menu for that column, for instance), choosing from a predefined set of types (integer, string matching a regex, and so on: I assume the specs of CSVW and Data Package roughly agree on this set of types). There could also be a feature to auto-guess the types of all columns (pretty much in the same way the type of a cell can be guessed from its content), provided either as an operation or as an import-time option. Then, we could provide a new facet that would single out the rows where the cell contents don't match the type of a particular column. This would help the user fix these issues via transformations, blanking, etc. Once this is done, the project could be exported as a Data Package / CSVW with the corresponding types reflected in the tabular metadata. Of course we could provide importers for Data Packages and CSVW. I suspect many existing importers could also be adapted to expose the data types they know of in column metadata.\nSo, the case for column datatypes seems a lot clearer to me - I believe it should be possible to provide a streamlined and transparent experience to users, with clear added value: format validation is arguably an important first step before alignment with linked data sources, so providing better core support for that is a real gain.\nI think that kind of makes @rufuspollock's point for Data Packages, because that's exactly the type of metadata that they support. But I really insist: \"CSVW vs Data Packages\" is not the right question to ask. The right question is: what extra data do we want to store for each column in an OpenRefine project, and how is that going to be beneficial to the user. Once this is decided, we can add new importers / exporters and update the existing ones so that they play nicely with these new metadata fields. My answer to that question is: we should clearly store data types, and I don't see any seamless user story backing up the proposal of storing RDF mappings as proposed by CSVW. If you see one, please describe it!. @thadguidry my point is that it's not enough to just say that we want to add support for \"storing semantic types\". We can store a lot of things, but that will only make sense if that is part of a workflow. So you need to come up with particular use cases, describing why storing this sort of metadata will ease these use cases. When are these types going to be set? When are they going to be used? How will that semantic type interact with the reconciliation type that reconciled columns have? And so on.\nBy the way, both CSVW and Data Package can express RDF types for columns. As far as I can tell, the main thing that CSVW has and Data Package does not is mapping columns to RDF properties. So both formats can represent both data types and semantic types.\nAgain, can we please move the discussion away from this OKFN vs W3C battle and focus on discussing actual changes in OpenRefine? @thadguidry can you please describe how you would like to see semantic types associated with columns? Let's get a bit more concrete!. Great! Yeah we are all friends, I know! Good to finally see something more concrete.\nSo, currently, OpenRefine already stores the type used to reconcile a column in the column metadata (together with the reconciliation statistics). This metadata is not exposed at all in the UI (and is not editable), but I have relied on that to implement the data extension API (the property suggestion feature is based on that).\nSo, the question is:\n do we need a second field for the RDF type? Or can we reuse the existing reconciliation type?\n would it make sense to make the current reconciliation type editable? I guess we could reuse the type detection heuristics of the reconciliation dialog and let the user set the type afterwards (from one of the proposed types or to a manual type)\n* does it make sense to store RDF types on columns that are not reconciled? As far as I can tell, the use cases you mention would not really apply to unreconciled columns.\nSo it is more about exposing this existing info better rather than creating a new field\u2026 Not that I'm entirely happy with the way this is currently stored (this is not exposed in ReconConfig but only StandardReconConfig, that should be adapted I think).. @OpenRefine/core I agree with @Opennat, I have run into the same problem myself. I think displaying the last operation at the top makes more sense. Should we revert that before the next release?. @ostephens indeed, only point 2. was solved.\nI remember looking for the change of order and I have not found it either. I think it would be a sensible change, feel free to reopen.. Hmm, I think having value as a default is also sensible, because users normally want to enter expressions that depend on the value. Creating empty rows sounds a bit less common in a data cleaning workflow (I expect you would use them to enter data manually in them?). @thatbudakguy Your DropBox link has expired, any chances we could have a new copy? Ideally, a minimal non-working example would be very useful. The following example works fine for me:\nfoo,bar\n\"hello, world\",4\n\"this, works\"\npretty,well. @ostephens thanks for investigating! let's just close as non-reproducible then.. @zacharynanfelt good catch! It seems to work indeed. We have plenty of other good first issues if you want to get started with contributing to OpenRefine though! Happy to help you find something to your taste if you need ideas.. This was solved by the Maven migration.. Closing stale pull request. This is definitely fixable by redesigning the way GREL is evaluated. I would say that it is not high priority because there are workarounds:\n use another expression language where much more effort has been put in the design (Python, Closure, R one day maybe?)\n use nested if statements to emulate the and\n* in many contexts, we treat exceptions as null anyway. The recent change of evaluating null.field to null instead of throwing an exception also mitigates the need for this.\nAn example use case would be using guard conditions on an index before accessing an array, for instance.\nIn short: designing an expression language requires proper thinking and planning, not something we should be tweaking iteratively as issues arise (especially because users have the right to expect stability in the semantics). So yes, GREL is a toy language that is quite limited, but I don't think we should spend too much energy on fixing that: the long term solution is to use existing languages (and it makes onboarding of new users a lot easier, too).. Closing as a duplicate of #1017.. @juanmas07 I don't understand your last line - where do you see for=\"guess-407969\" and why should it be different?. No response, closing as not reproducible.. @ignacio-chiazzo can you provide more detail - what operation are you doing, what data are you working on? can you share the logs?. Looks like the user is happy with the proposed workaround.. @debpaul Thanks for the bug report. What file did you use to create a project? What happens exactly when you try to create the project? Do you get an error message?. Closing as non-reproducible.. Any idea if this is still current after the new cross function?. I've made a different issue for that as it would have implications beyond the cross function.. Fixed by #1852.. @rjakshayjain13 You should rather report this issue at https://github.com/fadmaa/grefine-rdf-extension , this is the repository for the core functionality of OpenRefine.. @thadguidry I've made a simpler version, where the settings of the endpoint are not hardcoded but fetched if no other interface is present. This seems to be more in line with the previous state of the code.\nThe only downside is that people upgrading to the new OpenRefine will not see the new interface if they have already added other reconciliation interfaces. And also, when changing OpenRefine's language, one needs to delete the existing service to see the new one with the updated language.. Magnus does deserve a lot of bounties for his amazing work, I've reached out to him about that.. Actually it wasn't that hard to test so you get that too!. @jackyq2015 I've moved to Guava's cache features: https://github.com/google/guava/wiki/CachesExplained\nThis does not add any dependency and looks like a good compromise between simplicity and scalability.\nThe maximum cache size and invalidation delay are currently hard-coded, is there any sensible place where they should be defined?. Yeah I guess we don't want to lose ground in the random.org community either! What's the point of having a power tool to work with messy data if there's no entropy to play with\u2026. I've added the checkbox to the dialog to create the column, because there was some empty space where it fits nicely, I think. Whether the cache is enabled or not is stored in the JSON of the operation.\nThe code is retro-compatible with JSON payloads generated by previous versions: if the parameter is not present, we assume that there should not be any caching (because that's how it was before). Users can upgrade their scripts by adding \"cacheResponses\": true to their payloads.. @thadguidry yeah the timeout for my test should be changed to a larger value - or make the test more clever. It can usually be solved by re-running the build. I'll fix that asap.. @codeforkjeff Yes, we need to design the API for essentially two things:\n * Retrieving the list of properties that can be fetched from a reconciled column. This could take as parameter the type we reconciled the column against, and/or a few sample reconciled identifiers. In the case of Wikidata, the proposed property list will never be complete, so we need to let users set their own properties using the existing suggest features.\n * Retrieving the column itself. One HTTP request should cover a batch of rows.\nI think these new functionalities should be exposed in two new endpoints that the service metadata would point to.\nAbout discussing things, how about the OpenRefine mailing list? (I am also happy with this GitHub issue). @codeforkjeff @magdmartin As announced on the mailing list, I have drafted an API specification here:\nhttps://github.com/OpenRefine/OpenRefine/wiki/Data-Extension-API\nComments welcome!. @codeforkjeff that is definitely a copy and paste error, sorry about that!. @thadguidry GraphQL is fantastic, but as I explained on the mailing list, I don't think it would be a good choice for this protocol. Here are the reasons. (Some of them are new!)\n\n\nThe reconciliation API already uses a JSON-based format (not sure we can really call that REST), so it is more consistent to extend it in a consistent way with the rest (no pun intended). Moreover, we need other endpoints such as the one for property proposal: these are outside the scope of GraphQL (or at least it would be quite weird to construe them in this language, I think).\n\n\nGraphQL is hard to implement for service providers: although quite a few bindings are provided for various languages, this is quite a heavy machinery to plug to reconciliation services, which are usually fairly simple services.\n\n\nGraphQL makes it a lot harder to ensure that batches of queries will be evaluated efficiently by the service. With the current API, if you run a reconciliation service from a SQL database (for instance), it is straightforward to convert a data extension query to a single efficient SQL query that will return all the results you need. If you had to write a full GraphQL server, good luck with that. (For instance, in Python, the graphene library (the standard GraphQL binding for Python), they use aiodataloader to ensure that, which requires the fancy native asynchronous io capabilites of Python 3.5. That's not widely available.)\n\n\nRequiring services to implement GraphQL would go against the goal of presenting users only a very simple interface that does not expose the query language. Sure, we would be able to build such an interface, but it would be frustrating for the service providers to comply with such a rich API and to be only called with a very particular form of query (which happens to be hard to evaluate efficiently).\n\n\nSure, GraphQL \"eliminates extra round trips\", but so does this API draft! And similarly, the current API also \"queries for the exact data that you need\" and nothing else. So using GraphQL would not bring any efficiency at all (in fact, as explained above, it would make it harder for data providers to evaluate queries efficiently).\n\n\nIt would also introduce an asymmetry between the notion of properties the reconciliation API currently uses (an ID and a name), and the fields GraphQL uses (just an ID). That's probably not too big a deal but it's just the tip of the iceberg: I'm sure if you try to actually implement what you are proposing you will run into other discrepancies like that, and as you cannot tweak the way GraphQL works, it is going to feel awkward for both the service provider and the user.\n\n\nAnyway: I think GraphQL looks very promising and it would be fantastic to have support for it in OpenRefine (just like SQL, SPARQL, and probably many others I am not aware of). The reconciliation API is just not the right place for that, as far as I can tell.\nBut enough said, I think I'll just wait for @codeforkjeff's feedback on this as he has had a look at the current draft.. @codeforkjeff: friendly ping! We are thinking about making a release in a few weeks, so the API specs will be harder to change after that.. I have reached out to various organizations who run reconciliation services for feedback and no issue was raised as to these specifications. I will therefore consider that this data extension API is good to go.. @thadguidry I don't think I'll have much time for that, it's more to keep track of what's being said on the mailing list.. This issue is a duplicate of #218, sorry.. @jyf1997 please report your problem to the mailing list:\nhttps://groups.google.com/forum/#!forum/openrefine. @ostephens I agree with your analysis: if a JSON payload is already returned in case of error, I think the best solution would be to add client-side error handling. I'm not sure why JSONP vs JSON is relevant for that though.. Nice, that was quick! If you have time to write one or two unit tests that would be fantastic too!\nAlso, what happens if cross gets something else than just a single literal value?. @jackyq2015 So, if an existing transformation script assumes that the cross function expects a cell, and the new version of that function does not accept cells anymore, this transformation script that worked for OpenRefine 2.7 will not work for a later version. (That is how I understand the problem). It might be worth adding open-refine.log to .gitignore to prevent this :). @charmygarg thanks a lot! it looks good to me.. @magdmartin it seems quite hard to make something that will work for everything directly\u2026 I suppose DERI's RDF export tool is the closest approximation to that. But that would not work very well for Wikidata.\nThe problem is that all these databases have different data models (not just schemas). Typically the notions of qualifier, references and ranks are quite specific to Wikibase.\nBut I agree GOKB looks great, thanks for sharing!. @ettorerizza you said on the ML that it is clearly a bug - do you know if there is any documentation anywhere explaining what this function is really supposed to do? From my understanding after reading the source code, it seems that this function just expects the initial data to be ordered by final row in the result, as follows:\nColumn1;Column2\nSourceFile1;2\nSourceFile2;3\nSourceFile3;-1\nSourceFile1;3\nSourceFile2;4\nSourceFile3;6\nSourceFile1;-3\nSourceFile2;4\nSourceFile3;1\n. Also pinging @ostephens who took part to the discussion on the mailing list.. @ostephens I agree it is confusing! (And I did understand that you were not sure it was a bug - my first comment was for @ettorerizza.) Here is how I understand the logic of the code.\nIt wants to support input cases like this one:\nKey,Value\nmerchant,Katie\nfruit,apple\nprice,1.2\nfruit,pear\nprice,1.5\nmerchant,John\nfruit,banana\nprice,3.1\nwhich is (correctly) transposed to\nmerchant,fruit,price\nKatie,apple,1.2\n,pear,1.5\nJohn,banana,3.1\n(We insert a blank cell in the merchant column to create a record.)\nTo support this kind of input, the code needs to assume that the first column it encounters (here, SourceFile1) is the primary key. Then, for every other (key,value) pair (the ones that are not primary keys), OpenRefine assigns the pair to the last primary key it has encountered. This implies that the function is order-sensitive and explains the output @belm104 obtained.\nAs far as I can tell, this is a reasonable assumption. The input format @belm104 had to deal with seems rather uncommon. There could potentially be an option to switch between the two behaviours, but I suspect the current behaviour is useful for some users too (including myself actually)\u2026\n. Calling that an \"anti-pattern\" might be a bit strong: OpenRefine users rarely get to choose the dirty formats they have to deal with, I think. So all dirty formats are legitimate: some of them are just not covered by OpenRefine's cleaning capabilities.\n@thadguidry it would be wonderful if you had some time to improve the docs on this subject! After all, we cannot blame @belm104 for reporting a non-bug, since the function does not seem to be documented anywhere\u2026. @thadguidry Well she just wants to have her rows in the expected form she gave us, because she knows it is the shape that makes sense for her dataset\u2026 That is a legitimate need! This function just cannot do that at the moment. And 2, 3 and -3 are indeed related because of their relative position to the first row with the same key. It's legitimate to expect something like OpenRefine to pick that up\u2026\nI was saying that \"anti-pattern\" sounds a bit too strong to me because in the ears of a coder it almost sounds like an insult but I am sure you did not mean that!! :-). @thadguidry I think we all understand what is going on here now, my comment was just about language, really! :-) Just want to make sure our users feel welcome here and keep filing such good bug reports.. @ostephens  Yes, as explained above, this function requires indeed its input to be listed in a particular order.\nThe code assumes that the first key it encounters (here, Key 1) is the primary key in the resulting table (in other words, the first column). Then, for every other (key,value) pair (the ones with Key 2 and Key 3), OpenRefine assigns the key-value pair to the last primary key it has encountered.\nSo, let us do manually what the algorithm does. \nRead the initial table sequentially:\n- Read (Key 1, Value 1). As this is the first row we are reading, we deduce that Key 1 is going to be the primary key in our new table. We create a row containing Value 1 in the newly-created Key 1 column.\n- Read (Key 1, Value 4). This is a new value for the primary key. We deduce that the record for the primary key previously seen (Value 1) is complete, so we leave the first row as it is (with only a value for Key 1) and create a new row containing Value 4 in the Key 1 column.\n- Read (Key 2, Value 2). We create a new column for Key 2 and insert Value 2 at the position of the primary key that we have seen last (Value 4).\n- We keep doing that for the other values: they are all assigned to the last primary key seen, Value 4.\nTo understand why such an algorithm is desirable, you can run it on my merchant-fruit-price example above.\nSo, yes, it is normal that changing the order of the rows beforehand breaks the result.. @ettorerizza oh, that's good to know, thanks!. A wikidatan is interested in something that looks like transposition functions, but I could not point them to any documentation for that, so I gave the link to this issue:\nhttps://www.wikidata.org/wiki/User:John_Cummings/Useful_spreadsheet_processes_for_Wikidata#Select_every_Nth_line. Whoops, curious that there is this discrepancy between OpenJDK and OracleJDK. Thank you Travis!. @thadguidry that's a good idea! feel free to add that on the branch (it's a branch in this repo so you can push to it). @thadguidry yeah it's clearly better than coverall with the code review features!. @thadguidry yeah it'd be good to configure it a bit so that only relevant issues are reported. Sure, happy to hangout when you want!. The code review features are nice but I think the most important thing is the code coverage. Because 20% coverage is really low for a big project like that.. @ericjarvies thanks for this very detailed issue!\nThe User Agent OpenRefine uses is currently determined by the version of the underlying Java library, which is not a very good thing. For instance, OpenRefine 2.7 uses \"Java/1.7.0_121\" as User Agent on my platform. This is definitely something we should change (the default User Agent should mention OpenRefine and give its version number).\nIt would totally make sense to let users define their own headers. There is a long-standing need for that and the earliest issue seems to be #218. It would make sense to put a bounty on that.. @ericjarvies I agree this should be supported natively by the \"Add columns by fetching URLs\" operation.\nIt is hard to judge what a fair bounty would be for this feature - in terms of orders of magnitude, the last two bounties on this project were around $300. In theory I guess people should just contribute what they can afford, and it would add up for issues where there is more demand.. @paregorios it would be great if you can help with this issue! I agree that it is an important topic (that's also why I have worked on caching for URL fetching, so that OpenRefine is a bit more polite with data providers).. Closing as non-reproducible.. @ettorerizza hmm, this bug seems to have been introduced by my caching feature\u2026 I will investigate asap.. @ettorerizza This should be fixed in the development version. Sorry again!. There are issues with the following url:\nhttps://api.oadoi.org/10.13306/j.1672-3813.2015.03.001?email=dev@openrefine.org. I confirm that this is fixed on master.. That would make a lot of sense! The original source and format of the data could be tracked there. This might also be useful for rich format imports (such as https://specs.frictionlessdata.io/data-package/ ).. Some ideas here: #1045. Hey @denim2x, welcome! Can you give us an idea of what your plans would be for this feature? There are various issues related to project metadata (see the tag: https://github.com/OpenRefine/OpenRefine/labels/metadata, although not all of them are relevant), so it would be good to think about a solution with these broad use cases in mind.. Thanks a lot @magdmartin for this summary of the other issues! That's very convenient. I think it would also be good to have other generic metadata fields as @ettorerizza proposes.\nWe really should keep #778 and #1096 in mind for that. This document gives some ideas about what sort of metadata data packages provide: https://www.w3.org/TR/2015/PR-tabular-metadata-20151117/\nIf we want to support these sort of formats, I think it would be reasonable to designate a list of core metadata fields (things that apply to all OpenRefine projects and would be exposed in the UI) and also provide some support for custom metadata provided by the original import format. This extra metadata could be reused by exporters or extensions (so that, for instance, no metadata is lost if you import a data package, clean it, and export it to another data package).. @denim2x I think the Tabular Metadata specification goes way beyond what @ettorerizza had in mind when he started his issue and bounty. So I do not think implementing that should be required to close this issue.\nHowever, solving this issue will probably involve some changes to the model, possibly impacting the format in which projects are persisted (if we want to add fields that cannot be stored yet). Doing this sort of change is slightly complicated: we might need to adapt the importers so that they fill the fields we introduce, ensure the existing workspaces will be migrated seamlessly to the format that supports the new fields, and so on.\nSo, this relatively heavy change should be made with the other use cases in mind: ideally, we should avoid further model changes and adopt a model that works for a wide range of metadata needs. That does not mean the UI should fully expose the flexibility of this new model, or anything. But maybe that will turn out to be too complicated and therefore out of the scope of this issue. I just want us to have that discussion and not rush too much on adding very particular fields without stepping back and looking at the bigger picture.. @denim2x I don't know! Do you think it would make sense to use it to solve this issue? I am not familiar at all with that.. @denim2x any news about this? Let us know if you have any question about the feature.. @ettorerizza it looks like their SSL certificate expired yesterday - I expect it will be renewed soon. I don't think they ran away with your bounty ^^. @jackyq2015 yeah, actually maybe we should have a discussion about Bountysource\u2026 it has a number of issues so we might want to switch to something else instead.\n- there is a fairly high service fee\n- github integration is broken\n- the website is currently unavailable\n- bounties take a long time to be paid out\nI quite like Liberapay (https://liberapay.com/ ), which is actively maintained and has a smaller service fee (basically just the payment processing fees of their bank). It does not allow to put bounties on specific issues though.. what do you mean by \"identifier\" (what would you take the MD5 of?). @ettorerizza oh, then we already have the project id for that (the one that you find in the URL of a project in OpenRefine).. @jackyq2015 thanks a lot for taking my rants into account! That was quick! I have just tried the new projects view and it looks great! Here are a few other things:\n\n\nWhen your mouse is not positioned on any row of the table, the first three columns look a bit mysterious - maybe it would be worth having some labels for them too? Or not having empty headers at all?\n\n\n\nI just tried creating a project from a URL (CSV, with default import options) and the row count in the project list was displayed as 0. It took me a hard refresh (Ctrl-F5) to see the correct count.\n\n\nDo we store the importer used to create the project anywhere? I can see you store the importing options (that's great!) but I would find it useful to know which importer was used.\n\n\nQuite a lot of comments about the metadata view:\n\n\n\n. @jackyq2015 it looks much better now! I still see a few points that we can still discuss:\n do we really want to display the fields with raw JSON? Especially the one with column metadata (as it will currently remain empty and should be eventually manipulated from the project view anyway?)\n the \"Create Date\" is currently marked as editable but does not have any validation (it's possible to type in invalid datestamps)\u2026 I would either make it properly editable (with a stock date picker widget) or simply disable editing for that (why would you change that  date anyway?)\n I like the light blue better in the metadata view (and it's more in line with the overall style of the UI) but I still think it's a bit too heavy - why don't we reuse the two colors used in the grid view for that? (#fff and #f2f2f2). That would be more consistent I think.\n the \"About\" cell is capitalized whereas \"rename\" isn't. I would change it to \"about\" for consistency. (Also, I think the other labels should only be capitalized for the first word - again for consistency with the rest of the UI - so \"Project Name\" -> \"Project name\")\n Do we still need the rename link now that the name can be edited in the metadata view?\n It's not clear to me that \"about\" is the think you should click if you want to edit what you see in the table. I would find it more intuitive with \"edit\", or even better: a pencil symbol (for consistency with the delete button on the left hand side).\n* Would it make sense to have a metadata field that stores the original format (or name of the importer) of the dataset? It does not look like this is stored in the import metadata.\nIf you're fed up with these UI tweaks let me know, I can do them :) I just think it's important to keep the UI as clean as we can, especially for a part that is seen a lot.\nOne nice byproduct of this change is that the delete buttons are now always visible - I remember that it took me a while to figure out how to delete a project and that is now much clearer!. Oh, and two other things (sorry):\n the delete button is placed weirdly with Chromium (not centered in its cell)\n it would be nice if the \"last modified\" could still be displayed in the nice human-readable form they had while preserving the sorting functionality\u2026 I have no idea how hard that would be. But in the current state it's kind of a regression in terms of user-friendliness.. The \"source\" key in import metadata is not enough to tell which importer was used! For instance, if it's just from the clipboard (and in general not all file names indicate their format correctly, and the user might have chosen a different importer)\u2026 So I think it would really be worth storing that explicitly!\nI'm curious to know if users want to see ISO timestamps rather than human-formatted dates\u2026 I think it looks quite hacky (especially with all the useless \"000Z\" at the end\u2026). If you prefer to see dates rather than time deltas, then dates should be human-readable too. Think about our non-programmer users: they should not have to know about ISO timestamps to use OpenRefine!. I agree \"Rename\" and \"About\" take a lot of space\u2026 That's why I think \"Rename\" should be removed. I don't see any reason why this shortcut should be kept:\n it only saves one click anyway\n it is not placed logically next to the title\n* why provide a shortcut for the title (which is generally filled at project creation) while there are so many other empty fields that the user could want to fill?. @jackyq2015 in the end do we have tests for this feature?. @thadguidry it is failing because of #1223.. The problem was fixed by 9c045b7, so we can merge now.. @thadguidry hmm, you can try, but I don't think it is going to work. The issue you linked to is quite old and I think it is a different problem.. @thadguidry yeah any of these would do - the hard part is generating the coverage data in Travis. It looks like we have to migrate to Maven first (#1054 should help).. @thadguidry awesome!. This is solved!. @akbertram yes, the jython extension is a good model, as it isolates very well the support for that language.. Yes Graal would be a great fit to implement expression languages in OpenRefine. I don't think we can replace our current Jython extension with their Python implementation, as it is still less mature than Jython, but Graal could potentially be used to implement new languages. Also, it should not be too hard to reimplement GREL in Graal, which would probably add a lot of benefits too.\nAs GREL shows, OpenRefine's expression languages do not need to be fully-fledged programming languages to be useful, so Graal's main shortcoming of the partiality of its implementations is less of a concern than in many other places.. Hmm, Codacy did not find anything to say here. It did find issues for this one though: https://github.com/OpenRefine/OpenRefine/pull/1216\nBy the way, this closes #1224.. Following the discussion on #1216 I merge this first.. @thadguidry @jackyq2015 I need admin rights to add a GitHub hook for Codacy integration (so that the dashboard gets updated when we push things on master). I've sent you an invite link for Codacy if you want to do it yourself.. This is done, translations can be edited here:\nhttps://hosted.weblate.org/projects/openrefine/translations/\nI've started a German translation, if people want to get involved\u2026. By the way if we do not like weblate's automatic pushes we can always ask it to do pull requests instead - let me know if it is better.. I see a few alternatives to creating a dedicated operation for that:\n- improving the XLS importer to parse the first N lines as metadata (or ignore them)\n- add an operation that removes the N first rows or parses them as project metadata (this way, it can also be easily combined with other operations, such as Columnize by key value, for instance).\n- add this functionality as an option of the existing transpose operation\nEven if this data format is widespread, writing a specific operation for it (which would largely duplicate the functionality of the transpose operation) does not sound like a very viable approach.. I agree this is an important problem. But it's not entirely clear to me what the solution should look like.\n- I can put a server-side cache on top of the reconciliation queries (but that would only solve the problem for Wikidata)\n- We can put a client-side cache in OpenRefine but it would probably not survive crashes. It would also need to be persistent across operations (which is not the case, for instance, for the cache I have added to the URL fetching operation).\nAs always with caches, we have to be careful about the size, invalidation strategy, time to live, and so on\u2026\n. Related issue: #580. I don't think that running an autosave while the operation is running would do anything - the reconciliation results are stored in memory and they are only added to the project at the very end of the operation. Even if the column was populated gradually, you would still need to write the change to the history, and recover from an interrupted run (that could be done by faceting, but it would be quite manual).. @codacy-bot I don't think we are going to be friends for a long time\u2026. Hi @tentonsofhours!\nThanks for the report. The first lines of the stack trace are not visible in your screenshot, do you think it would be possible to have the full error?\nThanks!. @thadguidry great that you could find the cause just by looking at the logs! Can we track the status of this issue anywhere in Jython? We should not forget to update the .jar we ship with OpenRefine once they have released a version that fixes this bug.. Hi @tentonsofhours, the bug should be fixed in the development version of OpenRefine.\nIt would be great if you could try to install this new version (see https://github.com/OpenRefine/OpenRefine/wiki/Get-Development-Version ) and confirm that it solves your issue.. @thadguidry I've added the sources JAR and listed the library in our LICENSE.txt file, let me know if anything else is missing!. @thadguidry That is just something that will be solved when we merge. It's just that I have included 5390aa248cd044 in both branches so that the tests do not fail in this PR.. @thadguidry yeah, I have to admit Codacy is too verbose to my taste. I tried disabling some of the patterns but I get a 404\u2026 And also, they never report any information about code coverage, which was the goal of the integration\u2026. @thadguidry Yeah why not giving a try to Codecov - personally I just love Coveralls, all it adds on PRs is this: https://github.com/dissemin/dissemin/pull/426#issuecomment-319653015, and that's essentially all I need. Code issues are useful, but only if they can easily been enabled and disabled, IMHO.. Hmm, the failure seems to be random as Travis did not encounter it this time\u2026 Looks like a race condition somewhere.. @thadguidry I think I did remove the 1.7.0 version, see the changes here: https://github.com/OpenRefine/OpenRefine/pull/1238/files. Closing as duplicate of #461. I would inspect the places where images are supposed to appear (like, the OpenRefine logo in the top left corner) with the developer tools of your browser. If you can also access the server's logs, that would be helpful, I think.. @thadguidry no, I haven't investigated further. Closed by #1263.. @bluntmagic have you tried increasing the memory limit for OpenRefine?\nhttps://github.com/OpenRefine/OpenRefine/wiki/FAQ:-Allocate-More-Memory\nif that does not work, could you share your dataset (or a part of it) so that we can investigate what is happening?. Closing as non reproducible.. @thadguidry it's going to be clearer once the wikidata extension is merged.. This is not going to be feasible given the sort of metadata we have in projects and the sort of reference that is expected in Wikidata. One better way to ease adding references in Wikidata schemas would be to provide a way to copy/paste them as suggested by Owen.. @AndrewRCalderon  Please use the mailing list to ask for help - we only use this issue system to track bug reports or feature requests. Thanks!\nhttps://groups.google.com/forum/#!forum/openrefine. Everything is green, let's merge!. @ViktorHolas is that an error you get when trying to create a project from the URL of your sitemap? Could you share that URL with us?. @ViktorHolas Which version of OpenRefine and which operating system are you using? It works fine for me.. Great! Then most likely the problem is independent from OpenRefine itself.. Some discussion is available here:\nhttps://phabricator.wikimedia.org/T173749. Closing this because we still don't have any description of how RDF would work for data import - we would need detailed specs for that. People can use the existing QuickStatements exporter and plug that to https://github.com/marfox/qs2rdf.\nFeel free to reopen if specs are published eventually.. thank you @codacy-bot for being cooperative tonight.. @ettorerizza Thank you! It is really valuable to have someone dealing with questions on StackOverflow and reporting as issues those which are related to bugs.\nI agree with your analysis and have proposed a PR accordingly. I think this change is pretty safe. Maybe the original author of this fingerprint method had some weird case in mind, but they did not include it in the unit tests.. @msaby sure! we try to avoid GitHub for simple requests for help (\"how do I do this thing in OpenRefine\") but reports of bugs (or other \"unexpected features\") are always welcome!. @thadguidry there is no difference in efficiency here, it's really equivalent. But it's easy to get these things wrong, we just need to gather test cases to document the choice of order in the operations.. @thadguidry I don't understand, what are the 10% cases the changes do not deal with? Do you have an example in mind?. @ettorerizza that would be great!. According to GitHub\n\nIt\u2019s common practice to prefix your version names with the letter v. Some good tag names might be v1.0 or v2.3.4.\n\nSo I would recommend we follow this practice\u2026 (this lets us define tags which are not releases).. Also, I wonder if the download link should open in a new tab. I think I was expecting that when I clicked the link. Any thoughts?. @Gucci1986 I have added closing ] and } to turn your text into valid JSON.. @Gucci1986 importing the JSON payload above in OpenRefine and exporting it to CSV gives the following result:\n_ - id,_ - message,_ - created_time,_ - from - id,_ - from - name\n1551460621555337_1551461834888549,Or\u00e9lie Or\u00e9lie ... mdrrr j en peux plus  \ud83d\ude02\ud83d\ude02,2017-05-31T15:53:45+0000,10203499588585347,JJ Nina\n1551460621555337_1551462631555136,\"\"\" tu veux voir mon clitoris de 15cm ?\"\"\",2017-05-31T15:55:03+0000,10203735457525161,Emilie Duthoy\n1551460621555337_1551464234888309,Trol Lex \u00e7a te rappelle qqn ?  \ud83d\ude02\ud83d\ude02,2017-05-31T15:56:50+0000,133169370361082,Mi Stick\n1551460621555337_1551461858221880,Mdr,2017-05-31T15:53:46+0000,123755004648034,Seb Seb\nwhich is exactly what I would expect.\nPlease provide an example input where your bug can be observed! (Make sure you can see the bug you are describing given the data you send us.). @ostephens thanks for looking into that! Your fix looks perfect to me.. I think this user found a fix - it'd be nice to know if there anything we can do upstream to prevent that problem:\nhttps://twitter.com/27point7/status/918002725048279040. It would be great to make the dialogs resizable!. Never trimming seems to be the correct approach here as the user can always trim afterwards or adapt the regex to eat whitespace on both sides.. Hi @jackyq2015 - the tabular capabilities of Wikimedia Commons are (in principle) independent from Wikidata. However, they are of high interest to the Wikidata community because they provide an alternative storage method for some types of datasets that are too big to be represented as Wikidata claims (e.g.: if you have an hour-by-hour history of some exchange rate between two currencies over the past 10 years, you don't want to add one claim per line!).\nHere is an example of what it looks like:\nhttps://commons.wikimedia.org/wiki/Data:Taipei_Population.tab. Here is an example of the sort of metadata that these files can contain and that OpenRefine does not currently store:\n\n@thadguidry I think the specs are a subset of OKFN's Data Package (#778) but the features are quite limited for now.\nSo, to implement this exporter, we would need to have support for more project-level metadata (description, source) and column-level metadata (type, label, and maybe more?).. This feature is still experimental and few properties use it in Wikidata - so I'm removing the high priority.. The simplest option for this would be to go with the BigDecimal datatype. However I don't think it is wise to add a new datatype given the current lack of typing for cell values.. This is a big breaking change so intuitively I would do at the same time as other major infrastructure changes (such as moving out of the in-memory backend for some Spark/Beam/other implementation).\nBut if we can form a group of motivated extension maintainers who support the change, we could do it independently too.. hi @aadrian, I don't think OpenRefine supports JDBC natively. You can try exporting your dataset from JDBC to a format that OpenRefine supports.. Closing as the feature was implemented.. @ostephens would it make sense to add a few unit tests for this feature?. @ostephens here is an example: https://github.com/OpenRefine/OpenRefine/blob/820b6f395ad37c52d5fc091fc4642d1d72ccf780/main/tests/server/src/com/google/refine/tests/model/UrlFetchingTests.java\nLet me know if I can help to clarify anything!\n. @ostephens Yes I completely agree with you, this is a mess. Feel free to reorganize as you wish.. @ostephens yes we can ignore them. @thadguidry could you disable them please?. @thadguidry if you want to keep that then I'll just vote for getting rid of Coveralls integration altogether\u2026 It really clutters PRs with a lot of useless stuff. If you like going to coveralls itself, fine, just let's disable the GitHub integration so that we can work on PRs in peace.. @thadguidry Yes sorry! And we can replace Codacy by Coveralls, it's much cleaner in PRs.. @remram44 I completely agree! Pull requests are more than welcome.. @remram44 The project leads (if there is such a thing) do not master all the languages that we currently support, so we cannot fix these translations ourselves\u2026 So I don't really see what you mean! What \"commitment from project leads\" do you expect? The people who wrote the code that you quoted are not active in OpenRefine anymore.\nFeel free to invalidate existing translations, people will go to Weblate to fix them if they use the translation.. @remram44 Feel free to use Weblate for that (so that you will not even have to file a PR).\nhttps://hosted.weblate.org/projects/openrefine/. This is a known issue (duplicate of #1219) that has already been fixed in the development version. You can either disable caching, or use the development version (https://github.com/OpenRefine/OpenRefine/wiki/Get-Development-Version ) or wait for the next release.. Thanks!. I confirm this was unused. Thanks!. Thanks Owen!. @remram44 I think prefixing the string ids with the module name would make sense indeed!. @remram44 that would be fantastic! But I was thinking, maybe it would be better to avoid collisions of translation strings, by adding the namespacing mentioned above?\n$.i18n._('core-dialogs')[\"content\"] -> $.i18n._('core-dialogs_content')\ninstead of\n$.i18n._('core-dialogs')[\"content\"] -> $.i18n._('content') which looks more risky (especially because we have extensions)!\nAlso given that we have had quite a lot of changes since this PR was started, it might be easier to restart it from the current master branch.\nI can help with any of this - you already made a great contribution by pointing us to the right library and good practices :) . @remram44 As this PR is quite old now, I'm taking it over and restarting it from the current master. I'll try to reuse your commits where possible to preserve attribution.. I agree\u2026 and splitting the cell contents on the fly with a regex is quite hacky too. The cross function already feels hacky in the first place actually. This is just a hack around the fact that OpenRefine's model is inherently designed for single-table databases. Ideally, we would need a proper join operation between two tables.. > Team, S L O W D O W N here. :)\nHmm, can you remind us who merged this PR on sight? ^^. hey @thadguidry I think it'd be safer if you let another coder review code contributions. Having a green Travis is not enough in general, especially for PRs like this one which change some non-trivial things. I think this has been pointed out in the past so let's try to stick to that rule.. @thadguidry thanks! I did not have strong feelings about this particular PR, it was just a matter of principle. I have re-created it in  #1292.. @claussni I realize you don't have push access to this repository so you won't be able to fix the remaining issues in this PR\u2026 Sorry for this mess! It looks like we need to create yet another PR from your own fork. Let me know if you need git help to set that up.. @claussni yes let's do that, thanks a lot!. I agree that the \"Unexpected internal error\" wording should be changed to something less scary (\"invalid regular expression\" or something like that). I'm not sure it's worth making special cases for some types of unterminated expressions\u2026 I think the user is grown up enough to accept that their expressions may not be valid while they type them.. @jackyq2015 this PR was not about this test case, it is about the date parsing issue. I have reopened it at #1427.. Adding a \"filtered duplicates facet\" also brings in more issues - it would mean that the order of facets would need to matter (otherwise, with two filtered duplicates facets, you get a circular definition). So this is prone to introduce bugs and unexpected behaviours.. Sure, good idea!. @ygherman hi, we are aware of this issue but the RDF extension is not maintained by this team. I think this extension is looking for a maintainer actually.. @fpompermaier just fork it and you'll be the owner :). @fpompermaier my point is that you don't need to transfer the repository at all! If you start maintaining your own fork of it, you'll be de facto the owner of the \"official\" repo. That's the good thing about FOSS! :)\nIn other words, there is no such thing as ownership: there is only maintained and unmaintained forks\u2026. This exception cannot happen anymore, as we do not import JSONException and JSONWriter anywhere anymore.. Yes this is annoying, this has already happened in the past and I have just deleted the translation. I will work on a cleaner solution so that OpenRefine just ignores these translations.. @nijel thanks a lot for chiming in, it's good to know! In our case I don't think it would work as the language name would be empty, so we could end up with blank labels in our drop-down list of languages to choose from.. @ostephens I would write a test like this:\n create a sample project with a CSV\u00a0importer\n add a facet that uses your inversion (this is done via the \"Engine\")\n create a RowVisitor and run it on the engine - the visitor will only be called for the rows that match the facet - you can do assertions in the visitor, or based on the visitor result.. @ostephens actually forget about that, I think there is a simpler way to test a facet: just run it on a row and check the result. So:\n instanciate your Facet\n use its getRowFilter to get a RowFilter out of it\n run it on a few rows from your project and check the result\nThanks for your effort, it's brave to write tests for a part of the code that is currently very poorly tested\u2026  Once we have a few, it will be easier!. Hi @thadguidry, sorry but I will not be able to make any commit on the project until the end of the year. I can still review PRs and comment on issues though. Feel free to change this yourself - it is just a matter of renaming these \"_\" to any variable name you like (for instance exception).. @thadguidry again I think you should wait for a programmer to review the code before merging! Your reviews are very useful for the user standpoint, but we also need to watch for code quality. We really need to stick to this rules, otherwise we are just going to ruin the codebase. I am not saying I should have the last word on every PR: any coder with some familiarity with the code can do it (e.g. @ostephens).. @thadguidry sure! preferably in your morning so that it's not in the middle of my night :). That would break the idea behind OpenRefine's data model, that transformations are applied row-wise (or record-wise). Most transformations in OpenRefine can be applied to a data stream, where each row is discovered gradually: you don't need to know the whole dataset before starting to run your operations on it. While that's indeed a restriction for the user, it's a very useful guarantee to have if we want to develop more scalable backends (for instance in a map-reduce fashion).\nSo, I might be wrong, but this really looks like an important design decision to me. OpenRefine is not a spreadsheet software.\nThat being said, there already are some exceptions to this philosophy (for instance, the transpose operations), so it wouldn't be the first hack that bypasses the spirit of the model.. This PR is outdated.. On the long term, we should consider migrating to something more modern like Jackson or Gson.. We probably need to configure weblate so that by default the strings are copied from English into the localization files for other languages. Also, it could be useful to have a \"feature freeze\" period before a release so that translators have the time to catch up on new messages to translate.. The problem I have with the \"subject\" field is that it is just free text: that's not very useful to organize your project library. The notion of subject is ill-defined, no importers fill that field because no serious file format will bother providing a field for that\u2026 And most users will just not bother filling up that field either, so we'll have a sad empty column taking a lot of space. So yeah, this field is just useless for me, that's why I'd be happy to see it replaced by the tags.\n@jackyq2015 I don't understand why you say that \"subject is formal and tag system is more free style\"\u2026 In fact I was about to write the exact opposite!\n. @jackyq2015 if the \"Subject\" field in OR is meant to be \"dc:subject\", I think this should be made clear in the UI\u2026 The screenshots I have seen did not use it in this way ;-) . But anyway, you can also decide to use a controlled vocabulary for the tags (and you should!). Both of them can be used with a controlled or uncontrolled vocabulary, but the tagging system seems a lot easier to use for that purpose. If you like \"dc:subject\", then we can decide that each tag is meant to be a value for this property! :)\n@thadguidry yeah if people prefer free text over structured lists, fine! I don't actually care about that, I'm just saying that this looks pretty useless to me (and a waste of space) in the current form. But it's clearly not a hill I want to die on\u2026. Thank you so much for this work! Here are my thoughts after the changes:\n Why are the tags aligned to the right? In a left-to-right language I would expect to see them aligned to the left.\n There is no consensus to replace the subject field by the tags. What if we kept the subject field, but without displaying it in the table? (It would only be accessible via the about link. I would do the same for the author and description fields actually) Or, if there is still no consensus for that: changing the position of the tags column so that it is just after the \"Name\" column.\n Tags should be clickable (that was pointed out by others)\n The tag input on the project list is not as nice as the tag input at project creation - it would be cleaner to have the same input in both situations.. @thadguidry your review is still marked as \"requested changes\" - do you still have any concerns?. @fpompermaier does this actually perform any fixes? Codacy already provide style warnings:\nhttps://www.codacy.com/app/OpenRefine/OpenRefine/dashboard. @fpompermaier I'm not sure if Codacy has any Eclipse or IntelliJ integrations - your best bet is probably to look at the underlying tools that they use to generate the warnings and configure them locally.. @magdmartin @thadguidry yes let's keep README.md only and remove README.txt!. Are we sure that we want to put Google on an equal footing as individual contributors? Their contribution is of a different nature\u2026. * QA results - I agree this is obsolete and should be removed. Does anybody know what sort of things cell.recon.features.qaResult used to store? @thadguidry?\n The two label changes look sensible to me (except the case: only the first word should be capitalized to keep consistency with the rest).. @magdmartin I did not find any such option in the configuration panels.. @thadguidry no, I just tried triggering a push from Weblate and it's still rejected:\nremote: error: GH006: Protected branch update failed for refs/heads/master.        \nremote: error: At least one approved review is required by reviewers with write access.        \nTo github.com:OpenRefine/OpenRefine.git\n ! [remote rejected]   master -> master (protected branch hook declined)\nerror: failed to push some refs to 'git@github.com:OpenRefine/OpenRefine.git' (1). It only works if we uncheck \"Require pull request reviews before merging\". (I've just tried without that, it works, and I've re-checked the box after that.). @jackyq2015 hmm, that's a nice idea, but I see a few issues:\n it does make things a bit harder for users as a simple \"git clone\" will not fetch everything as it currently does\n* currently we are only translating files in one folder but we might want to include translations of extensions later (for instance, the wikidata extension)\nMaybe we can ping @nijel for his kind input on this issue?. @nijel the docs say that this feature (github pull requests) isn't available on hosted.weblate.org (and indeed I cannot find it in the configuration screens).. @nijel thank you so much!. @ostephens absolutely! It would be very useful to migrate this as you suggested.. @ettorerizza ah, thanks a lot! Do you think it make sense to also allow the dot notation too?. We should also let reconciliation services return the list of types in the suggest service - these types should then be stored in the Recon object. Otherwise, this creates a discrepancy between the cells which were matched from existing suggestions, and the ones where the record was looked up manually.. Just realized that the types are already supported - reconciliation services just need to return types as follows:\n{\n     \"id\": \"Q38082\",\n     \"name\": \"Lewis Caroll\",\n     \"notable\": [\"Q5\"],\n     \"description\": \"English writer, logician, Anglican deacon and photographer\"\n}\nI have updated the docs accordingly.. @thadguidry my point is that facets can contain arbitrary GREL, Python, Clojure expressions\u2026 so you need to dive down in these expressions, find out where columns are used, rename them, and generate a new expression. That's because we have access not just to the value variable, but also to the whole cells object.. Closing as a duplicate of #133.. that seems to indicate that OpenRefine is not properly url-encoding this parameter. I suspect it should not be too hard to solve.\nOut of curiosity, are you encountering this problem in an existing reconciliation service, or are you building a new one?. @ostephens it looks like it triggers the recovery procedure for damaged projects - as if OR could not find the project metadata file and had to infer it from the data itself. When starting OR again from an earlier version I can see many new projects, created from the original ones (which are still available).\n\nMost of them don't have any particular metadata set.\n. That was indeed a bug on jacky's branch - sorry about that!. @tiagofernandez at least I don't get import options appear twice in the first message of this PR. yes, this happens when the network is too slow. Your fix looks great, thanks!. @jackyq2015 your commits about this issue are on the data package branch, maybe it would be simpler to put them on a separate branch as the changes are mostly unrelated.. I think storing the note in the project metadata is cleaner than having an operation for it. Renaming a project does not perform any operation, why should adding a note do that?. @tcbuzor Many of the tests are empty - wouldn't it be better to provide actual test cases?\nI will have some time to do a thorougher review next week - please do not merge yet!. @tcbuzor any news on this?. @tcbuzor woaw, that's fantastic! so much testing! Thank you so much!. We need to disable these tests on Appveyor too, as Appveyor does not seem to support databases. Actually, we should disable the tests by default and only run them for Travis, so that folks don't have to launch three different database engines to test OpenRefine.. oh great! then why don't you configure appveyor so that it goes back to green? :-P. @thadguidry I'm so rusted on windozy-things that I was hoping I could ignore the issue ^^. Thanks for the changes. Concerning point 4. and 5., I really think this is the wrong approach.\nAdding support for some standard in a piece of software does not mean \"add a slot to display and edit the JSON representation of the format described by the specs\". It means integrating the specs of the format into the data model of the software to enable a seamless use.\nLet's make an analogy to illustrate my point. Suppose you are asked to add FTP support in a web browser that only supports HTTP so far. Your task is not to add a window where users can type FTP commands. Your task is to make it possible to click on a \"ftp://\" link and get the exact same browsing experience that you have with HTTP.\nJust look at examples of presentation of data packages, like https://datahub.io/core/s-and-p-500-companies\nThe JSON is not displayed at all, and for a good reason: that is not what users care about! Users care about their data, the types of their columns, their constraints, and so on. They do not care about the specifics of the Data Package JSON format. They should not have to read the specs to use Data Packages in OpenRefine. It's called \"frictionless data\" for a reason!\nSo, yes, a proper integration would take more effort than what you have done. And of course it is fine not to have a perfect implementation in one PR. But the issue with your implementation is that it is not even a step towards a proper integration - it's just not in the right direction at all. If we merge and release this, we will have to undo a lot of things later on and migrating projects to a better implementation will be a nightmare.\nI'm sorry to say that at this stage of your work, but I did voice my concerns in #1096 originally.. > The link from datahub is for an existing data package. But in order to generate a data package, the creator have to deal with the detail metadata definition one way or another.\nNo. The data package can be generated by a tool (for instance some implementations of the specs infer types automatically). So the user does not need to know anything about the way Data Packages are represented in JSON.\n\nFor example, user have to input the \"license\" if he want it be part of his metadata.\n\nThis only works for very simple, unstructured parts of the metadata, but that is completely impractical for something like the tabular schema.\n\nI am not sure if you are saying we should swap out the current Project Metadata with the \"data package\" metadata. I had disucssion with Thad, and we agreed that the current Project Metadata will be preserved because it's already fully integrated with the OpenRefine and it is easier to extend since it does not tie with any external spec.\n\nWell, I think this is a big mistake. This should have been discussed publicly. One one hand you have introduced these metadata fields in OR 2.8 (supposedly following Dublin Core), and now you are introducing duplicates of these fields - I don't see why this would be practical at all for users. This is going to be confusing (for instance, the metadata that is displayed in the project list will not be exported to a data package\u2026 as a user I would find that very frustrating if I go the extra mile to fill these fields).\n\nI already posted a brief design for your review before I started the implementation and haven't got any response yet:\nhttps://docs.google.com/document/d/1RdnpZ1Nr0NWXG4RfNQ7tyDbmkSPSltFMcCI6hxl5484/edit#\n\nOh, I did not see that! Where did you post it? It should really have been on GitHub so that people who got involved in #1096 could comment. If I had seen that earlier I could have warned in advance that this was the wrong approach.\n\nIn terms of how it should be integrated into the existing system, my understanding it we should always use the existing resources.\n\nYes exactly! And guess what, OpenRefine already had a good basis for a tabular schema! We have the Column class, that already holds some limited metadata about the column, such as its name and its reconciliation statistics (which service type was it reconciled against, importantly). The tabular schema should be baked into that, by adding new fields to these classes. That makes the integration much tighter, much more transparent for the user, and much easier to integrate with the UI and the operations and changes.\n\nBut for the data package data itself, user still need to input somehow\n\nNo! The user could simply click on a column and have some operation there to set the datatype there, for instance. (Or infer it automatically). The user should simply not have to interact with the JSON at all.\n\nRegarding the workflow you mentioned, I think one thing we are missing is to incorporate the current \"column model\" and the \"data package schema\" then we can get the final schema.\n\nThis should really have been the starting point of the integration. That is what I tried to explain in #1096. (but clearly I failed\u2026)\nJust step back and think about why and how standards like CSVW and Data Packages were created. The goal of these standards is to enhance data files (mostly tabular ones) with rich metadata. But shifting the open data ecosystem from CSV to something richer takes a lot of effort: you need to adapt the tools, educate the data providers, the data transformers, and the users. If these users are required to read the specifications to use the format, this is never going to work! Many users do not even know what JSON is, and they should not need to. OpenRefine is used by these people, and OpenRefine is (so far) really good at hiding a lot of the complexity of data formats (typically, with its JSON and XML importers). So let's keep it that way, and provide a clean integration of Data Packages in the model, where both project metadata and the schema can be edited where users expect it, and not in an obscure JSON editor. People do not need OpenRefine to edit some JSON, they can already do it very well with any decent text editor.. > Also think about if CSVW is introduced in the future, there is for sure lots of duplicate between metadata models. Even we can get rid of current project metadata. we will still come to a point to decide which spec is better/more useful if we want to only keep only 1 metadata and thus remove the duplicate.\nThat is exactly my point! This duplication should be removed right now, there is no point in introducing it in the first place.\n\nFor the metadata 2.8 version, I confirmed with @ettorerizza, @magdmartin and @thadguidry on the github issue publicly.\n\nThat is a totally different issue - this did not include the schema at all.\n\nThe point is that I did not decide what metadata should be included. There is duplication but we discussed in \"tag system\" PR, we will clean it up gradually after reach the consensus. we cannot get rid of current metadata for now.\n\nThe problem is that by leaving these metadata fields outside OpenRefine's core, you are making it very hard to build any meaningful integration with the rest of the software. This is way too loosely coupled and this is going to hit us back as soon as we try to implement features that will actually be useful to users.. @jackyq2015 @thadguidry @ostephens @magdmartin I think it would be good to have a skype meeting to talk about all this in person and try to find a solution. I feel bad about obstructing Jacky's work because he has clearly put a lot of effort into this but at the same time I am very concerned by the shape of this PR.. @thadguidry I'm available too! Just sent you an invite.. Example workflow that should (ideally!) be supported:\n- create a project from a bare CSV with no metadata\n- rename and reorder the columns\n- add types on some columns\n- enhance the project metadata (from the project list, for instance)\n- reconcile a column to a reconciliation service, with some type\n- export the project to a data package\nThe data package should contain:\n- the updated project metadata\n- the tabular schema induced by the new datatypes and the reconciliation type. @thadguidry ah now I understand why you want to keep types as strings! That's because you are mixing up two things: the data type and the RDF type.\nThe data type says whether the column is made of integers, strings, booleans and so on. The Data Package format does specify the acceptable values for that, see https://frictionlessdata.io/specs/table-schema/#types-and-formats. So we should just take do the same: use an enumeration (which might not initially contain all the types they support).\nThe RDF type (called \"Rich Type\") is a different sort of type that can be provided, which can accept things like \"http://schema.org/funder\". We should also store rich types, but in a different field.. @thadguidry yes the controlled list is just there in the specs - \"string\", \"number\", \"integer\", \"boolean\" and so on.\n@jackyq2015 okay I had not seen that this library uses Strings to encode these types too. All I am saying is that we should validate this String against the list of supported types.. @jackyq2015 let me know when the PR is ready for a new code review :). So, I have found time to give it a go. I haven't reviewed the code yet, just trying it as a user for now:\n\n\nIn the import UI, the data package importer should be integrated just like the other importers: it should be possible to import a datapackage both from a file, a URL, and the clipboard. So the \"Data Package (JSON URL)\" option should be removed.\n\n\nIt does not look like zipped data packages are supported but I thought you intended to support that - if it is supported, how?\n\n\nThe import code looks a bit brittle. Importing http://pintoch.ulminfo.fr/38972ea28/datapackage.json gave me the following error in the logs:\n```\n\n\n09:10:48.444 [                   refine] POST /command/core/importing-controller (5015ms)\n09:10:48.506 [..ata.DataPackageMetadata] Data Package metadata loaded (62ms)\nException in thread \"Thread-7\" org.json.JSONException: JSONObject[\"keywords\"] not found.\n    at org.json.JSONObject.get(JSONObject.java:471)\n    at org.json.JSONObject.getJSONArray(JSONObject.java:618)\n    at com.google.refine.importing.ImportingUtilities.populateDataPackageMetadata(ImportingUtilities.java:1163)\n    at com.google.refine.importing.ImportingUtilities.createProjectSynchronously(ImportingUtilities.java:1116)\n    at com.google.refine.importing.ImportingUtilities.access$200(ImportingUtilities.java:114)\n    at com.google.refine.importing.ImportingUtilities$7.run(ImportingUtilities.java:1070)\n09:10:49.466 [                   refine] POST /command/core/get-importing-job-status (960ms)\n```\nThis error stalls the UI without anything being reported to the user.\nSame issue with http://pintoch.ulminfo.fr/347584eb7/datapackage.json .\nCan you point me to any data package that is imported correctly? It would be useful to review the rest.. Thanks for the example data package that works. It's great to see that everything from the Data Package seems to be imported correctly and that the description shows up automatically on the project list. I noticed a small issue though: any tags entered while creating the project are overridden by the tags contained in the Data Package. Wouldn't it be better to merge the two lists?\nAbout the mandatory keywords: it's your call, you can choose to require keywords - you just need to report the error to the user somehow. You cannot expect people to check the logs for that, it needs to be properly exposed in the web UI. That is true for all required fields. Your code should just not throw any exception when used normally. Never trust user input.\nAnother example: when importing https://pkgstore.datahub.io/core/cofog/datapackage_zip/data/ed438b8b276d0169131523969f4e3582/datapackage_zip.zip , I get this exception:\n```\n09:27:58.272 [                   refine] POST /command/core/importing-controller (8439ms)\nException in thread \"Thread-7\" java.lang.NullPointerException\n    at io.frictionlessdata.tableschema.TypeInferrer.infer(TypeInferrer.java:161)\n    at com.google.refine.importing.ImportingUtilities.createProjectSynchronously(ImportingUtilities.java:1132)\n    at com.google.refine.importing.ImportingUtilities.access$200(ImportingUtilities.java:114)\n    at com.google.refine.importing.ImportingUtilities$7.run(ImportingUtilities.java:1070)\n09:27:59.297 [                   refine] POST /command/core/get-importing-job-status (1025ms)\n```\nAgain this should be reported to the user in a clean way. If the underlying Data package library is buggy then fix it or just wrap your calls to it with try/catch blocks.\nAbout the way the import is presented: there already are cases in which a file can be handled by multiple importers - I would just tweak format detection so that the Data Package importer is selected for JSON payloads that look like Data Packages, and fall back on the standard JSON importer otherwise. It is going to be much more usable.\nFor the zip import: okay, I think I got it working now. But it's a bit curious because at first it looks like the Zip is being treated like any other Zip file. It's not clear which files in the zip should be selected, for instance. Only the JSON files were selected by default in my case - I had to tick the main CSV one and manually set that it was a CSV, not a TSV. It's not a huge deal but it means people really have to understand how a Data Package is made to use the feature.. @jackyq2015 thanks for merging the tags.\nFor the local and clipboard import: if you don't see how to do it that's fine, I can take that on once this PR is merged. The clipboard import is still useful in some cases, for instance the ones with inline resources (typically from Wikimedia commons). In this case the Data Package is indeed a data format in itself. So let's keep it like this for now and I will migrate the importer to a standard one when I do #1269 (but that should be done before the next release otherwise users will be confused).\nI will now try to review the code. It would really help if you could avoid merging master in your branch because it makes code review a lot harder. If you want to keep your branch up to date with master, it would be nicer if you could just rebase your branch instead. That would keep the history a lot cleaner. And also, please try not put unrelated changes in the PR (such as updating the dependencies or removing the \"creator\" field from the project list). Doing separate PRs and rebasing once they get merged works really well.. @thadguidry yes I can see some of these changes are related to eclipse but I am not sure why they are necessary. As I said I would rather have liked not to have them at all in this PR at all because I think we already have enough things to discuss here\u2026. @jackyq2015 okay, then could you put some infos about that on the wiki - how to recover these sub-projects with gradle?. @jackyq2015 okay, so if it needs more work, let's keep all the eclipse/gradle-related stuff for a different PR?. @jackyq2015 great. So can you remove the eclipse-related changes in this PR?\nYes, we can open a different issue for the gradle vs. maven discussion.. @jackyq2015 I have left comments on the PR, flagging many of these files - can you see them?\nHere are some instances of such files: .gitattributes, .settings/.jsdtscope, .settings/org.eclipse.wst.jsdt.core.prefs, .settings/org.eclipse.wst.validation.prefs, broker/appengine/.project, broker/appengine/.classpath, extensions/gdata/.classpath\u2026 I understand some of them need to be updated to include the new dependencies, but I really don't see why so many of them should be deleted entirely?\nThe idea is pretty simple: you can view the difference between this PR and master here:\nhttps://github.com/OpenRefine/OpenRefine/pull/1398/files\nAll of the changes that you see there should be required to implement data packages. For simplicity, it would be great if you could reply to the comments I have made there. For instance, I have added a comment on the .gitattributes that you deleted:\n\n. @jackyq2015 okay so I think we have covered the whole PR now - unless people have other issues in mind I will merge in a few days.. @isaomatsunami thank you very much! I cannot read Japanese but it looks good to me. Did you know you can also use Weblate for that? https://hosted.weblate.org/projects/openrefine/translations/. This is really needed. Are you thinking about a generic option that would be applicable to multiple importers? It seems to me that this issue is mostly focused on the JSON importer (and XML to some extent). So maybe adding an option to only these two could be enough.. I believe this issue will disappear as we have decided to store all the project-level metadata in the existing ProjectMetadata object (so, no DataPackageMetadata object).. This is probably linked to the failing test case in #1300.. @thadguidry, sure, but let's first get #1564 fixed, maybe? As there is some overlap.. @zhouyao1994 Please ask your questions on the mailing list, Stack Exchange or Twitter. Do you mean keyboard shortcuts? It would help if your question was a bit more explicit.. I agree there should not be any security concern with HTTP -> HTTPS. By the way, if URLConnection does this sort of nonsense, it might be worth migrating to a more modern library (https://stackoverflow.com/questions/1322335/what-is-the-best-java-library-to-use-for-http-post-get-etc). Ideally one that we already have in our dependencies, and in a dream world something that can be easily mocked for tests.. Wow, that is a strange bug. It would be great if you could post an excerpt of your file and a JSON operation history which lets us reproduce the issue.\nOut of curiosity, why did you disable caching? Is there still any issue with this feature?. Closing as not reproducible.. @thadguidry it seems that your link is about something else - but I agree in theory we could try to mock that. It looks quite painful to me - I suggest we just merge this so that appveyor goes back to green, and we can consider separately whether we should mock HTTP requests in the tests, in general (which could speed them up).. @ettorerizza sure, it was just for the user perspective! :). @jackyq2015 if you are happy with this PR, can you approve it so that we can merge?. Aaah it's so good to see all these green checks :). okay, so that assumes we let the user choose multiple types, which isn't currently possible. The point of this PR is to test jdk9.. On the long term, I'd be in favour of a different architecture, that really breaks down operations into individual processes that can be run on different nodes in parallel.. I think there is more than just translating GREL to Spark: it is the operations themselves that need translating. I will try to come up with a more concrete proposal soon.. No that's correct! Anyway this is a huge task so it's great to have many voices chiming in :). (Travis seems to have problems at the moment - we should wait for it to go green before merging this.). @ostephens your tests sometimes fail here. It must be something with the timeout being too low. I had the same issue with the URL caching test before. We could look into mocking the HTTP requests in the URL fetching tests to make this more reliable.. The test works fine on Travis but we had issues on Appveyor.. I think a cap on text length would be reasonable.\nOtherwise, one possible workaround is to do the fetching in Python and extract the interesting data in the same go (so, without storing the full response).. I'm not sure, maybe something like 1024 characters?. Yes if there is a text cap it should be large by default and configurable by the user.. @ettorerizza I was not even aware that we had added pretty-printing there\u2026. @srht do you have any non-ASCII characters in your username on this computer?. This was solved by the Jython update, I think.. @jackyq2015 thanks! I'm going to fix a few other things soon, let's wait a bit before merging.. Bonus track: no more\n[       FileProjectManager] Failed to load workspace from any attempted alternatives.\nin the test logs!\nNow the tests are run in the same test workspace, which makes it a bit easier to inspect the test projects if need be.. @jackyq2015 this time it's ready. Why are you saying that we are only testing MySQL? I can see both MySQL and PgSQL:\n[testng] PASSED: testGetMySQLDBService\n   [testng] PASSED: testGetPgSQLDBService\nI don't know about the PgSQL error for Appveyor - feel free to investigate, I am not hugely bothered as it seems that the tests pass fine with it.\nIntuitively I would expect that the SQL files are identical for Windows and Linux (they should only depend on the SQL vendor) but again I know nothing about Windows (I actually had to lookup the Windows-equivalent of cp to write this PR\u2026)\nSo feel free to look into that if you think that is important - you know much more about SQL on Windows than I do. As long as Appveyor is green, I am happy :)\n. @pabloab That's very true! This mostly requires familiarity with these packaging systems rather than OpenRefine itself, so it's a good task for anyone who wants to help without diving in the internals too much.. @thadguidry I tried to reproduce this but everything seems to work correctly for me - the state of the options dialog is saved and restored when I go back to the column. Not sure how to debug that :-/. It looks like this could be done by adding the appropriate GREL function.\nAbout displaying consecutive spaces, I also think this is an issue but I am not sure how to go around it. Replace all spaces by non-breaking spaces when displaying a cell? That will break word-wrap (and we are distorting the content).. @ostephens Oh I see. Oops. Apologies and thanks for the report! I will look into that.. @adamhadani I agree that the UI could be improved there.\nThe problem is that the most reconciliation services will only accept a limited, closed-list of strings as properties, which is why we recommend to provide auto-completion there. We cannot expect that the column names will be recognized by the reconciliation service as valid properties: the point of this dialog is to let the user perform the alignment, ideally with the help of suggestions provided by the service as they type.\nThere are also cases where my dataset contains a column matching a property of my foreign service exactly, but I do not want it to be used for reconciliation (for instance because it has unreliable values). So even if we could magically match columns to properties, it might not be something we want to do implicitly. We could definitely consider adding heuristics to do some automatic schema alignment here, but I think the user should ask for it explicitly (and that's a non-trivial task).\nOne change I would definitely like to see is some sort of visual clue that a property has been matched with the suggest service. Currently if you type the property name exactly as it is stored in the reconciliation service, there is no way to tell if I have correctly clicked on the suggestion to match it with an identifier.\n. @adamhadani Okay for 1. (so you would just refuse to start the reconciliation process without a property name there).\nBut I'm still not convinced by 2. By providing a default value there, you would indicate that there could be cases where the user would not have to edit this field and leave the default value as it is. The problem is that for all reconciliation services which provide a suggest service to translate property names to property ids, this is wrong: we always want the user to use the suggest service to translate the property name to an id. So even if you have a column \"country\" in your dataset, whose name matches exactly the English label of the P17 property in Wikidata, simply writing \"country\" in the reconciliation dialog will not work.. Arf - shameful! It shouldn't be hard to get right. Let me try again.. @ostephens Your examples work fine here - are you sure you have properly recompiled OpenRefine when you tried the issue1448 branch?. Thanks a lot Owen!. @thadguidry it's just that you removed the method when you merged. I added it back in the commit above.\nFor your git issues, I think you can just do\ngit pull\ngit checkout stundzig-develop/1086-quotecharacter\nand that should work, I think.. > We should probably also allow the quoteCharacter to be changed by the user ?\nIsn't that already the case? (Note: you might be confused by the fact that OR does not pick the CSV/TSV importer by default for some inputs. Make sure you select the CSV/TSV importer, and you should see the input to change the character.). Very nice! But\u2026 Am I the only one to think that the font used for the logo looks quite bad in this context? (It's not about your work Joanne obviously). I would say the font is too small and too bold.. @joanneong good catch! That's because the German translation is still very far from complete, so many strings don't even show up. I thought that we had a proper language fallback in place (so that the English strings are used instead) but if we have one it clearly does not work. #1285 could address that so if you want to work on this it might be useful to coordinate there.. @magdmartin no this was already merged and the problem is still here. That would make a lot of sense! I am not sure whether this should be stored directly in the recon objects, or if it should be recomputed on the fly with the id and the URL template. (So, in the unlikely case where the template would change later on, the URLs would be transparently updated.). Storing explicitly is simple to implement, but it is going to make recon objects even heavier (they already take up a lot of memory, I think). Potentially we could take up this opportunity to refactor the Recon objects to keep the service and remove identifierSpace and schemaSpace which would be retrieved from the service at runtime. But that is more involved: reconciliation services are currently stored in the preferences, not in the project data at all\u2026 so it is harder to get right.. @isaomatsunami no problem, let us know if you need git help for that!. @jcpuzs1 sure, you can do that! and we would be very grateful.\nYou can either do the pull request yourself, or you can use Weblate as pointed out above by Martin. I recommend the latter, as it is much easier. If you use your github account to sign up for weblate, you will get exactly the same result (a commit attributed to your account in a PR on this repo).\nLet us know if anything is unclear.. @jcpuzs1 sure - please only use weblate, do not file a PR yourself.. @agentzero1 just go here and follow the instructions:\nhttps://hosted.weblate.org/engage/openrefine/. I would be in favour of not including the bound, just to be consistent with forRange, Python and a good deal of other programming languages.\nrange(10) should return the array of the first 10 integers.\nBut it is clearly not a hill I want to die on.. @joanneong I think it would be cleaner to return arrays of integers rather than strings. This would be more in line with the spirit of the function, and other implementations (for instance in Python).\nConcerning the test case: I think that range(1, 5, -2) should return [], not [\"\"]. Again that is for consistency with other implementations.. @jcpuzs1 hi - I'm not sure what is going on here? Why do you revert your work?. hmm, I'm not too sure about the point of this PR. If @fpompermaier wants to work on Beam integration in a branch of this repo, then he should be given commit rights to that branch (I don't see why it would be easier here than in fpompermaier:apache-beam though).\nIn any case we should keep pull requests for actual discussions about changes, where code is properly reviewed.. @thadguidry yeah let's just give him push access!. @fpompermaier hmm it's not clear to me why you are duplicating most of OpenRefine's source code in a new folder? didn't you want to break down the existing model in reusable parts?\nThe advantage of working on the existing files means that your commits are more portable: for instance, if you realize that getters are needed somewhere in the existing classes, there are chances that we might want these changes to end up in master. If you work on a separate copy (as done in this PR), we simply cannot cherry-pick the commit\u2026. @fpompermaier yes I can see this is not a normal PR - that's why I don't want you to do any PR at all! Just push to your branch, do your experiments, and that's fine. There is nothing to review here, so it's better if we can focus on PRs where review is required.\nIf you want to raise issues about the structure of OpenRefine, feel free to use the GitHub issues for that: we can discuss there.. It's not clear to me that it is possible to catch errors selectively like this: when GREL functions are evaluated, their arguments are already evaluated (so if there was any error upstream, the downstream function is not evaluated at all). This is related to other issues, such as #1145: our boolean operations are not lazy (which is annoying)\u2026 To solve this properly, we would need to redesign the way GREL is evaluated.\n. Hi. You can do it here: https://hosted.weblate.org/engage/openrefine/. @vinzruzell thanks! but there is no need for this PR as you have already submitted your translations via Weblate (#1510).. yes, just click on the link I provided (#1510), read there and see that it has been merged.. closing this PR as the changes have already been submitted in #1510.. Hi @vinzruzell, that is just how git works. If you don't master git fully I would recommend you to use Weblate only.. hi @kerim020, thank you for this but I don't think it would be convenient to have many different languages in this file. The development team does not master Turkish so we unfortunately have to require that issues are filed in English.. Oh okay - I had not noticed the copying indeed. So I think this should solve that particular issue - but I am still worried about the overall instability brought by the Data Package PR. What is now clear is that it has introduced bugs that are unrelated to Data Packages.\nThe risk is high that some of the bugs it has introduced have still not been discovered and could make it into the next release. I tried my best to scrutinize the PR but it was very hard given its poor state. The fact that we have serialization issues is serious - potentially, that could lead to corrupting users' projects and losing data. This is a high risk to pay for a feature that will go mostly unnoticed given the lack of integration in the UI.. * Each update of dependency should be made in its own PR, explaining why the update is needed and assessing how the library is currently used in the code. It does not need to be extremely detailed, of course: it's just about assessing what are the sort of changes made in the library (any breaking ones) and see what sort of risk it implies for us. For instance, when we updated Jython it was fairly clear that we were touching a fairly well isolated part of the code base, and we saw that their new release included mostly bug fixes, so that's good. It's just an analysis that needs to be done.\n No dead code should be introduced - currently there are loads of constraint checkers that are not exposed at all in the UI: it's much safer just not to include them at all.\n If the Data Package library we rely on is not quite ripe yet, fix it. Yes, that takes time, but it's necessary to avoid ugly hacks like the use of reflection to handle datatypes.\n More unit testing. A lot more.\n Commits should reflect logical changes and have meaningful commit messages. That makes reviewing much easier: you can just look at each commit in order, understand its logic and assess it independently of the rest. It's impossible to do that if the PR is crippled with unrelated commits and merges: you have to review the final diff and it's much harder to spot problems.\nGenerally speaking, just pretending to do some quality software engineering\u2026\n. Well, it's related because it was introduced by your PR, but take the standpoint of a user: they just want to import an XML file and they don't care at all about Data Packages. These are two unrelated things for users. What I mean is that it is a lot more serious than any bug in the Data Package exporter for instance (which are not going to disturb any existing user workflow). And the other issues (#1517 and #1508 ) are even more unrelated\u2026\nSorry if it sounds like I am trying to teach you a lesson - I was just trying to give concrete points to @thadguidry. I am just trying to point out some elementary principles that could have helped avoid the critical bugs that you have introduced in OpenRefine. Bugs happen, it's a normal thing - but these ones are really serious and you are getting paid for this work. There is just a minimum quality standard that should be met.\nI clearly have a lot to learn myself about good practices in software engineering - and I will soon submit a PR for the Wikidata package where your criticism will be valued. I am happy to skype today if you want to discuss things directly.. @jackyq2015 sorry yes, I should have made clear that I am actually more worried about the other issues #1517 and #1508 than this one\u2026\nAnd just to be clear: the tests that were already in place in OpenRefine are really sparse (the test coverage is so low!), so it really does not help us in our work\u2026 typically, this bug should have been caught by an existing test for the XML importer (one that you should not have needed to write)!. oh @tfmorris, good to see you! please do stick around if you have time!. hi @fokky - thanks for this report, I couldn't agree more!!!\nThe reconciliation service does use Magnus' autodesc, but currently only on items that do not have a description yet. And I think the feature might even be broken (maybe by a change in autodesc's API? I have to investigate).\nOverall I think these overviews could be improved a lot. The question is: beyond descriptions, how should we pick the \"interesting\" properties that users will find useful to disambiguate items? It's possible to pick birth dates manually for humans - but what about other domains? I would ideally like to have a generic approach that works everywhere.. yeah I already use P1963 to suggest properties to fetch in the data extension dialog so that would make sense\u2026 but sometimes there are a lot of them! so the preview dialog should probably be extended to fit a bunch of them.. @fokky I have fixed a bug in the call to autodescribe - you should now see more of them in OpenRefine (no need to update anything on your side).\n@thadguidry I still don't see exactly what you mean by your \"complete Recon Panel\" - could you describe that a bit more? Maybe with a drawing?. @thadguidry I kind of get the broad idea but it's still fuzzy, sorry  - I guess we can discuss that directly at the next meeting.. @ettorerizza really nice idea! and it should be fairly simple to implement too. I'll give it a try.\n@thadguidry okay, that's much clearer, thanks!. I quite like the idea of showing more metadata without any click or hover - I guess just including descriptions (Wikidata's native ones or Magnus' auto-generated ones) along each match candidate would already help a lot.. @thadguidry yeah, but the simpler the better, both for the servers, the users, the reconciliation service providers, and the OpenRefine dev team!\nI think we have identified a few cheap changes that will make a big difference - once this is in place we can think more about the more involved stuff.. @fokky @ettorerizza Here is what you get when using m.wikidata.org to do the previews:\n\n(you can try it yourself with this temporary reconciliation endpoint: http://ulminfo.fr:3893/en/api)\nMy gut feeling is that it is not condensed enough so I have made a version where the view is scaled down, but unfortunately it renders as blurry (at least for me):\n\n(you can try this second version at http://ulminfo.fr:3894/en/api).\nLet me know what you think\u2026. @ettorerizza When you say that \"the window display needs to be improved a bit\", do you mean that the position of the preview window is not optimal? I'm not sure what we can do about this\u2026 I guess it will be solved once we have a dedicated reconciliation UI that does not rely on pop-ups.\nDo you prefer the blurry zoomed out version or the un-zoomed clear one? Do you think it makes sense to replace the current preview with any of them?\nNote that we can also change the dimensions easily (for now it's 500x400px).. I plan to work on this at the Wikimedia Hackathon 2018:\nhttps://phabricator.wikimedia.org/T193875. Here is an NPE I got:\n```\n14:38:51.813 [                   refine] GET /command/core/get-languages (1174ms)\n14:38:52.037 [ GDataImportingController] doListDocuments exception:java.lang.NullPointerException\n    at com.google.refine.extension.gdata.GDataImportingController.listFusionTables(GDataImportingController.java:152)\n    at com.google.refine.extension.gdata.GDataImportingController.doListDocuments(GDataImportingController.java:97)\n    at com.google.refine.extension.gdata.GDataImportingController.doPost(GDataImportingController.java:67)\n    at com.google.refine.commands.importing.ImportingControllerCommand.doPost(ImportingControllerCommand.java:62)\n    at com.google.refine.RefineServlet.service(RefineServlet.java:178)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n    at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n    at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n    at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n    at org.mortbay.jetty.Server.handle(Server.java:326)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:923)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:547)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\n (224ms)\n14:38:52.756 [                   refine] GET /command/core/get-version (719ms)\n```. @jackyq2015 I propose that we merge this PR as is - the OAuth credentials can be used for dev. For releases, it would be good to generate fresh ones, so that invalidation of these credentials do not affect users. What do you think about that?. Well in OAuth it is always possible for a provider to revoke the application credentials (the ones that you have put in the source code) if these credentials are misused - typically if they are reused in another application for malicious purposes. I know you have not found anything in the docs about that, but that is just how OAuth works in general. That's why I would be in favour of changing the tokens before the release - the tokens you put in the packaged versions will not be as visible as the ones in the source code. As I said before, I think this is quite important, otherwise we will be quite vulnerable.. @jackyq2015 here is what I propose:\nWhen building the packaged versions of OpenRefine (when we make a release), we change the OAuth tokens (without committing that change to git). In this way, the compiled jars contain a different API key than the one that is on GitHub. It is still possible to extract these API keys from the jars and classes, but these keys are not as visible as the one that is on GitHub.\nIf Google is not happy about this API key being distributed publicly on GitHub, I would not be surprised if they revoke that key - but that would not affect the users because they would be using a different key.\nIs that clearer?. I just don't see any solution where OpenRefine will fully comply with Google's terms and I am therefore proposing a way to mitigate the risk caused by this situation.. @jackyq2015 thanks! I have added my comments there.. @jackyq2015 did you see my comments at #1527 ?. @jackyq2015 given that the previous fix has failed, I would prefer that we err on the side of caution on this - the UI was behaving like this before, so it's possible that someone is doing the same thing from a different client somewhere. I mean, if we are really going to do this separation of backend and frontend, we need to be serious about treating the two as independent systems.. @thadguidry hmm\u2026 did you just merge this PR without discussion? I have reverted it.\n@jackyq2015 I think it would be better that the server accepts JSON where the limit is either an int or a string. That would be more robust, for instance if people reuse existing JSON edit histories or use OR as a server for batch processing.. @thadguidry yes, that's why it's really important to make sure we are maintaining compatibility with previous versions.. @jackyq2015 it just does not cost anything to add this check\u2026 it is just about backward compatibility with the UI. No I am not aware of a client that still does this, but who knows? It's cheap for us to keep things compatible, so why would we break them on purpose?. @thadguidry sure! it's clearly not a hill I want to die on. I completely agree with you that it's important to improve our communication. It's important that things get discussed. Be wordy! PRs are meant to be an occasion to discuss things and help each other (or at least that's what I would like to see) but that does not work if we merely use them just because we cannot push directly to master.. Ah, glad to see such a nice long answer! You are making good points, I have removed my commit.. @jackyq2015 @thadguidry thanks for the comments so far! I forgot to mention that it can good to backup your workspace to test this (see the notice added in the PR description above).. @thadguidry about the behaviour of the reconciliation suggest service when moving the dialog, this is a generic problem that we also have at other places in the UI (for instance the reconciliation dialog or the \"search for match\" dialog). I agree it would be nice to fix it but I propose we do that in a separate PR (with the rest of the improvements on reconciliation, like adding descriptions\u2026). @thadguidry which ones specifically? It is flooded by cosmetic javascript issues.. @thadguidry yeah - but typically the issue you point out is debatable (some prefer not to have visibility modifiers there). I have reviewed the java issues and don't find anything compelling\u2026 Is there any particular issue you are worried about?. @thadguidry yeah, sometimes Codacy is useful\u2026 and sometimes it isn't. I guess if we wanted to use it better we could spend time configuring it better so that it only shows the issues we care about\u2026 but last time I checked their interface was half broken, I got errors when I tried to do that :-/ . @jackyq2015 @thadguidry thanks for your detailed feedback - I think I have covered it all now. Do you see anything else or should we merge this?. @thadguidry yes that message is annoying. The good news is that I now have commit rights in this library (Wikidata-Toolkit) so it's something I should be able to fix soon. (A decent chunk of the work for this PR was actually to refactor the datamodel of that library\u2026 something I had not really planned to do).\n@tfmorris about the documentation: absolutely, there is a lot of work to do on this. I don't think the documentation should be built into the software itself (that makes it harder to update), I was thinking about writing it in OpenRefine's wiki or even directly over at Wikidata.\nConcerning Wikidata-Toolkit: yes that's exactly what I am using (and contributing to). The datamodel there cannot be reused directly to model expressions though.. @tfmorris this schema module is the equivalent of the \"protograph\" or skeleton in Freebase terms: it's an expression that can be evaluated into the Wikibase data model. So you cannot reuse classes for the data model itself: you need to be able to represent variables (columns), and serialize them as such (to store the schema as an overlay model).\nThat being said, I do think some of the classes introduced in this extension could eventually make it into Wikidata-Toolkit (the maintainer is keen).. @thadguidry I value your feedback very much, I think you always make very good points as a user. As I have said before, I think you should leave the programming side of PR reviews to programmers (and not merge any PR that involves a code change without the approval of someone who contributes to the code). Codacy is nice, but you need to understand what is going on in the code to take advantage of its warnings. The ones you point out are spurious, and some of them are even wrong apparently: for instance, https://app.codacy.com/app/OpenRefine/OpenRefine/file/15383025410/issues/source?bid=6695960&fileBranchId=6695960#l615 gets the scoping wrong: I do want to declare var endpoint independently in each branch.\nThere is a lot of valuable reviewing work that you can do as a user: just pointing out the things that do not work as you expect, or those where you cannot even figure out how to use them (in which case, that's my own fault). Things about the appearance of the UI, the wording of the labels (as you did), and so on. That sort of feedback is absolutely critical - because that is what eventual users care about.. @thadguidry it looks like I offended you - that was not my intention. I am available to talk any time today.\nI am happy to rename that toolbar to grouptoolbar if that floats your boat, as a token of good will :). @ostephens @tfmorris @thadguidry I have made a quick tutorial to explain how the extension works. A lot more documentation needs to be written but it should already give a good understanding of the basic workflow: https://www.wikidata.org/wiki/Wikidata:Tools/OpenRefine/Editing\nIt would be great if someone could try to follow the tutorial (including performing the edits on Wikidata, which I have not done) and tell me if anything is wrong / unclear\u2026 Of course feel free to edit the tutorial directly too.. @tfmorris Detailed docs are now available at https://www.wikidata.org/wiki/Wikidata:Tools/OpenRefine/Editing#Reference_manual if you want to take a look.. @ettorerizza awesome, let me know how it goes! If you are not happy about the result it is easy to revert the changes in Wikidata.. And now two more tutorials ready to be tested - don't be shy, give it a try!\n https://www.wikidata.org/wiki/Wikidata:Tools/OpenRefine/Editing/Tutorials/Inverse_Listeria\n https://www.wikidata.org/wiki/Wikidata:Tools/OpenRefine/Editing/Tutorials/Working_with_APIs\nAlso, let's try not publicize this extension too much before the release, so that we get a clean occasion to announce all the new features and acknowledge our funder.. @thadguidry sure, that's because #1529 has not been merged - I will do that now.. @thadguidry hmmm, I'm not sure about your first issue - but as far as I understand it, it is something specific to the way standard reconciliation services are treated (which has not changed for years now). This PR does not change anything in terms of how reconciliation services work (or at least it should not!). Shall we move this to a different issue? (If you can confirm that you observe this problem in master too).\nFor the reference - it looks like your browser does not like this unicode character. That's very good to know, I will replace it by an actual image.\nAccording to your first screenshot, your schema is incomplete: you have not entered any property in your second statement. The UI should make that clearer indeed, I will change that.. @thadguidry what do you mean by \"Statement X\"?. absolutely!!. @thadguidry sure. I am actually thinking about making bigger changes: instead of having a small dialog (where it feels a bit too narrow for all this) I was thinking about making it a new tab in the main UI, so that users could switch from the main view (where they see the cells). There would be three new tabs: one for the schema, one for the issues and one for the statements preview. They would not show up by default (as we do not want to annoy the users who do not use the wikidata interface), but only when first triggered via the extension menu. Also, I will drop the quickstatements preview (which is only useful if you know quickstatements, and people should not have to know about QS to use the interface) and replace it by a preview of the actual statements, with the same look&feel as the schema. I think this is going to make the interface a lot clearer.. @thadguidry most of the UI improvements we discussed are implemented (the rest needs changes in the backend which I am going to do later)\n\n. @ostephens @ettorerizza in general if there is anything that you find unclear in the interface or the documentation, it would massively help if you could report it\u2026  I need to fix it, because nobody else will understand if you don't! We had a very productive hangouts with Thad yesterday, I'm happy to do that one-to-one with others to collect more feedback if you prefer this format.\nOtherwise this is what is going to happen at the next release: http://thehipperelement.com/post/93404286580/how-it-people-see-users-using-their-app-for-the  . Awesome! Thanks a lot for the feedback.\n the drag and drop to the empty schema makes a lot of sense\n save and close would make sense too\u2026 maybe we just need a \"save\" button that closes, and a \"cancel\" button that leaves without closing (I don't think the \"reset\" button is really needed)\n the low edit rate is intentional (high edit rates hurt the servers) but it can be adjusted, I will discuss that with the Wikidata dev team\n for now we don't store the edits locally indeed - I am not sure what it would look like?\nFinally, I have left a message here: https://www.wikidata.org/wiki/Wikidata:Edit_groups/OR/27b276a - did you get a notification for that? (This sort of message is quite experimental too).. that would make sense - or at least point to a video tutorial, just like the default text we have when there are no facets.. @ostephens thanks a lot! both points make a lot of sense and I will address them.. New preview tab (work in progress):\n\n. @jackyq2015 sure, I'll just rebase to keep the history clean. New UI, more tightly integrated with the rest of the UI. The main advantage is that you can use facets when inspecting the edits and issues. And there is spaaaaace to breathe.\nTo get the statements previews and issues to be updated when the facets change, I have added an extension point in the core JS (rather than monkey-patching that code as GOKb had to do).\n\n. @ostephens Cool! I haven't seen your edits show up in EditGroups, did you push them via QuickStatements maybe? Oh I hadn't realized that you were User:Owenpatel, okay ^^\nFor novalue and somevalue claims, you are correct that they are not supported yet. It's not clear to me how that should work:\n either we only let users add claims that are always novalue and somevalue in the schema (by replicating Wikidata's interface)\n or we make it possible to generate novalue or somevalue for variables too. I'm reluctant to translate nulls or empty strings to these values because I think users rarely mean that, so we would probably need to define a special string that we never expect to represent an actual value, like #Wikibase:NoValue or something ugly along these lines\u2026. Woot!? For me it does link to Wikidata. That's very puzzling. Do you have more info about in which context that has happened? Is it just for the subject of the item or also the values?\nAh I can see you have created new items - it must be related to that. I will investigate.. I assume the issue comes from the operation used to mark the cells as new items. Somehow the identifierSpace must have been lost in that process and the default Freebase space was used instead. I'd be interested to know how @ostephens has marked the cells as new? Or maybe the error was introduced by the code that updates the cells to their new Qids once they are created, but I don't see how that is possible. In any case, I'll add a check to make sure cells reconciled to a different space are rejected at evaluation time and add a warning for that.. @ostephens yes, the problem about refreshing column variables should have been fixed by the commits I pushed yesterday. There are other issues with this new UI for the schema (for instance, if you undo some operations that changed the schema, the schema editor is not updated to reflect that\u2026 but it's tricky because we might not want to override any unsaved changes there). I am thinking about adding buttons to save the schema (as we had before) and to clear unsaved changes.. @ostephens I have added the buttons to save and clear changes (plus a bunch of other UI tweaks).. @ostephens yes, the library used to edit (Wikidata-Toolkit) has built-in throttling features. The edit rate is 30 edits/sec on the long run (but the first few edits get through more quickly). This can be observed at https://tools.wmflabs.org/editgroups/?tool=OR .. I have added the number of edits made both to the preview tab and the upload dialog as suggested by Owen.. I am going to merge this in two days unless there are any objections.. Thanks! I should definitely have tested this on a fresh workspace\u2026. I have added a commit that should solve the issue - let me know if it works for you.. @DavidFichtmueller thanks for the PR! Honestly we are not happy with the way HTTP requests are currently made - there are a number of other issues related to that (like #1410).\nI think we should just migrate to a modern HTTP client library that does all these things for us - all these issues should be handled by that sort of library, not OR itself.. @thadguidry feel free to propose new properties at Wikidata\u2026 we're always happy to discuss new properties, especially if it's not yet another external id\u2026 and then try to populate them with the wikidata extension maybe?. @jackyq2015 @ostephens happy with merging this?. It's a good question, I am not so sure.\nWe also need to keep in mind that the current location of the \"Facet by blank\" button is hidden deep down the column menu. I would be in favor of putting the new facets somewhere more accessible (in the main Facet menu).\nMaybe one solution would be to put the new facets in the main Facet menu, and keep the \"Facet by blank\" burried where it currently is? This way, seasoned users will still be able to dig for that, and new users will first discover the new versions, without being immediately confused by the \"Facet by blank\" (because you really need to be looking for it to find it\u2026). @ostephens sure\u2026 but when introducing new buttons, wouldn't it make sense to put them at a more convenient place directly? That would not move any existing option around and I think it would help reduce the confusion. But it's just my intuition.. @joanneong For now the UI is completely untested indeed. I would not go down the Windmill route given that is completely abandoned. If you want to try to set up Selenium that would be fantastic - but it will probably take some effort. Sauce labs can be used in conjunction with Travis to run browser tests in builds. But I think this PR can be accepted without UI tests.\nIf you are keen to work on testing, the backend itself is mostly untested (the test coverage is really poor and tons of bugs routinely go through) so that might be an easier way to improve the tests.. @tdombos thanks a lot for this translation! keep up the good work!. Would it make sense to be able to enable/disable displaying this? I think that is quite a big visual change that might not be of everyone's taste\u2026 (incidentally it would mean that I need to shoot again all the screenshots for the docs of the wikidata extension ^^). argh - no option to deactivate this? are we 100% sure about that?. @ostephens I don't think you need to revert this, I think a toggle could just be built on top of that with little effort. It can be quite minimal: just a button that, when clicked, toggles a class on a div that contains the grid, and a CSS rule that puts the nulls in display: none when the parent div has that class.\nI would be in favour of making this feature opt-in for the coming release (so, disabled by default). Based on user feedback we could consider enabling it by default in the following release.\nBasically I'm all in for a better treatment of nulls and empty strings. This is actually something that would be highly beneficial to the Wikidata extension. But I fear that some users might be used to the way things currently are - we don't want to \"brutalize\" them by forcing on them this distinction that they not might need in their own workflows.\n@ettorerizza I'd be careful with such a transformation - it would break the records mode pretty badly, no?. @ettorerizza I agree that the documentation of the records mode is very poor (which is sad because it's quite a fundamental feature) - thankfully there's @magdmartin post but that would clearly deserve to be treated in the wiki. I agree with your analysis of the code - I thought I had read in the past that it relied specifically on nulls but that's apparently wrong. I think that if empty and nulls are conflated in such a crucial part of the code, it's yet another reason not to precipitate displaying the distinction to users\u2026. This is now possible here:\nhttps://hosted.weblate.org/projects/openrefine/. @jackyq2015 it has always been like this as far as I know. what is \"row-based importing\" exactly? The distinction between the rows and records mode only arises after project creation in my opinion. I think it makes sense that projects created from CSV file can be treated in records mode. It is possible for a tabular file to have a records structure before import in OpenRefine (this would typically happen in Wikitables, or some Excel files).\nThe default mode does not matter that much AFAICT, given that it just takes one click to change. I think we can close this.. Very interesting question! I guess the current heuristics were appropriate for Freebase, but not for Wikidata. What it currently does is count the number of times each type appears in the suggestions for the first few rows, and order the types by decreasing frequency.\nThe queries made to guess the types look pretty much like any normal query so reconciliation services are not expected to return something tailored for the type detection algorithm. That is usually quite nice because as a reconciliation service developer you do not have to worry about this part. But it makes it harder to tweak the type detection results without changing the global behavior of the interface.\nOne thing that could potentially solve the issue without changing anything in OpenRefine would be that the Wikidata service returns not only the direct types, but also the indirect types via the \"subclass of\" (P279) property, up to a certain level. In this way, more general types would emerge. The problem is that fetching these more general types is expensive, so we probably don't want to do it for all queries - ideally it should happen only during type detection (and even so - we want this part to be quick too!).. oh sorry I overlooked your proposal, just dumped my own ideas >_<\nSo, given a list of names in the column, how would you find the qids to feed to that query?. Ah right, I see, that would make sense. It would probably be quite accurate and efficient. The only issue is that this would be specific to Wikidata, so we would hard-code something for Wikidata in OpenRefine (which has been done for Freebase, but it's better if we can avoid that).\nI guess the \"clean\" solution to implement that is to add a new (optional) endpoint to the reconciliation API, that takes a list of strings and returns type suggestions. If the reconciliation service does not provide it we would resort to the current heuristics. That would enable us to use a custom algorithm for Wikidata. Potentially we could reuse that for the data extension feature too (so that we get more property suggestions, for instance in the case where the column was reconciled against no particular type).. @thadguidry okay, it seems that it checks that the cell values are equal before transferring the reconciliation results (which was not the case for me).. @tcbuzor the bug reported by @ettorerizza above looks quite important (congrats to him for finding it!), I think it should be addressed before merging. Support for records would be nice but is beyond the scope of this PR I think.. @tcbuzor or you could just change the table schema so that null values are accepted in all columns, maybe? SQL has support for NULL, so why not use it?. Can you give a bit more detail about how that would work? What sort of python expressions would you be able to write with this feature?\nIntuitively I would be more in favor of exposing a clean virtualenv inside OpenRefine's folder where users could directly write their python modules (or install existing ones) that would be accessible from the scripts.. @jackyq2015 I think the Z should also be added at parsing time if not present, then. My workspace is completely screwed by that\u2026. @thadguidry haha, thanks for the lecture ^^ I do have backups, that's not the point.. @jackyq2015 just noticed another place where this change causes a bug: in the Data Extension API, if the service returns a date in the previous format (which the wikidata interface does), parsing fails. That makes it impossible to fetch dates.\n```\n22:08:44.887 [                  command] Exception caught (856ms)\njava.time.format.DateTimeParseException: Text '2006-01-01T00:00:00+00:00' could not be parsed at index 19\n    at java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)\n    at java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1851)\n    at java.time.Instant.parse(Instant.java:395)\n    at com.google.refine.util.ParsingUtilities.stringToDate(ParsingUtilities.java:191)\n    at com.google.refine.expr.functions.ToDate.call(ToDate.java:88)\n    at com.google.refine.model.recon.ReconciledDataExtensionJob.collectResult(ReconciledDataExtensionJob.java:196)\n    at com.google.refine.model.recon.ReconciledDataExtensionJob.extend(ReconciledDataExtensionJob.java:122)\n    at com.google.refine.commands.recon.PreviewExtendDataCommand.doPost(PreviewExtendDataCommand.java:124)\n    at com.google.refine.RefineServlet.service(RefineServlet.java:178)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n    at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n    at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n    at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n    at org.mortbay.jetty.Server.handle(Server.java:326)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:938)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\n22:09:01.687 [           ProjectManager] Saving some modified projects ... (16800ms)\n```\nMore broadly, this is also an issue that affects the toDate GREL function: things like toDate('2006-01-01T00:00:00+00:00') worked in earlier versions, and not in master.\nIn short: please ensure that date formats that were supported before are still supported. Those are three different examples of bugs introduced by this change, how many do I need to collect so that this mess is cleaned up?. Well, I think it is in our interest that date parsing utilities accept a wide range of formats, so adding support for that format would definitely make sense regardless of the bug\u2026 How would it \"cause confusion\" to accept timestamps without a Z? That would make the toDate function more flexible.. The date parsing utilities class is used at various places in the code, so we should definitely make sure the fixes go there, and not just in the toDate function. Get date parsing right for once and then use that everywhere else. Making ParsingUtilities.stringToDate accept dates without Z does not \"cause confusion\", it just widens the formats accepted when reading dates in OpenRefine. This will be very useful to our users and authors of reconciliation services.\nThe metadata format for OpenRefine remains the one that is defined by the serializer, which includes Z. No confusion about that.. @Vern1erCa11per it should be merged in the coming days unless anybody else disagrees. I am going to merge this in two days unless there are any objections.. note that indentation is important in python: make sure you get the line breaks and spaces right!. That does not solve the bug I reported. The behaviour of stringToDate was changed and this breaks the data extension operation, among others. Please make sure that stringToDate supports all the formats it supported before.. Hi @jackyq2015, have you seen my report in #1564? Just reconcile a column (for instance containing the names of famous people) and fetch dates from that (for instance their date of birth, P569) and you will observe the error. This can be observed from the master branch.\nSo, here is the situation. In commit 23ec54b78 you have changed the behaviour of ParsingUtilities.stringToDate. Many parts of the code rely on this function to parse dates. Therefore, this potentially introduces bugs in all these areas. I have shown you two instances of such bugs, but of course we should not fix just them: they are just the symptoms and we should fix the root cause. The root cause is that ParsingUtilities.stringToDate previously supported ISO dates with offsets, and now it fails on them. So, it would be great if you could fix that.\nIf it helps, here is a test case that should pass (it passes before 23ec54b78 and breaks after):\n@Test\n    public void parseOffsetDate() {\n        String offsetDate = \"1958-08-29T00:00:00+00:00\";\n        OffsetDateTime parsed = ParsingUtilities.stringToDate(offsetDate);\n        OffsetDateTime expected = OffsetDateTime.parse(offsetDate);\n        Assert.assertEquals(expected, parsed); \n    }\nIf anything is still unclear, do let me know (I am happy to discuss that directly over hangouts for instance). If you want me to take this over, I can also do that.. Hmm, it's curious that you cannot reproduce the issue\u2026 Which JDK are you using? I can observe the issue with OpenJDK8.\nI'll go ahead and propose another version that should hopefully work for everyone.. Hi @Gautamshahi, this issue is specific to the RDF extension, which is not maintained by our team. As far as I am aware nobody maintains it at the moment.. Can someone make an approving review so that we can merge this?. I am going to merge this in two days unless there are any objections.. @msaby what behaviour would you expect instead?. First, you should do your second blank down operation in records mode - otherwise you should not expect records to be respected. The result that you are describing above shows that you are using the rows mode for the second blank down operation, which is clearly a mistake I think.\nSecond, you can get your desired behavior by switching back to rows mode after your two blank downs, and removing empty lines.\nHere is the desired workflow:\n[\n  {\n    \"op\": \"core/blank-down\",\n    \"description\": \"Blank down cells in column city\",\n    \"engineConfig\": {\n      \"facets\": [],\n      \"mode\": \"row-based\"\n    },\n    \"columnName\": \"city\"\n  },\n  {\n    \"op\": \"core/blank-down\",\n    \"description\": \"Blank down cells in column gender\",\n    \"engineConfig\": {\n      \"facets\": [],\n      \"mode\": \"record-based\"\n    },\n    \"columnName\": \"gender\"\n  },\n  {\n    \"op\": \"core/row-removal\",\n    \"description\": \"Remove rows\",\n    \"engineConfig\": {\n      \"facets\": [\n        {\n          \"type\": \"list\",\n          \"name\": \"gender\",\n          \"expression\": \"isBlank(value)\",\n          \"columnName\": \"gender\",\n          \"invert\": false,\n          \"selection\": [\n            {\n              \"v\": {\n                \"v\": true,\n                \"l\": \"true\"\n              }\n            }\n          ],\n          \"omitBlank\": false,\n          \"selectBlank\": false,\n          \"omitError\": false,\n          \"selectError\": false\n        }\n      ],\n      \"mode\": \"row-based\"\n    }\n  }\n]\nSo I think your \"bug\" should rather be phrased as a feature request: should the blank down operation remove the blank rows that it generates? I think that is a feature worth considering. But the Blank down operation is really not designed for cases like that. Blanking down is designed to be used on keys of records, not on the values. What if your gender data was not ordered for each city? You would get gibberish.\nAnother feature worth considering is changing how records are generated in the presence of empty rows. But that is independent from this PR.. @msaby can you post a CSV representation of the initial state of your project, and the JSON operation history?. @ettorerizza I agree that is not super intuitive - it would make sense to make an issue for this (if we don't have this already\u2026) but I think we should be cautious about changing this logic as it is likely to affect existing workflows.. @msaby I would be convinced that there is a bug if you provided a copy of your JSON workflow or project. I cannot reproduce the issue on my side. I still think you ran the second blank down operation in rows mode by mistake (or with a previous version of OR). Please prove me wrong by providing evidence of the contrary!. Hmm okay, there is definitely something strange happening here. Thanks for the JSON workflow, I will try to debug.. Hi @joniarroba, thanks for this report. The Portuguese translation will be available in the next version of OpenRefine, which should arrive soon-ish. It will look like this:\n\n. hi @Vern1erCa11per, thanks for starting this interesting discussion.\nI think you are using the OverlayModel in a way that is quite different from the use case it was designed for (storing the Freebase Protograph or the RDF skeleton). As far as I can tell, the idea behind it being an overlay means that it is a data structure that it independent of the table data itself (it just stores metadata).\nBut why not! We can definitely explore exposing callbacks that are similar to those you mention. We should do that in a way that is backwards compatible (for instance by providing default blank implementations for the new methods that we add, so that existing extensions that use the overlay model do not break). We need to explore the various use cases and come up with a signature that works for everyone. For instance, it would be useful for the schema overlays (RDF, Wikidata) to be notified when a column is renamed or deleted (because these schemas store references to these columns), but that does not seem to be possible with your current approach.\nIn https://github.com/Vern1erCa11per/OpenRefine/commit/35b7e2e13a29d5366b94718e606f19947663eeef you add the hooks in various Change classes, I would prefer to have it only in one place (where changes are applied). In general, any update to the table data is always performed by applying / unapplying changes, so I guess there could be just one or two method in the OverlayModel like:\npublic void onApplyChange(Change change);\npublic void onUnapplyChange(Change change);\nIf necessary we could extend the Change interface to make it easier to know what sort of change it made (for instance, which column(s) it modified).. @jackyq2015 not that I am aware of. Maybe @thadguidry knows of one?. I am going to merge this in two days unless there are any objections.. Agreed, let's do that.. Explanation of the above:\nI think it is a design mistake to have allowed arbitrary java Objects as cell values. It would have been much safer to restrict it to a specified list of java types, so that the compiler would enforce correct typing of all values. I believe this choice was made to make it easy for extensions to manipulate complex objects - but this extendability does not work well in many parts of the code: there is no simple way for an extension to implement the expected behaviour of these objects in rendering, faceting, and other operations. So type safety should really prevail here. We have already paid the price of this design flaw when migrating the dates to java 8 classes.\nSo, adding a new possible type is dangerous - the compiler cannot spot for us the other places in our own code that we would need to adapt for array handling. And the compiler will not let extension maintainers that they might need to update their own code too.. Hi @Gautamshahi, reconciling via SPARQL endpoints is a feature that is added by the RDF extension, which is not maintained by our team.. Thanks for catching this before the release! I'll investigate.. @jackyq2015 I'm now totally confused - I don't get why we made changes to date parsing in the first place. My understanding was that you had changed date parsing while dropping support for JDK7, but as far as I can tell the code we had before that runs fine with JDK8. I am tempted to just restore ParsingUtilities.stringToDate to the version before your change 2afdcbf54 because I don't see what is wrong with it?\nMaybe we should have a call about this because I feel like I have misunderstood something\u2026. So, here is how I understand the situation:\nwhen @jackyq2015 worked on last modified dates he undertook to migrate from old-style Date and Calendar classes to the modern implementations, which was not necessary in the migration from JDK7 to JDK8 but is generally a sensible thing to do. However, this work has not been finished so there are discrepancies in the code (some parts still work with the old-style classes, some use the new ones), which causes the bug observed by @ostephens (which btw is not specific to wikidata-extension, it can be observed in master too).\nThere are two ways out of this situation:\n- migrate everything to the new classes (which can potentially cause some unexpected side effects, and it is a lot of work)\n- revert the changes and go back to the original classes (which is a step backwards, but might be safer if we want to have a new release out quickly)\n@jackyq2015 any info would be welcome\u2026. @thadguidry a full migration would be very costly (as you said there are many different classes which still use that). There might be ways to isolate that usage (for instance, it is probably hard to migrate the internals of CalendarParser to use the new classes, but it should be doable to just change its interface).\nPersonally I will not have time to do a full migration before at least 6 weeks\u2026. toDate('2017-01-01T00:00:00Z').type() gives java.time.OffsetDateTime whereas toDate('2017-01-01').type() gives date because since your partial migration the toDate function can return various types depending on the input\u2026 I can provide a quick fix for that but it does not fix the root cause.\nThis situation where both old and new date classes are allowed as cell values is dangerous - now I realize that this change was quite ill judged. It was a mistake to do it lightly: in most applications, you could have relied on the type checker to do the migration, but because OpenRefine uses Object for cell values, we cannot rely on that.. I don't think dates should be stored as strings - this would go against the current design of storing strings, numbers, patterns and dates as different Java objects. For now, we need to make sure all dates stored in cells use the Java 7 classes, not the Java 8 ones, so that we don't break compatibility with GREL and extensions which rely on that. This is especially important because this introduces bugs that cannot be detected at compile time.\nIf we were months away from the next release we could try to do a full migration but right now I don't think we can take that risk.. Hi @Gautamshahi, reconciliation via SPARQL endpoints is a feature of the RDF extension, not OpenRefine itself. This extension is not maintained by our team. The most up to date fork is https://github.com/stkenny/grefine-rdf-extension, you can try this version and submit your issues there.. This will be redundant with your migration, so I am closing this one.. @thadguidry @jackyq2015 I think we should have a call about this - just to make sure you understand the implications fully. I don't think there are \"differing opinions\" - but taking the decision requires understanding what it implies\u2026. I was hoping to get a release before 20th May, so that it's ready for the Wikimedia Hackathon 2018, but that's not going to happen then. I guess I'll just release a snapshot with the fixes and the extension then.. @jackyq2015 thanks! But this is a big change that needs thorough reviewing and testing, so even if we manage to do it quickly we should not rush to release that\u2026. Hi @LisovaOlena, the issue you have filed is empty. I am closing it, feel free to get back to us if you actually have something to report.. Merging this in a few days unless anybody has any concerns. The main reason why it is hard to get rid of calls to the service is that we currently need to fetch both the labels and the types for each item. We could in principle have an operation that blindly reconciles the cells without any calls, using the ids as labels and without adding any types, but it would be a sort of poor-man's reconciliation\u2026\nAs for the speed: the Wikidata service should not do any search API call when you reconcile a column of Qids: it only needs to fetch the labels and types (which it does by getting the full JSON of all items in each batch). So it should be significantly faster than reconciling free text. Any improvements there are welcome though.. After working with a big dataset I have come to agree with @lucaswerkmeister that it is quite frustrating to have to wait for Qids to be reconciled. I think we should give maximum control to users and provide this operation, with the appropriate caveats that no checks for validity are done. (But anyway, it is also possible for reconciliation results to get out of sync with the reality online, so the rest of our code should not assume that the Qids always remain valid.). Let's just merge this and hope that the inevitable bugs that will arise early enough\u2026. So I have looked into this and this bug comes from the original Freebase implementation that I have reused: the serialization and deserialization codes are just inconsistent. I'm trying to think of a fix that will both fix serialization for new changes while fixing somehow deserialization for the changes that have already been written.. Indeed this constraints needs updating as we no longer support Java 7 or older. Would you consider making a pull request for that?. Okay so there was probably a problem with my #1568, let me investigate.. @ostephens I cannot reproduce this. Based on your first screenshot above, it looks like you have not used the Wikidata reconciliation service, but another one (maybe VIAF?). So that would explain why you cannot create new Wikidata items based on these reconciled cells.. The ideal thing to have in this context would not to have this option in the column menu (or have it disabled)\u2026 I think the menus would be much simpler if they only contained the options that are relevant at the given state of the table. For instance, for unreconciled columns, the \"reconcile\" menu should just propose to start reconciling - all the other options are meaningless I think.\nSo the broader task would be to figure out how to adapt these menus to the column, maybe? But I agree we can also change things in the backend to prevent these new cells from being created.. In case of a partial reconcile of a column, you still get a ReconConfig object in the column metadata, so matching unreconciled cells to new should not be an issue.. Exceptionally merging without review because it is necessary to review other PRs where the test fails.. I thought about the workflow for the consumer-only tokens and it still looks a bit blurry to me. To request an owner-only token, I assume you must be logged in as the owner. So, the first time a user uses OpenRefine, they would first need to login with login / password via OpenRefine so that we can request n owner-only token, and then have them go through the OAuth process with these credentials\u2026 so they would log in twice.. Okay. By the way, do you know if it is possible to edit with a bot flag via OAuth? Maybe there is a particular scope to request for that, but at least for https://tools.wmflabs.org/oabot/ it seems that the bot flag is not added to the edits\u2026 So we should make sure to get the right scopes when registering the consumer.. yes, but even when both conditions are met, User:OAbot's edits made via OAuth were not flagged.. Makes sense, so if the callback is supplied when applying for OAuth credentials, it does not have to be supplied by OR during the OAuth process. still, if you are cloning from a terminal and using a text editor to read the README, you might not want to have to click on links to find out that all you need to do is ./refine (if you already have Java and Ant installed, that will build OR for you before launching it).\nBasically all the information this person was looking for was this ./refine command - it does not take much space to mention it in the README. I am clearly not opposed to having more instructions on the wiki of course.. also, the contributors we have in the readme were copied from GitHub so that goes out of sync regularly (unless someone vouches to keep that up to date?)\u2026 If we want to credit the original creators there, that's fine with me, but the list of all committers does not belong there IMHO.. last grief about this authors list - are we sure these people want their email addresses listed like this, without any spam protection? I know it's also available from the git logs, but still, some might find it a bit impolite.. @ostephens I'm surprised you need to run ./refine clean - for me (and this user) it is sufficient to do ./refine from a fresh clone. What happens exactly for you if you don't clean first?. hmmmm, interesting oO. I guess it would be interesting to know exactly what ./refine clean does for you on a fresh clone (maybe just dump the contents of a find . call before and after, and look at the diff?). The failing tests are due to Wikidata changing the way they store some constraints\u2026 Grrrr.. @jackyq2015 because the former rows and the new rows are also serialized - so the DataExtension objects themselves are actually redundant with that. See the unapply code for instance: no reference is made to the DataExtensions.. If what you mean is \"convert a series of OpenRefine operations into equivalent SQL queries\", then this is basically infeasible as far as I can tell.\nOpenRefine's operations have not been designed to be easily translated to SQL. Some of them can, but for the majority of them this would be virtually impossible. OpenRefine's data model is just very different from SQL's so there is no hope to build such a seamless integration between the two.. Closing as this is not something we are going to do directly.. @susannaanas Thanks for this brilliant bug report. Here are my first reactions when opening your project:\n\nThe reconciliation statistics (green bar) is consistent with the state of your first column: about half of the items are matched to existing items (dark green), and the other half is matched to new items (light green).\n\nThe first column is underlined in green in the schema panel (and your schema looks great).\nHowever, there seems to be a different issue: the reconciliation space (which is supposed to be \"http://www.wikidata.org/entity/\") is not set properly, on many cells. It is not clear to me where this problem is coming from, I will investigate.. Indeed - it seems that we have multiple operations, and I only fixed one of them recently: ReconJudgeSimilarCellsOperation should not have this problem.\nReconMarkNewTopicsOperation still inserts null values for new items, and\nReconJudgeOneCellCommand seems to be called incorrectly (without the identifierSpace and schemaSpace set), probably from the UI.. @susannaanas Ok so, as a temporary measure, here is what you can do to avoid the issue: never match an individual cell to a new item. Always use either the double-tick button for the new item (which also marks as new all the other cells in the same column with the same content), or the action \"Create a new item for each cell\" from the Reconcile menu.\nGiven the current state of your project, I would advise you to:\n- clear all reconciliation data in the entire column\n- reconcile it all again\n- mark the relevant items as new again (\"Create a new item for each cell\" should be the easiest way to go)\nThanks again for filing this bug - it should be solved in 3.0.. Actually, to fix the issue, we could also ensure that if we already have reconciliation data for the cell, but the identifierSpace is null, then infer it from the ReconConfig\u2026\nAlso, it seems that there is still a problem with the \"Create a new item for each cell\" option (still incorrect identifierSpace).. Right now it does not, but yes it should!. @susannaanas it's a significant amount of work to do it properly. The first step would be to make the reconciliation interface run on any Wikibase, then create a notion of \"profile of Wikibase instance\" which contains all the information necessary to run the extension (MediaWiki API, reconciliation API, settings of WikibaseQualityConstraints, EditGroups\u2026) and then forward that everywhere where this is needed.. sure, and that will need even more adaptations because the further tweaks in the data model (ids of the pages to edit will have a new format, but properties ids and items will be borrowed from Wikidata\u2026). for reference, this is a feature request I have heard quite often (especially at the Wikimedia Hackathon) so it would be good to keep this in mind as a mid-term goal.. @susannaanas it should now be easier to run the reconciliation interface for other Wikibase instances. I have added some docs and a Dockerfile at https://github.com/wetneb/openrefine-wikibase. Roadmap for this feature:\n- [ ] Work with the MediaWiki developers to have the MediaWiki API endpoint expose configuration of extensions (such as Wikibase, Quality Constraints). This is a long term goal, I expect it will take ages so it should not be blocking. Task: https://phabricator.wikimedia.org/T155155\n- [ ] Draft and document an initial version of the manifest format. The format should include versioning so that we can switch to something lighter once MediaWiki exposes more info. Task: https://phabricator.wikimedia.org/T197588 (2h)\n- [ ] Write Java classes to represent these manifests (with versioned implementation), with serialization and storage in OR's preferences. Each Wikibase instance should be linked to a default reconciliation service, used in the schema UI. The registry key should be something stable, such as the MediaWiki API endpoint or the manifest URI. (10h)\n- [ ] Create UI to list known instances, add a new one by providing the URL of a manifest. (5h)\n- [ ] Change schema serialization to include a mention of the Wikibase instance (not the full manifest). When not provided, fallback to Wikidata. (2h)\n- [ ] Percolate the instance through schema evaluation, quality assurance and editing. (15h)\n- [ ] Change the UI of the schema editor to let the user choose which instance to use. (3h)\n- [ ] Cleanup mentions to Wikidata -> Wikibase in the source, UI and translations. (2h)\nQuestions to be resolved:\n- which key to use for the instances? Because the workflows must be reproducible, the manifest URL must appear in the operations, so using only the MediaWiki API URL does not seem to be enough. But using the manifest URL means pinning down the manifest version (including constraint configuration, and other things that might change fairly regularly).\n- do we store only one overlay model or parallel schema for different instances? leaning to the former for simplicity of the UI.. @thadguidry I will remove some of them so that it reduces these bugs a bit. Merging this in a few days unless anybody disagrees. merging this in a few days unless anybody disagrees. Migrating out of this library is a lot of work that is going to cause major incompatibility with most extensions. It's something that we want to do eventually, but it's most likely going to happen gradually. It could be possible to migrate gradually, by isolating the parts that use Jackson / JSON: for instance, the Wikidata extension uses Jackson everywhere except where it interfaces with the core.. I have started to work on this. Here is my approach:\n Write tests to pin down the current behaviour of the serialization / deserialization functions for each Jsonizable instance. The tests are written with the current implementation, so they rely on org.json, but are designed with the migration in mind: the test cases will be easy to migrate to Jackson. When the classes can be both serialized and deserialized, the test generally consists in deserializing and re-serializing a given JSON payload, and checking that the result is equal to the original (as a JSON object). Sample JSON representations can be collected by hijacking the write method to dump every representation it writes to a file, and running OR on diverse projects.\n Rewrite some parts of the code where JSON objects are explicitly used are storing objects by introducing classes for the data they represent (example: engine configuration in operations). This is cleaner and will ease the migration to Jackson by reducing the footprint of org.json in the code base. List of places where this is needed:\n  * Engine -> create EngineConfig (also used in operations)\n  * ImportingJob -> create ImportingJobStatus\n  * Probably others, to be discovered later\n Annotate all classes for serialization with Jackson, without breaking the existing implementation. Thanks to the tests we can easily check that the JSON format is preserved.\n Annotate all relevant classes for deserialization and replace all calls to the static reconstruct, loadStreaming and other deserialization methods to the Jackson equivalents. This includes the serialization tests. Deserialization should be made as flexible as possible (use @JsonIgnoreProperties(ignoreUnknown = true) almost everywhere) to match current behaviour.\n* Remove all the old org.json-based (de)serialization code, remove the jar from the repository.\nThe target end state is:\n The Jsonizable interface is deleted.\n All classes that are implemented Jsonizable are annotated with Jackson configuration, similarly for deserialization. Most classes should not need any custom code for serialization or deserialization, the annotations should be sufficient. The serialization format is kept identical in all cases.\n* The registration mechanism for commands, operations, functions, overlay model and others is kept as is (this will require the implementation of various TypeIdResolver to interface cleanly with Jackson). However this registration mechanism will go away when we migrate out of Butterfly - this will require some changes to these resolvers.\nAs an extension/fork maintainer, here is what will happen:\n Compilation breaks because the Jsonizable interface does not exist anymore.\n Every class that interacts with OR needs to be rewritten to use Jackson for (de)serialization. This mostly applies to classes that are registered (such as operations or overlay models).\nIt would be useful to provide a migration guide to the extension maintainers, with example transformations of classes to explain the migration process.. Quick update on this: pull request #1755 contains all serialization changes, and deserialization is ongoing work in the same PR.\nIn many cases it is possible to use Jackson annotations to make the (de)serialization implicit with Jackson, which is cleaner, but in others the entanglement with business logic is just too deep to go down this route - so for things like importers I am just translating from org.json classes to Jackson classes (JSONObject -> ObjectNode and others). The end result is not as clean but it is faster and less error-prone. If we want to do more refactoring in this area it will still be possible later (and not harder than before).. Fixed by #1755.. The migration guide is available at https://github.com/OpenRefine/OpenRefine/wiki/Migration-guide-for-extension-and-fork-maintainers#migrating-from-orgjson-to-jackson. @thadguidry makes sense, done.. Thanks! That's an issue for https://github.com/wetneb/openrefine-wikidata. I will look into it.. @susannaanas the problem should be fixed now - can you confirm this on your side?. This was fixed in the reconciliation service.. merging this in a few days unless anybody disagrees. What the actual f***?!. Hi @pachamaltese - thanks for this PR! However, there is a problem: you deleted most files from our repository. For OpenRefine to work, we need these files to stay.\nCan you try making a new pull request without deleting any files? Thanks.. Ideally a PPA would help I think - it is more useful than just a .deb file. I am not familiar with Debian's policies but in an ideal world we could try getting in the official repository. But that is probably a lengthy process.. @ostephens yes, but it's a pretty big issue IMHO, especially as the UI was not updated for this facet.\nIt was a mistake to make this change, we knew it would break things\u2026 One way we could have made the move would have been to introduce a new facet (\"string facet\" instead of \"list facet\") with the new behaviour.. The problem is that users have no idea whether their values are stored as booleans or strings - this is something we do not expose (unless they are explicitly looking for it and create a facet to display the type).. Well, now that we have made this change we kind of have to be consistent and add a boolean facet indeed, but I think the end state will be less usable than where we started from. Maybe one way to mitigate this would be to display boolean values in a different color (similarly to the way dates are highlighted), so that it becomes more intuitive for users to pick the right facet (list or boolean) at the first sight.. Ah? Great, I did not realize that.\nThe main problems for me are:\n incompatibility with previous JSON workflows\n breaking change in 3.1 because the UI was not updated everywhere\n* frustration for users who will first use a list facet and realize they need a boolean facet (especially if they do not understand this directly and have to look it up / ask a question on the mailing list)\nBut yeah, a boolean facet would be quite natural now that we have taken this path.. That is another option, but I would rather call the current behavior \"string facet\" (I would expect a \"type facet\" to collect all string values into one facet choice, \"(string)\").. @yarl thanks for the report. Can you try to reproduce this with the latest version from git?\nInstructions to run the development version are here: https://github.com/OpenRefine/OpenRefine/wiki/Installation-Instructions#development-version. @yarl @SannitaSSJ this bug should be been solved in the latest release (3.0) - it would be great if you could let me know if you still encounter the issue (on new projects, created with the new version).. Closing this as it appears to be solved.. @ostephens I tried to fix the merge conflicts earlier but the tests did not pass after that. Could you have a look? It would be great to have this PR merged before 3.0.. The backwards incompatibility with previous workflows spotted by @ostephens worries me a bit. (Very good catch by the way!) Maybe we should only merge this after the next release, and make sure the changelog explains how to migrate existing workflows by adding the extra .toString() to the facet expressions.. @ostephens is this ready to be merged?. This is partly an issue of OpenRefine, partly of the reconciliation service.\nThis happens because the reconciliation interface does not return types in the suggest service. It should, but I still haven't got round to doing it yet, because it raises some performance issues.\nBecause it returns an empty list of types, the expression forNonBlank(cell.recon.features.typeMatch, v, v, if(isNonBlank(value), \"(unreconciled)\", \"(blank)\")) treats the entire cell as blank. So maybe we should just return a default uninformative type in the reconciliation service.\nIn any case the GREL expression could be improved to display something else in this case. Something like forNonBlank(cell.recon.features.typeMatch, v, v, if(isNonBlank(value), if(cell.recon != null, \"(no type)\", \"(unreconciled)\"), \"(blank)\")).. @thadguidry hmmm, curious\u2026 are you 100% sure that you recompiled on this branch? Can you show your JSON operations?. @michaelavoigt sure, no problem!\n@ostephens yes I think I understand the spirit of your original version. My reasoning for this new version is as follows:\n Letting users use arbitrary headers is a feature - it accommodates for more situations and does not pose a security risk - the user controls these headers fully, they do not depend on the project data.\n KISS principle\nbut I'm happy with anything else that solves the issue, really.. @jackyq2015 good catch - I'll remove that.. I am not sure it is better to put this in the HTML. The typographic rules for these things depend on languages. For instance, in French you would need to put a space before the \":\", but not in English. It would be weird to change the French translation to have a space at the end (the translator would need to know that it is going to be concatenated with a \":\" afterwards).. @omkarnix no need to create issues about tests, we will welcome them in any form and any PR!. Actually it's cell-ui.previewMatchedCells=false, my commit message was wrong\u2026. @pachamaltese yes I think the current storage directory should be kept identical. In any case we cannot merge a PR that breaks travis build.. @pachamaltese I think that would be a good idea actually!. @pachamaltese awesome, thanks a lot! For now I am working on migrating the build system to Maven, I will then have a look at this and see if we can integrate your .deb workflow in the packaging process, so that we could distribute the .deb along with the other artifacts.. @pachamaltese it would be great! Ideally I think we should aim to get in the Debian/Ubuntu repositories, so it would be fantistic to package OR with this goal in mind:\nhttps://wiki.debian.org/Java/Packaging. @okkn thanks, I will investigate. That should not be hard to fix.. In other words, unescape(escape('\u00c4','javascript'),'javascript') evaluates to \\\\u00C4 instead of \u00c4.. Merging this in a few days unless anyone has concerns. The issue was only about rendering: the reason why no alias was added on wikidata is that the item already contained the exact same alias for that language (set by a previous OR batch).. Merging this in a few days unless anyone has concerns. Hi @Gautamshahi \nAs we have already explained to you in issues  #1587, #1590,  #1686 and #1572, this feature is part of the RDF extension, which is not maintained by our team.\nYou are reporting these issues to the wrong team.\nYou will always get the same answer. Reporting more issues will not change this.\nPlease stop reporting issues in this repository now. You are wasting our time.. It's a genuine bug, we should not be failing like this\u2026. @thadguidry it's clearly a bug because running an operation should not fail with an exception. Period.\nConcerning the desired behaviour, the UI is designed to treat blank values uniformly, by letting the user decide where they should appear in the results, so the backend should agree with that.. > Since I have no java errors when sorting by number with your given test case in OpenRefine 2.0-r1836\nImporting my example as a CSV with the default settings will not trigger the bug, you need to make sure the empty cell is treated as an empty string and not a null (which is the default behaviour).\n. @magdmartin I can't reproduce this either (with chromium on linux)\u2026 maybe a sample project would help indeed?. Closing as not reproducible.. @ettorerizza I can't reproduce this on master. Is it possible that you made too many requests and that you got blocked? Can you try fetching few URLs from a different IP address, with a large delay between consecutive requests?\n\n. Closing as not reproducible, feel free to reopen if more information is provided.. Thanks for filing this issue! I agree it would make sense to make these links clickable by default.. Merging as I addressed the comment about URL validation. I'll do this once the project is migrated to maven, #71.. Closing as this is superseded by #1722.. Merging this in a few days unless anybody sees any issue.. Thanks!. @LibrErli that's curious\u2026 could you give a screenshot of your schema?. @LibrEli thanks a lot for the analysis! However I cannot reproduce the problem on my side, neither with Firefox nor with Chromium. May I ask you what configuration you use? (operating system and browser). @ostephens great, thanks a lot for the video, I will investigate.. Indeed! Adding this to the todo list.\nI also updated the FAQ: https://www.wikidata.org/wiki/Wikidata:Tools/OpenRefine/Editing/FAQ#Can_I_add_unknown_value_/_no_value_claims?. Olivier from packed.be would also be interested in this.. Thanks for the report! It looks like this is specific to your environment and not really OpenRefine. I am not sure what these assistive technologies would normally do - if this is not relevant for OpenRefine maybe we could add -Djavax.accessibility.assistive_technologies=\" \" to the refine script.. In general yes, importers will throw exceptions if used with the incorrect file types. I guess there should be a clean way for an importer to notify the user that the format is incorrect. So this is mostly a UI issue.. Hi! Thanks for the report. Can you open the javascript console in your browser and check if there are any errors there?. Can you click on the index-bundle.js:63726 link and post another screenshot of what you see?. can you scroll down to line 63726 and see what is there?. @wentianq which version of Chrome are you using and have you tried updating it?. Ok then I really don't know - have you added any extension or is it just the default OpenRefine package?. Ah yes of course, why didn't we think about the Java version! That's clearly the issue.. @wentianq @pollyhunter have you tried running older versions of OpenRefine, like 2.8 or even 2.7? Does it make any difference?. @jackyq2015 I have the feeling that this bug has to do with the Data Package PR. The Javascript error happens in the javascript library used for JSON editing. Not sure why though..  main/webapp/modules/core/externals/jsoneditor/jsoneditor.js. Given that this seems to affect quite a lot of people (three have reported it so far) I am removing the not-reproducible tag. The next release will be without the data package PR so hopefully it should solve the issue.. Merging this in a few days unless anybody sees any issue.. Oh yeah I completely forgot to adapt refine.bat, of course this needs to be done too.. Okay I still haven't found a good solution to install the custom jars both in Windows and linux. I will migrate to the maven-install-plugin, I think.. @thadguidry @jackyq2015 and any other Windows users, I will need your help to test on this platform. Can you check if refine.bat runs OpenRefine as usual?. @thadguidry there just shouldn't be any quotes there, actually. I have changed refine.bat to fix that.. @thadguidry interesting, thanks. What version of maven are you using? What happens with just mvn process-resources?. @thadguidry OK great! I hope this new version of refine.bat will solve the issue then.. Thanks a lot for the testing!\nI realized that I actually need to somehow deal with the dynamic class loading that is used by the butterfly modules\u2026 That's going to be interesting. Either by using Maven to copy the jars directly in the WEB-INF / MOD-INF directories, or by changing the class loading so that it can be done from the classpath directly. Or just require OR to be packaged in a jar first, like @fpompermaier does I believe.. @thadguidry I think the commits above should solve the issue. I went for the no disruption route - just copy all the dependencies where Butterfly expects them to be.\nRemaining steps:\n- [ ] migrate packaging to Maven\n- [ ] migrate Eclipse config to use Maven\n- [x] write quick transition guide for extension developers: https://github.com/OpenRefine/OpenRefine/wiki/Migration-guide-for-extension-and-fork-maintainers\n- [x] migrate coverage computation to Maven. @jackyq2015 I think it is still useful to provide a wrapper, first because some people might be used to that (so that they will not notice much difference), and because it lets us add custom parameters like allocated memory and other things like that. But of course it is possible for anyone to use maven directly.\nIn other words, what is the downside of keeping the refine script? It does not seem to add a lot of maintenance work.\nPlus, if we migrate later on to gradle, it will help users run OR as before.. wow, thanks for merging - I thought we would do it after 3.0 though. More work is needed (see above) to get test coverage and more importantly migrate the packaging commands to make new releases.. Even more importantly, as I wrote in the PR, \"Sometimes this induced slight version differences, so we should make sure we let plenty of testing time on this before a stable release.\". @jackyq2015 see above, thanks ^. note that this relies on the data package implementation, which we are going to (temporarily) remove to free ourselves from the org.json dependency. So this is a long term goal.. Merging this now that 3.0 is released.. Merging this now that 3.0 is released.. @fpompermaier @xseris then I would be more in favour of a command to rename all columns at once. This would have the advantage of being usable from all data sources, not just CSV/TSV, and appear as an operation in the JSON workflow. I understand this is useful for your own workflows but we should also keep the importer UI simple and principled.\nAlso I don't find it very clean to expose a simple text field to input a list, by expecting the user to separate the items with a designated, non-configurable separator.\nThat's just my intuitive reaction, it's not a hill I will die on!. \"if you rename all columns at once you have to provide the comma separated list of fields\" -> why is that? I don't think we have an operation for that yet, so are you saying that any UI for such an operation would use this input method? Surely we could do something nicer.. Okay, why not then. I think the UI is still not quite ready though. Here are some comments on the layout:\n\nAlthough I do understand your logic to disable the field, it will probably confuse other users. How about adding a checkbox in front of the option (just like the other options) - checking this box would disable the one for \"parse next n lines\" and conversely? Ideally these two should be located next to each other, and be radio buttons - that would be even clearer.\nAlso, we would need server-side unit tests.. @thadguidry let's avoid giving orders in CAPS to our kind contributors, especially when they are contradictory with previous suggestions :)\nI am happy with the current solution too.. @bbonza I see you closed the issue yourself - did you solve your problem?. @ostephens makes sense!. @thadguidry can you avoid editing my own posts? It's better to add your own comment so that we can keep track of who says what.. With Maven as a build system, we need to update the Eclipse config files in the repository which point to the locally-stored jars that do not exist anymore. It is quite simple with the Maven integration in Eclipse to import the project and run it there.\nThis generates new .classpath files, with entries such as\n<classpathentry kind=\"var\" path=\"M2_REPO/org/apache/httpcomponents/httpcore/4.4.9/httpcore-4.4.9.jar\"/>.\nBut in fact, keeping these Eclipse config files in the repository seems bad practice: it pins down the libraries versions in many duplicated files, it forces developers to use these designated config files instead of being free to chose the settings they prefer, and so on. Most Java projects I am aware of simply ignore these config files in their .gitignore.\nTherefore I propose to remove these files from git and add their paths to .gitignore (see commit below). As a developer, just remove your OpenRefine project and reimport it as a Maven project - everything should be set up properly by default.\n@jackyq2015 @ostephens are you happy with that?. I have migrated the packaging workflow for Linux and Windows to Maven. It would be great if users of these operating systems could try these sample packages and check that they work as usual:\n http://pintoch.ulminfo.fr/365d7a1fc2/openrefine-win-3.1-SNAPSHOT.zip\n http://pintoch.ulminfo.fr/013dfd78c2/openrefine-linux-3.1-SNAPSHOT.tar.gz\nI am working on mac packaging.. @thadguidry thanks, that's very helpful. I'll tweak things and get back to you.. @thadguidry could you try this new version?\nhttp://pintoch.ulminfo.fr/f4e65b27a0/openrefine-win-3.1-SNAPSHOT.zip\nThanks!. @thadguidry yes I have changed this so that it matches Maven's conventions. This is a change that should not impact developers or users.\nOn the long term, the goal is to migrate the directory hierarchy to something that matches the Maven standards. This will be made easier by migrating out of Butterfly and will make developer on-boarding much easier.. @thadguidry yes I have stumbled on this today too\u2026. So after quite some sweat Mac packaging is migrated too. The .dmg file can be generated directly with Maven, both on Mac and Linux platforms. Sadly, signing still requires access to a Mac. When releasing, we will need to sign the binary on a Mac machine and recompress the .dmg file.\nExample .dmg generated (unsigned):\nhttp://pintoch.ulminfo.fr/43915db4bc/openrefine-mac-3.1-SNAPSHOT.dmg\n@jackyq2015 I think this PR is now in a mergeable state.. In the end I realized that signing isn't actually needed - it does not prevent the security warning because the certificate used is self-signed. So it is now possible to do the full release process on Linux too.\nMerging this now so that we have plenty of testing time before the next release.. @yayamamo thanks! The good news is that this seems to be a bug of the reconciliation service, not OpenRefine itself, so we can fix that even if we just made a release.. In fact it is not a bug in the reconciliation service: the bug was introduced by an anonymous translator who changed the language code for Japanese (currently is jp, should be ja). So I'll fix this in OpenRefine and introduce a silent mapping in the reconciliation service so that users do not have to upgrade anything.. Hi @skyarchers, can you tell us what operating system and java version you have. What exactly do you get when trying to launch the new version?. @skyarchers could you provide a screenshot of the issue?. @skyarchers I assume you attached the screenshot to your email - unfortunately that does not work - you need to go on GitHub and attach it from there.. @skyarchers thanks, it is a known bug (#1717).. This is actually a bug in the upstream library:\nhttps://github.com/Wikidata/Wikidata-Toolkit/issues/402. @snussik thanks! Did you know that you can fix this on Weblate?\nhttps://hosted.weblate.org/projects/openrefine/translations/ru/. Changed on Weblate myself, will come at the next update.. @thadguidry I won't have time to work on this soon, but if someone wants to work to implement this before 3.2 I'm happy to get that on board, yes.. @skyarchers did you try to increase the available memory? See https://github.com/OpenRefine/OpenRefine/wiki/FAQ%3A-Allocate-More-Memory. See the instructions here:\nhttps://github.com/OpenRefine/OpenRefine/wiki/Back-Up-OpenRefine-Data. Merging this in a few days.. This PR should now be ready to be merged. This just one part of the migration, with one first milestone reached: we do not import JSONWriter anymore (except in the broker, but we will delete it as it is no longer used).\nThis move to Jackson relied heavily on all the tests introduced in #1730, #1729 and #1722 and introduced many other tests: these tests were validated against the org.json implementation and were used in turn to validate the Jackson implementation.\nThat being said, a lot of the changes in this PR are not covered by tests: these were mostly translations of JSON production in commands. This is simply because these translations were fairly straightforward compared to the rest, less critical (because if they introduced some issue it is less likely to cause data corruption). Writing tests for all these would basically amount to writing tests for the entire code base from scratch, which is not doable within our time budget.\nTherefore this will need extensive testing before any stable release. I have already done a lot of it on my side but I expect quite a few bugs will inevitably show up.\nThe next step of this migration will be the conversion of deserialization methods. This is also a very significant step and should take about the same amount of time.. @jackyq2015 indeed, this was introduced by my rebase\u2026\nBy the way, make sure you test this on a disposable workspace - there is A LOT of potential for data corruption in this PR.\nAlso let's not merge this before a 3.1 final release is out.. @jackyq2015 as far as I can tell, the main reason for incompatibilities is the use of ant as build system, because this introduces duplicate and conflicting jars of common dependencies. So the migration to Maven will ease that (but of course the migration to Maven will break all extensions initially by doing so).\nThe good practices to avoid incompatibilities are pretty much the same as for any other software component: exposing stable interfaces (which haven't changed much for the last few years) and keeping the same libraries. (That is exactly the opposite of the move to Jackson but hey\u2026)\n@thadguidry yes extensions can have arbitrary metadata on their operations; no, they cannot configure the engine (facets cannot be extended in a clean way currently). The interaction with the model is limited to the creation of overlay models.\nSo the extension points currently are:\n commands;\n operations (and therefore changes);\n overlay models;\n preference states;\n* arbitrary front-end changes.\nSo the back-end is pretty clean in terms of extension points.. @ostephens sure, that would make sense, but that would probably require tweaking Butterfly, I think.\nRather than trying to fix this obsolete and unmaintained library I would prefer migrating to something standard like Spring.. Closing this as the issue is about the RDF\u00a0extension.. @pranavjadhao our API is poorly documented and not very consistent - apologies for that!\nAre you calling the API over HTTP from a Java client or using OR as a library that you call in Java directly?\nHave you tried inspecting the shape of the requests made in the browser and replicating these manually?. @pranavjadhao it seems that you have correctly copied the POST data, but what about the GET parameters?. @pranavjadhao the HTTP request that you are trying to replicate uses not only POST data but also GET data (controller, jobID, subcommand). You should make sure you provide both.\nAlso it looks like the request that you are analyzing in your browser and the one you are performing in Postman use different endpoints (importing-c\u2026 and create-project-from-upload) so I would not expect that they work in the same way.\nIn other words: just make sure your requests match those made by the UI.. @pranavjadhao your browser provides you sample requests. For instance you can use \"copy as cURL\" to replicate a query with the curl program.. @ancore ah great - do you want to open a PR for that?. @ancore the usual workflow is that you fork the repository to your own account, create a branch there, and then submit the PR for that branch.. Closed by #1764.. @msaby if you get a chance to try OpenRefine from the master branch and check that it works for you, it would be great! We will soon make a new release.. @ostephens thanks for working on this! Yes we can wait for this for 3.1 beta.. @thadguidry I am a bit puzzled by this\u2026 Why would we close a resource just after it is created? It looks like you are trying to solve a warning from some tool, maybe? Or did you run into issues yourself when using the exporter?. Hi @omkarnix, thanks a lot for this pull request! As discussed in the issue it would be great to have a few tests for this (as you have noticed the test coverage is really poor at the moment so we are trying to improve that for all new changes). Let me know if you need any help.. @ancore great, thanks a lot! Sorry for the delay in reviewing. Ideally it would be useful to have a unit test demonstrating that the options are recognized in the exporter.. @ostephens makes a lot of sense! Merging this then.. I think this is a duplicate of #1717.. Thanks for the report! However this seems to be an issue with the RDF extension rather than OpenRefine itself. Could you report it in this repository instead?\nhttps://github.com/stkenny/grefine-rdf-extension. This was done in 1939af343d36d0744f8034c98c43f5398e7d2a95.. Yeah, what I was advising at the hackathon was just to copy the JSON of the operation that saves the schema (and apply it on another project), but that's a bit convoluted.\nAdding import/export buttons is not very hard and would be useful indeed.. The problem comes from the fact that the form isn't submitted:\nhttps://stackoverflow.com/questions/15462991/trigger-autocomplete-without-submitting-a-form. This does not seem to be working with Chrome (only Firefox). https://github.com/wetneb/openrefine-wikibase/issues/51. Actually the problem only occurs in the preview - the order is preserved when edits are made.. @ostephens I'm not sure about non-printable characters, maybe they indicate some underlying encoding problem? I don't know, I haven't encountered the problem very often.. One possibility would be to add a new column containing the editing errors, when they happen.. it seems to be due to a hard limit on the number of tokens parsed:\nhttps://github.com/OpenRefine/OpenRefine/blob/3b3e319e0bda368eaeccb471ca5130e0cc8777e2/main/src/com/google/refine/importers/XmlImporter.java#L75. In fact it's just because that HTML is not valid as XML\u2026 so this should just be solved by implementing an HTML importer - that's not in the scope of this issue.\n18:48:30.866 [..e.importers.XmlImporter] Error generating parser UI initialization data for XML file (1ms)\njavax.xml.stream.XMLStreamException: ParseError at [row,col]:[16,35]\nMessage: Le nom de l'identit\u00e9 doit imm\u00e9diatement suivre le caract\u00e8re \"&\" dans la r\u00e9f\u00e9rence d'entit\u00e9.\n    at com.sun.org.apache.xerces.internal.impl.XMLStreamReaderImpl.next(XMLStreamReaderImpl.java:604)\n    at com.google.refine.importers.XmlImporter.descendElement(XmlImporter.java:181)\n    at com.google.refine.importers.XmlImporter.descendElement(XmlImporter.java:186)\n    at com.google.refine.importers.XmlImporter.extractDomSample(XmlImporter.java:118)\n    at com.google.refine.importers.XmlImporter.createParserUIInitializationData(XmlImporter.java:87)\n    at com.google.refine.importing.DefaultImportingController.doInitializeParserUI(DefaultImportingController.java:215)\n    at com.google.refine.importing.DefaultImportingController.doPost(DefaultImportingController.java:90)\n    at com.google.refine.commands.importing.ImportingControllerCommand.doPost(ImportingControllerCommand.java:62)\n    at com.google.refine.RefineServlet.service(RefineServlet.java:190)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n    at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n    at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n    at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n    at org.mortbay.jetty.Server.handle(Server.java:326)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:923)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:547)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748). @msaby hmmm that is annoying! Have you tried doing ./refine clean first?. @msaby yes we would need to change the way empty rows are treated in records (which I have not touched neither in this PR nor in #1573). It's definitely something we can consider, but I maybe not do this in a rush before 3.1 beta.. @thadguidry yes I agree it is annoying, I will look into that.. I have started to update the docs (help welcome!):\nhttps://github.com/OpenRefine/OpenRefine/wiki/Building-OpenRefine-From-Source\nHere is how the build configuration should be set up:\nhttps://stackoverflow.com/questions/6079253/running-maven-exec-plugin-inside-eclipse. Merging per confirmation on the mailing list that it works.. It is better to keep the artifact names matching the directory structure.. The Wikidata reconciliation service is quite unstable at the moment. That is generally due to the instability of the underlying APIs used by the wrapper - but if this bug persists over time, it might be something else.\nIn any case, we should definitely make sure OR reacts correctly to any HTTP error it gets from services.. Marking as not reproducible for the time being.. Thanks, well spotted! It's massively useful to look out for bugs like this one in the wild.\nIn general, vertical centering (if done correctly) is the right thing to do. This particular dialog is just not vertically centered at the moment, because of the fiddling with the visibility of the second tab in this dialog. I think we should just fix that fiddling and restore the two-tab interface that it was originally designed for.. Thanks for the report! I think this should be solved by migrating to a modern web framework.. Oh yes, the current situation is pretty bad indeed! What you are proposing makes a lot of sense to me.. Closed by #1845.. @tcbuzor yes we understand that - but users should still be allowed to set their own LIMITs and OFFSETs - the code should gracefully handle these (for instance, by replacing a user-provided LIMIT by a smaller LIMIT during preview).. More broadly the problem with the current code is that it chunks queries manually with OFFSET and LIMIT when importing a project from a SQL\u00a0query. This sort of chunking can be quite inefficient, depending on the query. I think it would be preferable to use the existing JDBC API for that, with setFetchSize and setMaxRows, to let the JDBC stream the query for us directly. In the case of PostgreSQL at least, this is much more efficient as the driver will use server-side cursors to stream the query.. @thadguidry looks good to me, but have you also tested the refine.bat file itself? On what sort of invocation does your code make a difference?. Awesome!. Hi @vineetharkut,\nThanks for providing a video. I watched it but it's not exactly clear to me what is the problem you have. Is the get-rows command not returning all columns? I would find that surprising. If so did you check which column is missing from the results? Could you maybe minimize your dataset to make the bug reproducible on a small example, or share your entire project with us?\nIn the meantime I am marking this as not reproducible but feel free to come back with more details.. duplicate of #1158. Fixed by #1852.. That seems to be caused by an extension that was compiled for a previous version - the com.google.refine.ProjectMetadata class has been renamed to com.google.refine.model.metadata.ProjectMetadata in the data package PR.. Thank you for this report. We recommend to use Java 8 with OpenRefine. There are known issues with Java 9. Could you let us know if the problem persists with Java 8?. Java 8 Update 191 should be fine.\nHave you tried version 2.8 too? Does it also have this speed issue?\n. @mlhale7 oh okay so it's slow on her computer no matter which version you try - sorry I thought there was a problem a specific version (your first message was clear, I just read too quickly).\nIf she has any extensions on her Chrome it would be worth trying without them, maybe?. @DimEvil can you give an example of cell values for which the reconciliation service works in 2.7? I have tried with taxa from their database but their service returns an invalid response:\n13:13:13.536 [    refine-standard-recon] Service error for text: Acanthobdella peledina\n  Job code: {\"query\":\"Acanthobdella peledina\",\"type\":\"/biology/organism_classification/scientific_name\",\"type_strict\":\"should\"}\n  Response: {} (0ms)\nSo it seems to me that this error does not come from OpenRefine, but from the service itself.. @DimEvil thanks for the confirmation - so this is not a bug in OpenRefine. If you have any way to contact the author of the reconciliation service to get this fixed, it would be fantastic!. @DimEvil no we have not changed anything to the protocol.\nFrom what I can tell, the service is NOT working with OpenRefine at all (neither 2.7 nor 3.0). I tried reconciling the file you provided in OpenRefine 2.7 (trying both GBIF and EOL) and it does not work.\nIf you had an existing project in 2.7 that was successfully reconciled some time ago it is possible that the service has been broken since.\nAt the moment the reconciliation service does not comply with the protocol described here:\nhttps://github.com/OpenRefine/OpenRefine/wiki/Reconciliation-Service-API. That is curious. I don't see why any of the commits since 3.1-beta would have introduced this - I think this is probably due to a half-built copy of OpenRefine. At the moment we detect whether OR should be built with ls $REFINE_LIB_DIR | grep openrefine, that can surely be improved to match your case.\nCan you paste the output of ls server/target/lib from a copy of OR where ./refine fails?. @ostephens when I copy the repository to a different location in my computer, I find it useful to use git clone my_old_location my_new_location rather than an actual copy such as with cp -r my_old_location my_new_location. That ensures that only the files tracked in the repository get copied over. (Not sure if you are already doing that or if it fits your workflow at all of course!). Thanks for the report!. @mdbaehre Yes this was fixed in 3.2-beta.. In the interest of staying in sync with Weblate and to avoid conflict with other changes I am merging this PR without review - the change has already been discussed in #1285.. I have added migration instructions to https://github.com/OpenRefine/OpenRefine/wiki/Migration-guide-for-extension-and-fork-maintainers. Thanks for reviewing! Yes more work will be needed to take advantage of the plurals and interpolation too. Merging this now to avoid further merge conflicts as this impacts most of the javascript files in the code base.. @ostephens thanks, that makes sense. Yes mapping edits to rows is not trivial. Solving this problem would also let us create facets for each quality assurance issue reported by the Wikidata extension, which would be quite useful I think.\nAre you suggesting that we add a dedicated field for this error in the reconciliation objects com.google.refine.model.Recon? I would be a bit reluctant to do that as I like to keep the core model free from any wikidata-specific functionality (but arguably we could try to make this generic enough for other uses, potentially) - and that would also add to the many other fields stored in Recon that are kind of hidden in the UI (such as the matching features or the types for instance).\nI am not sure if the API is consistent with the UI on this particular issue of existing labels and descriptions (I will have to check and potentially report it to the Wikidata team).\n. Yeah adding a dedicated field, either to Recon or to Cell, can potentially be useful indeed.\nAnother problem with storing errors in a dedicated column is what to do when there are no errors - for consistency I guess an empty column should be created (so that subsequent operations in a workflow can rely on it) but that is not very user-friendly. Letting the user choose in advance if they want to store errors is not an option either, for the reason you mention.. We could also store errors in a custom overlay model (different from the schema). That would give us more freedom and would not mess up with the core reconciliation data model. The errors stored there could be made available via a GREL function (given a Qid, return the editing error for this Qid, if any) and therefore facets.. @thadguidry the Wikibase data model still bases itself on dates and just adds a quite rudimentary notion of precision on top of that:\nhttps://www.mediawiki.org/wiki/Wikibase/DataModel#Dates_and_times. @jackyq2015 do you mean changing their representations of era to match that of Java? I think it would be quite unlikely.. @ostephens I can't reproduce this at the moment - do you have any errors in the JS console maybe? Could that be browser-dependent? . Ah indeed! Sorry I was trying from master, not from 3.1-beta. I suspect this was solved by the other fix about facets (empty facet choices disappearing, #1827). Let's close this then?. @orr721 this will be fixed in the upcoming release 3.1.. project-bundle.js is what @lucacanella meant, this is where all the JS code for the project UI is distributed.. @thadguidry yes, this test does just that: checking the behaviour of this method when the engine config is not a valid JSON object. So the test passes as expected, and the exception is printed. There are various other tests that check the behavior of methods with invalid inputs and they also log errors so I think this is fine.. Duplicate of #1874.. Thanks\u2026 yeah this is an annoying one. I think it will require fixing things in Butterfly itself. That's not a big issue, we are already using a custom build of that, and it is not maintained anyway.. @jackyq2015 sure. From the master branch, create a sample project. Click Wikidata -> Edit wikidata schema and create a simple schema there (for instance, click Add item, pick \"Wikidata Sandbox\" in the input box for this newly-created item, and add an alias by clicking Add term, choosing a language and inputing some string in the input box). Save the schema with the button in the right-hand corner.\nNow stop OpenRefine and start it again. Load the test project. You will find in the console an error about this._schema.itemDocuments not existing. This is because the schema was not serialized correctly (the Jackson annotations were ignored).. Sure, it makes sense to populate these features!. @jackyq2015 ah okay, that was probably the reason why this was overridden, indeed!\nYes I also think most extensions will need upgrading anyway. But if you can think of any other way to fix this I am more than happy to consider anything else!. @tfmorris thanks for jumping in\u2026 here are a few answers:\n\nButterfly isn't our project, so I'd recommend that the version numbering make it clear that it's a private fork, in case they ever do a 1.0.2 (unlikely, but still not a good practice to make a possibility -- and yes, looking back at the commit history, I realize I did it wrong 6-7 years ago).\n\nButterfly is as dead a project can be and we are the only users in town. OpenRefine was already using a 1.0.1 version which actually included some custom patches\u2026 It really means that in effect, we are the maintainers of this library. Of course, the sooner we can get rid of Butterfly, the better.\n\nIf an extension gets broken by the JSON serialization change, I'd argue that that indicates that it's not well enough isolated and the solution isn't to make the abstraction even more leaky, but, rather, to force the extensions to provide their own JSON serialization, unless it's a facility provided by OpenRefine through an opaque API.\n\nI would be curious to hear how you would have done this migration then. Just have a look at the project before the migration: most of the extension points (operations, overlay models, GREL functions) are defined by interfaces (most notably Jsonizable) which explicitly refer to the org.json package in their signature. How can you get rid of this dependency without changing the interfaces?\nIf we want to let extensions use their own JSON serialization library, that probably means using interfaces such as Jsonizable but where the JSON payloads are communicated via bare Strings or other standard java classes such for streams\u2026 I don't think this would be a very good move because:\n it is a breaking change just like my jackson migration\n it will lead to implementations where we keep parsing and writing JSON objects to strings\u2026 For instance, deserializing an EngineDependentOperation defined by an extension will involve deserializing an EngineConfig (defined in the core) and often other objects from the core model, so if the extension and the core do not use the same JSON serialization library this is bound to become very inefficient.\n it is honestly just very ugly, no?\n I don't see how Butterfly could be fixed to both respect the delegation model and provide the isolation you propose.\nAnyway, the extensions and the core will always be coupled by some interface - we can try to make it as tiny as possible for the sake of it, but I don't think doing so by tweaking Butterfly is a durable approach. We urgently need to migrate out of it and into a mature framework - we are not into the business of making web frameworks and we should not try to reinvent the wheel. It is pretty clear so far that the modular architecture of OpenRefine has not provided the isolation of extensions that it promised - changes of dependencies in the core have affected extensions before, even with Butterfly's funky classloading.\nWeb frameworks like Spring impose Jackson as serialization library and that is fine - serialization is a very basic piece of infrastructure. In addition, it is possible for a module to only import the Jackson annotations (as they are packaged separately), which makes it easy to add Jackson compliance with a very small footprint on the code and memory usage.\nSo: yes I agree that the jackson migration is a very bad breaking change (as explained in #1652 and over email), but I don't see any other way the migration can be done. I did try to make that clear to the Conservancy (this move is very bad for the OpenRefine community), but their policy is not negotiable. We could have chosen not to join the Conservancy for this reason, but we would have hit the same road block later on (Apache has the same policy, for instance).\n\nThere's not a lot of context on #1882. Is there an email thread or something else which has more detail?\n\nNo, but feel free to ask any questions if you need more detail. The bottom line is: Jackson requires its ObjectMapper and the annotations on the POJOs it deals with to be loaded with the same ClassLoader (https://github.com/FasterXML/jackson-databind/issues/542) and Butterfly 1.0.1 did not enforce that.. @thadguidry yes I did look for workarounds but did not find any that were applicable to our situation. As mentioned in https://github.com/FasterXML/jackson-databind/issues/542 the real fix would be in Jackson itself, by using class binary names as keys for the internal hashmaps instead of the classes themselves, but the maintainer does not seem to be very keen. That would take a long time before the feature could be released anyway - and \"fixing\" Jackson to accommodate for Butterfly's non-compliance with classloading protocols feels just plain wrong in my opinion.. @tfmorris sorry for the length of this reply (I don't want to sound rude and I am genuinely interested in your opinion on all this - I have just spent a lot of time thinking about this over the past few months).\nHere is one other thing to consider: at the moment JSON handling is still highly entangled with the business logic, sadly. For instance, the importers' configuration was stored in JSONObject (now ObjectNode), so classes from the JSON library are still mentioned by the interfaces between OR and its extensions. So there is no hope that we can easily decouple JSON serialization for core and extensions (even if we agreed that it would be a good move in the first place).\nI did try to reduce this, for instance for operations in #1729 or for clustering in #1730, but it's a big task to do this cleanup everywhere else. (And it's prone to introducing bugs, such as #1827 introduced by #1729).\n. Thanks for the reply!\n\nWas any investigation done into a clean room implementation of the critical pieces of the org.json APIs? I don't know how much work it'd be, but that'd be one way to finesse the problem if compatibility is paramount. It would mean hijacking their namespace and would still require a recompile, but might be worth considering depending on the number of extensions, how hard they are to update, etc.\n\nYes, I have considered that. Beyond hijacking their namespace, that would mean introducing a breaking change with little maintainability benefit - we would keep all the issues that org.json has, and add yet another poorly maintained library to our dependencies. Also I am not a lawyer but I would find it questionable to copy org.json's interface without acknowledging their license, especially because for a library like this, the interface pretty much determines the implementation.\nIn addition, with the datapackage PR, we depended on the datapackage library, which depends org.everit.json which itself depends on org.json. So with this approach we would also have to package our own versions of these libraries with our hijacked implementation of org.json (and keep them up to date\u2026) Honestly, I think we can agree that this is very dirty.\nEven worse: consider an extension that depends on org.json via other dependencies. If we hijack the namespace, that means that loading this extension will load completely unrelated classes under the same binary name\u2026 and Butterfly is of no help here, because the extension itself still needs to load our version of org.json too, to interact with the core. With the Jackson migration, extensions can still safely depend on the original org.json as long as they annotate the classes that communicate with the extension points with Jackson.\nAnyway this choice was made months ago, I don't think it would be wise to reverse it now. I have made my plans clear in August and then was the time to discuss other approaches.\n\nBreaking the API will make extensions incompatible, whether it's a big break or a little break, so it makes sense to take the time to get the new API right and do all the redesign at once, in my opinion. Making a series of smaller breaking changes, just makes life more difficult for the extension developers.\n\nI still think it was sensible not to do the Maven, i18n and Jackson migrations in the same release, as these changes are unrelated. But I agree that it is worth putting effort into improving the extension mechanism before the first 3.2 release. Ideally we could migrate out of Butterfly in the same go.\n\nMy recommendations, which I, unfortunately, don't have time to test out right now are:\n *  evaluate an API compatible org.json workalike if the surface area of the used API is small enough\n *  if that's not feasible, make a clean break and don't expose Jackson through the API\n\nNo for point 1 - we have already made the decision to migrate to Jackson.\nFor point 2, I don't see how this is possible. Concretely, how do you propose to expose serialization through the extension interface? Via JSON payloads represented in String?\n. This approach would only make some sense if we are to stick with Butterfly on the long term (because migrating to another web framework such as Spring will impose other serialization choices anyway). And even so I still think it would be a big design mistake, but hey, I respect your different architectural tastes.\nSo it looks like there are basically two approaches here:\n @tfmorris and @jackyq2015 seem to be keen to stick with Butterfly, its current classloading behaviour and change the interfaces so that they do not refer to any JSON library;\n @thadguidry and I (and Wes on the mailing list who seems to be enthusiastic about Spring) are thinking about migrating to a modern web framework and let the JSON library be part of the interface between extensions and the core.\nIn my opinion, Butterfly is dead, so I think it our n\u00b01 priority should be to get rid of it, and it would be a mistake to let it guide long term design choices. Migrating out of it is a natural first step before cleaning up the web API, documenting it properly and getting a more modern frontend.\nThe background dilemma is to determine what we are aiming for with the current development. Is the goal to just keep the lights on, maintaining OpenRefine as it is now, with the legacy we inherited (legacy build system, legacy web framework, legacy data storage, legacy web UI) and keeping compatibility with extensions? Or do we want to modernize it, breaking compatibility with extensions on the way?\nI think both plans are definitely worth pursuing - there is definitely value in maintaining things as they are. That does not require much effort but will continue to give value to our existing user base. But in that case we should not have migrated any of the build system, localization library and JSON library - because none of it was needed to maintain existing functionality. Joining SFC is really not needed if all we want to do is to fix bugs, implement some new GREL functions, tweak the clustering methods and other things like that. I few months ago I would have voted for this option.\nIf on the contrary we think the project can be updated and revamped, then we need to do it completely and not stop halfway through, so that we get it right the first time and minimize the trouble for any extension maintainer. My opinion is therefore that we should get rid of Butterfly in the same go.\nA few months ago I clearly was not keen to go down that route, because of all the incompatibilities that would generate. But the SFC application process has forced the decision. And I don't regret it at all, we already have had positive feedback about these migrations, the RDF extension is being updated, we have opened up the possibility of being packaged on Debian & Ubuntu. \nThe good thing is that this is an open source project and we are running out of funding anyway, so if we cannot find an agreement we can simply fork and part ways :). @thadguidry thanks, but in any case moving to a different web framework is a big move. What I can do in the remaining time is just to come up with architectural proposals: evaluating how we can translate OpenRefine's extension mechanism to other web frameworks. And then we can do an informed choice about the successor of Butterfly.\n@tfmorris yes indeed all extensions will be broken by the move. Most extensions have also been broken by the move to Maven and the Wikimedia jQuery i18n plugin. About not having any \"any discussion, agreement, or disagreement\", well, I would have very much welcomed any!\nI did not realize that this migration discussion would be more visible on the mailing list for some developers than on GitHub - the people who got involved on openrefine-dev recently were all active on GitHub. I will make sure I post these sort of things there in the future if that is helpful for you.\nIn any case, your informed feedback will be very welcome in the future!. Let's take the heat off this! It's fine for contributors to be busy with other things and Tom is right that we should have better communicated about these plans.. @thadguidry great! Just so you know the fact that someone watches a repository on Github does not necessarily mean that they are receiving any emails about that repository (on-site notifications do not need to be sent by email).. Unless anyone else has another concrete plan forward, I will merge this soon - this is blocking for the migration of the RDF extension to Jackson.. Thanks for the pointers, I should have searched for them myself\u2026. Hmm, that's curious - I am pretty sure this worked at some point.. @ostephens which version of OpenRefine did you use when you had that issue? Would you have an example project to reproduce this maybe?. @ostephens I'll close this as not reproducible for now, feel free to reopen if you can narrow down the issue to a reproducible scenario.. I don't see why this relates to the history: I think what is asked for here is just a JSON\u00a0exporter that follows the initial structure of the imported JSON file. That could be done either by trying to infer it again from the column names (but I suspect this will be ambiguous) or by storing the internal path in column metadata as suggested by the author. In the meantime, the easiest solution within OpenRefine is probably the templating exporter.. I have disabled in the backend the keyers and distances that were not exposed in the frontend. We can re-activate them on an individual basis if users request them.. @ostephens thanks for taking care of this. It is very useful to have screenshots like this one, we need to make it more visible (on the wiki or even the website, maybe).. @abartov this is the correct repository for the Wikidata extension (the Wikidata extension is shipped with OpenRefine). https://github.com/wetneb/openrefine-wikibase is the repository for the small service that interfaces OpenRefine and Wikidata (running at https://tools.wmflabs.org/openrefine-wikidata/). I understand that the separation of responsibilities is not necessarily clear for users, it's fine to report issues in either repositories.. Hi! Thanks for the report. I agree this behaviour is annoying. See #33 for possible workarounds.. Oops! That is annoying. Thanks a lot for the report, I will investigate.. @thadguidry no, currently it is not possible to pass parameters to custom clustering methods, as the UI is not extensible.. @webieseo make sure you use ./refine clean in this sort of situation, too.. @webieseo oops, many thanks for reporting this - there was indeed a leftover reference to butterfly 1.0.1 that I have just removed (Travis did not pick up the issue because of dependency caching). Can you git pull, ./refine clean and ./refine again?. @tuukka if you are using a released version (as downloaded from the GitHub release) then you should not need to use ./refine build, just ./refine should work out of the box. But it is true that you will have this issue when trying to compile OpenRefine from the corresponding git tag. Apologies for that! A new version should be released soon.. @itsacoderepo I think your instructions to reproduce the vulnerability are missing one step: rename payload.txt.gz to payload.tar.gz.\nBut even with that fix I cannot reproduce this - as far as I can tell this was fixed by #1901. So closing as non reproducible in a few days unless I missed something.. Closing as non reproducible.. That is a good idea! Here are the main steps:\n add a dependency to a Java Parquet library (in main/pom.xml)\n write a ParquetImporter which implements ImportingParser: you will probably want to extend an existing class such as TabularImportingParserBase for that, using the classes from the Parquet library\n write a few tests for the importer with some small sample parquet files (there are plenty of examples for that, such as WikitextImporterTests.java)\n write a UI for the importing options in main/webapp/modules/core/scripts/index/parser-interfaces/\n* register the importer and its UI in main/webapp/modules/core/MOD-INF/controller.js\nThis can be also implemented as an extension (see the existing extensions for examples of how to set that up) but unless the Parquet library is very heavy or pulls in a lot of dependencies, I think this could fit in the core software easily.\nDo let me know if you need more indications to get started!. @browncow5 so it looks like there is a strong consensus for changing the order of end date and end time. Could you change that? (You can do this with another commit on the same branch, no need to make a new PR.). @thadguidry hang on, you are running the refine script on windows? Shouldn't you use refine.bat instead?. @thadguidry right, so this PR did not touch refine.bat at all, so the problem should not be new (we can reopen the corresponding issue if this is still a problem on windows).. If anyone knows batch files and wants to work on that, great. This does not seem like a huge priority as I expect most Windows users use our .exe releases (whereas the refine script is the default for Linux users). But of course it would always be good to have.. @marlara I think it would help a lot if you could share your exported project with us. If you do not want to make it public, you can also email it to me (first name @ last name .eu).\nEdit: I have the project now, thanks!. @ettorerizza could well be! Can you let us know which service it is?. Thanks.\nBased on the project communicated by @marlara it looks like the issue could well come from issues during JSON serialization or deserialization of reconciliation operations. This is probably due to some unusual settings in the reconciliation service (such as not providing an identifierSpace and schemaSpace). Otherwise it could be an issue in OR which might have been fixed in master by the Jackson migration.\nBecause the change files are still in the history/ folder of the project it is conceivable to recover these changes: it would require re-generating the JSON items for the corresponding operations, but it is not clear if that would be easier than redoing the work directly.. So it's indeed because these reconciliation services do not declare an identifierSpace and schemaSpace. These are two fields that should be added to the service metadata: https://github.com/OpenRefine/OpenRefine/wiki/Reconciliation-Service-API#service-metadata. These fields are required, so it's the services fault.\nThat being said OR should handle these more gracefully than this. In OR 3.1 we have exceptions like this:\n18:32:19.335 [             recon-config] Reconstruct failed (2ms)\njava.lang.reflect.InvocationTargetException\n    at sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at com.google.refine.model.recon.ReconConfig.reconstruct(ReconConfig.java:100)\n    at com.google.refine.model.changes.ReconChange.load(ReconChange.java:177)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at com.google.refine.history.History.readOneChange(History.java:83)\n    at com.google.refine.history.History.readOneChange(History.java:69)\n    at com.google.refine.io.FileHistoryEntryManager.loadChange(FileHistoryEntryManager.java:96)\n    at com.google.refine.io.FileHistoryEntryManager.loadChange(FileHistoryEntryManager.java:80)\n    at com.google.refine.history.HistoryEntry.revert(HistoryEntry.java:155)\n    at com.google.refine.history.History.undo(History.java:236)\n    at com.google.refine.history.History.undoRedo(History.java:172)\n    at com.google.refine.history.HistoryProcess.performImmediate(HistoryProcess.java:82)\n    at com.google.refine.process.ProcessManager.queueProcess(ProcessManager.java:97)\n    at com.google.refine.commands.history.UndoRedoCommand.doPost(UndoRedoCommand.java:69)\n    at com.google.refine.RefineServlet.service(RefineServlet.java:190)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n    at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n    at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n    at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n    at org.mortbay.jetty.Server.handle(Server.java:326)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:938)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\nCaused by: org.json.JSONException: JSONObject[\"identifierSpace\"] not a string.\n    at org.json.JSONObject.getString(JSONObject.java:721)\n    at com.google.refine.model.recon.StandardReconConfig.reconstruct(StandardReconConfig.java:116)\n    ... 42 more\nIn master, the services are not usable because running a reconciliation operation gives this:\n18:37:04.795 [                   refine] POST /command/core/reconcile (1726ms)\nException in thread \"Thread-6\" java.lang.NullPointerException\n    at com.google.refine.model.recon.StandardReconConfig.computeFeatures(StandardReconConfig.java:567)\n    at com.google.refine.model.recon.StandardReconConfig.createReconServiceResults(StandardReconConfig.java:555)\n    at com.google.refine.model.recon.StandardReconConfig.batchRecon(StandardReconConfig.java:485)\n    at com.google.refine.operations.recon.ReconOperation$ReconProcess.run(ReconOperation.java:282)\n    at java.lang.Thread.run(Thread.java:748)\nThat's already better because it will not cause data losses in the first place.\nSo to solve this we need to change the way reconciliation services are parsed to include default schema spaces and identifier spaces for these reconciliation services.\n. @marlara so I think I have a fix in #1937 - but unfortunately it is not going to get you your data back by itself.\nI am sure it is very frustrating to lose hours of work (apologies for this on behalf of the rest of the team!).\nIf I the data loss is a serious problem for you I can try to hack something around get the data back.. Basically, this is the approach I would use to recover the data: for each history file that does not correspond to an operation in the history, generate a JSON representation of an operation with the same timestamp (the description and the actual operation should not matter as long as they correspond to some actual operation). Then add these JSON representations back in the data.txt file (in the section of unapplied changes). This should make it possible to redo the changes from the UI, updating the grid with the new data.. @marlara I will try to find the time to recover your data tomorrow.. @marlara I have tried to recover the data with the idea sketched above, however I suspect that the state of the project is inconsistent with the position in the history. If you had the source file for this project it would help: that would let me replay all the changes from the start.. @rogargon interesting! And does the released .dmg work for you?. I don't think we should just throw a warning, the expected behavior described above looks reasonable to me.. Hovering links does not do anything on any browser at the moment, see #1943. But clicking should do something.. I can confirm that the code was just written with the cell.cross(...) use case in mind, it is not working for values constructed on the fly.\nI would say this is a pretty important bug - we should not rely on the column of the project where the cross function is applied. Moreover, the current design requires that both project names are unique in the workspace, whereas one would expect that only the target project (whose name appears in the invocation of the function) would need to be uniquely named.\nIntuitively it should not be hard to redesign this: instead of creating ProjectJoins, we should just create a single index on the target project, and look up values from that.. Apparently the names of the two columns to be joined on must be the same, according to the following (from the mailing list):\n\nWhen using \u201cAdd a column based on a column\u201d, the expression I\u2019m using\nis: cell.cross(\u201cDates\u201d,\u201dCreation dates\u201d)[0].cells[\u201cStart date\u201d].value\nI thought that the column name (\u201cCreation dates\u201d), only had to match the\nname of the column in the cross project [\u201cDates\u201d]. (i.e. I thought it\nshould look at the \u201cCreation dates\u201d column in the \u201cDates\u201d project and\nreturn the value in the \u201cStart date\u201d column\nWhat I have now discovered is that the name of the \u201cbased on column\u201d\nalso has to match (i.e. also has to be headed \u201cCreation dates\u201d). Is that\nhow cross is meant to work? Or is that a bug?. @psychemedia OpenRefine tries to reconcile the first 10 rows of your column using your service, so that it can use the types returned by your service to suggest types to the user. So the request looks like this:\ncurl -X POST http.//127.0.0.1:888/ -d '{\"q6\": {\"query\": \"locus ceruleus complex\", \"limit\": 3}, \"q3\": {\"query\": \"titrant\", \"limit\": 3}, \"q4\": {\"query\": \"costal\", \"limit\": 3}, \"q9\": {\"query\": \"carbon dioxide assimilation\", \"limit\": 3}, \"q0\": {\"query\": \"Chilaiditi's sign\", \"limit\": 3}, \"q7\": {\"query\": \"endoplasmic reticula\", \"limit\": 3}, \"q1\": {\"query\": \"genioplasty\", \"limit\": 3}, \"q5\": {\"query\": \"lactococcal\", \"limit\": 3}, \"q8\": {\"query\": \"BFU\", \"limit\": 3}, \"q2\": {\"query\": \"astute\", \"limit\": 3}}'\n\nThat being said the shape of the request should not matter, as it seems it cannot even connect to the server anyway. So there is probably a problem with your Docker setup - have you tried querying your reconciliation service from your browser and from the command line?. Ah, if you are running this in a Binder instance, then using 127.0.0.1 for the address of the reconciliation service is definitely a bad idea, as some requests to the reconciliation service are done in AJAX by the web UI (so, from your own browser). So even if you managed to get the guess-types-of-column command working with that setup, you would not be able to use the previews and suggest services.. @psychemedia yes, the problem is that both 127.0.0.1, 0.0.0.0, localhost or anything like that is only going to be understood locally, on your Binder instance. You need to use an address that can be understood anywhere, including in your browser.\nOne such address would be https://hub.mybinder.org/user/psychemedia-jup-roxy-openrefine-RANDOM/proxy/8895/parliament/reconcile/ for instance.. And I agree we should update https://github.com/OpenRefine/reconciliation_service_skeleton, or delete this fork as we should not endorse by the organization something that is not functional.. @psychemedia please see my replies above:\n\nAh, if you are running this in a Binder instance, then using 127.0.0.1 for the address of the reconciliation service is definitely a bad idea, as some requests to the reconciliation service are done in AJAX by the web UI (so, from your own browser). So even if you managed to get the guess-types-of-column command working with that setup, you would not be able to use the previews and suggest services.\nthe problem is that both 127.0.0.1, 0.0.0.0, localhost or anything like that is only going to be understood locally, on your Binder instance. You need to use an address that can be understood anywhere, including in your browser.\nOne such address would be https://hub.mybinder.org/user/psychemedia-jup-roxy-openrefine-RANDOM/proxy/8895/parliament/reconcile/ for instance.\n\nThe communication between your browser and the MyBinder instance is not going to be local, so you should not use a local address to add the reconciliation service. Is that clearer? There is just no chance your setup is going to work - do not use a local address if you want to use the reconciliation service in a MyBinder instance. I honestly cannot think of any other way to explain this, sorry.. In other words:\n- your browser is running on your own machine (the one that sits on your desk or lap or whatever), let's call that \"machine A\"\n- your OpenRefine instance is running in the cloud, on some server in some datacenter, let's call that \"machine B\"\nFor reconciliation services to work you will need \"machine A\" to talk to \"machine B\". So \"machine A\" needs to know where \"machine B\" lives, so that it can follow internet cables across the many mountains, rivers and seas that are between them! For this magic to happen, you need to give an address of \"machine B\" that \"machine A\" can understand. Local addresses such as localhost will typically not work as \"machine A\" will understand them as referring to itself.. Yes, for postgres I think all connections should be happening directly between the Java backend and the database server. What I have been trying to explain above is that this is not entirely the case for reconciliation services: communication with reconciliation services happens both from the Java backend and the web frontend. The error you are observing comes from a failure of communication between the Java backend and the reconciliation service, but fixing it will not give you a fully functional reconciliation service if the service is configured with a local address - so it is not worth trying to make that work.\nPassing the full URL should work though. To debug this, you can try to access the reconciliation service manually from your own browser: check that it returns a valid service metadata, and that it responds adequately to reconciliation queries of the form mentioned above. You should also be able to add your reconciliation service with the full URL in a locally running OpenRefine instance. Good luck!. @psychemedia Yes trailing slashes are respected.\nCalls to the reconciliation service from the browser are done when previewing an item (clicking on an unmatched reconciliation candidate), configuring properties or types in the reconciliation dialog, or manually choosing a different match for a cell.. Yes, the browser also accesses the reconciliation service at registration: yet another reason not to try to register the service with a local address.. When downloading the results of a SPARQL query, the column contains full URIs (http://www.wikidata.org/entity/Q55830494) whereas OpenRefine expects identifiers (Q55830494). One can convert one from the other with an expression such that value.split('/')[-1] but that requires an extra step and confuses newcomers.. Hmm actually we do store the schema space http://www.wikidata.org/entity/ so we can use that too.. @thadguidry yes I understand the motivation of the changes. I think the new facet is great but it needs to be introduced in a different way to avoid the bugs that this change has created. This has confused a lot of users.\nSo yes, if someone wants to work on making this new behaviour work for everyone, I would welcome a PR which would follow the steps above. In the meantime, the first step is to revert the change.. What I am proposing is to re-introduce exactly the change you made, but in a different way. We can have the best of both worlds: a new facet which has the new behaviour you proposed, without the breaking change it has incurred. For that to happen, we only need to introduce your new facet as a separate facet, with a different class name and facet identifier. This has the following advantages:\n we can keep the existing facet available in the backend, meaning that running existing JSON workflows will not break;\n migrating the UI to use your new facet will force us to update the facet ids there, migrating to either your new facet (and adding .toString()) or migrating to another new facet (such as a boolean facet). If we forget to update any facet (as we did), it will stay with the old facet implementation and will therefore not break. Suppose some extension adds some UI for new predefined facets: we will not break that either.\nThat ensures a smooth transition for both users and developers. I am happy to work on reintroducing these facets between 3.2-beta and 3.2.\nI think it is fine to have breaking changes when they are clearly needed to bring a much needed improvement - but in this case we can very easily avoid breaking anything, so my gut feeling is that we should make sure OpenRefine workflows live up to their expectations of reproducibility and reusability.. @thadguidry a toggle could be useful, but where would we put it?\n I don't think it would make sense to put it in the reconciliation dialog - that contains the configuration options for an operation, not for the UI.\n Putting it in the global preferences would be pretty useless - users are not likely to discover it and it is going to be inconvenient to go back and forth between the project and the preferences to switch that when required. Moreover the preferences are global, not per project, so it's inconvenient if you have one project where you need the previews and not the other.\n* Putting it next to the records/rows mode and the number of rows to display could work, but IMHO it's a very niche setting that does not deserve to occupy highly-visible space there. And it's going to make this bar longer, which is likely to cause problems on small screens.\nIt's easy to add a toggle in the global preferences but my gut feeling is that it's not going to be used\u2026. Still not convinced, it's hacky. It really does not belong to project metadata: UI settings should not go there. I will add a toggle in the global preferences for users who get annoyed by the popups and that will do.. My commit message was incorrect, it's cell-ui.previewMatchedCells=false that you need to add to disable previews on matched cells. This requires reloading the project page too.. @thadguidry that's the file name inside the zip, what they mean is the name of the zip file itself.. Ah ok! @nanobrad, does this solve your problem? Or did we misunderstand it?. @Manu1400 this is already supported by the reconciliation interface: https://github.com/OpenRefine/OpenRefine/wiki/Reconciliation#comparing-values\nThis is demonstrated in the second video of this tutorial:\nhttps://www.wikidata.org/wiki/Wikidata:Tools/OpenRefine/Editing/Tutorials/Video\nHowever, note that coordinates are currently only used in scoring, not at the retrieval phase. So it can be used to distinguish between namesakes, but will still require that the names are spelt reliably enough to bring up the candidates in the first place.. @Manu1400 I'm closing this per the above - if there is still an issue on your side, please report it to https://github.com/wetneb/openrefine-wikidata instead. Thanks!. @ostephens sorry I had not realized that I did not push the commit that reverts the main change #1666, indeed. This is now done.\nI agree that the problem can be resolved without reverting #1888. I think it is simplest to do it anyway for clarity as this was introduced as a fix which is not needed once we revert #1666. Once a new facet with the new behaviour is introduced, we can then add the .toString() where the new facet is used.. @TeraBlitz this is because OpenRefine uses its records mode by default. This records mode is a way to represent a hierarchical structure between rows and columns. This only makes sense for some data sources though, so if that does not apply to yours, just switch to the rows mode and you will obtain the desired result.\nSee https://librarycarpentry.org/lc-open-refine/03-working-with-data/index.html for some explanations about these two modes.. Freebase has been shut down so any calls to their webservices will fail and are normally only made from dead code. This is therefore an exercise of removing dead frontend JavaScript code. It is mostly down to understanding how reconciliation works client-side and taking out the bits that are no longer used. You are more than welcome to ask questions here if you want to tackle this!. Related:\u00a0https://github.com/wetneb/openrefine-wikibase/issues/57.. I don't think we should log this, this is likely to generate lots of redundant logging messages as the cross function is typically applied on all rows. Using the existing GREL error mechanism seems more natural.. @felixlohmeier sounds like a sensible thing to do! Do you want to try submit a PR for that? It should not be too hard, you can get some inspiration from the existing facets in the menus.. @felixlohmeier it looks great to me, thanks a lot! This is going to be very useful, I think. I'll have a closer look and merge this soon, it should make it into 3.2.. It looks like this is a regression indeed! Good catch.. Yes I checked and it is new.. Closing then as a duplicate of #1957.. I think that is the same root cause as #1981.. @ostephens thank you very much for working on this! It looks very sensible to me. Have you considered adding a few tests to demonstrate?. The data extension tests probably failed because the Wikidata reconciliation service was down for some time. We should mock these tests (PRs welcome).\nThe testToDate issue was fixed after 3.1 was released: see #1874.. Why would it help to remove entries when the key is no longer in use? Here the keys are strings or urls that are constructed just before the request, and are no longer referenced anywhere after we continue to the next row\u2026 So if the GC is a bit aggressive, this could effectively keep the cache empty, I think.\nI am not an expert but using WeakHashMap here sounds a lot like the misunderstanding described here: https://stackoverflow.com/questions/1802809/javas-weakhashmap-and-caching-why-is-it-referencing-the-keys-not-the-values#1802879\nBut I agree that we need to make sure the HashMap does not take too much memory.\u00a0A cache with timed invalidation (or automatic pruning beyond a certain size) would be better.. I am happy with that too.. Done in the next commit.. We should remove this console.log before merging.. Why not store parseInt(thisVerParts[i],10) in a variable and do the comparisons on that variable? Same for parseInt(latestVerParts[i],10), it would look a bit cleaner to parse the string only once.. This should be latestVerParts (no s). Have you tested your code?. Things of the form if foo then return false else return true can be rewritten as return not foo.. I think it would be more efficient to compile this regexp to a Pattern only once, outside the loop, and re-use that inside the loop.\nhttps://stackoverflow.com/questions/1720191/java-util-regex-importance-of-pattern-compile#1721778. Calling cross with more than 4 arguments should fail with an error, it looks like it currently does not.. Thanks for updating the existing tests! It would be great if we could also have one or two tests to demonstrate the functionality that you have added.. In principle I'd prefer an if () { ... } else { ... } rather than if () { ... ;\u00a0return; } ... but it's clearly not a big deal\u2026. Why was this deleted?. What is this rowNumber metadata? If that counts the number of rows in the project, shouldn't it rather be rowCount? rowNumber reads like a row index\u2026. I would spell this Metadata rather than MetaData to keep it consistent with the rest (and common usage).. Same here (Metadata instead of MetaData). I'd replace that by just !invert.. And that by invert.. @thadguidry for a test like this it really does not matter at all: that's not what it is meant to test. Wouldn't it be cleaner to store that in CSS? It's generally recommended to avoid inline styling.. Why this .replace(/\"/g, '\\\"')?. This PR contains two unrelated changes\u2026 It's generally better to use separate branches and PRs for that.. Is that related to the bug?. What if the row count changes later on (for instance if the user removes some rows, or uses a transpose operation)? Is the row count going to be updated somewhere else?\nI would check if the row count is different from what is stored in the metadata, rather than comparing to zero.. then I guess it's cleaner if we don't include that in the PR :). I wouldn't print anything like this in a unit test.. Sorry, but this is really not acceptable: it is not extensible at all. What if an extension declares a new operation that changes the columns? It will not be able to register the operation in your switch.\nI am afraid it is going to be very hard to recover your implementation - the whole architecture needs to be started again from scratch. Again, the clean solution here is to store the schema in the Column objects themselves (or in a class that is linked directly from them).. Sorry but again, that is not a solution - what if the metadata changes later on? For instance:\n buildDataPackageMetadata is called\n the user edits the schema\n* the user edits project metadata\n=> the data package metadata is out of sync from the project metadata\nAt this stage, calling buildDataPackageMetadata again would discard the changes made to the schema.\nAgain, the clean solution for this is just not to duplicate this metadata in the first place.. What is the difference between the title attribute and the _name that we already have?\nAlso, shouldn't these attributes follow the naming convention of the previous ones (start with _)?. Do we want to allow any type or restrict them to an enumeration?. It seems quite dangerous to do this if we allow types to be any string.. there is a typo here: threashold should be threshold (and at other places). oh yeah, okay, thanks!. it's not the trys, it's just that the introspection features should be used only when needed, and this does not seem to be a case where they should be used. I would go for an enum for the type and a switch for the cast.. @thadguidry concretely, what is it going to look like as a user? if I want to set a type for a column, will I see a text field where I can input any string that will represent the type? it seems sloppy to me.. @thadguidry you are mixing up two things (see my comment below). Data types should not be free-form strings or even URIs.. Again we should really use jars to avoid that.. That's what we use in OpenRefine so far - we can update it, but it's a different topic. Yep, that just lets you skip tests that are known to fail for some reason.. sure, done (I've left out the bit about ./refine because people can also use ant directly). it's something you add manually, when you are fed up by one particular test that fails because of something outside your control. sounds good (it does not actually work anyway but oh well). This <h3> renders as bold text for me. I think this should be normal text just like the other options, so that \"Formulate the URLs to fetch:\" remains the single bold label. Otherwise I feel like I am prompted to set custom headers before defining the URLs.. Why are all these new dependencies needed? Are they required by the table schema library? It's hard to distinguish which libraries were updated and which ones were added.. Why was this file deleted?. What is this change doing? (I have no idea what this file does - but we should have a good reason to change it.). Same comment here - I don't know what this change does, so I am not necessarily opposed to it, but it would be nice to know why it is needed and how it relates to data packages.. Same comment.. Would you mind explaining that change too?. Now that the jars have been added I guess we should delete this line?. I would find it cleaner to have these JSON\u00a0files in a specific data directory and not just at the root of the webapp sources.. Same comment - maybe it's a good thing to delete these configuration files but it's not clear to me why, and in any case this should be discussed.. Same comment.. Same comment.. I don't think this file is in an appropriate location - this folder is for Java source files.. So again this file is full of changes that do not seem related to the feature - if I remember correctly they are meant to address issue #1380 so it looks fine to me, but please use a separate PR next time.. I'm not sure I understand this change - do we used to have project-level preferences that have been removed by this PR?. It's not clear to me how this metadata is saved to the project - it looks like it is just validated and then it is not used - can you explain what is happening here?. It looks like this interface is used to translate relative paths to full URLs - maybe it would be worth putting a few comments explaining what it does (for instance, why does it return a List of Results instead of just a Result)?. hmm, I don't see why? Yes we do want to maintain the gdata extension - actually one of our goals is to update this extension so that it works again with the new Google Sheets API. In any case this change is unrelated to data packages, so let's keep this file.. I think the sample extension is definitely useful - I don't see why it would be a good thing to delete its .classpath. Again this is unrelated to data packages so let's not delete this file in this PR.. Is this command currently called anywhere? If so, it should be renamed to something like ValidateMetadataCommand otherwise it will cause confusion. If not, it should be deleted.. well you did change the code too - project.getMetadata().getPreferenceStore() is not called anymore, so it's not just about adding some comments.. typo in DATAPAAKCAGE. Is this class needed? It does not seem to do much\u2026. How can we localize this file?. These validators look great to me, but I cannot figure out how to access them as a user. Is there any way to see them in the UI?. I think this file should be kept too. same here. thanks. Any idea how we could localize this? Could this be done UI-side?. or it should also be possible to use this file directly from the UI\u2026 but okay, let's keep this for now.. okay you wrote in the PR that it's not implemented yet. fine.. well let's remove it for now, because it is not needed and redundant with the command to set the project metadata. I don't see how it could be useful in the future given the new way we store metadata. We should rather have commands to change column metadata (types, constraints) and this should be exposed in the menu for each column.. no @jackyq2015 it is not \"added backed as above\", there is a difference in the code and I would like you to explain it to me. I accept to turn a blind eye on some dirty corners of this PR but it would be nice if you could cooperate a bit more\u2026 I am trying to help you improve your PR.. it should not be too hard to add it back once it is needed, so let's remove it. okay, thanks for the explanation!. okay, great. okay, fine for this time. Well then we are not seeing the same diffs!\nThis is what I see:\nThree lines removed:\n-        PreferenceStore ps = project != null ? \n-                project.getMetadata().getPreferenceStore() : \n-                ProjectManager.singleton.getPreferenceStore();\nThree lines added:\n+        \n+        // project level has no specific preference\n+        PreferenceStore ps = ProjectManager.singleton.getPreferenceStore();\nSo do we agree that there is a change in the code? You removed a condition. It does not seem harmful to me, but that is what I wanted to discuss\u2026 Makes sense?. It's actually a change worth being discussed, because it means that the project passed as parameter to the command is now completely ignored - so that means the preceding line can be removed too:\nProject project = request.getParameter(\"project\") != null ? getProject(request) : null;\nBut that is suspicious - why was there any need to pass this argument before? Why would the preference store be different depending on the project? Do we rely on that anywhere? If so, there is a risk that this PR introduces a bug. I am not saying it does, I am just saying this needs to be investigated.\n. Great, thanks a lot! (But that's worrying - it means we can't rely on GitHub to review PRs?! That sounds like a serious bug to me!). I am not sure what you mean - we already have three files for local, travis and appveyor (the fourth one should be deleted - I am just waiting for @tcbuzor's confirmation). It's not clear to me how to avoid the copy but feel free to commit on this branch with the setup you are thinking about. Note that currently no copy is needed when you test locally - the default configuration file already excludes all tests which require database access. Copying is only required on Travis and Appveyor (but that does not seem to be a big issue to me).. The front end already enforces that the maximum length of this string is 1, but sure, we can make that more robust for people who do not use the web frontend.. Unfortunately not, we would need to work on backend localization and that is a non-trivial task.. Well I thought there wasn't any point in continuing to use \"com.google.refine\" given that the project has changed name. I don't think we should necessarily migrate the rest of the software to a new package, but for new and independent code I thought it would be nice to feel a bit more at home :) This choice was made before we had funding from the Google News Lab. But I honestly don't care ^^. No, but it can still be referred to: Freebase ids are still in use, for instance on Wikidata. There are still a number of places in the code (and possibly in other extensions / reconciliation services) where this is used. Are you proposing to delete this, and if so why?. This comment is actually out of date - OR saves preferences by itself. Thanks for picking that up though.. Not sure why I introduced this - I'll see if it still works as a plain static member (I don't see why it wouldn't\u2026). yes, this field holds a boolean value that says whether we are logged in or not.. what do you mean?. yes, for added statements the order matters.. It's not clear to me how to do just one streaming. Can you elaborate?\nThe overhead of parallelStream() is way too high for the data sizes that we are dealing with here - typically these sets will contain just a bunch of elements (a dozen, say).\nhttps://stackoverflow.com/questions/20375176/should-i-always-use-a-parallel-stream-when-possible#20375622. sorry I didn't realize your comment was about deletedStatements. For deleted statements the order does not matter (because the document will be the same no matter in which order you deleted the statements).. @jackyq2015 it was like this before, but I have changed EntityUpdate to be immutable (just like the other classes in Wikidata-Toolkit). It's something we don't do much in OpenRefine (everything is public and mutable) but it brings a lot of good things (thread safety, easier testing)\u2026. it's just meant to provide some structure to match the JSON's own structure (but I can remove it if you prefer, it is cosmetic). We can avoid the conversion at deserialization, but I don't see how to avoid it at serialization. I did try to find a way to convert a JsonNode to a JSONObject but there does not seem to be any simple solution for that. Going through a String isn't great but the overhead isn't much and I don't see any other solution\u2026. It just means we add an empty one by default, which does not cost anything\u2026 I believe that's how it's done in the Freebase and RDF extensions. I don't see how you would \"select\" which projects to enable?. \"Edit Wikidata schema\" creates the schema (and saves it as an overlay model with SaveWikibaseSchema), \"perform edits\" pushes the edits to Wikidata (with this operation).. Yeah I'm not that happy with that structure, I think I see a different way which should reduce iteration costs.. Is this tied to your own Google account somehow? And is this token really meant to be public? (Could other applications maliciously reuse this token?) (Not that I can see any way to avoid that\u2026). I assume we should remove this logging because the request body (and response below) could be quite verbose.. Is there any reason why the full import path is specified here, rather than just List<CellData>?\nAlso, since we have migrated to Java 8, the right-hand side can be simplified to just new ArrayList<>();. I'm confused - wasn't this change already merged?. The language fallback does not work so if you leave this string untranslated the button will show as blank (or \"undefined\") in the UI.. that is a regular expression which validates URLs - as you can see there is a ? after the s, which means that s can be omitted - so yes, both http and https are accepted in this input.. I get the following JS error here:\nUncaught SyntaxError: Unexpected token ]\nBoth with Firefox and Chromium. Given that stringToLocalDate actually uses stringToDate, we do need to support local dates there too. Otherwise stringToLocalDate will not accept timestamps without timezones.. Why was this renamed?\nYou probably do not want to refer to https://en.wiktionary.org/wiki/calender\n. Where is this commit? If it is in this PR, then this typo should not be visible here anymore.. that's just a mistake - I did not intend to change this, it's just my own setting that got pushed with the rest, I will revert. that is planned - this will be part of making the extension usable for any Wikibase. But that's a lot more work and I don't think it should be ready for 3.0.. the implication is that now OpenRefine and the Wikidata UI will agree on the default value for this \"after\" parameter. I can elaborate here but I am not sure how useful it would be to add more comments in the code itself, given that it's just one of the default values (with the \"before\" and \"timezone\" parameters) - having a default value of 0 is fairly intuitive, no?. Should we really be deleting user data here? I think this should only happen with the --purge option.. We currently use .local/share/openrefine by default - why change to .openrefine?. Could we move this file to .deb-exclude maybe, so that it is hidden by default, just like .gitignore?. I think the architecture should not be restricted to amd64, as it should run independently anywhere with a JVM, no?. Should we enforce Java 8 in the dependencies?. I would rather not put logging here, as it is going to be displayed every time this is run (every 10 minutes) even if there is no job to clean:\n18:10:08.083 [                importing] Cleaning Stale Import Jobs ... (410217ms)\n18:20:08.083 [                importing] Cleaning Stale Import Jobs ... (600000ms)\n18:30:08.083 [                importing] Cleaning Stale Import Jobs ... (523849ms). Actually, Ant should not be required at all, because the refine.bat script should download it by itself. This would be worth checking though.. Great, thanks! I don't have access to a Windows machine at the moment so it's very useful.. @thadguidry the database extension uses this older version of Jackson - I migrated that to Jackson2 when writing the pom.xml file but it turns out that this introduces some runtime exceptions. I am keeping the dependencies as they are now for this PR to minimize the risk of introducing - it can be migrated later, in a different PR.. Fantastic, thanks a lot! As a small note, I think invoke should not need to take the function name as parameter, as it is always FUNCTION_NAME in this class (and the method is private). But that is a cosmetic concern.. @omkarnix moving to a test util class would make a lot of sense too.\nThe RefineTest class is used to create a test workspace and initialize various other things that are required when dealing with projects. For function tests it is not needed to extend it, as function registration is done statically as you have rightly noted.. yes don't worry about unrelated test failures (although this one is curious). I can restart the build when it happens.. Why are we registering SelectXml both as select and selectx? Isn't select enough?. @jackyq2015 as far as I can tell it makes sense to return an error (instead of null) in these cases, but maybe I miss something? Do you have a particular use case in mind?. Looks like a copy-paste typo here.. No, the JSON format implemented with org.json used type, not types. We need to keep this format so that reconciliation services still work.. Sure, there you go.. @thadguidry sure, feel free to add that!. @thadguidry I think that should rather go on the Keyer interface itself. Would it be cleaner (and more consistent with the facet name) to write \"values per column\" instead of \"values/column\"?. Same here.. ",
    "ettorerizza": "Here is a visual example. As you can see, I'm on pages 91-100. But as soon as I make a reconciliation, I come back to page 1.\n\n. I'm just working on extracting \"name like\" from very short and unstructured texts (eg \"Thad Guidry New York 2016\") and I can say it is very difficult... I would be delighted that Open Refine has a function to extract names, but I also know that it will work very badly depending on the length of the text or the language. Adding such a function that would work once in five, I think, would frustrate many users.. >  I think someone has already wrote that enhancement somewhere if I recall\nThe Deri's RDF extension ? Never tried.\n\n. Probably related to #1299 . @DFoltzMorrison The workflow \"Bibtex -> JabRef : export to CSV -> Open Refine\" explained above seems to work quite well.. Ouch, the Python bibtex parser mentioned in your first link is anything but simple. I hope there is something similar in Java, otherwise we will not see this importer anytime soon.... The \"refresh-and-goes-back-to-the-first-page-after-a-match-all-identical-cells\" issue is a pain in the ass. It should be considered as a high priority issue for reconciliation.\n\n. @wetneb Yeah, you put the right words on something that I felt without formulating it ! When it comes to examine records one by one manually, we move from a logic of columns (a logic of database) to a logic of rows (a logic of spreadsheet) for which the interface is not appropriate.. > we look at one row/record at the time -> the other rows should not be visible\nBe careful that there are some cases where you have to look at another column to choose the best candidate. For example, to check if a matched place is in the right country, or if it is the right kind of place (some place names refer to both a municipality and a province).\n\nAnother case where it is necessary to look at another column is when the matching is done on a copy of the original column (in order to then visually check the quality of the matching.).\n\n. Ouch, sorry, I had misread (with the copy-paste of the sentence in front of the eyes, moreover). :/ . XML rendering in the browser is still very, very, very slow, even with an xml of a very average size and complexity. I only go through sites like this one to try to get a CSV that will then import into Open Refine.\n\n(Open Refine 2.7, Windows 10, Chrome)\nFile used for the screencast is in attachment.\ndesc_en.zip\n. Related to #1302 . Issue still relevant for many users : https://groups.google.com/forum/#!topic/openrefine/TjxiExVpJ6w. There should be a way to create a safe \"Fill/Blank Down\", since we know the number of records and the length of each in the previous column (I do not know if I'm clear?) \nAnother important improvement would be a \"Fill/Blank\" ALL, which would allow to get in a click a clean denormalized dataset.. Just a little visual reminder of the problem, for the sake of documentation. In this case, the 1 in the first record spreads to the second record.\n\n. Related to #1302 . It would be a very simple yet usefull enhancement.. With Open Refine RC2 (Windows), I have a lot of problems for parsing XML\nfiles as simple as this :\nhttps://www.dropbox.com/s/acj3x3b8wl3bkso/4408000.xml?dl=0 \nI just get a white screen. \nGoogle Refine 2.5 does not have this issue.\n. Arf, you're right @ostephens. This is particularly misleading, since these extra spaces are also invisible in the transformation window.\n\nThe toggle mode proposed in #1286  seems really essential. How to clean data correctly if we do not see some characters?\n. Oops, sorry, I did not see the previous mention. No, it's clearly not something that I think is useful, personally. I very often select the name of a column to copy it. A completely clickable column name seems to have many disadvantages for very little advantages.. This may not have anything to do with this, but it often happens that Open Refine fails to parse a bad CSV properly. I had to first open it with Libre Office, save it, then import it in OR. I will try to find an example.. Here is this sort of CSV : Libre Office or even old specialized softwares can parse it, but OR 2.7 is struggling. It seems impossible to import in any other way than as \"line-based text files\".\nPages_that_link_to_Q5_Wikidata.zip\n\n. @thadguidry : you're right, it was a bad example: my csv does not contain any column of text with commas, so everything works well with your solution. This is not very bad, but let's say it can annoy people who export strange CSV from old databases full of long text fields. Archivists or librarians often work with this kind of database.. Hi,\nHave-you tried htmlText() instead of text() ?\nhttps://github.com/OpenRefine/OpenRefine/wiki/GREL-Other-Functions\n2016-04-07 16:25 GMT+02:00 Andreas notifications@github.com:\n\nGREL Jsoup is great, but I cannot seem to extract only the text from the\nhtml, even though it seems possible in the reference. Like this for example:\nvalue.parseHtml().text() or even value.parseHtml().select('body').text()\nI get the error that function text cannot be found.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1127\n. The Deri's RDF extension is outdated and doesn't work well with Open\nRefine. Try with Google Refine.\n\nKind regards.\n2016-09-05 20:40 GMT+02:00 Patrick Maroney notifications@github.com:\n\nVersions: Mac OSX 10.10.5, OpenRefine Version 2.6-rc.2 (1), Google Chrome\nVersion 47.0.2526.80 (64-bit) [same behavior with any browser).\nMany issues creating/editing RDF Schemas. Looking for general guidance\n-- Linked Data schema editing behavior is inconsistent with any and all\ntutorials, books, etc. For example resetting the RDF Schema for the\nPowerhouse Museum Data Set presents a window where the data overwrites the\nwindow and buttons cannot be easily reached (i.e., OK, Cancel)\n[image: openrefine-schemaissue-1_1]\nhttps://cloud.githubusercontent.com/assets/3220193/18255243/afd0e65e-7375-11e6-9037-b49fb8422149.png\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1154, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AF_sSj26V3YXoLbSXJQta78zLiXA_u-6ks5qnGI7gaJpZM4J1Q6O\n.\n. Updating extensions depends on their developer. The version \"Open Refine\n2.6\" of Deri's RDF extension  is in pre -release for two years ... (\nhttps://github.com/fadmaa/grefine-rdf-extension/releases )\nIts author was probably moved on. Personally, I use without problems three\nversions of Open Refine ( 2.6 RC2, Google Refine 2.5 and LODRefine ). I\nexport my projects and i re-import them into one or another version based\non the specific work that I want to perform.\n\n2016-09-06 0:35 GMT+02:00 Patrick Maroney notifications@github.com:\n\n@ettorizza: Many thanks - seconding the question as to which rdf-extension\nto use with 2.6 (or is advice to revert to Google Refine for RDF related\nwork?)\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1154#issuecomment-244818054,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AF_sSs9gkBZzzYMOtmLN6R4YwSUoZdUfks5qnJk_gaJpZM4J1Q6O\n.\n. Hi @thadguidry. According to my own tests, two columns of numeric IDs do not match with cell.cross() if one of the columns consists of \"normal\" numbers and the other of numbers from Excel (with an invisible trailing zero). @thadguidry @jackyq2015 I can assure you that cell.cross() works with numbers stored as \"numeric\" just as well as with strings (screencast). I have not tried it yet with dates.\n\n\nI used the VIB-BITS extension here, but the hidden GREL formula does not perform any number -> string conversion.\nforEach(cross(cell,\"clipboard\",\"Column 1\"),v,v.cells[\"Column 2\"].value)[0]\nI'm not a computer scientist, but I know Open Refine a bit and I'm not totally unaware of this kind of triviality ;). The cross() function sometimes has erratic behavior, with or whitout the VIB-BITS extension. Just now, a matching returned me, I do not know why, a \"NullPointerException\". I  restarted Open Refine and it worked correctly. (OR 2.7, Windows 10, Chrome).\nPrecision: I have a LOT of projects in my workspace directory. I do not know if it matters.. @jackyq2015 I did some tests to answer this question on SO, and it looks like it is no longer possible (with OR 3 RC1) to perform a cell.cross() with two numeric columns. \nError: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.String\nYet nothing seems to have changed in importerUtilities.java. How to explain that?. @jackyq2015  No other error message in the console.\n\n. As I feared, this change \"breaks\" the VibBits extension, which allows to make a cell.cross() visually. Too bad, it is very convenient.\n\n. @wetneb @ostephens Well, you make me doubt, and you are right : The behavior of this function was  exactly the same in Google Refine 2.5. \n\nI maintain the word \"bug\", but let's say that the bug seems to go back to the origins of the functionality. Now that I think about it, I have never seen it work as I wanted. That is, as the \"gather\" and \"spread\" functions of the R's tidyr package. And maybe that's why I never used it. :/\nFile used \ud83d\udc4d \nColumn 1,Column 2\nsourcefile1,24\nsourcefile1,34\nsourcefile1,17\nsourcefile2,-6\nsourcefile2,12\nsourcefile2,45\nsourcefile2,56\nsourcefile3,22\nsourcefile3,90\nsourcefile3,31\nsourcefile3,0\n. I was tired and busy last night, so I did not properly examine the problem. Probably also the day I said it was a bug. Thad is obviously right: the behavior of the function \"columnize by key-value\" makes perfect sense. This will be clearer with a chart:\n\n@belm104 has a table like the 1. \"Columnize by key-value\" will change it to 2. This matches the castfunction of R base, or the spread function in the package tidyr. But the table 2 can not be in a logical way what belm104 want. Here is what the cast function produces if applied to the table we are talking about: \n\nAs you can see, it is exactly the same as a \"columnize by key-value\".\n\n. But when we have an explicit Primary key (unique or composed), the function works exactly as expected and order doesn't matter.\n\nDataset : \ncountry,year,key,value\nAfghanistan,1999,cases,745\nAfghanistan,2000,cases,2666\nBrazil,1999,cases,37737\nBrazil,2000,cases,80488\nChina,1999,cases,212258\nChina,2000,cases,213766\nAfghanistan,1999,population,19987071\nAfghanistan,2000,population,20595360\nBrazil,1999,population,172006362\nBrazil,2000,population,174504898\nChina,1999,population,1272915272\nChina,2000,population,1280428583\n. @ostephens If I'm not mistaken, \"columnize by key-value\" works in the same way as the spread/cast functions in R, while \"transpose cells across columns into rows\" is the equivalent of gather/melt. The documentation  and tutorials of these two functions can therefore serve as a source of inspiration, I believe.. Hi Eric, \nFor now, the function only accepts GET requests, and I have no idea how to change the internal agent. \n+1 for the bounty. In the meantime, you can try this. Add column based on the column that contains your URLs, then use Python / Jyton instead of GREL.\nimport urllib2\nopener = urllib2.build_opener()\nopener.addheaders = [('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/602.3.12 (KHTML, like Gecko) Version/10.0.2 Safari/602.3.12')]\nresponse = opener.open(value)\nreturn response.read()\nCan you give a try ? If it works, you can manage a site that requires a username / password with a few more lines. \n. @ericjarvies It all depends on the authorization system that the site uses. But if it's the kind to ban user-agents and impose a username/password, it must also have terms of use that prohibit scraping. It will not take long for it to detect that your queries are being sent by software. Open Refine can do basic scraping, but this is not its primary function.. @ericjarvies Can you provide an example ?. This must be a very specific problem, which is why a real URL would be welcome. In general, Open Refine has no problem with URLs that return a pure XML response. I suspect an encoding problem in your URLs. Web browsers are very nice, when the URLs contain forbidden characters, they transform them and display an answer anyway. But not Open Refine. You need to use value.escape('url') when your value contains blank spaces and other forbidden characters.. I do not know if someone has already done that for Java, but I know that you can transform a Json cluster & edit to R code using a small Python script that performs the translation: \nhttps : //medium.com/optima-blog/semi-automated-text-cleaning-in-r-68054a9491da. @wetneb Interesting ! Open refine has become a sort of data warehouse for me, and for others also I guess. It is widely used in the information sciences community. For a librarian, not being able to add metadata is a torture.. We have an offer for $ 250 (100 missing)\nhttps://www.bountysource.com/issues/47809745-enhancement-add-fields-for-projects-metadata\n. Dublin Core can be an inspiration for metadata fields, although not all of the fifteen elements are needed in this case.\nTitle\nCreator\nSubject\nDescription\nPublisher\nContributor\nDate\nType\nFormat\nIdentifier\nSource\nLanguage\nRelation\nCoverage\nRights. My initial idea was to have fields directly editable right next to the project name, but a key-value menu like preferences makes a lot of sense. I just want to mention a possible UI implementation already used by Google Fusion tables. A screencast will be clearer than my explanations.\n\n. @denim2x Personally, I do not have any other expectations than those mentioned above: key-value fields that allow the user to add custom metadata to each project and a summary table of the metadata that allows to sorts and find project by date, by keywords, by title, etc. @thadguidry or @magdmartin probably have a more precise and more general view of what the enhancement should look like in practice.. Is anyone able to connect to BountySource? https://salt.bountysource.com/teams/openrefine. @jackyq2015 He stopped working on the issue. I wonder why. Too difficult for the bounty amount ?. Thanks for your answer, jacky. If I had to choose ten metadata fields, putting aside the dynamic metadata, here is what my selection would be.\n\nIdentifier (To make sure everyone is working on the same version. MD5 ?)\nCreator\nModified by/contributors (not essential for me)\nSubject/project (Several refine projects may be linked)\nDescription (free form of comment)\nSource (filepath or url)\nSize (at the creation ? Essential for cleaning old projects too heavy)\nCreated (from metadata.json)\nModified (from metadata.json)\nEncoding (from metadata.json)\n\nDoes anyone have any other suggestions?\n. @wetneb I mean something that makes it possible to identify the project in an unambiguous way. Oddly, Open Refine allows two projects to have exactly the same name. But this could be just the ID under which the project is stored in the Workspace Directory (1464554182862.project, etc.)  But I do not know how that number is created. May two people have two different Open Refine projects with the same ID?. @jackyq2015 At first glance, this really looks like what I had in mind! Good job ! Just a question for my information: I see that the metadata fields are not sortable. Is there any way to find a project by its metadata in this version?. As far as I'm concerned, you can claim the bounty. I do not know what @magdmartin thinks about it.. @jackyq2015 I can not launch the development version anymore.\nError in console\norg.mozilla.javascript.EcmaError: TypeError: [JavaPackage com.google.refine.commands.project.SetProjectMetaDataCommand] n'est pas une fonction, est un object (file:/C:/Users/Boulot/Documents/GitHub/OpenRefine/main/webapp/modules/core/MOD-INF/controller.js#71)\n        at org.mozilla.javascript.ScriptRuntime.constructError(ScriptRuntime.java:3654)\n        at org.mozilla.javascript.ScriptRuntime.constructError(ScriptRuntime.java:3632)\n        at org.mozilla.javascript.ScriptRuntime.typeError(ScriptRuntime.java:3660)\n        at org.mozilla.javascript.ScriptRuntime.typeError2(ScriptRuntime.java:3679)\n        at org.mozilla.javascript.ScriptRuntime.notFunctionError(ScriptRuntime.java:3734)\n        at org.mozilla.javascript.ScriptRuntime.notFunctionError(ScriptRuntime.java:3722)\n        at org.mozilla.javascript.ScriptRuntime.newObject(ScriptRuntime.java:2324)\n        at org.mozilla.javascript.gen.c22._c1(file:/C:/Users/Boulot/Documents/GitHub/OpenRefine/main/webapp/modules/core/MOD-INF/controller.js:71)\n        at org.mozilla.javascript.gen.c22._c4(file:/C:/Users/Boulot/Documents/GitHub/OpenRefine/main/webapp/modules/core/MOD-INF/controller.js:304)\n        at org.mozilla.javascript.gen.c22.call(file:/C:/Users/Boulot/Documents/GitHub/OpenRefine/main/webapp/modules/core/MOD-INF/controller.js)\n        at org.mozilla.javascript.ContextFactory.doTopCall(ContextFactory.java:398)\n        at org.mozilla.javascript.ScriptRuntime.doTopCall(ScriptRuntime.java:3065)\n        at org.mozilla.javascript.gen.c22.call(file:/C:/Users/Boulot/Documents/GitHub/OpenRefine/main/webapp/modules/core/MOD-INF/controller.js)\n        at edu.mit.simile.butterfly.ButterflyModuleImpl.scriptInit(ButterflyModuleImpl.java:636)\n        at edu.mit.simile.butterfly.ButterflyModuleImpl.init(ButterflyModuleImpl.java:94)\n        at edu.mit.simile.butterfly.Butterfly.initializeModule(Butterfly.java:476)\n        at edu.mit.simile.butterfly.Butterfly.initializeModule(Butterfly.java:470)\n        at edu.mit.simile.butterfly.Butterfly.configure(Butterfly.java:451)\n        at edu.mit.simile.butterfly.Butterfly.init(Butterfly.java:308)\n        at org.mortbay.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:440)\n        at org.mortbay.jetty.servlet.ServletHolder.doStart(ServletHolder.java:263)\n        at com.google.refine.RefineServer.configure(Refine.java:296)\n        at com.google.refine.RefineServer.init(Refine.java:208)\n        at com.google.refine.Refine.init(Refine.java:114)\n        at com.google.refine.Refine.main(Refine.java:108)\n\n. Same problem.. @jackyq2015  Oh, gosh... Sorry for my stupidity, I'm tired. It works !\n\n. Hmm, looks like the edit buttons (by clicking on \"edit metadata\") appear in the old projects, but not for those just created.\n\nEDIT : OK, I see, the problem comes from the ImportOptionMetadata row, far longuer than the screen.\n\n. \"Also, when change the userMetaData, the server has to restart to load it since it is a singleton.\"\nYep. Clicking on the column name did not seem to have any effect, but the sort works after restarting. I will do other tests tomorrow. Thanks again for everything, @jackyq2015 !. Just before I forget it: sorting by \"last modified\" does not work as one might expect. It does not sort values as dates, but as strings. If you click on it, all the chronological order of your projects is lost.\n\n\n. @jackyq2015  Wow, it looks like you did a lot of work last night! Thanks !\nI did some tests again this morning and here are the few bugs I have identified so far:\n\n\nSorting by date still does not work properly, even if the column now displays a date. I guess that projects are sorted by hour-minute-second rather than by day-month-year.\n\n\nLong free descriptions tend to mess up the display. Related to #1306 \n\n\nThere is a problem with user metadata. The new custom columns appear in the table, but they appear as \"undefined\" in the editing window and seems not editable. This is probably related to this error message in the console (unless I made a mistake somewhere by creating the metadata in preferences?):\n\n\n11:59:16.032 [       FileProjectManager] Exception when mergeEmptyUserMetadata (831ms)\norg.json.JSONException: JSONObject[\"name\"] not found.\n        at org.json.JSONObject.get(JSONObject.java:406)\n        at org.json.JSONObject.getString(JSONObject.java:577)\n        at com.google.refine.io.FileProjectManager.mergeEmptyUserMetadata(FileProjectManager.java:436)\n        at com.google.refine.io.FileProjectManager.loadFromFile(FileProjectManager.java:377)\n        at com.google.refine.io.FileProjectManager.load(FileProjectManager.java:338)\n        at com.google.refine.io.FileProjectManager.<init>(FileProjectManager.java:92)\n        at com.google.refine.io.FileProjectManager.initialize(FileProjectManager.java:79)\n        at com.google.refine.RefineServlet.init(RefineServlet.java:129)\n        at javax.servlet.GenericServlet.init(GenericServlet.java:241)\n        at edu.mit.simile.butterfly.Butterfly.init(Butterfly.java:180)\n        at org.mortbay.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:440)\n        at org.mortbay.jetty.servlet.ServletHolder.doStart(ServletHolder.java:263)\n        at com.google.refine.RefineServer.configure(Refine.java:296)\n        at com.google.refine.RefineServer.init(Refine.java:208)\n        at com.google.refine.Refine.init(Refine.java:114)\n        at com.google.refine.Refine.main(Refine.java:108)\n11:59:16.034 [       FileProjectManager] Exception when mergeEmptyUserMetadata (2ms)\norg.json.JSONException: JSONObject[\"name\"] not found.\n        at org.json.JSONObject.get(JSONObject.java:406)\n        at org.json.JSONObject.getString(JSONObject.java:577)\n        at com.google.refine.io.FileProjectManager.mergeEmptyUserMetadata(FileProjectManager.java:436)\n        at com.google.refine.io.FileProjectManager.loadFromFile(FileProjectManager.java:377)\n        at com.google.refine.io.FileProjectManager.load(FileProjectManager.java:338)\n        at com.google.refine.io.FileProjectManager.<init>(FileProjectManager.java:92)\n        at com.google.refine.io.FileProjectManager.initialize(FileProjectManager.java:79)\n        at com.google.refine.RefineServlet.init(RefineServlet.java:129)\n        at javax.servlet.GenericServlet.init(GenericServlet.java:241)\n        at edu.mit.simile.butterfly.Butterfly.init(Butterfly.java:180)\n        at org.mortbay.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:440)\n        at org.mortbay.jetty.servlet.ServletHolder.doStart(ServletHolder.java:263)\n        at com.google.refine.RefineServer.configure(Refine.java:296)\n        at com.google.refine.RefineServer.init(Refine.java:208)\n        at com.google.refine.Refine.init(Refine.java:114)\n        at com.google.refine.Refine.main(Refine.java:108)\n\nThe row number column does not extract project metadata (even for a new project) and is not editable manually.\n\nNote: I bothered every time to restart Open Refine to check if the bug was persistent or not.\n\nDetail: we can remove the small \"sort triangles\" in the first three columns (delete, rename, about). In addition, is it still necessary that the text of these cells appear only on mouse over? It's a question of aesthetics, but these three empty columns seem weird.\n\nHere is a summary of these bugs in a walktrough screencast:\n\n. Great @jackyq2015 ! If we put aside the \"user metadata\", I think we are close to the end. I see that you have chosen as an option to keep a fixed size for the display of columns on the screen, and a fixed size for the columns. I think it's a good choice considering the limitations of the UI. Here is what it looks with the original metadata and three additional columns of user metadata.\nWith a 100% zoom in the browser (15-inch screen).\n\nWith a zoom of 80%.\n\nIt is therefore necessary to save space. The most obvious way seems to strongly reduce the first three columns (or eliminate them with a clever trick), as well as the column \"row number\" (it can be fixed at the width of 7 digits, I can not see how to manage more than 99 million lines in Open Refine) .\nEveryone: What do you think about it?. The \"edit cells\" -> Transform\" and \"All -> transform\" windows sometimes extend the entire screen (but not the \"add column based on this column\" window)\nNot really a problem for me.\n\n. The display of these dates on a 15-inch screen is problematic. It might also be necessary to reduce a little the font of the words \"rename\" and \"about\".\n\nA shorter timestamp, without the milliseconds, could solve the problem easily.\n. @wetneb @jackyq2015 If I rely on my own practices, it's true that I rename projects much more often while they are open. But I do not know if everyone does it that way.. Thanks a lot for all this work, @jackyq2015 ! As I said above, you already deserved the bounty with the first draft.\nThe system as it suits me perfectly. However, I wonder if it's ready for a public release. There are still some display issues that could frustrate users.\nThe dates display in Chrome or Edge on a 15-inch screen with a 100% zoom is perfectible, but that's just aesthetics:\n\nNote that there is not this problem with Firefox:\n\nBut in every browsers, the metadata editing window comes out of the screen again. Each time you have to zoom out to access the \"edit\" buttons.\n\n\nThere is also a small problem of consistency between the \"transform\" window, which is displayed all along the screen, and other edit windows, which remained the same.\n\n\nAll this annoys me very little. I set Open Refine on a 90% screen zoom. But I'm afraid it looks a little buggy in an official version 2.7.2.\nWhat do you think, everyone?. Would be a great extension of course, but a lot of work. Maybe with JRI ?. @thadguidry +1000 \nR has a very active community, but a bit apart from other programmers. It's a booming language, as evidenced by each StackOverflow survey. R has also a steep learning curve, and being able to use it in a visual interface like that of OR would facilitate its learning.\nMost importantly, a full support of R would bring a lot of potentialities to OpenRefine. The Jython extension was a great idea. The problem is that Jython only supports some of the Python modules (those that are not written in C). We can not use a whole range of great packages like numpy, pandas, NLTK and so on. With full support for R (including packages written in C++), there will be not much you cannot do in OpenRefine.. @thadguidry Yes, of course, you can close this thread if you want. I keep in mind the idea that GREL or Python code snippets exchangeable between users would be a good idea, but it's not a priority.. I have no problem with this file and this GREL formula.\n\n. > I believe it's because UTF-8 with BOM\nYep, there is an invisble character juste before the word UNID, so cells['UNID'].value doesn't work. If you rename the column UNID, it works. . Memory problems are so common when you organize Open Refine training. I wonder if it would not be a good idea to display a pop up when OR runs out of resources. The other solution would obviously be to set the -Xmx automatically according to the computer possibilities and the presence or not of Java 64 bits, but I imagine it would be more work.. No problem on my Windows 10 installation (Open Refine 2.7)\n\n. Yep, the smartSplit() result is more consistent:\nrow value   value.smartSplit(/a/)\n1.  abcdef  [ \"\", \"bcdef\" ]\n2.  bcdefa  [ \"bcdef\", \"\" ]\n3.  badef   [ \"b\", \"def\" ]\nSame thing for partition() with omitfragment=true:\nrow value   value.partition(/a/, true)\n1.  abcdef  [ \"\", \"bcdef\" ]\n2.  bcdefa  [ \"bcdef\", \"\" ]\n3.  badef   [ \"b\", \"def\" ]\n. @msaby But before, my dream would be that you accept my answers on StackOverflow. I can not sleep anymore (\"but why does not he accept ???\") :p . \"I'm just saying that you will find that some folks (including me) might treat this as a false positive in some other languages.\"\n@thadguidry That's exactly what I said on SO. Imho, \"ecole\" and \"\u00e9cole \u00e9cole \u00e9cole\" are two strings far too different to be merged by a fingerprint, which is supposed to be the most conservative operation among the clustering algos. One might ask whether the deduplication of tokens is really indispensable.. There may be room for a \"safe clustering\" based on fingerprint, something that could be advertised as \"click on 'select all' and then on 'merge', your results will be correct in 99% of the cases.\" \nAt the same time, new algorithms of string similarity have been developped since 2012, or in any case ported in Java. If you wish, I can make a list of those that deserve to be added to OR.. Open Refine probably does not accept this story of \"clitoris de 15 cm\" in the Json.. @Gucci1986 No, it was just a bad joke ^^ Pleased that you have solved the problem by another way, but it would still be interesting to understand what happened. This is maybe a bug. A screenshot isn't very useful, we need a file that can be imported into OR to reproduce the problem. If there is no privacy concern, you could add the complete json file as an attachement.. @ostephens When you say  \"using a bundled java\", would that mean that Open Refine would become totally portable and would not need to install anything else to work? If so, it might interest some users, I believe.\nhttps://twitter.com/parody_bit/status/923931736479883264. Thank you for this work @ostephens ! Regarding the automatic trim, I have no specific opinion, except that I personally prefer doing this kind of thing myself rather than letting the software perform \"under the hood\" operations that I may not wish.\nJust a detail, perhaps unimportant: this change breaks the symmetry between \"split mutlivalued cells\" and \"join multivalued cells\", its sister feature. I'm not sure it could be a problem for the users, but I point it out anyway.. Issue #1113 is a good example of the frustration that can be felt when a trim is done without being asked.. @ostephens I do not have a specific use case in mind. No doubt there is at least one person on this earth who dreams of being able to join multivalued cells by \\n, but I am not sure that it is very frequent.. The colored symbols also seem to me both clearer and more aesthetic.. This question on  StackOverflow shows the importance of this new feature. The file on the Open Refine screenshot looks very clean compared to the original Excel file, actually full of tabs and spaces.. Hi @thadguidry ,\nYep, I noticed the same thing as I tried the formulas you proposed in response to the question on the mailing list. I also get 550 milliseconds. In addition, I have the impression that there is an error in the documentation. It says :\n```\n\nFor a date of the form: \"1/4/2012 13:30:00\" use GREL function:\ntoDate(value,\"dd/mm/YYYY H:m:s\")\n```\n\nBut using this method, I get a wrong date. \n\nThe only method that seems to work is:\ntoDate(value,\"d/M/y H:m:s\")\n(But this does not solve the problem of diff dates calculation with a hundredth of a second precision.)\nEDIT : okay, I can also get the correct date using:\nvalue.toDate(\"dd/MM/yyyy H:m:s\")\nAnd parsing milliseconds seems ok provided that it does not exceed three digits of precision (probably because milliseconds with four digits = 1000 milliseconds = 1 second.)\n\nI'm sure I'm missing something obvious. The best is to think about it tomorrow before editing the documentation...\n. According to this thread on SO, this looks like the normal behavior of SimpleDateFormat. The word \"millisecond\" must be taken in the sense of the unit of measure, not as a synonym for \"decimal part of the seconds\". So, if a user supplies a date with nanosecond precision and uses SimpleDate directly on it, he will get a result with an error of 11 days in excess. But it looks like Java 8 fix the problem with a new package, if I understand correctly. This seems misleading for those who use both, since in java.time, S means this time \"the fraction part of the seconds\".. I can parse it in OR 3.1\nvalue.toDate(\"EEE MMM dd h:m:s z yyyy\")\n[image: screenshot-127.0.0.1-3333-2019.01.07-16-02-08.png]\nEttore Rizza\nLe lun. 7 janv. 2019 \u00e0 15:59, Owen Stephens notifications@github.com a\n\u00e9crit :\n\n@srugano https://github.com/srugano no - it looks like there is a\nproblem in parsing dates starting with the week day name right now - see\n1908 https://github.com/OpenRefine/OpenRefine/issues/1908\nAs a work around right now you could use:\nvalue.substring(4).toDate()\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1287#issuecomment-451961797,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AF_sSqBUhtRdPDoaupu8rGsuTPacOEMzks5vA2C2gaJpZM4QFNab\n.\n. You are obviously right, @jackyq2015 . But we can safely bet that, nowadays, most users will increase the memory and give it a default level based on their hardware capabilities. They will then increase it on a case-by-case basis. Half of the RAM seems to me a good starting point. I give more personally (about 75%, 12GB out of 16), but I am kind of heavy user.. @jackyq2015 Simply that it works in a more natural/classic/straightforward way. On the string \"Hello World\", how would you, with match(), extract all words that start with a capital letter? In any regex engine, the answer will be something like [A-Z]\\w+, and it would be nice if value.find(/[A-Z]\\w+/) gives as result [\"Hello\", \"World\"].\n\nOr maybe I don't understand this function correctly. The problem may be that I'd like to see it do things for which it was not designed.. @jackyq2015 It's not a bug, it's unfortunately a feature. The documentation is clear on this. \nOn the string \"hello 123456 goodbye\", you can't simply extract the number with value.match(/(\\d{6})/).  You must match the whole string and indicate between parentheses the part you want to extract, that is to say: value.match(/.*(\\d{6}).*/).\nIt's a little boring when it comes to extracting a single element, but when there are several in the string, it becomes a puzzle.\n\nThat's why I prefer to use Jython when I have to find something with regex.\n\n. @jackyq2015 Exactly. Looks like the find() method  already exists and it would be easy to create a match() variant named find().\n\nmatches()\nThe matches() method in the Matcher class matches the regular expression against the whole text passed to the Pattern.matcher() method, when the Matcher was created. Here is a Matcher.matches() example:\nString patternString = \".http://.\";\nPattern pattern = Pattern.compile(patternString);\nboolean matches = matcher.matches();\nIf the regular expression matches the whole text, then the matches() method returns true. If not, the matches() method returns false.\nYou cannot use the matches() method to search for multiple occurrences of a regular expression in a text. For that, you need to use the find(), start() and end() methods.\n\nI've never programmed in Java, but I would be interested to immerse myself one of these days in this @ostephens tutorial.. Related question on StackOverflow.. I see the development version no longer behaves in the same way when the regex box is checked and a bad regular expression is written. Errors always appear as you type, but no more the gray spinning wheel. It's certainly related to #1203.\n\n. @jackyq2015  @schignel Ok, I see, the problem comes from Poper Blocker extension. \n. https://github.com/fadmaa/grefine-rdf-extension\nThe author is @fmaali . The Github repo does not mention a type of license. I don't know who owns the intellectual property. This extension was developed within the DERI research center, which no longer exists, I believe, or has been merged. The best is to ask the authors (The second is cygri).\nEDIT : it's a BSD licence, according to the official website.. @fpompermaier For your information, here is another fork that I have not tested, but quite recent. However, it uses a modified version of Open Refine with Jetty9, if I understand correctly, and I don't remember why.. By the way, it would be a good idea to consider adding a simple \"select all the sheets\" checkbox for Excel files. It is not uncommon for OCRs to produce Excel files containing several dozens of sheets (one per page).. Another related question on the mailing list : https://groups.google.com/forum/#!topic/openrefine/kOyVbjbe6s8. @thadguidry @wetneb No problem to close the discussion. But I want to make sure I've been clear, because the column variable I'm proposing is not primarily about making stats or turning Open Refine into a spreadsheet. Its main purpose would be to be able to answer questions such as \"in the following dataset, what names in ColumnA are also in ColumnB?\"\nColumnA, ColumnB\nJean, Albert\nAlbert,Jean\nPierre,Thad\nAntonin,Jacky\nI can answer the question using a cheap trick, but it's difficult for users to understand the logic behind it.. I've before my eyes a user case which illustrates the potential utility of this variable \"column\". Let's take a txt file containing lines (for example those of an ocerized PDF) with no other structure than this one: the lines we are interested in are always followed by a line starting with the word \"total\".\nExample:\nrow1\nrow2\nINTERESTING ROW1\ntotal 1MB\nrow4\nrow5\nrow6\nrow7\nINTERESTING ROW2\ntotal 16MB\nrow8\nINTERESTING ROW3\ntotal 3MB\nIn the real file, interesting lines are not in capital letters. In fact, let's say that they do not contain any pattern that allows to filter them by a regular expression. They can only be found by first identifying the rows starting with \"total\", then taking the ones preceding them.\nHow to extract interesting lines with GREL? This is, I think, a fairly common problem.\nRegarding the file size issue : is there no way to ensure that operations on a number of rows too large to fit in RAM are done by chunks, as does pandas with the argument chunksize (pd.read_csv (\"very_big_file.csv\", chunksize = 1000))?.  \"Is it because we do a crappy job on hiding columns ? \"\nNot at all, @thadguidry . Open Refine offers many ways to filter rows or columns. But these are facets, inherently unstable. It is often much safer to create a subset from these facets. Especially since multiple facets require a lot of RAM. When you have a column containing a million rows and lots of duplicates, it is much faster to export this column to a deduplicated version, to perform clustering, reconciliations, so on, and then cell.cross() join the result to the original file. Sometimes, too, the project has become so heavy (especially because of multiple reconciliations with Wikidata, a lot of json files extracted from APIs, too many records...) that it is better to work on a lighter and cleaner copy. \nThis is not very different from what we do in R or Pandas by creating multiple dataframes related to the same project.\nThe new \"metadata project\" feature will allow us to group all OpenRefine projects related to the same real \"Subject\". The feature I suggest would only be to facilitate this work on multiple datasets related to each other.. One of the vib-bits extensions has a \"save facets\" option. By clicking on \"permalink\" just under the name of the project, you can save the facets in the form of a long url. However, sometimes the process fails. I wonder if this is not due to a question of maximum URL length ...\n\nBy the way, I read on Twitter some comments from users who ask why these vib-bits extension so convenient are not integrated into the Open Refine core. I 've looked for that, but I can not find an Open Source license for these extensions.. I'm not even sure it's worth putting a bounty on this kind of feature. If I understand correctly, they pay late, get a large commission, all for something that will be done anyway by someone from the Open Refine team.. @jackyq2015 Thanks for the information, this confirms my intuition: the \"permalinks\" of vib-bits that display a blank screen are those that contain more than 2000 characters, like this one (2019)\nhttp://127.0.0.1:3333/project?project=1545787044416&ui=%7B%22facets%22%3A%5B%7B%22c%22%3A%7B%22type%22%3A%22list%22%2C%22name%22%3A%22_%20-%20spatial%20-%20key%20-%20key%22%2C%22columnName%22%3A%22_%20-%20spatial%20-%20key%20-%20key%22%2C%22expression%22%3A%22value%22%2C%22omitBlank%22%3Afalse%2C%22omitError%22%3Afalse%2C%22selectBlank%22%3Afalse%2C%22selectError%22%3Afalse%2C%22invert%22%3Afalse%7D%2C%22o%22%3A%7B%22sort%22%3A%22count%22%7D%2C%22s%22%3A%5B%7B%22v%22%3A%7B%22v%22%3A%22LOC_MONUMENT%22%2C%22l%22%3A%22LOC_MONUMENT%22%7D%7D%5D%7D%2C%7B%22c%22%3A%7B%22type%22%3A%22list%22%2C%22name%22%3A%22_%20-%20spatial%20-%20value%20-%20value%3A%20judgment%22%2C%22columnName%22%3A%22_%20-%20spatial%20-%20value%20-%20value%22%2C%22expression%22%3A%22forNonBlank(cell.recon.judgment%2C%20v%2C%20v%2C%20if(isNonBlank(value)%2C%20%5C%22(unreconciled)%5C%22%2C%20%5C%22(blank)%5C%22))%22%2C%22omitBlank%22%3Afalse%2C%22omitError%22%3Afalse%2C%22selectBlank%22%3Afalse%2C%22selectError%22%3Afalse%2C%22invert%22%3Afalse%7D%2C%22o%22%3A%7B%22scroll%22%3Afalse%2C%22sort%22%3A%22name%22%7D%2C%22s%22%3A%5B%7B%22v%22%3A%7B%22v%22%3A%22(unreconciled)%22%2C%22l%22%3A%22(unreconciled)%22%7D%7D%5D%7D%2C%7B%22c%22%3A%7B%22type%22%3A%22list%22%2C%22name%22%3A%22record_spatial%3A%20judgment%22%2C%22columnName%22%3A%22record_spatial%22%2C%22expression%22%3A%22forNonBlank(cell.recon.judgment%2C%20v%2C%20v%2C%20if(isNonBlank(value)%2C%20%5C%22(unreconciled)%5C%22%2C%20%5C%22(blank)%5C%22))%22%2C%22omitBlank%22%3Afalse%2C%22omitError%22%3Afalse%2C%22selectBlank%22%3Afalse%2C%22selectError%22%3Afalse%2C%22invert%22%3Afalse%7D%2C%22o%22%3A%7B%22scroll%22%3Afalse%2C%22sort%22%3A%22name%22%7D%2C%22s%22%3A%5B%7B%22v%22%3A%7B%22v%22%3A%22none%22%2C%22l%22%3A%22none%22%7D%7D%5D%7D%2C%7B%22c%22%3A%7B%22type%22%3A%22list%22%2C%22name%22%3A%22beeldid%22%2C%22columnName%22%3A%22beeldid%22%2C%22expression%22%3A%22value%22%2C%22omitBlank%22%3Afalse%2C%22omitError%22%3Afalse%2C%22selectBlank%22%3Afalse%2C%22selectError%22%3Afalse%2C%22invert%22%3Afalse%7D%2C%22o%22%3A%7B%22sort%22%3A%22name%22%7D%2C%22s%22%3A%5B%7B%22v%22%3A%7B%22v%22%3A%2200029386%22%2C%22l%22%3A%2200029386%22%7D%7D%5D%7D%5D%7D\nEdit : Mmh, no, the screen eventually appear after one minute.. I will try to translate \"About\" into all supported languages. But how does this system work? Does Weblate update the interface itself in the development version? \nNote: no way to set it for untranslated English words to be displayed in orginal version?. The word \"About\" has just been translated on Weblate into all languages.. Exact. If I create a project from the \"\u0141ukasz.csv\" file, it is saved as \"ukasz csv\". If I then rename it \"\u0141ukasz\", the change is replaced at the next start with \"?ukasz\".\nWindows 10, OR 2.8, JDK 8\n. Most dangerous option ever ! Especially since it can be done easily by opening the workspace directory, CTRL + A, DEL. \n@xseris Your tag system is pretty cool. Could you share the recipe? :). In this case, it's different. The option seems quite justifiable.. @xseris Many thanks, I'll try this !. \"By showing them, it provides a good access point to edit the tag, but at the same time it take too much space. \"\n@jackyq2015 Looks like the system is able to handle a relative large number of tags without adversely affecting the display.\n\nI think the \"Subject\" metadata would become a duplicate with this implementation, which would free up a column.\n. I wonder if it would not be a good idea to make the dark blue tags clickable.. You may be right @thadguidry . I do not have a strong opinion on the topic. In Dublin Core, the \"Subject\" element can be expressed as \"keywords, key phrases or classification codes that describe a topic of the resource. Recommended best practice is to select a value from a controlled vocabulary or formal classification scheme\". \nIn this case, using tags instead of a single subject has the advantage of the polyhierarchy. But I know little Schema.org.. \"\"subject is formal and tag system is more free style\"\u2026 In fact I was about to write the exact opposite!\"\n@wetneb I agree for this particular case. I really like the possibility of adding stored tags when creating the project. It can become more than just folksonomy. As long as these tags are stored in a separate Json file, this would give users the ability to create and use their own controlled vocabulary.\n\n. \"this looks pretty useless to me (and a waste of space) in the current form. \"\n@wetneb @jackyq2015 Same problem, and even worse, with the metadata \"creator\". I did not bothered once to fill it. In 99.9% of cases, I am the creator. Should not it be automatically filled in with the username of the session, leaving that field editable?. Right ! This is obvious for if() or forEach(), but isBlank() and co look much more like functions. I'll add a clarification in the documentation.. Would there not be the occasion to add Owen to the contributors ?. I can create a project with these symbols, and they will be displayed, but they turn into white squares at the first edition attempt. \n\nTheir unicode value also changed.\n\n\n(Windows 10, OR 2.8, Chrome). @jackyq2015 I can reproduce the issue on Windows 10. We have a strange phenomenon in the text facet.\n\n. @ostephens Are we talking about the same thing? The \"match each cell to its best candidate\" feature  works with any reconciliation service, not just Freebase. I see it as a very practical way to batch reconcile cells  when, for example, a facet has been used to filter candidates with a high score. This is at least how I use it, but perhaps it had another goal in the beginning?. Yes, this feature has certainly become useless. Having never used it in the past, I don't know if it was useful nor if it should be adapted.. @wetneb It can be accessed, but as a value in a dictionary ( return cell['recon']['match']['id'] ). the dot notation does not work in Jython.\n\n. Of course, it would be much more convenient ! To get the list of values in a record, the formula is row['record']['cell']['column name']['value']\nThis is very boring to write without autocompletion .... @ostephens I tested yesterday the development version by adding tags to a project already created, and now I get this bug when I try to open the project in question in the stable version 2.8. Is it a related issue?\nNote : I can open the project without problem in the development version.\njava.lang.NullPointerException\n        at java.util.Calendar.setTime(Unknown Source)\n        at java.text.SimpleDateFormat.format(Unknown Source)\n        at java.text.SimpleDateFormat.format(Unknown Source)\n        at java.text.DateFormat.format(Unknown Source)\n        at com.google.refine.util.ParsingUtilities.dateToString(ParsingUtilities.java:177)\n        at com.google.refine.history.HistoryEntry.write(HistoryEntry.java:112)\n        at com.google.refine.history.History.write(History.java:270)\n        at com.google.refine.commands.Command.respondJSON(Command.java:311)\n        at com.google.refine.commands.Command.respondJSON(Command.java:297)\n        at com.google.refine.commands.history.GetHistoryCommand.doGet(GetHistoryCommand.java:54)\n        at com.google.refine.RefineServlet.service(RefineServlet.java:170)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n        at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n        at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:923)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:547)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n        at java.lang.Thread.run(Unknown Source)\n22:46:21.738 [                   refine] GET /command/core/get-history (270ms)\n22:46:21.742 [          org.mortbay.log] /command/core/get-history (4ms)\njava.lang.NullPointerException\n        at java.util.Calendar.setTime(Unknown Source)\n        at java.text.SimpleDateFormat.format(Unknown Source)\n        at java.text.SimpleDateFormat.format(Unknown Source)\n        at java.text.DateFormat.format(Unknown Source)\n        at com.google.refine.util.ParsingUtilities.dateToString(ParsingUtilities.java:177)\n        at com.google.refine.history.HistoryEntry.write(HistoryEntry.java:112)\n        at com.google.refine.history.History.write(History.java:270)\n        at com.google.refine.commands.Command.respondJSON(Command.java:311)\n        at com.google.refine.commands.Command.respondJSON(Command.java:297)\n        at com.google.refine.commands.history.GetHistoryCommand.doGet(GetHistoryCommand.java:54)\n        at com.google.refine.RefineServlet.service(RefineServlet.java:170)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n        at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n        at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:923)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:547)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n        at java.lang.Thread.run(Unknown Source). @jackyq2015 Yes, looks like projects (or only some projects, don't know) created with the development version can not be opened in previous versions. This is a major change that will need to be reported in bold to users. For example, we can no longer advise those who want to use the RDF extension to export their project and re-import it into LODRefine. cc @ostephens . \"we have another ticket to deal with operation history/comments.\"\nHi @jackyq2015 . I'll take a look at this ticket. Where is it ? \nTo be more clear, this metadata \"comments\" that I propose is not to comment on the history of JSON operations. It would rather be a notepad integrated into the project. Indeed, most users (librarians, datajournalists, scientists ...) do not just clean up their data with Open Refine: most often, they must also write a report on the operations they have performed. By attaching the draft of this report to the OpenRefine project, all the information would be stored in one place.\n\"we can have a preference setting \"creator\" and each time we pull it from there and feed to the new project created.\"\nGood idea. We could define a preference setting \"username\". If the project is created, this username becomes the \"Creator\". If another username modifies the project, this name would be added to a list of metadata \"Contributor\". Once a \"username\" is set, all old projects that did not have \"Creator\" metadata will be automatically assigned to this username.. Exactly @jackyq2015 , even if @thadguidry goes a step further with the idea to annotate each column. When working on an Open Refine project, we take all sorts of notes to document our work and then describe what we did. We sometimes create Grel formulas or Jython scripts that we would like to reuse. It is not always easy, months later, to extract these scripts from the history in JSON.\nHere is an example of note taken months ago (in French, sorry). This text file is in my Dropbox, far from the relevant Open Refine project. I do not even know which one it is, since there are six versions of the same. It would be great if this kind of note could be integrated into the project itself.\nI have no strong views on whether such notes should be regarded as metadata or as part of the project. After all, any data can be considered as a metadata depending on the way we look at it.\nI really believe that the documentation of research data is a problem in the scientific community. With all the improvements in recent months, Open Refine could become a true Electronic Lab Notebook. \nOperations_Refine_BelgicaPress.txt\n. Thank you, @jackyq2015 . Regarding \"subject\", I use it for the moment to regroup files in the same real world project. Maybe we will discover later that it duplicates the tags, but for now, better to leave it visible. Description, on the other hand, should not be hidden under any circumstances. It's this field that makes it easy to find a project among others with very similar names.. @magdmartin Ah, but I totally agree with Thad's comment. As long as OpenRefine allows one day to associate notes with a project, I'm happy, no matter how those notes are stored. I first suggested including them in the metadata schema because for me any information about the file is a metadata. But from a conceptual point of view, these notes can also be considered as a separate document that would be linked to the OR project using a DCMI term like \"isReferencedBy\" or \"relation\". @wetneb Yes, I saw this as a generic option. It is almost essential for JSON files, but it would make it much easier to work in Open Refine, as I said above, as well as to export the file to a database.\nAnother possibility, much more flexible, would be to allow a batch \"transform\" on the column names. Something like colNames.replace(/_-_^/, '').replace(' ', '_'). Related question on the mailing list.. Everything looks fine for me!. Thank you, @jackyq2015 !\n\nAdded a few lines about it in the Wiki, feel free to correct any error.. @jackyq2015 I juste noticed a strange behavior by reviewing the new contains()PR (very similar to your find() ) \nLooks like some special characters are always matched in string mode.\n\nEdit : actually, there is no string mode if I understand correctly. Everything is transformed into regex, between // or not, as in match(). This is a little misleading I think, especially because the behaviour is not the same using \"\" or //.\n(this should match either a litteral  \\s+ or blank spaces, but not a s alone )\n\n. @thadguidry Nice catch. It seems that it is related to the Adblock+ extension  for Chrome. Thanks !. @wetneb I had thought of this possibility. After all, we often know in advance what we want to extract from the JSON / XML / HTML. The only case where storing the answer is useful it's when one wants to create several columns from the same response. The ideal would be to have the possibility to store the response, or parse it on the fly with something like:\nvalue.fetchUrl().parseJson().... Hello all. Looks like OR 3 has taken the exact opposite of what has been discussed here: the HTML extracted from URLs is now pretty-printed, which greatly increases the weight of the page. Now, scraping 100 URLS, and even 10, has become a pain. OR crashes or slows down. . Until the problem is solved, I share the better solution I found for the browser to support these tons of heavy HTML or JSON:\n1 \u00b0 fetch URLs, store the result in a column called for example \"HTML_RESULT\".\n2 \u00b0 Click immediately on \"View -> Collapse this column\". \n3 \u00b0 Extract the HTML elements that interest you in an empty column by using cells['HTML_RESULT'].value.parseHtml().select(<YOUR JSOUP SELECTOR>)\n(or cells['JSON_RESULT'].value.parseJson().pathtotheelement, of course)\n. Since most data cleaning softwares have a function for padding, it must be a common need. @magdmartin gives a workaround in GREL, but it's quite difficult to remember.. Mmh, yes, this font is a little cheap I think. It looks a bit \"Times New Roman bold and the deal is done\". I'm sure we can find a better one by looking a little.. I wonder if we should not instead put this feature in the \"All\" menu with a check list of columns, as with \"transform all\". If two columns are checked, only the blank rows in both will be deleted. . Related to #1439, #239 and #68 .. Hi @joanneong .  The documentation is right to say that slice() returns an array. The problem is that the arrays do not display in the columns. Open Refine columns only display strings, numbers, booleans, or dates. To display all the elements of an array in the column, you have to concatenate them with join():\nvalue = [1,2,3,4]\nvalue.join (',')\nI'm sure all of this is hidden somewhere in the doc, but it might need to be mentioned more explicitly in the Array functions section.\nEdit : If it's in the documentation, it's VERY well hidden.... I also think that this function should produce an array, not a string. It would be on the one hand more coherent intellectually and on the other more practical in many cases. After all, a range function is most often used as iterable.\nThe optional step argument is a good idea. In this way, we could make the forRange function obsolete  (who uses it?) and use only forEach to iterate arrays:\nforEach(range(number from, number to, number step), variable v, expression e). Technical detail: must the stop be included in the array? The range function of Python does not include it:\nlist(range(1,10)) --> [1,2,3,4,5,6,7,8,9]\nThe current function forRange either:\nforRange(1,10,1,e, e) --> [1,2,3,4,5,6,7,8,9]\nPersonally, I would support the idea of including it, as 90% of humans that would answer the question \"give me the numbers from 1 to 10\".. It's true that it'll introduce inconsistency, since all OR array functions use a zero-based indexing.\nvalue = [1,2,3]\nvalue.slice(1,2) -> [2] #(and not [1,2] as it would be the case with this range() function)\nvalue.slice(1,3) -> [2,3] \nAll this seems very clear to any programmer (except perhaps those who only use R), but I remember that it disturbed me a lot when I started learning OpenRefine.... @joanneong The current behaviour is : \nGREL\nforRange(2,2,1, e, e) --> []\nPython/Jython\nreturn range(2,2) --> []. > can you remind me why range returns an array of strings and not integers?\nI thought first it was to facilitate concatenations, but I forgot that the join() function is a good girl and it automatically cast into strings.. @thadguidry can I suggest a \"report issue\" template a little less invasive, such as : \n```\nWhen reporting a bug please provide the following information to help reproduce the bug:\nVersion of OpenRefine used (Google Refine 2.6, OpenRefine2.8, an other distribution?):\n2.8\nOperating Systems and version:\nBrowser + version used - Please note that OpenRefine doesn't support Internet Explorer but works OK in most cases:\nSteps followed to create the issue:\nIf you are allowed and are OK with making your data public, it would be awesome if you can include the data causing the issue or a URL pointing to where the data is (if your concerned about keeping your data private, ping us on our mailing list):\nCurrent Results:\nExpected Results:\n```. @thadguidry It's just the size of the text in H1 that seems to me too big. We only see the questions and hardly the answers.\n\nMy proposition of markdown template looks more like this : \nWhen reporting a bug please provide the following information to help reproduce the bug:\nVersion of OpenRefine used (Google Refine 2.6, OpenRefine2.8, an other distribution?):\n2.8\nOperating Systems and version:\nWindows 10\nBrowser + version used - Please note that OpenRefine doesn't support Internet Explorer but works OK in most cases:\nChrome\nSteps followed to create the issue:\nNothing\nIf you are allowed and are OK with making your data public, it would be awesome if you can include the data causing the issue or a URL pointing to where the data is (if your concerned about keeping your data private, ping us on our mailing list):\nOK\nCurrent Results:\nExpected Results:. A brand new find()function is implemented in the development version. . @ostephens I'm trying to understand how to use this function.  If you have a column containing given names and another with the names, the safe concatenation can be done with coalesce(cells.given_name.value, cells.name.value), that's it ?\nI can't figure out what's my mistake.\n\n. Sorry @ostephens, I had followed the threads distractedly and I remained on the idea that the primary purpose of this function was to facilitate concatenation. But I see by reading the totality of the discussions that you went to a transposition of coalesce exactly as in SQL. In this case, indeed, it seems better to not disorient those who are already familiar with this function.. @ostephens Would turning errors into null only in the coalesce function not solve the problem? \nIn my example, I do not see any other case that can produce an error than having a nullmiddle_name -  since dates, numbers and booleans are casted to strings.\n\nIn other words, this would be equivalent to this formula : \nvalue + \" \" + coalesce(if(isError(cells.middle_name.value), null, cells.middle_name.value), \"\") + \" \" + cells.name.value\nBut I suppose we can be even more explicit and limit the transformation of errors to null only if the error is \"cannot retrieve field from null\" (since this type of error cannot exist in a function which consists precisely in managing the null values)\nSo:\ncoalesce(null, null) -> null\ncoalesce(Error: Cannot retrieve field from null, \"string\") -> \"string\". Why a forEach()? Basically, looks like this would add a shortcut to this boring formula:\nrow.record.cells['column name'].value.length() - row.record.cells['column name'].value.uniques().length() (where 0 means 'no duplicate in the record')\nBut there are many other useful calculations that can be done on the values of a record.\nThis ties in with Martin's idea of allowing users to store their own functions in the drop-down menu.. It would probably be a little slower to load, but what about an iframe that would display the mobile version of the Wikidata page in question? A bit like this, but with two scroll bars to move the mini-web page.\n\n. Tanks @wetneb , but I just have a good memory (I think it was the system used in LODrefine). ;)\nThat said, it would not reduce much the number of clicks in my example with Amsterdam. I know that the first one has the smallest Qid and it's probably the capital of the Netherlands, but what if I'm looking for Jacques Brel's song? Thad's system also involves hovering each candidates and I guess it will take a second or two to load the information into the dock panel.\nIdeally, each candidate label should contain a descriptive element to spot the right one at a glance, like Wikipedia's qualifiers in brackets. eg, Amsterdam (City in The Netherlands), Amsterdam (Jacques Brel 'song)... To avoid pointless API calls, we could reserve the operation to candidates tied who have the maximum score and the closest spelling  to the original value. If this increases the reconciliation time too much, we can make the operation optional. At the start of the reconciliation, the user could choose between \"simple reconciliation\" or \"reconciliation with qualifiers\". \nBut I think aloud, I completely ignore if it's implementable.. Really useful, thanks @wetneb ! The windows display need to be improved a bit, but the speed seems very good !\n\n\nbut unfortunately it renders as blurry (at least for me)\n\nYes, it's weird. The text appears very clear, then becomes a little blurry.\n@fokky If you want to give a try, note that you have to wait a big minute with a spinning wheel before the new reconciliation service is available.. @wetneb Yes, the preview window sometimes pop up in the upper direction and no longer fit on the screen.\nI've not tried the first version, of course, but from what I see, it could perfectly suit too. After all, we just need some clues to disambiguate, not a complete description of the item.. > I'm not sure what we can do about this\u2026 I guess it will be solved once we have a dedicated reconciliation UI that does not rely on pop-ups.\nA quick fix might be to tweak data-table-view.less, but it's probably not a very good practice...\n```\n.data-table-cell-editor {\n  border: 1px solid @chrome_primary;\n  background: @chrome_secondary;\n  padding: @padding_tight;\n  .rounded_corners();\n  }\n/separate this class from .data-table-cell-editor /\n.data-table-topic-popup {\n  border: 1px solid @chrome_primary;\n  background: @chrome_secondary;\n  padding: @padding_tight;\n  .rounded_corners();\n  top: 10%!important; / element added /\n  }\n```. Is this related ? https://www.quora.com/Why-is-it-OK-to-expose-the-OAuth-client-ID-and-use-it-as-an-API-key. Thanks for the tutorial, @wetneb ! I was a little worried about testing the extension with fake data, which I could inject by mistake into Wikidata. But now I've a real dataset containing useful information about Belgian municipalities, I will use it for the test.. @wetneb  Sorry for being late. First, congratulations for this tool! \nNote: I used the extension whitout first reading the tutorials (as would any lazy user...) and I'm not a big editor of Wikidata.\nFor the moment, everything seems very clear and my only remarks concern details :\n\n\nIt would be nice if the first drag and drop automatically triggers an \"add item\" (or if an item is present by default). \n\n\nA \"save and close\" button in the \"Align with Wikidata\" window would be useful.\n\n\nAnd why not a button \"quick edit Wikidata\", which would apply immediately the schema ?\n\n\nThe operation \"perform edits on Wikidata\" seems a bit slow (30 edits/min), but I guess it does not depend on your extension.\n\n\nPerhaps it would be usefull to store somewhere the history of the editions made, with a link to their \"Edit group\" (https://tools.wmflabs.org/editgroups/b/OR/d9f4bef/).\n\n\nI'm going to continue my tests, and I feel that I will finally be able to thank Wikidata for all the bandwidth I use with my huge reconciliations (I went from noob to great contributor in a morning ^^)\n\n. Thanks also for your feedback on my poor edits (note : I didn't receive any notification about it). This is precisely what I meant by \"storing somewhere a link to the \"Edit groups\" page : we should have in Open Refine a link to cancel the edits.\nI will start again, but reading the tutorials this time.. > I am actually thinking about making bigger changes: instead of having a small dialog (where it feels a bit too narrow for all this) I was thinking about making it a new tab in the main UI, so that users could switch from the main view (where they see the cells).\nIn this case, if as Thad says you can use more modern libraries, an interactive tutorial might be a good idea.. Could it be related to #117 ?. @adambako I think the folder should be in Users > YourUserName > Library > Application support > OpenRefine\n\ni just need help but i cant find the place where to ask..\n\nThe best place to ask such questions is probably the Google group, since it's not a issue. Hope this helps.. Hi @joanneong , Yes, I wrote:\n\n\"Maybe I did not understand the changes in progress. I thought it was adding a facet by null and by empty string, but keeping facet by blank (null and empty). \nThis function is probably the most used. Users need to know which lines contain a blank, either the null value or the empty string. This is why I regret that it is so difficult to find in the tree drop-down menu.\"\n\nBut seeing your code and Thad's comment here (https://github.com/OpenRefine/OpenRefine/issues/820#issuecomment-325765811), I thought I must have misunderstood, so I deleted my comment.\nFrom my point of view, it's the null-empty string distinction that brings confusion. That's why software like Rstudio offers the possibility to treat nulls and \"\" as NA when importing. But we must not forget that in Open Refine, the blanks have the particular use of allowing to create records. Finding empty cells quickly is therefore even more important than in R. Without Facet by blank, we need two facets to identify empty cells!. > Maybe one solution would be to put the new facets in the main Facet menu, and keep the \"Facet by blank\" burried where it currently is? \n@wetneb I thought precisely the opposite: put facet by blank in the foreground, since it is the most generic function, and hide the other two (since being able to distinguish nulls vs empty string is something more specialized).\nI mean, the Open Refine file is often exported to CSV, where the null-empty string distinction does not make sense. . @joanneong @ostephens If everyone agrees on this version, can I just suggest leaving \"facet by blank\" at the bottom, so seasoned users aren't disoriented ?\n\n. Problem a little different, but related: the creation of records is not well documented on the Wiki. One might think that any empty cell below a non-empty cell in the first column belongs to the same record, whereas it works only for nullcells.\nIt's confusing, even for seasoned users. Will the current work on displaying invisible characters allow to distinguish visually nulls and empty strings?\n\n. Yep, with just 13.2 % of translated terms, it is hard to say that there is a German version...\nhttps://hosted.weblate.org/projects/openrefine/translations/\nMaybe we should not mention a language in the drop-down menu below a certain threshold, for example 80% of translations?. @ostephens This looks like a small improvement, but it helps to understand why your example certainly produces three records rather than one. In addition, users will better understand the utility of facet by null and facet by empty strings (which may seem very abstract).. The more I think about it and the more I think that the simplest solution would be to not authorize empty strings, to automatically turn it into null... What would be the drawbacks ?. @ostephens I also feel that there is way to make big mistakes by merging the two, but the nuance seems very subtle. Based on your PR, if I manually edit a cell, even if I do nothing, the null becomes an empty string. It's hard enough to justify intellectually.\n\n. I'm reading Thad's explanation, which I had not seen last month.. Ok, I understand better the need to keep a distinction between null and empty string. As a user, I have no problem with \"do more work to take care of NULL\" myself. But perhaps we can leave this choice to the user by adding an option \"treat null values as empty strings/null\" when importing a file (basically, it would be a \"transform all\" -> coalesce(value, \"\")). Similarly, it should be possible to choose how to treat nulls in a Custom tabular export and before exporting in data package. I'm afraid that the final operation \"transform all -> coalesce(value, \"INVALID\"), for example, is not easy to understand for everyone (coalesce is part of standard SQL, but it's not in SQL 101 courses). The result looks very good! Perhaps we should now use a slightly bolder font for non-null values, but it's a detail.\n\n. Rather than an option to disable the display of null, why not an option to use empty strings instead of null (basically, a shortcut to Transform all -> if(isNull(value), \"\", value) ? I feel that it is good practice that the users always know when they're working with columns containing null -  that cannot be concatenated directly to one another, unlike columns containing empty strings.\nI am not a big fan of the empty strings vs null distinction, but since it exists, and now has its own facets on the menu, so let's display it in the interface.. > I'd be careful with such a transformation - it would break the records mode pretty badly, no?\n@wetneb That's what I thought too, but after a few tests, it seems that records can be created with any blanks, nulls or empty strings. \n\nIn fact, it's not clear to me how exactly records are created and how they can be broken. I mean, I know the big picture of course, but some details are confusing. And there is no documentation on the Wiki. The only mention of the row-records difference I found is on a Martin's blog post (@magdmartin : do you think it could be a good idea to copy-paste it on the Wiki?). We will probably have to dive into the source code to flesh out the doc.\nI don't know enough Java to understand the details, but I guess that the notion of \"blank cells\" is determined by !ExpressionUtils.isNonBlankData(). This method seems quite clear: an empty string (((String) o).length() == 0) fulfill the conditions. \nstatic public boolean isNonBlankData(Object o) {\n        return\n            o != null &&\n            !(o instanceof EvalError) &&\n            (!(o instanceof String) || ((String) o).length() > 0);\n    }\nWhat did I miss?\nEDITED. I cannot reproduce the problem with 2.8 - Windows 10 - Chrome. \n\nNo problem either using the pr/1544 you were testing when you discovered the issue.\n\nBased on your description, it looks like a mix between rows mode and records mode.\n\n. @wetneb The Sparql query I'm talking about seems fast. I'd rather wait five seconds for relevant proposals than get bad ones in two seconds.. You're putting your finger on the difficult part. That's why I thought we could use the QID of the proposals made by the standard reconciliation service. For example, the QID of the first two props (or all five, that would work just as well) in my screenshot. But of course, it's always easy to make suggestions when you do not code them yourself.. > The only issue is that this would be specific to Wikidata\nThis is obviously a problem. I personally think that Wikidata (Open Refine's standard reconciliation service) might deserve special treatment.\nThat said, the Least common subsumer method can be transposed to any RDF graph with a basic ontology (ie, a system of entities and classes linked by rdf:type or instanceOf relationships).. If someone wants to test the idea on a list of QID, here is a little Python3 script (remember that i'm not a developer...)\n```\n-- coding: utf-8 --\nimport requests #need to be installed : pip install requests\nimport json\nfrom bs4 import BeautifulSoup #need to be installed : pip install bs4\ndef getLCS(list_of_qids):\n    \"\"\"\n    Get the Least common subsumers between several Wikidata qids in a list\n    Return a Python list with their english labels\n    \"\"\"\n    array_string = \", \".join([\"wd:\" + x for x in list_of_qids])\nquery = {\"query\": \"\"\"\nSELECT ?lcs ?lcsLabel WHERE {\n?lcs ^wdt:P279* %s .\nFILTER not exists {\n?sublcs ^wdt:P279* %s ;\n      wdt:P279 ?lcs .\n  }\nSERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en \" . } }\"\"\" % (array_string, array_string)\n         }\nurl = \"https://query.wikidata.org/sparql\"\nr = requests.get(url, params=query)\nsoup = BeautifulSoup(r.text, \"lxml\")\n\nreturn [x.text for x in soup.find_all(\"literal\")]\n\nif name == 'main':\nlist_of_qids = ['Q2785216', 'Q493522' ]\n\nprint(getLCS(list_of_qids))\n\n```\n. Oh gosh, never paid attention to this very useful function... The reconciliation menu is too crowded with obsolete features related to Freebase. Since all this stuff is not documented, when something seems not work, we tend to give up too quickly. \nI am just discovering the \"best candidate's types\" facet that looks super-interesting. It allows to classify candidates by their classes (the QID of the classes in the case of Wikidata) !\n. I'll try to work with Firefox today, but my version is 59.0.2.. @jackyq2015 No problem yesterday working with this version of Firefox and OR 2.8 on Windows 10, but I have allocated a lot of memory to Refine and my datastets are relatively small.. Hi @quan17 . I understand what you mean, but it would be very dangerous to change the partition behavior (if only for reasons of backward compatibility). For cases like yours - still very specific - it might be better to make a GREL formula that triggers an error if the pattern is missing from the string. For example :\nif(length(value.find(/(?<=e)/)[0]) + 1 > 0, value.partition(/(?<=e)/), null)\nYou can then play with the options to decide what to do in case of an error.\n\nNote: The find function I used only exists in the development version, it will be officially implemented in Open Refine 2.9.\nFor smartSplit(), it's not clear to me that this function accepts regular expressions. It accepts as a second argument expressions between //, ok, but I feel that it always transforms them into a string (its code is here). On the other hand, value.split(/(?<=e)/) will give the result you expect.. Note that in your example, the only case that could triggers an error is the absence of the letter \"e\" in the string. So you could just as well use contains():\nif(value.contains(\"e\"), partition(value,/(?<=e)/), null). Some observations:\n\n\nDROP TABLE triggers an error if the table does not exist. Ideally, DROP TABLE IF EXISTS should be used, but I'm not sure it's standard SQL.\n\n\nThe SQL script exported produces an error in the database (\"all VALUES must have the same number of terms\") if the OR project contains null (try with csv attached). Possible solution: transform all null into empty strings just before export.\n\n\nThe system does not handle records (one to many relationship). In my example, it would ideally be necessary for the column \"instanceof\", which is multivalued, to become a separate table linked to the main one by a foreign key. If it's too complicated, then the ideal would be to perform a \"fill down ALL\" just before the export, in order to get a correct denormalized table. But this function does not exist yet in Open Refine (the fill/blank down is also imperfect and to be used with caution).\n\n\nwikidata.zip\n. > I would rather not force opinion but think we should instead give the user an option\n@thadguidry I agree that the user should have the choice of how he treats nulls. But for the moment, he has absolutely no choice. If a cell is null, the SQL script will not work.\nIn other words, if you have this:\n\nThe export will produce this, which is incorrect.\nCREATE TABLE clipboard (\nCol1 VARCHAR(255),\nCol2 VARCHAR(255),\nCol3 VARCHAR(255)\n);\nINSERT INTO clipboard (Col1,Col2,Col3) VALUES \n( 'A','B','B' ),\n( 'C','D' ), <------------- incorrect\n( 'D','E','E' )\n\nGo ahead and open a new enhancement issue for your design thoughts\n\nI will try to draw that. The opinion of @gobertm could be interesting on this question.. Hi @gobertm , here is the complete Jacky's PR for the find function, this can perhaps be used as a model.. Looks like special characters such as . ^ $ are always matched, even in string mode. \n\nCould you try with : \n```\nvalue = \"rose is a rose is a rose\"\nvalue.contains(\"[a]\")\nvalue.contains(\"[a]\")\nvalue.contains(\"$\")\nvalue.contains(\".\")\nvalue.contains(\"^\")\nvalue.contains(\"\\s\")\nvalue.contains(\"\\w\")\nvalue.contains(\"[ei]\")\nvalue.contains(\"r..e\")\nvalue.contains(\"r.\\Se\")\n``\n. @gobertm Forfind()andcontains(), it would be problematic for both functions to return a bad string ortrue ` when looking for the $ currency symbol - if this symbol is actually not in the text at all.. > Those that accepts regex in \" \" are : find and match . Others are ok.\nGood news, match() can only be used with regexp anyway. Just leave it like that, it will avoid retrocompatibility problems. And find() is not yet released.\nList of functions that use regexp:\n```\nfind()\ncontains()\nmatch()\npartition()\nrpartition()\nsplit()\nreplace()\n```\n. I think it's okay now.\n```\nvalue = \"rose is a rose\"\nvalue.contains(\"\\s+\")--> false\nvalue.contains(\"\\s+\") -->  false\nvalue.contains(/\\s+/)--> true\nvalue.contains(\"$\")  --> false\nvalue.contains(/$/)  --> true\nvalue.contains(\"rose\") --> true\nvalue.contains(\"r.se\") --> false\n```. @thadguidry I was reading the code of the RDF importer and, if I understand correctly, it should be able to parse N-triple, N3 and RDF/XML. Weird.. Nothing special.\n16:40:47.238 [                   refine] POST /command/core/get-models (109ms)\n16:40:47.247 [                   refine] POST /command/core/get-rows (9ms)\n16:40:54.674 [                   refine] POST /command/core/importing-controller (7427ms)\n16:40:54.759 [                   refine] POST /command/core/get-models (85ms)\n16:40:54.765 [                   refine] POST /command/core/get-rows (6ms)\n16:40:56.930 [                   refine] POST /command/core/importing-controller (2165ms)\n16:40:57.031 [                   refine] POST /command/core/get-models (101ms)\n16:40:57.035 [                   refine] POST /command/core/get-rows (4ms)\n16:41:02.097 [                   refine] POST /command/core/importing-controller (5062ms)\n16:41:02.195 [                   refine] POST /command/core/get-models (98ms)\n16:41:02.200 [                   refine] POST /command/core/get-rows (5ms)\n16:44:53.100 [                   refine] POST /command/core/cancel-importing-job (230900ms) <--- I cancelled myself. it does not work either with LODrefine (based on OR 2.6) So I wonder if the n3 importator used to work in the past.\n\n. This ?\nindex-bundle.js:10335 JQMIGRATE: Logging is active\nindex-bundle.js:9594 [Deprecation] Synchronous XMLHttpRequest on the main thread is deprecated because of its detrimental effects to the end user's experience. For more help, check https://xhr.spec.whatwg.org/.\nsend @ index-bundle.js:9594\nchrome-extension://jnhgnonknehpejjnehehllkliplmbmhn/content_script.js:24 initializing Content Script message listener\n. I tried with Firefox (disabling all extensions) and the error messages look more explicit.\nThe page was reloaded, because the character encoding declaration of the HTML document was not found when prescanning the first 1024 bytes of the file. The encoding declaration needs to be moved to be within the first 1024 bytes of the file.  localhost:3333:36\nJQMIGRATE: Logging is active  index-bundle.js:10335\nWebconsole context has changed\nSynchronous XMLHttpRequest on the main thread is deprecated because of its detrimental effects to the end user\u2019s experience. For more help http://xhr.spec.whatwg.org/  index-bundle.js:9594:5\ngetRecipes: falling back to a synchronous message for: \"http://localhost:3333\"  LoginRecipes.jsm:244\nthis._recipeManager is null  LoginManagerParent.jsm:86\nThe character encoding of a framed document was not declared. The document may appear different if viewed without the document framing it.  importing-controller\nXML Parsing Error: not well-formed\nLocation: http://localhost:3333/command/core/get-importing-job-status?jobID=2\nLine Number 1, Column 1:  get-importing-job-status:1:1\nXML Parsing Error: not well-formed\nLocation: http://localhost:3333/command/core/importing-controller?controller=core%2Fdefault-importing-controller&jobID=2&subCommand=initialize-parser-ui&format=text%2Frdf%2Bn3\nLine Number 1, Column 1:  importing-controller:1:1\nXML Parsing Error: not well-formed\nLocation: http://localhost:3333/command/core/importing-controller?controller=core%2Fdefault-importing-controller&jobID=2&subCommand=update-format-and-options\nLine Number 1, Column 1:. I tried with Google Refine 2.5 and with OR 2.6 RC1 on the Data science workbench cloud : same result. Are we sure this importer has already worked once?. @jackyq2015 Could you post an example of n3 file that works ? When I try, i get this error.\n\nLooks like it doesn't like prefix. But a n3/ttl file without prefix is just ntriple, am i wrong ?\nFor ntriple : it works *, but it doesn't recognize the extension (the file is parsed as line based). You have to click yourself on \"RDF/n3\" (even if the file is not n3).\nFiles tested : \nhttps://www.wikidata.org/wiki/Special:EntityData/Q42.n3\nhttps://www.w3.org/2000/10/swap/test/meet/blue.n3\nhttps://www.wikidata.org/wiki/Special:EntityData/Q42.nt\nhttps://gist.github.com/kal/ee1260ceb462d8e0d5bb (turtle )\nhttp://www.agfa.com/w3c/rdf/rdfs-transitive-subSubProperty/test002.nt (ntriple)\nNote: Parsing a Ntriple file produces a new column by predicate. This is fine when there are few triples, but it becomes invasive when you import a whole Wikidata or DBpedia page. A better parsing would probably consist in creating only three columns: subjects, predicates, objects. You can then use a transpose only if you want.\n. Nice catch, @ostephens. In summary, the parser called \"RDF/N3\" cannot read n3, just Turtle, but is able to parse N-triple, which is not indicated. It should at least be renamed.. @Gautamshahi You're using a mix of GREL and Python on your screenshot. An IF does not look like that at all in GREL. Moreover, your dates will be difficult to parse. If you just want the month (optional) and the year, use this GREL formula instead.\nvalue.match(/.*?\\/+(.+)/)[0]\n\n. @Gautamshahi  toDate() does not work because your column contains multiple date patterns. OpenRefine cannot guess which one is right. Rather than trying to solve everything with a one liner, first create a new column with the formula above ( value.match(/.*?\\/+(.+)/)[0]), and then parse this new column into date. Please, refer to the documentation of this function. You can use facets to divide your dates into subgroups or try more than one date pattern. In your case, the GREL formula could be something like value.toDate('MM/yyyy', 'yyyy')\n\nNote: for questions that are not related to bugs or enhancement suggestions,  its better to use the Google group/mailing or StackOverflow (adding the tag openrefine).. I agree with Mathieu, it seems to me that it would be much cleaner if only the first column had an influence on the number of records.. In the following screenshot, I do not feel that there are four records. For me, there should be only two.\n\nAnd if we add a third column, the number of records changes. This is counter-intuitive in my opinion.\n\nProject:\nclipboard.openrefine.tar.gz\n. Everything seems ok for me. I'm testing with this list of formats.\n5/12\n10/27\n12/22/78\n1/17/2006\n1/17/6\n2008/6/30\n1978/12/22\n2008-6\n2008-06\n1978-12\n2008-6-30\n78-12-22\n8-6-21\n30-6-2008\n22.12.1978\n30.6.08\n22\\t12.78\n30-June 2008\n22DEC78\n14 III 1879\nJune 2008\nDEC1978\nMarch 1879\n2008 June\n1978-XII\n1879.MArCH\nJuly 1st\n2008\nApril 17\n1790\nMay.9\n78\nJuly 1st\nApr 17\nMay.9\n1 July\n17 Apr\n9.May\nMay-09-78\nApr-17-1790\n78-Dec-22\n1814-MAY-17\n1978\n2008\nMarch\njun\nDEC. The tutorial is probably a bit outdated. Simply change \u201c-Xmx1024M\u201d into something else, eg \u201c-Xmx2048M\u201d or \u201c-Xmx8G\u201d.. @yaeln This version of the RDF extension should work with OR 3.. @ostephens A \"Facet Type\" in the menu would easily solve this kind of case, wouldn't it?\n\n. We must find a solution at least for Booleans. The current behavior of OR 3.1 does not allow anymore to use text facet \"true vs false\", for example to select the first 100 rows of a dataset with row.index < 100. Big breaking change.\nOR 3.1\n\nOR 3 and previous\n\n. Would it be a good idea to restore the previous behavior of text facet (ie everything is stringified, as is in a CSV file) and to move the current behavior (OR 3.1) into a new kind of facet, called for example \"type facet\"?. @thadguidry @wetneb I will not meddle with the question of whether it is a bug or not. Let's say I understand both points of view. \nMy 2cts to feed the reflexion: Martin's latest survey showed that the majority of users came from the worlds of library and journalism, not computer science. I've been both in my career, and I'm not sure I'll be able to explain, if anyone wonders in the Google group, why he/she manages to sort some columns and not others simply because some cells that he/she thinks are empty are not empty, but contains an empty string (and why any null cell on which you click on \"edit\" automatically becomes an empty string).. @thadguidry : error 403 (with Firefox too)\nHTTP error 403 : Forbidden | <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error>\n  <Code>AccessDenied</Code>\n  <Message>Access denied</Message>\n</Error>\nCould it be related to some recent changes in the user agent used by OR ? The only difference I see between the OR 2.8 and OR 3 requests is the new HTTP header : \nOR 2.8\nhttp://127.0.0.1:3333/command/core/add-column-by-fetching-urls?baseColumnName=x&urlExpression=grel%3Avalue&newColumnName=-%27-%27un&columnInsertIndex=2&delay=2000&onError=set-to-blank&cacheResponses=true&project=1616927736031\nOR 3-RC1 \nhttp://127.0.0.1:3333/command/core/add-column-by-fetching-urls?baseColumnName=x&urlExpression=grel:value&newColumnName=c'rc'&columnInsertIndex=2&delay=2000&onError=set-to-blank&cacheResponses=true&httpHeaders=[{\"name\":\"authorization\",\"value\":\"\"},{\"name\":\"user-agent\",\"value\":\"OpenRefine+3.0-beta+[TRUNK]\"},{\"name\":\"accept\",\"value\":\"*/*\"}]&project=2037720374740\n. Any idea what the problem is, everyone?. I remember long threads around null and empty strings (like #820, #1635), but not that \"string\" + null = \"string\" got consensus. \nOn the other hand, as discussed in #1439, the simple concatenation of columns is one of the hottest topics on the Google group. Perhaps it would be worthwhile to create a specific menu for this operation, as it was just done for value.replace() (another heavily used function). In this menu, the user would have the choice between treating null as empty strings or as an absence of value that prevents any concatenation. Would not it be the most user-friendly solution?\nEdit: already proposed in #1473, labeled with milestone 3.5. . Yes, I confirm that Tue Dec 11 00:00:00 GMT 2018 can be parsed with value.toDate(\"EEE MMM dd h:m:s z yyyy\"), at least in OR 3.1.. @ostephens Looks like the problem with the Excel file comes from the blank spaces between each part of the date-time string, since you can parse it using this Grel formula : \nvalue.replace(\"\\p{Blank}\", \" \").toDate(\"EEE MMM dd h:m:s z yyyy\")\nEdit\nThis one works too : \nvalue.toString().toDate(\"EEE MMM dd h:m:s z yyyy\")\nWhen directly imported from Excel, the date \"string\" is not really a string, but a java.util.Date object. The difference is indistinguishable to the naked eye.\n\n. It seems that the feature \"Match each cell with its best candidate\" just takes the first one, not the one with the highest score. It is also something that should be improved.. I am actually using a reconciliation service in OR 3.1 and, under the hood, I see these messages. Could it be related to this bug?\n18:06:34.541 [           ProjectManager] Saving some modified projects ... (300022ms)\njava.lang.NullPointerException\n        at com.google.refine.operations.recon.ReconOperation.getBriefDescription(ReconOperation.java:107)\n        at com.google.refine.operations.recon.ReconOperation.write(ReconOperation.java:116)\n        at com.google.refine.history.HistoryEntry.write(HistoryEntry.java:115)\n        at com.google.refine.io.FileHistoryEntryManager.save(FileHistoryEntryManager.java:69)\n        at com.google.refine.history.HistoryEntry.save(HistoryEntry.java:121)\n        at com.google.refine.history.History.save(History.java:297)\n        at com.google.refine.model.Project.saveToWriter(Project.java:156)\n        at com.google.refine.model.Project.saveToOutputStream(Project.java:138)\n        at com.google.refine.io.ProjectUtilities.saveToFile(ProjectUtilities.java:103)\n        at com.google.refine.io.ProjectUtilities.save(ProjectUtilities.java:66)\n        at com.google.refine.io.FileProjectManager.saveProject(FileProjectManager.java:254)\n        at com.google.refine.ProjectManager.saveProjects(ProjectManager.java:316)\n        at com.google.refine.ProjectManager.save(ProjectManager.java:231)\n        at com.google.refine.RefineServlet$AutoSaveTimerTask.run(RefineServlet.java:92)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n        at java.util.concurrent.FutureTask.runAndReset(Unknown Source)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(Unknown Source)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n        at java.lang.Thread.run(Unknown Source). My own reconciliation service with AAT Thesaurus, but copied without shame from LC Reconcile, which served as a basis for quite a few other services in Python.. Since at least one other OpenRefine user has been faced with the same problem (Micon, in the Google Group), could the hack to retrieve the data be documented somewhere publicly?. @MiconSchorsij The fastest for you would probably be to modify your reconciliation service to add the missing items. Which service do you use?. Yes, right, you already mentioned it. This fork of the service contains default values for identifierSpace and schemaSpace, and should therefore avoid the bug in question.. If you wish, I can also take a look. Just send me the project to ettorerizza at gmail.com.. > Apparently a broken project stays broken.\n@MiconSchorsij Sorry for the late response. Indeed, it seems that if the file data.txt in data.zip always contains a reconciliation performed with a service that was incorrectly set, the error will continue. It even looks like the projects are corrupted to the point that we cannot extract the history by clicking on \"undo/redo -> Extract\". It seems possible to repair a project. This requires to :\n\nfind it in your workspace directory\ndelete the file data.temp.zip; \nopen the data.zip archive and, in the data.txt file, use a text editor to replace (CTRL + h) each mention of \"identifierSpace\":null,\"schemaSpace\":null by identifierSpace\":\"something\", \"schemaSpace\":\"something\".\n\nI can do that for you if you want.\nThis won't restore lost operations. In your case, you will recover only 59 changes, but your project will now be healthy. To recover the missing operations, it would be necessary to parse each history file. But as already said @wetneb, it is not certain that it goes faster than to start again thirty manipulations.. The Chinese version of Wikipedia does not try to translate it: https://zh.wikipedia.org/wiki/Null_(%E7%B7%A8%E7%A8%8B). It's interesting, I did not know that so many technical words were not translated at all.. On Wikipedia, it looks like null is never translated when it means null (and not \"nothing\"). It's a kind of constant, a bit like Pi. This is probably the best option since the word null is something that, in Open Refine, appears on the screen in the same way in all languages. It's not just an expression in natural language like \"Remove this column\".\n\n. Do a better distinction between String and Pattern, as in the replace() method ?. ",
    "ostephens": "See also #569 - similar issue (user is returned to start (row 1) of project), but in relation to a different action (\"Apply to all identical cells\" on manual cell edit). See also the 'Named Entity Recognition' extension from 'Free Your Metadata' https://github.com/RubenVerborgh/Refine-NER-Extension. Just moved this into the 3.1 milestone - @thadguidry @wetneb is this correct? (i.e. is this where we are targetting this change?). @thadguidry 'contains' now supports regular regex which I think covers all use cases (as you can use ^ and $ in that if you want to test for starts/ends with)\nI'm not sure adding regex support to startsWith or endsWith as well is useful or really makes sense. What do you think?. Same behaviour applies if edited cell is Null\n. Just an update - I'm working on this and work in progress is at https://github.com/ostephens/OpenRefine/tree/mass-edit-fix\ntl/dr fix #1631 is relatively straightforward, but I'm trying to make tests and fix #1632, #180 and #332 at the same time which is proving more complicated and taking more time. Fixed by #1642. This can be done by using the construction:\n{{forEach(row.columnNames,cn,\"|\"+cells[cn].value)}}\nIs this sufficient to close this issue?. I've submitted a PR #1434 which allows the user to set three http request headers:\n\nUser-Agent\nAccept\nAuthorization\n\nAdding additional headers can be done easily either in an extension or in the core code.\nThe headers that can be set on the request are set in the back-end, so adding a proprietary header (which might hold an API key for example) would need to be added through an extension (the user can't just add them in on submitting the request). Fixed by #1434. Think this is a duplicate of #68 ?. Is this delivered (at least partially?) by https://github.com/OpenRefine/OpenRefine/commit/20acc17c05c3d76a643908fe8918d788abc2cf6f. This is still a problem in 3.0 Beta\nEditing a cell which contains either a null or a zero length string, and then clicking \"Apply to all identical cells\" leads to NO edits being made (including the cell you started from).. Just an update - I'm working on this and work in progress is at https://github.com/ostephens/OpenRefine/tree/mass-edit-fix\ntl/dr fix #1631 is relatively straightforward, but I'm trying to make tests and fix #1632, #180 and #332 at the same time which is proving more complicated and taking more time. Fixed by #1642. For certificate/SSL issues on Mac see #1265 - it works through the different levels of problem and solution\nIf you are using OpenRefine 2.7 it's possible on Windows you are seeing #1219 - which is fixed in 2.8 and can be avoided in 2.7 by unchecking the 'cache' option when fetching URLs. (I'd recommend updating to 2.8 generally)\nAs @wetneb says - if you can provide further information about any errors displaying in the console that would help with analysis.. I've just released an extension[1] that allows the addition of blank rows into an existing project. This is a step up from the work around described by @tfmorris above, and may meet the use case described in the original issue. However it doesn't go as far as #715 \n\nhttp://github.com/ostephens/refine-gokbutils. Just to check - although you aren't changing the facets while you reconcile, you may want to only deal with a filtered set of rows as you go into the reconcile process - would what you are suggesting only deal with the filtered rows (I guess yes, but just to be clear). So I'm currently looking at toDate as per #1759 and @thadguidry reminded me of this (thanks Thad!).\n\nAt the moment I can think of two options for handling this:\nExtend toDate to have a GREL specific 'format' option which is (something like) 'epochms' (epoch milliseconds) which tells toDate to use a long number (or a string which can be converted to a long number) as millisecond Unix time and use that to generate a date/time\nThe alternative I can see is to have a special new function such as 'epochToDate' which does this specifically.\nThe second approach avoids making the toDate function even more complex than it already is, but has the downside of introducing a new function rather than a single 'toDate' function handling all conversions to a date.\n@thadguidry @magdmartin  do you have thoughts on this?. Thanks @thadguidry. I can see a new format option working \n\nOur toDate() already has an optional format argument. Where I could see this leading to is possibly extending to handle a 3rd argument for timezone or utc.\n\nOne of the complexities of the toDate function in code is that it has a set of optional arguments- including one boolean type (month first) and two string types (locale and repeatable format). \nAdding another optional argument (presumably of type string) for timezone will make it even more challenging to make sure the arguments are being interpreted correctly . I think the new find function solves this problem. E.g. you can extract the two names using:\nvalue.find(/[A-Z][a-z]+\\s[A-Z][a-z]+[\u00e1\u00e3\u00e2\u00e9\u00ea\u00ed\u00f3\u00f4\u00f5\u00fa][a-z]+/)\n@thadguidry @ettorerizza do you agree?. @biiip short answer is No. I have released an extension which allows you to add new blank rows to an existing project, but there is no way to import a file into an existing OpenRefine project.. Agreed that the ability to apply operations to multiple columns would mitigate this.\n. I'm trying to create individual issues for each of the enhancements identified under this issue. I've done all except one I believe. I wanted to have a further discussion before I went ahead and created the last enhancement to be clear on what we wanted.\n@thadguidry has suggested the implementation of the 'safe navigation' and 'elvis' operators. This is in place (I believe) of the 'coalesce' operation suggested by @eximius313 and @wetneb \nI'm unclear as to what the expectation of the 'safe navigation' operation does for us here. As I understand it, the usual purpose of a safe navigation operator is to avoid trying to call an operation on a null value - and instead returns null at that point.\nIn OpenRefine this is less of an issue because whether there is null or an error here, the user can decide through the transform process whether they keep the error or set to blank (actually null) in the transform window.\nThe Elvis operator makes more sense to me in place of coalesce, as it seems to provide the required functionality.\nCan someone explain a bit more about how we'd expect the safe navigation operator to work in OpenRefine - I think I'm missing something. Also would appreciate views on whether this is two separate enhancements, or a single one covering both safe nav and Elvis.\n. @eximius313 I've added a new issues #1491 to deal with your Conclusion (1) in the original report . I've added a new issue for the Coalesce function enhancement.\nI'm not completely convinced by the need for replace(null,null,\"\") although I'm happy to write an issue for it if it is needed. At the moment the reason this fails is because the replace function accepts only a string as the first argument and a string or a regular expression as the second argument.\nI can't see a problem extending it to work if both the first and second argument are nulls, but it feels it adds complexity to the replace statement which is not needed.\n@thadguidry - are you keen we do this?. I'm in the midst of implementing the new coalesce function but have hit an issue. I have a working function as defined in #1496 but I think there is a problem with this fulfilling the original need by @eximius313 (and others).\nIf we have:\nrowId | Col1 | Col2 |\n----- | ----- | ------\n1        |null | string\n2       | string | string\nIn Col1 we can run the function value.coalesce(\"\") we get the outcomes:\n\nRow 1: null ->\"\"\nRow 2: string->string\n\nas expected.\nHowever if we try the function cells[\"Col1\"].value.coalesce(\"\") + cells[\"Col2\"].value we get:\n\nRow 1: \"Error: Object does not have any field, including value\"\nRow 2: \"stringstring\"\n\nThis is because cells[\"Col1\"].value is not null. Instead cells[\"Col1\"] is null, making cells[\"Col1\"].value an error.\nThis means that it does not solve the problem \n\n\"My problem was, that I cannot concatenate column values when one cell was null.\"\n\nSo - I'm looking for suggestions as to how we might deliver the desired functionality - I don't have any immediate ideas. @thadguidry @wetneb @ettorerizza ?\n. I slept on it, and have a proposal for a possible way forward - I'd be interested in feedback. \nProposal: Extend the toString function with two new arguments as follows:\n\nBoolean: Treat null as empty string (Default false)\nBoolean: Treat error as empty string (Default false)\n\ntoString(null) -> \"null\" (current behaviour)\ntoString(null,true) -> \"\" (treats null as an empty string)\ntoString(<error>) -> \"error\" (current behaviour)\ntoString(<error>,<true or false>,true) -> \"\"\nUsing my example from above:\nrowId | Col1 | Col2 |\n----- | ----- | ------\n1        |null | string\n2       | string | string\nIn Col1 we can run the function value.toString(true) we get the outcomes:\n\nRow 1: null ->\"\"\nRow 2: string->string\n\nIf we try the function cells[\"Col1\"].value.toString(true,true) + cells[\"Col2\"].value we get:\n\nRow 1: \"string\"\nRow 2: \"stringstring\"\n\nThe only thing I don't like about this approach is that it still feels like it is making the user jump through some hoops and it feels a bit verbose still, but it is definitely better than the current situation where you have to use something like:\nif(isNonBlank(cells[\"Col1\"]),cells[\"Col1\"].value,\"\") + ... . Thanks @thadguidry. \nI'm afraid I don't quite understand your suggestion of using get/setNullText - it feels tangential to the problem here.\nThe problem is that for a null cell, the expression cells[\"Col1\"].value doesn't return null, it returns an error. This means that cells[\"Col1\"].value.toString().replace(\"null\",\"\") would also fail in this case.\nThat's why I'm suggesting that toString would be extended to treat errors as empty strings - because the problem of is dealing with an error output, not dealing with a null output.\n. That's it - but it's actually the bit cells[\"Column 1\"].value that's the problem. If you strip out the 'toString' part of your expression you'll see that this gives an error Error: Cannot retrieve field from null - because the cell is null, rather than the value in the cell. \nI'm not completely sure how this works tbh - but I'm reluctant to change the innards of how we handle null cells - it feels like it could have unexpected side-effects. Hence looking for other approaches. I'll have a look - thanks @thadguidry . With #1598 all the work spawned by this issue has been completed. Thanks to @eximius313 and @thadguidry for their patience and explanations and @joanneong for delivering some of the new functions.\nFeeling really pleased with the work we've done here - hope @eximius313 and others find it helpful. We will be releasing a beta version of 3.0 very soon (in the next few days I hope but it is all down to someone having the time to build the release). This was eventually fixed by #1500. I think this is the expected behaviour. A 'Numeric Facet' can only represent numeric outcomes - but by changing the facet expression to value>2 you have a boolean outcome. For this to work, you need to use the 'Custom Text Facet' (or do a text facet and click 'change' and enter the expression). @answerquest  If you use the straight \"comma-separated value\" export option, it should default to using UTF-8 encoding - this is set in https://github.com/OpenRefine/OpenRefine/blob/5a0304f3636386af506c47f3c73a38129e478a40/main/src/com/google/refine/commands/project/ExportRowsCommand.java#L104\nIf the data in your project isn't utf-8 encoded you need to use the 'custom tabular exporter' - in that you can set the encoding you want to use for the export\nObviously if your text is utf-8 encoded but it isn't exporting correctly with the default csv export we need to investigate further - it would be great to get a sample project we can look at to investigate/test with in that case. I'm happy to have a look, but probably not this week now :)\nI see this working the same as 'invert' on the facets - you retrieve the inverse set of rows/records to those that would be retrieved usually - i.e. all those that don't match the text in the filter.\nWhat about implementing as an 'Invert' link to keep consistency with the Facet panel UI and terminology? And probably a 'reset' link as well to clear all options - again bringing in line with the Facet panel\n\n. Tested on OpenRefine 2.6 beta, and OpenRefine 2.6 RC1\n. This issue has now come up under #820 (and related issues) as it causes problems when trying ignore null values when concatenating values from across columns.\nI've been investigating and just trying to document before I lose the thread although probably not all relevant to the issue in hand...\nIf we have a null value/cell cells[\"col name\"].value fails with an error. The error comes from:\nhttps://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/grel/ast/FieldAccessorExpr.java#L66\nso is part of the general field accessor code rather than something specific to the cell model. \nThe code here also introduces some inconsistencies in what the potential outcomes when trying to access a field\npublic Object evaluate(Properties bindings) {\n        Object o = _inner.evaluate(bindings);\n        if (ExpressionUtils.isError(o)) {\n            return o; // bubble the error up\n        } else if (o == null) {\n            return new EvalError(\"Cannot retrieve field from null\");\n        } else if (o instanceof HasFields) {\n            return ((HasFields) o).getField(_fieldName, bindings);\n        } else if (o instanceof JSONObject) {\n            try {\n                return ((JSONObject) o).get(_fieldName);\n            } catch (JSONException e) {\n                return new EvalError(\"Object does not have any field, including \" + _fieldName);\n            }\n        } else {\n            return new EvalError(\"Object does not have any field, including \" + _fieldName);\n        }\n    }\nWhere there is an attempt to access a field of an object which is an instance of 'HasFields' (i.e. Cell, Row, etc.), then this passes off to the getField method for the object, and these generally return null when there is an attempt to access a non-existant field. e.g. GREL:\ncell.non_existant_field -> null\nrow.non_existant_field -> null\nHowever if the object is a JSONObject, an attempt to access a non-existant field the is caught as an exception and a (slightly inaccurate) error message is returned rather than a null. e.g. GREL:\n'{\"field\":\"val\"}'.parseJson().non_existant_field -> <Error: Object does not have any field, including non_existant_field>\nIf the object has no fields then an error is also returned. e.g. GREL:\n\"text\".non_existant_field -> <Error: Object does not have any field, including non_existant_field>\nThese latter two behaviours seem inconsistent with the behaviour of HasFields objects.\n. One option here would be to always return null when you attempt to get an non-existant field, including situations where there is an attempt to get a field from null. This would introduce consistency, and resolve the issue reported here and in #820.\nWhat would the negative impact of always returning null in these cases?. @jackyq2015 @tfmorris @wetneb @thadguidry any view on the idea of standardising that we always return null (rather than either null or error) when trying to access a non-existant field (even from null)?. Will do. Closed by #1598 . There is more extensive discussion of this concept in a thread on the Google Group https://groups.google.com/d/msg/openrefine/aDxquAI4MqY/P-KD7mghFAAJ. In that discussion it was proposed that rather than giving an error, then the array could be stored as a string which is a JSON array, that could then be parsed in future transforms with parseJson().\n@thadguidry did a proof of concept implementation at the time saying\n\nFor this, I made changes only to ExpressionUtils.java\n```\nstatic public boolean isStorable(Object v) {\n    return v == null ||\n        v instanceof Number ||\n        v instanceof String ||\n        v instanceof Boolean ||\n        v instanceof Date ||\n        v instanceof Calendar ||\n        v instanceof EvalError ||\n        v instanceof Arrays ||      // NEW\n        v instanceof Collection ||      // NEW\n        v instanceof List;      // NEW\n}\n\nstatic public Serializable wrapStorable(Object v) {\n    if (v instanceof JSONArray) {\n        return ((JSONArray) v).toString();\n    } else if (v instanceof JSONObject) {\n        return ((JSONObject) v).toString();\n\n// NEW\n        } else if (v instanceof List) {\n            return ((List<?>) v).toString();\n        } else if (v.getClass().isArray()) {\n            return (Arrays.deepToString((Object[]) v));  // This is kinda cool and uses http://docs.oracle.com/javase/7/docs/api/java/util/Arrays.html#deepToString(java.lang.Object[])\n        } else if (v instanceof Collection<?>) {\n            return ((Collection<?>) v).toArray().toString();\n// NEW\n    } else {\n        return isStorable(v) ?\n            (Serializable) v :\n            new EvalError(v.getClass().getSimpleName() + \" value not storable\");\n    }\n}\n\n```. @thadguidry I'm inclined to change this issue to implement the solution you prototyped given here, and leave the 'warning' to #443 as a display hint rather than an explicit warning.\n\nWhat do you think? That would also make this another 'easy' issue for implementation I think as it really just needs checking over and tests written/modified appropriately. It shouldn't break any existing functionality or backwards compatibility, but it will prevent the 'I did this but got nothing in my cell' issues being reported in the group - it will store exactly what you see on the screen in the expression preview. I'll have a look this weekend and let you know. Initial feeling is that this is doable in the next 2 weeks (that is to implement the solution you already wrote some code for, NOT implement the warning/display hint as in #443). Hi @thadguidry - yes I've been having a look at the possibility of populating the cell with a string representation of an array. I had a play around with the code suggested (see above), but have a problem with it in that this produces a different outcome to the one you see in the 'preview' window - which seems wrong to me - I think if we are going to store anything we should store what is shown in the preview.\nThe preview is generated by the writeValue method in PreviewExpressionCommand (https://github.com/OpenRefine/OpenRefine/blob/eeb8ade5c03e78b5ae7a83e315e39e36f93727b1/main/src/com/google/refine/commands/expr/PreviewExpressionCommand.java#L176). This could easily be moved into ExpressionUtils.java and then used to produce both the preview and the stored value.\nI've been experimenting with some small improvements to the writeValue method at the same time, but if we were to just leave the writeValue code as it is then it is easy to implement.. In the interests of \"show, don't tell\" I've made a PR #1586 to show what I think is the most basic implementation. Looking for comments on this rather than approval at this stage. This is marked as closed but I can't see the original issue (sort order of operation history) is addressed? But also can't see that the order of the history panel has ever changed. Am I missing something?. So I've been having a look at this - it looks to me like the issue is with display rather than with the join. In the display, the spaces are collapsed (standard behaviour for html/web browsers), but if you actually access the value in the field, the spaces are present. See screenshot recreating scenario above with cell shown next to edit box for the cell - both contain the same information, but display differently.\n\n. Which leads me to ask - is there any reason we don't style cells with css white-space: pre-wrap ? This would be a simple and I think useful change. @thadguidry I'm ok with not changing this, but I don't understand what you mean that css pre-wrap changes the data value for the user?\nAnd looking forward to news!. @thadguidry just going back to the issue of using css pre-wrap - can you clarify why you think this changes the data value for the user? Surely this is just for display, and would make the display of the data more accurate (as at the moment whitespace is collapsed, which means we are displaying the data differently to how it really is). Making case sensitive and regular expression text clickable in the Text filter was fixed by af1cc19c6db0f55a7226bfaa527eb0cdb244cd76\nI'm against making the cells in the layout table clickable with the current editing mechanism - I often need to copy cell values, and I want to be able to do this without opening up an Edit pop-up.\nI'm not sure about the column headings. Being able to copy these is sometimes useful, but I don't do it so often as copying cell values.\nAny other views? @ettorerizza ?. @ettorerizza any views on making the column headings more generally clickable? I'm against for the reasons I state above, but if you disagree then it would be good to have a discussion. Otherwise I'll close this. Agreed\n\nMake case sensitive and regular expression text in Text filter DONE\nMake all headers of columns or all cell in layout table clickable WON'T DO\nThis would cause problems in the UI where people want to simply copy text rather than interact with controls.\nIf an alternative approach to making clicking on column/cell controls easier can be suggested it will definitely be considered - please do this in a new issue if you have any suggestions. Hard to analyse without the original file (no longer available on the dropbox link given). However, could potentially be an encoding issue - I can recreate the perceived issue if I try to import (e.g.) a UTF-16 encoded file with UTF-8\n\nScreenshots attached showing how works if encoding set to UTF-16 but not if UTF-8\n\n\nIf this was the issue in this case, then I think the system is behaving as expected - until encoding is correct the data will not import correctly\n. I'm not seeing this error on the current master - @thadguidry are you able to re-test with the same file as reported above?. I'm afraid no-one has tackled that documentation yet. However, the existing code libraries that interact with OpenRefine might be a good starting point for understanding how you can exploit the API. E.g:\nhttps://github.com/PaulMakepeace/refine-client-py/\nhttps://github.com/maxogden/refine-python\nhttps://github.com/maxogden/refine-ruby\nhttps://github.com/pm5/node-openrefine\nOr if there is something specific you are trying to do, you could ask on the discussion group at https://groups.google.com/forum/#!forum/openrefine. I've now updated the HTTP API documentation to indicate that the project ID is not directly returned, but available from the redirected URL. Also expanded / updated some other parts of the documentation. See https://github.com/OpenRefine/OpenRefine/wiki/OpenRefine-API. Looks like this issue was caused by this change: https://github.com/OpenRefine/OpenRefine/commit/aa65bc5c18ea7cd7582572b6d2fb1a7522016e4c\n(returns an error code in the JSON rather than just logging to console)\nI guess we need to handle an error being returned in the JSON here? But also might want to be a bit less aggressive updating the view when the user is entering a regular expression?\nNot quite sure on the best approach at the moment. @thadguidry @wetneb Any ideas?\n. Of course we could revert https://github.com/OpenRefine/OpenRefine/commit/aa65bc5c18ea7cd7582572b6d2fb1a7522016e4c until we've sorted out the error handling at the client end. Just some notes for reference as I've investigated:\n\nAs filter updates, calls Refine.update({ engineChanged: true })\nIn turn calls Refine.createUpdateFunction(options, function()\nIn turn calls ui.dataTableView.update(onDone);\nIn turn calls Refine.fetchRows(start, this._pageSize, function()\nRefine.fetchRows in project.js is the code that gets the error response from what I can see\nNo error handling here\nAlso Refine.fetchRows asks for JSONP response - I'm not clear why this is done as a JSONP call instead of plain JSON (uses JSONP here means you get a Fail on the xhr request. If you used JSON, you'd get a valid response that could be parsed for an error message). @wetneb the JSONP vs JSON thing causes an additional error at the client end - I think because it requests JSONP it is expecting a javascript function in response - but it seems the JsonException function in the java returns just plain JSON.. @thadguidry @jackyq2015 do either of you know why JSONP is used for Refine.fetchRows? I can't see any obvious reason to use JSONP as it would never be cross-site as far as I can see. But I'm worried I'm missing something.. Note that I said I thought this was a bug - I'm not 100% sure, but it behaves in a manner different to what I would expect. However, I can't find any documentation that says how it is meant to work, so all I can say is that it doesn't work in the desired manner. I don't know if this is the intended manner or not, although it seems unlikely for the following reasons:\n\nMy view is that at the moment the order of the data changes how the transpose function works - and that  this is not expected (at least by me!). I'd expect Columnize by key/value to work the same no matter the order of the data initially, because the keys and values are the same whatever order is used.\nAt the moment using the same function on:\nColumn1;Column2\nSourceFile1;2\nSourceFile2;3\nSourceFile3;-1\nSourceFile1;3\nSourceFile2;4\nSourceFile3;6\nSourceFile1;-3\nSourceFile2;4\nSourceFile3;1\nGives a different outcome to using it on\nColumn1;Column2\nSourceFile1;2\nSourceFile1;3\nSourceFile1;-3\nSourceFile2;3\nSourceFile2;4\nSourceFile2;4\nSourceFile3;-1\nSourceFile3;6\nSourceFile3;1\nThis is particularly problematic in that the first ordering gives the desired outcome, while the second ordering does not, BUT there is no way to sort the second data set to be like the first data set - which means you cannot trivially solve this issue by re-sorting the data.\nIt also seems like a bug because I cannot think of a situation where the desired result would be to have the values in the second and third columns shifted down.. @thadguidry I still don't understand why @belm104 is an anti pattern since they have the same data, just in a different order?. I'm happy to improve the docs in this area. I've split the documentation question into a separate issue - I'm happy to take that one. Still trying to get my head around this. Some of the discussion above makes sense to me. @thadguidry's point about not being able to do the transpose if there is no relationship between the values and the keys. However, I'm still not clear on the Sorting issue.\nAt the risk of introducing more confusion, I've done a further example to try to make the issue I'm grappling with clear:\n\nIf I use \"Transpose->Columnize by key/value columns\" with (obv.) KEY COL as the key column and VALUE COL as the value Column I get the expected result:\n\nHowever, if before I transpose I do a sort on the KEY COL and reorder permanently to get this as my starting point:\n\nThen  I use \"Transpose->Columnize by key/value columns\" with (obv.) KEY COL as the key column and VALUE COL as the value Column I get the result:\n\nThis is not the result I was expecting, and seems to have been caused purely by the order of the data in KEY COL.\nI feel like I'm missing something here - apologies for extending this conversation - I just can't quite get my head around why this sort order changes the outcome in this case.. @wetneb thank you!. So it would be easy to set a hard coded User-Agent for OpenRefine. However, this issue is slightly broader than that - offering the ability for the user to set the user agent - which makes it a more challenging enhancement.. I'm against us (as a default) deliberately sending an agent string that identifies a different piece of software. I understand that if we make it possible to set the Agent, then no doubt that will be used in this way, but I don't think we should make this the default behaviour of the s/w.\nMy other aim will be to support some other http headers - which will probably be the more useful part of the enhancement to be honest - for example to enable content negotiation. I've submitted a PR #1434 which allows the user to set three http request headers:\n\nUser-Agent\nAccept\nAuthorization\n\nAdding additional headers can be done easily either in an extension or in the core code. @thadguidry has suggested adding 'Proxy-Authorization' request header as well - v happy to do this if there is a need for it.\nIf anyone (@paregorios?) is in a position to test and feedback on the functionality from the user perspective that would be v welcome. The ability to set HTTP headers when using Edit Column -> Add columns by fetching URLs is now part of the 3.0 (beta) release:\n User-Agent\n Accept\n* Authorization\nhttps://github.com/OpenRefine/OpenRefine/releases. Just investigating this for the moment\nI tested the supplied file with and without UTF-8 set on import, and found the same result both ways. Potentially related to a known issue in Java which will not be fixed http://bugs.java.com/view_bug.do?bug_id=4508058\nPotentially could use http://commons.apache.org/proper/commons-io/javadocs/api-release/index.html to detect BOM and handle appropriately\n. Maybe two aspects to this:\n1) Displaying any special characters in Column names - this might overlap with #1286 ?\n2) Making sure BOM is never treated as part of the first column name on import?\nI've not looked at the code, but feels like the latter could be a relatively easy think to check for on import and fix?. I've been having a look at this, and there is some old code in /main/webapp/modules/core/scripts/index.js that does something very similar, but checking Google Code site. If it finds a more recent release it puts a notification at the top of the page.\nI've been playing around with modifying it so uses https://api.github.com/repos/openrefine/openrefine/releases/latest to check the latest version and that's straightforward.\nI'm assuming we use the 'tag_name' from the Github JSON to get the latest version number - is that a safe assumption (that the tag_name will reflect the version number)?\nIn terms of comparing versions, should I assume semver for versioning?. OK - that sounds reasonable.\n(n.b. you can have tags that aren't releases anyway - a tag isn't a release by default. The Github /releases/latest api call only returns full releases - not draft releases or prereleases).. I've made an attempt at fixing this. It assumes Semantic Version numbering format of form X.Y.Z where X, Y, and Z are non-negative integer. It ignores any trailing letters e.g. 2.7.1-rc.1 will be interpreted as equivalent to 2.7.1\nComments on PR welcome if this could be approached in a better/more robust way. The problem (or at least part of the problem) seems to be with the java encryption suite. This is different to the solution linked above.\nConnecting with:\nopenssl s_client -connect pleiades.stoa.org:443\nI  can see the following in the response\nNew, TLSv1/SSLv3, Cipher is ECDHE-RSA-AES256-GCM-SHA384\nThis encryption isn't supported in the default Java 8 encryption suite. (a nice way to check what is being used by OpenRefine - add the URL \"https://www.howsmyssl.com/a/check\" to an OpenRefine cell and use Add column from URL to retrieve the JSON :) \nTo support this encryption you need to install the JCE - Java Cryptography Extension http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html\nI'd copied the jars from that download into jre/lib/security/ (replacing the existing versions). I was able to get this working (although by then I'd already installed additional certificates, so I'm not able to confirm whether you need to do both steps, or just the JCE step - I suspect just the JCE will be fine).\nAt this point I now get a HTTP error 403 : Forbidden  from https://pleiades.stoa.org/places/540867/json\nI believe this to be down to the due to the User Agent being specified by OpenRefine. If I curl:\ncurl -I -A \"Java/1.7.0_121\" \"https://pleiades.stoa.org:443/places/540867/json\"\ncurl -I -A \"Java/1.8.0_72\" https://pleiades.stoa.org:443/places/540867/json\netc.\nI also get a 403 forbidden from this website. At the moment it is not possible to change the User Agent used by OpenRefine. See #1217 \"The User Agent OpenRefine uses is currently determined by the version of the underlying Java library\"\n. @paregorios I was using the Linux version of the latest code on Mac, running from the command line (and with an additional tweak and rebuild to set a recognised user agent successfully retrieved JSON from Pleiades)\nJust tried with the OS X production 2.7 distro and found I got the handshake error again :( So now trying to debug that .... @paregorios OK - success. It looks like if you run the Mac version it uses the Java cacerts from /Applications/OpenRefine.app/Contents/PlugIns/jdk1.8.0_60.jdk/Contents/Home/jre/lib/security\n(this is mentioned in https://groups.google.com/forum/#!msg/openrefine/6adewVTQh4Y/gsJdepAmAAAJ;context-place=forum/openrefine, although I definitely have $JAVA_HOME set, so I wonder if there is an issue with the Mac build in this respect).\nAnyway, so I copied the JCE local_policy.jar and US_export_policy.jar into this directory, but then got the \"sun.security.validator.ValidatorException: PKIX path building failed\" error, which means that it is missing a certificate.\nSo I then installed the cert pleiadesstoaorg.crt I got from my browser by visiting pleiades.stoa.org - but that still didn't resolve the issue.\nSo then I installed the root cert from Lets Encrypt (obtained from https://letsencrypt.org/certificates/) using:\nkeytool -import -alias isrgrootx1 -keystore /Applications/OpenRefine.app/Contents/PlugIns/jdk1.8.0_60.jdk/Contents/Home/jre/lib/security/cacerts -file /Path/to/isrgrootx1.pem\nAnd Success!. @jackyq2015 don\u2019t think getting OR on Mac to observe $JAVA_HOME will help with this issue specifically- it just pushes the issue back to a local system  java issue rather than a bundled java issue. \nThere are (I think) two issues:\n inclusion of a specific  root cert\n use of local_policy.jar and US_export_policy.jar from the JCE\nNeither of these are necessarily fixed if we use $JAVA_HOME - it would then just depend on how the local Java was setup. \nI wonder if using a bundled java might be a feature rather than a bug. If we can bundle a version of Java with updated cacert and the policy jars - which would fix both issues. . @jackyq2015 just to note - as far as I know, the local_policy.jar and US_export_policy.jar in the Java distribution is not sufficient for this to work. You need to use the ones from the Java Cryptography Extension (JCE) available at http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html for the decryption to work. The policy jar in the bundled java would need to be updated to be the JCE ones. @thadguidry that's a really good question! I don't know, but I think this blog post might describe the same issue https://blog.spaziodati.eu/en/2014/11/17/the-dark-side-of-ssl-on-java-virtual-machine-jvm/ - so if I'm right, we aren't alone in seeing some issues around this.\nHaving thought about it a bit, on balance I'm in favour of what @thadguidry suggests - avoid bundling the JCE stuff (some slight nagging worry in the back of my mind about legal issues related to this as well), but offer better support.\nI think @jackyq2015 point about whether @paregorios could make a change at Pleiades to avoid the problem in this particular case is also a good one.\nThis Stackoverflow post seems to have a lot of good stuff in - we could even consider testing for the issue and falling back (or just preferring) the suggested Bouncy Castle cryptography library/API that could help avoid the problem https://stackoverflow.com/questions/1179672/how-to-avoid-installing-unlimited-strength-jce-policy-files-when-deploying-an. @msaby which version of OpenRefine are you using?. I've now added a new page on troubleshooting problems fetching data from URLs, putting in the problems noted in this issue and the solutions at https://github.com/OpenRefine/OpenRefine/wiki/Troubleshooting:-Fetching-data-from-URLs\nAs noted by @paregorios some of the solutions are technical, and will be challenging for a user without experience of using the command line or similar level of technical knowledge.\nI've also confirmed through testing that if you use OpenRefine for Mac, the Java cacerts and encryption policy files that are important are the ones that are in the JRE included in the distribution (e.g. in my current install of OR2.8, this is /Applications/OpenRefine.app/Contents/PlugIns/jdk1.8.0_60.jdk/Contents/Home/jre/lib/security/ although the path can obviously vary across installs and versions)\nThis is as far as I'm able to take it in terms of the security certs - we haven't identified a good/easy solution for ensuring that users don't encounter problems here. I think @thadguidry's suggestion of a separate script to do the necessary updates is probably the right approach, but I don't know how to start approaching this.\nIt would be good to update the Java version which is included in the Mac version at some point, but I think that's a connected but separate issue.\nFinally #1217 is also related and outstanding - and I think is something we can do something about - I'm planning on having a look at how we might allow the user to specify request headers including user-agent soon.\nClosing this specific issue now.\n. @thadguidry To be clear, the Java version distributed in the Mac package is JDK 1.8.0_60 - so already on Java 8 - just would be good to update this to a more recent 1.8.X at some point which could improve the cacert situation on Mac - but of course Win and Linux (afaik) would depend on the local Java install still, so it's a minor issue I think.\nI'm not clear what determines the version of Java included in the Mac build - @jackyq2015 ?. @thadguidry happy to have a look at this. The current behaviour is to trim leading and trailing whitespace from any values created by the split. This makes less sense to me once you add in the ability to split in a more precise way (either by regular expression or by string lengths).\nWhich seems the most desirable behaviour:\n Never trim\n Trim when split by text separator but not otherwise\n* User configurable option to trim (or not) values created by split (and if this option, what should the default option be - trim or not trim). Thanks @thadguidry. Pinging @ettorerizza as the person who opened this issue, but I'm inclined to go with 'never trim' at the moment - this puts control with the user, and it is always possible to follow up a 'split multi-valued cells' with a 'trim' if you need to. Thanks @ettorerizza. I've gone ahead with the 'never trim' option at the moment.\nIn terms of the 'join' function - do you have any views on how it should work if it is to 'mirror' this updated split function? \n(Changing behaviour of join to not add whitespace as reported in #1113 is probably v straightforward - I can definitely have a look at that, I'm wondering about any other changes you might have in mind to 'mirror' the update 'split'). @ettorerizza ok thanks - once this is finished off I'll have a look at #1113 at least. Fixed by #1278 (now merged thanks @wetneb). @wetneb probably a good idea, but writing tests isn't something I've done much, and not in Java or OR at all. I tried to look for tests for similar bits of functionality (e.g. split into multiple cols) but couldn't see any.\nAny examples or pointers to get me started?. @wetneb I've written some tests - although now waiting for some feedback on desired behaviour as my test raised a question (which is good - my test failed in an interesting way :). Just a quick question - in terms of which directory the test should live in - I notice that KeyValueColumnizeTests is in  /tests/model - is there any reason why? I would have expected these to go under /tests/operations or similar, but that doesn't seem to exist?. @wetneb I'm seeing codacy issues relating to the naming of methods in the tests I've written. I followed existing tests for these methods - can I ignore these, or should I be fixing?. @wetneb thanks! OK - I think I've done everything to get this ready for merging - as long as you are happy!. @remram44 thanks for noting the missing licence - now submitted a PR to add this in and correct this oversight. @remram44 Is there anything I can do to help progress this work?. @remram44 no problem at all! V grateful for your work - just shout if there is anything I can do to help. To be contrary, I have to admit I prefer the text based control character symbols. But happy to go with the majority view!. I prefer them both because they are easier to read (larger text) and I feel they are more explicit and easier to learn - I don't have to learn what each of the icons/graphics stand for.\nOn the latter point - easier to know the difference between SP and NBSP in comparison to learning which one is represented by a filled in square, and which is an empty square.. @thadguidry I agree some of the icons are clear/well understood (the tab is a good example). I also accept that the icons/graphics are defined internationally - so it maybe that I just need to get used to them.\nAt the moment I think the weight of opinion (such as it is) is in favour of the graphic symbols - you and @ettorerizza vs me. Unless anyone else wants to support my view, then I think graphics win it tbh. Maybe of interest - in this post the approach taken is to replace char(0)-char(32) with the equivalent Unicode control picture https://charlee.li/display-non-printable-characters.html. @srugano no - it looks like there is a problem in parsing dates starting with the week day name right now - see #1908 \nAs a work around right now you could use:\nvalue.substring(4).toDate(). Hmm - OK - something odd here - in #1908 there is definitely a problem, but it seems to be something about the state of the date after import from Excel - I'll do some more analysis on #1908 \n@srugano are these dates imported from Excel?. An issue here is that you cannot tell from the results which value resulted in the listed outcome. To take the simplest example:\nproject \"Christmas Gifts\"\n| gift | recipient |\n| ------------- | ------------- |\n| lamp  | mary  |\n| clock  | john  |\n| sweets  | mary,john  |\n| wine  | anne,mary  |\nproject \"My Address Book\"\n| friend | address |\n| ------------- | ------------- |\n| john  | 120 Main St.  |\n| mary  | 50 Broadway Ave.  |\nTransformation\nadd values from column \"address\" from project \"My Address Book\" based on keys (with multiple values separated by \",\") in column \"recipient\" from project \"Christmas Gifts\"\nforEach(value.cross(\"My Address Book\", \"friend\", \",\"),r,r.cells[\"address\"].value).join(\",\")\nOutput\n| gift | recipient | address |\n| ------------- | ------------- | ------------- |\n| lamp  | mary  | 50 Broadway Ave. |\n| clock  | john  | 120 Main St. |\n| sweets  | mary,john  | 50 Broadway Ave.,120 Main St.\n| wine  | anne,mary  | 50 Broadway Ave.\nIn the last row of this Output it is impossible to know whether the address is related to anne or mary.\nIn more complex situations you could have multiple matching rows in the project named in 'cross', resulting in a list of matching values, which you cannot tell which key matched.\nThe only way I can see you could make this explicit would be to return a more structured response (e.g. maybe JSON which labelled each returned array of values with the key used to look them up).\nI'd suggest this would be better in a separate operation, rather than making cross more complex or capable of returning different format of results depending on the parameters. Also worth noting that with the change to use a string rather than just a 'cell' object for the lookup, the desired outcome can be achieved in GREL:\nforEach(value.split(\",\"),v,forEach(v.cross(\"My Address Book\",\"friend\"),r,r.cells[\"address\"].value).join(\",\")).join(\",\")\n(although I agree it isn't pretty). @felixlohmeier OK - hadn't noted the performance difference, and agree that's a big factor.\nI have a common use for cross in which the target project for the cross contains multiple matches for a row in the source project - which I'm not sure this deals with so effectively - although maybe I need to experiment a bit more with the possible GREL!\nI'm still not completely convinced the best thing to do here is change the cross function this much but I'd be v happy to see a new function that works as you need - I don't particularly mind it being hacky, just slightly uncomfortable that it introduces ambiguity into the function.\nI'm just one voice though. See my latest comment on #1289 - not sure just amending the cross function is going to be sufficient. Thanks all. @thadguidry yes have checked it in Safari, FF & Chrome. \nThanks to you all for guidance & encouragement :). It feels like what is needed here is just a \u2018regex\u2019 function that takes a regex and returns an array of any capture groups\nI would say the two things that frustrate me about \u2018match\u2019 are:\n1) the need to match the entire string\n2) the inability to do a /g flag to do a global reg ex. Isn\u2019t this the same as #1203?. I\u2019m happy to have another look at this . @jackyq2015 I thought about that, but it felt like that was just trying to avoid the issue rather than resolving it, and will decrease the overall responsiveness of the filter - which I think would be a negative outcome for the user.\n@thadguidry I'm not quite clear on why '\\' is a special case - why is getting an error with ^tha\\ any different to getting an error with ^tha[ ?. Thanks @thadguidry thanks @jackyq2015 . I definitely understand that the user needs better feedback on what is happening and how to fix the problem. \nWhat I'm thinking is that I can write something to validate the regular expression at the server end, and pass back a meaningful error message in JSON - which would be displayed in place of the current error generated by the java exception. Does this sound like a good approach?\nLooking at the regexr example shared by @thadguidry, it catches and reports the following errors:\ngroupopen:\"Unmatched opening parenthesis.\",\ngroupclose:\"Unmatched closing parenthesis.\",\nquanttarg:\"Invalid target for quantifier.\",\nsetopen:\"Unmatched opening square bracket.\",\nesccharopen:\"Dangling backslash.\",\nquantrev:\"Quantifier minimum is greater than maximum.\",\nrangerev:\"Range values reversed. Start char is greater than end char.\",\nlookbehind:\"Lookbehind is not supported in JavaScript.\",\nfwdslash:\"Unescaped forward slash.\",\nesccharbad:\"Invalid escape sequence.\"\n\nWe won't need all of these but it looks like a good starting point.\n. No progress as yet, but will be looking at it next week if that's OK. @thadguidry  I think this is done, at least to the extent needed for now, I think it can be closed. Thanks @wetneb - is there an example of a test already that uses a RowVisitor with assertions?. Thanks for that hint @wetneb - now added tests. @jackyq2015 @thadguidry not thinking of adding tests for the javascript/UI at the moment. I think there was an earlier attempt to add some tests for the JS using Windmill (https://github.com/windmill) but these have been disabled in the Travis build by #1227 because they were broken and Windmill discontinued. Thanks all!. :) I wrote the tests anticipating I was going to need a test if I made changes because of #1113 - but then found it was actually a display issue - so more bad planning than heroic!. Thanks @marioa - there is a fix for this ready for the 2.7.2 release  - see https://github.com/OpenRefine/OpenRefine/commit/e482878596cbda7d6e7e99cb1f02d57aa8f4d549#diff-6d8a16be362b3c276c8837f74ac81415. I'm feeling (and I guess this is what @thadguidry is also saying) there are several different types of (potentially overlapping) requirements in this issue and they don't necessarily all have the same solution. It might be worth us considering the different cases separately and seeing whether there is a reasonable approach without breaking OpenRefine.\nPerhaps our starting point should be one or more use cases describing the scenario and what the desired outcome is (basically formalising the examples Ettore has linked to so far)\nFor example I don't think we need a column variable to deliver (e.g.) a facet which provides a union of values across two columns (basically a union of two facets?). @jackyq2015 I've had a look, but I think until we switch to using the wikipedia i8n library (as per the PR @thadguidry asked about) we can't use the fallback option in isoloation.\nSo I'd suggest that any effort here should be put into getting #1285 to the point where we can merge?. I can't replicate this error with OpenRefine 2.8 (Linux version, running on macOS High Sierra) and Java:\nOpenRefine >>java -version\njava version \"1.8.0_72\"\nJava(TM) SE Runtime Environment (build 1.8.0_72-b15)\nJava HotSpot(TM) 64-Bit Server VM (build 25.72-b15, mixed mode)\nI do find that non-ascii characters are dropped from file names on import and export (presumably not trusting the OS to necessarily handle filenames containing non-ascii nicely), but if I name my projects with non-ascii chars, then that seems to work fine within OpenRefine\n\nIf the problem is noticed after a restart, it could be that the issue occurs when the metadata is written to disk?. I can reproduce using that Dockerfile.\nIf I set the lang/locale to C.UTF-8 the issue goes away\nFind locale on starting the Docker container:\nroot@37c04a8bf856:/opt# locale\nLANG=\nLANGUAGE=\nLC_CTYPE=\"POSIX\"\nLC_NUMERIC=\"POSIX\"\nLC_TIME=\"POSIX\"\nLC_COLLATE=\"POSIX\"\nLC_MONETARY=\"POSIX\"\nLC_MESSAGES=\"POSIX\"\nLC_PAPER=\"POSIX\"\nLC_NAME=\"POSIX\"\nLC_ADDRESS=\"POSIX\"\nLC_TELEPHONE=\"POSIX\"\nLC_MEASUREMENT=\"POSIX\"\nLC_IDENTIFICATION=\"POSIX\"\nLC_ALL=\nSet lang to C.UTF-8 and re-check:\nroot@37c04a8bf856:/opt# LANG=\"C.UTF-8\"\nroot@37c04a8bf856:/opt# export LANG\nroot@37c04a8bf856:/opt# locale\nLANG=C.UTF-8\nLANGUAGE=\nLC_CTYPE=\"C.UTF-8\"\nLC_NUMERIC=\"C.UTF-8\"\nLC_TIME=\"C.UTF-8\"\nLC_COLLATE=\"C.UTF-8\"\nLC_MONETARY=\"C.UTF-8\"\nLC_MESSAGES=\"C.UTF-8\"\nLC_PAPER=\"C.UTF-8\"\nLC_NAME=\"C.UTF-8\"\nLC_ADDRESS=\"C.UTF-8\"\nLC_TELEPHONE=\"C.UTF-8\"\nLC_MEASUREMENT=\"C.UTF-8\"\nLC_IDENTIFICATION=\"C.UTF-8\"\nLC_ALL=\nOnce I've done this, when I use non-ascii chars in the Project name it persists through the metadata save process. See PR #1497 . In the 'test.json' file that you've posted there is an invisible Byte Order Mark character in the column name \"Item #\".\nMy guess is that this is the problem, but I need to do some more testing to check. Scratch that, seems like the presence of the BOM is a red herring. From what I can see the problem is with the request.\nIf you try:\ncurl -d operations='[{\"op\":\"core/column-addition\",\"description\":\"Create column item no at index 1 based on column \ufeffItem # using expression grel:value\",\"engineConfig\":{\"mode\":\"row-based\",\"facets\":[]},\"newColumnName\":\"item no\",\"columnInsertIndex\":1,\"baseColumnName\":\"\ufeffItem #\",\"expression\":\"grel:value\",\"onError\":\"set-to-blank\"}]' http://127.0.0.1:3333/command/core/apply-operations?project=2126170820257\nI think it will work (I could make this work locally)\nHowever, I've not yet managed to get a curl which reads the JSON from the file working - suspect this is me not quite getting the right curl incantation rather than anything else. I'm at the limit of my knowledge of curl,  but from a bit of poking around, it looks like to pass the content of the file to a named POST parameter, you have to use something like:\ncurl --data-urlencode operations@test.json http://127.0.0.1:3333/command/core/apply-operations?project=2126170820257\nHowever from what I've been able to see this will only work if there are no newlines \\n in the file.\nI think we've established this is not an OpenRefine, but a curl issue, so I'm inclined to close? @quocvu @jackyq2015 . @quocvu as in my previous example, I'd suggest the simplest curl option is to use --data-urlencode having removed the newlines from the JSON in the operations file.\nI'm looking at the code and the documentation to see what I can do about the issues. I think the first step will be to update the documentation to be honest. We can then look at the code to see if anything can be done to support other options for passing parameter values/data to the API. I've updated the HTTP API documentation at https://github.com/OpenRefine/OpenRefine/wiki/OpenRefine-API\nIf there are further issues with the documentation, or you would like the HTTP API to work differently to  how it works currently, please open a new issue for each problem / request (and of course, feel free to add to the documentation where you can improve it directly). Just chipping in on the subject vs tag question. I think the key thing to differentiate is 'metadata to help navigate OR projects on your local install' vs 'metadata that may be imported/exported with the project'. I'm in favour of the tag system for local project management, but it may not be the right information to (e.g.) export as 'keywords' if we support metadata export for things like data packages in the future.\nI don't have very strong feelings about it, but my instinct is with @thadguidry that 'subject' should be kept for more general purposes and tags should go into a field of their own (which could be 'local-subject' rather than 'tags'?). But it's only an instinct - no strong reasoning to back this up. Just looking again at this, I think David's role as the originator is signalled through the opening para in the Credits section:\n\nThis software was created by Metaweb Technologies, Inc. and originally written and conceived by David Huynh dfhuynh@google.com. Metaweb Technologies, Inc. was acquired by Google, Inc. in July 2010 and the product was renamed Google Refine. In October 2012, it was renamed OpenRefine as it transitioned to a community-supported product.\n\nI think this works alongside the new Emeritus creators list. Sorry - my comment was meant to say 'Emeritus Contributors' - basically I agree with you @thadguidry  - the Credit para + the two lists of Emeritus & Current contributors is sufficient.. +1. I think all the backers contribute in different ways - whether they are individuals, groups or organisations. I'm OK with having a single list including all the different types of backer. @jackyq2015 yes - it takes a PatternSyntaxException as an argument and looks for specific patterns in the error message. If it doesn't find any of the defined patterns it falls through to just returning the message which PatternSyntaxException returns from its getMessage() method. @thadguidry I think that's caused by the lack of a break statement in the default case - I can update that . Pickles. Fixed by #1373 . @ettorerizza no - this is a different option - apologies I got the labelled slight wrong originally - I've amended this issue.\nHere it is in the menu:\n\n. Just to check - is this loading projects every time, or just those that don't have (certain) metadata set?. No exceptions or issues flagged in the console. However, checking I can see that get-all-project-metadata is coming back with an empty list:\n{\"projects\":{}}. OK, think I have tracked down the issue to the parsing of the \"modified\" date from the project metadata.json. This happens in ParsingUtilities.stringToLocalDate(String s)\n```\nstatic public LocalDateTime stringToLocalDate(String s) {\n      System.err.println(LocalDateTime.parse(s, DateTimeFormatter.ISO_LOCAL_DATE_TIME));\n      return LocalDateTime.parse(s, DateTimeFormatter.ISO_LOCAL_DATE_TIME);\n    }\n```\nThe string is going in, but nothing is being returned (which I think leads the creation of the projectmetadata to fail in ProjectMetadata.loadFromJson\"\nProjectMetadata pm = new ProjectMetadata(JSONUtilities.getLocalDate(obj, \"modified\", LocalDateTime.now()));\nThe issue seems to be that the existing metadata.json records time with strings like:\n\"modified\": \"2014-01-15T21:46:25Z\",\nBut LocalDateTime.parse(s, DateTimeFormatter.ISO_LOCAL_DATE_TIME); requires it to have no timezone information.\nIf I manually edit the metadata.json to read:\n\"modified\": \"2014-01-15T21:46:25\",\nThe project displays OK in the UI\n. @jackyq2015 I have projects created over a long period - I don't know which version of Java or OR I was using with them I'm afraid. I think breaking backwards compatibility with previous versions of OpenRefine is a very bad idea. OpenRefine 2.5 no longer works on my Mac, so I can't go back and do an export/import for older projects. \nI think definitely newer versions of OpenRefine should open projects created in older versions of OpenRefine.\nI can see an argument for saying that projects created in more recent versions of OpenRefine may not open in older versions - that feels a less problematic breaking change, although still I think we should avoid this where possible. I feel like I'm missing something here - didn't the options metadata already get added? I already see the import options metadata at the endpoint /command/core/get-project-metadata?project=\\<id>. Once that syntax issue fixed then this PR solves the immediate problem. Just wondering - are we sure that the only dates out there in older projects are of the format yyyy-MM-ddThh:MM:ssZ ? Could they ever be expressed with other timezone data - e.g. the Java ISO_DATE_TIME format which allows datetimes like:\n'2011-12-03T10:15:30+01:00[Europe/Paris]'\nIf we are sure it is only those ending in Z that are problematic then that's OK, just wondering if we need a more comprehensive solution.\nIn relation I think it would be good to catch and deal with situations where the string->datetime conversion fails silently (which is essentially what happened in this case) so that the project isn't hidden from view. Potentially have a check for a valid datetime before trying to create the ProjectMetdata object, and fallback to using current date if the one parsed from string is invalid/null/zero length?\n(this could be a separate issue and PR though if you want to get this one through). I can't recreate the error, but I would note that from what I can see this XML file doesn't work well in OpenRefine because it isn't a single hierarchical document, but rather contains 51 different types on thing each with it's own properties. From what I can see there are relationships defined between the entities described in the document based on 'id'. So this single file seems to me to be like a set of tables from a relational database, rather than a single table.\nLoading this into OpenRefine doesn't work well as it can't be easily represented as a single table of data. In fact, OpenRefine ends up putting data in the same row which should be in separate rows, and groups rows together as 'records' when it shouldn't. There are also a large number of columns in the project because of all the different types.\nThe issue you've encountered loading the xml aside, you may want to consider what you are trying to achieve with this data in OpenRefine, and whether it will work for you. The OpenRefine Google Group is a good place to ask questions about the best approaches to different data and data structures, based on what you are trying to achieve.. @lafaurie any more information on this one? We've not been able to replicate the error you reported, so hard for us to progress unless you have any other details you can share with us?. Additional support for this at https://groups.google.com/forum/#!topic/openrefine/ugbIcPJCBpM. I don't think anyone has got back to @AlexandruAmarandei so I don't think is resolved yet. I'm already looking at the URL fetching as part of #1217 so I can look at this as well. OK - the issue seems to be that the URLConnection library does not follow redirects when they use different protocols to the original request. In particular this means that if you request an http URI, and there is a redirect to an https URI, this will not be followed. Redirects which are to the same protocol (http->http or https->https) work as expected in OpenRefine.\nThis behaviour is deliberate, as it stops redirects taking the user from a secure protocol to an unsecured one (https->http). The second answer in this StackOverflow post gives a good explanation of why this is a bad idea https://stackoverflow.com/questions/1884230/urlconnection-doesnt-follow-redirect\nWhat is less clear to me is whether there are any issues in supporting http->https redirects (which will be the far more common scenario) - this feels like it is increasingly common and we could support it without any security concerns. \nAny other views?. Looks like a work around for cross-protocol redirects was already implemented for data import in response to #748. Relevant commit is 4f7da9d18e05361a6b1135528394b59f1e13b244. Not that it matters I don't think, but the use of the spelling \"heterogenous\" rather than \"heterogenous\" is deliberate because it directly quotes the Nobel citation which uses the first spelling rather than the second (both are valid spellings of the word). I'm having issues with some tests locally at the moment - I don't think it is anything in this PR... is Master testing OK currently?. @ettorerizza would be good to get any feedback you have on the functionality if you have any chance to look at it. Ah yes - I saw that discussion previously. I think there was an issue with access to the URI being used in the testing being blocked as well?\nI think the 'solution' was to allow Travis to skip these tests at the moment until we can mock the requests?. Ah - thanks. Looks like its built successfully now :). Unless there are any further comments, I'm going to go ahead and merge this .... Thanks @wetneb !. On the VIAF side if you use the SRU interface you can limit the number of records returned, but I don't think this is the problem here. The issue is that requesting a single record that has many alternative name representations etc. you get a very large chunk of JSON which you can't limit through the API.\nI'm in favour of a user configurable text cap on the amount of text show in a cell with a couple of caveats:\n\nThere needs to be a 'show everything' setting (rather than just putting large numbers to the limit and hoping)\n\nWhen you click 'edit' it should always offer the full text for editing. See #1354 for the display of consecutive spaces (separate to the display of non-printing characters). I think it is just an easy css fix tbh but I need to try it and test. #1497 implemented the display of consecutive spaces. Thinking that the padding could be done by adding 'lpad' and 'rpad' functions to GREL, following similar rules to \n\n\nRedshift LPAD/RPAD functions https://docs.amazonaws.cn/en_us/redshift/latest/dg/r_LPAD.html \n\nMySQL LPAD/RPAD functions\nhttps://dev.mysql.com/doc/refman/5.7/en/string-functions.html#function_lpad \n\nhttps://dev.mysql.com/doc/refman/5.7/en/string-functions.html#function_rpad . Just noted that org.apache.commons.lang3.StringUtils (which we already use) has leftpad and rightpad functions which we can probably use to create GREL lpad() and rpad() functions\n\n\nhttps://commons.apache.org/proper/commons-lang/apidocs/org/apache/commons/lang3/StringUtils.html#leftPad-java.lang.String-int-char-\n\nhttps://commons.apache.org/proper/commons-lang/apidocs/org/apache/commons/lang3/StringUtils.html#rightPad-java.lang.String-int-java.lang.String-\n\n. Still some problems with this I'm afraid.\nThe following wikitable works:\n{| class=\"wikitable\" style=\"width:80%; text-align:center;\"\n|-\n! President\n! Name\n! Term of office start\n! Term of office end\n! Political Party\n|-\n| rowspan=\"2\" align=\"center\"|[[Carlos Menem]]\n| '''[[Eduardo Bauz\u00e1]]'''\n| 8 July 1995\n| 5 June 1996\n| [[Justicialist Party]]\n|}\nIf I add in a colspan=\"2\" in one of the column headers, with \"Extract references in additional columns\" checked, I get an error:\n{| class=\"wikitable\" style=\"width:80%; text-align:center;\"\n|-\n! President\n! Name\n! colspan=\"2\"|Term of office\n! Political Party\n|-\n| rowspan=\"2\" align=\"center\"|[[Carlos Menem]]\n| '''[[Eduardo Bauz\u00e1]]'''\n| 8 July 1995\n| 5 June 1996\n| [[Justicialist Party]]\n|}\nand if I take the original table and add a header cell in one of the rows I also get an error:\n{| class=\"wikitable\" style=\"width:80%; text-align:center;\"\n|-\n! President\n! Name\n! Term of office start\n! Term of office end\n! Political Party\n|-\n! rowspan=\"2\" align=\"center\"|[[Carlos Menem]]\n| '''[[Eduardo Bauz\u00e1]]'''\n| 8 July 1995\n| 5 June 1996\n| [[Justicialist Party]]\n|}\nIn both cases if I uncheck the \"Extract references in additional columns\"  then the table parses successfully. Oh dear :( - let me check - apologies. duh - forgot to clean before building - now successful. The current i8n solution in use doesn't support a fallback option in the way we need (I did look at this previously but can't remember the details) - so agreed that #1285 is the way forward. At the moment there is some information stored in the recon object that feels like it would cause problems if there is a change to the service:\n\ncell.recon.service\ncell.recon.identifierSpace\ncell.recon.schemaSpace\n\nGiven these are already there, and because my instinct is it will be simpler option, I'm inclined to store the link directly in the recon object. I was sure we had an issue (or maybe a discussion on the Google group?) about doing an enhancement so that if there was an attempt to store an array in a cell, it would get serialised to a string (probably a JSON representation of the array) before it was stored - to prevent this odd behaviour of the preview looking fine but the outcome of the operation being unexpected.\nCan anyone (@thadguidry @magdmartin ?) remember this discussion and point to where it happened so we can cross reference to here?. Thanks @thadguidry - that's definitely related and would be helpful, but wasn't the one I was thinking of, which was explicitly about what to store in the cell when the output of the expression was an array. Hi @dbucci24 you can contribute to existing translations and create new translations through a service called Weblate. Got to https://hosted.weblate.org/engage/openrefine/ and follow the instructions.\nThere is already a Spanish translation which you could review and add to\nThanks very much. Hi @vinzruzell - you can contribute to existing translations and create new translations through a service called Weblate. Got to https://hosted.weblate.org/engage/openrefine/ and follow the instructions.\nThere is already a Filipino translation which you could review, or you can start a new translation for Cebuano etc\nThanks very much. Thanks @joanneong - would you be able to look at doing #1487 at the same time? They should be pretty similar. Hi @joanneong did you make any progress with this?. No problem at all @joanneong - we are always very grateful for any time people can spare to contribute! (I've done barely anything over the last month but just got a few hours last night and was reviewing stuff, that was the only reason I asked). With #1500 you can see I tried to put in translations where I could. I don't entirely trust Google translate for the OpenRefine interface because there are so many ambiguities (e.g. 'row', 'cell', 'blank' all have multiple meanings in English) - so I used a combination of existing translations and Google translate.\nIdeally we could just implement the English and rely on the i8n library falling back on English where no specific translation is supplied. Unfortunately the current  i8n library we use doesn't support fall back options, so not having translations in place can lead to problems (#1350 #1455) but the solution to this is to replace the i8n library (see #1285). May require #1491 to be done first, as simply using the TextTransform operation here (as with the current 'Blank cells' function) results in the inconsistent behaviour described in #1491 (null is left as null rather than replaced with an empty string). It looks like it is 'sameValue' in main/src/com/google/refine/expr/ExpressionUtils.java that leads to the current behaviour. This treats a null value and a zero length string value as being the same.\nIt's easy to change this, but before doing this, I'm trying to think of any implications of ceasing treating these as equivalent here.\n@thadguidry do you have any background info on why these are currently treated as equivalent when comparing values?. A looks sensible to me. I'd go further and suggest that in addition it would be good to have the option to decide whether inverted commas are commented (e.g. with a \\ character) or doubled (as currently) or left alone.\nB I'm not clear on - what does it mean to omit blank or null rows? Wouldn't it make more sense for the user to filter to the non-null rows in the  given example?\nIn the example given it looks simple - just omit that single value. But Templates are often (usually?) more complex than this with multiple values in a single template. How would the 'omit blank/null rows' work in this case?\n. I'm inclined to keep coalesce(null) -> null because that keeps us in line with the SQL function and treating null and blank in a more relaxed way is partly what has led to the need for the coalesce function in the first place.\nVery happy to get other input on it @ettorerizza @eximius313 . Hi @ettorerizza all coalesce does is find the first non-null value in a list, it doesn't do any concatenation. You can use this to fall back on an alternative string value or empty string when the value in the cell is null. So it's a helper function to avoid trying to concatenate null values, but doesn't reduce it to a single step.\nSo for example if you try, from the 'name' column the GREL\ncells[\"given_name\"].value + \" \" + value.coalesce(\"\")\nOR\ncells[\"given_name\"].value + \" \" + coalesce(value,\"\")\nYou should see\n\nThe question @thadguidry is asking is whether rather than require the blank string in there could we allow coalesce(value) to automatically fall back to an empty string if no second string is specified. I'm slightly against this as it contradicts how the coalesce function works in SQL where it falls back to null if no non-null value is available.\nChanging the behaviour would allow for the slightly simpler GREL:\ncells[\"given_name\"].value + \" \" + value.coalesce()\nBut I don't think that's a big enough saving to divert from the SQL usage of coalesce.\nYou can also have a long list of values in the coalesce function and it will pick the first non-null one - so if you had data spread across several columns in theory you could include each column.value in the coalesce function like\ncells[\"given_name\"].value + \" \" + coalesce(value,cells[\"other_name_1\"].value,cells[\"other_name_2\"].value,\"\")\nHowever - there is another problem with this - which I've reported against #820 (and #1036) - which is that this doesn't work because if cells[\"other_name_1\"] (etc.) is null, then cells[\"other_name_1\"].value gives an error rather than null - and so coalesce fails with an error in this scenario :((( But I think that needs it's own solution (I may have one) and needs to be separated out from the addition of the coalesce function, even though from a user perspective until we solve this other problem the value of coalesce is limited.\n. @eximius313 good point about at least 2 arguments - that would be in line with the ANSI SQL definition - I will implement. @eximius313 PR updated to require at least 2 arguments. I\u2019d need to have a look & think about this. Making the function treat errors in a specific way probably possible but making it treat only a specific error like that could be difficult.. Going to close this with the merge of #1536. Unfortunately as @wetneb says the suggestion by @ettorerizza won't work as the error comes from upstream, so can't be caught by the coalesce function.\nI've made a relevant suggestion for discussion at #1036. Thanks @thadguidry I've done what I can with the translations - but some I've not been able to update/add. . @thadguidry I think #1486 and #1487 cover the first point - I was going to look at these after this PR was done.\nThis PR doesn't change the current behaviour of 'toDate', 'toNumber' or 'toString', but now I've looked at them toDate and toNumber are written in a slightly odd way, and with toNumber I think lead to unexpected results (converting an empty string toNumber results in null). I'll create issues for these separately.. Thanks for the translation you have done @vinzruzell. There are no other files to translate for this software.. @wetneb @fokky thinking out loud, but would it make sense to use the values of the P1963 property (properties for this type) from the type chosen to reconcile against?\nI guess we'd need to define this somewhere in the existing API as something like: 'disambiguation properties' or similar to allow this to work across multiple sources..... This fixes #1524. There is still some slight oddness with date/time when switching between versions, but it is no longer preventing the project loading in older versions of the s/w. @tfmorris fair point - I've added an issue to make sure this is tackled. Hi @wetneb - sorry not to be more forthcoming with feedback - I have tried it out a few days ago but hadn't managed to feedback.\nI'm probably not a very typical user at this point so finding it hard to approach this as a novice. However a couple of things I managed to do wrong:\n\n\nI added a 'label' to the schema, but didn't populate it - leading me to overwrite the existing labels - I think a warning about empty labels/descriptions/properties is worthwhile? (although I guess we have to allow it if that's what the user wants to do)\n\n\nI had originally filtered my data set to only do one edit - but at some point I removed the filter and then went ahead with the wikidata edit. I think an extra pop-up to warn \"You are about to edit XX wikidata items\" OK/Cancel might avoid others making the same mistake. @wetneb sorry to have been quiet - but I've been using the extension and really appreciating it - saving me lots of time! I've not had a chance to try the latest version yet but one thing I wanted to be able to do and couldn't find a way was setting a property to 'no value' or 'unknown value'. Have I missed something, or is this not possible?\n\n\nThanks again for this fantastic work so far :). Agreed that using null or blank strings is probably a bad idea as we know these are ambiguous! I don't have a better idea than some special value that could be used I'm afraid. Links for the items from the Preview tab go to Freebase rather than Wikidata - highlighted in this screenshot\n\n. Yes I was creating new items.\nI think the relevant JSON in the history is:\n{\n    \"op\": \"core/recon-judge-similar-cells\",\n    \"description\": \"Mark to create one single new item for all cells containing \\\"Junta Departamental de Concepci\u00f3n\\\" in column Name of House or Chamber ES\",\n    \"engineConfig\": {\n      \"mode\": \"record-based\",\n      \"facets\": []\n    },\n    \"columnName\": \"Name of House or Chamber ES\",\n    \"similarValue\": \"Junta Departamental de Concepci\u00f3n\",\n    \"judgment\": \"new\",\n    \"shareNewTopics\": true\n  },\n  {\n    \"op\": \"core/recon-judge-similar-cells\",\n    \"description\": \"Mark to create one single new item for all cells containing \\\"Junta Departamental de San Pedro\\\" in column Name of House or Chamber ES\",\n    \"engineConfig\": {\n      \"mode\": \"record-based\",\n      \"facets\": []\n    },\n    \"columnName\": \"Name of House or Chamber ES\",\n    \"similarValue\": \"Junta Departamental de San Pedro\",\n    \"judgment\": \"new\",\n    \"shareNewTopics\": true\n  },\nAfter the new items were created I also then did notice some oddities in terms of the content of the cell - not linking to the Wikidata item as I'd expect - I ended up re-reconciling the cells in the column to fix it. Another issue I've noticed. Once I've started to setup the Wikidata schema, if I add a new column it doesn't appear in the choice of items I can add to the wikidata schema unless I completely refresh the screen (losing facets and any unsaved schema changes). @wetneb I think options to save the schema / clear unsaved changes would be good. Just noting https://www.wikidata.org/wiki/Wikidata:Project_chat#Limit_page_creation_and_edit_rate - does/should the extension have rate limiting to avoid falling foul of limits on Wikidata?. Discussion on #1496 highlighted that coalesce should have at least 2 arguments to follow the same as coalesce in the SQL ANSI standard - I will update this PR to make the function require at least 2 arguments. Now updated as per #1496 discussion. Definitely in favour of keeping Facet by Blank. For backwards compatibility (applying historical transformations that use a \u2018blank\u2019 facet as part of criteria) and also in many situations users won\u2019t care about the difference between empty string and null because they just want to identify missing data etc.\nI would keep the options together where they currently are for this PR - moving things around in the menus means lots if past documentation becomes inaccurate or needs updating- we should only do this where we are v clear on the benefits. \nI\u2019m not against re-thinking how these menu options are arranged but I\u2019d prefer to see an overall approach as part of a major release . Ah @wetneb I misread your comment - sorry. Agree that the new options could go elsewhere in the menu without breaking anything. \nPersonally I\u2019m in favour of keeping the three options together as I think this is easier for instruction etc and also gives context when explaining the function (blank == empty OR null). . I\u2019m in favour of the arrangement in the screenshot posted by @joanneong\nI\u2019m against putting the new options higher in the menu hierarchy thank \u2018by blank\u2019 for the same reasons as @ettorerizza. Can we use \u201cnull or empty\u201d rather than \u201cnull & empty\u201d?. @ettorerizza - suggest we need a separate issue to display null in cells - could you add one? I see this as distinct from #1286 which is about the display of invisible characters. This is what I'm proposing: null will display (as it does in the Expression preview window). Empty string - by definition there is nothing to display. I suggest we just leave this blank.\nNote that we will still need a resolution to differentiate between an empty string and a string of only invisible/non-printing characters\n\nRow 2 = empty string\nRow 3 = null\nAdding this display of null is trivial and I can do a PR now if @ettorerizza is happy with this approach. My instinct is that this would be the wrong thing to do - it's definitely a much bigger discussion! My first thought is that the outcome of a transformation can be an empty string, or can be null and these happen for different reasons - so I feel they should be treated differently.. Agree it\u2019s subtle :)\nI\u2019d say that behaviour you illustrate is correct- but what is missing is the option to set \u2018null\u2019 when you manually edit a cell !. Fixed by #1544 . @jackyq2015 styling invisible characters is separate issue #1286. null is not an invisible character in that sense - it is a cell value not a character\nThe character issue is more complex and better to keep separate IMO. V happy to look #1546 but prefer to keep tgat separate to this issue & PR. OK - Apologies for jumping the gun @thadguidry @wetneb. Is the best approach to revert this PR merge and then do some further work for a new PR?. OK - no problem. Will have a look at it hopefully this weekend. @jackyq2015 why is it not the right behaviour? \nThe only way I can see to avoid this would be to treat the missing value at the start of row 2 as an empty string (rather than null). Is that what you think should happen?. @quan17 the 'find' function is quite discrete so you could add it into OpenRefine 2.6 if you wanted. See this PR for the files that were added/changed to add the new 'find' function https://github.com/OpenRefine/OpenRefine/pull/1432/files. Hi Maxime @gobertm - sounds great!\nWhat I find useful when looking at adding functionality like this is to have a list of test cases and what the expected outcome would be. This is useful in terms of communicating how the function will work, and also gives you a set of test statements you can include in your unit tests for the functionality.\nIn this particular case where you are changing an existing function please make sure (and add test cases if they don't already exist) that the function continues to work as expected when used as currently intended.. @msaby reported further issues importing n3 at https://groups.google.com/d/msg/openrefine/t67D8_JrUs0/W-wL1jdXBQAJ\nI had a quick look at Jena, and found some things that suggested it doesn't full support n3 - only the subset of n3 that makes up Turtle / ttl.\nThis page http://jena.apache.org/documentation/io/index.html says \".n3 is supported but only as a synonym for Turtle.\"\nLooking at https://github.com/apache/jena/blob/master/jena-arq/src/main/java/org/apache/jena/riot/lang/RiotParsers.java#L58 seems to confirm this (although I'm not 100%)\n. I had the same issue - fixed it up by fixing all the data via find/replace across all metadata.json\nI tend to feel that adding the Z at parsing time as suggested by @wetneb might be worth it to avoid issues for any other testers, although I don't have strong feelings about it. I think the function is doing what it is meant to do in that it has a very literal function - blank cells that contain the same value as the cell above, within the context of the record.\nIn this scenario, this results in a situation where some of the rows are then treated as new records.\nThe 'records' mode in OpenRefine is really an interpretation of the data in the table at the time, not a strong records model - it is very easy (as here) to separate rows that previously belonged to one record. Changing the order of the columns or resorting rows results to changes to what records exist.\nSo I think the first question is what are you trying to achieve here that leads you to blank down?. It feels like there are several issues here:\n\n\nDoes blank down work as currently intended?\n\n\nThe Paris/Nice example in your screencast seems to work as I would expect (although I understand it doesn't work as you want).\n\n\nThe Paris/Berlin/London example doesn't look like it is working as I would expect as I'd expect the first Berlin row to have \"Female\" in it - this looks like a bug to me\n\n\nShould the blank down work differently?\n\nDoes the Record calculation work as currently intended?\nShould the Record calculation work differently?\n\nThe only thing I'm reasonably sure about now is that the Paris/Berlin/London blankdown doesn't work as I'd expect!\n. Oh - and another Q:\n5. When should Records be recalculated?\nAt the moment editing the cells by hand keeps records together but using blank down leads to a new set of records being calculated (which results in the outcome here).\nThere is definitely some inconsistency in what happens with Record calculation right now - undo/redo does not seem to guarantee getting the same Record calculation as previously at the same step. In terms of a practical and quick solution in terms of working with the data, the way I approach this type of data is given the CSV:\nColumn 1,Column 2\nParis,Male\nParis,Male\nParis,Male\nParis,Female\nBerlin,Female\nBerlin,Female\nBerlin,Female\nLondon,Male\nLondon,Male\nI would apply the transforms:\n[\n  {\n    \"op\": \"core/blank-down\",\n    \"description\": \"Blank down cells in column Column 1\",\n    \"engineConfig\": {\n      \"mode\": \"record-based\",\n      \"facets\": []\n    },\n    \"columnName\": \"Column 1\"\n  },\n  {\n    \"op\": \"core/multivalued-cell-join\",\n    \"description\": \"Join multi-valued cells in column Column 2\",\n    \"columnName\": \"Column 2\",\n    \"keyColumnName\": \"Column 1\",\n    \"separator\": \"|\"\n  },\n  {\n    \"op\": \"core/text-transform\",\n    \"description\": \"Text transform on cells in column Column 2 using expression grel:value.split(\\\"|\\\").uniques().join(\\\"|\\\")\",\n    \"engineConfig\": {\n      \"mode\": \"record-based\",\n      \"facets\": []\n    },\n    \"columnName\": \"Column 2\",\n    \"expression\": \"grel:value.split(\\\"|\\\").uniques().join(\\\"|\\\")\",\n    \"onError\": \"keep-original\",\n    \"repeat\": false,\n    \"repeatCount\": 10\n  },\n  {\n    \"op\": \"core/multivalued-cell-split\",\n    \"description\": \"Split multi-valued cells in column Column 2\",\n    \"columnName\": \"Column 2\",\n    \"keyColumnName\": \"Column 1\",\n    \"mode\": \"separator\",\n    \"separator\": \"|\",\n    \"regex\": false\n  }\n]\nWhich results in\n\n. @thadguidry thanks for testing. I think my expectation is that using smartSplit on:\nrecord|||Alfredo Santamaria 2013\nshould return:\n[ \"record\", \"\", \"\", \"Alfredo Santamaria 2013\" ]\nThe three pipes between 'record' and 'Alfredo' translate to two fields - so e.g.\nrecord| 1 | 2 |Alfredo Santamaria 2013 -> [ \"record\", \"1\", \"2\", \"Alfredo Santamaria 2013\" ]\nIs your expectation different?\n. This PR is pretty stale by now. I'm not really sure how we should progress this. I'm going to close this right now - if we want to pick this up we probably need some more thought about exactly what we want from that behaviour. Tested with #1597 and this seems to have fixed this particular problem. This behaviour is controlled in /main/src/com/google/refine/grel/ast/OperatorCallExpr.java in the evaluate method. Updated to correct the expected behaviour as I had previously mis-represented the outcome of toString(null). Note that @thadguidry describes this behaviour as correct in #820 . OK - closing this then as it is preferred behaviour. I think what you'd need to be able to do for this to work is to essentially be able to set the:\n\nidentifierSpace to the correct URL for the service from which the IDs are derived (e.g. https://wikidata.org/wiki/)\ncell.recon.match.id\n\nfor each cell containing an identifier. The only way of doing this at the moment is via the Reconciliation process afaik.\nI suspect the issues in terms of adding such a function is that allowing you to set this directly avoids the check that the ID in your cell actually does exist on the remote service - which means you'd then have potential errors generated if you tried to push updates or pull down more information.\nI'd be interested to have @wetneb's opinion but just converting the cell content to a match without checking the service feels wrong to me. I feel it would be better for us to try to find a way of speeding up the reconciliation process where you already have the IDs - even with the need to check the remote service it feels like it should be quicker to check 'does this ID exist'.\nJust my initial thoughts. Checked and I see the same behaviour. I'm guessing that until you restart OR the value is cached?. Tested and the behaviour is the same on 2.8 and latest build - so looks like this is a bug introduced in 2.8 but not noticed until now. I don't think that feature was in 2.7. My guess is that it is something to do with the history writer on that feature not writing dates to the history correctly - but I'm about to get on a plane for 9hrs so can't look in more detail now. @thadguidry @ettorerizza I'm inclined to change the tests \nAssert.assertEquals(invoke(\"toString\", CalendarParser.parse(inputDate), \"yyyy-MM-dd hh:mm:ss\"), \"2013-06-01 12:00:00\");\nAssert.assertEquals(invoke(\"toString\", CalendarParser.parse(inputDateTime), \"yyyy-MM-dd hh:mm:ss\"),\"2013-06-01 01:12:11\");\nAs these current convert to midday and 1AM, rather than midnight and 1PM which is what I'd really expect. This is because the current behaviour in 2.8 does this as well.\nDo you agree we should change this behaviour rather than keeping backward compatibility in this case?. But shouldn't:\n[date 2013-06-01 13:12:11].toString(\"yyyy-MM-dd hh:mm:ss\") -> \"2013-06-01 13:12:11\"\n? \nIn 2.8 it goes to 2013-06-01 01:12:11\nThat doesn't seem right to me. @thadguidry doh! Thank you :). Thanks @jackyq2015 - closing this now. This problem also occurs when choosing Reconcile->Actions->Create one new item for similar cells. This problem also occurs when using the option to:\n\"Search for match\"\nIn the pop-up Selecting \"Match other cells with same content\" \nClicking 'New item' button. I think this is essentially any time \"recon-judge-similar-cells\" is called with parameter shareNewTopics: true. Hmm - I think I may have tested on a version without your #1568 fix in place - at least I've double checked I'm working from the most up-to-date version of Master and now I'm not seeing the issue.\nSorry. I'd wondered about the situation where you did a reconcile when you already had a filter on - would the cell then lack the identifierSpace?\nBut agree about the general principle. I don't think it is unusual for authors/contributors to be in the README file - but I'm not against having a separate CONTRIBUTORS file.. For someone checking out the master branch they would need to build before they could run. This is probably more information than we want in a README I think. We have a whole wiki page on it! https://github.com/OpenRefine/OpenRefine/wiki/Building-OpenRefine-From-Source\nI'm sure we could improve this - but I think it would make more sense for the README to:\n1) Point at how to download the current built version of OR from the usual places\n2) Point at the Wiki if someone wants to build from a branch (we could consider improving the wiki page for this at the same time). I'm OK with something in the README about running quickly as long as there is a clear link to the full directions and pre-requisites are mentioned.\nI think you'd need to run ./refine clean before ./refine - I get errors if I just run ./refine without cleaning first (not sure if that is something we could tidy up somewhere?). I get errors like this\n```\nOpenRefine-master >>cp -R ~/Sites/ostephens_OpenRefine/* .\nOpenRefine-master >>./refine\nYou have 770M of free memory.\nYour current configuration is set to use 1400M of memory.\nOpenRefine can run better when given more memory. Read our FAQ on how to allocate more memory here:\nhttps://github.com/OpenRefine/OpenRefine/wiki/FAQ:-Allocate-More-Memory\nStarting OpenRefine at 'http://127.0.0.1:3333/'\n17:16:58.181 [            refine_server] Starting Server bound to '127.0.0.1:3333' (0ms)\n17:16:58.182 [            refine_server] refine.memory size: 1400M JVM Max heap: 1407188992 (1ms)\n17:16:58.195 [            refine_server] Initializing context: '/' from '/Users/damyantiandowen/Downloads/OpenRefine-master/main/webapp' (13ms)\n17:16:58.251 [            refine_server] Starting autoreloading scanner...  (56ms)\n17:16:58.743 [                   refine] Starting OpenRefine 2.8 [TRUNK]... (492ms)\n17:16:59.375 [       FileProjectManager] Failed to recover project in directory 2004041557762.project (632ms)\n17:16:59.376 [       FileProjectManager] Failed to recover project in directory 1722464916959.project (1ms)\n17:16:59.376 [       FileProjectManager] Failed to recover project in directory 2026918261293.project (0ms)\n17:16:59.377 [       FileProjectManager] Failed to recover project in directory 1935972566663.project (1ms)\nException in thread \"main\" java.lang.NoClassDefFoundError: org/jrdf/graph/ObjectNode\n    at java.lang.Class.getDeclaredMethods0(Native Method)\n    at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)\n    at java.lang.Class.privateGetPublicMethods(Class.java:2902)\n    at java.lang.Class.getMethods(Class.java:1615)\n    at org.mozilla.javascript.JavaMembers.discoverAccessibleMethods(JavaMembers.java:380)\n    at org.mozilla.javascript.JavaMembers.discoverAccessibleMethods(JavaMembers.java:335)\n    at org.mozilla.javascript.JavaMembers.reflect(JavaMembers.java:450)\n    at org.mozilla.javascript.JavaMembers.(JavaMembers.java:76)\n    at org.mozilla.javascript.JavaMembers.lookupClass(JavaMembers.java:838)\n    at org.mozilla.javascript.NativeJavaClass.initMembers(NativeJavaClass.java:84)\n    at org.mozilla.javascript.NativeJavaClass.(NativeJavaClass.java:78)\n    at org.mozilla.javascript.NativeJavaPackage.getPkgProperty(NativeJavaPackage.java:164)\n    at org.mozilla.javascript.NativeJavaPackage.get(NativeJavaPackage.java:114)\n    at org.mozilla.javascript.ScriptableObject.getProperty(ScriptableObject.java:1617)\n    at org.mozilla.javascript.ScriptRuntime.getObjectProp(ScriptRuntime.java:1437)\n    at org.mozilla.javascript.ScriptRuntime.getObjectProp(ScriptRuntime.java:1423)\n    at org.mozilla.javascript.gen.c2._c3(file:/Users/damyantiandowen/Downloads/OpenRefine-master/main/webapp/modules/core/MOD-INF/controller.js:214)\n    at org.mozilla.javascript.gen.c2._c4(file:/Users/damyantiandowen/Downloads/OpenRefine-master/main/webapp/modules/core/MOD-INF/controller.js:314)\n    at org.mozilla.javascript.gen.c2.call(file:/Users/damyantiandowen/Downloads/OpenRefine-master/main/webapp/modules/core/MOD-INF/controller.js)\n    at org.mozilla.javascript.ContextFactory.doTopCall(ContextFactory.java:398)\n    at org.mozilla.javascript.ScriptRuntime.doTopCall(ScriptRuntime.java:3065)\n    at org.mozilla.javascript.gen.c2.call(file:/Users/damyantiandowen/Downloads/OpenRefine-master/main/webapp/modules/core/MOD-INF/controller.js)\n    at edu.mit.simile.butterfly.ButterflyModuleImpl.scriptInit(ButterflyModuleImpl.java:636)\n    at edu.mit.simile.butterfly.ButterflyModuleImpl.init(ButterflyModuleImpl.java:94)\n    at edu.mit.simile.butterfly.Butterfly.initializeModule(Butterfly.java:476)\n    at edu.mit.simile.butterfly.Butterfly.configure(Butterfly.java:451)\n    at edu.mit.simile.butterfly.Butterfly.init(Butterfly.java:308)\n    at org.mortbay.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:440)\n    at org.mortbay.jetty.servlet.ServletHolder.doStart(ServletHolder.java:263)\n    at com.google.refine.RefineServer.configure(Refine.java:296)\n    at com.google.refine.RefineServer.init(Refine.java:208)\n    at com.google.refine.Refine.init(Refine.java:114)\n    at com.google.refine.Refine.main(Refine.java:108)\nCaused by: java.lang.ClassNotFoundException: org.jrdf.graph.ObjectNode\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n    at org.mortbay.jetty.webapp.WebAppClassLoader.loadClass(WebAppClassLoader.java:401)\n    at org.mortbay.jetty.webapp.WebAppClassLoader.loadClass(WebAppClassLoader.java:363)\n    ... 33 more\n```. This is on macOS High Sierra.\nIf I run clean first:\n```\nOpenRefine-master >>./refine clean\nYou have 1293M of free memory.\nYour current configuration is set to use 1400M of memory.\nOpenRefine can run better when given more memory. Read our FAQ on how to allocate more memory here:\nhttps://github.com/OpenRefine/OpenRefine/wiki/FAQ:-Allocate-More-Memory\nBuildfile: /Users/damyantiandowen/Downloads/OpenRefine-master/build.xml\nclean_extensions:\nclean:\n     [echo] cleaning extensions\nclean:\n   [delete] Deleting directory /Users/damyantiandowen/Downloads/OpenRefine-master/extensions/sample/module/MOD-INF/classes\nclean:\n   [delete] Deleting directory /Users/damyantiandowen/Downloads/OpenRefine-master/extensions/jython/module/MOD-INF/classes\n   [delete] Deleting directory /Users/damyantiandowen/Downloads/OpenRefine-master/extensions/jython/tests/build\nclean:\n   [delete] Deleting directory /Users/damyantiandowen/Downloads/OpenRefine-master/extensions/gdata/module/MOD-INF/classes\nclean:\n   [delete] Deleting directory /Users/damyantiandowen/Downloads/OpenRefine-master/extensions/pc-axis/module/MOD-INF/classes\nclean:\n   [delete] Deleting directory /Users/damyantiandowen/Downloads/OpenRefine-master/extensions/database/module/MOD-INF/classes\n   [delete] Deleting directory /Users/damyantiandowen/Downloads/OpenRefine-master/extensions/database/test-out\nclean:\n   [delete] Deleting directory /Users/damyantiandowen/Downloads/OpenRefine-master/extensions/wikidata/module/MOD-INF/classes\nclean:\n   [delete] Deleting directory /Users/damyantiandowen/Downloads/OpenRefine-master/server/classes\n   [delete] Deleting directory /Users/damyantiandowen/Downloads/OpenRefine-master/main/webapp/WEB-INF/classes\n   [delete] Deleting directory /Users/damyantiandowen/Downloads/OpenRefine-master/main/tests/server/classes\nBUILD SUCCESSFUL\nTotal time: 0 seconds\nOpenRefine-master >>./refine\nYou have 1248M of free memory.\nYour current configuration is set to use 1400M of memory.\nOpenRefine can run better when given more memory. Read our FAQ on how to allocate more memory here:\nhttps://github.com/OpenRefine/OpenRefine/wiki/FAQ:-Allocate-More-Memory\nBuildfile: /Users/damyantiandowen/Downloads/OpenRefine-master/build.xml\nbuild_server:\n    [mkdir] Created dir: /Users/damyantiandowen/Downloads/OpenRefine-master/server/classes\n    [javac] Compiling 4 source files to /Users/damyantiandowen/Downloads/OpenRefine-master/server/classes\n     [copy] Copying 1 file to /Users/damyantiandowen/Downloads/OpenRefine-master/server/classes\nbuild_webapp:\n    [mkdir] Created dir: /Users/damyantiandowen/Downloads/OpenRefine-master/main/webapp/WEB-INF/classes\n    [javac] Compiling 487 source files to /Users/damyantiandowen/Downloads/OpenRefine-master/main/webapp/WEB-INF/classes\n    [javac] Note: Some input files use or override a deprecated API.\n    [javac] Note: Recompile with -Xlint:deprecation for details.\n    [javac] Note: Some input files use unchecked or unsafe operations.\n    [javac] Note: Recompile with -Xlint:unchecked for details.\n     [copy] Copying 1 file to /Users/damyantiandowen/Downloads/OpenRefine-master/main/webapp/WEB-INF/classes\n     [copy] Copying 1 file to /Users/damyantiandowen/Downloads/OpenRefine-master/main/webapp/WEB-INF/classes/schemas\n     [copy] Copying 1 file to /Users/damyantiandowen/Downloads/OpenRefine-master/main/webapp/WEB-INF/classes/schemas\n     [copy] Copying 1 file to /Users/damyantiandowen/Downloads/OpenRefine-master/main/webapp/WEB-INF/classes\nbuild_extensions:\nbuild:\n     [echo] Building extensions\nbuild_java:\n    [mkdir] Created dir: /Users/damyantiandowen/Downloads/OpenRefine-master/extensions/sample/module/MOD-INF/classes\n    [javac] Compiling 1 source file to /Users/damyantiandowen/Downloads/OpenRefine-master/extensions/sample/module/MOD-INF/classes\nbuild:\n    [mkdir] Created dir: /Users/damyantiandowen/Downloads/OpenRefine-master/extensions/jython/tests/build\nbuild_java:\n    [mkdir] Created dir: /Users/damyantiandowen/Downloads/OpenRefine-master/extensions/jython/module/MOD-INF/classes\n    [javac] Compiling 3 source files to /Users/damyantiandowen/Downloads/OpenRefine-master/extensions/jython/module/MOD-INF/classes\nbuild:\nbuild_java:\n    [mkdir] Created dir: /Users/damyantiandowen/Downloads/OpenRefine-master/extensions/gdata/module/MOD-INF/classes\n    [javac] Compiling 10 source files to /Users/damyantiandowen/Downloads/OpenRefine-master/extensions/gdata/module/MOD-INF/classes\nbuild:\nbuild_java:\n    [mkdir] Created dir: /Users/damyantiandowen/Downloads/OpenRefine-master/extensions/pc-axis/module/MOD-INF/classes\n    [javac] Compiling 2 source files to /Users/damyantiandowen/Downloads/OpenRefine-master/extensions/pc-axis/module/MOD-INF/classes\nbuild:\n    [mkdir] Created dir: /Users/damyantiandowen/Downloads/OpenRefine-master/extensions/database/test-out/build\nbuild_java:\n    [mkdir] Created dir: /Users/damyantiandowen/Downloads/OpenRefine-master/extensions/database/module/MOD-INF/classes\n    [javac] Compiling 29 source files to /Users/damyantiandowen/Downloads/OpenRefine-master/extensions/database/module/MOD-INF/classes\nbuild:\nbuild_java:\n    [mkdir] Created dir: /Users/damyantiandowen/Downloads/OpenRefine-master/extensions/wikidata/module/MOD-INF/classes\n    [javac] Compiling 77 source files to /Users/damyantiandowen/Downloads/OpenRefine-master/extensions/wikidata/module/MOD-INF/classes\nbuild:\nbuild:\nBUILD SUCCESSFUL\nTotal time: 6 seconds\nStarting OpenRefine at 'http://127.0.0.1:3333/'\n17:18:43.665 [            refine_server] Starting Server bound to '127.0.0.1:3333' (0ms)\n17:18:43.667 [            refine_server] refine.memory size: 1400M JVM Max heap: 1407188992 (2ms)\n17:18:43.676 [            refine_server] Initializing context: '/' from '/Users/damyantiandowen/Downloads/OpenRefine-master/main/webapp' (9ms)\n17:18:43.742 [            refine_server] Starting autoreloading scanner...  (66ms)\n17:18:44.452 [                   refine] Starting OpenRefine 2.8 [TRUNK]... (710ms)\n17:18:44.452 [                   refine] initializing FileProjectManager with dir (0ms)\n17:18:44.452 [                   refine] /Users/damyantiandowen/Library/Application Support/OpenRefine (0ms)\n17:18:45.106 [       FileProjectManager] Failed to recover project in directory 2004041557762.project (654ms)\n17:18:45.107 [       FileProjectManager] Failed to recover project in directory 1722464916959.project (1ms)\n17:18:45.107 [       FileProjectManager] Failed to recover project in directory 2026918261293.project (0ms)\n17:18:45.107 [       FileProjectManager] Failed to recover project in directory 1935972566663.project (0ms)\n17:18:45.959 [       database-extension] Initializing OpenRefine Database... (852ms)\n17:18:45.968 [       database-extension] Database Extension Mount point /extension/database/ [*] (9ms)\n17:18:45.968 [       database-extension] Registering Database Extension Commands...... (0ms)\n17:18:45.978 [       database-extension] Database Extension Command Registeration done!! (10ms)\n17:18:45.979 [       database-extension] Database Operations Registered successfully... (1ms)\n17:18:45.979 [       database-extension] Database Functions Registered successfully... (0ms)\n17:18:45.991 [       DatabaseModuleImpl]  Database Extension Module Initialization Completed!! (12ms)\n17:18:51.774 [       database-extension] receiving request for styles/pure.css (5783ms)\n17:18:51.774 [       database-extension] receiving method for GET (0ms)\n17:18:51.786 [                   refine] GET /command/core/get-version (12ms)\n17:18:51.789 [                   refine] GET /command/database/saved-connection (3ms)\n17:18:51.790 [       database-extension] receiving request for styles/bootstrap.css (1ms)\n17:18:51.790 [       database-extension] receiving method for GET (0ms)\n17:18:51.791 [       database-extension] receiving request for styles/database-import.less (1ms)\n17:18:51.791 [       database-extension] receiving method for GET (0ms)\n17:18:51.792 [       database-extension] receiving request for styles/jquery.contextMenu.css (1ms)\n17:18:51.792 [       database-extension] receiving method for GET (0ms)\n17:18:51.807 [                   refine] GET /command/core/get-preference (15ms)\n17:18:51.836 [                   refine] GET /command/core/get-all-project-tags (29ms)\n17:18:51.836 [                   refine] GET /command/core/get-all-project-metadata (0ms)\n17:18:51.855 [                   refine] GET /command/core/get-languages (19ms)\n17:18:51.903 [       database-extension] receiving request for scripts/index/database-import-form.html (48ms)\n17:18:51.904 [       database-extension] receiving method for GET (1ms)\n17:18:52.522 [                   refine] POST /command/core/load-language (618ms)\n17:18:52.558 [                   refine] POST /command/core/load-language (36ms)\n17:18:52.563 [                   refine] POST /command/core/load-language (5ms)\n17:18:52.673 [                   refine] POST /command/core/get-importing-configuration (110ms)\n```. @jackyq2015 you are right there is something odd going on here - if I clone directly from the OpenRefine repo it works fine. At the moment there is something odd if I try to do it from my fork - even though that is synced up to the OR master.\nAnyway - definitely something about my setup rather than a general problem. Thanks @thadguidry that seems to have done the trick. Already fixed via weblate. The \"XML Parsing Error: not well formed\" looks like it might be FF specific and not related to the specific import/option here. As soon as I try to do an import in FF (tested a couple of different methods) I get errors like:\nXML Parsing Error: not well-formed Location: http://127.0.0.1:3333/command/core/get-importing-job-status?jobID=3 Line Number 1, Column 1:\nIt might be related to this s/o Q&A: https://stackoverflow.com/questions/7642202/xml-parsing-error-not-well-formed-in-firefox-but-good-in-chrome. I can't recreate the console error right now - you say \"When I tried to view detail of the error\" - did you do something specific to try to view the detail? Just wondering what is happening to trigger that console error. From console:\n23:56:25.124 [                  command] Exception caught (1ms)\norg.json.JSONException: JSONArray[0] not a string.\n    at org.json.JSONArray.getString(JSONArray.java:405)\n    at com.google.refine.operations.cell.MassEditOperation.reconstructEdits(MassEditOperation.java:124)\n    at com.google.refine.commands.cell.MassEditCommand.createOperation(MassEditCommand.java:59)\n    at com.google.refine.commands.EngineDependentCommand.doPost(EngineDependentCommand.java:78)\n    at com.google.refine.RefineServlet.service(RefineServlet.java:178)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n    at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n    at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n    at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n    at org.mortbay.jetty.Server.handle(Server.java:326)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:938)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745). This works in 2.8\nIn both 2.8 and 3.0 the same JSON message is passed from the front end:\nOR 3.0 mass edit number:\ncolumnName: Column 1\nexpression: value\nedits: [{\"from\":[2],\"to\":\"2\",\"type\":\"text\"}]\nengine: {\"facets\":[],\"mode\":\"row-based\"}\nOR 2.8 mass edit number:\ncolumnName: Column 1\nexpression: value\nedits: [{\"from\":[2],\"to\":\"2\",\"type\":\"text\"}]\nengine: {\"facets\":[],\"mode\":\"row-based\"}. None of:\n com.google.refine.operations.cell.MassEditOperation\n com.google.refine.commands.cell.MassEditCommand\n* com.google.refine.commands.EngineDependentCommand\nchanged between 2.8 and 3.0 beta. Through testing, I think the issue was introduced in \n1398 (commit c4b0ff6bea1278032855c56bb8c6393619c42acd)\nwhich was a big change. Not sure what it was in this commit that caused the problem but it gives a place to start the investigation. My guess is the update of the json jar used:\njson-20100208.jar  -> json-20160810.jar\nis where the problem lies. @jackyq2015 any ideas?. OK - this commit https://github.com/stleary/JSON-java/commit/f4cb14728f13629972a0ea76bb3dc0705a735fa8 changes the behaviour of JSONArray.getString() to throw an exception where it doesn't find a string. Previously it tried to convert any object to a string\nThis is the problem (and error message) we are seeing here. So basically com.google.refine.operations.cell.MassEditOperation.reconstructEdits needs to be smarter - my first instinct is we will need to use getJSONObject instead of getString and then decide what to do with the retrieved object, but not looked closely yet. Note from #1639 this issue also occurs if you create a text facet on a column containing numbers/booleans and attempt to do an edit on a value in the facet. Just an update - I'm working on this and work in progress is at https://github.com/ostephens/OpenRefine/tree/mass-edit-fix\ntl/dr fix #1631 is relatively straightforward, but I'm trying to make tests and fix #1632, #180 and #332 at the same time which is proving more complicated and taking more time. Fixed by #1642. Just an update - I'm working on this and work in progress is at https://github.com/ostephens/OpenRefine/tree/mass-edit-fix\ntl/dr fix #1631 is relatively straightforward, but I'm trying to make tests and fix #1632, #180 and #332 at the same time which is proving more complicated and taking more time. @dbutlerdb \nThe short explanation is that the behaviour of 'toString' on a null cell is deliberately set to result in the string \"null\".\nThe short solution:\nUse the 'Facet on blank' option on the column to remove all rows with blank and/or null values before applying the 'toString' transformation on the column\nMore information: \nprobably worth giving some background on this first - apologies if some/all of this you may know or is obvious\nEach cell in an OpenRefine project contains a value of a specific 'type'. The types supported are:\n\nString\nNumber\nDate\nBoolean\nerror\nnull\n\nThe first four are different types of data. The data type will determine what you can do with the value. For example, if you want to sum two values by adding them together, they must both be of Number type.\nerror is when the cell is storing an error generated during a transformation in OpenRefine\nnull is a special value which basically means 'this cell has no value' (see https://techterms.com/definition/null for a slightly fuller definition).\nYou can convert between types within some limits (e.g. you can't turn the string \"asdfsd\" into a Date or a Number, because OpenRefine has no way of knowing how to make a Date or Number from a random collection of letters).\nThe 'toString' option will work on any of the value types and gives a String version of the value. This works in different ways depending on what type you are converting from. The deliberate decision in OpenRefine is that using 'toString' on null results in the string \"null\"\nWhen you see an empty cell in an OpenRefine project there are broadly three possibilities:\n\nthe cell contains the null value\nthe cell contains a string value of zero length\nthe cell contains a string value that only contains whitespace\n\nWhen you are using the toString() function, any null cells will get coverted to \"null\"\nIn OpenRefine 3.0 upwards we've introduced some more options for handling the difference between null and zero length strings, although there are probably more improvements to be made, and we probably can't get it right for everyone at the same time.\nThese improvements include:\n\nability to view where there are null values in your project by using the menu All->View->Show/Hide null values in cells\noption to \"facet by null\" as well as \"facet by blank\" (the latter was previously the only option and tests for both null and empty strings) (in the \"Customised Facets\" drop down menu)\noption to convert cells \"to null\" or \"to empty string\" (in the \"Common Transforms drop down menu)\n\n. Hi @dbutlerdb \nI'm not sure I can completely explain this. I have to admit I struggle to see the advantage of converting a null into the string \"null\" rather than an empty string (which is the behaviour that occurs if you use '+' with a null)\nThis change was introduced 5 years ago as part of a fix for another issue #783. I can't really see quite why this change decided to convert null to the string \"null\". I think there is a strong argument for reverting to the previous behaviour of converting it to a blank string.\n@thadguidry ?\nAs a slight aside it is important to state that it is not the case that \"the default action when converting a numeric cell with no values into a string is to add 'null' into the cell.\" The 'cell' itself is not numeric or non-numeric - it is the value in the cell that is numeric or non-numeric. An empty cell cannot be 'numeric'.\nThe default behaviour is that a cell containing the null value converts to the string \"null\" if you use 'toString'. As I say above, I can't really explain this behaviour, and I'm not defending it but I just wanted to be clear how this works.\n. @thadguidry I'm suggesting:\nnull.toString() -> \"\"\ninstead of \nnull.toString() -> \"null\"\nThis would still allow a null to convert to a string\nI think it potentially hurts in that it seems like unexpected behaviour (as found by @dbutlerdb here) that using 'toString()' results in a non-zero length string appearing where there wasn't one previously - it means the user has to be very careful to exclude null from their 'toString' function if they don't want to end up with the word \"null\" in their output.\nI also think it would be nice if:\n\"example\"+null\nand\n\"example\"+null.toString()\ngave the same result - as they both convert null to a string, but this is not currently the case:\n\"example\"+null -> \"example\"\n\"example\"+null.toString() -> \"examplenull\"\nSo my argument would be that having\nnull.toString() -> \"\"\nwould be more intuitive for users and be more consistent in overall OR behaviour. Fixed by #1650 . Thanks to @dbutlerdb for raising this - new behaviour will be in 3.0. I don\u2019t have a solution currently but I run the Linux version of OpenRefine on my mac. You could try that to see if it works. \nDownload the Linux version, unzip the folder, in a terminal change to the directory which contains OpenRefine you\u2019ve just  downloaded and type:\n./refine. Downloaded and ran successfully on macOS High Sierra. My list of projects found as usual\n@yaeln can you check what version of Java you have installed?\nrun java -version on the command line and paste the results\n(I'm clutching at straws a bit here but just trying to think of things that might be difference across setups). Mine is slightly more up to date, but I'd be a bit surprised if that is the reason for any different behaviour:\n\njava version \"1.8.0_72\"\nJava(TM) SE Runtime Environment (build 1.8.0_72-b15)\nJava HotSpot(TM) 64-Bit Server VM (build 25.72-b15, mixed mode). So from the log do we assume that:\nException in thread \"main\" java.lang.LinkageError: loader constraint violation: loader (instance of edu/mit/simile/butterfly/ButterflyClassLoader) previously initiated loading for a different type with name \"org/slf4j/Logger\"\n\nIs related to this problem?\nI definitely don't see that in mine or when I run that version shared by @yaeln \n. The lines\n+ '[' -z '' ']'\n++ /usr/libexec/java_home\n+ export\nJAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_25.jdk/Contents/Home\n+ JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_25.jdk/Contents/Home\n+ '[' /Library/Java/JavaVirtualMachines/jdk1.8.0_25.jdk/Contents/Home ']'\nIndicate your JAVA_HOME isn't set, but the script picks it up anyway from /usr/libexec/java_home so that can't be the problem (I tested unsetting JAVA_HOME on my own install and I got the same output as your script at this stage, but no other problems). This is probably obvious, but I've used a custom text facet for cell.recon.namespace previously to find the rows that had issues and debug from there\n(and so far I think it has always been an issue with how the 'new' entities are being declared as new - it feels like we might have too many ways of doing this!). Yes this is a duplicate of #1631 - closing as duplicate and will cross reference from there that it also effects editing from a text facet. I'm going to make a cleaner PR - so closing this one. The fix for #180 treats empty strings and nulls as the same. I think that's OK for now (and is also how the mass edit from a text facet across the same values would work).\nWe may want to revisit this later to at least offer the option to handle empty string and null separately, but that's beyond the scope of this fix. @jackyq2015 are you OK with this? I'd like to get it merged if possible. I'll merge this in a day if there are no further comments. @thadguidry this is on Win 10, not a Mac. This has some extraneous commits in it - going to close and try a tidier PR. I guess we need to discuss exactly how we are going to approach this? Can it be broken down into smaller tasks that less experienced developers (like me!) can tackle?. It looks like the problem is that the lang string \"custom-tab-exp\" has been used for multiple purposes. It should only really be applied to the Custom Tabular Exporter pop-up custom-tabular-exporter-dialog.js\nHowever it has also been applied in:\n\nexpression-column-dialog.js\nsql-exporter-dialog.js\n\nI think these two latter places need two new language strings defining and using . See also #1662 for discussion of list facet behaviour with non-strings. Following discussion no #1662 there may be some further changes to make before this is ready to merge. \nComments on this PR and on #1662 would be welcome. The issue is that the mass edit of dates relies on a string representation of the date. If different string representation of the dates are used in different places then the mass edit doesn't work.\nThe way List facets are built also relies on a string representation of the date, which means that making a change to the date format and the resulting string representations of those dates causes knock on problems. In particular both the 'v' and 'l' values in the facet are used creating the List facet and list facet counts, which means you can't just change the 'v' and leave the 'l' (label) alone. These values were previously created through two different methods with different outcomes - which led to various aspects of the facet failing.\nMy argument for changing the default format of StringUtils toString is so we consistently do the same thing when we transform a date to a string. I don't have strong opinions on the conciseness or human readability of the date format - what I've done is bring this one outlier in line with everything else. Also having multiple different ways of transforming a date to a string seems like a bad idea overall - it would be great to try to do this in a standard way across OR - I think this is a step towards that (as it passes the actual transform of the date to ParsingUtilities.dateToString() - it would be nice for this to be the way we transform a date to a string IMO)\nHowever, it should be noted that following discussion at #1662 this may become irrelevant as I'm now working on a change to this PR which means that non-strings only appear as a grouped set in List facets. This would remove the issue of the default transform of a date to a string completely from the Facet, and so in that scenario the previous default behaviour in StringUtils could be changed back. I'd be against that because it seems inconsistent with the rest of the date behaviour in OpenRefine (in particular the display of dates in both the transformation preview and the data grid is different to what you actually get if you do date.toString() - that seems inconsistent to me).. @jackyq2015 thanks for this - this PR doesn't address the issue of the selection of a date in a text facet - see new PR #1666 and related issue #1662.\nI think these issues have been in OR for some time to a greater or lesser extent - see @thadguidry comments on #1662 . Thanks @thadguidry.\nI actually like that the Text facet includes non-string values - it is sometimes very convenient (and I don't find the UI on the number and timeline facets that good).\nI think my preference is (3) - so actually what we have is a proper 'list' facet - essentially it would work like the current text facet in general, but handle non-string values more intelligently. I'll have a play and maybe post a proposed approach here with some mock ups of how it would work and see if others have views as well.. @ettorerizza a type facet would allow you to filter the data set by type of course, and a type facet seems like a good idea. \nHowever I think we still need to look at the behaviour of the text facet. To \u201ccount\u201d four values, and mass edit four values but only select 2 of those values makes no sense to me\nI think we need the text/list facet to treat non strings in a consistent way . @thadguidry I'm trying to understand what is the most sensible consistent behaviour here that we should aim for. It seems from what you say, you would like to see the values treated consistently as strings within the context of a text facet? So if we have data:\n[date 20170101T00:00:00Z]\n20170101T00:00:00Z\nThen this should result in a facet:\n20170101T00:00:00Z   (2)\nAnd selecting the value in the facet would filter to both rows. Is that correct? Have I understood your preference for how it should work?. Thanks @thadguidry that's very clear. What you are suggesting:\n\nChange the Text Facet, so that Users can only see String Datatype values.\nshow at the top of the Text Facet something like \"non Text Rows\" (like we do with Blank and Null) and a Count next to it\n\nThis definitely makes sense to me and was the approach I was trying to describe in my Number 2 above:\n\nSimilar to how nulls/empty strings are handled in text facets, we could have a bucket facet value for \"dates\", \"numbers\", \"booleans\" which would allow the user to select the set of boolean values but not see counts of true vs false (a further boolean facet would be needed to see that)\n\nSo with this method\n[date 20170101T00:00:00Z]\n20170101T00:00:00Z\nWould give the facet\n(dates)  (1)\n20170101T00:00:00Z  (1)\nI still wonder if there is a role for a mixed type 'list' facet (which is what I was suggesting in (3) above) where\n[date 20170101T00:00:00Z]\n20170101T00:00:00Z\nWould give the facet\n[date 20170101T00:00:00Z]  (1)\n20170101T00:00:00Z  (1)\nbut I think you are probably right that we should keep the Text facet as Text, and then if there should be a mixed type List facet we can look at that as a separate issue\nThanks\n. Thanks for this @jackyq2015. I think the issue of having a data type at column level is different to the question of how to facet on a column which contains a mixed set of data types. It's the latter question that I think we are trying to resolve here.\nI understand what you mean when you say \n\nGrouping the cell by data type also looks weird\n\nBut I'm not sure we've found an approach yet that doesn't have some downside. Based on the discussion in this issue, I see four choices for dealing with the data of different types in a List face:\n\nSimilarly to how timeline/number facets treat other values we could simply not include the dates, booleans, numbers in the facet and have a checkbox as to whether they are included in the filter or not\nSimilar to how nulls/empty strings are handled in text facets, we could have a bucket facet value for \"dates\", \"numbers\", \"booleans\" which would allow the user to select the set of boolean values but not see counts of true vs false (a further boolean facet would be needed to see that)\nDates, Booleans and Numbers could be included in the facet but as separate values in the facet - so all boolean true are grouped and counted separately to all string \"true\"\nDates, Booleans and Numbers could be converted to strings for the purposes of faceting and grouped with the equivalent string value\n\nPR #1666 implements (2)\n(4) is the closest to the current behaviour, but I think doesn't work well for dates because of the variety of ways a date can be parsed into a string. It could also be a problem for numbers and booleans in some cases, although  Booleans and Numbers are different to dates because:\n  a) They have native representations in JSON so it is easy to know if we have a Boolean or Number in our facet without any additional information\n  b) They convert to strings in a more consistent manner than dates (although Numbers can vary depending on whether they are LONGs or INTs)\nMy instinct is against (4) because it involves switching between data types - which seems to me risky behaviour and likely to lead the user astray.\nHowever, I can probably be persuaded that any of the four approaches described here is the right way to go. I implemented approach (2) in PR #1666 because it seemed to be the approach myself @thadguidry and @ettorerizza all felt was reasonable.\n@jackyq2015 which do you think would be the best approach out of the 4 described? Or is there another way of approaching this which I've not described that you would prefer?. @ettorerizza you need to add toString() to see the values:\n(row.index < 100).toString()\nwill get the result you want\n. In the medium term - I think adding a 'boolean facet' would be the correct solution to make this more straightforward. @wetneb I'm not sure I agree that scenario is a problem. If they make a list facet on a column they will see immediately how many boolean values are in there.\nI think the more problematic area is the one @ettorerizza demonstrates where the user wants an easy way to get a boolean true/false result in a facet. That used to be simpler than it is now. I'm happy to look at other approaches - but as always it is difficult to know where to strike the balance between giving the users choice and making those choices for them.\nI believe Boolean values already display in green as with dates and numbers. Agree with @wetneb \nThe previous behaviour of the text facet was problematic (as documented above). For example in the previous behaviour a cell with the text string 'true' and a boolean cell or result of 'true' would cluster together but selection did not work (documented above). Not to mention the issues with dates (above).\nI think a \"type\" facet would be helpful, but this isn't what the OpenRefine 3.1 behaviour delivers.\nThe OpenRefine 3.1 behaviour delivers a \"text\" (or string) facet - and as with \"number\" and \"date\" facets, it doesn't try to blindly convert non-text values into text values - that is left as something the user can choose to do if they want. I feel this brings to consistency to the facet behaviour, although I absolutely acknowledge that this change to behaviour is a breaking change and needs users to amend their previous practices.. Another approach I suggested above (option \"3\") when I listed the options was \n\nDates, Booleans and Numbers could be included in the facet but as separate values in the facet - so all boolean true are grouped and counted separately to all string \"true\"\n\nAt the time I said:\n\nI still wonder if there is a role for a mixed type 'list' facet (which is what I was suggesting in (3) above) where\n[date 20170101T00:00:00Z]\n20170101T00:00:00Z\nWould give the facet\n[date 20170101T00:00:00Z]  (1)\n20170101T00:00:00Z  (1)\nbut I think you [was referring to Thad] are probably right that we should keep the Text facet as Text, and then if there should be a mixed type List facet we can look at that as a separate issue\n\nSo this would be another option - have a \"List\" facet which mixes types in a single facet but treats them separately.. @thadguidry I guess that design is an ongoing discussion :)\nI'm good to add new facet types and if we create some enhancement issues for them this will put them on the list. \nI struggle more with the overall UI design - it's the kind of thing that's beyond my experience to start from scratch - although once there is a framework in place I think I'm pretty good at picking up how things work and can start extending/copying existing work. I'm not sure why we use a javascript alert for this. Would it make more sense to alert within the page where we display other messages (such as the version check, or the reconciliation progress info)?. Just noticed that this PR breaks \"Facet by blank\" functionality. I think it's an easy fix, but I need to do this before this PR is merged.\nIn fact it breaks any customised facet that relied on 'toString()' being implicit in the creation of the facet. This affects the following custom facets:\n\nDuplicates Facet\nFacet by error\nFacet by null\nFacet by empty string\nFacet by blank\n\nThey can all be easily rectified by adding a 'toString()' operation to the relevant GREL\nWorth noting that this could be a breaking change in terms of applying operation histories from previous versions of OpenRefine which don't require 'toString()' on such facets. Now fixed the issue with the custom facets:\n\nDuplicates Facet\nFacet by error\nFacet by null\nFacet by empty string\nFacet by blank\n\nThese all now explicitly convert toString to ensure the facet offers the appropriate choices to the user. @wetneb yes. As noted it potentially breaks reapplying transforms from previous versions of refine but @thadguiry has argued quite strongly to go ahead and I'm convinced. I think the desirable behaviour in this case would be to name columns based on the hierarchy of their heading columns. In this case I could see having the columns:\n\nRessort\nZuwendungsempfanger\nHaushalts-stelle(n)\nZuwendungszweck\nInstitutionnelle Zuwendungen Bremen - 2014\nInstitutionnelle Zuwendungen Bremen - 2015\nProject-forderungen Bremens - 2014\nProject-forderungen Bremens - 2015\nInstitutionelle Forderung / Projektforderung Dritter - 2014\nInstitutionelle Forderung / Projektforderung Dritter - 2015\nFinance-rungsart\nStadteil\n\nHowever, whether this is achievable based on the information available to us in the Excel file I don't know.\n@GiantCrocodile are you able to share an example Excel file with us?. Hi @wetneb I need to think about this. I deliberately wrote it that way to make it easily extensible by extensions and to stop arbitrary headers being pushed. I think it also deals with some issues in having different naming in the JSON to the header key (from memory I think there was an issue with valid characters around this).\nI need to take a look at the code again and try to remember all the reasons I did it that way!. So I think the problem for #1669 is that the current code (before this PR) submits the Authorisation header with an empty string. Any fix which stops this should resolve #1669 \nHaving reviewed the code, the original code was intended to stop the passing of arbitrary headers to end services - basically just sanitising end-user input. On the basis that this doesn't present much of a security risk (if at all), happy to simplify as @wetneb has here.. I think that still needs including to register the headers for use? This needs to happen somewhere . HttpHeadersSupport is also referenced in /main/src/com/google/refine/commands/project/GetModelsCommand.java\n. @jackyq2015 the HttpHeadersSupport class was originally meant to control which HTTP Headers were supported. It provides a registerHttpHeader method so that extensions can also register headers.\nIt uses the registerHttpHeader internally to register the ones we currently support and set default values.\nThe registered headers and default values are passed to the client via GetModelsCommand.java\nIn the client these are displayed in the Add Column from URL dialogue, and the user can change the values (e.g. set Authorization to \"Basic YWxhZGRpbjpvcGVuc2VzYW1l\") in that dialogue.\nIt is the values passed from the client that are used when making the http request which is used to put values into the column being added.\nHope that helps. This originated on the discussion list - see https://groups.google.com/d/msg/openrefine/gsmP9QOKuhQ/LGPpvuovBAAJ. @Gautamshahi how are you trying to do the reconciliation? Are you using the RDF Extension for this?\nWhat @thadguidry is saying above is that OpenRefine (without the RDF Extension) has no built in ability to reconcile against a SPARQL endpoint. However, the RDF Extension (if installed) adds this ability.\nIf you could provide information on:\nThe version of OpenRefine you are using\nThe version of the RDF extension you are using\nWhat SPARQL endpoint you are trying to reconcile against (if public)\nThis could all be helpful. However, please note that the OpenRefine team do not directly support the RDF extension, so if the issue is with the extension, you would need to report it to someone working on the extension. We currently would recommend using the RDF extension from https://github.com/stkenny/grefine-rdf-extension and reporting specific issues there.\nThe more information you can provide, the more likely we will be able to give a helpful answer.\nBest wishes\nOwen. I can reproduce this. I think it specifically happens when you delete the text from the Description field, keep the cursor in the field and drag the column into the schema without the cursor leaving the field (I'm guessing this leads the field not being updated and so being treated as empty when you try to save the schema?)\n\n. Do you have any extensions installed in Chrome? If so can you try turning these off and seeing if that helps? Or trying in another browser? . I think this request is subtly different to what has been implemented.\nThe current implementation does the following:\n\nAllows you to write a transformation\nAllow you to select the columns it applies to\nApplies that transformation to each column selected, using the column name as the key\n\nThis results in multiple items in the project history with one entry per column in the project. When applying this to other projects, the transformations will only be applied if the columns have exactly the same name as in the original project.\nThis request, as I understand it, is to allow the user to select to apply the transformation to 'All columns' which would iterate through all columns in the project and apply the transformation to each column regardless of name. This would create a single item in the History which was to 'apply transform to all columns' and would iterate through all columns in a project regardless of their name.\nThis would enable the transformation to be applied to future projects regardless of the column names\n. I think it would be an additional option on this screen:\n\nSo the existing \"Transform\" option under the 'All' menu would still be used, you'd just have another selection option on this screen \"Apply to all columns regardless of name\" (or something)\n. I think https://github.com/OpenRefine/OpenRefine/blob/9f7d5b878628c40d68a445b751fd50140d922e7d/main/webapp/modules/core/scripts/dialogs/expression-column-dialog.js#L34 needs changing as well and another new lang string needs creating for that to full fix #1657 ?. I have to admit that build systems are very blackbox to me - so I don't feel very qualified to have an opinion - but logically what you propose makes sense. @msaby The PR is automatically updated each time you update the branch that has been used to create the PR . Can you given any more information about the dataset? How many columns, overall file size, is it all rows or are you using records mode? All these things could affect speed.\nAlso - how many unique values are you expecting in the facet?. In another piece of s/w I\u2019ve written extensions for the extension was required to state a version compatibility which would be used by the main s/w to decide whether to load the extension or not. \nWould this be an approach we could take with OR?. Hi @msaby - thanks for this.\nI've done some quick testing, and it looks like adding support for import JSON-LD is trivial.\nWe could also easily add support for Turtle / *.ttl files\nI'm happy to work on adding this support\nI don't completely understand what you mean when you sa:\n\nFrom UX point of view, maybe we could rename the importer menu \"Semantic data\" with options : \"Semantic data (RDF/XML)\", \"Semantic data (JSON-LD)\", etc., to make it clear that it is not ordinary JSON and XML data?\n\nDo you mean just changing the labels on the import screen in the \"Parse data as\" section?\n\nI'm not so keen on using the term \"Semantic data\" here as that seems quite a general label. The approach so far is to use the file format, and my first thought is to keep to that approach. Have I misunderstood what you mean here? Could you give an example?\nThanks\n. Done! See #1783 . I think the issue is with https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/expr/functions/ToDate.java#L92\nThis line only triggered when you include the format to be used to parse the string to date as the first argument (because of an optional first argument of a boolean to treat the string as 'day first' or 'month first'). This line needs updating to account for the changes to the internal date format.\n@noelmas for the moment if you include the (meant to be optional) boolean argument you can then follow this by a format. However, you may find the boolean is sufficient for your purposes. true means \"treat as month first\", false means \"treat as day first\"\n\"12/04/2008\".toDate(true) -> [date 2008-12-04T00:00:00Z]\n\"12/04/2008\".toDate(false) -> [date 2008-04-12T00:00:00Z]\nif you provide both boolean and format, then the format will be used anyway:\n\"12/04/2008\".toDate(true,\"dd/MM/yyyy\") -> [date 2008-04-12T00:00:00Z]\n. Hmm - this is a bit more messy than I thought. I'm looking at refactoring ToDate.java a bit at the same time as fixing this.. I've finally made some progress with this! I've been trying to do a bit of refactoring on the toDate function so the code is easier to understand - I'll ask for feedback on that once I have a PR ready.\nHowever, as I've worked on this I've been wondering on whether the current behaviour of toDate needs changing, or at least what should work/not work when using it. What would be really useful is to collect examples of conversions people would like to see working, and then I can add them into the automated tests we run to make sure that toDate works as we expect. @msaby @ettorerizza @noelmas do you have examples that you want to see working?\nI'm especially interested in examples specifying locales, as my understanding of how the system should behave when different locales are specified is not great.\nBut any examples of starting strings with your expectation of how 'toDate' would convert them to a date would be useful\nFor reference, these are essentially the behaviours I've currently got tests for:\ntoDate() -> error (no string, number or date given to toDate)\ntoDate(null) -> error (no usable string, number or date given to toDate)\ntoDate(\"\") -> error (no usable string, number or date given to toDate)\ntoDate(1.0) -> error (no usable string, number or date given to toDate)\ntoDate(\"2012-03-01\",\"xxx\") -> error (no valid format specified to use when parsing to date)\ntoDate(\"2012-03-01\") -> 2012-03-01\ntoDate(01/03/2012\") -> 2012-01-03\ntoDate(\"01/03/2012\",true) -> 2012-03-01\ntoDate(\"01/03/2012\",false) -> 2012-03-01\ntoDate(\"01/03/2012\",\"dd/MM/yyyy\") -> 2012-03-01\ntoDate(\"2012-03-01\",\"yyyy-MM-dd\") -> 2012-03-01\ntoDate(\"2012-03-01\",\"MMM\",\"yyyy-MM-dd\") -> 2012-03-01\ntoDate(\"01/03/2012\",false, \"MMM\",\"yyyy-MM-dd\",\"MM/dd/yyyy\") -> 2012-03-01\ntoDate(01-\u516d\u6708-2013\",\"zh\",\"dd-MMM-yyyy\") -> 2013-06-01\ntoDate(\"2012-03-01\",\"XXX\") -> 2012-03-01 (ignore invalid date formats and locales)\ntoDate((long)2012) -> 2012-01-01 (treat numbers as years)\ntoDate([date 2012-03-01]) -> 2012-03-01 (leave dates as they are)\n. @orr721 are you able to check this behaviour is fixed in 3.1 - I think it should work OK, but if not we can work out what the problem is. @ancore @nsiebler-dm we'd really prefer to have a unit test which checks for the correct behaviour alongside this change - we always try to add tests when we change code because we have had a history of poor test coverage.\nIs there any chance tests could be added to this PR?. Related (possibly) it would be really helpful to be able to export/import schemas. On import if columns used in the schema were not present it would need to import the schema without those values mapped.\nThis would make it easy to share schemas with others making it easier for people to work towards a common data standard (at the moment I'm sharing whole projects with the schema setup, and others then have to copy my examples).. Possibly this should trim any non-printing characters rather than just spaces?. You maybe correct - I've seen issues with invisible characters in Wikidata labels (e.g. I had to do an edit to remove a unicode control character at d https://www.wikidata.org/w/index.php?title=Q42307623&direction=prev&oldid=744504811)  - but the issue wasn't introduced by an OpenRefine edit afaik. To address #1758 . @vineetharkut can you confirm which version of OpenRefine you are using? I've just tested this on OpenRefine 3.0 (the current version) and it seems to be working OK for me.\nIt would be good to know if this is just an issue in an earlier version of OpenRefine or if something else is happening. Thanks for the video. From that, it looks like there is a more general error than in particular using return value if value !=\"\" else \"Fill Empty Cell\" as I can see even just return value gives an internal error.\nWhat operating system are you on?\nCan you have a look at the command line window (on Windows or Linux - you won't see this on Mac) and see if there are any error messages in there?\nThanks. I've started some work on this\n@msaby would you be able to give some test cases for parsing XML that I could include as tests to make sure this works as desired? i.e. starting XML, some process and expected output. Thanks @msaby \nSo I think I can improve the code, but what I have working so far is:\nvalue.parseXml().select(\"foaf|Person\") ->\nforEach(value.parseXml().select(\"foaf|Person foaf|name\"),x,x.ownText()).join(\"|\") -> John Doe|H\u00e9lo\u00efse Dupont\nforEach(value.parseXml().select(\"foaf|Person head\"),x,x.ownText()).join(\"|\") -> head1|head2|head3\nvalue.parseXml().select(\"BODY\")[1].ownText() -> nice body\nvalue.parseXml().select(\"foaf|homepage\")[0].xmlAttr(\"rdf:resource\") -> http://www.example.com\nThis is close, but not identical to what you suggest, but it is close. The main difference is that you need  to use ownText() to get the text content of an XML element, and xmlAttr uses the colon after the namespace not the pipe (this is different to 'select' but seems to be how JSOUP handles it by default so I'm not inclined to change it unless there is a strong argument for changing)\nThis all seems reasonable and useful to me. I'll start trying to tidy up the code (basically at the moment I've duplicated a lot of code which I'd prefer not to duplicate and I need to fix that). In Jsoup it seems a pipe char is used when selecting elements with a namespace, but not attributes. Hence:\nvalue.parseXml().select(\"foaf|homepage\")[0].xmlAttr(\"rdf:resource\") -> http://www.example.com. I think it would be reasonable for LIMITs and OFFSETs to be set via a specific control if necessary rather than within a query (thinking this could make it easier to manage rather than having to parse LIMIT/OFFSET statements out of a user authored query). Because they could still use a LIMIT that is much higher than the 100 records that is set to keep the Preview screen responsive.\nMy view would be that it should behave as follows\nIf they set a limit <100 we should use that for both preview and full data import\nIf they set a limit >100 we should use 100 for preview, and the user set limit for the full data import\nFor OFFSET I think we should use a user set value (defaulting to zero as currently) for both preview and full import. Thanks for testing @thadguidry \nAgree we should test:\n\"02-02-01\".toDate() - > [date 2001-02-02T00:00:00Z]\nI will add that in.\nI don't understand:\n\"0002-0222-021\".toDate() -> [date 0222-02-21T00:00:00Z]\nIt does work with the current code, but I don't understand why that string be parsed to a valid date?\nI think dealing with pre-gregorian calendar dates and dates using AD/BC/BCE are new feature requests and so need new issues describing expected behaviour (btw once we've got this PR done I can look at adding the epoch format as discussed in #608). @wetneb thanks for the review. Haven't had time to check thoroughly, but my guess is that the change in behaviour is related to https://github.com/OpenRefine/OpenRefine/commit/f327b09a596d8723a2915fd53a4d38c1bfc36c40. Away from computer right now but will check later . OK - checking and downloading the latest branch directly as a zip file works and using ./refine leads to it building without a problem.\nMy problem seems to be when I'm working from my Git repo - copying the content of the repo and then trying to build from the copy\nThere is no ./server/target/lib directory in my Git repo - just ./server/target/test-classes  - which is empty\n. Yeah - I don't often push the master to https://github.com/ostephens/OpenRefine (I have done just now), but on my local repo I keep it up to date\nWhen I say \"copying the content of the repo\": I copy the Git repo to another directory to do test builds to avoid the built files ending up in my branch. If there is a better / different way of doing this part of the workflow then I'd definitely like to learn!\nTrying both git reset soft & hard still doesn't fix the problem. Ah - I see /server/classes and others all in Git ignore - so I'm guessing that there is something in there...\nYep - removing /server/classes from my end sorts the problem. So currently having any /server/classes directory stops the build when just using ./refine even if /server/classes is empty. Not sure it is worth changing this behaviour tbh. Thanks @wetneb  - that's a good hint (I was using cp). I've submitted a request to the CVE database for CVE-2018-19859 to reflect this earlier fix to the vulnerability. It looks like the issue here is that currently:\n\"string\"+null -> null\nRather than\n\"string\"+null -> \"string\"\nIf you use toString() on the null then the null gets converted to an empty string and thus you get the desired behaviour:\n`\"string\"+null.toString() -> \"string\"\nI'm not sure when the \"string\"+null -> null behaviour started, as in #1635 I documented that \n\"example\"+null -> \"example\"\nSo this seems to be something that has changed since June.\nI'll investigate further\n. It looks like I was incorrect when I documented the behaviour of \"string\"+null in #1635. Testing back to 2.7 this results in null, and the code that controls this hasn't changed since 2011\nI'm slightly unsure what the \"right\" behaviour should be when you attempt to add null to:\n\nA string\nA number\nA null\nA date\n\nHappy to make changes here but we could do with agreeing behaviour\n@jenyoung @wetneb @ettorerizza @thadguidry  (feel free to add others). @jenyoung just to make sure you know - you don't have to change the nulls to empty strings in the project overall, just use GREL like\nvalue.toString() + cells[\"worktype[1]'].value.toString() \nBy adding toString() that will make sure any null values are treated as empty strings when concatenating.\nAside from this it seems like the solution is to have a concatenate columns function as per #1473 . Closing in favour #1473 . Fixed by #1850 . @msaby are you able to test this?. Removing rows can be done from the All->Rows dropdown menu. It removes all the rows currently showing in the project, so to remove a single row you have to isolate that row in a facet or filter in some way.\nOften starring a row, then faceting by star is the easiest approach.. I think you are correct - the broader issue is how to report back errors to the user. \nAn alternative to storing the error in a new column might be to add it to the reconciliation object. Since it might(?) be possible to have multiple errors for each row from a Upload Edits to Wikidata errors would need to be stored in a list or array. Making this part of the Recon object would mean you could see which errors referred to which item.\nPossibly the Upload Edits to Wikidata could be followed by automatically creating a 'Reconciliation errors' facet on each column used as an 'Item' in the current schema?\nIn this particular scenario I think the right thing to do is probably to not create the item (because that's what happens if you try to directly create an item with duplicate label+description on WD)\n. Adding a dedicated field to com.google.refine.model.Recon was what I was suggesting - which wouldn't necessarily be Wikidata specific - it could be used for storing errors from any reconciliation process?\nAt an even more general level it might make sense to have an error field on a Cell object. At the moment you have to choose between storing the error in the cell and throwing it away - which is a pain in some situations - I'm thinking especially of 'add column from URL' where if you forget to check 'store error' and get back a load of empty cells you have to do the process again to actually  find what the error was.\nBut just throwing ideas around really - I don't have strong views on this. No JS Console errors. Happens on both Safari and Chrome. Not tested on Ff yet - can do later.. Also occurs using FF. Ah - sorry - should have tested from master. I don't feel I have enough knowledge to have a view on the technical aspects of these decisions, but I think it is clear that Butterfly is not under active development and is unknown to the majority of developers. On this basis I would be in favour of migrating to a more widely used and actively maintained framework sooner rather than later.. I think a more extensible set of clustering methods is a really good idea - and would have application beyond just languages. For example, when I work with data containing company names I often need to remove strings that are common prefixes/suffixes to the company name (e.g. \"ltd\", \"plc\", \"GmbH\")\nVery much in favour of this.. Possibly related to #816 \nPossibly related to #507. nb this replaces the following quotes:\n\u201c \u201c U+201C (left curly quote) -> \"\n\u2018 \u2018 U+2018 (left single curly quote) -> '\n\u201d \u201d U+201D (right curly quote) -> \"\n\u2019 \u2019 U+2019 (right single curly quote) -> '\n\u00ab \u00ab U+00AB (left angle) -> \"\n\u00bb \u00bb U+00BB (right angle) -> \"\n\u2039 \u2039 U+2039 (left single angle) -> '\n\u203a \u203a U+203A (right single angle) -> '\n\u201e \u201e U+201E (bottom quote) -> \"\n\u201a \u201a U+201A (single bottom quote) -> '\n. I suspect this may not just be an Excel issue but rather an issue with dates starting with a weekday name - I can't get Tue Dec 11 00:00:00 GMT 2018 to parse with toDate() currently (but Dec 11 00:00:00 GMT 2018 works fine)\nWe need to add a test case for this and then fix the toDate to work\nOnce this is working we can re-test the Excel import - there may be other issues but I think fixing the date parsing for this form of date is the starting point\n. OK - I may need to revise my first opinion as @ettorerizza has pointed out in #1287 that he can get dates of this form to parse OK in 3.1\nI'll investigate further. It looks like this issue is specifically connected with the import from Excel. If I use the supplied xlsx file from above then after import toDate() does not work. If apply a transform to the cell with just value (i.e. should be no change) then apply toDate() this then works.\nThis suggests there is something about the cell content after the import that is causing a problem. Hi @SineadM apologies for the delay in responding. You need to allow OpenRefine to install you need to open your System Preferences and go to \"Security and Privacy\" and the General tab. Here you will see a message indicating that \"OpenRefine was blocked from opening because it is not from an identified developer\". Click the \"Open Anyway\" button to complete the OpenRefine installation.\n\n. @wetneb Updated the wiki at the same time with same info :). I like the use of bold for readability\nI agree that date/time should be in the same order on both sides\nPleased you found the documentation good - that's not usually feedback we get but we have tried to improve it - so really pleased this has worked for you!. @rogargon if you copy an old project file across from the renamed folder, does that project show up OK in your new install, or does that break the installation again?\nAlso - which version were you upgrading from?. Just to keep this stuff together, worth reading https://github.com/OpenRefine/OpenRefine/issues/1204#issuecomment-326320954 and noting https://github.com/OpenRefine/OpenRefine/commit/ad807e525d5d9b3d1d400f5bbd1acbcce515926b which concern dealing with using cross on multi-value cells. Also https://github.com/OpenRefine/OpenRefine/issues/1289 and https://github.com/OpenRefine/OpenRefine/projects/1#card-5380183. I've commented against #1969 that I'm not against reverting the current changes - and I'm still happy if you want to go ahead with this. However, re-reading #1662 I'm not completely convinced that reverting the changes is really the right thing to do - #1662 is clear that there was some really funky behaviour in the previous situation (like selecting a value from the facet and not seeing all the rows from that facet).\nIn #1662 I mention (but don't directly suggest) the idea of introducing a \"Boolean\" facet - I feel that this would be a much better way of handling all of the predefined facets that use tests with a boolean outcome to create a custom text fact.\nI acknowledge absolutely that the current change has created confusion for users (and I've had to re-write training material because of it so I definitely know this has caused problems!) - but the previous approach was also broken, but we'd got used to the way in which it was broken and so either ignored it or worked around it.\nI'm worried that avoiding a breaking change here is just embedding an already broken approach. As long as the Custom Text Facet exists and works in the old way we have a problem - we can't change it, but it simply doesn't work correctly for columns of mixed value types. I can't help but wonder if the right thing to do would be to:\n\nAcknowledge this is a breaking change, but stick with it\nEnsure all predefined facets use 'toString()' as necessary\nIntroduce a new \"Boolean\" facet type and convert predefined facets with Boolean outcomes to that facet type\n\nI'm not convinced leaving the Custom Text Facet as it is makes sense in the long term.\nAll this said, I don't want to hold changes up, and I'm not in a position to move forward changes at the moment - so if reverting is what needs to happen, and my thoughts above don't sway you, then please go ahead. Reading the last para above it comes across as a slightly passive aggressive attempt to wash my hands of any decision made here - that wasn't my intention, so apologies if it comes across that way. I'll 100% support any decision made because I can see the arguments both ways - unfortunately I just don't have the time right now to write code here :(. Closing as Duplicate of #1750 . Not against this - but a couple of points:\n\nassume the intention is to revert all the changes in this PR and you've just opened it with the single change as a starter?\nThese reverts you've made here don't actually need reverting if #1888 is reverted - they should work exactly the same whether 'toString()' is appended or not once #1888 is reverted (I don't mind reverting them, but just noting it isn't necessary). I agree with @wetneb that console logging is going to be too verbose here. I can see an argument for having a 'verbose' mode for the console log which enable us to push information for debugging when necessary - but that's much bigger scope than this particular issue. Just putting down some notes about this:\n\nHaving had a look at cross and ProjectJoin.getJoin my first thought is that the process of translating a Project Name string to a project ID should be separated out from ProjectJoin.getJoin. If the project ID lookup is done directly from cross this gives us better opportunity to report on any errors in the process. If ProjectJoin.getJoin accepts project IDs rather than Project Name strings, I think this will also resolve one of the issues noted by @wetneb in #1950 :\n\nMoreover, the current design requires that both project names are unique in the workspace, whereas one would expect that only the target project (whose name appears in the invocation of the function) would need to be uniquely named.\n. It is possible for the number of rows in a tsv to be greater than the number of records (in a general sense, not the OpenRefine 'record') in the file. This is because it is valid for a tsv 'cell' to contain new line characters.\n\nI have also occasionally seen other issues that have cause OpenRefine to not parse the file as expected when imported - I've never really understood the underlying cause, but I've seen it in situations where there are hebrew characters in the original data, which leads to what should be separate columns being lumped into a single cell\nSome things to check:\n Are you sure there are really 755846 in the file, or is it possible that some of the rows counted by wc -l are actually multi-row records?\n Can you identify any rows in the OpenRefine project that have unusually large amounts of text in a single cell, or any cells which are empty which you would expect to be populated - in this case look for multiple bits of data being incorrectly merged into a single cell\n* Do you unexplained empty columns in your OpenRefine project that might be populated on specific lines where (for e.g.) two lines in the original file have been interpreted as one line in OpenRefine? - in this case, look at what character has been used to indicate a new line in the original text file.\nHope that makes some kind of sense and helps you to identify the issue. @only1chunts don't apologise - all issues welcome (here or on the forum http://groups.google.com/forum/#!forum/openrefine). Glad you sorted. I think this is deliberate, because the element selected in this case has no innerXml only text - so I think the right expression (as it currently works) would be:\nvalue.parseXml().select(\"datafield[tag=035] subfield[code='a']\")[0].ownText()\nI think this reflects usage in other languages (e.g. see https://stackoverflow.com/questions/7877609/xmlnode-value-vs-innertext). It also means you can differentiate in situations where an XML node has both text and further child elements:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<record>RECORD VALUE\n   <datafield tag=\"035\" ind1=\" \" ind2=\" \">\n      <subfield code=\"a\">TEST</subfield>\n   </datafield>\n</record>\n value.parseXml().select(\"record\")[0].innerXml() selects only the xml elements inside \n value.parseXml().select(\"record\")[0].ownTextl() selects only the string \"RECORD VALUE\" inside \nThis gives finer grained control than innerHtml which selects both.\nI can see arguments both ways for how this ought to work. I tend towards the current behaviour, although open to changing it. @msaby I've added some basic documentation on the various XML parsing functions, although I suspect it could be improved if anyone is inclined to add further information and examples.. Happy to add some tests when I get some time (I'll try to find some time soon!). @jyf1997 Could you include some more information? Specifically what version of OpenRefine were you building?. The failure of \n\ntestFetchCurrent\ntestFetchRecord\n\nI think are probably timeout issues on trying to retrieve some remote data but I'm not sure @wetneb maybe able to offer more analysis\nWe've seen failures of testToDate due to locale issues previously, and this looks like an 8 hour time difference between the expected result and the one found - can you confirm which locale your computer is set to use?\nI'm not sure what you mean by \"this Refine with extensions. launch file\" - can you point at the specific file?. Thanks @wetneb I thought we had a fix for the date conversion issue but failed to find it eariler. Odd - I did test. Thanks for the catch. doh!. OK. Thanks - will re-write that.\nIn terms of opening link in new window, I didn't change the notification code, so it is just following its previous behaviour. I think opening in a new window makes sense - I'll amend. The approach @jackyq2015 has taken here is consistent with all the other 'dialog-frame' divs in the app at the moment. IMO ideally all these should be done instead by adding a class to the div and styling via css. BUT, I'd suggest consistency is better than doing this one instance 'correctly' - then we can tidy them all together (I'd be happy to take this on once this PR is merged). Lazy just copying existing files as a template - will amend!. I used the list from Regexr as a starting point, but basically it was just the ones that seemed the most likely (to me) to be used and cause confusion. However, no problem in extending the ones it catches - could do them all if we feel it is worthwhile. Missing brace. Basic auth is prob going to be the most common, but there are other methods that work through the Authorization request header (e.g. http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-auth-using-authorization-header.html)\nWould be easy to add the Proxy-Authorization request header as well. Agreed - will take this out. Not sure I understand (sorry). Do you mean the headers should be set at a preference level? I did consider this, but each request might need different headers, and if you haven't recorded the headers used when making a specific request you can't undo/redo in a predictable way. The same thought had occurred to me with some other translations of null as it has quite a specific meaning in this context. I can't (sadly) comment on the specific of the translation, but I think it is OK (and sensible) to the term with specific meaning where there is no direct translation. When I looked at this for #1500 I found that most translations used null but Spanish and Brazillian Portuguese translated it as \"nulo\". I copied this pattern when I updated the translation files for that PR on the basis that the people who did that translation were in a better position to make that call than me :)\nI've been doing some looking round this morning and found the Microsoft terminology search on their language portal https://www.microsoft.com/en-us/language?sString=lock&langID=fr-fr. Based on this it seems that Microsoft translates in some languages and not others (and for Brazillian Portuguese it agrees with the use of nulo, while for Chinese it agrees with using null)\nI suspect we could do worse than follow Microsoft practice here.\nNote that the use of null in the transformation window as illustrated by @ettorerizza could equally be because this comes from the OR backend where we don't yet support any translations.. But I also don't want to prolong this discussion - I'm happy with the decision either way - lets get this PR done! :). Doh! I caught that error when I tested and managed not to fix it in PR. I\u2019ll fix when I\u2019m back at my computer. Sorry!. Agreed that we don't want to store a WrappedCell or WrappedRow. I believe isArrayOrList returns false for WrappedCell or WrappedRow. isArrayOrList checks for v.getClass().isArray() || v instanceof List<?>. Ah - thanks @jackyq2015 I'll have a look. This test is incorrect (L122). It should be like\nAssert.assertEquals(invoke(\"toString\", CalenderParser.parseAsOffsetDateTime(\"2013-06-01\"),\"yyyy-MM-dd\"),\"2013-06-01\");. I've made a PR on this branch to fix this #1606 . Should this mention that a JDK and Ant are pre-requisites for this working?. At the moment these are per operation. I'm not clear that having these at a project or global level is that useful? Even within a single project different operations might require different headers? What's the demand for having these per project or global?\nThat said I don't have any particular objection, but it would need a new issue. Good catch \nThat\u2019s a mistake on my part - I originally added a new function then removed it (incompletely it turns out) when I realised it wasn\u2019t needed.\nI\u2019ll fix it and update the PR. @jackyq2015 can you explain further? Are you saying you think it would be better to return something other than an error if the user tries (e.g.)\n[2018-11-22T00:00:00Z].parseXml()\nOr do you mean something else?. ",
    "ultrageek": "There is a Natural Language Processing package for Java, OpenNLP, similar to the NLTK (Natural Language Tool Kit) in Python. I'm not sure if it has noun extraction, but here is the link: https://opennlp.apache.org/. The description used to have the word \"Java\" in it, so I presume it still is. Here's another link for some example Java code to use OpenNLP: http://stackoverflow.com/questions/5836148/how-to-use-opennlp-with-java\n. In addition to what I mentioned in my other comment, here's a list of NLP packages (many in Java) than might come in handy: http://www.quora.com/What-are-good-alternatives-to-NLTK\n. I have seen this problem with both sub- and over-2000 rows, on both Mac and Win 10. Problem is, on Mac OS X as of El Capitan, the fetch no longer works - at least not for me. There's a certificate error hiding behind the problem, but I fixed it in the commandline. Fetch still works in Win 10, but as of today, I'm finding it hanging at 88% for one group of rows. When I took a slightly smaller slice of the same rows, it hangs at 98%. (But note: i have not yet updated Refine; just reporting that I started seeing similar problems as of about 1.5-2 years ago). I'm having this problem this very moment with a numeric column. It worked for three other numeric columns but not this one. This never happened to me before. All are numeric stock prices. I tried on a recent Mac Mini (OpenRefine installed around June or July 2016) and on a Dell Inspiron running Win 10 (OpenRefine installed around early April 2016). At this point, to meet client deadlines, I'm going to have to write code and start my analysis all over, but I can supply more details. ",
    "junwei-wang": "why not build openrefine with maven package bundled instead of other library????\nIt's really a bad idea to use ant.\n. Covention is usually better than congfiguratuion. Hence, maven is better\nthan ant. I couldn't edit ant configuration any more. I would create a\nbranch which is built with mvn.\n2015\u5e748\u67085\u65e5 23:43\u4e8e \"magdmartin\" notifications@github.com\u5199\u9053\uff1a\n\nFor reference here is the most recent discussion on that topic and why we\nare staying with ant:\nhttps://groups.google.com/d/msg/openrefine-dev/NT4tNgSzYZw/Amq1_jZ_aqIJ\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/71#issuecomment-128045221\n.\n. @jackyq2015 @fpompermaier \nOpenRefine with Maven\n\nFor now, you can build OpenRefine in the ${project.basedir} by following command\nsh\nmvn clean package\nand start refine by \nsh\njava -jar server/target/openrefine-server-2.7.0-SNAPSHOT.jar\nI will add more feature in the near future.\n. @jackyq2015 I see the CI error. Do not worry. I will update the code relating with your two issues in the near future.\n. what going on with streaming inputs?\n. @tfmorris I am wondering whether Refine support stream processing. I mean the input is a stream and the output is a stream. I don't have to load and process the data all at once. I process the data with all operations one by one.\n. @jackyq2015 It is because butterfly library is not in Maven repository. \nTest the following command\nmvn initialize\nmvn clean compile\n. I am sorry that the repo has been deleted.\nOn Aug 2, 2017 7:20 PM, \"Antonin Delpeuch\" notifications@github.com wrote:\n\nHi @junwei-wang https://github.com/junwei-wang,\nI'd like to merge this but it seems that the repository where the PR was\nfrom has been deleted. If you still have the data somewhere, do you think\nyou could file a new PR? The only solution I see for me is to download the\nfiles individually and commit them myself, but that would not preserve\nattribution and it's quite dirty.\nThanks a lot for your help!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/1054#issuecomment-319739686,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA04GTFH6-a-jy6ZywhizPci7Vql0PdPks5sUK_mgaJpZM4FwzBz\n.\n. \n",
    "magdmartin": "For reference here is the most recent discussion on that topic and why we are staying with ant: https://groups.google.com/d/msg/openrefine-dev/NT4tNgSzYZw/Amq1_jZ_aqIJ\n. See PR  #1493. See discussion list thread\nWe should make it easy for a superuser to create those shortcuts based on GREL function or series of operation (with or without a combination of functions and facet). \nWe won't be able to accommodate each and everyone needs in the core application, your common operations are different than mine ;) . @Opennat can you open a new ticket for your bug report. This thread is when one rename the full column, not data in a facet. \nThank you for your help. \n. Now in 2015 and the search  or search and replace across multiple fields is a frequently requested feature by the community via the mailing list or direct feedback I have during demonstrations.\n. works in 2.6-beta with chrome and firefox. Thank you Tom\nHowever it generate the following error if the password contains specific characters like % with the index of the character.\nError uploading data\nMalformed escape pair at index\nI tried escaping with a \\ and made things worse\nError uploading data\nIllegal character in authority at index 8: https://....\n. Alternatively that could be done via the All drop down menu where the user can select which column to apply the operations (all, on a subset with point and click selection like the remove rearrange function)\n. I still have the error in 2.5 and 2.6rc-1 with both chrome and firefox on a linux machine. Here the project with the error: https://drive.google.com/file/d/0B7tW6wTrzLf7X3dkQUg2QUVzbG8/view?usp=sharing\n. martin@martin-Lenovo-G585:~$ locale\nLANG=en_CA.UTF-8\nLANGUAGE=en_CA:en\nLC_CTYPE=\"en_CA.UTF-8\"\nLC_NUMERIC=\"en_CA.UTF-8\"\nLC_TIME=\"en_CA.UTF-8\"\nLC_COLLATE=\"en_CA.UTF-8\"\nLC_MONETARY=\"en_CA.UTF-8\"\nLC_MESSAGES=\"en_CA.UTF-8\"\nLC_PAPER=\"en_CA.UTF-8\"\nLC_NAME=\"en_CA.UTF-8\"\nLC_ADDRESS=\"en_CA.UTF-8\"\nLC_TELEPHONE=\"en_CA.UTF-8\"\nLC_MEASUREMENT=\"en_CA.UTF-8\"\nLC_IDENTIFICATION=\"en_CA.UTF-8\"\nLC_ALL=\n. see also this discussion.\n. convert-unix-time offer an API that you can call using the add column by fetching URL feature in OpenRefine. \n. Thanks for sharing!\nOn Wed, Apr 30, 2014 at 10:12 AM, iosonosempreio\nnotifications@github.comwrote:\n\nHey, for me those API worked fine. The steps to get the readable date are\nthe followings:\n- select the column containing the unix date, then add a new column by\n  fetching url using this code:\n'http://www.convert-unix-time.com/api?timestamp='+ substring(value, 0, 10)+'&timezone=Rome&returnType=json'\n- then transform the new column with this:\nvalue.parseJson().utcDate\nNote that I took the utcDate since I didn't need any timezone\nconversion for my date. If you need it, change the &timezone=Rome in\nthe first code with the one you like. After you've fetched the urls,\ntransform with this new one:\nvalue.parseJson().localDate\nThat's it!\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/608#issuecomment-41800770\n.\n. Issue have been merged with Issue 590 and not 591 as previously stated (error due to the issue migration from Google Code to GitHub) \n. The Google Language API can also do that job see old wiki for details: http://code.google.com/p/google-refine/wiki/FetchingURLsFromWebServices\n. I confirmed this minor bug on my side.\nRefine v2.5 with windows vista\n\nMartin\n On 2013-01-14 5:38 AM, \"Herwig Van Marck\" notifications@github.com wrote:\n\nI just noticed (on January 14th 2013) that a project I created on December\n20th 2012 is shown in the Open Project list as \"-49 weeks ago\" instead of\n\"3 weeks ago\". Looks like a bug that does not take the new year into\naccount. On the other hand, a file created on December 13th 2012 is\ncorrectly shown as \"a month ago\".\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/654.\n. Issue closed as the VIB extension solved it.\n. Is this issue still open? I haven't see the matching pull request.\n. Issue closed as the VIB extension solved it.\n. I can take care of it\nI've several  tutorial on http://googlerefine.blogspot.ca/search/label/API\nI have also this tutorial  with yahoo place API (\nhttp://googlerefine.blogspot.ca/2011/10/fetch-city-and-province-state-based-on.html)\nif you think that's a good candidate.\n\nI still have the idea to migrate the blog to the wiki.\nMartin\nOn 2013-01-25 3:17 PM, \"Tom Morris\" notifications@github.com wrote:\n\nI think a new tutorial would be great, but that's really above and beyond\nthe work required for the migration. Feel free to tackle it if you want\nthough.\nOne candidate API might be Twitter. It's JSON, fairly simple, and pretty\npopular. I don't think the example needs to be as elaborate as the current\none, although more than one step might be good (e.g. getting your followers\nand then counting the followers of the followers). Bonus points using a\nsecond API for the followup step. :-)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/662#issuecomment-12718748.\n. thanks, I was wondering what I've done wrong!\nfixing that this afternoon.\n\nThanks\nMartin\nOn Mon, Jan 21, 2013 at 2:36 PM, Tom Morris notifications@github.comwrote:\n\n@magdmartin https://github.com/magdmartin If this is intended to show\nup at openrefine.org, it needs to go in a different repo. The one you\nwant is https://github.com/OpenRefine/openrefine.github.com\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/pull/663#issuecomment-12513704.\n. Thanks for your fix!\n. Is this due to Fusion table deprecated their API? If so should we close\nthis issue?\n\nMartin\nOn 2013-02-07 12:48 PM, \"Tom Morris\" notifications@github.com wrote:\n\nThe following error when trying to upload a relatively small table (2,221\nrows x 2 columns) which seems unusual/wrong. It also looks like we got the\nerror multiple times which shouldn't happen. We (or the GData client lib?)\nshouldn't be retrying a hard error like this.\n12:41:37.211 [ gdata_upload] Request Quota Exceeded (2ms)\ncom.google.gdata.util.InvalidEntryException: Request Quota Exceeded\nRequest Quota Exceeded\nRequest Quota Exceeded Error 400\nat com.google.gdata.client.http.HttpGDataRequest.handleErrorResponse(HttpGDataRequest.java:602)\nat com.google.gdata.client.http.GoogleGDataRequest.handleErrorResponse(GoogleGDataRequest.java:564)\nat com.google.gdata.client.http.HttpGDataRequest.checkResponse(HttpGDataRequest.java:560)\nat com.google.gdata.client.http.HttpGDataRequest.execute(HttpGDataRequest.java:538)\nat com.google.gdata.client.http.GoogleGDataRequest.execute(GoogleGDataRequest.java:536)\nat com.google.refine.extension.gdata.UploadCommand$FusionTableSerializer.sendBatch(UploadCommand.java:391)\nat com.google.refine.extension.gdata.UploadCommand$FusionTableSerializer.addRow(UploadCommand.java:382)\nat com.google.refine.exporters.CustomizableTabularExporterUtilities$1.visit(CustomizableTabularExporterUtilities.java:157)\nat com.google.refine.browsing.util.ConjunctiveFilteredRows.visitRow(ConjunctiveFilteredRows.java:76)\nat com.google.refine.browsing.util.ConjunctiveFilteredRows.accept(ConjunctiveFilteredRows.java:65)\nat com.google.refine.exporters.CustomizableTabularExporterUtilities.exportRows(CustomizableTabularExporterUtilities.java:171)\nat com.google.refine.extension.gdata.UploadCommand.uploadFusionTable(UploadCommand.java:301)\nat com.google.refine.extension.gdata.UploadCommand.upload(UploadCommand.java:143)\nat com.google.refine.extension.gdata.UploadCommand.doPost(UploadCommand.java:106)\nat com.google.refine.RefineServlet.service(RefineServlet.java:179)\nat javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\nat org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511\n)\nat\norg.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n...\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/676.\n. Hi\n\nThanks for the heads up.\nYou can deal directly with me for the typos or submit a pull request if you\nwish on the gh-pages branch.\nThanks for your help.\nOn Mon, Feb 18, 2013 at 11:44 AM, mroswell notifications@github.com wrote:\n\nThere are several typos on the OpenRefine.org home page.\nactivly -> actively\nlastest -> latest\n(Is this the proper place to report these? If not, please give guidance.)\nAlso:\nhttp://openrefine.org/OpenRefine/community\nwritting tutorial -> writing a tutorial\n\"The event page also list previous presentations as catch up and source of\ninspiration for your presentation!\"\ncould use a rewrite\nAnd maybe replace\nhttp://openrefine.org/OpenRefine/blog\nwith a twitter feed, since there's no blog content\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/682.\n. So how will you cluster the full group? In several pass so when one click\nmerge and re-cluster it display the rest of the group?\n On 2013-03-06 8:28 PM, \"Tom Morris\" notifications@github.com wrote:\nI have a file where 31,787 clusters were found and the top clusters have\n39 variants in them.\nWe need to cap the number returned to the browser because otherwise it\nbecomes unresponsive (or crashes).\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/695\n.\n. Moving to 2.7 milestone to reduce the scope of 2.6 to speed up the release.\n. I fully support this feature.\nIt might help to reduce project creation time.\n\nNothing more painful to wait because if this field full if html you don't\ncare. And once the prohect created wait again for refine to remove field.\nOn 2013-04-13 6:17 PM, \"Tom Morris\" notifications@github.com wrote:\n\nCurrently there's no way to import only a subset of the columns during\nproject creation. It would be desirable to have a way for the user to\ndeselect any columns that they didn't want included in the import.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/707\n.\n. Great idea! This is the opportunity to have a better way to organize your projects (currently I have 100+ projects in Refine and this is a bit messy).\n. This pull request should resolved #728 \n. Is there any documentation on what are current test protocol and what\nshould be migrate to a new tool?\nThis can be an quite independent project for someone to investigate / work\non.\n\nMartin\nOn Wed, Jun 12, 2013 at 3:05 PM, Tom Morris notifications@github.comwrote:\n\nThe testing tool that we use for our (relatively limited) front end tests\nis basically abandoned, so we should switch to something better supported\nsuch as Selenium 2. One possibility would seem to be to use a free open\nsource plan from Sauce Labs in conjunction with our Travis CI build/test\nsetup.\nhttp://sauceio.com/index.php/2013/03/run-your-selenium-tests-completely-in-the-cloud-using-travis-ci-and-sauce-labs/\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/733\n.\n. Can you describe further your use case, and why you want to start a new\nRefine project?\n\nReading between the line, it looks like a \"fork / branch\" option in the\nhistory might be an interesting option to create a new branch with the\ncurrent project history and data layout.\nOn Thu, Jun 13, 2013 at 3:38 PM, Charles Pritchard <notifications@github.com\n\nwrote:\nPossibly re-using or starting from the Export -> Custom Tabular Export\ntool, make it easy to create a new project from an existing view. Currently\nwe have to download the tab, then re-upload it. It's not particularly\ndifficult but it is clumsy.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/734\n.\n. Please use the mailing list for support, the issue list is to track bug and change request.\n. Thanks for this contribution Blakko!\n\nBased on your email I open a wiki page to document this new functionality for people willing to translate Refine in other language. Feel free to edit it.\n. What is the new behavior now? Does Refine include the line break in the cell? \n. it also show on the linux 2.6-rc1 version\n. Should we add some doc for user to migrate their existing project then?\nOn 2013-08-08 6:41 PM, \"Tom Morris\" notifications@github.com wrote:\n\nOne last bit of branding cleanup. It's not strictly necessary, but it'll\ncause less confusion in the future.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/777\n.\n. OpenRefine is moving away from freebase. This issue is a no fix\n. Overall, I agree to close all freebase related issue in github.\n\nReconciliation service support is tied to freebase integration due to the\nproject history. With freebase being shut down I think OpenRefine core\nshould support generic reconciliation protocols and have specific\nintegration with Knowledge Graph and Wikidata APIs done via extension.\nThis tied back to how are we removing freebase extension. What is specific\nto freebase and what is generic for any reconciliation service.\nOn Oct 16, 2015 4:46 PM, \"Tom Morris\" notifications@github.com wrote:\n\nWe're going to have the same need for both Knowledge Graph and Wikidata\nAPIs, but I guess we can just create new issues for those.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/789#issuecomment-148830499\n.\n. @jackyq2015 can you check #849 and comment if it fix the issue.\n. If the reconciliation service is hosted on the machine running refine, what are the impact on the local resource comsumption (RAM and processor). Refine is already demanding on local resource for large project, should we worry about adding an other local service?\n. thanks for reporting it. Can you post what is in the console, this will help with debugging. \n\nThis may be related to https://github.com/OpenRefine/OpenRefine/issues/773\n. Hi thanks for reporting this.\nCan you share your configuration (OS. and browser) so we can narrow down\nthe source of this issue.\nThanks\nOn 2013-09-26 7:03 PM, \"shlomobl\" notifications@github.com wrote:\n\nHi,\n1) I'm learning how to use openrefine with a small dataset. I'm trying to\nsplit a column in two using a \" - \" sign but the new column just won't show\nup. I tried all the options (Split column, create new column based...). It\nis a single column dataset, imported from clipboard. What might be wrong?\n2) I also cannot export. Export seems not to be working. Nothing happens\nwhen I click export and the option I want.\nThanks!\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/819\n.\n. @wetneb I documented the point 3 here: http://kb.refinepro.com/2011/07/merge-2-columns-that-have-both-blank.html\n\nI think the idea is if a column 1 has blank or null value and one does cells[\"column 1\"].value + \" \" + cells[\"column 2\"].value  then OpenRefine should only display cells[\"column 2\"].value in the result.\n. Thanks for the pull request. Tom, OpenRefine's lead developer, will review it merge or comment back. \n. I've been using the extension recently and I haven't been able to reproduce it\n. Please use the discussion list for support related topic. The issue list is for bug and new feature request. \nThanks\n. Please use the discussion list for support related topic. The issue list is for bug and new feature request. \nThanks\n. Hello Pablo,\nFor support question please use the mailing list: \nopenrefine@googlegroups.com\nWhen creating a new bug issue please provide as much information as \npossible to help our team to understand your process. You can include \nthe release you are using, your operating system, example of the data \nyou are working on ...\nThanks you.\nOn 13-12-28 06:02 AM, Pablo Moyano wrote:\n\nJust try to create a Numeric (or Timeline) facets and it won't work, \nwhen you click the option, nothing happends\n\u2014\nReply to this email directly or view it on GitHub \nhttps://github.com/OpenRefine/OpenRefine/issues/839.\n. @tfmorris https://github.com/tfmorris, OpenRefine lead developer, will \nbe the best person to merge your pull request.\n\nThanks for your help and involvement.\nOn 13-12-30 09:11 PM, Pablo Moyano wrote:\n\nYes sorry, I'll try to be more specific next time.\nI've already made a pull request to fix this bug.\n840 https://github.com/OpenRefine/OpenRefine/pull/840\nYou need more details anyway?\n\u2014\nReply to this email directly or view it on GitHub \nhttps://github.com/OpenRefine/OpenRefine/issues/839#issuecomment-31380640.\n. I just test it with openrefine-2.6-beta.1 (linux) and the numeric facet \nworks fine. The beta version has been tested by the community and such \nbug should have been caught earlier in the process.\n\nCan you double check that you are dealing with numeric value (they \nshould be in green on your screen).\nIf so can you provide more information regarding the distribution you \nare using (beta or alpha release with your operating system)\nThanks for your help.\nOn 13-12-29 06:44 AM, Pablo Moyano wrote:\n\nChoose a numeric column, select numeric facets, the automatic facet \ndraw should be shown, instead the message \"No numeric value present.\" \nis shown there.\nIn 2.5 this is working ok\n\u2014\nReply to this email directly or view it on GitHub \nhttps://github.com/OpenRefine/OpenRefine/issues/841.\n. tested again in OpenRefine 2.6 RC1 on a linux machine. The numeric facet works fine.\n. Qi have fix it with #1079 reusing some of your code. \nThank you very much for your help and analysis on this issue, very much appreciated. \n. This issue is related to https://github.com/OpenRefine/OpenRefine/issues/612\n. Did you check the utilities extension by spartika? \nOnce the project creatd you can remove column by just selecting them. You can a select all / clear all option.\n. This is a duplicate of https://github.com/OpenRefine/OpenRefine/issues/805\n. There is a version of OpenRefine using Orientdb instead of RAM. You can\nfind more information in this thread:\nhttps://groups.google.com/forum/?nomobile=true#!searchin/openrefine/orientdb/openrefine/TOgkfvI4ZV4/RQoP1AcXmuUJ\n\nSince this is a fairly new project feedback are more than welcome!\nMartin\nDear OpenRefine team,\nIs there any plan to implement a Disk based Algorithm when data is too big\nto fit into memory?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/882.\n. The issue list is here to reports bug and feature. For support related questions please email our discussion list: https://groups.google.com/forum/#!forum/openrefine\n. Can you share the link to the XML it might come from the data structure.\nOn Jul 13, 2014 6:45 AM, \"dgbdgb\" notifications@github.com wrote:\n\nUsing OSX 10.9.4 on iMac & latest version of Chrome..\nI use the create project Web Addresses (URLs) option - takes a while, but\neventually pulls in what is perhaps 5 megabytes of Eventbrite data.\nThen I try and use the Parse data as XML files (this worked in the last\nversion of Google Refine). When I click on the option XML Files I get a\nbrief flash \"Inspecting selected files\" and nothing happens.\nIs this a known problem or a new one?\nThen just for the heck of it tried the \"Update Preview\" option in the\nParse data panel.\nSystem has frozen on \"Updating preview\" option.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/893.\n. @gideonthomas thank you for proposing your help. We will be more than happy to review your contribution. \n\nIf you have any questions you can post on the developer mailing list\n. Hi,\nThe issue list is for reporting bug or new feature request. For assistance regarding your installation please use the mailing list (https://groups.google.com/forum/#!forum/openrefine) where we will be happy to help. Please note that new member on the list are moderated, your message might not be distributed right away. \nThank you for understanding.\nMartin\n. @noamoss is your translation complete with cebb10f or there still work to do for RTL support?\n. @thadguidry we can also reach out to the community (twitter / discussion list) for help to complete the translation. This approach was successful with the Spanish one. \nWe just need to have the initial work done by @noamoss  available on a branch so other contributors can send their PR. \n. If there is no one commenting against in the next five days, I will:\n fix the conflict (help welcome for that part)\n merge this PR in the new branch hebrew-trans \n* reach out to the twitter feed + mailing list for PR to complete the translation. . From Refine's home page under create a project you can download a ZIP file with multiple CSV and then select which you want to load in Refine. I've just done the test with the  WDI (CSV)-ZIP (39 MB) file  from the World Bank's WDI data. \nDoes this answer what you are looking for? \n. +1\n. in that case, we might\n- want to fix the pop up that say the freebase service is not available\n- add message in the application saying OpenRefine support and this\n  function will be removed when freebase will be totally discontinued\n  and some functionality might not work.\nOn 15-10-15 02:21 PM, Thad Guidry wrote:\n\n@tfmorris https://github.com/tfmorris We have source control, so if \nthere are bits and pieces we need later from the extension work, we \ncan always refactor them in for the Knowledge Graph API. I have \nactually been using @MattBlissett https://github.com/MattBlissett \nextension against his Plant List API on a couple of fun projects. So I \nwould rather just see a broken Freebase menu option and I can deal \nwith the emails regarding that it is a work-in-progress to remove the \nFreebase guts in a 2.6 minor release later. Lesser of 2 evils. KISS \nprinciple.\n\u2014\nReply to this email directly or view it on GitHub \nhttps://github.com/OpenRefine/OpenRefine/pull/948#issuecomment-148479223.\n. I see two differents issue in your ticket. \n\nFirst, for cell.cross() returning 0 value, this has been addressed in OpenRefine2.6-beta see #432 for reference. \nSecond you mention that on a smaller data set Refine doesn't return the value as expected\n- Did you trim() your key column before? \n- In the dataset2 do you have duplicate value for your key?\n- Can you share your data so we can reproduce it locally?\n. @srugano you need to define which column from the \"results_export\" project\nyou want to import. For example cell.cross(\"results_export\",\n\"Values\").cells[\"Column 1\"].value[0]\nYou can learn more on the cell.cross() function here:\nhttp://googlerefine.blogspot.ca/2011/08/vlookup-in-google-refine.html\nFor support request please use our mailing list:\nhttps://groups.google.com/forum/#!forum/openrefine\nThank you\nOn Thu, Mar 26, 2015 at 12:05 AM, RUGANO notifications@github.com wrote:\n\nI'm on OpenRefine 2.6 beta and the cell.cross(\"results_export\",\n\"Values\") is returning me an empty array [] .\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/954#issuecomment-86330698\n.\n. Fixed in OpenRefine2.6 RC1\n. Thanks for reaching out! Always interesting to have Refine in a new language.\n\nFor support and question please use the developer mailing list: https://groups.google.com/forum/?fromgroups#!forum/openrefine-dev and keep the issue list for issue and feature requests.\nAs first pointer, did you check the documentation for Internationalization / Localization: https://github.com/OpenRefine/OpenRefine/wiki/Translate-OpenRefine\n. I've done a diff between the English and Spanish translation and I confirm that the Spanish translation is complete at 85%. I haven't check translation for extension.\nI personally don't speak Spanish, so any help is appreciated to complete the translation. Please submit Pull Request or if you don't know how to do it just answer here wit the line number and the translation (or no need for translation). I will update the file directly. \nThe following lines in https://github.com/OpenRefine/OpenRefine/blob/master/main/webapp/modules/core/langs/translation-es.json needs to be translated:\n56\n158\n159\n160\n162\n163\n180\n200\n208\n209\n240\n273\n276\n278\n290\n293\n310\n311\n313\n314\n357\n405\n406\n407\n431\n458\n459\n460\n461\n467\n468\n469\n482\n516\n559\n569\n570\n571\n572\n577\n581\n584\n585\n586\n587\n588\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n629\n633\n638\n643\n647\n653\n654\n658\n660\n663\n664\n665\n. closed via #1064 please open new issue / submit new pull request if you want to make some change to the translation. \n. You also see the documentation on OpenRefine APi on the refine-python wiki: https://github.com/maxogden/refine-python/wiki/Refine-API\n. testing RC-1 on linux I still have Locate an existing Refine project file (.tar or .tar.gz): when creating a project. \n. Fixed in Chrome\nI cleared the cache in Firefox and still see it.\nOn 15-05-01 03:22 PM, Tom Morris wrote:\n\nI just double checked the 2.6-rc1 Windows kit and it's definitely \nfixed. It seems unlikely that it's a Linux specific bug.\n@magdmartin https://github.com/magdmartin Can you make sure that \nyou're not seeing cached artifacts by doing a force reload in your \nbrowser. What browser are you using on Linux?\n\u2014\nReply to this email directly or view it on GitHub \nhttps://github.com/OpenRefine/OpenRefine/issues/962#issuecomment-98211052.\n. I just updated the download page. \n. I know few people who will be very pleased with this translation. Thank you a lot. \nI will check it this week before merging. \n. I just started to review it quickly and have a couple of comments.\nFirst great job, thank you for this translation. If you don't mind I'd like to suggest slightly different translation for some strings.\n\nI also open issue #974 for the text overflow in some menu. \n. Do we want to fix that for the next release? \nIf so is it better to update the French translation to fit in the menu or fix the UI to accommodate the translation length? \n. I understand by minor fix changes in the actual translation to make it fit the actual menu size. \nBumping it to 2.7 to change the menu size. \n. I just test with master today (b51dbdcbf223)  and this is still an issue. It also impacts the Portuguese (Brasil) translation. . Tested on OpenRefine2.6 RC1 and looks closed to me.\nWhen changing clustering method the \"No clusters found\" is replaced by a \"clustering\" message with a spinning icon.\n. Thank you for the fix. \n. This affect all translation, not just the French one. So I think it comes\nfrom the core, but I haven't been able to narrow the root cause.\nOn Apr 21, 2015 1:43 PM, \"Tom Morris\" notifications@github.com wrote:\n\nThanks for the bug report. If the code is correct, the next place to check\nis the French translation.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/980#issuecomment-94884209\n.\n. I personally don't have the knowledge to fix that. \n. sorry I meant to do the opposite (update my personal from the master): https://github.com/magdmartin/OpenRefine/pull/2\n. I am talking about the custom__text facet and custom numeric facet \nchoices number 5 and 6 in the facet sub menu.\nBoth options open the same transformation window.\n\nOn 15-05-01 03:12 PM, Tom Morris wrote:\n\nPlease use the mailing lists for questions.\nText facets and numeric facets are handled completely differently. One \ncreates a list of choices and the other shows a histogram of counts vs \nnumerical values.\n\u2014\nReply to this email directly or view it on GitHub \nhttps://github.com/OpenRefine/OpenRefine/issues/990#issuecomment-98209032.\n. I see what you are referring to now. my bad sorry for the confusion. \n. Thanks for the pull request Nestor! This is awesome! \nChecking your changes it seems you have a translation for every string. Can you precise what's need to be done to make the translation complete?\n\nIf you are interested, I can ask for contribution on the OpenRefine mailing list (you can do it too) and twitter account.\n. Sorry for the late answer. You may create the item in your TODO in new issue with a Spanish translation tag to easily sort them.\n@tfmorris are you ok to merge this partial translation so it is easier for other contributors to help? Should be we do that on a separated branch?\n. @nestorjal any update on the missing 15% of the translation. If you can point out the missing part we can ask the community for help to complete it. \nrelated to issue #958 \n. In Refine2.6-beta when in exclude mode the color of the select value change from orange to black, the value is strike-through and a minus sign is added. So I personally don't think we need to do any additional changes on that point. \nHowever, I agree with @esiegerman the inverse function should be available right away to the user. The initial facet can look like this: \n\n. I keep supporting this suggestion. \n. I understand the pain of having a lot of facet and loosing the big picture or relationship between them by having to scroll up and down. \nIn the same time I enjoy a lot to have most of the screen to display the data itself. Taking extra real estate to display more facet or option like the redo should be thoughts carefully keeping in mind people working on laptop with smaller screen.\nSome random (crazy) thoughts: \n- can we make the left panel resizable so the user can decide to display facet on two or three columns\n- can pop up facet (at the user discretion) like one can pop out a chat in gmail? This will allow user with larger (dual) screens to organize windows with data and facet as he wish. This will looks like photo edition software (photoshop, gimp ....)\n. I will be part of the next release.\nFor now you will have to build Refine from the repository.\nOn Thu, Sep 24, 2015 at 12:55 AM, answerquest notifications@github.com\nwrote:\n\nYay! So, curious: is this fixed in the latest version to download?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1015#issuecomment-142811624\n.\n. +1 \n. I'll suggest we have this as a default behaviour for any windows prompting over the data grid including the following interface:\n- cluster \n- all GREL related windows (fetch url, transform, add column, custom facet)\n- transpose\n- template export\n- custom export\n- other that I can't think right now.\n. Just to confirm, did you try to move right the last column or move left the first column. On the top of my head this might generate an error. \n. I confirm that move column right doesn't work on 2.6-rc1 (firefox, ubuntu) - no error generated.\n. Move column right doesn't work in OpenRefine2.6-beta (chrome and firefox, ubuntu)\n. As temporary work around you can use:  All >  Edit columns > Re-order / remove columns ...\n. If few people (including RefinePro) run OpenRefine as an hosted service, it has been designed as a local application. I am open to see how many people are interested to take Refine as an hybrid local / hosted solution. \n. +1\n. Couple of comments:\n- Refine haven't initially designed to support more than few millions rows project (I'll say up to 5M). On the top of the DOM limitation, the memory usage is getting. Some people investigated integration with Spark or orientdb. \n- I personally don't navigate with the pagination when I have over 1000 records to see my data and rely mostly on facet.\n\nHappy to see what we can do to have Refine support larger dataset. \n. fix for #925\n. I experience the same behaviour on linux + firefox. When you export to any format (csv, tsv, xls, export project ...) firefox open a new tab (or windows depending on your default local settings I guess) to download the file. This new tab remains open at the end of the download. \n. I am running Refine on linux. \n. I am experiencing this issue again with the RC1 on a ubuntu machine. I also realized that the data set use quotation mark and un/checking \"Quotation marks are used to enclose cells containing column separators\" have no effect on the preview.\n. fix for #708\n. related to (duplicate of ?) #1012 \n. fix for #1021 \n. Thank you for the improvement. Tom will review and merge your PR\nFYI, we are currently working on the 2.6 release so it might not be \nmerge immediately.\nMartin\nOn 15-07-30 06:22 PM, Scott Wiedemann wrote:\n\n\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/OpenRefine/OpenRefine/pull/1047\nCommit Summary\n- Fixed #1046 Combine xls and xlsx formats by inspecting file header\n  information in ExcelImporter.\nFile Changes\n- M main/src/com/google/refine/importers/ExcelImporter.java\n  https://github.com/OpenRefine/OpenRefine/pull/1047/files#diff-0\n  (21)\n- M main/webapp/modules/core/MOD-INF/controller.js\n  https://github.com/OpenRefine/OpenRefine/pull/1047/files#diff-1\n  (21)\n- M\n  main/webapp/modules/core/scripts/index/parser-interfaces/excel-parser-ui.js\n  https://github.com/OpenRefine/OpenRefine/pull/1047/files#diff-2 (1)\nPatch Links:\n- https://github.com/OpenRefine/OpenRefine/pull/1047.patch\n- https://github.com/OpenRefine/OpenRefine/pull/1047.diff\n\u2014\nReply to this email directly or view it on GitHub \nhttps://github.com/OpenRefine/OpenRefine/pull/1047.\n. fix for #1015\n. Thank for reporting this issue. Having the files will help to reproduce the issue.\n\n@jackyq2015 looks like similar to #708 \n. @niconoe thank you for your update. Anyone with a github account can edit the documentation. Do you mind updating the installation page with your finding: https://github.com/OpenRefine/OpenRefine/wiki/Installation-Instructions\nThank you very much. \n. Thank you for your contribution. \n. Now available in  #1493. fix for #512\n. Duplicate of #1060 which provie more details. Closing this issue. \n. I support Herve proposition. Preparing data is like telling a story as there is never one way of doing something.\nQuickly a data preparation process can go through hundreds of steps and understanding the logic afterwards can be complex. Like developers comment their code to with their logic, we should be able to do the same in Refine.\nThis will help with collaboration and reproducibility of the code along with improving auditing. \n. Herwig\nThank you for sharing this! Is it currently available in a standalone\nextension ? Available on public repository? I'd love to see how we can\nintegrate this to OpenRefine.\nMartin.\nOn Sep 2, 2015 9:23 AM, \"Herwig Van Marck\" notifications@github.com wrote:\n\nI have been using comments for a while now, by adding column\ntransformations of the form\njython:return value # comment\n(which do nothing) and then using an overloaded renderer of the\nHistoryPanel class in a custom plugin:\n// labels in history panel\nHistoryPanel.prototype._renderOrig=HistoryPanel.prototype._render;\nHistoryPanel.prototype.render= function() {\nthis._renderOrig();\nvar self=this;\nvar elmts = DOM.bind(this._div);\n//console.log(elmts);\nelmts.bodyDiv.find(\".history-entry\").each(function() {\nvar text = this.childNodes[1].firstChild.nodeValue;\nvar mtch=text.match(/Text transform on 0 cells in column [^:]+: jython:\n_return +value *#(.)/);\nif (mtch) {\nthis.childNodes[1].firstChild.nodeValue=mtch[1];\n$(this.childNodes[1]).addClass(\"history-comment\");\n}\n});\n}\ntogether with some css for the history:\n.history-past a.history-entry .history-comment {\ncolor: #00f\n}\n.history-now a.history-entry .history-comment {\ncolor: #ff4\n}\n.history-future a.history-entry .history-comment {\ncolor: #88f\n}\nCheers,\nHerwig\nHerwig Van Marck, PhD.\nSenior Expert Research Informatics\nRijvisschestraat 126 3/R, 9052 Zwijnaarde\nTel: +32 (0)9 248.16.01\nBio Informatics Training and Service Facility (BITS\nhttp://www.bits.vib.be/)\nOn 2 September 2015 at 15:04, magdmartin notifications@github.com wrote:\n\nI support Herve proposition. Preparing data is like telling a story as\nthere is never one way of doing something.\nQuickly a data preparation process can go through hundreds of steps and\nunderstanding the logic afterwards can be complex. Like developers\ncomment\ntheir code to with their logic, we should be able to do the same in\nRefine.\nThis will help with collaboration and reproducibility of the code along\nwith improving auditing.\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/OpenRefine/OpenRefine/issues/1057#issuecomment-137067159\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1057#issuecomment-137076887\n.\n. not sure if it is a duplicate or related to #1056\n. Going back few version this bug has been introduce between the 2.5 and 2.6-beta release.\n\nI am also closing #1056 as this issue provide more details. \n. I have the issue with Linux. You need to click on the merge button to get\nthe error message.\nOn Sep 6, 2015 8:14 PM, \"QI\" notifications@github.com wrote:\n\ncan't reproduce it under windows7 + chrome(Version 44.0.2403.130 m). Can\nanyone confirm this only happens on the Mac?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1060#issuecomment-138139533\n.\n. duplicate of #332 \n. @rudygt I just added proposition made by John Otto Knok via a google doc. Translation is hard, I find always good to have multiple option. Please take those as proposal, I let Spanish speake decide what the translation should be. \n. thank you for the hard work. Usually I try to stick with vocabulary the user is already familiar with and used by software suite like OpenOffice / Microsoft Office or other. \n. fix for #1060\n. Junaid,\n\nThank you for reporting it. Can you tell us with Operating System and \nversion of Refine are you using? That will help us to trouble shoot it.\nThank you\nMartin\nOn 15-09-15 12:18 PM, Junaid Atari wrote:\n\ni have problem with UTF-8 encoded files, Right after upload my file \n(text of URDU Language) converted to following text:\n16 \u00d8\u00a7\u00d8\u00b7\u00d8\u00a7\u00d9\u201e\u00d9\u02c6\u00db\u0152\n7 \u00d8\u00a7\u00d9\u2020\u00da\u00af\u00d8\u00b1\u00db\u0152\u00d8\u00b2\u00db\u0152\n19 \u00d8\u00a7\u00d9\u02c6\u00d8\u00b3\u00d8\u00aa\u00d8\u00a7\u00d8\u00a6\u00db\u0152\n\u2014\nReply to this email directly or view it on GitHub \nhttps://github.com/OpenRefine/OpenRefine/issues/1066.\n. This should also fix some of the 36 issues that contains the freebase keyword\n. Thank you for the correction. \nPlease send new correction in a separate pull request. \n. I just test it with Chrome on a chromebook on the test deployment and I don't see the issue. Works fine for me.\n. Thank you for the pull request, however I can't find the attached filed. I think Github now support file attachment, can you retry to upload it. \n\nOn a separate note, I am curious to learn more about the integration you are doing with polymap4\nThank you.\n. Which version of OpenRefine are you using the 2.6beta or RC1?\nThis seems related to #871 \n. Rational to merge Qi PR are on the dev mailing list.\nhttps://groups.google.com/forum/#!topic/openrefine-dev/PZ_nmjPwlzo\nOn Oct 16, 2015 4:54 PM, \"Tom Morris\" notifications@github.com wrote:\n\n@magdmartin https://github.com/magdmartin Who reviewed this before you\nmerged it? I'm happy to have you merge pull requests for translations,\nREADMEs, etc, but you don't have the technical knowledge to review code\ncontributions.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/1079#issuecomment-148834271\n.\n. Thank you for reporting this. At which point the program failed? Do you see any error message? \n\nThank you for the additional information.\n. Thanks you for your contribution. I am sure a lot of user will appreciate\nthe cluster export.\nTom will review and merge the PR.\n. @thadguidry this is similar to #871 and looks like a missing translation or text.. @PeterJPower OpenRefine works in Chrome and Firefox only. Bugs are expected in Internet Explorer or Microsoft Edge\n. This is related to Add support for hotkeys / keyboard accelerators #1028 \n. Please use our mailing list for support question\n. Thank you for reporting this. Did you try with previous version of OpenRefine 2.6 like the rc1 or beta release? \nI've been using OpenRefine on ubuntu with Firefox without issue personally. \n. Filtering in record mode will show the full record. In your case, since the first row of your record contains the value true OpenRefine display all the rows including other values. \nIf you are not sure about an OpenRefine functionality, please use the mailing list for support question.\n. Which version of OpenRefine are you using and what is your OS?\nI can reproduce the issue with RC2 and linux. \nThis may be related to #1034\n. Thank you a lot. We usually merge translation related PR quickly, but I want to understand what the AppVeyor build failed first. \n. Please use the mailing list for support type question. \n. Please use the mailing list for support related question. \n. For support related question please use the mailing list\n. The 2.6-beta version contain a lot of improvement compared to the 2.5 version (see release note) and the RC1 and RC2 have minor improvement and new language packages. This is more a release naming issue than anything else and I recommend using the RC2 version to new users. \n. Please use the mailing list for support request regarding the usage of OpenRefine. Github ticket are to report bugs and request new feature. . Please use the mailing list for support request regarding the usage of OpenRefine. Github issues are to report bugs and request new features. . I vote for option 2 make it a generic use case. There is a lot of\nreconciliation endpoint out there and I think the community will greatly\nappreciate it.\n2017-03-10 19:14 GMT-05:00 Antonin Delpeuch notifications@github.com:\n\nThe Freebase extension for OpenRefine had a nice feature: once you had\nreconciled a column to Freebase, you could fetch data from Freebase using\nthe properties associated to the items.\nhttps://youtu.be/5tsyz3ibYzk\nIt is currently possible to do that for Wikidata, using the \"Add column by\nfetching URLs\" feature. The reconciliation endpoint provides an\n(undocumented) API to help you do that.\nhttps://tools.wmflabs.org/openrefine-wikidata/en/fetch_\nvalues?item=Q1377&prop=P856\nWith the following parameters:\n\nitem=Q1377 gives the item to fetch the value from\nprop=P856 gives the property storing the value\nflat=true returns the plain value instead of a JSON payload\nlabel=false can be used when the property points to an item and we\n   want to retrieve the identifier instead of the label\n\nThis works well, but is clearly not as user-friendly as the Freebase\ninterface! So I see two options:\n-\neither we migrate the freebase extension to Wikidata\n   -\nor we recognize that fetching data associated with ids is a fairly\n   generic use case, so we augment the reconciliation API with an additional\n   endpoint to do that. This would enable other reconciliation endpoints to\n   implement the feature and have it nicely integrated in OpenRefine.\nWhat do you think?\nP.S: bounties welcome!\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1179, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACeDkHMG0RmUIVcK08RgDgSxyMojP0r0ks5rkedogaJpZM4MaAv7\n.\n. I just ping @codeforkjeff and @nickynicolson on twitter for their inputs. The conciliator and RBGKEW Reconciliation -and-Matching-Framework projects will definitely benefit from it.\n. you can use your browser search function (ctrl + F) as a work around. . It will be nice to have an extension that allows users to define a schema they want to match against (Wikidata, GoKB or anything else). \n\nThe GOKB project already did some neat work for schema alignment. See https://www.youtube.com/watch?v=0wAY94hc7Gw  @ostephens may be able to give more background on it.\n. That make sense. I was referring more to a general validation engine for flat file (csv, XLS ...). We can make that through a different issue. . Here is a summary of the comments and suggestion from the different tickets together (adding few of my own). Feel free to comment and enrich.\nList of meta to create: \n\ncreated (from metadata.json)\nmodified (from metadata.json)\nencoding (from metadata.json)\nother the import option settings which were used (from #1045 -  #182)\nfile source (path or url?) (from #1045)\nfree form of comment (from #1045)\nnumber of rows (from #657)\nnumber of columns (from #657)\nnumber of operation\nproject id \n\nMake the list of project sortable by metadata (from #657)\nMeta information are available from metadata.json ;  /command/core/get-all-project-metadata, /command/core/get-models and /command/core/get-columns-info.\n. I will prefer if we follow the proposal from @thadguidry in #1221#issuecomment-320259551 and provide key/value store where the user can define its own field. \nCreate new field / metadata\nIn preference.vt the user can: \n create the key he want to track\n define which keys are displayed on the homepage \nWe need to provide a link to the preference page from the homepage. \nDisplay and edit the metavalue\nAs @jackyq2015 suggested: we can do it from the homepage by adding the columns for some fields on the index page and also provide a \"About\" link to biew and edit all the supported meta data.\nWe can also add a button next to Export to edit the metadata once someone opened the project.\nDefault metadata\nI suggest we keep as default metadata information we already have available in OpenRefine . I compiled the following list of default key:\n\ncreated at (from metadata.json)\nmodified at (from metadata.json)\nencoding (from metadata.json)\nother the import option settings which were used (from #1045 -  #182)\nfile source (path or url?) (from #1045)\nfree form of comment (from #1045)\nnumber of rows (from #657)\nnumber of columns (from #657)\nnumber of operation\nproject id . It looks great @jackyq2015  thanks a lot.  My only comment is to add a link to the preference page - see #1303\n\n@ettorerizza ctrl+F also work to search for a specific meta on the page ;). I think we can leave it to the person who creates the issue to share the file with the platform of his/her choice. \nIt can be good to add a line in the CONTRIBUTING.md Guidelines for Reporting a Bug to invite the reporter to add a file. . Linking to the discussion on OpenRefine user mailing list. . this will help to address #1235. Can we allow the user to set up the memory from the preference page? We can show to the user how much memory is available on the machine and let him/her adjust.\nlike @jackyq2015 mentioned, I will cautious to auto set the memory as 1) we don't know the size of the data set and 2) we don't know if the user is running and other RAM greedy application. @fpompermaier Please lets us know what is the new project URL so we can promote and redirect users. . @thadguidry Agree - Don't Repeat Yourself ;). Any reason we are maintaining two readme file (readme.txt and readme.md)? . Can weblet push to a dedicated branch?\nOn Dec 5, 2017 6:48 AM, \"Antonin Delpeuch\" notifications@github.com wrote:\nNow that the master branch is protected, Weblate fails to push new\ntranslations. One possible solution would be to change the weblate user to\nadministrator, but that's not really ideal\u2026 Any better ideas?\nAlso, it's currently failing to merge translation files. I will look into\nsolving that.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1374, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACeDkIYfYwyKyIMoaxf3nfKW0G1aM2phks5s9S2lgaJpZM4Q2Kir\n.\n. @ettorerizza I am not sure I follow you. Why do you want to keep your note in the project metadata?\nI'll rather use the option add note described by @thadguidry to let the user add a new step in the history tab with a description of what a group of GREL expression does (option 1 from Thad comment). . @wetneb I think we are discussing two different things and I just highjack this thread (sorry about that). \n\nA master note of the project in the metadata as discussed in this ticket. \nA documentation of the code (like you add comments in other programming languages) in the history tab. I think this should be discussed in #368.. ping @OpenRefine/core regarding @AlexandruAmarandei  question. . Yann, thanks for reaching out. As you can see, it is on our roadmap under Performance Improvements\nHowever, this is a huge milestone, and we need first to break it down into concrete steps and define the architecture. Is it something you will be able to help us with? . @YannBrrd regarding your architecture diagram, the front-end and back-end are currently not independent in OpenRefine. \n\nWe recently received funding from Google News Labs and we plan to use that funds to modernize OpenRefine architecture including:\n\nSeparating the front end from the back end. \nOnce the back end independent, we will be able to focus on the performance. \n\n@wetneb did I missed something?. @fpompermaier see also our CONTRIBUTING document. I am also migrating the draft Governance model in #1435. I found this repo by @pachamaltese : https://github.com/pachamaltese/openrefine-deb \nCan it help?. @pachamaltese can you send a pull request so our core team can review, provide feedback and merge it?. @thadguidry @joanneong I created a new issue to refresh the website styling. @joanneong this is a separate repo, see https://github.com/OpenRefine/openrefine.github.com. Should #946 solve it?. @isaomatsunami  thanks for taking care of this. Did you check for overflow in other translation> The issue was initially reported in  #974\nI guess a single PR for #1458 #1459 #1460 will be appreciated by the dev team. \n. @jcpuzs1 thanks for offering your help. We use Weblate to manage translation. You can start a Filipino project from here - see start new translation at the bottom.\nYou can register to Weblate using your Github account. @jcpuzs1 Weblate submits automatically the Pull Request. Your work is available in #1479 \n. @Cyyy1998 please see the discussion with @jcpuzs1 in #1464  for the Filipino translation. \nHappy to see you participate. . We already have a Spanish translation in progress via Weblate. You can check it on Weblate or in OpenRefine repo. \nLooking forward your pull request. . A first mapping is available on the page 5 of OpenRefine Batch Processing with Apache Beam document.. See also documentation regarding the OpenRefine API. I cannot open or create project after building OpenRefine from master 14e49d0099d9\nsystem: Ubuntu 16.04 LTS\nbrowser: chromium\nThe two project open properly in 2.8. \n```\n ./refine \nYou have 15747M of free memory.\nYour current configuration is set to use 1400M of memory.\nOpenRefine can run better when given more memory. Read our FAQ on how to allocate more memory here:\nhttps://github.com/OpenRefine/OpenRefine/wiki/FAQ:-Allocate-More-Memory\nStarting OpenRefine at 'http://127.0.0.1:3333/'\n22:55:47.530 [            refine_server] Starting Server bound to '127.0.0.1:3333' (0ms)\n22:55:47.531 [            refine_server] refine.memory size: 1400M JVM Max heap: 1407188992 (1ms)\n22:55:47.541 [            refine_server] Initializing context: '/' from '/home/martin/app/OpenRefine/main/webapp' (10ms)\n22:55:47.594 [            refine_server] Starting autoreloading scanner...  (53ms)\n22:55:47.936 [                   refine] Starting OpenRefine 2.8 [TRUNK]... (342ms)\n22:55:47.936 [                   refine] initializing FileProjectManager with dir (0ms)\n22:55:47.936 [                   refine] /home/martin/.local/share/openrefine (0ms)\n22:55:48.814 [       database-extension] Initializing OpenRefine Database... (764ms)\n22:55:48.817 [       database-extension] Database Extension Mount point /extension/database/ [*] (3ms)\n22:55:48.817 [       database-extension] Registering Database Extension Commands...... (0ms)\n22:55:48.825 [       database-extension] Database Extension Command Registeration done!! (8ms)\n22:55:48.826 [       database-extension] Database Operations Registered successfully... (1ms)\n22:55:48.826 [       database-extension] Database Functions Registered successfully... (0ms)\n22:55:48.830 [       DatabaseModuleImpl]  Database Extension Module Initialization Completed!! (4ms)\nCreated new window in existing browser session.\n[25814:25850:0307/225552.789968:ERROR:browser_gpu_channel_host_factory.cc(121)] Failed to launch GPU process.\n22:55:53.646 [       database-extension] receiving request for styles/jquery.contextMenu.css (4816ms)\n22:55:53.654 [       database-extension] receiving method for GET (8ms)\n22:55:53.705 [       database-extension] receiving request for styles/pure.css (51ms)\n22:55:53.708 [       database-extension] receiving method for GET (3ms)\n22:55:53.715 [       database-extension] receiving request for styles/bootstrap.css (7ms)\n22:55:53.724 [       database-extension] receiving method for GET (9ms)\n22:55:53.744 [       database-extension] receiving request for styles/database-import.less (20ms)\n22:55:53.747 [       database-extension] receiving method for GET (3ms)\n22:55:54.336 [                   refine] POST /command/core/load-language (589ms)\n22:55:54.378 [                   refine] GET /command/core/get-preference (42ms)\n22:55:54.389 [                   refine] POST /command/core/load-language (11ms)\n22:55:54.401 [                   refine] POST /command/core/load-language (12ms)\n22:55:54.589 [                   refine] POST /command/core/get-importing-configuration (188ms)\n22:55:54.604 [                   refine] GET /command/core/get-all-project-tags (15ms)\n22:55:54.633 [                   refine] GET /command/core/get-all-project-metadata (29ms)\n22:55:54.680 [                   refine] GET /command/core/get-languages (47ms)\n22:55:54.722 [                   refine] GET /command/core/get-version (42ms)\n22:55:54.903 [       database-extension] receiving request for scripts/index/database-import-form.html (181ms)\n22:55:54.903 [       database-extension] receiving method for GET (0ms)\n22:55:54.913 [                   refine] GET /command/database/saved-connection (10ms)\n22:56:02.214 [                   refine] POST /command/core/create-importing-job (7301ms)\n22:56:02.224 [                   refine] POST /command/core/importing-controller (10ms)\n22:56:03.219 [                   refine] POST /command/core/get-importing-job-status (995ms)\n22:56:03.235 [                   refine] GET /command/core/get-all-project-tags (16ms)\n22:56:03.261 [                   refine] POST /command/core/importing-controller (26ms)\n22:56:03.313 [                   refine] POST /command/core/importing-controller (52ms)\n22:56:03.362 [                   refine] POST /command/core/get-models (49ms)\n22:56:03.370 [                   refine] POST /command/core/get-rows (8ms)\n22:56:09.110 [                   refine] POST /command/core/importing-controller (5740ms)\n22:56:10.118 [                   refine] POST /command/core/get-importing-job-status (1008ms)\n22:56:10.121 [                   refine] POST /command/core/cancel-importing-job (3ms)\n22:56:11.191 [                   refine] POST /command/core/load-language (1070ms)\n22:56:11.213 [                   refine] GET /command/core/get-preference (22ms)\n22:56:11.219 [                   refine] POST /command/core/load-language (6ms)\n22:56:11.223 [                   refine] POST /command/core/load-language (4ms)\n22:56:11.310 [                   refine] GET /command/core/get-project-metadata (87ms)\n22:56:11.311 [          org.mortbay.log] Error for /command/core/get-project-metadata (1ms)\njava.lang.NoSuchMethodError: com.google.refine.ProjectManager.getProjectMetadata(J)Lcom/google/refine/ProjectMetadata;\n    at com.google.refine.commands.project.GetProjectMetadataCommand.doGet(GetProjectMetadataCommand.java:62)\n    at com.google.refine.RefineServlet.service(RefineServlet.java:171)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n    at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n    at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n    at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n    at org.mortbay.jetty.Server.handle(Server.java:326)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:923)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:547)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\n22:56:36.725 [                   refine] POST /command/core/load-language (25414ms)\n22:56:36.745 [                   refine] POST /command/core/load-language (20ms)\n22:56:36.750 [                   refine] POST /command/core/load-language (5ms)\n22:56:36.955 [                   refine] POST /command/core/get-importing-configuration (205ms)\n22:56:37.279 [       database-extension] receiving request for scripts/index/database-import-form.html (324ms)\n22:56:37.279 [       database-extension] receiving method for GET (0ms)\n22:56:37.292 [                   refine] GET /command/database/saved-connection (13ms)\n22:56:45.741 [                   refine] POST /command/core/create-importing-job (8449ms)\n22:56:45.750 [                   refine] POST /command/core/importing-controller (9ms)\n22:56:46.746 [                   refine] POST /command/core/get-importing-job-status (996ms)\n22:56:46.761 [                   refine] GET /command/core/get-all-project-tags (15ms)\n22:56:46.785 [                   refine] POST /command/core/importing-controller (24ms)\n22:56:47.031 [                   refine] POST /command/core/importing-controller (246ms)\n22:56:47.039 [           ProjectManager] Saving all modified projects ... (8ms)\n22:56:47.059 [        project_utilities] Saved project '1806062362487' (20ms)\n22:56:47.075 [                   refine] POST /command/core/get-models (16ms)\n22:56:47.080 [                   refine] POST /command/core/get-rows (5ms)\n22:56:49.509 [                   refine] POST /command/core/importing-controller (2429ms)\nException in thread \"Thread-7\" java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\n    at java.util.LinkedList.checkElementIndex(LinkedList.java:555)\n    at java.util.LinkedList.get(LinkedList.java:476)\n    at com.google.refine.importing.ImportingUtilities.inferColumnType(ImportingUtilities.java:1143)\n    at com.google.refine.importing.ImportingUtilities.createProjectSynchronously(ImportingUtilities.java:1130)\n    at com.google.refine.importing.ImportingUtilities.access$200(ImportingUtilities.java:115)\n    at com.google.refine.importing.ImportingUtilities$7.run(ImportingUtilities.java:1076)\n22:56:50.515 [                   refine] POST /command/core/get-importing-job-status (1006ms)\n22:56:51.514 [                   refine] POST /command/core/get-importing-job-status (999ms)\n22:56:52.516 [                   refine] POST /command/core/get-importing-job-status (1002ms)\n22:56:53.515 [                   refine] POST /command/core/get-importing-job-status (999ms)\n22:56:54.515 [                   refine] POST /command/core/get-importing-job-status (1000ms)\n22:56:55.515 [                   refine] POST /command/core/get-importing-job-status (1000ms)\n22:56:56.515 [                   refine] POST /command/core/get-importing-job-status (1000ms)\n22:56:57.515 [                   refine] POST /command/core/get-importing-job-status (1000ms)\n22:56:58.515 [                   refine] POST /command/core/get-importing-job-status (1000ms)\n22:56:59.515 [                   refine] POST /command/core/get-importing-job-status (1000ms)\n22:57:00.514 [                   refine] POST /command/core/get-importing-job-status (999ms)\n22:57:01.515 [                   refine] POST /command/core/get-importing-job-status (1001ms)\n22:57:02.515 [                   refine] POST /command/core/get-importing-job-status (1000ms)\n22:57:03.515 [                   refine] POST /command/core/get-importing-job-status (1000ms)\n22:57:04.516 [                   refine] POST /command/core/get-importing-job-status (1001ms)\n22:57:05.517 [                   refine] POST /command/core/get-importing-job-status (1001ms)\n22:57:06.515 [                   refine] POST /command/core/get-importing-job-status (998ms)\n22:57:07.515 [                   refine] POST /command/core/get-importing-job-status (1000ms)\n22:57:08.515 [                   refine] POST /command/core/get-importing-job-status (1000ms)\n22:57:09.515 [                   refine] POST /command/core/get-importing-job-status (1000ms)\n22:57:10.515 [                   refine] POST /command/core/get-importing-job-status (1000ms)\n22:57:11.515 [                   refine] POST /command/core/get-importing-job-status (1000ms)\n22:57:12.515 [                   refine] POST /command/core/get-importing-job-status (1000ms)\n22:57:13.515 [                   refine] POST /command/core/get-importing-job-status (1000ms)\n22:57:14.515 [                   refine] POST /command/core/get-importing-job-status (1000ms)\n22:57:15.516 [                   refine] POST /command/core/get-importing-job-status (1001ms)\n22:57:16.516 [                   refine] POST /command/core/get-importing-job-status (1000ms)\n22:57:17.514 [                   refine] POST /command/core/get-importing-job-status (998ms)\n22:57:18.515 [                   refine] POST /command/core/get-importing-job-status (1001ms)\n22:57:19.517 [                   refine] POST /command/core/get-importing-job-status (1002ms)\n22:57:20.515 [                   refine] POST /command/core/get-importing-job-status (998ms)\n22:57:21.515 [                   refine] POST /command/core/get-importing-job-status (1000ms)\n22:57:22.515 [                   refine] POST /command/core/get-importing-job-status (1000ms)\n22:57:23.515 [                   refine] POST /command/core/get-importing-job-status (1000ms)\n22:57:24.515 [                   refine] POST /command/core/get-importing-job-status (1000ms)\n22:57:24.780 [                   refine] POST /command/core/cancel-importing-job (265ms)\n^C\ngi22:57:31.130 [           ProjectManager] Saving all modified projects ... (6350ms)\n22:57:31.136 [        project_utilities] Saved project '1806062362487' (6ms)\n22:57:31.137 [        project_utilities] Saved project '1862328122730' (1ms)\n```\nFYI the second load was a very small XLS file and take few seconds to create a project from it with 2.8. \nWhen I go back to the home page, the two projects are listed under open project. When I try to open them I got the spinner running and the following logs\n```\n23:01:51.599 [          org.mortbay.log] Error for /command/core/get-project-metadata (5ms)\njava.lang.NoSuchMethodError: com.google.refine.ProjectManager.getProjectMetadata(J)Lcom/google/refine/ProjectMetadata;\n    at com.google.refine.commands.project.GetProjectMetadataCommand.doGet(GetProjectMetadataCommand.java:62)\n    at com.google.refine.RefineServlet.service(RefineServlet.java:171)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n    at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n    at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n    at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n    at org.mortbay.jetty.Server.handle(Server.java:326)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:923)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:547)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\n``. doing some investigation. I got the error up to commit c4b0ff6b which is for data package metadata #1398 I can open and create projects when I checkout 47f44b06c45. @jackyq2015 thanks for the tip. Usingant clean build` fixed the issue.. @kerim020 English is the default language for the community. We will use only English to open new issue, discuss feature or the project roadmap. \nIf you want to translate OpenRefine into Turkish you can do it from here: https://hosted.weblate.org/engage/openrefine/. External licenses are already available in the licenses folder.  Do you\nthink we need a file listing them ?\n. Ok I will do the edits.\nOn Feb 24, 2018 4:43 PM, \"Thad Guidry\" notifications@github.com wrote:\n\n@magdmartin https://github.com/magdmartin We just need to also update\nour README.md file to include the URL to the Github folder path, then I\nthink we are good. Notice this sentence in our README.md \"See that file\nalso for information on open source libraries that OpenRefine depends on.\"\nwhich should be replaced with something like \"See (text) for license\ninformation on other open source libraries that OpenRefine depends on.\"\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/1519#issuecomment-368262584,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACeDkAPtY9GSzdsfCkKOVpkFRvZpRXkpks5tYIJ2gaJpZM4SSBzN\n.\n. @jackyq2015 as discussed on the mailing list can you merge this one before cutting the 3.0RC1 release \nIt fixes #1578 . @Bowiemb disconnecting the front and back end is part of our roadmap. \n\nHere are my 2 cents for this proposal. What if we allow the user to generate an UPDATE statement following the same principle than the current SQL Exporter for INSERT INTO. The worflow will be something like this:\n connect to the database to create the project\n clean up in OpenRefine\n* generate an update statement to save back the result in the database with an indication of the key field to use for the update (all the other field will be updated). \nObviously, this scenario will work only have several limitations including:\n Work with only one table\n Does not support changes to the table schema \n. duplicate of #1631 ? . I am adding this ticket to the front/end back end separation so we can take it into account when we define the technical roadmap. . it was first PR for #1445 (linking the two tickets). @pachamaltese  any update on the new PR? . @thadguidry @wetneb @jackyq2015  any preferences? :point_up: \n. It is also reported under CVE-2018-19859. I'd rather use \"masquer\" ou \"cacher\" rather than \"Plier\"\n. I'd go with \"Regarder ces tutoriels vid\u00e9os\"\n. I'd rather use \"favorites\" for \"expressions favorites\". \n. This is in the edit cells > transform expression windows.\n\n. QA stands for Quality Assurance in that case\n. I guess it is forward \"avanzar\" - Refine doesn't send email ;)\n. did you meant Freebase instead of Firebase. \nMQL and triple loader aren't specific to freebase. \n. It is intended to check the quality of the results from the reconciliation server. Process is the following: \n1. The user send data to a reconciliation server,\n2. Reconciliation server returns matches with a score\n3. The user can use those facet to navigate the matches and approved / reject them\n. The other think to keep in mind is the length of the text. Currently Refine overflow text longer than the menu (see #974 for details)\n. I received this proposal separatly via John \" Convertir Columnas a Filas\"\n. received this proposal: \"Convertir celdas en filas a Columnas\"\n. received this suggestion: \"Crear Columnas por Columnas Claves con Valores\"\n. received this suggestion: \"Restablecer la plantilla\"\n. received this proposal: \"Realizar Operaciones\"\n. received this proposal: \"Coincidir\"\n. This is when pasting JSON operations to bulk apply them in the Undo / Redo tab.\n. ",
    "fpompermaier": "+1 for using maven..definitely\nOn 5 August 2015 at 17:55, Junwei Wang notifications@github.com wrote:\n\nCovention is usually better than congfiguratuion. Hence, maven is better\nthan ant. I couldn't edit ant configuration any more. I would create a\nbranch which is built with mvn.\n2015\u5e748\u67085\u65e5 23:43\u4e8e \"magdmartin\" notifications@github.com\u5199\u9053\uff1a\n\nFor reference here is the most recent discussion on that topic and why we\nare staying with ant:\nhttps://groups.google.com/d/msg/openrefine-dev/NT4tNgSzYZw/Amq1_jZ_aqIJ\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/OpenRefine/OpenRefine/issues/71#issuecomment-128045221>\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/71#issuecomment-128048225\n.\n. We have a mavenized version of OpenRefine it this could help\n\nOn Sun, 12 Aug 2018 at 19:12, Thad Guidry notifications@github.com wrote:\n\n@wetneb https://github.com/wetneb Actually, I'd rather baby step\nthis... Can we move to Maven first ? Don't we already have a PR past where\nsomeone did most of that work already and we could just tweak to get the\nMaven build working properly ?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/71#issuecomment-412357315,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ADmeJZDEZwUznjJRbitqNIXNiP_UgI88ks5uQGILgaJpZM4AM1Ju\n.\n. I agree with @thadguidry...null should be replaced with whatever you want and should not throw any errors when concatenating to other strings. In the end, also when you export a dataset to CSV null and \"\" are equivalent. I do speak italian if you need some translation. Just tell me what you need\nOn Apr 20, 2015 12:39 PM, \"Matt Blissett\" notifications@github.com wrote:\nIt is indeed all under the extension.\nDoes anyone speak Italian? There are a few translations that need doing\n(or not, if this is going to need rewriting in a few months anyway).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/948#issuecomment-94418789.\n. Hi to all,\nI'm happy to see OpenRefine regaining interest from mantainers!\nI think that nowadays Refine should be able to load data from local filesystem (as usual) but it should also evolve to exchange data from/to external systems like databases or Hive/HBase tables or Parquet Directory, delegating transformation of those datasets to Big Data engine like Spark or Flink (if the required memory to process a dataset goes over a certain threshold).\nIn this direction, it should be nice to introduce into Refine the concept of Catalog, in order to be able to properly manage metadata and interface with other existing repositories (like HCatalog and similar).\nIn this direction, a change to Refine datamodel could be also useful (e.g Apache Arrow could be a perfect fit).\n\nI know that comment s a little bit out of discussion but I'd like to stimulate some discussion in this sense... We could tale care of it...could it be possibile to move it under an\nofficial repo?\nOn 9 Nov 2017 18:15, \"Antonin Delpeuch\" notifications@github.com wrote:\n\n@ygherman https://github.com/ygherman hi, we are aware of this issue\nbut the RDF extension is not maintained by this team. I think this\nextension is looking for a maintainer actually.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1316#issuecomment-343225188,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ADmeJUnBWXF5KGJA15IRwGWSJfXX_Hj4ks5s0zMagaJpZM4QYUEx\n.\n. How can we ask for its ownership ?\n\nOn 9 Nov 2017 18:35, \"Ettore Rizza\" notifications@github.com wrote:\n\nhttps://github.com/fadmaa/grefine-rdf-extension\nThe author is @fmaali https://github.com/fmaali\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1316#issuecomment-343231528,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ADmeJfyASBcbuDdcp4tum5kSAAMkwjQzks5s0zfpgaJpZM4QYUEx\n.\n. @wetneb I was referring to an official ownership...the right term is repository transfer: https://help.github.com/articles/about-repository-transfers/. We made a fork at https://github.com/okkam-it/grefine-rdf-extension but we\nneed some time to align our code (that use a mavenized version of Refine)\nand to check if it works with Refine 2.7 and the code on the master branch.\nWe'll let you know once ready\n\nOn 10 November 2017 at 03:49, Martin Magdinier notifications@github.com\nwrote:\n\n@fpompermaier https://github.com/fpompermaier Please lets us know what\nis the new project URL so we can promote and redirect users.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1316#issuecomment-343359607,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ADmeJU5MrcE2Ni1Dh30fPDAOs-0Dms4dks5s07mugaJpZM4QYUEx\n.\n. We usually use those tags to (improperly) separate projects coming from\ndifferent customers..this because IMHO Refine lacks the cocept of workspace\n(i.e. a group of projects).\nI think that addition would be VERY useful\n\nOn 25 Nov 2017 11:09, \"Antonin Delpeuch\" notifications@github.com wrote:\n\n@wetneb commented on this pull request.\nVery nice feature! I agree with @ettorerizza\nhttps://github.com/ettorerizza that this should replace the \"subject\"\nfield in the project metadata. But we need to think about the migration of\nthe contents of the \"subject\" field into the tags: ideally, users who\ndecide to fill the \"subject\" fill with OR 2.8 should still have access to\nthis data in the next release.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/1357#pullrequestreview-78999656,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ADmeJUetrD_b37_HrOxLFdD1sa_frgFyks5s5-c9gaJpZM4QqI2P\n.\n. The simplest thing is to use google checkstyle:\nhttp://checkstyle.sourceforge.net/google_style.html\n\nOn 28 November 2017 at 01:52, Jacky notifications@github.com wrote:\n\n@fpompermaier https://github.com/fpompermaier Any recommendation on\nyour mind?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1359#issuecomment-347377847,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ADmeJZxt8TbAxZEzUjsjj4p_RN5tD08Pks5s61lKgaJpZM4QrZHv\n.\n. I just didn't know Refine was using a checkstyle. Is there any wiki about how to enable it on Eclipse and IntelliJ?. I totally agree. OpenRefine should be able to work on a sample of the data (when this is to big to fit in memory), create a set of operations to perform on a project and applying those operations on the remote framework like Spark/YARN/Flink/Apache Beam/etc. \nIn order to consider the process finished some validation rules need to be set. It those rules are not satisfied, another sample of the data (that doesn't satisfy those checks) is reported to the user in order to properly fix them with some additional operation. \nThus, the very first thing tho accomplish this goal is to be able to export the Refine history in a common format/language (independent from GREL/Jython/Python/etc) and reply all those operations in a batch program. \nI suggest to create a proper external library (e.g. OpenRefine-Language or OpenRefine-History) that will be imported by OpenRefine and all other framework that needs to reply the history (somehow). I'd keep things separated. But this is just my opinion.\nThe problem right now is that the history operations are quite hard to translate into a batch process.\nMaybe GREL could be quite easy to translate but Jython and other languages are more tricky...\nThe first thing I'd try is to extract GREL and the history-related classes into a separate project and see whether this lib could be imported and used to replay the project history in a separate batch engine (like spark or beam).. I think that all the confusion arises from the fact that this found received from Google to modernize the OpenRefine architecture is not very clear (or well documented).\nApart from a very brief discussion on the OpenRefine Google Group there's almost nothing about who \"we\"/\"ALL\" refer to, who is leading this thing and if the community could be involved in this decisional process/analysis.. @magdmartin excluded, all other commiters are new to OpenRefine: I'd be nice to know who is now involved in the project and what are its responsibility or educational background.\nIs there any requirements/design document that the community can see and discuss on or is everything already decided?\n\nThe only real change I saw is that now OpenRefine has some milestone set, but no clear idea if those milestone are strict or not..\n. I don't have access neither to https://github.com/orgs/OpenRefine/teams not https://github.com/orgs/OpenRefine/teams/core.\nI was subscribed to user group but not to dev. So problem fixed now..\nThanks for the link to https://github.com/OpenRefine/OpenRefine/projects, it's very useful. Actually my work is not a normal PR, it's more like an experimental branch related to a very first attempt of integation between OR and Apache Beam. I replicated almost all core classes because, CURRENTLY, it is bot possible to import them via maven. I just wanted to see which were the minimal set of classes required to support grel expressions...The goal of this repo is basically to have a playground where to prematurely highlight what will be the problems that we will encounter in the future. For example: should we support current history replication and grel expressions? Should we use the current row model or should we use something like Arrow?\nI don't know if I made myself clear.... Hi @wetneb , I don't think this is a fair review..we use it A LOT and it's super useful..also because there's no an easy way to rename all columns at once! We just need to fix the option button labels (that \nin our version are close to the circle) but for the rest I think this simple feature is very useful. I will list my thoughts on it:\n1. The fixed length importer suffer of the same identical problem (you have to provide a comma separated list of field names)\n2. Also if you rename all columns at once you have to provide the comma separated list of fields\n3. I don't think that producing an header as a separated list of column names is a very big problem (at least for Refine users) also because it remains optional\n4. The new option doesn't occupy \"a lot of space\" at all...in the current Refine version that space is just empty (there are check boxes on the right side that go far below the new option)\n. Do you mean if this error wasn't present in the previous version? Actually I don't know... ",
    "jackyq2015": "+1 for the maven. Also I noticed that there's some issue with the automation test. Not sure it can be fixed in a decent way after migrating to maven. \n. That's great! Thanks for your effort.\nFew comments:\n1. Your PR failed to build: https://github.com/OpenRefine/OpenRefine/pull/1054\n2. I think you can still use the \"refine\" script under root to start the openrefine. It provided more options.\n. Those file provided are huge. I can see the column count issue and it's hard to debug. can somebody provide a minimum data set on which can reproduce this problem? or this issue only happens when have large data set like this?. @wetneb Tried this file. it generated a project with 63 columns and 63 rows in row mode. But I did not see the missing rows. The problem I see is that the column and data are messed up. I think for that big file where appeared to import partially may due to the \"record mode\"?  \nI need to find out what exactly the issue before I can fix it.  . I think the problem is under records mode OR miscalculate the row dependency under some situation. It causes some root node were flagged as a child node. From the UI aspect, they are merged together. Not found the solution yet. will work out it.  . I fixed it. can somebody help to test it from https://github.com/jackyq2015/OpenRefine? If it works as expected, I will create a PR for this. cheers!. Just verify it under chrome \"Version 43.0.2357.132 m\", can't reproduce it. Since Chrome 11 is pretty old also JQuery has been upgraded at Sep of 2014, I suggest we close it.\nFeel free to reopen it if you still experience this issue on the new version.\n. I can take on this task. \nI will put a busy box at at the front end and put some synchronizer at the SortingRowVisitor class. what do you guys think? or I should submit separate PR?\n. WIll do. \nThe dialog.js utility provided dismiss function and it's by level(ie if you dismiss the level 2 and the level 3 will be also dismissed). I will add one function to allow it dismiss specified level so the \"Working\" dialog can survive after the main dialog is gone. \nFor the back end, I will change two methods:\npublic synchronized FilteredRows getAllRows() \npublic synchronized FilteredRecords getAllRecords() \nIt will handle all the cases which processing the rows and records to prevent the reentry. Hopeful it's not overkill.\nPlease suggest.\n. Put a PR #993 (prevent the multiple sorting) for this issue.\n. Is this still a problem for current code base? seems Tom already fixed it by putting a default encoding when it's empty.\nI tried the testing file under win 7. it works fine. If someone can provide the testing file and procedures to reproduce it, that will be helpful.\n. @tfmorris I tried the attached testing file with GREL reinterpret(value,\"utf-8\") and reinterpret(value,\"big5\"). Neither of them has the error as mentioned in original post. \nCould you please elaborate if you still think it's problem?\n. I downloaded the project and confirmed that the encode was wrongly set to us-ascii. In windows the same file when importing it is set to utf-8. Will come up a solution for this issue under Linux. \n. @magdmartin can you please run command locale at command line and paste the result? \n. For this particular file, adding JVM option -Dfile.encoding=Cp1252 when starting the OR can fix the encoding issue under Linux and mac OS. Linux use the UTF-8 by default but windows use Cp1252 by default. That's why this problem only happens on Linux. The file is read in a wrong encoding and thus writing incorrectly in the project data file. So there's no way if you try to use the \"reinterpret\" to override the encoding because the byte streams already got mess up and there's no way you can restore it.  Seems for the JDK6, you will get UnsupportedEncodingException when doing String.getBytes(). But for the JDK7, you won't. That's why the same can't be reproduced but reflected in another way.\nAdding this option is just a temp patch for this situation(BTW, it applies to other issues in the group related to German language encoding which are not off the top of my head. Adding the option to the configuration also possible if you are sure what kind of files you are dealing with). IMO, The permanent fix would be always detect the encoding of the file and read it in that encoding. Otherwise, it's likely mess up. For the file writing, the UTF-8 should be a good choice. \n. Move to 2.7\n. Claimed it! \nWill ignore the empty fields.\n. in current master code base, it works fine. Should we close it? @thadguidry \n. @manugarri This is different issue. Please create separate ticket. \n. can't reproduced under current dev master. The \"P\\Invoke\" was properly exported to Fusion table.  I suspect it's already fixed by Google. \nWe can close it.\n. Are we talking about the revision number at the right bottom on the start page?\nI think something like 2.6.123456 is expected. In that way, we can exactly identify which revision we are using from the repository. \n. Need some configuration file hosted as well as code change. Move to 2.7 milestone\n. Yes. In place since version 3.0 beta. Tested again @ultraklon PR, It works. But as he pointed out, seems there's some discrepancy between the test result from the UI and from the Unit testing. \nWill be looking into it.\n.  @ultraklon, for the PR #849 you created. To answer your concern for few null cell values, it's a side effect of the reuse row which aimed to address the performance. The post-operation column model is actually not consistent with the data itself because of the reuse. Ie, the cell[1], cell[2] as the old cell values should have been removed completely. Some cleanup has to be done for this operation on the column model and row model. Also releasing the null reference can free up some memory especially for big data set. \nAlso for the UT, the expected cell number will be differ from row to row since not all the 5 cells are fully populated. so the expectation should also get changed.  \nWill be working on the 2 changes mentioned above and hopefully can submit the PR this week.\n. PR created. It was based on the change @ultraklon made.\n. dup of #1138 \n. How about ExtJS? In terms of widget support, it's better. I think it's important for the data presentation and visualization.  But its footprint is heavier than AngularJS\nhttp://www.techferry.com/articles/ExtJS-vs-AngularJS.html\nit's GNU GPL license v3. Since openRefine is BSD, it is compatible. \n. I believe that you missed the step \"Edit the Cell\"-->\"Common Transform\"-->\"To Number\". The idea that the engine dose not aware of the numeric type unless you tell it so. Suggest to close if there is no further information to indicate otherwise.\n. @Rookev, According to this:\nhttp://stackoverflow.com/questions/23725085/failed-to-load-resource-neterr-network-io-suspended\nIt might caused by the ajax call. \nIf you can exactly put the steps to reproduce this error, I am happy to pin point its root cause and fix it. \n. @hvmarck The get operation is not allowed. so http://127.0.0.1:3333/command/core/set-preference?project=1801073216107&name=xxxxx&value=vvvvvv will generate exception:\njava.lang.UnsupportedOperationException\n    at com.google.refine.commands.Command.doGet(Command.java:86)\n    at com.google.refine.RefineServlet.service(RefineServlet.java:170)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n    at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n    at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n    at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n    at org.mortbay.jetty.Server.handle(Server.java:326)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:923)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:547)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nOr you mean you do it in POST? Please clarify. \n. Close it since it can't be reproduced. Please reopen it if have any further concern. \n. @kevinwong15 , could you please explain in more detail what did you do and what exception stack you got from the back end if there's any?\n. The latest version is 3.12\nhttps://poi.apache.org/download.html#POI-3.12\nShould we update to this version?\n. done. PR #1029 created for this.\n. Put new tag \"encoding\" to this issue and will be addressed with the ones with the same tag in next release. \n. fixed the ant build\n. Agree. The feature branch make perfect sense.\nOn Sun, Jan 25, 2015, 12:19 PM Tom Morris notifications@github.com wrote:\n\nThanks for fixing the things that @maniksurtani\nhttps://github.com/maniksurtani pointed out. I agree with all his\ncomments.\nThe pull request also needs to be done from a feature branch, not from\nyour forked master, because otherwise it's susceptible to getting polluted\nby unintended later changes (e.g. your removal of openjdk6 from the Travis\nconfig which might have been intended to be a purely local change). Each\nfeature branch/pull request should include a single coherent set of changes\nwithout any unrelated changes.\nWe won't be updating any dependencies until after the 2.6 release, but\nthat should happen very soon. At that point I'll be doing a major\ndependency update and probably another relatively quick release, so these\ncode changes will come in handy then.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/934#issuecomment-71381878.\n. Close it for now due to the file confliction. Separated PR needed to do the library upgrade. . add the JDK8 support when removing the JDK6. Also fixed some Travis UT exception by using the LinkedHashMap instead of HashMap\n. I fixed this one my fork, this is is the PR:\nhttps://github.com/OpenRefine/OpenRefine/pull/937\n\nCan someone review it and merge into trunk?\n. The problem may be from somewhere else.\nI will try reproduce and check the exception stack and the memory usage.\nOn Jan 30, 2015 6:41 AM, \"Martin Graham\" notifications@github.com wrote:\n\nJava memory allocation soars past whatever I can max at (-Xmx1200M on a\n32-bit Windows 7 OS, Java 8u31) when trying to open what would seem to be\n(at first glance) a small .xslx file\nIn truth, it's a file of 10 worksheets, 35,000 rows by 4 columns each =\n1.4 million cells in total, so it's well compressed in the .xslx format.\nHowever, if I convert it to the old .xls format, the file becomes bigger,\nbut is now manageable by OpenRefine.\nOne thing I did notice is that the POI documentation (\nhttp://poi.apache.org/spreadsheet/quick-guide.html#FileInputStream)\nstates that using InputStreams rather than Files can increase memory\nfootprint, and poking about in the code (ExcelImporter.java) it seems to be\nusing InputStreams rather than Files to open worksheets etc, I wonder if\nchanging to use Files for excel .xslx data is possible and would make a\ndifference?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/940.\n. I upgraded the POI to updated version 3.11, still got:\n\n10:22:56.705 [                   refine] POST /command/core/get-importing-job-status (990ms)\n10:22:57.735 [                   refine] POST /command/core/get-importing-job-status (1030ms)\n10:22:58.733 [                   refine] POST /command/core/get-importing-job-status (998ms)\n10:23:00.690 [                   refine] POST /command/core/get-importing-job-status (1957ms)\n10:23:05.534 [                   refine] POST /command/core/get-importing-job-status (4844ms)\nException in thread \"Thread-5\" 10:23:05.899 [                   refine] POST /command/core/get-importing-job-status (365ms)\njava.lang.OutOfMemoryError: GC overhead limit exceeded10:23:05.901 [                   refine] POST /command/core/get-importing-job-status (2ms)\n10:23:05.904 [                   refine] POST /command/core/get-importing-job-status (3ms)\n        at org.apache.xmlbeans.impl.values.NamespaceContext$NamespaceContextStack.(NamespaceContext.java:78)10:23:05.908 [                   refine] POST /command/core/get-importing-job-status (4ms)\n10:23:05.910 [                   refine] POST /command/core/get-importing-job-status (2ms)\n        at org.apache.xmlbeans.impl.values.NamespaceContext$NamespaceContextStack.(NamespaceContext.java:75)\n        at org.apache.xmlbeans.impl.values.NamespaceContext.getNamespaceContextStack(NamespaceContext.java:98)\n        at org.apache.xmlbeans.impl.values.NamespaceContext.push(NamespaceContext.java:106)\n        at org.apache.xmlbeans.impl.values.XmlObjectBase.check_dated(XmlObjectBase.java:1273)\n        at org.apache.xmlbeans.impl.values.XmlObjectBase.stringValue(XmlObjectBase.java:1484)\n        at org.apache.xmlbeans.impl.values.XmlObjectBase.getStringValue(XmlObjectBase.java:1492)\n        at org.openxmlformats.schemas.spreadsheetml.x2006.main.impl.CTRstImpl.getT(Unknown Source)\n        at org.apache.poi.xssf.usermodel.XSSFRichTextString.getString(XSSFRichTextString.java:297)\n        at org.apache.poi.xssf.usermodel.XSSFCell.getStringCellValue(XSSFCell.java:247)\n        at com.google.refine.importers.ExcelImporter.extractCell(ExcelImporter.java:252)\n        at com.google.refine.importers.ExcelImporter.extractCell(ExcelImporter.java:262)\n        at com.google.refine.importers.ExcelImporter$1.getNextRowOfCells(ExcelImporter.java:201)\n        at com.google.refine.importers.TabularImportingParserBase.readTable(TabularImportingParserBase.java:132)\n        at com.google.refine.importers.ExcelImporter.parseOneFile(ExcelImporter.java:210)\n        at com.google.refine.importers.ImportingParserBase.parseOneFile(ImportingParserBase.java:112)\n        at com.google.refine.importers.ImportingParserBase.parse(ImportingParserBase.java:83)\n        at com.google.refine.importing.ImportingUtilities.createProjectSynchronously(ImportingUtilities.java:1017)\n        at com.google.refine.importing.ImportingUtilities.access$200(ImportingUtilities.java:92)\n        at com.google.refine.importing.ImportingUtilities$7.run(ImportingUtilities.java:992)\n10:23:06.233 [                   refine] POST /command/core/get-importing-job-status (323ms)\n10:23:07.234 [                   refine] POST /command/core/get-importing-job-status (1001ms)\nTerminate batch job (Y/N)?\n^C\nI think the current XSSF parser is using the DOM instead of SAX. Another SAX implementation for excel import may fix the issue: \nhttp://poi.apache.org/spreadsheet/how-to.html#xssf_sax_api\n. The version I am using the 2.6 Beta. That might be reason. Will keep watching the new version if it still happens.\nThe evidence is that I can find *.corrupted file in my workspace. when I trace back from the code, the auto saver most likely the reason. My understanding is that even the auto saver and user initiated save use the same code, but the timing matters. The auto saver could happen at any time point even when some task is running. In that case may cause issue.\n. I am fine to merge this.. We had discussion in the mailing list here:\nhttps://groups.google.com/forum/#!topic/openrefine-dev/E8V1ogpzyqo\nWe may need to extract reusable classes from freebase. @MattBlissett do you have time to do that?\n. I believe that this can be fixed by updating the version of poi library. I already did this upgrade on my own fork:\nhttps://github.com/jackyq2015/OpenRefine/\n@daviddou82, If you can use the same file to test against the build from above, that will be helpful.\n. PR merged. I never used Mac.  But you can try to use chrome. Most likely your problem will be gone. Freebase reconciliation service is retiring so that URL is not accessible. Chrome can deal with silently but I think Safari(or FF) can't. \n. Sorry forgot to mention that it works if it is deployed under windows 7 and access from 127.0.01. \nBut if deploy under Linux and access from another windows 7  firefox, problem happens. There is no error log displayed on FF console.  It happens from the current master back to the the commit made by @DavidLeoni (maybe even more back but I have not tested more). But for the way of deployment, the following is good:\ngood: [c68c1bb2b11a2c4894c0a7bbb7ec9b048a7367f4] Upgrade to Clojure 1.5.1 & switch to clojure-slim JAR - #792\nI tried some bisect. But the result was confusing. I don't think it's correct. so I won't post here.\n. Here is the output from the console running firefox, back end Linux. Looks the load-language are loaded again and again from some reason.\n09:59:26.581 [                   refine] POST /command/core/load-language (4093ms)\n09:59:26.970 [                   refine] POST /command/core/get-importing-configuration (389ms)\n09:59:27.230 [                   refine] GET /command/core/get-all-project-metadata (260ms)\n09:59:27.356 [                   refine] GET /command/core/get-languages (126ms)\n09:59:27.453 [                   refine] GET /command/core/get-version (97ms)\n09:59:30.610 [                   refine] POST /command/core/load-language (3157ms)\n09:59:30.666 [                   refine] GET /command/core/get-preference (56ms)\n09:59:30.773 [                   refine] GET /command/core/get-project-metadata (107ms)\n09:59:30.803 [                  project] Loaded project 1783721362509 from disk in 0 sec(s) (30ms)\n09:59:30.853 [                   refine] GET /command/core/get-models (50ms)\n09:59:31.252 [                   refine] POST /command/core/get-rows (399ms)\n09:59:47.015 [                   refine] POST /command/core/load-language (15763ms)         <--- why load again?\n09:59:47.081 [                   refine] GET /command/core/get-preference (66ms)\n09:59:48.163 [                   refine] GET /command/core/get-project-metadata (1082ms)\n09:59:48.228 [                   refine] GET /command/core/get-models (65ms)\n09:59:48.449 [                   refine] POST /command/core/get-rows (221ms)\n09:59:48.903 [                   refine] POST /command/core/load-language (454ms)           <-- another time\n09:59:48.968 [                   refine] GET /command/core/get-preference (65ms)\n09:59:50.059 [                   refine] GET /command/core/get-project-metadata (1091ms)\n09:59:50.126 [                   refine] GET /command/core/get-models (67ms)\n09:59:50.343 [                   refine] POST /command/core/get-rows (217ms)\n09:59:50.799 [                   refine] POST /command/core/load-language (456ms)       <-- another time\n09:59:50.856 [                   refine] GET /command/core/get-preference (57ms)\n09:59:51.954 [                   refine] GET /command/core/get-project-metadata (1098ms)\n09:59:52.029 [                   refine] GET /command/core/get-models (75ms)\n09:59:52.245 [                   refine] POST /command/core/get-rows (216ms)\n09:59:52.675 [                   refine] POST /command/core/load-language (430ms)       <-- another time\n09:59:52.834 [                   refine] GET /command/core/get-preference (159ms)\n09:59:53.916 [                   refine] GET /command/core/get-project-metadata (1082ms)\n09:59:54.034 [                   refine] GET /command/core/get-models (118ms)\n@tfmorris, could you please take a look? From my bisect, it seems that this start from the Jquery upgrade. 4f2ebed676c201ca050e1dda0c1a9b561342ad5a is last good commit. I checked the previous one 094562e54b5c6a9af0df151f28996e860081bd47 which is a upgrade of JQuery. I know you may not have a Linux box to test. But If you can give some clue what could be the cause, I will go from there.\n. It only happens when run it from Linux Server + Win 7 FF. If you run OpenRefine on Win 7 it works fine. Maybe that's the reason it hasn't be discovered for such long time. Also if you use chrome, it works fine.\nwhen it \"hangs\" I can see the spinning at the left panel while the data got loaded at right panel. Firebug shows it tried to load the page and do \"resize\", calculating the padding etc(without exception). After running for a while, firefox will ask if you want to debug it since it won't response. But each time it stops at different place.  From the back end, I can see it keep getting \"Get \" ajax requests as I paste above.\nI guess there might be some event is handled differently in FF + JQuery. \n. I tried to upgrade to latest version of jquery, jquery-ui, apache and firefox. The problem still there as long as I use the htpasswd authentication. Finally I roll back the jquery update to the old one( version before Sep 18, 2013), then It behave normally. \nFor those who may come across the similar issue as me.\n. I closed it because it won't happen on current development sources. It only happens on our hosting environment which the deployment and access path is different(the OpenRefine core part is the same though). Our deployment environment is: Ubuntu + Apache + Basic Authentication. The problem only happens when the following combination is used:\n1. Firefox (Windows or Linux)\n2. passing the user name and password in the URL for Basic Authentication. If you type it, it won't happen\nIt took quite time to find out the problem. I will rather to treat it as a deployment related than the code based related. Since we will be upgrading the Apache Basic Authentication to SSL proxy, so I downgrade the version of JQuery for now.\n. Not sure if it's the same issue. My issue only happens on a OpenRefine behind a password-protected Apache server. You can go back to check your console to see if it is keeping loading some page or just hang somewhere.\nBut it seems Refine has more issue with FF than chrome.\n. There is a python library for this:\nhttps://github.com/PaulMakepeace/refine-client-py/\nNot sure if this is what you are looking for. But you need have your openRefine server running when you run this. Basically it's a agent program to manipulate the web server. \n. If you could share the dataset you tried to load that will be helpful.\n. under windows 7, 64 bit, 37.0.1 firefox, I have the same issue. chrome Version 42.0.2311.90 m works fine\n. It turned out that the label Id of the \"import from local\" and \"import from project\" is the same.  FF and firefox handle this situation differently.\nI will fix it and create a PR\n. Yes. that's an known problem and already addressed in trunk. refine script need to support JDK 8. Why don't you just use 2.6 instead of 2.5?\n. Just curiosity how come you have so many options(90000)? what kind of data set are you dealing with?\n. Agreed! Downgrading the  guava to 1.3 solves the problem. \n. I will help with this to create a PR to fix this.\n. PR created. \nThe code was deployed under:\nhttp://208.75.74.214:3333/\nPlease help to verify if it works properly and also check the reconsideration part. If need to install some extension, let me know.\n. Current gdata is from:\nhttps://gdata-java-client.googlecode.com/files/gdata-src.java-1.47.1.zip\nI think migrating the gdata library can solve this problem:\nhttps://github.com/google/google-api-java-client\nAnother option is to introduce the apache camle to handle different import/export requirement. It also has the google drive component:\nhttp://camel.apache.org/googledrive.html\nAny comment?\n. @tfmorris can you assign this ticket to me? Just make sure community members can focus on different ticket to prevent duplicate work.\nAlso I think this ticket depends on #970. You can assign that to me so you can put your energy on more important thing.\n. @tfmorris Glad to hear that you got things fixed. Please Let me know if you find some 2.6 tickets that I can help with. I will also try to \"claiming\"issues from the 2.6 milestone.\nRegarding this issue, I did some exploration to integrate the OpenRefine with Apache Camel. I would like to post my findings to the mailing list to get some feed back.\n. It's ok as long as it got fixed. I deleted my fix branch already\n. Seems the jython library can not be loaded.\nCheck if you have the file in place under:\nOpenRefine\\extensions\\jython\\module\\MOD-INF\\lib\\jython-standalone-2.5.3.jar\n. If you build the source yourself, try this:\ncd OpenRefine/extensions\nant\nthen run it again.\n. Agree, we should include the source code also in order to review and distribute.\n@lispc Can you please commit the source code for the algorithm?\n. My understanding is that \"invert\" will do a \"include others item except this one\". so there's nothing to invert if no item is selected. The function is like in file browser you can do a \"Invert Selection\"operation. But it only allow you start with one item. correct me if I am wrong.\n. @esiegerman I tried the given steps. But after I did the step 5, it marks the selected in red and shows the \"change\", \"invert\" and \"reset\" link without \"already inverted\" showed. For me, it's normal. can you paste a screenshot for it? Maybe you come across different issue.\n. @esiegerman Still, I can't reproduce it even I did the step 4 as you corrected.\nThe deselect(by click it) will also set the mode to include. Actually, it only allow you \"include\" either all items or only 1 items, there is no multi selection.\n. I suggest to cast in explicitly like this:\nreturn str(value==\"2acdf11b911faba3183630de42a22a70)\";\nTo tweak the expression engine to change the default behavior of the Jython could lead to problem if you use it in other ways.\n. Please check this:\nhttp://stackoverflow.com/questions/16279212/how-to-use-dot-notation-for-dict-in-python\nI believe that even upgrading the version of Jython to 2.7 won't have this feature.\n. Exporting in UTF-8  covers (almost) all the characters, punctuations, and symbols in the world. \nhttp://www.w3schools.com/tags/ref_charactersets.asp\nI will make the change to implement this if there is no objection.\n. That's good point. I also feel annoying when we run the Refine under hosted\nLinux server. It won't cause issue, but I agree the configuration way to\ndisable this\nOn Fri, Jul 10, 2015 at 5:06 PM, Opennat notifications@github.com wrote:\n\nDefault option not always (almost never for me) needed, as it only can\nopen Refine in default browser (it could differ from browser to work with\nRefine).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1022.\n. Noted. Will check how to implement this.\n. From which aspects do you think the current implementation of  Refine need\nto be replaced? Or what's the benefit we can gain from the Clusterize.js\nhttp://nexts.github.io/Clusterize.js/ you mentioned?\n\nOn Fri, Jul 10, 2015 at 7:32 PM, Opennat notifications@github.com wrote:\n\nWith (or instead of) current pagination\nhttp://nexts.github.io/Clusterize.js/\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1027.\n. I just imported this file on master build. it parsed the zip and created up to Column 139. which revision is the 2.6 RC1 built from?\n. Indeed the RC1 behave differently with the current master and 2.6 Beta when importing the given file. The source and binary of RC1 can be found at:\nhttps://github.com/OpenRefine/OpenRefine/releases/tag/v2.6-rc1\n. There is some inconsistency. i am using current dev master on windows 7. what I saw is the last column is \"Column 139\". From \"Column 31\" until Column 139\" are all empty and somehow the parser get it wrongly. Is there someone else can test the same file on other platform such as Mac? \n. Very sharp eyes and very thorough explanation!\n. You can remove the extensions/freebase folder, also remove the line in the controller.js\n      \"scripts/reconciliation/freebase-query-panel.js\",\n\nFreebase supposed to be removed completely as my understanding and it is a pending issue in next release(2.6). Will confirm with Tom regarding this.\n. What error did you see when you say it is not working? Can you also provide\nthe version of the openrefine and how you run it\nOn Mon, Jul 27, 2015, 03:44 ScientificProgressGoesBoing \nnotifications@github.com wrote:\n\nTried did advice but program did not work anymore. Would be grateful for\nanother quick fix because the error message keeps popping up and changing\ntabs which is annoying. Thank you for your work! I appreciate the\ndevelopment effort put into this program!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1043#issuecomment-125112287\n.\n. PR created.\n\nThe code was deployed under:\nhttp://208.75.74.214:3333/\nPlease help to verify if it works properly and also check the reconsideration part. If need to install some extension, let me know.\n. Is this a duplipcate of issue #512?\nStore file source in each row not working for JSON from URL #512\nIf so, please close it.\n. I see. That make sense.\nOn Mon, Aug 10, 2015, 7:09 AM Nicky Nicolson notifications@github.com\nwrote:\n\n512 https://github.com/OpenRefine/OpenRefine/issues/512 describes\nstoring file source as a column in the data (and a problem whereby this is\nnot populated for JSON from URL). I'd like the file source(s) to be stored\nas project metadata (not data columns) and output when the project is saved\nin Open Refine project format, in the metadata.json file.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1045#issuecomment-129409435\n.\n. That fix is for encoding issue. but StringEscapeUtils on the same method will fix the escape issue. That's why I self-assigned this issue to me. But we are actively working on the 2.6 milestone. The actual fix / merge will be postponed a little bit.\n\nFor the import, I am not quite sure whether we have the similar issue. If someone can help out to verify it that will be great!\n. thanks for the file. Did you see some error message at back end\uff1f\nOn Mon, Aug 10, 2015, 8:04 AM a-blohm notifications@github.com wrote:\n\nSure. I can't attach it to this comment, but you can download the files\nfrom http://alexanderblohm.de/jnkdnfkjndfgn3iuu4u3sdf/json.zip. Will be\navailable for at least a month or so.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1052#issuecomment-129423354\n.\n. seems that the butterfly library still missing. Could you please take a look?\n. I see. Can you check the .Travis.yml under the root dir and change the\nrefine script to instruct  Travis to build it as the steps you mentioned?\n\nOn Tue, Sep 15, 2015, 1:07 AM Junwei Wang notifications@github.com wrote:\n\n@jackyq2015 https://github.com/jackyq2015 It is because butterfly\nlibrary is not in Maven repository.\nTest the following command\nmvn initialize\nmvn clean compile\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/1054#issuecomment-140284069\n.\n. @hvmarck That's a interesting hack. what's the proper way to handle the comment according to your experience? I think it's a standalone plugin which you can apply the comment to any selected step chosen from the history panel. \n\nJust curiosity if you don't mind. what's the other functionalities of your internal plugin? \n. can't reproduce it under windows7 + chrome(Version 44.0.2403.130 m). Can anyone confirm this only happens on the Mac?\n. @thadguidry Tried, the same.\nBut it works if I deploy to Linux machine. weird. \n. My bad. I realized some of the operations can not be extracted. \n. can't find 15B30a. Is it sha? I tried the updated version  8b4a1d on Mac it start fine. \n. duplicate of #1138 \nPR already created\n. Added the inline comments. But I agree that encoding detector such as ICU4J should be added into OR. Confirmed it's a bug. The extra empty lines seem are generated by one wb:data node.\nDo you have similar issue for other xml files?\n. @joewiz May I know how did you pre-process the XML in order to make it work?\n. @rufuspollock Is there any online demo I can try? You mentioned you contributed to both. Just wondering what happened to W3 one? any comparison for the two or other standard? Thanks . @rufuspollock I saw there is a \"rdfType\" in the \"Table Schema spce\". Just wondering if there is any further detail around this? As far as I know, the CSVW covers the RDF. \nAnother question is that the criteria to choice between implementing the \"Data Package spec\" and \"Table Schema spec\" for the OpenRefine. Is it feasible that we start with the \"Table Schema spec\" firstly then move to  \"Data Package spec\"? How hard it will be?\nThanks!. @wetneb I am not try to compare the Table Schema and Data Package. I understand they are different things. My question is that does it make sense to do the \"table schema\" firstly then move to \"data package\" since  it can be put in the \"data package\" later on. Also Thad and I share the same concern the limitation of the rdf support.. not fully fixed. Also have issue with undo/redo since the original table structure is altered after transpose. \n. I believe that's the legacy project names. You can change the project setting and create a PR for this.\n. Seems it caused by a bug of appveyor:\nhttps://github.com/ogrisel/python-appveyor-demo/issues/6\n. How do you create the project? Did you use the \"Choose file\" button to select the csv file or you use the URL? . I cannot reproduce the issue. The file path you selected should not has\nbeen taken the full path. Should be file name only. Did you copy paste the\nfull path\nOn Mon, Apr 24, 2017, 11:16 jls4dtna notifications@github.com wrote:\n\nYou can simply retain the drive letter but remove the colon.\ne.g. C:\\foo\\bat\\bar\\data.xml is mapped to\n...\\raw-data\\C\\foo\\bat\\bar\\data.xml\nFrom: cameronstewart [mailto:notifications@github.com]\nSent: Saturday, April 22, 2017 11:04 PM\nTo: OpenRefine/OpenRefine\nCc: Stamper, Larry (164-Extern-Larry); Comment\nSubject: Re: [OpenRefine/OpenRefine] error uploading data from csv,\nperhaps due to colon included in temp filename (#1142)\nSo how would you fix this considering everything will be somewhere on a\ndrive so a drive: will be replicated Irrespective of where it is?\nOn Sat, 22 Apr 2017 at 6:20 am, jls4dtna notifications@github.com\nwrote:\n\nI have this same issue, and it has nothing to do with the value of TMP\nor\nTEMP. Using the Browse button, I selected the file Librarygood.xml,\nwhich\nwas in my Windows 7 home directory C:\\Users\\STAMPEL. When I tried to\nload\nit, I got this error, much like the others received:\n[image: image]\n<\nhttps://cloud.githubusercontent.com/assets/27868729/25293817/456ad426-2691-11e7-8f60-8bff136d2a79.png>\nIt seems clear that OpenRefine was trying to copy the xml file in its\nown\nprivate temp directory\n'C:\\Users\\STAMPEL\\AppData\\Local\\Temp\\Jetty_127_0_0_1_3333_webapp____4ulpc9\\import\\2\\raw-data',\nreplicating the original file's path - i.e., C:\\Users\\STAMPEL.\nThe problem is that the colon character is not allowed in Windows file\nnames. The colon needs to be removed or replaced with a legal character.\nThat should fix the issue.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\n<\nhttps://github.com/OpenRefine/OpenRefine/issues/1142#issuecomment-296296306>,\nor mute the thread\n<\nhttps://github.com/notifications/unsubscribe-auth/AAZibTkUSEyRaE4L3XHsAxVnLa0Nf0MDks5ryQ99gaJpZM4JNdNI>\n.\n\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub<\nhttps://github.com/OpenRefine/OpenRefine/issues/1142#issuecomment-296421999>,\nor mute the thread<\nhttps://github.com/notifications/unsubscribe-auth/Aak-OcO0NMks1pRQozVBYswBmLishTozks5ryunngaJpZM4JNdNI>.\nIf you are not the addressee, please inform us immediately that you have\nreceived this e-mail by mistake, and delete it. We thank you for your\nsupport.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1142#issuecomment-296701816,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AKCGi0Oxo-J5KOTVRoeeTiIsw9wHRyh1ks5rzLzTgaJpZM4JNdNI\n.\n. Seems there are some inconsistency between chrome and IE for the file selector component. I can put a patch on either front end or back end to extract the file name only. But MS is moving to Edge from IE, not sure it is still worth fixing it.. There is a user option for supervisord.\n\nuser=juan\nThe supervisord can not locate the project location since it's running\nunder another user.\nOn Wed, Aug 31, 2016 at 8:53 AM, juanmas07 notifications@github.com wrote:\n\nI've created a project in refine, I've ran the program\n(/home/juan/openrefine-2.6-rc.2/) with the command:\n./refine -i 0.0.0.0\nNow, I'm using the next supervisor script to run it, but the project does\nnot show up (unless I cd to the openrefine directory, and run it from there)\n[program:refine]\ndirectory=/home/juan/openrefine-2.6-rc.2/\ncommand=/home/juan/openrefine-2.6-rc.2/refine -i 0.0.0.0\nautostart=true\nautorestart=true\nSo I cannot use supervisor to see the current projects. Of course I can\nexport / import but that's not the idea\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1152, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AKCGiyVXmIm3wuodAUlkvw7vzzvf419-ks5qlXlegaJpZM4Jxk5M\n.\n. If I am not mistaken, the cross is designed for match based on string , not number. If you need cross a number, a added column with converted number should required.. @ettorerizza According to code from ImporterUtilities.java:\n```\nstatic public Serializable parseCellValue(String text) {\n        if (text.length() > 0) {\n            String text2 = text.trim();\n            if (text2.length() > 0) {\n                try {\n                    return Long.parseLong(text2);\n                } catch (NumberFormatException e) {\n                }\n\n            try {\n                double d = Double.parseDouble(text2);\n                if (!Double.isInfinite(d) && !Double.isNaN(d)) {\n                    return d;\n                }\n            } catch (NumberFormatException e) {\n            }\n        }\n    }\n    return text;\n}\n\n```\nThe cellType could be Long and Double if it's numeric. In that case, the cross cann't behave properly unless you can make sure the types are the same. Also for Double if you have the same type, the scale still an issue. You may get it right under some situation but the result is not deterministic. \nFor the GREL forEach issue, you can refer to #1204 \n. @ettorerizza Will take a look. could you please paste the full exception stack?. The output format out java -version for Java 9 is different from 6,7,8. That's where the problem comes from. Need to change the startup script to address this. . can you share the link of that zip file?\nBasically, the targeted project dump should be in zip format. Either the file is corrupted or it was not exported properly.  . Close it for now since no further information provided. . There was some package issue on my Mac. Just fixed it and built a signed one. The old one was replaced from the 2.7.rc.1 release.. I know it is not small effort. But if you can split your project into several coherent PRs that will make it feasibility to merge into the master. . Though or only accept 2 arguments, but you can use multiple boolean operator to achieve what you need. something like this:\nor(or(isNotNull(cells['column1'].value.match(/A/)),isNotNull(cells['column1'].value.match(/B/))),isNotNull(cells['column1'].value.match(/C/))). Wiki mentioned above fixed. . Can you describe how you do the upload and the full path of the file?. \nwhat's your JAVA_HOME? seems a wrong JAVA_HOME setting.. please fix the CI errors. I mean for #1181, can you import a multi-sheet xls and follow the procedure to verify that your fix works? . I haven't used chromebooks or Android PC. I would image that OR can run\nunder Android PC as a java application. For Chromebooks, maybe need much\nmore work to embed the server side code into the Chrome.\nAs long as there's fair amount of user base it make sense.\nOn Tue, May 30, 2017 at 4:19 AM, Jens Willmer notifications@github.com\nwrote:\n\nWith the Chromebooks, Android PC's and the world moving applications to\nthe WWW it would be nice to be able to have a version of OpenRefine that\nruns on Chrome/Android.\nI don't know how easy it is to (auto?) transform the Java code to\nJavaScript for a Chrome extension but I guess for an Android app the\nbiggest workload might be the GUI.\nPlease feel free to use this issue to discuss about the possibilities as\nwell as if it might make sense. I obviously had the idea as I was working\non my Chromebook.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1196, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AKCGi9a_TnyWqm5NsUPNmsTTNoSDzzYuks5r-9EBgaJpZM4Np9wm\n.\n. can you  try to add -Dfile.encoding=UTF-8 to java command options to enforce the encoding?. @Yuutakasan  Given your description, your file is not properly decoded as utf8. that's why I asked you to enforce it. Please note that system cannot 100% accurate to detect the encoding of random stream. There is some library like icu4j can help to improve the accuracy.  Actually there is a PR(not merged yet) to introduce it. If you want to wet your hand, you can create your own branch and merge the PR to your own branch and have a try.. can you please add the encoding switch I provided above and try again? \n\n. I just tired on refresh windows 10 64 bit Java 8. It works well. Are you sure you downloaded the binary here not the source code? . Are you sure REFINE_AUTOSAVE_PERIOD is being used? seems it is hard-coded. For current OR deployment, json and jsonp make no difference. I know OR have a unfinished broker model to separate the front end and engine itself. that could be the reason that jsonp was used for wider support. . Pushed the change f03be76, it covers:\n\nTake the cell value instead of the cell as the first argument\nAdded the necessary action to flush the stale cache. They were missed before. \n. @wetneb Will add few test case for non-literal value.. @felixlohmeier can you please elaborate the symptom of \" breaks existing user transformation history\"? Definitely we can address it. Either make it back compatible or fix the history issue you refer to.. I will review the document related to the cross function to make it consistent and can have the code also support cell if necessary. \n\nFor the transformation, I can still redo and undo between cross function. could you please give an example?\nFor the logic of cache, the key of the cache is based on:\n fromProject + \";\" + fromColumn + \";\" + toProject + \";\" + toColumn; \nwhich means:\n- The cache will only do once if there is no change to columns\n- The cache will be done when first cross get called(in your case, on the mary cell)\n- The cross function does not and should not aware of the custom operation(in your case, split). It will treat \"mary, john\" as one single value. The reason it can be parsed properly is that mary AND john already cached properly. so if you put a separate row which contains anne, the \"anne,mary\" can be parsed properly. But the \"mary,john\" will be also get cached since cache is based on the column.. Will find some time this weekend for the compatibility issue. \nFor the cache logic I think the good practice is that always do the cross on normalized data(source and destination) rather than row data to prevent the unpredictable  result. Also in this way you can leverage the cache efficiently.. @ettorerizza \nAdded the backward compatibility for cross function as discussed. https://github.com/OpenRefine/OpenRefine/commit/4950d2907444990bd5584dea91179c5a7ff3c8f3\nCould you please test from the head?. @felixlohmeier corrected the description. fixed . It is feasible that just set the Agent as the chrome? cannot see the benefit to set OpenRefine's own Agent. It is the opposite.. @ostephens You probably right. but the hard part is not to add those http headers from OpenRefine IMO. The hard part is to have 3rd party to accept them and support them. But sure how others similar software deal with this situation. They should come across the same issue. It is worthy to take a look. I have a little concern. I haven't received my payment after more than 2 months. I was told that \"the payment though cheque is rare\". . @denim2x any progress on this?. @ettorerizza I think one of the reason could be the requirement is not so clear for him.I can take on this if you can narrow down the required fields. There are some dynamic data such as row number I don't think it's good to put as meta data.  My thought is to pick some from:\nTitle\nCreator\nSubject\nDescription\nPublisher\nContributor\nDate\nType\nFormat\nIdentifier\nSource\nLanguage\nRelation\nCoverage\nRights\nAnd we also store the \"import options\" as a json data filed to the meta data. \nLayout could be adding the columns for some fields on the index page and also provide a \"About\" link to edit all the supported meta data.. I finished most of the work as we discussed above. Those are some screenshots that I want to share with you firstly:\n- Index page(client name is user defined)\n\n\n\nTo display all the meta and edit them\n\n\n\nTo define the user metadata  from preference page\n\n\n\nThere is some minor issue to populate the data etc. will fix in few days.  Also just a heads up, we plan to add columnar metadata to support further development. that will be added in near future as separate PR.. @ettorerizza Glad you like it. :) There is no support for \"search project by metadata\" yet. But I will try to add  a function to sort by header. is that work for you?. @thadguidry when click edit, a popup box will show and you can enter the new value. It is the same way we edit the preference value. Ideally, we should use a pretty popup box including the height and other css. But I would like to tackle it as a separate task for all the prompt/alert function.. @ettorerizza sort added\n@magdmartin pref link added\ndone by PR #1301. @ettorerizza just put a fix. please try again.. After pull, did you run ant? I tried on Linux, windows and Mac. there is no issue to start. Can you move the dialog to the left a little bit?\nAlso, when change the userMetaData, the server has to restart to load it since it is a singleton. . @ettorerizza Thanks for the test. created issue #1306, #1307, #1308 to track them separately . @ettorerizza I fixed the bugs you mentioned above except \"user metadata\" one and the row number one. The row number one is related to the singleton refresh. It was populated but has to be reloaded.\nI disabled some metadata which should NOT be changed manually.\ncould you please create a separate  issue with detail to reproduce the user metadata issue? Thanks\nOne off the topic question, is there any way that I can drag  back and forth a gif file? . The width css is not taken for some reason after apply of the table sorter. Will try to fix it to make it nicer. But as Thad said, the long term solution will be the resizeable table.. @ettorerizza I fixed the table width issue. Not perfect as I said. but much better. :)\nAlso the refresh issue was fixed. Please have a try.. @wetneb Thanks for the feedback. I agreed all of them. I will be working on the improvement.\nFor that 3 mysterious link at left hand, I think we can just give them a label instead of hiding it. Also we can remove the header borders for those 3. \nFor the \"about\" view, are you suggesting to remove the background of \"edit\" button to make it fit?\nAny thoughts? . Pushed the code. It is much better now! cheers \n. I would suggest to close this issue if there is no objection. Please raise issue if there is any.. @weblate please see below:\n do we really want to display the fields with raw JSON? Especially the one with column metadata (as it will currently remain empty and should be eventually manipulated from the project view anyway?)\nA: For column metadata, I would imagine we may need an extra view just for handling its complexity. For the import option, since it's read only. It should be sufficient for now\nTHAD AGREES WITH ANSWER , WE WANT A DIFFERENT VIEW FOR COLUMNAR META EDITS\n the \"Create Date\" is currently marked as editable but does not have any validation (it's possible to type in invalid datestamps)\u2026 I would either make it properly editable (with a stock date picker widget) or simply disable editing for that (why would you change that  date anyway?)\nA: Will disable the edit for this field\n I like the light blue better in the metadata view (and it's more in line with the overall style of the UI) but I still think it's a bit too heavy - why don't we reuse the two colors used in the grid view for that? (#fff and #f2f2f2). That would be more consistent I think.\nA: Agree. will change style.\n the \"About\" cell is capitalized whereas \"rename\" isn't. I would change it to \"about\" for consistency. (Also, I think the other labels should only be capitalized for the first word - again for consistency with the rest of the UI - so \"Project Name\" -> \"Project name\")\nA: I can change the rename to Rename for consistence. But I think we should keep the \"About\" rather than \"edit\" since \"edit\" may means \"edit the data\". For the first upper case only, will make the change.\nTHAD AGREES WITH ANSWER\n Do we still need the rename link now that the name can be edited in the metadata view?\nA: It is a shortcut. I would prefer to keep it\nTHAD AGREES WITH ANSWER\n It's not clear to me that \"about\" is the think you should click if you want to edit what you see in the table. I would find it more intuitive with \"edit\", or even better: a pencil symbol (for consistency with the delete button on the left hand side).\nA: see above comments\n Would it make sense to have a metadata field that stores the original format (or name of the importer) of the dataset? It does not look like this is stored in the import metadata.\nA: There is key \"source\" in the import option. from that mostly likely you can tell the file format. Also the options are different from importer to importer.\n the delete button is placed weirdly with Chromium (not centered in its cell)\nA: will try to fix it if I can. :)\n* it would be nice if the \"last modified\" could still be displayed in the nice human-readable form they had while preserving the sorting functionality\u2026 I have no idea how hard that would be. But in the current state it's kind of a regression in terms of user-friendliness.\nA: For me, the \"human-readable form\" is actually harder to read than the current way(at least for me). Especially given that it can sort asc and desc. For example, if I want to find a project I did at 2017/11/01, it is much easier to do a sort then I can locate it. For the old way, you have to calculate it's XX days ago then you can go get it. It is personal  preference. If there are more person like the old way to present, it can be done.\nTHAD AGREES WITH ANSWER AND THIS HELPS WITH CONSISTENCY INTERNATIONALLY AS WELL. How about shorten the date to such as  \"2017-11-11 07:25 PM\" ? The rename and about font is already small. Also I can reduce the width of \"Row Count\" a little bit but not too much(the header has to fit in). Above was done by PR #1323. I will remove the \"Rename\" link from the index page if there is no objection by next Monday before release 2.7.2. All, please speak out if you want to keep it as is.. \"Rename link\" was removed by PR #1324 . @ettorerizza I understand you concern. Do you mind open 2 issues(or more if you want) for metadata and the transform UI. In the meantime, I will take a look what we can do. @ettorerizza @wetneb @thadguidry created PR #1331. Please try / review.\nwhen doing the test for long string(for example description), please add while space in between like normally. that's how browser know where to wrap the word. . I would say it is an expected behaviour.  The original number has the 1 decimal. so the 14.0 will show as 14 when it's a number. But when you convert to text, it shows 14.0 to preserve the decimal. There is no need and also should not cut the trailing 0. Agree. I found that issue number #1271 for full precision. will treat this one a duplicate of it.. Maybe some tweak around line: https://github.com/OpenRefine/OpenRefine/blob/master/main/src/com/google/refine/process/ProcessManager.java#L152\ncould save partial reconciliation results.  Or even better to use Interceptor design pattern make it more flexible and clean.  Just a wild idea. :) . will be release at 2.7.2 soon. close it. not deal with BS for now.. can you provide a sample of the JSON file?. @paregorios, can I say the root cause is OR 2.7 for OSX ignore the $JAVA_HOME? or OR will not well with Pleiades with jdk 1.8 regardless the OS due to the missing root CA issue ?. @ostephens , I checked the folder under /Applications/OpenRefine.app/Contents/PlugIns/jdk1.8.0_60.jdk/Contents/Home/jre/lib/security after installing 2.7RC2. I did find the 2 jars you mentioned.\nBut the cacerts does not include the DST cert indeed.\nFor the linux box and openjdk, I do see the entry:\nroot@hp:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/security# keytool -list  -keystore cacerts | grep \"\\.pem\" | grep dst\ndebian:dst_root_ca_x3.pem, 24-Apr-2017, trustedCertEntry, \ndebian:dst_aces_ca_x6.pem, 24-Apr-2017, trustedCertEntry,\nNext time we release, we can try to bundle the 1.8.0_101 or later as @ostephens suggested. Hopefully that will solve the issue.\n. According to this, there are no AES256 cipher suites offered by current bundle java client. My understanding is that in order to make it work, we have several options:\n-  Provide the bundled version with  (JCE) Unlimited Strength Jurisdiction Policy and dst_root_ca_x3.pem as @ostephens recommenced \n- Provide the script to automate the the above process without bundling as @thadguidry recommenced\n- Another option could be have the site to provide other more populate cipher which is included to most of Java version.  that feasibility has to be left to @paregorios to answer. . @thadguidry self-signed cert is not mandatory from OSX level. Since we are not distribute the software to apple store, that not mandatory either(actually, if you want to distribute to store, you need a developer certificate signed from apple). AFAIK, The reason we self-signed it is to avoid some warning popup for security from the OSX. And for some system, OSX will hide the icon when it is not self-signed. Basically it's poor man's choice to try to align with the security framework from apple. :).\ncode signing is to sign the distribution as whole to assure the identify of developer does the release, which means it does not matter  you bundle something or not.. @richyvk PR is welcomed. just make sure you test it out under different browsers. Also you can do a screenshot after your change. :). @thadguidry Are you sure The Expression input box itself (inside the Transform dialog) is resizeable? I tried but seems not.. Is exporting metadata to tabular data format  a general approach or it is just for wikidata? why we need to upload to wikimedia Commons. Since you are using Openstack, I think it is related to the IO. can you use iostat to check while you starting it?. that's weird. but from the log it shows the process \"scanForUpdates\" takes much of the time. You are using root to launch it. I am not sure where does it get launched. You may on a mounted storage. can you use a regular user to download a new copy and try again? The idea is to try to make it running locally, not from remote storage.  . Right! that must relate to your cloud environment. which provider are you using?  Run this and paste the result:\nmount | grep $(df -P ~/.local/share/openrefine | tail -1 | awk {'print $1'}). looks fine for me. I assume you did not tune those parameters yourself for the file system. The last thing that I can think of is the number of your projects. maybe it's too many? OR will try to load all the project meta data when starting up. that could take long if you have too many project and on a slow disk IO.\nDo you have other applications run on Genouest in similar way? I never heard of this provider. . Can you delete the ~/.locale/share/openrefine folder and try again? Back up it if necessary. can't see any obvious issue which can cause the slowness. You said that the first time after you removed the folder it's work though? what kind of project did you create after that and what else did you do?. jdk 8 is good to work with container. jdk9 is not supported yet.. close for now. if you think it's an issue of the OR itself. please reopen it.. @remram44 Thanks for the suggestion. Just one question, do you see any drawback of the form $.i18n._('core-index')[\"delete-key\"] except the complexity . @thadguidry There is nothing wrong with the parser.  5550 = 5 sec 550 ms, 7380  = 7 sec 380 ms.. It is more accurate if parsing as  \"fraction of the second\" rather than \"milliseconds\". Good thing is this way can cover both from the Java 8- and Java 8+. Ie, when people use 999+ they know they are using nano sec so they are expecting the API parse the same. For who don't even know you can parse 999+ they assume it's a ms, so does the API. so it cannot be wrong.\nI agreed that we should move to java.time to replace SimpleDateFormat and Calendar class. Also for datePart function, the 'ns' option should be added accordingly  . Since this post has been a while, just to recap that we agree that we should treat the ms date part as the \"fraction of the second\", It 7380 will be 380. You don't lost the 7 sec since it is shifted to sec unit. \nBut I will add the support for ns for datePart function. Please confirm. I agree the idea of simplicity. cross function behaves like the database join operation but lacks the validation aspect due to the unawareness of schema of each other. so we have to make assumption that user understand their data before applying this operation. The ideal situation is the data elements are normalised and can be matched without extra tweak. that's from the functionality point of view as well as the performance point of view.\nAlso there is a myth that cross can handle the foreach properly with given separator. It can NOT. because the cache will always cache the whole cell value as the key(ie, \"ABC, EFG\" etc), not the element itself.\n. @thadguidry CodeMirror is great! but AutoCompletion seems broken. \n@felixlohmeier I submitted one issue for your repo:\nhttps://github.com/opencultureconsulting/openrefine-batch/issues/4\n. looks good for me. will merge.. @thadguidry Auto setting is fine. But leaving the decision to end user might also an option. Your question is hard to answer because only end user knows their situation such as size of data set, complexity etc. Instead of auto set it, a responsive warning message when Out of Mem occurs could helpful. OXS don't have the free command. But I confirmed this works:\nvm_stat | perl -ne '/page size of (\\d+)/ and $size=$1; /Pages\\s+([^:]+)[^\\d]+(\\d+)/ and printf(\"%-16s % 16.2f Mi\\n\", \"$1:\", $2 * $size / 1048576);'\nLink:\nhttps://apple.stackexchange.com/questions/4286/is-there-a-mac-os-x-terminal-version-of-the-free-command-in-linux-systems. Yes. it works:\n$ top -l 1 | grep PhysMem\n  PhysMem: 1473M used (439M wired), 4543M unused.\n$ top -l 1 | grep PhysMem | awk '{print $6}' 4535M. I just realised there is some issue if we want to implement this:\n1. it will automatically set the RAM for user which will disable the possibility to tune the JVM themselves. Unless you provide extra switch to skip the automatically setting or enforce the user setting. That defeats the purpose of the intention to ease the process. \n2. The calculated RAM size may too small to too big to have the application launched.\nFor example 32 bit system allow max around 4G, but 64 bit system is different. Also the different JVM vendors have its limitation differently. There are lots of combination of OS, Arch and JVM, not even mention about the user preference.\nI would suggest that we leave this to end user or only provide  the suggestion instead of set it directly.\nThoughts?. sure. we can do that. . but what's the fundamental difference do you expect from the future find() and current match()? . @ettorerizza you should have be able to do group and get the result. something like this:\nvalue.match(/((A-Z)) /)\nBut seems it doesn't work. I would say it's a bug.. GREL match just wrapped the java java.util.regex.Matcher. But java.util.regex.Matcher is able to do the same thing your python script does. If we stick with the document, current behaviour follow the spec well. \nMaybe we can add another function findall to do the same thing. thought? . maybe add more timeout before send request to back end?. I agree that there is no special case. This is only case which is code handle it properly or not. I think\n-  it is an exception from the back end somewhere is not handled(that caused the hanging)\n- Because of above, the exception is not converted to json error, which is why the error message from U looks weird.. just tied with the head version without issue. Did you see any exception from the console. or can you try chrome instead?. I just tried on firefox and works well also. It's hard to say which plugin cause issue. I haven't heard most of them. You can open the developer tool by press ctrl-shift-I to watch if there is any exception on the javascript console to help troubleshooting. close for now. we need to fix those failed test cases. . The UrlFetchingTests.java has been fixed. it is not about how long to sleep. The web site block the appvevor. we have to skip it when it happens. I will close this one. Maybe you can submit another one to add the options in the script. done by #1301. Push this task. it should be done with part of the build system migration(gradle or maven). fixed by PR #1313 . there is one for AugularJS. just for the records:\nhttps://tympanix.github.io/angular-table-resize/\n. what did you do to get this error message? that \"name\" is from the translation json file. Did you try to switch the language?. There is an issue with translation on master branch. Issue #1325  created . Not sure if this can be done from the weblate side. Another option is to merge to a dedicated branch and manually merge to master after review.. @ostephens You can take a look of CacheTests.java. For the Javascript, I am not quite sure. There are some popular framework such as  Karma seems quite good. But I don't know how OR can fit into it.. Put a fix. will create a PR soon. But remember to enter 0 line as the column header since your files don't have the header at all. @ettorerizza the application will automatically select non-empty sheets.. done. Will put a filter when user define the userMetadata. also added the open office for the same. Thanks all to catch this. will remember to update readme.md next time.. Your exception comes when setting the refine.autosave parameter from\nrefine.ini file.\nDid you make the change to that file? can you delete the old version and\ntry again?\nI tried on my own Ubuntu. it works fine.\nJacky\nOn Sun, Nov 19, 2017 at 8:39 AM, @hpiedcoq notifications@github.com wrote:\n\nHi everybody,\nI just downloaded version 2.8 for linux (ubuntu 17.04 up-to-date).\nI get an error :\n\nHTTP ERROR 503\nProblem accessing /. Reason:\njava.lang.NumberFormatException: null\nCaused by:\njavax.servlet.UnavailableException: java.lang.NumberFormatException: null\nat org.mortbay.jetty.servlet.ServletHolder.makeUnavailable(\nServletHolder.java:415)\nat org.mortbay.jetty.servlet.ServletHolder.initServlet(\nServletHolder.java:458)\nat org.mortbay.jetty.servlet.ServletHolder.doStart(ServletHolder.java:263)\nat com.google.refine.RefineServer.configure(Refine.java:294)\nat com.google.refine.RefineServer.init(Refine.java:208)\nat com.google.refine.Refine.init(Refine.java:114)\nat com.google.refine.Refine.main(Refine.java:108)\n\nThe complete log is the following :\n14:35:12.680 [ refine_server] Starting Server bound to '127.0.0.1:3333'\n(0ms)\n14:35:12.684 [ refine_server] refine.memory size: 1400M JVM Max heap:\n1407188992 (4ms)\n14:35:12.713 [ refine_server] Initializing context: '/' from\n'/home/herve/Applications/Openrefine2.7/webapp' (29ms)\n14:35:13.573 [ refine] Starting OpenRefine 2.8 [TRUNK]... (860ms)\n14:35:13.694 [ /] unavailable (121ms)\njava.lang.NumberFormatException: null\nat java.lang.Long.parseLong(Long.java:552)\nat java.lang.Long.parseLong(Long.java:631)\nat com.google.refine.RefineServlet.init(RefineServlet.java:132)\nat javax.servlet.GenericServlet.init(GenericServlet.java:241)\nat edu.mit.simile.butterfly.Butterfly.init(Butterfly.java:180)\nat org.mortbay.jetty.servlet.ServletHolder.initServlet(\nServletHolder.java:440)\nat org.mortbay.jetty.servlet.ServletHolder.doStart(ServletHolder.java:263)\nat com.google.refine.RefineServer.configure(Refine.java:294)\nat com.google.refine.RefineServer.init(Refine.java:208)\nat com.google.refine.Refine.init(Refine.java:114)\nat com.google.refine.Refine.main(Refine.java:108)\n14:35:13.698 [ org.mortbay.log] Nested in javax.servlet.ServletException:\njava.lang.NumberFormatException: null: (4ms)\njava.lang.NumberFormatException: null\nat java.lang.Long.parseLong(Long.java:552)\nat java.lang.Long.parseLong(Long.java:631)\nat com.google.refine.RefineServlet.init(RefineServlet.java:132)\nat javax.servlet.GenericServlet.init(GenericServlet.java:241)\nat edu.mit.simile.butterfly.Butterfly.init(Butterfly.java:180)\nat org.mortbay.jetty.servlet.ServletHolder.initServlet(\nServletHolder.java:440)\nat org.mortbay.jetty.servlet.ServletHolder.doStart(ServletHolder.java:263)\nat com.google.refine.RefineServer.configure(Refine.java:294)\nat com.google.refine.RefineServer.init(Refine.java:208)\nat com.google.refine.Refine.init(Refine.java:114)\nat com.google.refine.Refine.main(Refine.java:108)\n14:35:15.788 [ org.mortbay.log] /: javax.servlet.UnavailableException:\njava.lang.NumberFormatException: null (2090ms)\n14:35:25.686 [ org.mortbay.log] /: javax.servlet.UnavailableException:\njava.lang.NumberFormatException: null (9898ms)\n\nAny clue?\nThanks\nHerv\u00e9\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1344, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AKCGi3FufhJd9XnWHq10e5Yyuqgkrr2Nks5s4C-MgaJpZM4QjYfW\n.\n. Just for you reference, the last part of refine.ini under the root of installation folder should be like this:\n```\n\nUncomment to increase autosave period to 60 mins (default: 5 minutes) for better performance of long-lasting transformations\nREFINE_AUTOSAVE_PERIOD=60\n```. implemented by PR #1398  . @ettorerizza the URL max length is around 2K. Not sure normally the length of your URL. If the length is an issue, that can be done by a menu or something like that. right, the \"About\" link is not shown because it is not translated. could you help to translate it in French from the weblate(you can find the link from readme page). The key is under \"core-index-open\" section.. That will be merged automatically. It supposed to fallback to English. I will double check on this.. Just tried to remove a translation \"rename\" from import page, the fallback dose NOT happen. something is broken on this part in general. Not sure started from when though.  will look into this. \n. I am sure we have the fallback. Just tried up to version 2.6, the feature is not working properly. have to dig further. . looks like we are wrong. there is \"default language\" in the code but never worked.\nwe can enable the fallback plugin jquery.i18n.fallbacks.js to have this:\nhttps://github.com/wikimedia/jquery.i18n\n@ostephens , could you take a look of this?\n. @thadguidry It is not merged yet. Seems it only happens when rename it from project page. It works fine when do it from the about link. that is weird. . I can reproduce under windows 10.\n@rezabj can you please run a \"locale\" command in the environment running the OpenRefine . That's what I thought. But ideally it should be handled from the application level . should just go to work space and delete all files easier? but delete all is really dangerous option to provide. PR was merged. can you try this:\ncurl -d \"project=1681144421091\" -X POST  http://127.0.0.1:3333/command/core/delete-project. You can also take a look of this if you are looking for some library to do resetful operation:\nhttps://groups.google.com/forum/#!topic/openrefine/a9EiNzHPvJA\nhttps://github.com/felixlohmeier/openrefine-client \n. @xseris I see. I will test further. I am worry that the editing the metadata and the user metadata might not work properly because the tag takes a column but the current UI does not aware of this. \nThe regression test failed:\nhttps://travis-ci.org/OpenRefine/OpenRefine/jobs/306899471\nNeed to fix. \nBesides that, you may try to polish the codacy issues as you can(some are irrelevant which you can ignore). Also some basic Unit Test will help also.\nI think we can merge it if other developers has no issue. \n@ettorerizza It looks good. I really like the navigation bar at the top!\n. Agree. I think subject is formal and tag system is more free style. Maybe it can replace the \"user defined metadata\" instead?. @wetneb quoted from @ettorerizza :\n In Dublin Core, the \"Subject\" element can be expressed as \"keywords, key phrases or classification codes that describe a topic of the resource. Recommended best practice is to select a value from a controlled vocabulary or formal classification scheme\".\nBut the tags is user defined which means you can choose what ever you like and they just have meaning for that user. It is NOT a mandate that they have to chosen from a controller vocabulary. In other words, the tag system could be local and does not have to to published. \nCurrently since OR's metadata is not standardised, you can argue that subject means something else.  But that's not the point.. @xseris Those verifyNoMoreInteractions(project) issue means that your code introduced new invocation against the project other than ALL the verify statements before it. you can find out which invocation you missed from the testng log and add tha to do a verify. It is not that straightforward. But once you does one, rest of them most likely are the same. You can try to see if you solve them yourself or not. Or I can take a look and merge back to your repo.\n@ettorerizza The problem is that OR don't know who you are thus can not fill that for you. we can put a setting as a workaround though. @xseris Great work! I will checkout locally and do some UI testing. If there is no issue found, I will approve this PR. We can have more discussion regarding which field can be replaced with the \"tag system\" before or even after the merge. It is not I don't like the change. It just take a while to have everyone at the same page. Agree?. @xseris I found some issue with the position of the \"tag\". we support the \"user defined metadata\" and I just write down the procedure to do so:\nhttps://github.com/OpenRefine/OpenRefine/wiki/Metadata\nCurrently the \"tag\" column always take the last column and it will mess up the \"user defined metadata\" if there is any ( the tag will take the space of the last user metadata). But if you can move the tag right after the \"name\" column and that will solve the problem automatically without doing anything else. We can  hide \"subject\" and \"creator\" later on if everyone agree. The point is that after the position of \"tag system\" get settled then we can change rest of the columns without/less impact(because all the columns are added dynamically ). But that can happen after the merge.\nSo from my side, I only expect that tag column get moved after \"name\" column(which @wetneb  already pointed out)\n. @xseris Do you mind I merge your change into my repo from where I am doing some other meta-data feature? All the history can be kept. \nI need to add a \"Data Package\" link besides the \"About\". I would like to merge your change to avoid more complex  3-way merge in future. Let me know if you are ok with that?  I will move the tag column right after the name column as we discussed. We can continue the discussion and the other pending issues.. @xseris I merged to my repo:\nhttps://github.com/jackyq2015/OpenRefine/commits/feature/metadata\nLooks great! I noticed that there is some sample tags like \"big data test1 test2\". where those comes from? I have not start to set the tag yet.. My bad. I think i tested it before.. @xseris Seems when moving mouse point to the \"tag\" column, there is not menu displayed to add tags any more in the current code base. I tested on your repo and it's the same. could you please take a look and create another PR to fix this issue?. isX is not a function. All the isX() implements \"Control\" interface. A control can decide which part of the code to execute and can affect the environment bindings.  Functions, on the other hand, can't do either.. @ettorerizza thanks for the documentation. . @fpompermaier Any recommendation on your mind?. Upgraded to new API.. can this take of all the exceptions or just the ones listed in the code?. Poor IE. :). Did you see any error? I tried to edit but seems no issue.. Tried from the \"text facet\", did not found the issue either. could you please provide the procedure to reproduce this issue?. I think there are two factors should be considered. One is how fast it can load and another is how much it can load. Current OpenRefine come across issue with big data set which will be failed to load into the system. I am not blaming the CSV parser. There are several reason and one reason is the OpenRefine itself. \nIf the library you mentioned above can handle this that will be great! But normally the faster it can load, the more memory it requires.. they really should provide this push to branch feature. it is quite useful. got it. I thought they do not have that.. we can separate out the translation as a submodule by creating a repo under openrefine and lift the restriction just for translate. For user there is no difference.. I cannot reproduce on master. But I can see it from my own repo metadata branch. The only thing I can think of is that you did not fire a build after switch back to the master.\nI have fixed it on my branch. . Not sure if it is caused by merge or not. let me check and update.. @ostephens Did you see any exception at back end? could you please paste the console message while startup? I have no problem to show \"All\" projects. But I cannot add tags anymore. can somebody confirm?. @ostephens  May I know your version of Java while you created those old projects?  It is a regression introduced by local time stamp. I had a UT for testing the new API can parse the old project. but still missed something. will figure out and close it. . that is fine. no worry, Owen!. I guess this project is created using the new version of refine. if so, the reason is that the old API in jdk 7 can not process the date time written from jdk8. but from vise versa is fine. since jdk 7 was dropped already, I will mark it as not fix if it is the case.. @ettorerizza Do you have the instruction to do the export/import somewhere(include versions information)? I can take a look. The back capability is possible if there is a need. The thing is that every change to the core could potentially break other extensions. Since we don't have the full picture and usage of those extensions, it is a little bit hard to do the major change and also keep all the back capability at the same time.  . My bad. have to revert this PR then. :). @tiagofernandez can you submit another PR for this? The import options was included already actually. Please verify it with the previous version of your change.\nYou can change your UT (but make sure it is ok) and we can keep it so it won't get wasted. Thanks. @tiagofernandez I added the importOptionMetadata field month ago. if your projects were created before that you will not have that info in the metadata.json that's why you cannot retrieve them. To verify that, you can roll back your code base to previous version of your PR and create a new project and access the same end point you mentioned.. yes, I am sure that is the only format. there is a formatter defined for iso8601 in the parsing utility class. . actually the problem still there. there is some other reason. . submitted the PR. it is a network issue. Appveyor has some network policy to control the access the external network.. can you provide a sample xml file and the procedure you import the file?. Tried your file. I have no problem to load it. Did you use mouse to select whole document before creating the project? or you just select part of it so it only extract that block of xml?. As discussed, this batch will not be called when user run openrefine.exe. But with this PR, we still can have user to run refine.bat if we need extra info to do the troubleshooting. It is not ideal. We can have some sort of file logger to collection all the general info for troubleshooting in future. . There is a json editor at my branch for the \"data package\"\nhttps://github.com/jackyq2015/OpenRefine/\nIt can do the in-place editing and validation. you can take a look.\nWe can change the \"About\" dialog to the same. But it does not support the attachment for now. Also I am afraid that the way you try to use the  \"description\" field is not so usual. Those operations/comments should be separated out from metadata. we have another ticket to deal with operation history/comments.\nI agree that the metadata such as creator should be populated automatically. we can have a preference setting \"creator\" and each time we pull it from there and feed to the new project created. what do you think?. This is ticket:\nhttps://github.com/OpenRefine/OpenRefine/issues/368\n. @ettorerizza is that what you refer to? Or you can provide a sample of the notes if you don't mind.  I think attaching the notes to the project level and column level make sense because it is nature for the user and easy to access when they want to edit/view it. Metadata level data only accessible on the \"project index\" page. Most importantly, My understanding is that those notes should be part of project and operation instead of metadata itself.  . @ettorerizza I added the \"username\" preference and it will be pulled as the creator of the metadata.\nWill hide the \"creator\" column from the index page, do you want me to hide the \"description\" and \"subject\" column as well? . ok. then only the \"creator\" will be hidden then. . implemented by PR #1398  . @wetneb Thank for the review. For the 5 pointed you pointed out:\n1. I can add different way to import a data package.\n2. The \"path\" specified in that json is not exist. so it will generate a 404 error. Change was pushed to  handle this error more friendly. \n3. the inline data is not supported from the official data package repo. I will raise an issue and try to create a PR to merge into the official data package.\n4. The project can be created with a \"data package\" or without. The json editor to add or update the data package information before they can export it as a data package. That's the rationally behind the editor. It is not just for display.\n5. The design is the ProjectMetadata will be still the primary metadata for openrefine.  The data package and the other forms are extra metadata based on the need of the user. The data package will also NOT replace the current data model for schema. But I agree that we should loop the current column model into the data package. That can be deferred to the stage when user want to use the data package ( for example to export as data package or do the data validation). That will go back to item 4. Still the user  need a UI interface to define the detail data schema for the constrain etc ( the application can only populate basic skeleton of the schema from the current model).. The link from datahub is for an existing data package. But in order to generate a data package, the creator have to deal with the detail metadata definition one way or another.  And there is no system can automatically do it for you. Currently the json editor is an interface to allow user to do this from one entry point.  It has a template which can be validated to guide user to create the proper metadata based on the data package schema. It is not just a random json file. One area I can think of is to enhance the way user define the schema though. For other field, it is quite straightforward. For example, user have to input the  \"license\"  if he want it be part of his metadata.\nI am not sure if you are saying we should swap out the current Project Metadata with the \"data package\" metadata. I had disucssion with Thad, and we agreed that the current Project Metadata will be preserved because it's already fully integrated with the OpenRefine and it is easier to extend since it does not tie with any external spec. we can get rid of it and it just not worth to do it. Any external metadata will be built as an extra with the import/export. Also other necessary functionality which can be enabled and leveraged by that particular metadata spec(for data package, that's \"data validation\"). I already posted a brief design for your review before I started the implementation and haven't got any response yet:\nhttps://docs.google.com/document/d/1RdnpZ1Nr0NWXG4RfNQ7tyDbmkSPSltFMcCI6hxl5484/edit#\nIn #1096 there are too many topics got discussed and most of them is about \"data package\" vs CSVW. In terms of how it should be integrated into the existing system, my understanding it we should always use the existing resources. in this case two repos were directly introduced and integrated with current internal metadata. As the UT I mentioned above, that also enabled the back end of data validation  using the data package spec. But for the data package data itself, user still need to input somehow. \nRegarding the workflow you mentioned, I think one thing we are missing is to incorporate the current \"column model\" and the \"data package schema\" then we can get the final schema. That can be done given the current information we have. Still some user input is required at the final stage. For example, you added another column from data extension. you may need some sort of constrain to apply to it. . I know the schema can be inferred from the data itself. The current  official \"data package\" already implemented this. But that cannot fit well in the OpenRefine where we already have the schema(it can be used to help combine the current column model into final schema though) Also the infer is NOT designed to generate the \"data package\". It is an educated guess based on the data set. For example, infer cannot know your constrain, not even mention other metadata fields other than schema.\nFor the metadata 2.8 version, I confirmed with @ettorerizza, @magdmartin and @thadguidry  on the github issue publicly. It will be sorry again that you did not see it. we cannot take advice from many users and also remember that there is no authority to define the internal metadata. what I did is totally based on the user input, field by field. The point is that I did not decide what metadata should be included. There is duplication but we discussed in \"tag system\" PR, we will clean it up gradually after reach the consensus. we cannot get rid of current metadata for now. One is because it was tight with other model, another reason is still consensus is hard to reach. Like the ongoing debate between \"data package\" and CSVW. Every one has different opinion and bias. Also think about if CSVW is introduced in the future, there is for sure lots of duplicate between metadata models. Even we can get rid of current project metadata. we will still come to a point to decide which spec is better/more useful if we want to only keep only 1 metadata and thus remove the duplicate. \nThe importer function is straightforward and user can use it to pull the data and import into OR without knowing the detail of data package. For exporter, if the user want to create a \"data package\" he has to fill in the information especially when the project is not created from existing data package . Schema can be taken from the current data model but he still need to fill in the constraints and other other simple fields. . All the types are defined in the Field.java from official repo which follows the spec. According to the spec, the rdfType is another properly parallel  with the type. But it is just a definition. Also the officail repo does not have this included( it can be included, but will have no reference to it) . comment removed. will take a look about the order request. The JSON is essentially unordered. but will do some research and get back.\nAll the validation you mentioned is based on the data package schema. but it has the non-strict mode, means you can disobey and save it, though it is not recommended. It may cause issue for other application follow strictly to the spec.\nFor now, only JSON URL is supported. will add the zip file support later on. \n  . It is ready. :). The zip is supported from the local file import. The reason a dedicated menu is given for data package url is that user has to indicate it's a data package instead of json file.  I will add an entry for importing from paste. \nThe keywords is mandatory according to the spec. The json you provided is not qualified json. But I can change it and let it go without it. You can try:\nhttps://raw.githubusercontent.com/datasets/gdp/master/datapackage.json\n. fixed the issue for zip:\nhttps://pkgstore.datahub.io/core/cofog/datapackage_zip/data/ed438b8b276d0169131523969f4e3582/datapackage_zip.zip \nFor clipboard integration, it is hard to for program to detect it's a data package unless user say so. Those json keys can be optional and can be there or cannot be there. For the spec, the ground to tell it's a data package is based on its name datapackage.json. obviously it can't apply to the clipboard.. Merged the tags from user input. \n@wetneb For the entry for \"JSON URL\", I would like to make it explicitly to the user that it deal with \"data package\" json metadata. User HAS to know what is \"data package\" but not necessarily the detail of it. Hiding those  import logic does not bring benefit, instead, it is confuse user sometimes. Also data package is not a data format(if it is, then it make sense to handle all the logic for the user as you mentioned), it is just how the data and metadata is organised. The same idea can apply to the clipboard.  That entry clearly state the pasted content it is for data. I would not think it's a normal user case you paste it from somewhere. there  are only 2 cases: you have the existing datapackage.json or you don't have it:\n- If you have it(could be from URL, or it is zipped in the zip file locally), you can use the current importing options to \"import\" it. \n- If you don't have the datapackage.json,  you either import the data as old ways then export to the data package if you need a data package and you will get a datapackage.json automatically. Or you need to create your own  datapackage.json by yourself(means you manually create it and all on yourself. :). For the Eclipse setting file, I think it make sense to keep it in the repo since OR is a single project and you really want to programmer can setup easily without too much effort. For the project you given above, they have the gradle to help them handle the setup and it is easier to maintain just one build system instead of 2(but gradlle can generate the eclipse project). \n . @wetneb We need to have the gradle build firstly then eclipse project can be generated from that.\nhttps://docs.gradle.org/current/userguide/eclipse_plugin.html. @wetneb Yes. Gradle migration is not minor change. we have to factor in the change both for the build script and the bootstrap script refine/refine.bat etc. Also we can discuss we should go gradle or maven? . I had a close look of those \"changed\" files. \n- Removed .gitmodules\n- For all the .gitignore, it were done by @wetneb but merged from master. will keep\n- For the .project and .class, it is required because we added the data-package library and it need dependency.  will keep\n- For the .settings files, it is caused by data-package also. They only works with Java 8.\n- Also there are some some files are from data extension. \nPlease take a look. And I will resolve below 2 conflicts by merging from master again before merge back.. yeah. that's very terrible bug of GitHub. Not sure if that is there any\ngood alternative to it? Atom might be a one. but not sure if it is behave\nthe same. I know @thad use it, @thad  can you please confirm\nOn Sun, Jan 28, 2018 at 1:21 PM, Antonin Delpeuch notifications@github.com\nwrote:\n\n@wetneb commented on this pull request.\nIn main/src/com/google/refine/commands/GetAllPreferencesCommand.java\nhttps://github.com/OpenRefine/OpenRefine/pull/1398#discussion_r164307288\n:\n\n@@ -52,9 +52,9 @@ public void doPost(HttpServletRequest request, HttpServletResponse response)\n             throws ServletException, IOException {\n\n     Project project = request.getParameter(\"project\") != null ? getProject(request) : null;\n\n\nPreferenceStore ps = project != null ?\nproject.getMetadata().getPreferenceStore() :\nProjectManager.singleton.getPreferenceStore();\n+\n// project level has no specific preference\nPreferenceStore ps = ProjectManager.singleton.getPreferenceStore(); try {\n     response.setCharacterEncoding(\"UTF-8\");\n\n\n\nGreat, thanks a lot! (But that's worrying - it means we can't rely on\nGitHub to review PRs?! That sounds like a serious bug to me!)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/1398#discussion_r164307288,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AKCGi05ljglpOdH8z6sXj_gBbUlImt_iks5tPLqugaJpZM4RL_J7\n.\n. seems have to stick to the github.com for PR review which other git clients don't provide.. @wetneb Thanks a lot! I just fixed last 2 conflicts of the master.. are you sure the project were removed? The project only will try to recovery if the project folder is there. . Did you test jdk9?  I would hold the jdk9 until we have it tested. It may bring more trouble than benefit even it past the UT. added the support. Your project has no issue standalone(ie, when pull it and install it as an\nextension to current master). The problem is that the library setting is\nnot in the current master .classpath UNDER THE ROOT. The current way of\nsetup in order to include the extension is to add the src and test folder\ndirectly under the build path instead of adding the project.\n\nThe problem you need to fix is on the OpenRefine project setting, NOT your\nown project setting. If you open the project setting of the OpenRefine and\ncheck how gdata is setup. you will understand what I mean.\nOn Sat, Jan 13, 2018 at 4:50 PM, Tonio notifications@github.com wrote:\n\nPlease provide more description to the problem. src and test is already in\nthe .classpath file.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/1430#issuecomment-357469861,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AKCGi7bFRjWf3WOFcLr_AR1zE4RpY41Qks5tKSUWgaJpZM4RdVzS\n.\n. @ettorerizza Thanks for the documentation. . Cap the text might be hard to suit user's need. \n\nMaybe the extraction can be done from the VIAF side. I will ease the burner from the OR side. Though I am not familiar with the syntax.\n\n\nVIAF in the OCLC API Explorer:\nhttps://platform.worldcat.org/api-explorer/VIAF\n\n\nThe SRU syntax used in the API SRUSearch function:\nhttp://www.loc.gov/standards/sru/\n\n\nVIAF self -Document:\nhttp://www.viaf.org/processed/search/processed\nThere is \"Record XPath:\" and \"Number of Records:\" parameters which can be used for this purpose.\n\n\n. Might relate to this issue:\nhttp://bugs.jython.org/issue2632\n. @thadguidry Build a snapshot is an option but may introduce other bugs. I would prefer to wait until it get stabled and release an official version unless this particular bug is a show-stopper. . Looks good. But should we refactor other UT to use the helper method. There is compile errors. @joanneong Nice job! Just one minor thing you can consider for the image file names. Instead of logo-openrefine-new.png, may be something like logo-openrefine-40.png to use the previous naming convention. Just an example.. +1 for returning a array instead of string. It will provide a easier way to loop through it also leave the decision to the end user how to decorate(split, padding etc) the elements  . PR #1524 created . @magdmartin Most likely it is build issue. there is some class can not be found. If you do a \"ant clean\" should fix the issue. . @wetneb Thanks for the sample. will look into this.. Fixed this issue. Reviewed other possible JsonObject.toString cases, seems this is the only one which try to read a int as string. Will create PR after more checking. Issue #1508 is not to do with the JSON lib upgrade though.. This is not to do with the actual memory the data set consumed. It will only take 100 rows as the sample to infer the type. After that it will be gced. \n@thadguidry It will apply to all importers.. @wetneb It is interesting to see that you say it is not related to the data package.  It looks like you are the only guy who can do the quality software work in the world.  I am speechless at your attitude. that's all I can say.  . The first thing I want to address is that this change is integral part of the data package change. It is NOT unrelated change. Since we need to apply the types infer defined by data package and that has to be done at the import stage. From the QA aspect, I agree we need more UT and that won't solve all the problem. That part of code  is covered by UT and not that that similar test data which can break it. For this particular case, I will write a UT to cover it.\n. @thadguidry The infer only impact the column model, the row model is not impacted. . Will fix the conflicts after review. Yes, I can merge it. Checked with google java client github, seems they don't have solution for this either. But I don't think we need to generate the fresh one for release. Firstly credentials cannot be hidden. Secondly and also most importantly, the blocked Id will be the malicious Ids who abuse it, not the OpenRefine application Id.. @ettorerizza I think it's related. The point is that though the OAuth, you can not really cause damage to others'(document in this case). The worst case is you use it too much, but again the Google will put a cap on the user but not the application itself(the cap for the application is unlimited according to the Google Console). \nAlso, @wetneb I don't quite get when you say \"the tokens you put in the packaged versions will not be as visible as the ones in the source code\". you mean we get a new token to replace the old one. what's the difference if it is open sourced.. @thadguidry I am worry about @wetneb worry too much. :). No problem, shall I go ahead to merge it then?. PR #1527 created for this.. @wetneb close it for now. Feel free to reopen if have any concern. @webneb  In json, \"limit\" was saved as int before and is saved as int now. So even user replay the history operations the value of \"limit\" will be still fed with int. When the limit is blank the value was/is stored as int 0.  Seems we don't need extra code to handle the backward compatibility on this.  . @wetneb As far as I can think of, the only scenario of client is history replay through the JSON interface which is keep untouched before and after change change. If you can name a client as an example which still send the string, then we should consider to support it. It is important we can keep the interface clean and unambiguous as possible (of course without service interruption). @wetneb As far as I can think of, the only scenario of client is history replay through the JSON interface which is keep untouched before and after change change. If you can name a client as an example which still send the string, then we should consider to support it. It is important we can keep the interface clean and unambiguous as possible (of course without service interruption).  There are 2 interactions involved here: one is UI to the back end and another is when application load the json from the file. I think we all agree that there is no impact for the latter interaction(ie, there is no service interruption if user want to replay their history because there is nothing changed on the persistent file). The only question is that should we support 2 ways to send the \"limit\" from the front end to back end? My answer is no. The reasons are:\n\nThis is internal communication. Even later we finish the frond/back end separation, the current front end has to be changed. Backward compatibility is only required if the UI and back end already separated and we want to connect the old version of UI to the back end.  It is not worthy to carry on the legacy debt here without benefit\nThe origin purpose it to fix a mistake made before. No need to keep it if there is no impact for historical record or the future record\nKeep the interface clean and unambiguous by only allow the int for limit, which is nature for the programmer. Having the extra lines to accept different types is cheap, but it cause confusion. . @wetneb I recommend you do a merge from master for this branch. This branch use the different format of time with the master for the project metadata. This issue was fixed on master and should be merged here otherwise it will cause issue with metadata. At least it is related to issue #1542 . #1524 actually use the old version's test case to keep it behave the same way as before for the date format part. Seems it 's so straightforward to write a unit tests to test at a general scope from the old version. but we can have UT to make sure it's behave the same way as before to some extend(for example the same content will be saved into file). Technically for a new build, we only have the binary for updated version and the normal UT can only cover the current build. Another way is to use other testing method than UT to cover the old versions. . @thadguidry  It happens some time when the metadata file get corrupted. It should nothing to do with this PR though. You can file an issue if you still can see this exception.. @joanneong That is a good question. Windmill was unused for quite long time. If you can take a look to see how much effort requires to recovery it, that will be good. That's the only automation testing tool we had used. It will be useful when we do the front end and back end split. But if it's too much to do, we may consider drop it for now. There are many good testing framework now a days can do better.. Caused by different time format for the metadata created by different branches. Syncing the branch will solve the issue. But will refactoring the code on this part to make the error clear and less confusing. . what's your version of FF exactly?. close it since it's dup of #1549 . I took a look of record Modle when I fixed one bug. Here is a simple one:\nwhen column can group together, we have the keyedGroup. Based on the keyedGroup, rowDependency is generated by searching  the last non-blank cell at following rows. The record model is build upon the rowDependency.In another word, if there is no rowDependency, record model is meaningless. That is exactly the case when importing a FLAT csv file. record model is more suitable for a tree based data structure.\n\nI would say there is nothing wrong with this PR itself, but there some issue with when can we apply the record model and how it should be done.. @ettorerizza You are right. it is in the records mode. but somehow it was automatically switched to record mode after importing. Not sure it is an bug or expected behaviour though. But for the given example, that's not the the right choice to apply the record mode.. @ostephens I think it is correct to leave the null as null instead of try to treat it at importing stage. But the record mode should NOT be triggered for the row-based importing. The idea is that \"I told you it is row based csv importing, why do you present it as  the record mode to me by default\". Only happens on Firefox 59.0.1 (64bit)? Do you mind upload your project dump(Not just data)?. @thadguidry I cannot reproduce this issue using the zipped project. It was loaded without any issue. could you please do a clean build on 2.8 and try again? If the problem still there, I guess the culprit might be other project. you can try to load the project only to verify this.. I use default one 1400M\n$ ./refine\nYou have 15904M of free memory.\nYour current configuration is set to use 1400M of memory.\nOpenRefine can run better when given more memory. Read our FAQ on how to allocate more memory here:\nhttps://github.com/OpenRefine/OpenRefine/wiki/FAQ:-Allocate-More-Memory\nStarting OpenRefine at 'http://127.0.0.1:3333/'\n21:52:31.646 [            refine_server] Starting Server bound to '127.0.0.1:3333' (0ms)\n21:52:31.647 [            refine_server] refine.memory size: 1400M JVM Max heap: 1407188992 (1ms)\n21:52:31.658 [            refine_server] Initializing context: '/' from '/media/cui/software/openrefine-2.8/webapp' (11ms)\n21:52:32.405 [                   refine] Starting OpenRefine 2.8 [TRUNK]... (747ms)\n21:52:39.252 [                   refine] POST /command/core/load-language (6847ms)\n21:52:39.291 [                   refine] GET /command/core/get-preference (39ms)\n21:52:39.302 [                   refine] POST /command/core/load-language (11ms)\n21:52:39.398 [                   refine] POST /command/core/get-importing-configuration (96ms)\n21:52:39.419 [                   refine] GET /command/core/get-all-project-metadata (21ms)\n21:52:39.455 [                   refine] GET /command/core/get-languages (36ms)\n21:52:39.511 [                   refine] GET /command/core/get-version (56ms)\n21:52:43.588 [                   refine] POST /command/core/load-language (4077ms)\n21:52:43.608 [                   refine] GET /command/core/get-preference (20ms)\n21:52:43.625 [                   refine] POST /command/core/load-language (17ms)\n21:52:43.642 [                   refine] GET /command/core/get-project-metadata (17ms)\n21:53:05.684 [                  project] Loaded project 1999903990309 from disk in 22 sec(s) (22042ms)\n21:53:09.158 [                   refine] GET /command/core/get-models (3474ms)\n21:53:09.222 [                   refine] POST /command/core/get-rows (64ms)\n21:53:09.230 [                   refine] GET /command/core/get-history (8ms)\n21:53:09.281 [                   refine] GET /command/core/get-history (51ms)\n21:57:32.454 [           ProjectManager] Saving some modified projects ... (263173ms)\n21:59:43.389 [        project_utilities] Saved project '1999903990309' (130935ms)\n22:04:43.392 [           ProjectManager] Saving some modified projects ... (300003ms)\n22:07:23.019 [        project_utilities] Saved project '1999903990309' (159627ms)\n. @thadguidry You can post you whole log. There might be other error which leads to this exception.. The root cause seems still the Firefox 59.0.1  issue. I closed the #1543 since it's dup. \n. @thadguidry I installed the 59.0.1 on windows 10, 64 bit, but still cannot reproduce issue. But I got something weird though:\n18:00:06.431 [                   refine] Starting OpenRefine 2.8 [TRUNK]... (62ms)\nOpenGL Warning: crPixelCopy3D:  simply crMemcpy'ing from srcPtr to dstPtr\nOpenGL Warning: crPixelCopy3D:  simply crMemcpy'ing from srcPtr to dstPtr\nOpenGL Warning: crPixelCopy3D:  simply crMemcpy'ing from srcPtr to dstPtr\nOpenGL Warning: crPixelCopy3D:  simply crMemcpy'ing from srcPtr to dstPtr\nOpenGL Warning: crPixelCopy3D:  simply crMemcpy'ing from srcPtr to dstPtr\nOpenGL Warning: crPixelCopy3D:  simply crMemcpy'ing from srcPtr to dstPtr\nOpenGL Warning: crPixelCopy3D:  simply crMemcpy'ing from srcPtr to dstPtr\nOpenGL Warning: crPixelCopy3D:  simply crMemcpy'ing from srcPtr to dstPtr\nDid you tune your FF about:config setting somehow?\n. can someone help to reproduce this bug with OpenRefine 2.8 under FF 59.0.1, Windows 10 64 bit?. @ettorerizza any result with 59.0.2?. @ettorerizza Thanks! If you use the default memory setting, can you reproduce it?. close it for now since cannot reproduce.. @gobertm can you please add a unit test?. From the code, seems the n3 format is still parsed as NT format. Tried to force it to parse as N3 but does not work for some reason. will switch to Jena and have another try. I don't think it has worked once. But no worry, since the JRDF library is deprecated, I think we can switch to JENA and that will be working.. @ettorerizza I created the PR #1563. I tried several n3 files and it works fine. \nCould you please help to verify it?. @ettorerizza My bad. There is some issue with the format guesser by .n3. Not a parser issue. \nJust fixed. Tested file: blue.n3. @thadguidry According to this:\nMuch of the functionality is accessed via the Jena Model API; direct calling of the RIOT subsystem isn't needed. \nI tried to remove few of jars and all lead to exception when loading \n. @thadguidry I cannot see the RIOT is a exposed option.\nFor the utf8 part, how is related to this change?. Your project maybe created from one of the dev version which don't have the Z at the end. Later in order to support old version, the Z is added to keep the same format as the old versions. . I tend to leave the code unchanged. It is unfortunate that it happened and screwed up the tester's workspace. But my impression is that it is not in a official release so very fewer user got impacted. So there is no backward comparability required. It is a mistake and it has been fixed already. Adding code to support the result introduced by the mistake is another mistake and will cause confusion. \nFor the toDate('2006-01-01T00:00:00+00:00') issue, I will take a look.. For the file format, we should have only one definition. Either have Z or without Z. Allowing 2 will cause confusion. However, to parse a date sting to date is a different story. I agree we should support more if possible. . Though both return OffsetDateTime, the ParsingUtilities.stringToDate always return the Zulu time, but the toDate will keep the offset as is.. @wetneb Please provide the cases to reproduce. If you do it from your branch, please make sure you merged the master to your branch and test it from the fresh workspace.. @wetneb I cannot reproduce this. I reconciled name \"George Washington\" and use \"Add column from reconciled data\"  to get the \"date of birth\", I can get the 1732-02-22T00:00:00Z without issue. And I can reload it after I bounce the OR. You need have the master code and a refresh project in order to work. If you can get a detail step to reproduce the issue I will give another try. \nPlease see my comment here. The commit 23ec54b is actual to restore the original behaviour, not to change it.\nBut still you can go ahead to improve it whatever you think it's appropriate.  Maybe I am mistaken. $ java -version\nopenjdk version \"1.8.0_162\"\nOpenJDK Runtime Environment (build 1.8.0_162-8u162-b12-0ubuntu0.17.10.2-b12)\nOpenJDK 64-Bit Server VM (build 25.162-b12, mixed mode)\n. According to this:\nhttps://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient/4.5.5\nhttpclient 4.5.5 better go with httpcore  4.4.9. Most likely your version will be working, but better stick to the compile dependency in case of surprise \n. Left the inline comments . Object might be too much. A well defined interface may able to repalce it. i will take a look. but date is an internal type of openrefine. we are\nmoving to java 7 date to java 8 time api. there is nothing wrong with the\ntype. the broken part is the function.\nOn Tue, May 1, 2018, 3:01 AM Antonin Delpeuch notifications@github.com\nwrote:\n\n@thadguidry https://github.com/thadguidry a full migration would be\nvery costly (as you said there are many different classes which still use\nthat). There might be ways to isolate that usage (for instance, it is\nprobably hard to migrate the internals of CalendarParser to use the new\nclasses, but it should be doable to just change its interface).\nPersonally I will not have time to do a full migration before at least 6\nweeks\u2026\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1588#issuecomment-385612547,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AKCGizcG9l9m0PbloZvihTi2k2RrDDSYks5tuAhLgaJpZM4TrYsr\n.\n. I think the java 8 time API is the right way to go even it will take effort. It is safe to just keep the java 7 Date and Calendar API but it is not the long term solution. The original code has a pattern to deal with the date like this:\nif (instance of date)\n  do something;\nelse if (instance of calendar)\n do something else;\nI doubt if it's necessary. Even it is, I think some of them is for backward compatibility.  We can consider to use the OffsetDateTime to unify those kind of block if possible. At least I know for the cell date type, we can store the \"date\" string to indicate its type, not the java class. So potentially it's possible to only change the code but keep the file format untouched. . @wetneb can you please  write a UT to cover this case?. @webnet No worry. you can cut a tag or branch to hold the version for the Wikimedia Hackathon 2018. At the same time, i will have a branch to fully migrate to java 8 time API. It will take a while but it is not impossible to finish before 20th. But if we cannot make it, you can still have the branch to show. :) what do you think?. No problem. If there is no objection, I will merge this PR to master so I can create another branch on top of it. Not sure that can be done directly or not. \nIf not,  you might need \"edit column\"->\"add column by fetching URL\" if you have have the wikidata Id:\n\"https://www.wikidata.org/wiki/\" + value. Then you need to parse the result using:\nhttps://github.com/OpenRefine/OpenRefine/wiki/StrippingHTML\n\n. @thadguidry It is already in datePart():\nif (\"nanos\".equals(part) || \"nano\".equals(part) || \"n\".equals(part)) {. I can reproduce the issue. But it does not have to open it from the master version of OR. To reproduce on V 2.8:\n1. add a date column from recon\n2. shutdown the OR\n3. Reopen the OR\n4. undo the column just added\n. yes, it is cached. Let me try V2.7 and report.. @ostephens Right, just checked this feature is not in 2.7 . fixed by PR #1617 . @ostephens Thanks for the UT. The PR branch has removed the Calendar to represent a date type. All the date cell will be represented as OffsetDateTime. so in my UT, I only pass the OffsetDateTime type as the 1st argument. The V2.8 use default Calendar constructor which use the local date. This does not sound right when dealing with the date whose data source could be come from anywhere. The test cases passed on 2.8 but won't pass in this branch. But as I mentioned, since in real life a Calendar won't be passed as argument due to all date will be using OffsetDateTime, probably it is not that relevant any more.\nI extracted your UT but use the OffsetDateTime for the date type.. why you need cross function instead of just use the column in the same project?\nAlso, could you please paste the exception from the back end?. You should be able to see the console for it. it is not in the log.. @martinjrobins Have you tried OR on 10? I never have one so I am not sure. for 9, we did some testing but  there was some minor issue at that time. It maybe has been fixed.  \nI would prefer to have the version 10 tested before we put into script otherwise will give people false hope. :). @ostephens when you copy from your current folder, maybe a merge happened so you ended up with some old classes which caused the issue.. why the \"apply and unapply the change\" is not needed for DataExtensions?. you need to remove the space in the path.. @DFoltzMorrison  Glad it works eventually. sorry for the inconvenience. Will add into next milestone. . Tested on my machine, it is true that quotes do not appear when it is unchecked. But no exception found at back end. Linux 18.0.4, Java 8, Chrome. . @thadguidry Maybe we had some false expectation on this switch. Seems it only define the field boundary, not a switch to enable/disable the show of the double quote.\nYou can try following data set and it will be clear:\nc1,c2,c3,c4,comma\nc1,c2,c3,44444444,\"1,000\"\nc1,c2,c3,44444444,\"hello, world\"\nIn teams of how to import a double quote, a double double quotes will work:\nc1,c2,c3,c4,comma\nc1,c2,c3,44444444,\"\"1,000 double double\"\"\n. we are still using opencsv 2.4, but the updated version is 4.2 already. we can consider upgrade it given the newer version provide more features and better performance.\nThere is some comparison of csv libraries. . created PR #1628.\nuse 2 keys in preference:\n\nThe connect time out: googleConnectTimeOut\nThe read time out: googleReadTimeOut\n\nFor this case, I set the googleReadTimeOut to 12000 and it imported without issue.. fixed. http://headers.jsontest.com cannot return json due to the \"Over Quota\" response. Will skip the UT if it's the case.. Can you try to reinstall the app but when you ask if you want to \"keep\" or \"replace\" the old version choose \"replace\"? I think that will fix the issue.. That's weird. I can reproduce your error if I choose \"keep both\". but if I choose \"replace\", it will fix the error.\nThen can you manually clean the installation folder by run: \nmv /Applications/OpenRefine3.app/ /Applications/OpenRefine3.app.bak\nThen do a installation.. @yaeln The old projects should be automatically imported.Also I noticed that you installed OR under /Applications/OpenRefine3.app/, the default installation folder should /Applications/OpenRefine.app/.\nCan you do a ls -ld /Applications/OpenRefine* before and after your installation? or you can upload somewhere as suggested by @thadguidry so I can take a look.. @yaeln Can you install version 2.8 and see if it's working? Seems it is related to the location of the path which has a space in between. I cannot find any code change might be affecting this. Also my version of MacOS Mavericks works fine and the old projects can be imported automatically.\nAlso you can try to move your project to: \nmkdir /Users/yaeln/.openrefine/\nmv /Users/yaeln/Library/Application Support/OpenRefine/* /Users/yaeln/.openrefine/\nThen edit the refine.ini under installation folder to add:\nJAVA_OPTIONS=-Drefine.data_dir=/Users/yaeln/.openrefine/\nLet me know the result.. @yaeln Seems related to the data itself instead of installation. will do some debug on the projects you shared and get back to you.. I can open the projects you shared using the same version. so it is not a data issue.\nDo you still see the exception after install, if so please share the whole log as you did.\nCould you please click \"open project\" then click the \"Browse workspace directory\" at the bottom and let me know the folder location? you should be able to see all of you projects in its folders and at least a file called workspace.json etc. . @yaeln Your default workspace is empty. I think for some reason you 2.8 version points to different workspace other than /Users/yaeln/Library/Application Support/OpenRefine. You may be did the change and forgot.\nThe solution is simple. you can just find the original workspace and copy all to the above folder. If you don't know where the original workspace is, you can just startup the 2.8 the same way you did above and you will find the location in the log by searching the line:\ninitializing FileProjectManager with dir. @yaeln I am got confused. From this post, I did not see you had exception java.lang.LinkageError as you did in previous post. You may only have this one issue  and the project loading is the consequence of the failure.\nSo please do a refresh install by choose \"replace\" when you see the dialog box then start it to see if you still see the LinkageError  exception. If you do, please zip your whole installation package(not the projects package) and share with me. Thanks a lot\n. I downloaded your app zip above and did a clean install and it can run on my Maverick and load the my projects without issue.I know you are using High Sierra\n@ostephens @wetneb , do you have some time to download above zip @yaeln provided and have a try on your mac. It is very weird issue.. I also don't have the JAVA_HOME set but it works fine. \n$ java -version\njava version \"1.8.0_60\"\nJava(TM) SE Runtime Environment (build 1.8.0_60-b27)\nJava HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode)\nNow I have @yaeln 's app and work space. it also works fine. can you installed the updated JRE and have another try? cannot think of other possibility. . can you please paste your whole startup log?\nAlso you can try to move the openrefine-2.8 folder and c:\\ and try again. There is a space in your current path which may break.. it is a typo. :). I think starting from the function package might be a good starting point. It is isolated and easier to testing. so you can get familiar with the migration path. Then can move to more critical part and modules which need more effort to do the testing.. what's the connection between the fix of #1632 and changing the default format of StringUtil's toString(date)? I think the previous format is more concise and human-readable. . Making the conversion consistent make sense. \nFew things I observed: \n- After pull your PR on local, I still can see the issue of #1632. Ie, when selecting a date facet, it gives me nothing instead of the 2 date rows\n- This is an issue on version 2.7 also\n- The \"facet by timeline\" works weird. The right boundary controller is gone   . OR's data type is stored at the cell level instead of the column level. It make some sense considering it deal with messy data where you can not expect the data is normalised as one specific type. But allowing set the data type to one cell looks weird. Imaging you have 1M rows data set, it is not feasible to set it one by one. I would recommend something like \"preferred data type\"(default to text) to the column and when doing the facet, user doesn't have to think about it should facet by text, Boolean or date. \nGrouping the cell by data type also looks weird. User then have to go into each data type and fix them one by one. In another words, cannot see/process the whole facet in just one view.. Could you be specific what's your expectation? OpenRefine can import the excel but does not support the filter like Excel does. The similar functionality of is facade though which you can filter the data and edit them by batch.. I am not sure it is possible to importing the sub-column like you expect. But supporting \"Column Groups\" and \"importing a auto-filtered excel\" seems different. Auto-filter is a \"view\" of the excel, not the real data get saved. Alternatively, you can manually merge your 2nd row with 1st row to build a header and import into OpenRefine. . @ostephens I am little bit confused about the intention of the HttpHeadersSupport class. It seems was designed as an immutable class to inject few http headers. In that case, how can a user to setup something like:\nAuthorization: Basic YWxhZGRpbjpvcGVuc2VzYW1l. The same error with this issue:\nhttps://github.com/OpenRefine/OpenRefine/issues/1636\n. Should this be part of the translation or part of html/style? I would prefer this change on the html/style to avoid mixing up those two. Also, the change only make the English looks better and that should be nothing to do with the languages. . I cannot reproduce it. can you please retry it?. close it for now. Please reopen if still an issue. Looks like related. @ostephens or @wetneb can take a look. Good catch! merged. can you please provide your JRE version? Try to remove the previous version and reinstall.. Is there anyone who has the same issue? Otherwise will close it if there is no response from the issue owner. Most likely it is a one off issue related to the run time environment. . @fgibaux what's your OS and version?. Is this installation a fresh one(a new folder)? Try 2 things to isolate the issue:\n- try to rename your current openrefine folder to different one and do a installtion\n- try to rename you workspace folder to see if it is caused by the data. The default workspace is under $HOME/.local/share/openrefine. @macfraser OR only support up to Java 8. Please uninstall your java 10 and install java 8. And delete your installation folder completely and install a fresh one.. could you please provide:\n- the exact version you downloaded\n- windows version\n- Java version\nAnother thing you can try is to switch to another user which has no space in between. @wentianq Most of the OR users are using Java 8. Java 9 is not fully tested and there are some known issues. Java 10 is barely tested. I would suggest you to install the version 8 and set the JAVA_HOME environment variable and have another try. . Are you sure  Java 8 is picked up? Did you set JAVA_HOME variable?\nYou can run the refine.bat from the command line and paste the output . It is not right to exit quietly when you run refine.bat. are you running it from the command line?\ncan you run \"refine /h\" to see what happen? . no, you don't have to do pause. it should print the message on your command line console when you run refine.bat. If it does not, then something is wrong . Sorry to hear that. It is not easy to troubleshooting your issue since it is hard to reproduce. could you please try two things\n- try to rename your current openrefine folder to different one and do a installation\n- try to rename you workspace folder to see if it is caused by the data. \nIf still cannot resolve, you can upload your copy of your installation somewhere I will take a look for you. . You can just rename the folder:\nC:\\Users(user id)\\AppData\\Local\\OpenRefine\nAnd start again from the command line:\ne:\\openrefine > refine.bat. I suspect that also. For some reason, the js may be combined in different way as usual. Maybe caused by Java version and OS specific. I am sure if I can reproduce it, I can fix it.\n@pollyhunter what's the version of your Java and the OS version?. put the fix in master. will release at V3.1. Or you can apply  PR #1745  before the release. can you run refine.bat from the command line under the same folder has the exe file and paste the whole output from the beginning?. seems one of your project is corrupted. \ncan you move the below folder somewhere and start again:\nC:\\Users\\dpache\\AppData\\Roaming\\OpenRefine\\1685431202422.project. We still have the refine script? I thought it will be remove and use maven completely.. put the fix in master. will release at V3.1. Or you can apply  PR #1745  before the release. I managed to reproduce the issue on a windows 10 machine. The root cause is that jsoneditor.js include some unicode in the code and has to be read in UTF8. otherwise the generated package will be messed up and that will lead to a syntax error.  It won't be an issue since for most of the js file it is optional to read it in UTF8. It won't be an issue on window 7 either, I guess they handle the encoding slight different(I cannot say it is a flaw on windows 10.). For any browsers under 10. @wetneb Seems there are some compilation error for testing. could you please take a look?. I am wondering if there is a good practice to maintain the interface between the core and plugin in a way that can allow the core part change without breaking plugin. Or Openrefine's plugin mechanism is too rigid to extend.. @wetneb I don't think the build system is the root cause of the current situation. But migrating to maven may help to expose the problem at early stage. Basically,  it boil down to the same idea @ostephens propose: the version control. One is from the build system, another is from code level though.. Could you please be specific about the issue or the steps which can reproduce the problem?. Disable of http2 from Ngix make sense since http2 only works better/faster when the application use the SSL/TLS:\nhttps://www.nginx.com/blog/7-tips-for-faster-http2-performance/\nAlso I think there will be some http1.1/http2 conversion occurs which also take time.\n. Let's do it as @ostephens suggested. It make sense for me. Maybe we can optimise for different DB if the performance is a concern.  . @wetneb Do you know if Wikibase has any plan to migrate to new API. @wetneb can you please provide a procedure to reproduce this issue so others can understand better about this issue?. can not see the butterfly change. where it is?. I think the purpose of overriding the \"loadClass\" is to have all the extension has its own set of classes instead of influence each other. Though it fixes the jackson issue, we may need to consider the impact overall. Not sure how many extensions are still in use. My expression is that most of them won't work after our major change recently.. For point 2, I would say String might be an option to find the common media between the core and the extension. It is a shame that Java don't have the light weight json  yet even other languages already embrace it as the 1st citizen class. Developer still need to reply on 3rd party library to convert the json back and force.\nIt will break anyway. so would be better to make it right and use the native representation . To close issues #1620, #1616, #1890, #1899. Sure. We can replace with openjdk7. Actually I raised the issue in the dev\ngroup yesterday. I can follow up the existing procedure to get this done.\nPlease advise\nOn Jan 25, 2015 12:01 PM, \"Tom Morris\" notifications@github.com wrote:\n\nIn .travis.yml\nhttps://github.com/OpenRefine/OpenRefine/pull/934#discussion_r23505520:\n\n@@ -1,6 +1,5 @@\n language: java\n jdk:\n- oraclejdk7\n  -  - openjdk6\n\nDropping support for an environment should be called out explicitly rather\nthan hidden in the patch. We'll probably replace this with openjdk7.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/934/files#r23505520.\n. Due to the order of the creation of the dialog, if use the dismissUntil the \"waiting\" window will get dismissed automatically when click the \"sort\" button. So dismiss the specific dialog to prevent the over kill.\n. If look at the RowVisitor interface, the getAllRows method is NOT read only which sounds like. so  it's also necessary to protect the methods since there is client such as python-client can access it in different ways\n. Agree, do you want me to commit the change or create another branch from the fork?\n. \"copied from somewhere\" always can be improved. will revisit the room to pull up to parent class.\n.  initial capacity of 1M is too much? 1k make more sense.\n. clear() won't shrink the HashMap.\nWeakHashMap maybe better in this case. \nAn entry in a WeakHashMap will automatically be removed when its key is no longer in ordinary use. \n\n. why not use the urlString as the key?. Better to keep this try block in the fetch() method. More efficient and clear.. Agreed. just changed. yes. will add some test for the new command. The deletion was an incident. thanks for catch-up. will put it back . Make sense. just changed. We are using mockup object, so no ServletException. :). I believe there is max size of JSONObject. need to dig more on this. . agree. Not sure the StringBuffer is necessary here. It is one time assign. String should be more efficient and straightforward. You can also consider a HashMap. . The row count should be recounted after operation. Thanks for checking. just make sure.. no. for some reason, my eclipse automatically changed the setting while importing project. fixed. Right, that's the spec. it is kind of weird when I first saw the title for the column. But after 2nd thought, it make sense.. Did you do git clone --recursive https://github.com/OpenRefine/OpenRefine. I think we should put name ahead if we are sure the name supposed to have a value. flip it around silently hide the issue and redirect the execution to last default case.. removed. removed. removed. to skip the 500 error etc, I think:\nif (ref_val.startsWith(\"HTTP error\"))\nwould be better. . The name is just a key for internal exchange(from the javascript), not the file name itself. Not sure if this should under the model level? maybe should a preference?. Files moved . added javascript facet to indicate having javascrip and eclipse can handle the check, highlight etc. sample project is not necessary . moved. No. nothing is changed. but just adding some comments to make it clear. comment added. /\n * We are noting save other metadata file for now, only the ProjectMetaData is saved.\n * But we keep this command in case we want to save the medadata which requires to be saved for different reasons. \n * For example we cannot restore the original metadata if don't save it\n \n/. The broker is not being used any more.. This extension was including in the openrefine. No need to keep it and maintain it. That's part of the table scheme java library. moved to single object . added back. No. this is server-side resource. we don't have the server-side i18n support yet. Can be added as a infrastructure and then do the localization.. it's remove and added backed as above. As I mentioned above, it can be used for future metadata format. FIXED. Once we have i18n support on the server side. For now, it's a placeholder for future use. There is UT ValidateOperationTests.java to demonstrate the basic use of it. but we don't have the UI to trigger  this. It's non-trivial task. we can add it in future on top of current UI infra or defer until we separate the front end and have a modern UI framework.. Current OpenRefine is a single project and single classpath. It is not worth keep it if we don't use it and have to maintain it and confuse programmer. Once we move to other build system such as gradle or maven, those setting will be all gone. Since we don't use it, deleting them can pave the road while we do the gradle/maven migration.. same above. The gdata src already covered by OpenRefine project. The way Butterfly handle the javascript can not fully compliant with the ECMA. Disable the semantic validation to stop the annoying message. It is required by table schema . The original intention is to suppress the warning messages from the js validation. But it can not exclude all the JS files it meant to. . removed. . Again, this already covered by OpenRefine project. No need redundancy to setup the project . Let me paste the diff and explain:\n-                ProjectManager.singleton.getPreferenceStore();      // 1 line was removed \n+      \n+        // project level has no specific preference                       // 1 comment added\n+        PreferenceStore ps = ProjectManager.singleton.getPreferenceStore();  // 1 line was added back. remvoed. The diff given by github didn't provided 3 \"-\" for some reason. it only shows 1 \"-\".\nReverted the change. can we have 3 files for local, travis and appveyro instead of doing the copy?. should this be handled at front end to only allow 1 char(or possible restriction on what char cannot be used)? If use input [blank]'[blank] then it will cause problem. . updated the json. There is no encoding options to get a spreadsheet:\nhttps://developers.google.com/sheets/api/reference/rest/v4/spreadsheets/get\nI think it make sense because all the data can be stored and unified to utf-8 by google service/google API. . It is a registered ids but with new set of APIs enabled. I created the Id . done. we do not need this. removed. . This is a INFO level logger. should we need that?. well, it is not recommendation from Google. 300 mins seems too much, Let me find out what value is better. . It is already there in json file.. My understanding is HttpRequest uses the HttpTransport:\nhttps://developers.google.com/api-client-library/java/google-http-java-client/reference/1.20.0/com/google/api/client/http/HttpTransport.html\nHttpTransport is underline layer. so HttpRequest should also thread safe. done. set to 3 mins. it make more sense.. Should the package be \"com.google.refine.*\" or there is reason for that?. A unsupported exception should be thrown instead of return null silently  . freebase not available anymore. . why we need the class to hold the instance?. No saved? Maybe a cookie is an option?. Always respond \"logged_in\" no matter if it is really login in at the back end?. As a best practice, consider to log it instead of printStackTrace.. A little bit weird to pass \"update\" and return \"update\". Can skip the return?. should be List here?. For performance, is that possible to process the \"update\" in one streaming. Also parallel can be used . Make sense. I misread this. I mean:\nrewriter.rewrite(update);\n. It is fine if there is not so many elements. . No need the {}. or  you want to extract into method?. why not just create a method reconstruct(String jsonString) to avoid string->JSONObject->String->Jackson ObjectMapper->Object conversion?. Using another advanced JSON library is fine. But if it is possible we should avoid converting back and force. It is hard to read and debug. Also itself create a hurdle to migrate off the JSONObject. If some places JSONObject is unavoidable(for example as part of the interface signature), maybe use a POJO is easier make more sense.. To EACH project? should overlay model always optional?. Maybe I am not quite understand the wikibase. Seems the menu \"Edit wikidata schema\" is to publish, \"Perform the Edit\" is actual edit. Am I right? . In general, the Scrutinizer's extension chain  is a little bit long. For example this class could consider be flatten. Providing a method to processing a list does not bring too much value. seems not worth another layer. Some abstract class(for example EditScrutinizer.java) can be replaced with interface(java 8 support default method):\nhttps://www.journaldev.com/2752/java-8-interface-changes-static-method-default-method\n. will change to DEBUG. @wetneb I sent the info though the email to dev core. The token is to identify the application. Further more, before can use the API you need to use your own email to authorise. . The reason is there is anther CellData in:\ncom.google.refine.exporters.TabularSerializer interface.\nBut yes we can skip the things between <> when new. This is part of result of the merge. It won't do anything after merge back to master.. I think maybe keep the null as-is is better to understand . I mean from the translation aspect, it is not so clear to me what's the difference between those 3 facets.  \u6309\u7a7a\u503c\u5f52\u7c7b is a little bit confusing even for the programmer, not to mention the non-programmer end user. keeping null untranslated(as is) may not be best option, but it is better than confusion. My suggestion is \"\u6309null\u5f52\u7c7b\". @wetneb Not the whole string not get translated, just part of it(null). \n@joanneong I can tell you can understand Chinese from your name. I am from China but from the translation itself alone, it does not get across the meaning of \"facet by null\". In other words, there is no good Chinese translation for null. Instead of trying to translate it, I would rather keep the null as is. Like the \"Unicode\" new lines above . I can only speak for Chinese language. For this particular term, I believe keep it as-is is better. There might be good fit for null in other languages. But the principle we should follow is that if the translation cannot get across the real meaning then we should NOT try to translate it to prevent further confusion. . Should this be controlled from the front end? We can have the validation at back end, but we may not need write the error into the dump file.. Not sure is there any database type required here? Oracle or Mysql? . Is assuming them a local time correct? There is method stringToLocalDate to handle this. For stringToDate should always parse as Zulu as my understanding. . Not sure the exact reason why the array of list were not designed as storable originally. But one of the reason might be storable also means recoverable. A array/list of string generated by split function is recoverable. But a list of WrappedCell or WrappedRow are not recoverable. So might be good idea to narrow the scope of the isArrayOrList . I mean array of wrapped cell , row.. It is CalendarParser now. a typo but got fixed at later commit. . 1400M is not enough any more?. can those properties and items configurable instead of hard coded? Also a document link will be helpful if someone want to add / update some of them.. what's the implication of this change? Maybe add some comments since Datamodel is from wikidata.. Any better way to check if the text is a valid URL other than converting it to string? . Not sure if we should return the error here(and other classes in this change). For example, when cell is Date type. Is it necessary to return another copy of the offsetDateTime with the same instant value? If so, could you please add a UT to prove the previous way does NOT work?. should be \"types\"?. :+1: . ",
    "answerquest": "Hi, wow this was filed quite long ago!\nI just want to be able to see the facet/cluster I'm seeing on a map. I came across this extension, but they don't seem to have that. What I presently have to do is export as CSV, then import it in a mapping app like geojson.io.\nThere's a complication that the latitude and longitude are usually stored in two separate columns.\nHow about Export > To Map? We could do some tweaks to the existing HTML table export option.. ok :+1:  looking forward to the new UI!. ~Hi, i'm also facing similar issues. The CSV writer part must be set to default encoding instead of unicode. If someone can guide me to where the relevant code lines are, I might be able to do a PR.~\nEDIT: My bad. Had to set encoding as UTF-8 at file import stage.. Hi @ostephens thanks for the workaround suggestions. I've created a test file with the troublesome chars in one column for testing:\nname,char\nleft quote,\u201c\nright quote,\u201d\nleft single quote,\u2018\nright single quote,\u2019\nen dash,\u2013\nem dash,\u2014\nhyphen,-\nellipsis,\u2026\n I just loaded it at my end (a Lubuntu OS) and exported as csv and there was no problem in the output. But the people who worked on the earlier file whose screenshot I shared had worked on win10 OS, so I'll have them test it and share the results.. @ostephens ok cancel all my last posts.. it turns out in Windows we have to set the encoding to UTF-8 when we import the file, else it reads as ansii. In linux it was UTF-8 by default. I found out from running the test file on my colleague's windows computer. There is no problem with the  CSV exporter so I was barking up the wrong tree, my bad. I'm going to ~strike~ my comments above. . Yay! So, curious: is this fixed in the latest version to download?\n. @wetneb yes, even I have the same suspicions, that it's specific to this environment. And the tool ran just fine on my end with the workaround. For now I'm closing this as it's solved at my end, and hope when others having this problem search in the issues they find it.. Hi @thadguidry , I'll have to correct this part : \n\nbut always same data structure\n\nI've suggested a way to be flexible about that at the end of my post:\n\nSave the internal path of the data item that was extracted in a column\n\nBy this I meant create a new column that stores the internal path, just like right now if I load multiple data files instead of one, then OpenRefine creates a new column to store the filename that the data came from.\nAnd yes, I too am using python-pandas to re-integrate my cleaned data back to my json files. But it would be great to have this as a feature / extension, to make the user experience with jsons match the case of uploading tabular files : you get your data back with everything else still the same, only the target values are changed in-place.\n. ",
    "ianfiske": "I think that this issue still exists.  I am using the new reconciliation API referred to at https://github.com/OpenRefine/OpenRefine/wiki/Reconciliation.  The new API url is  http://reconcile.freebaseapps.com/reconcile.  The initial reconciliation works well, but 'search for match' does not return anything.\n. ",
    "Downchuck": "Also seeing this issue in current.\n. Are there any updates on this bug? I am noticing random issues with cross join where I need to restart openrefine.\n. This would be a nice feature; I hit another roadblock when using the workaround, running a large \"sort\". It seems the sort code may not work well with large data sets (150k records).\n. I'll do that, looks like a possible issue in the RuleBasedCollator implementation; I'll post an issue and pull if I get it fixed.\n. Seems like it's possible that some of the insertion from jQuery is done live in the DOM instead of creating a dom fragment or otherwise building the nodes outside of the dom and doing a single insert.\n. @prairieskygal pop on a screen cap and paste the URL here and I'll try to reproduce.\n. This may also be a bit of an issue with the manner in which data records are added to the page. If we use a queue and setTimeout, we may be able to avoid freezing the browser. For an example of the data I'm working with: 10 records means about 10k rows and I'm freezing around the .append() method without an easy means of switching over to the rows view.\n. For those looking for a quick fix, add Math.min to limit rows on line 414, example:\nfor (var r = 0; r < Math.min(rows.length, 100); r++) {\n./main/webapp/modules/core/scripts/views/data-table/data-table-view.js\n. To workaround, make sure your second sorted row is not the first row, and [unconfirmed] follows your primary sorted row. For example: If row (a) has blanks, and row (b) does not, and you want to sort by row (b) then row(a), you will need to have row (a) follow row (b) in your layout.\n. Switching to row mode did not seem to address the issue. I will work on getting a repro setup.\n. Will do; it seems like it's kind of \"stuck\" in record mode once you blank down on (a), regardless of other actions.\n. I was just thinking PDF tables for this revision; it's a standard procedure to simply put each table in its own \"sheet\"; so the import would look very similar to the Excel import.\n. I'd keep the per user item out for this draft; simply having multiple workspaces is all that I'm after. I've currently got a project I have been working on on my own home PC. I'd like to transfer that project onto a server. The project uses cross() and given that I'd like the steps to be repeatable, it means that there are cross-project dependencies. If I just import one or two files and not the others, then I'm not maintaining those dependencies. Thus the workspace concept.\n. Big apologies on not having patch sets for these bug reports. Soon as I get something together, I'll start sending pull requests. I believe text filter as well as add column based on column textarea both have an issue. Noticed it as I was using a mouse to copy/paste things next to a client.\n. Yes, it's meant to represent the current subset of rows, if the data has an active facet, then the facet would effect the rows.\n. Not sure what that first word was supposed to be on this bug report; but, trying again: I'd like to be able to paste in / copy a JSON string during project creation when importing files, to restore and save the settings I've used to import the file.\n. It seems possible that the merge sort is a horrible idea for large record sets, it's also possible that the sorting comparison is doing some string manipulation on each run. I'm considering taking a peek at Guava and routing around the issue. Hoping that my 300k record set completes in the meantime.\n. Confirmed that this is a result of the commit 209f157656baf050068b698a4490fb9265039959 which introduced Collator for support of text with accent characters. \n. ",
    "Opennat": "Comment here, as I think, it's corresponding:\nSelected value in Text facet should be switched to new value (or values), if it changed via cells' Edit (when new value applied via \"Apply to all identical cells\"). Now it stuck on old value with zero cells.\n. Saved expressions in transform window should have option (or may be separate tab) to use them across all projects, not only in current one.\n. Pardon about so long answer, there was the reason.\nRefine importer does not check for empty keys (or it does not work).\nCheck it out on simple JSON:\njavascript\n[\n    {\n    \"key1\": \"value1\",\n    \"key2\": \"value2\"\n    }\n]\nColumn names in Refine for it is _ - key1 and _ - key2, respectively.\nIt is unnecessary to add any placeholders in this case.\n. Found this issue #524, but not the pull.\n. Yes, thanks for clarifying, @magdmartin\nAs now, you should use mouse or tabbing many times to press \"Close\" button to just close dialog.\n. Excuse me, it's already fixed in dev branch (not in 2.6-beta, available for download).\n. I've tried to move columns on each side in some project, and no errors/exceptions or any visual changes have been seen.\nbtw, in 2.6-rc1 \"Edit column -> Move column right\" does not work at all for me (any column name/position in some projects tried, any type of cells, Chrome/Firefox under Win9/Debian locally/remote combinations). No errors (or any actions) in Open Refine log, no actions/errors/exceptions in browser (only one for a long time for Refine: \"Synchronous XMLHttpRequest on the main thread is deprecated because of its detrimental effects to the end user's experience. For more help, check http://xhr.spec.whatwg.org/.\").\n. To disable browser autostart, add\nJAVA_OPTIONS=-Drefine.headless=true\nto refine.ini\nThanks for hint: https://github.com/tabulapdf/tabula/issues/534\n. True, but when you have project with many facets(/filters) number - you need to keep in mind them all time, then set them up again, or constantly (at any change of them) use \"permalink\".\n. We work, I think as many of OpenRefine users, at large datasets, say, current project is 18.000.000+ rows with 17 columns. Even when you limit data with some filters/facets, there still many rows in result. Before applying any transforms over the results' cells, there is a good practice to look at these results: to add/delete/move/replace some data in some columns or some data depends on data in another column(s) and you need to apply another filter, and so on.\nNow, we have ability to jump over pages in strictly predefined way: \"X rows(records) before/after\".\nYes, in our Refine we have already \"tuned\" data-table-view.js for\n\nthis._pageSize = 200;\n\nand\n\nvar sizes = [ 100, 200, 500, 1000, 5000 ];\n\nto get 200 rows in default result and to have ability to get more via filter menu.\nBut many rows in result table overloading and overheating browsers' DOM (browser lags getting worse as you increase count of results). So I think, that Clusterize or similar library will be very useful to avoid hard impact at DOM.\nPlus, in current implementation, every time you click \"previous/next\" links (you must use mouse/touchpad for this), focus is getting lost from results table, and you need to use click to the table to be able of using PgUp/PgDown again or should use mouse to scrollbar (or use touchpad for this). Each of these movements is loss for faster and effective work.\nAnd I did not said, that current pagination should be replaced, mind it as UI enhancement. Pressing \"X\" times on PgDown without clicking over pages is far faster and simple, that the way we have now.\nSo I think, with library, that I mentioned above, we could get almost free implementation of \"infinitive scrolling page\" of results (in limits of browser, sure), and all of the results will be sit in browser window always visible (in limits of Refine UI), so you dont need to scroll it in way, as it happens now.\nP.S. Maybe it's time to change default limit of showable rows to 30-50, as minimum, instead of current 10? I dont think, that users who use Refine, using such small limit at all, may be in testing purposes only. Anyway, default limit of 50 rows would not affect anyone on 10 years or more modern computer.\nP.P.S. Pardon about language, I hope you would get the idea.\n. I know about destination and limits of OpenRefine, and I don't complain of any part of it or pretend to provide \"best usage reciept\".\nIt was suggestion to make Refine more useful for anyone, who work on datasets even 10 times smaller, but have usability issues.\n. At the moment:\nOpenRefine 2.6-rc.2\nChrome 47.0.2526.106 / Firefox 43.0.4\nUbuntu 14.04\nAnd behaviour in these browsers is completely different: blank tab in Chrome still after saving, but not in Firefox.\n. In addition to what have been said above, sometimes your edit is really applied to the cells, and sometimes not (you could check this in another browser tab with current project). In both cases you will see yellow \"popup\": \"Edit ..... on row X, column Y Undo\".\nThis bug occurs (I believe) in 100% cases, when you have 2 or more filters/facets active.\nI'll check this later and make screencast.\nP.S. Forget to mention, this bug probably by the same nature of interface movement, as the one, when you apply facet/text filter, get some rows count, which need to switch to \"Next page\", and when you click on that link, you switched on next page, and immediately switch back (on second click you switch on next page(s) normally). Did not found in Issues at the moment, but sure saw him there.\n. Same https://github.com/OpenRefine/OpenRefine/issues/1017\n. ",
    "eswright": "Is this what might be causing my json import not to give all records an index?  . Any suggestions on how to rebuild a record index?   All my data is parsed but just a bunch of records are merged into one.. I'm confused.  If a facet is set to show a subset, the \"duplicate facet\" or facetCount should act on the selection set.   The question above is not trying to work on more than one column.  @thadguidry \nI've pulled my hair out on this in 2.8 and now see the same behaviour in 3.0. \nIs this the best workaround? Create a column of the facet selection you want first, so that you can remove any facets you have on before using facetCount or the custom \"facet duplicates\". ",
    "sparkica": "Same issue on Mac OS X 10.7 and 10.8 with Chrome as a default browser and 2.6 alpha mac installation kit.\n. In case there is no option to 'Open' the application on the warning dialog, control clicking it helps. I guess this depends on the computer's gatekeeper settings.\n. If I'm not wrong, linux target is for distribution only - classes are not copied, because a jar file for server is created instead and copied to server/lib. To build OpenRefine on Linux (I'm using Ubuntu) it is enough to use \"build\" target. \n. In this case you should build it like ./refine linux_dist or ./refine windows_dist (check refine script for more options).\n. I'm glad I could help :)\n. I apologize for trashing with two references to this issue. I was not able to remove the one with commit c9cc0e2, so please ignore it. It is basically the same as the next one but it was in a different branch... I just lost two hours of my life figuring out how to remove the first reference to the issue :)\n. What should we do with Fetching URLs From Web Services? It uses Google Translate API, which is not free anymore and it uses Facebook page, which requires to log in (i.e. to have an FB account). Should we come up with different tutorial demonstrating the service?\n. A friend of mine experienced the same issue on OS X 10.8. He solved it by building the OpenRefine from source on his own computer. I suggest you clone the repository, build it ( ./refine build) and try to run it. \n. Thanks! I'll merge the current repo and make a new pull request. I'm closing this one.\n. I had similar issue when from refine.bat. After changing REFINE_MEMORY=3000M to REFINE_MEMORY=2000M in refine.ini I was able to run it.\n. This happens with 2.6-alpha1 kit. Roughly once a month I merge OpenRefine's master into LODRefine without changing OpenRefine's files with exception of refine.ini and some stylesheets added. I tried with Java 1.6 and Java 7, I also set version, target, source and bootclass to 1.6. It works when I run it from terminal or from Linux dist kit - all the extensions are loaded... just not when I run it from .app.\n. All this was tested on Mac 10.7.5 in Chrome 28.0.1500.95. I made sure that there are no extensions in the workspace extension's folder.\n1. I downloaded 2.6-alpha1 kit for Mac, installed it and double clicked OpenRefine.app. Browser window did not open, so I had to enter localhost:3333 manually. It loaded fine, but without extensions. Running OpenRefine.app doesn't open an terminal so I couldn't see if there was an error.\n2. I ctrl-clicked on OpenRefine.app, selected Show Package Contents. Double clicking JavaAppLauncher in Contents/MacOS opened up a terminal window but there was no error related to extensions. That makes sense as extension's path was wrong.\n3. After setting butterfly.modules.path in Contents/Resource/webapp/WEB-INF/butterfly.properties from ../../extensions to extensions and running OpenRefine with JavaAppLauncher, terminal opened up and I got  org.mozilla.javascript.EcmaError: TypeError error for every extension (error for pc-axis below).\n4. I cloned latest OpenRefine master (not LODRefine!), built the mac kit myself I ended up with the same issue.\n   Kit was built with ./refine from the terminal.\n5. I removed every extension but freebase and added bootstrap class path to every javac in the main build.xml    and in the Freebase's build.xml. Bootstrap class path for 1.6 on Mac is /System/Library/Frameworks/JavaVM.framework/Versions/1.6/Classes/classes.jar\n   This didn't change anything.\nAfter displaying environment variables during build I noticed I don't have JAVA_HOME set. Setting it to 1.6.0.jdk got me following build error:\nOpenRefine_fork/clean/OpenRefine/build.xml:237: java.lang.UnsupportedClassVersionError:   com/oracle/appbundler/AppBundlerTask : Unsupported major.minor version 51.0\n6. So... I downloaded and installed jdk7, set JAVA_HOME to point to jdk1.7.0_25.jdk, left version and bootstrap set to 1.6. Every time before building I ran ./refine clean and ./refine distclean, just to make sure no old classes are messing it up.\n7. Finally I changed java.version to 1.7, removed bootstrap classpath in all build.xml files... but nothing helped. Same error.\nI'm out of ideas what else to try :)\n22:00:11.894 [         butterfly.module] Error initializing module pc-axis by script function init() (827ms)\norg.mozilla.javascript.EcmaError: TypeError: [JavaPackage com.google.refine.pcaxis.PCAxisImporter] is not a function, it is object. (file:/Applications/OpenRefine.app/Contents/Resource/webapp/extensions/pc-axis/module/MOD-INF/controller.js#39)\n    at org.mozilla.javascript.ScriptRuntime.constructError(ScriptRuntime.java:3654)\n    at org.mozilla.javascript.ScriptRuntime.constructError(ScriptRuntime.java:3632)\n    at org.mozilla.javascript.ScriptRuntime.typeError(ScriptRuntime.java:3660)\n    at org.mozilla.javascript.ScriptRuntime.typeError2(ScriptRuntime.java:3679)\n    at org.mozilla.javascript.ScriptRuntime.notFunctionError(ScriptRuntime.java:3734)\n    at org.mozilla.javascript.ScriptRuntime.notFunctionError(ScriptRuntime.java:3722)\n    at org.mozilla.javascript.ScriptRuntime.newObject(ScriptRuntime.java:2324)\n    at org.mozilla.javascript.gen.c2._c1(file:/Applications/OpenRefine.app/Contents/Resource/webapp/extensions/pc-axis/module/MOD-INF/controller.js:39)\n    at org.mozilla.javascript.gen.c2.call(file:/Applications/OpenRefine.app/Contents/Resource/webapp/extensions/pc-axis/module/MOD-INF/controller.js)\n    at org.mozilla.javascript.ContextFactory.doTopCall(ContextFactory.java:398)\n    at org.mozilla.javascript.ScriptRuntime.doTopCall(ScriptRuntime.java:3065)\n    at org.mozilla.javascript.gen.c2.call(file:/Applications/OpenRefine.app/Contents/Resource/webapp/extensions/pc-axis/module/MOD-INF/controller.js)\n    at edu.mit.simile.butterfly.ButterflyModuleImpl.scriptInit(ButterflyModuleImpl.java:636)\n    at edu.mit.simile.butterfly.ButterflyModuleImpl.init(ButterflyModuleImpl.java:94)\n    at edu.mit.simile.butterfly.Butterfly.initializeModule(Butterfly.java:476)\n    at edu.mit.simile.butterfly.Butterfly.configure(Butterfly.java:451)\n    at edu.mit.simile.butterfly.Butterfly.init(Butterfly.java:308)\n    at org.mortbay.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:440)\n    at org.mortbay.jetty.servlet.ServletHolder.doStart(ServletHolder.java:263)\n    at com.google.refine.RefineServer.configure(Refine.java:286)\n    at com.google.refine.RefineServer.init(Refine.java:200)\n    at com.google.refine.Refine.init(Refine.java:113)\n    at com.google.refine.Refine.main(Refine.java:107)\n. I checked OpenRefine.app and I can confirm it. I didn't notice this before, because I saw all .class files in build/webapp/extensions/freebase after running mac_dist. I assumed they were also in OpenRefine.app, because all extension's subdirectories were properly copied to Contents/Resource/webapp/extensions/freebase.\nApp for 2.5 has a slightly different structure. I see Contents/Resources/webapp/extensions which contains all the extensions and their .class files.\nLessons learned: 1. assumption is the mother of all... 2. it's not always java's fault :)\n. @ghirardinicola I'm currently maintaining the rdf-extension. I created the issue from what you reported here: sparkica/rdf-extension#7 \nI'll look into it ASAP. \n. ",
    "jrovegno": "(Zotero,Mendeley,EndNote,etc) Bibtex ->  (Jabref) csv -> OpenRefine \n. ",
    "DFoltzMorrison": "A problem with exporting from Mendeley to Bibtex is that non-ascii characters are converted into a LaTeX format, for example \u00c9 becomes {'{E}}.\nMy workaround was to export citations from Mendeley as an Endnote XML and import that into openrefine. I had to Join a few multi-valued cells after the import, but I avoided character encoding errors along the way.. In most cases, yes. But Mendeley -> Bibtex can cause character encoding issues. \u00c9 becomes {'{E}}, \u00f6 becomes {\"{o}}, etc. That's obviously an issue with Mendeley that hasn't been addressed (example 1 , example 2)\nThat's why I've suggested Mendeley -> export to Endnote XML -> openrefine as an alternative.. I can rename the workspace directory to remove the space, but the drive name has a space in it:\n /media/USER/HDD 00/openrefine_workspaces/workspace_a/.\nRemoving the space in the drive name allows openrefine to launch in my browser and displays the following:\n```\nHTTP ERROR 503\nProblem accessing /. Reason:\njava.lang.NullPointerException\n\nCaused by:\njavax.servlet.UnavailableException: java.lang.NullPointerException\n    at org.mortbay.jetty.servlet.ServletHolder.makeUnavailable(ServletHolder.java:415)\n    at org.mortbay.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:458)\n    at org.mortbay.jetty.servlet.ServletHolder.doStart(ServletHolder.java:263)\n    at com.google.refine.RefineServer.configure(Refine.java:296)\n    at com.google.refine.RefineServer.init(Refine.java:208)\n    at com.google.refine.Refine.init(Refine.java:114)\n    at com.google.refine.Refine.main(Refine.java:108)\nPowered by Jetty://\n```\n. I renamed the workspace directory to remove the space, and had to change the parent drive's label in a partition editor to remove another space.\nI can successfully switch between workspaces, but now have the job of fixing a lot of broken file paths.\nPerhaps a future version of openrefine can support file paths with spaces wrapped with quotes? . ",
    "vad": "+1\nYou can also add a checkbox \"Remember the type\" in the reconciliation window.\n. @tfmorris I mean the \"Reconcile -> Start reconciling\" Dialog. It would be useful to have a checkbox that restricts the chosen type in following \"Search for match\" too.\n. It would also be useful to see the type in the \"Search for match\" autocompletion. I mean:\n- Milan (type: /city)\n- Milan (type: /province)\n. ",
    "stevevance": "@wetneb I would like to be able to export all columns as \"text\" and then deal with them later in PostgreSQL/MySQL. . I would like OpenRefine to export SQL with two options: (1) the CREATE statement includes a datatype for each column (\"text\", \"varchar\", \"int\", \"bigint\") based on how OpenRefine reads the data in that column; (2) the CREATE statement uses a generic \"text\" datatype for every column. \nFor large datasets, with many columns, it's very time consuming to write or rewrite a CREATE statement that matches the (2) option above.\nI normally use a GUI program to import CSV files called Navicat, instead of the COPY command in PostgreSQL.. @thadguidry I found a program that will write CREATE statements! It's csvsql, which is part of csvkit: http://csvkit.readthedocs.io/en/0.9.1/scripts/csvsql.html. I prefer to use TEXT instead of VARCHAR, and I would also like to see a NUMERIC (decimal/float) option. . ",
    "tcbuzor": "is someone working on this?. @thadguidry , @wetneb  should this generate  a CREATE statement as well or just an INSERT statement? . @thadguidry Why the first 30 bytes? The user should be allowed to change the table name as he/she chooses. We also need to allow the user specify the column type and sizes.... The column names also have a limit right?. Hi All,\nWhat do you guys think about the proposed UI for database export attached here:\n\n. I'll take a look and implement something similar.\nOn Tue, Mar 20, 2018 at 10:40 PM, Thad Guidry notifications@github.com\nwrote:\n\n@tcbuzor https://github.com/tcbuzor I would also add hover text\nexplaining what goes into the input boxes when you click into like we have\non Wikidata Schema alignment that @wetneb https://github.com/wetneb has\non his wikidata-extension branch.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/205#issuecomment-374826362,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/Af2f5W06k28V6VXGGtJ1bpWnHgvl_vxyks5tgcurgaJpZM4AM1M3\n.\n. Download tab below:\n\n. I think the JDBC functionality should support the following:\na) Multiple database types, selectable from a drop down.\nb) Start with most common database types: MySQL, MariaDB, PostgreSQL\nc) Support the plugin of new database types. It should be as easy as dropping a jar containing the JDBC driver and Database implementation in  a folder and the JDBC extension should load all jars in the folder.\nd) Support a well defined API for common database operations. e.g: connect, list etc... aadrian, can you try out the new OR JDBC extension?\nRepository : https://github.com/tcbuzor/openrefine-db-extension.git\nThe extension supports MySQL, PostgresSQL and MariaDB.. Antonin,\n\nI am almost done with the issues you raised. I will push the changes before\nweekend.\n-Tony\nOn Wed, Jan 10, 2018 at 4:04 AM, Antonin Delpeuch notifications@github.com\nwrote:\n\n@tcbuzor https://github.com/tcbuzor any news on this?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/1394#issuecomment-356556707,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/Af2f5QlZIbWuOMrcvRx5IH30NmwnPmuhks5tJIs6gaJpZM4RLlJc\n.\n. Please  provide more description to the problem. src and test is already in the .classpath file.. Thanks Jacky.\nI made the changes and generated another pull request.\n\n-Tony\nOn Sat, Jan 13, 2018 at 4:36 PM, Jacky notifications@github.com wrote:\n\nYour project has no issue standalone(ie, when pull it and install it as an\nextension to current master). The problem is that the library setting is\nnot in the current master .classpath UNDER THE ROOT. The current way of\nsetup in order to include the extension is to add the src and test folder\ndirectly under the build path instead of adding the project.\nThe problem you need to fix is on the OpenRefine project setting, NOT your\nown project setting. If you open the project setting of the OpenRefine and\ncheck how gdata is setup. you will understand what I mean.\nOn Sat, Jan 13, 2018 at 4:50 PM, Tonio notifications@github.com wrote:\n\nPlease provide more description to the problem. src and test is already\nin\nthe .classpath file.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/\n1430#issuecomment-357469861,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AKCGi7bFRjWf3WOFcLr_\nAR1zE4RpY41Qks5tKSUWgaJpZM4RdVzS\n.\n\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/1430#issuecomment-357472739,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/Af2f5ejf88-ryr21TfblPPMyjcqj9g6uks5tKS_ngaJpZM4RdVzS\n.\n. Hi Antonin,\nYour changes look good. I was working on the Appveyor test but no longer required as you have it working nicely. \nIts okay to rename  int_test.xml and delete unit_tests.xml. \nGood Job!. Screen with 11 columns attached.\n\n. @thadguidry Check the latest code. Added the trim and facet options.\n\n. It respects them by default. Try it now.\n\nOn Sun, Mar 25, 2018 at 10:07 PM, Thad Guidry notifications@github.com\nwrote:\n\n@tcbuzor https://github.com/tcbuzor umm... I wasn't saying to ignore\nthe facets, but to respect them ?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/1552#issuecomment-376034004,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/Af2f5YK5fePyHC1oxrpmuMb4B9qqAYCXks5tiFtxgaJpZM4S6ACw\n.\n. Done!\n\nOn Sun, Mar 25, 2018 at 10:21 PM, Thad Guidry notifications@github.com\nwrote:\n\n@tcbuzor https://github.com/tcbuzor ok, the Facet respecting works now.\nBut not the trim on column names for preview or download..\nCREATE TABLE clipboard (\nrecord VARCHAR(255),\nColumn 1 VARCHAR(255),\nvalues 2 VARCHAR(255),\nvalues 3 VARCHAR(255),\nvalues 4 VARCHAR(255),\nvalues 5 VARCHAR(255),\nvalues 6 VARCHAR(255),\nvalues 7 VARCHAR(255),\nvalues 8 VARCHAR(255)\n);\nINSERT INTO clipboard (record,Column 1,values 2,values 3,values 4,values 5,values 6,values 7,values 8) VALUES\n( 'record','record','Alfredo Santamaria 2013','Spain','Castilla-y-Le\u00f3n','Cigales  ','Alfredo Santamaria Arias, S.L. (Producer)','+34 983 58 50 06','www.bodega-santamaria.com' ),\n( 'record','record','Al-Ria Reserva 2015','Portugal','Algarve','Vinho Regional    ','Casa Santos Lima (Producer)','+351 2 63 76 90 93;+351 2 63 76 06 21','www.casasantoslima.com' ),\n( 'record','record','Amaren Tempranillo Reserva 2009','Spain','Rioja','Rioja Alavesa  ','Araex Rioja Alavesa SL (Producer)','+34 945 15 05 88 <+34%20945%2015%2005%2088>','www.araex.com' ),\n( 'record','record','Aponte Reserva 2008','Spain','Castilla-y-Le\u00f3n','Toro ','Bodegas Frontaura SLU (Producer)','+34 983 88 04 88 <+34%20983%2088%2004%2088>','www.bodegasfrontaura.com' ),\n( 'record','record','Arzuaga Reserva Especial 2011','Spain','Castilla-y-Le\u00f3n','Ribera del Duero   ','Bodegas Arzuaga Navarro SL (Producer)','+34 983 68 11 46 <+34%20983%2068%2011%2046>','www.arzuaganavarro.com' ),\n( 'record','record','Baigorri Belus 2013','Spain','Rioja','Rioja Alavesa  ','Bodegas Baigorri (Producer)','+34 945 60 94 20 <+34%20945%2060%2094%2020>','www.bodegasbaigorri.com' ),\n( 'record','record','Bottega Il Vino degli Dei 2012','Italy','Veneto','Amarone della Valpolicella DOCG    ','Bottega SPA (Producer)','+39 04 38 40 67','www.bottegaspa.com' ),\n( 'record','record','Burro Loco Rosado 2016','Spain','Castilla-y-Le\u00f3n','Vino de la Tierra ','Concejo Bodegas SL (Producer)','+34 983 50 22 63 <+34%20983%2050%2022%2063>','www.concejobodegas.com' ),\n( 'record','record','Cancellaia 2012','Italy','Toscana','Toscana IGT  ','Az. Agr. Pakravan-Papi (Producer)','+39 3356 00 44 46','www.pakravan-papi.it' ),\n( 'record','record','Cardela 2014','Spain','Castilla-y-Le\u00f3n','Ribera del Duero    ','Bodegas Boh\u00f3rquez, S.L. (Producer)','+34 915 64 37 51 <+34%20915%2064%2037%2051>','www.bodegasbohorquez.com' ),\n( 'record','record','Casa Ferreirinha Vinha Grande Red 2014','Portugal','Porto E Douro','Douro    ','Sogrape Vinhos SA (Producer)','+351 227 86 80 08;+351 227 85 03 00','www.sograpevinhos.com' ),\n( 'record','record','Casa Mayor Old Vines Cabernet Sauvignon 2016','Chile','Valle de Colchagua','Vi\u00f1a Casa Silva SA (Producer)','+56 7 22 71 65 19','www.casasilva.cl' ),\n( 'record','record','Castello di Vicarello 2012','Italy','Toscana','Toscana IGT   ','Castello di Vicarello (Producer)','+39 0564 99 04 47 <+39%200564%20990447>','www.castellodivicarellovini.com' ),\n( 'record','record','Castillo Labastida Ermita Santa Lucia 2016','Spain','Rioja','Rioja Alavesa   ','Araex Rioja Alavesa SL (Producer)','+34 945 15 05 88 <+34%20945%2015%2005%2088>','www.araex.com' ),\n( 'record','record','Castroviejo 2013','Spain','Rioja','Rioja DOCa    ','Bodegas Pastor Diaz Sl (Producer)','+34 941 14 23 90 <+34%20941%2014%2023%2090>','www.pastordiaz.com' ),\n( 'record','record','Cava Amorany Brut Gran Cuv\u00e9e','Spain','Catalu\u00f1a','Cava   ','Vid Vica SL (Producer)','+34 933 10 22 62 <+34%20933%2010%2022%2062>','www.vidvica.com' ),\n( 'record','record','Champagne Bauchet Mill\u00e9sime Premier Cru 2009','France','Champagne','Champagne    ','Champagne Bauchet (Producer)','+33 3 26 28 17 72 <+33%203%2026%2028%2017%2072>; +33 3 26 58 17 72 <+33%203%2026%2058%2017%2072>','www.champagne-bauchet.com' ),\n( 'record','record','Champagne Fresne Ducret La Grande Hermine 2009','France','Champagne','Champagne  ','Champagne Fresne Ducret/SCEV JA MILAUR (Producer)','+33 3 26 49 24 60 <+33%203%2026%2049%2024%2060>','www.champagne-fresne-ducret.com' ),\n( 'record','record','Champagne Jean Dumangin Brut Mill\u00e9sime 2009','France','Champagne','Champagne ','EARL Jean Dumangin (Producer)','+33 3 26 03 42 17 <+33%203%2026%2003%2042%2017>','www.champagne-jean-dumangin.fr' ),\n( 'record','record','Champagne Marc Perla-Rosa 2012','France','Champagne','Champagne  ','Champagne Marc (Producer)','+33 3 26 58 46 88 <+33%203%2026%2058%2046%2088>','www.champagne-marc.fr' ),\n( 'record','record','Champagne Vincent Lamoureux Ros\u00e9','France','Champagne','Champagne    ','EARL Lamoureux Vincent (Producer)','+33 3 25 29 39 32 <+33%203%2025%2029%2039%2032>','www.champagne-lamoureux-vincent.fr' ),\n( 'record','record','Ch\u00e2teau La Fleur Peyrabon 2014','France','Bordeaux','Pauillac    ','Ch\u00e2teau Peyrabon (Producer)','+33 5 56 59 57 10 <+33%205%2056%2059%2057%2010>','www.chateau-peyrabon.com' ),\n( 'record','record','Ch\u00e2teau Moulin du Terrier 2016','France','Bordeaux','Bordeaux Rouge  ','GAEC Forcato (Producer)','+33 6 71 21 57 71 <+33%206%2071%2021%2057%2071>','www.chateau-lary.com' ),\n( 'record','record','Ch\u00e2teau Ventenac Carla Ros\u00e9 2016','France','Languedoc-Roussillon','Cabard\u00e8s  ','Maison Ventenac (Producer)','+33 4 68 24 93 42 <+33%204%2068%2024%2093%2042>','www.maisonventenac.fr' )\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/1552#issuecomment-376035645,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/Af2f5cH1tU5blCA87Wi88V_uy-aFkRWfks5tiF7LgaJpZM4S6ACw\n.\n. @thadguidry \n\n\n\n. @wetneb , @ettorerizza I am working on the reported bug. I will add an option to \"convert null cell to empty string\" that way a valid sql insert statement will be generated. This will work for VARCHAR, TEXT and CHAR types only. Need to decide what to do when the user selects a numeric type. Return an error maybe?. Just checked in the latest export code. I added 2 columns(Allow Null , Default Value). If the user indicates the column is not Nullable then he has the option of adding a default value, which will be used in the insert statement when the field value is NULL. \n\n. Sent another request.\nOn Sat, Aug 4, 2018 at 11:11 PM, Thad Guidry notifications@github.com\nwrote:\n\n@tcbuzor https://github.com/tcbuzor I don't see any changed files on\nthis pull request. Did you pull from master first ? This PR looks weird\nfrom Github view. Perhaps you should delete it and create a fresh one ?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/1693#issuecomment-410494396,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/Af2f5SbCYCTHuF-106qKdC3o1aZz7Z4Jks5uNnB3gaJpZM4VvOvO\n.\n. The LIMIT and OFFSET clauses are used internally in OR  to restrict the number of rows in the preview and create project SELECT queries.. @wetneb why will you allow the user to enter a limit and then \"gracefully replace the limit\"  ?. There is front end validation as well if you try to generate sql with no column selected. I can suppress the error dump but if this happens, the user will see an empty file. . This should work for MySQL, Oracle, PgSql and MariaDB. \n",
    "palewire": "+1\n. Thank you.\n. ",
    "coreyp": "+1, I'd love to have this kind of implementation feature on a OSX or Linux server\u2026 I've seen oblique references to this having been done but no confirmation or documentation.\n. ",
    "emanuil-tolev": "For other folks stumbling upon this issue by searching for \"run open refine as a service\", there is this now, by @davetaz: https://github.com/theodi/OpenRefine-WS . I ran it successfully on a VPS, it's a bit alpha (e.g. can't configure instance timeout, you just change the code), but it worked well.\nAlso using a firewall to ensure only 1 user is signed in is an interesting approach, but make sure to review how it does authentication thoroughly before putting anything sensitive in the managed OpenRefine instances.\n. ",
    "timClicks": "@tfmorris I don't know, sorry. Thanks for plugging away at this. \n. ",
    "cwiso": "Has there been any progress on this yet? Using OpenRefine 2.7 and still see the same behavior described by @tfmorris. When trying using the \"Apply to All Identical Cells\" it does not fill that cell, or any of the other identical cells when they are null, and it says Mass edit 0 cells in column X in the history. . ",
    "pmackay": "http://refinepro.com/hosting/ seems to provide some options for this. I havent tried.. ",
    "ultraklon": "For testing this in HEAD, you'll have to have in mind #839 \nI've made a fix for that bug here https://github.com/OpenRefine/OpenRefine/pull/840\n. And you will have the problem stated in #841 too\n. I can't assign this to myself, I'm working on this bug\n. I've been running the tests and the first error that shows is at XmlImportUtilitiesTests.java:288 (so the tests are not passing)\nDigging into the code, the problem is that, sometimes, when making someBasedList.rows.get(0) it tries to look for element -1 in the list, if you make someBasedList.rows.size() it is 1 and someBasedList.rows.get(1) gives you an actual element because it searches for element 0 (it get messed up by index-offset, see ImportRecord.java:41)\nI think the BasedList implementation is breaking this. Reading your comment in ImportRecord.java, it seems that's a hack someone else gave you. Can you give me a little more background about the hack so I can fix it / get rid of it?\n. @tfmorris I made a simple test, I just reverted commit 6b3592982ed60a7c04b8cdf71022a52c82725b60 and server's test stopped failing, so definitly the commit is breaking the test. Could you please revert it? Do you want me to make the revert pull request?\n. when I run ./refine server_test it happends what I state in a previous comment, please check it\nI see that the ui tests are not valid anymore, should I ignore server tests too?\n. @tfmorris Finally I understand the problem, in a word TreeImportUtilities.java does record.rows.set and record.rows.get all the time.\nSometimes it does set(1, someObject) and then get(0) so it crashes\nThe previous version will fill that 0 position with a empty ArrayList, but the new version gives an arrayindexoutofboundexception.\nI tried to return any ArrayList instead of crashing and it (almost) works (an assertion fails because I'm returning anything!)\nI think that what is broken is TreeImportUtilities, it's messing up with nextRowIndex variables\n. I'm pasting here a part, just the first stack trace for the first error presented, there are more than. I'll send you the full log by email.\n[testng] [Invoker 978391674] Invoking com.google.refine.tests.importers.XmlImportUtilitiesTests.findRecordTestXml\n   [testng] F[TestNG] FAILED: \"tests\" - com.google.refine.tests.importers.XmlImportUtilitiesTests.findRecordTestXml() finished in 2 ms\n   [testng] [TestNG] java.lang.AssertionError: null\n   [testng] [TestNG]    at org.testng.Assert.fail(Assert.java:94)\n   [testng] [TestNG]    at org.testng.Assert.fail(Assert.java:101)\n   [testng] [TestNG]    at com.google.refine.tests.importers.XmlImportUtilitiesTests.findRecordTestXml(XmlImportUtilitiesTests.java:288)\n   [testng] [TestNG]    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n   [testng] [TestNG]    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n   [testng] [TestNG]    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n   [testng] [TestNG]    at java.lang.reflect.Method.invoke(Method.java:597)\n   [testng] [TestNG]    at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:80)\n   [testng] [TestNG]    at org.testng.internal.Invoker.invokeMethod(Invoker.java:714)\n   [testng] [TestNG]    at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:901)\n   [testng] [TestNG]    at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1231)\n   [testng] [TestNG]    at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:127)\n   [testng] [TestNG]    at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:111)\n   [testng] [TestNG]    at org.testng.TestRunner.privateRun(TestRunner.java:767)\n   [testng] [TestNG]    at org.testng.TestRunner.run(TestRunner.java:617)\n   [testng] [TestNG]    at org.testng.SuiteRunner.runTest(SuiteRunner.java:334)\n   [testng] [TestNG]    at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:329)\n   [testng] [TestNG]    at org.testng.SuiteRunner.privateRun(SuiteRunner.java:291)\n   [testng] [TestNG]    at org.testng.SuiteRunner.run(SuiteRunner.java:240)\n   [testng] [TestNG]    at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52)\n   [testng] [TestNG]    at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86)\n   [testng] [TestNG]    at org.testng.TestNG.runSuitesSequentially(TestNG.java:1198)\n   [testng] [TestNG]    at org.testng.TestNG.runSuitesLocally(TestNG.java:1123)\n   [testng] [TestNG]    at org.testng.TestNG.run(TestNG.java:1031)\n   [testng] [TestNG]    at org.testng.TestNG.privateMain(TestNG.java:1338)\n   [testng] [TestNG]    at org.testng.TestNG.main(TestNG.java:1307)\n. I've tested it in a Linux machine and all tests pass.\nIn my Mac with java version 1.6.0_65 tests fails as I stated\n. Do you have a way to reproduce this or a file to import that can make this error show?\n. Hi, I've tested it in my Linux machine and it works ok.\nCould you please test it again (maybe your system have changed) and if it still fails, tell me what Linux version you are using and your JVM version please.\nIt may be useful that you share with us a spreadsheet that causes the error for us (edition permission to ultraklon [at] gmail) to test it.\nThanks!\n. ping @magdmartin\n. Thanks for the merge!, should I ignore the ui_tests from now on or should we fix them? (or should we refactor the tests to depend on other framework?)\n. Yes sorry, I'll try to be more specific next time.\nI've already made a pull request to fix this bug.\nhttps://github.com/OpenRefine/OpenRefine/pull/840\nYou need more details anyway?\n. @magdmartin sorry, I as told before, I'll start to be more specific, I really want to be helpful\n@tfmorris I'm testing this in development HEAD. As you mention, the jQuery version may be affecting the project, you can check this pull request as an example\nhttps://github.com/OpenRefine/OpenRefine/pull/840\nbut in this case I've been investigating and one clue could be that if you go to the file ExpressionBasedRowEvaluable.java and after line\nExpressionUtils.bind(bindings, row, rowIndex, _columnName, cell);\nyou add something like this\nSystem.out.println(\"is Number: \" + (_eval.evaluate(bindings) instanceof Number));\nyou'll have false instead of true\nI'm testing all this with the data set given here\nhttps://code.google.com/p/google-refine/issues/detail?id=400\nwith that same steps and it fails when creating a new facet\nMaybe I'm lost because I don't know the code that much, but I think that the number values may be evaluated as String instead of Number or something like it\n. I've noticed that before too testing it with the HEAD version.\nIf you change to another tab, then the window returns to it's correct height, this seems to be a tabs component's problem\n. I'f just tested it and you have to go through every tab and when you leave each one, it gets hidden. Is like if each tab content wasn't hidden at start (and they should)\n. LGTM\n. LGTM\n. number 3 of this fixes the same as #837 and is a better fix\n. Are there any open bugs for number 1 an number 2 ?\n. please, try to search for a bug that matches 1 and other for 2, if you can't find already reported bugs, please create them clarifying that you are using HEAD version.\nMaybe then we'll need to divide this PR in 3 parts.\nIf you have any questions, please contact me.\nThanks for the fixes!\n. Ok, I'm working right now, at night I'll take a look\nWhat you mention about Intellij, should be decided by @tfmorris that is the lead programmer\n. LGTM\n. I've just tested this with a file with the following content\na,b\nc,d\n,e\nf,g\n,h\n,i\nand it looks like it works, when you leave the default \", \" or you manually introduce that, you get\nc|d, e\nf|g, h, i\nwhen the value \",\" is used, you get\nc|d,e\nf|g,h,i\nBUT, when I tested it with \",\u00a0\u00a0\u00a0\u00a0\" (comma and four spaces), I got the same results as with \", \". IDK if this is expected or not\nWith \",\u00a0\u00a0\u00a0\u00a0\u00a0@\" (comma, five spaces and @), you get\nd|@e\ng|@h, @i\nso this is looks like collapsing multiple spaces into one\n. This is not very important, why do you preffer to create a separate object to make the locks instead instead of using Long in the previous line? to avoid autowrapping?\n. { } suggested\n. actually this could be just\nif (v1 == null ) {\n. changing this behavior is not a trivial thing, this should be thought with a few examples in mind at least (tests?)\n. perfect, just wanted to clarify, thanks\n. ",
    "cfdeveloper": "seems good for me as well.\n. This is being worked on.\n. ",
    "hvmarck": "I have also noticed very strange behaviour with the cross function! In one case renaming the column for some reason seemed to do the trick.\n@tfmorris Any idea when r2539 (or newer) will be released? (on the download site it is still r2407)\n. Because it makes sense to keep the old data I wrote a plugin that contains (amongst other stuff) allows you to execute history steps from other projects, or re-execute history steps for the current project. You can find the plugin and the manual on http://www.bits.vib.be/index.php/software-overview/openrefine\nCheers,\nHerwig\n. You can do it with\nwith(value.match(/[^|]+?([A-Z][^\\s]* [A-Z][^\\s]).+?(|.+)/),v,if(v==null,value.match(/[^|]*(.+)/)[0],value.split(v[0])[1]+\"|\"+v[0]))\nand the \"Re-transform up to ...\" check box checked.\nThis transforms \"The ex-president Luiz In\u00e1cio is not Marcos Val\u00e9rio friend, senator.\" into \"|Luiz In\u00e1cio|Marcos Val\u00e9rio\".\nHerwig Van Marck\n. Is any developer  actually reading this issue list?\n. Thanks!\n. Our plugin is completely written in javascript, so the contents of the downloadable zip file contains by definition the sources.\n. You are right about the 'not 100% accurate'! I didn't know I had to escape the < character, so all the html tags in the code were missing. I updated it now.\n. Thanks Tom!\n. As workaround I create a project from clipboard, with 1 column 'url' and the link to the json as a value. Then add a new column 'json' with 'Add column by fetching URLs...'. And then transform that column with the expression\n\"[\"+forEach(with(value.replaceChars(\"{}:\",\"[],\").parseJson(),v ,forRange(0,v.length(),2,i,v[i])),w,value.parseJson().get(w)).join(\",\")+\"]\"\nThen you can copy the transformed json to the clipboard (using the 'edit' button in the cell, and then Ctrl-C) and create a new project from the clipboard with that json.\nHerwig\n. I agree that the issue remains when to be used by non-developers! What I have been doing for them within my organisation is to create plugins that hide the steps behind a nice interface (haven't done that yet for the json case because I didn't think anybody in my organisation needed it up to now).\n. As a workaround, I added the following code to a plugin (changing the code of ProcessPanel.update), which adds an error callback, restarting the updating:\n```\nProcessPanel.prototype.update = function(updateOptions, onDone) {\n  this._latestHistoryEntry = null;\nfor (var n in updateOptions) {\n    if (updateOptions.hasOwnProperty(n)) {\n      this._updateOptions[n] = updateOptions[n];\n    }\n  }\n  if (onDone) {\n    this._onDones.push(onDone);\n  }\nif (this._timerID !== null) {\n    return;\n  }\nvar self = this;\n  $.ajax({\n    dataType: \"json\",\n    url: \"command/core/get-processes?\" + $.param({ project: theProject.id }),\n    data: null,\n    success: function(data) {\n        self._latestHistoryEntry = null;\n        self._render(data);\n    },\n    error: function(jqXHR,textStatus) {\n      console.log(\"Restarting ProcessPanel update\");\n      this._timerID = window.setTimeout(function() {\n    self._timerID = null;\n    self.update();\n      }, 500); \n    }\n  });\n};\n```\n. It is even worse than that. I have projects that contain steps (jython scripts) that take so long that not only the front-end gets stuck, but the history doesn't reflect the step afterwards. This results in projects that contain columns, the creation of which are not in the history!\n. I have been using comments for  a while now, by adding column\ntransformations of the form\njython:return value # comment\n(which do nothing) and then using an overloaded renderer of the\nHistoryPanel class in a custom plugin:\n// labels in history panel\nHistoryPanel.prototype._renderOrig=HistoryPanel.prototype._render;\nHistoryPanel.prototype.render= function() {\n  this._renderOrig();\n  var self=this;\n  var elmts = DOM.bind(this._div);\n  //console.log(elmts);\n  elmts.bodyDiv.find(\".history-entry\").each(function() {\n    var text = this.childNodes[1].firstChild.nodeValue;\n    var mtch=text.match(/Text transform on 0 cells in column [^:]+: jython:\n_return +value *#(.)/);\n    if (mtch) {\n      this.childNodes[1].firstChild.nodeValue=mtch[1];\n      $(this.childNodes[1]).addClass(\"history-comment\");\n    }\n  });\n}\ntogether with some css for the history:\n.history-past a.history-entry .history-comment {\n  color: #00f\n}\n.history-now a.history-entry .history-comment {\n  color: #ff4\n}\n.history-future a.history-entry .history-comment {\n  color: #88f\n}\nCheers,\nHerwig\nHerwig Van Marck, PhD.\nSenior Expert Research Informatics\nRijvisschestraat 126 3/R, 9052 Zwijnaarde\nTel: +32 (0)9 248.16.01\nBio Informatics Training and Service Facility (BITS\nhttp://www.bits.vib.be/)\nOn 2 September 2015 at 15:04, magdmartin notifications@github.com wrote:\n\nI support Herve proposition. Preparing data is like telling a story as\nthere is never one way of doing something.\nQuickly a data preparation process can go through hundreds of steps and\nunderstanding the logic afterwards can be complex. Like developers comment\ntheir code to with their logic, we should be able to do the same in Refine.\nThis will help with collaboration and reproducibility of the code along\nwith improving auditing.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1057#issuecomment-137067159\n.\n. Hey Martin,\n\nIt is part of an internal plugin (not public) because I see this as a\ntemporary solution/hack. It would be good if comments could be implemented\nproperly in OpenRefine!\nCheers,\nHerwig\nHerwig Van Marck, PhD.\nSenior Expert Research Informatics\nRijvisschestraat 126 3/R, 9052 Zwijnaarde\nTel: +32 (0)9 248.16.01\nBio Informatics Training and Service Facility (BITS\nhttp://www.bits.vib.be/)\nOn 3 September 2015 at 04:40, magdmartin notifications@github.com wrote:\n\nHerwig\nThank you for sharing this! Is it currently available in a standalone\nextension ? Available on public repository? I'd love to see how we can\nintegrate this to OpenRefine.\nMartin.\nOn Sep 2, 2015 9:23 AM, \"Herwig Van Marck\" notifications@github.com\nwrote:\n\nI have been using comments for a while now, by adding column\ntransformations of the form\njython:return value # comment\n(which do nothing) and then using an overloaded renderer of the\nHistoryPanel class in a custom plugin:\n// labels in history panel\nHistoryPanel.prototype._renderOrig=HistoryPanel.prototype._render;\nHistoryPanel.prototype.render= function() {\nthis._renderOrig();\nvar self=this;\nvar elmts = DOM.bind(this._div);\n//console.log(elmts);\nelmts.bodyDiv.find(\".history-entry\").each(function() {\nvar text = this.childNodes[1].firstChild.nodeValue;\nvar mtch=text.match(/Text transform on 0 cells in column [^:]+: jython:\n_return +value *#(.)/);\nif (mtch) {\nthis.childNodes[1].firstChild.nodeValue=mtch[1];\n$(this.childNodes[1]).addClass(\"history-comment\");\n}\n});\n}\ntogether with some css for the history:\n.history-past a.history-entry .history-comment {\ncolor: #00f\n}\n.history-now a.history-entry .history-comment {\ncolor: #ff4\n}\n.history-future a.history-entry .history-comment {\ncolor: #88f\n}\nCheers,\nHerwig\nHerwig Van Marck, PhD.\nSenior Expert Research Informatics\nRijvisschestraat 126 3/R, 9052 Zwijnaarde\nTel: +32 (0)9 248.16.01\nBio Informatics Training and Service Facility (BITS\nhttp://www.bits.vib.be/)\nOn 2 September 2015 at 15:04, magdmartin notifications@github.com\nwrote:\n\nI support Herve proposition. Preparing data is like telling a story as\nthere is never one way of doing something.\nQuickly a data preparation process can go through hundreds of steps and\nunderstanding the logic afterwards can be complex. Like developers\ncomment\ntheir code to with their logic, we should be able to do the same in\nRefine.\nThis will help with collaboration and reproducibility of the code along\nwith improving auditing.\n\u2014\nReply to this email directly or view it on GitHub\n<\n\nhttps://github.com/OpenRefine/OpenRefine/issues/1057#issuecomment-137067159\n\n.\n\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/OpenRefine/OpenRefine/issues/1057#issuecomment-137076887\n.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1057#issuecomment-137305308\n.\n. The other stuff that is in the internal plugin:\n- project locking support (to run OpenRefine on a server accessed by\n  multiple people), making sure only one person can open a project at a time\n- project ownership (keeps track who owns a project, defaulting to\n  whoever created it) in conjunction with the previous locking support\n- adding an extra export button in the 'Open Project' page (between\n  'delete' and 'rename')\n  -\n- error handling to ProcessPanel.update to avoid process update error\n  (automatically restarting when an error occurs)\n- a visualizer that treats columns that start with the string 'diff ' as\n  a special column in which diff like annotations (created with a diff tool)\n  are visualized with red strikethrough and green highlights\n- a function that compares a project to another project using the Coopy\n  highlighter diff library http://dataprotocols.org/tabular-diff-format/\n- a function that compares the history of a project to the history of\n  another project\n  -\n- a 'GoTo-record' to pagingControls, that allows you to click on the\n  page control numbers and select a row number to start the display of the\n  table (does not need to be a multiple of the range (e.g. 50) choosen)\n- Wrangler http://vis.stanford.edu/wrangler/ type bars in the headers\n  to show how many non-blanks a column has (can be switched on and off)\n\nSo all in all a nice collection. Some of these could probably be put in a\npulic plugin, but one reason I did not do that yet is that they rely on\noverloading the OpenRefine code, which must not change too much (after an\nupdate).\nCheers,\nHerwig\nHerwig Van Marck, PhD.\nSenior Expert Research Informatics\nRijvisschestraat 126 3/R, 9052 Zwijnaarde\nTel: +32 (0)9 248.16.01\nBio Informatics Training and Service Facility (BITS\nhttp://www.bits.vib.be/)\nOn 4 September 2015 at 00:58, QI notifications@github.com wrote:\n\n@hvmarck https://github.com/hvmarck That's a interesting hack. what's\nthe proper way to handle the comment according to your experience? I think\nit's a standalone plugin which you can apply the comment to any selected\nstep chosen from the history panel.\nJust curiosity if you don't mind. what's the other functionalities of your\ninternal plugin?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1057#issuecomment-137595820\n.\n. \n",
    "Busa78": "I completly agree with this issue. \nMy problem for example is to add new sheets to the project from the initial excel that I didn't include previously.\nA simple \"Edit/Change Project Dataset Configuration...\" function is needed, don't you think?\n. Any news about this issue?\nCould a trigger as \"re-load data into project\" for applayng same project layout be a first step solution?\nSomeone is working on this direction?\nI enpower my request becouse of I believe OpenRefine is really a powerfull and potentially essential tools for Enterprise Information Management expecially for all that concerns Open Data and Interoperability fields.\nThe problem is that to use it in an enterprise way is really essential to be able to reiterate transformation (layout) of a project programmatically (i.e. via API or cron scripts).\nLet me know what do u think about this topic.\nBusa ;-)\n. Hi Herwig.\nI already know and use your great extension to speed-up many of may tampleting task reusing from different Project.\nIt's really usefull but for my objective is a workaround. With your extension I can create new project with new data and then import the history of \ntrasformation from the old project. Unfortunatly this doesn't make me able create a system that programmatically refresh/reload data into an \nexixting project and make it possibile to export new transformation results as a source of another system.\nIt's a step toward the solution ...but still not the solution.\nMaybe we could start from this extension to extend or evolve the features if nothing will be done in the OpenRefine main project.\nThanx again.\nIl 01/07/2013 12:51, Herwig Van Marck ha scritto:\n\nBecause it makes sense to keep the old data I wrote a plugin that contains (amongst other stuff) allows you to execute history steps from other \nprojects, or re-execute history steps for the current project. You can find the plugin and the manual on \nhttp://www.bits.vib.be/index.php/software-overview/openrefine\nCheers,\nHerwig\n\u2014\nReply to this email directly or view it on GitHub https://github.com/OpenRefine/OpenRefine/issues/460#issuecomment-20274795.\n. I experience the same problem when I try to access the export RDF link. \n\nWhat steps will reproduce the problem?\n1. Export a project using RDF templating (es. http://127.0.0.1:3333/command/core/export-rows/Bankadati-BDC-Portafoglio-Servizi.ttl)\n2. Copy the url and try to re download the same ttl file (i.e. opern in another tab the same URL)\nWhat version of Google Refine are you using?\nGoogle Refine 2.5 [r2407]\nWhat operating system and browser are you using?\nWin 7 32 bit; Google Chrome Version 27.0.1453.94 m\nDid you resolved the problem or have any Idea of the strange behavior?\n. Yes.\nOk I'll tell them the problem.\nThanx.\n. ",
    "stevenqzhang": "Any update on this issue? It's been 4 years.... ",
    "drangons": "Hi,\nI am the issue with importing text written in German language. When csv file is imported to refine, the column values are displayed as ? for \u00f6 \u00fc characters. I am using 2.5-r2407 . \nfor example f\u00fcr is displayed as f\u00c3\u00bcr.\nCan you suggested me what can be done to get it to work.\nThanks.\nUpdate: I understand that I have to change the character encoding. Can it be configured in file ? . \n. ",
    "ldmosquera": "\n\ndon't update project metadata which hasn't been modified\n\n\nThis never happens; ProjectManager.saveProjects() only saves projects with changes since their last save.\nThe second suggestion is doable and would only require changes inside ProjectMetadataUtilities.\nA more extreme measure would be to do loadFromFile() right after saving to the temp file, and raise exceptions if it couldn't be read back. It would be safer though quite slower too.\nThoughts?\n. ",
    "timelf123": ":+1: \n. :+1: \n. ",
    "anayram": "The permalink option to preserve facets doesn't seem to work for larger amounts of facet selections.\nI tested the permalink option with approximately 50 text facets (worked fine), then tested with 136 --this returned a blank page.\nUsing OpenRefine 2.6 beta 1 [TRUNK] for Windows\n. You are right, the current method creates really long urls.\n. ",
    "florianm": "@anayram wouldn't permalinking 136 facets exceed the 2000 character limit of URLs?\nI've wondered is there a more concise way of encoding facet settings in a permalink than the current implementation?\n. ",
    "mjy": "I get the same thing opening tab delimited text sometimes, with columns enumerated like 'Column 54'.  These appear on tsv dumps then as well.  This on a 10.6.8 Mac in FF 17.x\n. I can do that- but I just realized I'm using Google Refine, not OpenRefine - still pertinent?\n. Ok- dumb question- where to provide the file?  Should I fork and load it somewhere?\n. Here's the file (link below). Steps to replicate- Start refine.  Create Project (don't change any settings).  Notice extra columns produced.  Then, also,  export -> tsv -> see extra columns tacked on.  This might be related to a problem with the textfile, i.e. perhaps there is an extra tab embedded in there such that there are different numbers of tabs/line, I haven't tested this.\nhttps://docs.google.com/open?id=0B0WA-4PfIZhoY3hQYWFZUjFkUUE\n. ",
    "ChristopherSouth": "I'm seeing a similar refresh issue when doing reconciliation -- if i choose the \"double checkmark\" icon - or match to all identical cells, it refreshes the entire page and goes back to the top instead of where you last were.  I suspect that could be intended behavior to ensure one doesn't have items to revalidate -- but it's a pain when you're on the bottom third of a large list and it keeps taking you back to the top....\n. ",
    "Rots": "As a workaround, is there a way to filter by row/record number?\nE.g. setting the filter to num>5100 would suffice in my use case to look at specific section of the input data.. Answering my own question, workaround is:\n1. add a new Facet->Custom Numeric Facet using rowIndexas value\n2. filter on the facet. I'll add my use case (which I suspect is much more common than the discussions above)\nAs a developer I would like to have a fast way of testing the (latest) openrefine without much hassle. Having an official Docker image would help me to easily deploy and test-run the software with the tools that are already familiar to me (Docker) without having to trust any other third parties ( e.g. there are tens of docker images available on Docker Hub that contain OpenRefine).\nEven though OpenRefine \"is already a self-contained widget that minimally inserts itself into a machine\" I feel much more comfortable pulling and running a container (without any bind-mounts) compared to running a script from the internet which has access to my file system and downloads some extra stuff (Java, Maven) to make the software work. Docker image is a standard way of delivering software (and it works mostly fine even without internet access once you have it).\nAlso that helps with the cleanup and I don't have to worry about having the conflicting versions of dependencies (Yes, even Maven has many versions) around or having to clean up these manually afterwards.\nI think the effort is worth for the project to gain some more traction/popularity among the \"average\" tinkerers.. add:\nVOLUME /mnt/refine. Does .bat file need to be adjusted as well? I don't have Windows to test, sorry.. ",
    "tobinski": "@Rots I use your workaround and it saves me allot of clicking. Guten Tag\nVom 22. Dezember bis am 18. Februar bin ich abwesend und nicht erreichbar. F\u00fcr Notf\u00e4lle melden sie sich telefonisch im B\u00fcro von Captns & Partner unter  031 331 76 36 oder per Email support@captns.ch\nFreundliche Gr\u00fcsse \nTobias Steiner. ",
    "eliohb": "hello i have downloaded the openrefine 2.5 version and the rdf extension 0.9 version when i am trying to use the rdf extension by uploading a owl file (ontology) by using the advanced option i receive the following error:\norg.mortbay.log] /command/rdf-extension/upload-file-add-prefix (1ms)\njava.lang.UnsupportedOperationException\n        at com.google.refine.commands.Command.doGet(Command.java:86)\n        at com.google.refine.RefineServlet.service(RefineServlet.java:170)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n        at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n        at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:923)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:547)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n        at java.lang.Thread.run(Unknown Source)\nAny advice. Dear,\nI have install red extension 1.1.0 and i put all the file in\n..\\openrefine-3.1\\webapp\\extensions\nand when i run the program the following error appear:\nHTTP ERROR 500\nProblem accessing /. Reason:\nButterfly Error\n\nButterfly incurred in the following errors while initializing:\njava.lang.Exception: Failed to initialize module rdf-extension\n[edu.mit.simile.butterfly.ButterflyModuleImpl]\n    at edu.mit.simile.butterfly.Butterfly.initializeModule(Butterfly.java:478)\n    at edu.mit.simile.butterfly.Butterfly.configure(Butterfly.java:451)\n    at edu.mit.simile.butterfly.Butterfly.init(Butterfly.java:308)\n    at org.mortbay.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:440)\n    at org.mortbay.jetty.servlet.ServletHolder.doStart(ServletHolder.java:263)\n    at com.google.refine.RefineServer.configure(Refine.java:291)\n    at com.google.refine.RefineServer.init(Refine.java:203)\n    at com.google.refine.Refine.init(Refine.java:109)\n    at com.google.refine.Refine.main(Refine.java:103)\nCaused by: org.mozilla.javascript.WrappedException: Wrapped\norg.apache.lucene.index.IndexFormatTooOldException: Format version is\nnot supported (resource\nBufferedChecksumIndexInput(MMapIndexInput(path=\"C:\\Users\\Toshiba\\AppData\\Roaming\\OpenRefine\\cache\\rdfExtension\\export\\luceneIndex\\segments_1\"))):\n0 (needs to be between 4 and 6). This version of Lucene only supports\nindexes created with release 5.0 and later.\n(file:/C:/Users/Toshiba/Desktop/openrefine-3.1/webapp/extensions/module/MOD-INF/controller.js#89)\n    at org.mozilla.javascript.Context.throwAsScriptRuntimeEx(Context.java:1932)\n    at org.mozilla.javascript.MemberBox.invoke(MemberBox.java:148)\n    at org.mozilla.javascript.NativeJavaMethod.call(NativeJavaMethod.java:225)\n    at org.mozilla.javascript.optimizer.OptRuntime.call1(OptRuntime.java:32)\n    at org.mozilla.javascript.gen.file__C__Users_Toshiba_Desktop_openrefine_3_1_webapp_extensions_module_MOD_INF_controller_js_8._c_init_4(file:/C:/Users/Toshiba/Desktop/openrefine-3.1/webapp/extensions/module/MOD-INF/controller.js:89)\n    at org.mozilla.javascript.gen.file__C__Users_Toshiba_Desktop_openrefine_3_1_webapp_extensions_module_MOD_INF_controller_js_8.call(file:/C:/Users/Toshiba/Desktop/openrefine-3.1/webapp/extensions/module/MOD-INF/controller.js)\n    at org.mozilla.javascript.ContextFactory.doTopCall(ContextFactory.java:405)\n    at org.mozilla.javascript.ScriptRuntime.doTopCall(ScriptRuntime.java:3508)\n    at org.mozilla.javascript.gen.file__C__Users_Toshiba_Desktop_openrefine_3_1_webapp_extensions_module_MOD_INF_controller_js_8.call(file:/C:/Users/Toshiba/Desktop/openrefine-3.1/webapp/extensions/module/MOD-INF/controller.js)\n    at edu.mit.simile.butterfly.ButterflyModuleImpl.scriptInit(ButterflyModuleImpl.java:636)\n    at edu.mit.simile.butterfly.ButterflyModuleImpl.init(ButterflyModuleImpl.java:94)\n    at edu.mit.simile.butterfly.Butterfly.initializeModule(Butterfly.java:476)\n    ... 8 more\nCaused by: org.apache.lucene.index.IndexFormatTooOldException: Format\nversion is not supported (resource\nBufferedChecksumIndexInput(MMapIndexInput(path=\"C:\\Users\\Toshiba\\AppData\\Roaming\\OpenRefine\\cache\\rdfExtension\\export\\luceneIndex\\segments_1\"))):\n0 (needs to be between 4 and 6). This version of Lucene only supports\nindexes created with release 5.0 and later.\n    at org.apache.lucene.codecs.CodecUtil.checkHeaderNoMagic(CodecUtil.java:213)\n    at org.apache.lucene.index.SegmentInfos.readCommit(SegmentInfos.java:302)\n    at org.apache.lucene.index.SegmentInfos.readCommit(SegmentInfos.java:286)\n    at org.apache.lucene.index.IndexWriter.(IndexWriter.java:938)\n    at org.deri.grefine.rdf.vocab.imp.VocabularySearcher.(VocabularySearcher.java:74)\n    at org.deri.grefine.rdf.app.ApplicationContext.init(ApplicationContext.java:31)\n    at org.deri.grefine.rdf.app.InitilizationCommand.initRdfExportApplicationContext(InitilizationCommand.java:38)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n    at java.lang.reflect.Method.invoke(Unknown Source)\n    at org.mozilla.javascript.MemberBox.invoke(MemberBox.java:126)\n    ... 18 more\nany advice\nOn Sun, Feb 17, 2019 at 8:22 PM Antonin Delpeuch notifications@github.com\nwrote:\n\n@eliohb https://github.com/eliohb This ticket is unrelated to your\nproblem. Please use the latest version of OpenRefine (3.1) and the latest\nversion of the RDF extension (1.1.0) from\nhttps://github.com/stkenny/grefine-rdf-extension/releases.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/585#issuecomment-464490820,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AtMriffCMw1k6VmRoxuXHxnO_gQ1Z9B-ks5vOZ3KgaJpZM4AM1eE\n.\n. \n",
    "gaurav": "It sounds like the easiest fix might be for a core OpenRefine developer to create a self-signed certificate (Keychain Access -> Certificate Assistant -> Create a certificate) and sign the .app with it at release. This changes the error message to the standard unsigned-app error, which you can override by right-clicking and clicking \"Open\". Unfortunately, this would have to be done by someone who is directly involved in release, since it'd be a bad idea to put that certificate into the repository so that the signing could be incorporated into the build process.\n. ",
    "emgee3": "I can confirming that the workaround noted at http://lists.apple.com/archives/java-dev/2012/Jul/msg00136.html does indeed work. Download the referenced script and run on Applications/Google Refine.app/Contents/MacOS/JavaApplicationStub and the error changes to unsigned application.\n. ",
    "kinjal": "I still encountered this error (2.5).  The workaround worked.  But one more step is to reset the security settings to what it was after you run the app once.  Subsequent executions will be allowed and your system will still be protected.\n. ",
    "acdha": "If you don't want to deal with installing a third-party utility just to run 2.5, the following process works:\n1. Replace the existing signature with an ad-hoc self-signature: codesign -f -s - /Applications/Google\\ Refine.app/Contents/MacOS/JavaApplicationStub\n2. Open /Applications, control-click on the Google Refine icon and choose Open\n3. Now the warning dialog will have an Open option in addition to Cancel\nAfter the first time, simply opening the app normally works.\n. @tfmorris This would potentially be quite useful for us. Do you have an idea how substantial the work needed is and/or something I could use as a reference for hooking it up?\n. ",
    "herrernst": "Removing the com.apple.quarantine extended attribute should be enough:\nxattr -rd com.apple.quarantine Google\\ Refine.app/\nWorks for me having gatekeeper set to \"Mac App Store and identified developers\".\n. Hi, I've tried both beta1 and rc1, and both did work on Yosemite without given the 'damaged' warning.\n. Hi,\nthere's not much more. I launch with /Applications/OpenRefine.app/Contents/MacOS/JavaAppLauncher, console output:\n```\n11:24:11.794 [            refine_server] Starting Server bound to '127.0.0.1:3333' (0ms)\n11:24:11.816 [            refine_server] Initializing context: '/' from '/Applications/OpenRefine.app/Contents/Resource/webapp' (22ms)\n11:24:12.430 [                   refine] Starting OpenRefine 2.6-beta.1 [TRUNK]... (614ms)\n11:24:14.730 [                   refine] Sorry, some error prevented us from launching the browser for you.\nPoint your browser to http://127.0.0.1:3333/ to start using Refine. (2300ms)\n```\nThanks.\n. Hi, I've just built current version and default browser now opens for me on OS X 10.9.2, thanks!\n. ",
    "berkeleymalagon": "Confirming that herrernst's xattr call does the trick for me on OSX 10.8.5.\n. ",
    "felixrabe": "@tfmorris - OpenRefine 2.6-beta.1 on OS X 10.9.2 still gives me the dialog:\n\u201cOpenRefine.app\u201d is damaged and can\u2019t be opened. You should move it to the Trash.\n@herrernst's tip WORKS, and I just also launch OpenRefine in that command:\nxattr -rd com.apple.quarantine /Applications/OpenRefine.app && open /Applications/OpenRefine.app\n@acdha's tip for 2.6 does NOT WORK (it worked for Google Refine 2.5 though):\nFelixs-MacBook-Air:~ fr$ codesign -f -s - /Applications/OpenRefine.app/Contents/MacOS/JavaAppLauncher\n/Applications/OpenRefine.app/Contents/MacOS/JavaAppLauncher: replacing existing signature\n/Applications/OpenRefine.app/Contents/MacOS/JavaAppLauncher: code object is not signed at all\nIn subcomponent: /Applications/OpenRefine.app/Contents/PlugIns/jdk1.7.0_25.jdk\n. Btw this is on OS X 10.9.2.\n. Yes, sorry. I'll keep the email of your reply as \"unread\" in my inbox and will try to get to it in the next few days. I do think I (and you) should be able to get to the stack trace following the above steps and I think I should be able to reproduce it in a fresh installation - it was just way more convenient to write the steps down than to follow them at the time two weeks ago.\n. ",
    "k5rola": "I've had the same issue installing Google Refine 2.5 on OS X Yosemite, Version 10.10.1\n@acdha's tip fixed it.\n. ",
    "thayneclark": "@acdha's tip worked for me with OpenRefine 2.5 on OS X 10.9.5. \n. ",
    "akanik": "@herrernst xattr worked like a charm. Thanks!\n. ",
    "BurgosNY": "@acdha tip also worked for me. I'm on Yosemite (10.10.2). Thanks!\n. ",
    "sbecainfo": "thanks @acdha, also worked for me on Yosemite!\n. ",
    "tongzz": "thanks @acdha, your method works.\nFor people who are non-techies like me, open your Terminal on your Mac and type in the command  codesign -f -s - /Applications/Google\\ Refine.app/Contents/MacOS/JavaApplicationStub\nQuoting @acdha \n\nReplace the existing signature with an ad-hoc self-signature: codesign -f -s - /Applications/Google\\ >Refine.app/Contents/MacOS/JavaApplicationStub\nOpen /Applications, control-click on the Google Refine icon and choose Open\nNow the warning dialog will have an Open option in addition to Cancel\nAfter the first time, simply opening the app normally works.\n. \n",
    "tomhundt": "Confirmed @tfmorris 's solution worked for me -- Refine v2.5 (r2407) on vOSX 10.10.3 -- i.e., whereas it had previously complained, upon trying to run it, the app was \"damaged\" and I ought to trash it, now it opens it.  Preferences > Security & Privacy > (click the lock and type password to unlock it, then) Allow apps downloaded from Anywhere.  Now it ran when I launched it as usual.  It popped up some kind of firewall warning, asking for permission to accept connections (I gave it).  Up came a browser window showing the app.  That's as far as I've gotten :-)\nIt says here the OpenRefine versions (which seem to be beta) won't have this \"Damaged\" issue, but since I'm new to this program I went for the stable release off http://openrefine.org/download.html which got me the same v2.5 Google Refine .dmg file as I got from Google themselves.\n. ",
    "iosonosempreio": "Hey, for me those API worked fine. The steps to get the readable date are the followings:\n- select the column containing the unix date, then add a new column by fetching url using this code:\nsh\n'http://www.convert-unix-time.com/api?timestamp='+ substring(value, 0, 10)+'&timezone=Rome&returnType=json'\n- then transform the new column with this:\nsh\nvalue.parseJson().utcDate\n\nNote that I took the utcDate since I didn't need any timezone conversion for my date. If you need it, change the &timezone=Rome in the first code with the one you like. After you've fetched the urls, transform with this new one:\nsh\nvalue.parseJson().localDate\n\nThat's it!\n. ",
    "nunobett": "As I mentioned in the googlegroups, results from both services [1] and [2] still differ a lot.\n[1] - http://reconcile.freebaseapps.com/reconcile?query=hiphop\n[2] - http://standard-reconcile.freebaseapps.com/reconcile?query=hiphop\n. ",
    "Blakko": "Right, i'll stick to the \"build\".\nBtw i was just pointing out that the \"build linux\" creates a package that gives an \"Buildfile: build.xml does not exist!\" when I launch it with the usual ./refine\nP.S. Same applies to the \"build windows\"\n. My bad i didn't check the refine script for that! Thanks a lot!\n. yes i'm using an old (about 4/5 months ago) 2.6 beta version.\nThe exception is the following:\njava.lang.RuntimeException: java.lang.IllegalStateException: No match found\n        at org.jrdf.parser.RdfReader.tryParse(RdfReader.java:141)\n        at org.jrdf.parser.RdfReader.parseNTriples(RdfReader.java:104)\n        at com.google.refine.importers.RdfTripleImporter.parseOneFile(RdfTripleImporter.java:91)\n        at com.google.refine.importers.ImportingParserBase.parseOneFile(ImportingParserBase.java:112)\n        at com.google.refine.importers.ImportingParserBase.parse(ImportingParserBase.java:83)\n        at com.google.refine.importing.ImportingUtilities.previewParse(ImportingUtilities.java:906)\n        at com.google.refine.importing.DefaultImportingController.doUpdateFormatAndOptions(DefaultImportingController.java:185)\n        at com.google.refine.importing.DefaultImportingController.doPost(DefaultImportingController.java:93)\n        at com.google.refine.commands.importing.ImportingControllerCommand.doPost(ImportingControllerCommand.java:62)\n        at com.google.refine.RefineServlet.service(RefineServlet.java:179)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n        at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n        at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:938)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n        at java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.IllegalStateException: No match found\n        at java.util.regex.Matcher.group(Unknown Source)\n        at org.jrdf.util.boundary.RegexMatcherImpl.toString(RegexMatcherImpl.java:116)\n        at org.jrdf.parser.ntriples.parser.NodeParserImpl.parseNode(NodeParserImpl.java:78)\n        at org.jrdf.parser.ntriples.parser.RegexTripleParserImpl.parseTripleLine(RegexTripleParserImpl.java:95)\n        at org.jrdf.parser.ntriples.parser.TripleParserImpl.parseTriple(TripleParserImpl.java:100)\n        at org.jrdf.parser.line.TriplesParserImpl.handleTriple(TriplesParserImpl.java:79)\n        at org.jrdf.parser.ntriples.NTriplesParser.handleLine(NTriplesParser.java:83)\n        at org.jrdf.parser.line.LineParserImpl.parse(LineParserImpl.java:91)\n        at org.jrdf.parser.line.LineParserImpl.parse(LineParserImpl.java:83)\n        at org.jrdf.parser.line.GraphLineParser.parse(GraphLineParser.java:89)\n        at org.jrdf.parser.RdfReader.tryParse(RdfReader.java:139)\n        ... 31 more\n. Got it. Thanks a lot :-)\n. Should be ok now!\n. Well i just saw you did a pretty exaustive documentation page (Thanks!), i only added some infos regarding the language setting menu for completeness.\n. Everything seems ok to me!\nThe point of having multiple LoadLanguage commands for the extensions was sort of \"extension developer\" oriented. This way the developers could have examples of how to internationalize their extensions.\nJust to let you know why i choose this approach!\n. Ops, my bad! i missclicked!\n. Hmm nope, i think that these texts have been added after my commit with the italian translation. I'll pull a fix asap :)\n. It happened to me a couple of times, but it was just related to the interface not being updated. Next time try to refresh the page to see if it's the case :)\n. ",
    "bfeeny": "Note I created a Post on the list.  \nI am now using Edit->Cluster & Edit and not creating a Text Facet.  Result is the same.\nThe error message is a Safari pop-up window, as it has a Safari Icon on it and it says exactly:\nhttp://127.0.0.1:3333\nError: Form too large6350290> 1048576\nIts just like I wrote above, with no space between the word large and the first number.\nLogged to the Console at the same time is:\n8:26:18 PM [0x0-0x1ff1ff].com.google.refine.Refine: java.lang.IllegalStateException: Form too large6350290>1048576\nCould it be possibly that Jetty isn't allowing a post size of > 1MB?\nthank you, I look forward to any direction so I can try for a better result.\nFor now, I just went to \"Edit Cells\" and then put into the box:\nfingerprint(value)\nbut I am not sure if that gives me the same thing that Edit & Cluster -> Select All -> Merge Selected & Re-Cluster gives, do you know?\nBrian\nOn Nov 27, 2012, at 4:35 PM, Tom Morris notifications@github.com wrote:\n\nCan you provide the exact text of the error message so we can see if it's coming from our code or somewhere else? Does it appear in the terminal attached to the Refine server or in your browser?\nAs for your questions, please use the mailing list for questions, not the issue reporting system. That sounds like a very large number of clusters. You definitely don't want to be using a text facet with that many entries (use Edit Cells->Cluster & Edit), but let's continue the discussion on the mailing list.\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "kiminoa": "One interesting use case to note for fine tuning ... I use OpenRefine for work on alphasyllabaries.  (The clustering algorithms are fantastic for a lot of language / inflection-identifying / etc. work.)  At least for Mycenaean Greek (Linear B) and Crypiot Greek alphasyllabaries, these are represented by delimiting between morphemes with white space and internally with dashes, i.e. ti-ri-to represents a 3-symbol word.  Would be slick if any language detection in OpenRefine was also wise enough to say \"oh, hey, this is isn't an alphabetic transliteration, is it? huh.\"\n. The latter, for starters.  Chrome's language detection (Compact Language Detection (CLD)) usually matches Linear B transliterations to Croatian, amusingly.  Are the CLD and Apache's implementation using the same underlying code?\n. I usually see that error if a particular field doesn't have a match.  Errors -> blank usually fixes it.  Are the fields in question not a match?  Maybe show some examples of the fields that are returning this to help clarify ...\n. ",
    "frodeseverin": "Unfortunately I am not well tenured in text processing algorithms.\nHowever, being a native Norwegian speaker, I can provide some extra\ninfo pertaining to the use of \u00e6 in computing.\n\u00c6 is part of ASCII (i.e. DOS) code tables 850 (Western European) and\n865 (Nordic). The character \u00e6 is indeed not a ligature, at least not\nin Norwegian and Danish. In some Norwegian texts, it has been used to\nrepresent the French ligature oe, but I believe this is a wrongdoing\nto both French and Norwegian. Similarly the Norwegian letter \u00f8 was for\nsome time printed on paper by superimposing a / on top of an o. The\nresults were grim.\n\u00c6 is a regular Norwegian and Danish letter, an should be treated as\nsuch. UTF-8 has a two byte code for it, if I am not mistaken. As far\nas I recall, Norwegian and Danish are the only languages to make\nactive use of \u00e6. Swedish uses \u00e4 for similar purposes, at least to my\nknowledge.\nI hope this helps. Please feel free to ask for clarifications.\n. I suppose the right course of action depends on what we are trying to\nachieve.\nI might have chosen the wrong wording for the issue headline. I am trying\nto use normalization on strings to create URI's out of them. Although UTF-8\nis supposed to be valid for URI's, it is not implemented as such in all\nsystems. Hence the need for normalizing.\nAny predictable and consistent form of normalization will be acceptable for\nmy purposes. Historically, Norwegian has been transliterated into the\nEnglish A-Z alphabet by replacing the three 'extra' Norwegian letters\nin this fashion:\n'\u00e6' is replaced with 'ae'\n'\u00f8' is replaced with 'oe'\n'\u00e5' is replaced with 'aa'\nThis is second nature to all Norwegians, and accepted as the way things has\nto be to avoid trouble caused by 'dumb 'computer systems. Hence this is\nlikely the correct course of action in order to normalize Norwegian strings.\nThe letter '\u00e6' is letter number 27 (i.e following 'z') in the Norwegian alphabet, '\u00f8' is number 28, and '\u00e5' is number 29. When sorting strings, transliterated strings get messy. The main problem is that some names for persons and places use legacy spelling in the first place, which follows the transliteration pattern given above. The character sequences 'ae', 'oe' and 'aa' also appear in normal Norwegian words. I suppose this does not have any implications for normalization, though.\nI myself am a purist, in the sense that I'd rather see all computer systems\nbe designed to work with all human languages by default. However, I am\npragmatic enough to make do with the tools at hand, as long as the result\nis predictable and consistent.\n;)Frode\n\nDa sa Gud: \"Det bli lys!\"\nOg det ble lys.\n                      1. Mosebok 1.3\nAnd God said, \"Let there be light,\"\nand there was light.\n                      Genesis 1:3, NIV\n. @tfmorris : Did you come up with a solution?\nAfter a bit more thought I find that using the 'legacy spelling' for the Norwegian characters, ie 'ae' for '\u00e6', 'oe' for '\u00f8' and 'aa' for '\u00e5' is the best way to proceed in this situation.\nHowever, if Java's text Normalizer is your preferred choice for the normalization, and it proves incapable of performing normalization in this manner, pragmatism wins, and the string '\u00e6\u00f8\u00e5' normalizes to 'eoa', or whatever Java's Normalizer produces.\nNorwegians are accustomed to deciphering words in different spelling formats. Moreover, the object of normalization in this setting is making stings legible for computers, and not for humans I suppose.\n. ",
    "dfhuynh": "One solution is to separate that out to 3 options\n( ) Don't omit any row\n( ) Omit rows that look blank (they may contain spaces)\n( ) Omit rows that are completely empty\nDavid\nOn Mon, Jan 14, 2013 at 11:06 AM, Thad Guidry notifications@github.comwrote:\n\nThe description should fit what the actual underlying code is doing.\nSo what is the code actually doing with the Output blank rows option ?\nOn Mon, Jan 14, 2013 at 11:27 AM, Tom Morris notifications@github.comwrote:\n\nA lot of places in Refine treat cells which are null, contain the empty\nstring, and contain a string of whitespace equivalently, although I'm\nnot\nsure such a loose interpretation is always desirable.\nAnyone have opinions on whether we should change the behavior or the\ndescription here?\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/OpenRefine/OpenRefine/issues/651#issuecomment-12229239>.\n\n\n-Thad\nhttp://www.freebase.com/view/en/thad_guidry\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/651#issuecomment-12234057.\n. \n",
    "FWennerdahl": "Thanks for lifting this in and separating the features and improving the backward compability. We will keep in mind to develop features separately when working on open source projects in the future.\nI noticed the String separator tests haven't been included yet but I'm guessing these will be added in another commit.\n. We tested this patch before developing ours. While it saves memory by keeping only one instance of DiskFileItemFactory and FileCleaningTracker, it does not close the file streams which causes the temp files to remain on disk.\nAlso, the FileCleaningTracker relies on the garbage collector to remove the marker objects before removing the files. Under a heavy load this is not fast enough. We experienced that when running a large amount of jobs subsequently (and in parallell) our disk ran out due to the FileCleaningTracker not removing the files fast enough.\nAs the temp files are no longer needed after the upload it is better that they are removed directly without the use of the FileCleaningTracker.\n. In our application we are creating a lot of concurrent jobs at a high pace which exposed some issues with thread safety.\nWhen calling /command/core/get-importing-job-status, we had issues with status updates being cut off due to the config being read at the same time as it was written to. To remedy this we made all reads and writes to the config thread safe.\nWe also identified the job Hashmap as not being thread safe. As this collection is vital to concurrent handling of multiple jobs we decided to add locks when accessing this collection as well.\n. Noticed that this is almost an exact duplicate of #846. I'll let you guys decide which one to accept, if any.\n. While your reworked version solves the current thread-safety issues with usages of ImportingJob, I'm a little concerned about maintaining exposure of the config object, seeing as there is no guarantee that third-party code will lock on it before accessing it.\nI would recommend marking all methods exposing any reference to the config object (or part of it) such as getOrCreateDefaultConfig() as deprecated and provide thread-safe alternatives for third-party code to migrate to.\n. Noticed that the Travix CI build failed. Looking into it now and will update the pull request.\n. Travis CI build passed but I'm a little worried about the inconsistency between build results. Both JDKs should have failed GrelTests.Setup AND ProjectManagerTests.canSaveAllModified in the first build.\nAlso, I'm having inconsistent results when running tests in Eclipse. I'm using TestNG and the RefineTests.launch script in accordance with https://github.com/OpenRefine/OpenRefine/wiki/Developers-Guide (although its called GoogleRefineTests.launch in the wiki). I did not see the errors reported by Travis when doing this (although I should have). If I run separate test packages I do see the failures although I evidently didn't do this prior to my pull request.\nInstead, using JDK6 I get 18 errors in com.google.refine.tests.importers as well as one in ToFromConversionTests.testToString. Using JDK7 I only get the later one. These were all present in our fork of the master.\nI also found one error in TsvExporterTests.Setup (again, the ProjectManager.singleton not set when creating projects) which was not reported by Travis or by the OpenRefineTests.launch script. I'll fix this and push the commit to this pull request as well.\n. The issue we had with being unable to delete projects was caused by concurrent access by the auto save thread. The delete operation could fail (depending on which thread won the race) and leave project files on disk.\nIn our use case we are running OpenRefine via the API with a large number of concurrent operations. Once imported, predefined operations are uploaded and executed on each project before exporting the results. This of course increases the risk of triggering a race condition (15% of the delete operations failed in our case), however UI-initiated operations can also occur at the same time as the auto-save operation which would cause one of the two to fail.\nInstead of just hacking a fix for this particular race we decided to rework the project file access to eliminate any unchecked race conditions and avoid new races to emerge, which also encapsulated locking. Doing this made our heavy-load use case run 40% faster, which is a significant improvement for anyone running batch-jobs in OpenRefine.\n. From what I've understood, since the JVM can (and will) cache and reuse wrapper instances, locking on such objects isn't recommended. Consider locking on instances of Long in several classes: if instances are shared across the application two synchronized-blocks might block each other even if they do not share locks. Depending on the implementation this can cause unexpected behaviours and deadlocks.\n. ",
    "Krisa": "Does this work? If yes, how? \nI have a CSV and when I go in the Custom Tabular Exporter, I have this:\n{\n  \"format\": \"csv\",\n  \"separator\": \",\",\n  \"lineSeparator\": \"\\n\",\n  \"encoding\": \"UTF-8\",\n  \"outputColumnHeaders\": true,\n  \"outputBlankRows\": true,\n  \"columns\": [\n    {\n      \"name\": \"first_name\",\n      \"reconSettings\": {\n        \"output\": \"entity-name\",\n        \"blankUnmatchedCells\": false,\n        \"linkToEntityPages\": true\n      },\n      \"dateSettings\": {\n        \"format\": \"iso-8601\",\n        \"useLocalTimeZone\": false,\n        \"omitTime\": false\n      }\n    },\n    {\n      \"name\": \"last_name\",\n      \"reconSettings\": {\n        \"output\": \"entity-name\",\n        \"blankUnmatchedCells\": false,\n        \"linkToEntityPages\": true\n      },\n      \"dateSettings\": {\n        \"format\": \"iso-8601\",\n        \"useLocalTimeZone\": false,\n        \"omitTime\": false\n      }\n    }\n  ]\n}\nI added the quote all option as follow:\n{\n  \"format\": \"csv\",\n  \"separator\": \",\",\n  \"lineSeparator\": \"\\n\",\n  \"quoteAll\": true,\n  \"encoding\": \"UTF-8\",\n  \"outputColumnHeaders\": true,\n  \"outputBlankRows\": true,\n  \"columns\": [\n    {\n      \"name\": \"first_name\",\n      \"reconSettings\": {\n        \"output\": \"entity-name\",\n        \"blankUnmatchedCells\": false,\n        \"linkToEntityPages\": true\n      },\n      \"dateSettings\": {\n        \"format\": \"iso-8601\",\n        \"useLocalTimeZone\": false,\n        \"omitTime\": false\n      }\n    },\n    {\n      \"name\": \"last_name\",\n      \"reconSettings\": {\n        \"output\": \"entity-name\",\n        \"blankUnmatchedCells\": false,\n        \"linkToEntityPages\": true\n      },\n      \"dateSettings\": {\n        \"format\": \"iso-8601\",\n        \"useLocalTimeZone\": false,\n        \"omitTime\": false\n      }\n    }\n  ]\n}\nBut it does not seem to have any effect.\n. ",
    "rayhenry": "Good advice -- Turns out that I didn't have the necessary access. Problem fixed.\n. I had a similar problem. The problem was that my company techs had restricted path access on my laptop as part of their security settings.\nThe solution was to install Refine somewhere where the program would have full access. In my case, installing in \u201cMy Documents\u201d fixed the issue, but that will vary based on individual security settings.\nFrom: gothwin [mailto:notifications@github.com]\nSent: Tuesday, August 13, 2013 5:14 AM\nTo: OpenRefine/OpenRefine\nCc: Henry, Ray\nSubject: Re: [OpenRefine] Error uploading data (#670)\nI am having the same issue on a Win 7 machine (trying to upload a csv file to Google refine). Can expand on \"Does %TEMP% point somewhere reasonable? Do you have write access to it?\" - am I to assume %TEMP% is the user or system TEMP variable or something else? Can you also please expand on 'somewhere reasonable'? Thanks.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/670#issuecomment-22552278.\nThe information contained in this communication is intended for the use\nof the designated recipients named above. If the reader of this \ncommunication is not the intended recipient, you are hereby notified\nthat you have received this communication in error, and that any review,\ndissemination, distribution or copying of this communication is strictly\nprohibited. If you have received this communication in error, please \nnotify The Associated Press immediately by telephone at +1-212-621-1898 \nand delete this email. Thank you.\n[IP_US_DISC]\nmsk dccc60c6d2c3a6438f0cf467d9a4938\n. ",
    "johngoodwin225": "I am having the same issue on a Win 7 machine (trying to upload a csv file from the Desktop to Google refine). Can you expand on \"Does %TEMP% point somewhere reasonable? Do you have write access to it?\" - am I to assume %TEMP% is the user or system TEMP variable or something else? \n. thank you rayhenry - I'll try this when I'm back in tomorrow. Sounds promising. \n. wu12345 something to do with security settings. Make sure you install openrefine in an area you have read and write access to (e.g. my documents) - this should fix the problem. Did for me. \n. ",
    "wu12345": "I have this exact issue. How do I fix the bug? it's unclear to me. Am I somehow supposed to go to the source code and edit it? \n. ",
    "jmcastagnetto": "Ok, done. Submitted pull request although it is just a one-liner trivial change :-)\n. Not a problem Tom. Glad that a namesake of mine (my middle name is Martin :-) applied the pull request. \nHave submitted a kludge which might be useful. Check the issue number 674.\nCheers\n. Let me know if there are some code standards issues in my patch. Just started using OpenRefine last Friday.\n. ",
    "manugarri": "somehow this issue is happening in Ubuntu 14.10\n\n. ",
    "pbhj": "I've got 2.5 r2407 for linux just now as a download from GitHub and the bug is still present, altering the \"(6|7)\" to \"(6|7|8)\" in the script works for me, I'm using  'java version \"1.8.0_45\" ' (according to \"java -version\").\nThe error I got was \"Google Refine requires Java version 6 or later. If you have multiple versions of Java installed, please set the environment variable JAVA_HOME to the correct version.\"\nOnce the script was altered in ran without any apparent problem.\nFWIW I'm running Kubuntu 14.10.\n. ",
    "baditaflorin": "Or just don`t render them at all, filter and show only the top 10, etc\n. not sure if this can help, for tables https://github.com/mleibman/SlickGrid/\n. How can i add the setting if i want to run directly the openrefine.exe file ?\n. @tfmorris thanks for the enhancement :) Open Refine is the best thing that i found online for Data Cleaning, thanks for making this happen :)\n@wetneb at least from my side, that could be useful, so that less persons end up with this message. \n. at least for code, i am now using and saving my code in a google spreadsheet.\nIf something like this would get implemented, i would use it and share my \"code\"\ndenumire    ce face cod\nCount   Numara cate instante ale literei/cuvantului exista   value.split(\"#\").length()-1 \nSterge ultimul # de la capatul celulei      value.replace(/#$/, \"\")\n. I don`t know for rodalpho, but for me, i have the court files from the romanian justice system, from 2000 to 2014, and the data is really bad. \nYou can have the same entity written in 1000 ways.\nLike \nCalea Ferata Romana\nCaile ferate Romane\nCalea Ferat\u0103 Romana\nCalea Ferat\u0103 Rom\u00e2na\nCalea Ferat\u0103 Rom\u00e2n\u0103\nCala Ferata Romana\netc\n. Even if i clean and rebuild, with refine file modified i still get the same error \nif [ -z \"$REFINE_MAX_FORM_CONTENT_SIZE\" ] ; then\n    REFINE_MAX_FORM_CONTENT_SIZE=\"99048576\"\nfi\nadd_option \"-Drefine.max_form_content_size=$REFINE_MAX_FORM_CONTENT_SIZE\"\n. I manage to change only by modify-ing the refine.java file from server/src/com/google/refine/\nfinal int maxFormContentSize = Configurations.getInteger(\"refine.max_form_content_size\", 99999999);\nhttps://github.com/OpenRefine/OpenRefine/blob/9b2a506caada4ea6d580d2140184fef8caa7566c/server/src/com/google/refine/Refine.java\n. I was able to extract the file, to select and paste in notepad ++ the JSON in firefox. \nGoogle Chrome seems to handle the conversion in different way, and fails to load. \nI will close this issue\n. what is the process that handles the data conversion ? Can it be linked diretly, to create a save as archive of the final JSON file ?\n. @tfmorris i think i will end up with 100 to 200M rows\nI have it in 240 pieces, but i wanted to do a single clustering on the whole database, and then apply to each of the pieces-es\nI did the same with another column, that the final row count is only 16M rows, and after 55 steps, i was able to extract the JSON operations history , that occupies 164 MB , and now i plan to apply to each of the 240 pieces individually, so i don`t have to do 10-20 manual operations of clustering for each individual piece.\nI know that OpenRefine is not designed for this, but i think in the future more and more people will try to use OpenRefine, for bigger and bigger data-sets, and also because is the only software to date that i have tested that works so good with clustering really bad data-sets, like the ones that i have. And i tested over 20 different software but none had what it take to be useful \nSo even if what i post will not get fixed, because is not the primary purpose to work with really big datasets, i think this will be useful in the future, to know what kind of problems can arise when you want to increase the order of magnitude by 10. \n. I don`t know what is the procedure, but if you say that is faster, this is a good think, i would love to test it, when it will be intergrated into OpenRefine. \nI run Clustering on big datasets ( 15 M rows ) and usualy i have to wait around 40-50 seconds even on the simpliest algorithm \nHere is the code for the actual clustering methods https://github.com/OpenRefine/OpenRefine/tree/9b2a506caada4ea6d580d2140184fef8caa7566c/main/src/com/google/refine/clustering/binning\n. If you will need a real live database to play, i have the database with all the objects of all the trials from 2000 to 2015, from all the justice system courts in Romania - 16 M Rows, 800.000 different values. \nThere are a lot of examples of real life errors made by real people\nhttps://www.dropbox.com/s/lg6nmb8l56l19px/240_instante_obiect.rar?dl=0\nWaiting to test your repo\n. @li-guoliang You can find the dropbox link in the comment above\nI also have a list of 46M rows containing all the firms, public administration or persons that have been in trial. \n. Was able to get to 200.000 clusters in Firefox \nAfter first removing \nvar browseLink = $(''+$.i18n._('core-dialogs')[\"browse-this-cluster\"]+'')\n                .addClass(\"clustering-dialog-browse-focus\")\n                .attr(\"href\",url)\n                .css(\"visibility\",\"hidden\")\n                .appendTo(div);\n$(tr.insertCell(2))\n            .mouseenter(function() { browseLink.css(\"visibility\", \"visible\"); })\n            .mouseleave(function() { browseLink.css(\"visibility\", \"hidden\"); })\n            .append(ul)\n            .append(div);\nBecause it was adding the code, and also after removing the first 3 cell, keeping only the merge and the new value. \nI will try to see if i can get more, and then i will try to find a way to make it in a script that will adapt, depending on the number of clusters that it finds \nI will try to with more then 200.000, but i am not sure if is possible, because of the DOM elements ( over 1 M of them )\nI will research to see if is possible, only for clustering, to use something like https://github.com/mleibman/SlickGrid/\nhttp://dobyjs.com/#grid or http://www.trirand.com/blog/phpjqgrid/examples/paging/scrollbar/default.php\n\n. ",
    "adrianpomilio": "Thanks, I searched and did not find that. Sorry for the issue. I have\ninstalled from source build.\nOn Wed, Feb 6, 2013 at 4:52 PM, Tom Morris notifications@github.com wrote:\n\nIt's actually MacOS which is damaged. Please see #590https://github.com/OpenRefine/OpenRefine/issues/590(or the various blog posts around the web when Apple broke this with\nMountain Lion 10.8).\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/675#issuecomment-13207398.\n\n\nRegards,\nAdrian Pomilio\nBlog: http://www.uiandtherest.com\n\nPlease consider the environment before printing this email.\n. ",
    "sutt": "Each google drive account only has a quote of 250MB worth of Fusion Table storage: https://support.google.com/fusiontables/answer/171181?hl=en#quotas\n. @nmggithub Could you please elaborate with an example?\n@tfmorris I have started working on multi-sheet export. Can the controls for this go in Custom Tabular Exporter GUI?\n. @nmggithub This is very helpful, thank you. \nI think I'll only be able to add a one-column-at-a-time solution for though. \n. per the example: You can do either: \n- Sheets for name: Workbook { Sheet_Kelly, Sheet_Carter, etc... }\n  or \n- Sheets for Course: Workbook {Sheet_Chemistry, Sheet_English, etc...}\nBut it won't be possible to do:\nWorkbook {Sheet_KellyChemistry, Sheet_KellyEnglish, Sheet_CarterChemistry, Sheet_CarterEnglish...etc}\nIs this still worthwhile?\n. OK, maybe multi-column will be possible...can't be sure yet. I'm just getting started with OR myself.\n. ",
    "robroc": "The browser was unresponsive. But switching to rows mode did fix the problem. Thank you for the speedy response.\n. ",
    "python4d": "sorry for this issue already close...\n\"use the \"quotation mark\" option to avoid this issue\"\n. ",
    "ghirardinicola": "I get this error usign \"freebase reconciliation service\".\nCan be related to this issue?\n11:36:13.186 [                   refine] POST /command/core/guess-types-of-column (9400ms)\n11:36:16.806 [                  command] Failed to guess cell types for load\n{\"q0\":{\"query\":\"bimbi\",\"limit\":3},\"q1\":{\"query\":\"cartoon - jap\",\"limit\":3},\"q2\":{\"query\":\"Spirited away - La Citt\u00c3\u00a0 incantata\",\"limit\":3},\"q3\":{\"query\":\"Totoro\",\"limit\":3},\"q4\":{\"query\":\"doppiat-oldi\",\"limit\":3},\"q5\":{\"query\":\"Le notti di Cabiria\",\"limit\":3},\"q6\":{\"query\":\"Life on the Mississippi\",\"limit\":3},\"q7\":{\"query\":\"M. Butterfly\",\"limit\":3},\"q8\":{\"query\":\"mala noche\",\"limit\":3},\"q9\":{\"query\":\"Mannheim - New York\",\"limit\":3}} (3620ms)\njava.io.IOException: \n\n\nError in //4.standard-reconcile.dfhuynh.user.dev.freebaseapps.com./reconcile\n\n\n\n\n```\n    Error in //4.standard-reconcile.dfhuynh.user.dev.freebaseapps.com./reconcile\n<p class=\"msg\">JS exception: Error: java.io.IOException: Could not fetch URL: http://api.freebase.com/api/service/search?read=15000</p>\n\n<ul>\n    <li class=\"internal\">\n\n    acreboot.js: 1020\n    </li>\n    <li class=\"internal\">\n\n    acreboot.js: 1124\n    </li>\n    <li class=\"internal\">\n\n    acreboot.js: 1169\n    </li>\n    <li class=\"internal\">\n\n    acreboot.js: 955\n    </li>\n    <li>\n\n    <a href=\"http://dev.freebase.com/appeditor/#!path=//4.standard-reconcile.dfhuynh.user.dev.freebaseapps.com./util&amp;line=219\" target=\"_blank\">//4.standard-reconcile.dfhuynh.user.dev.freebaseapps.com./util</a>: 219\n    </li>\n    <li>\n\n    <a href=\"http://dev.freebase.com/appeditor/#!path=//4.standard-reconcile.dfhuynh.user.dev.freebaseapps.com./reconcile&amp;line=91\" target=\"_blank\">//4.standard-reconcile.dfhuynh.user.dev.freebaseapps.com./reconcile</a>: 91\n    </li>\n    <li>\n\n    <a href=\"http://dev.freebase.com/appeditor/#!path=//4.standard-reconcile.dfhuynh.user.dev.freebaseapps.com./reconcile&amp;line=124\" target=\"_blank\">//4.standard-reconcile.dfhuynh.user.dev.freebaseapps.com./reconcile</a>: 124\n    </li>\n    <li>\n\n    <a href=\"http://dev.freebase.com/appeditor/#!path=//4.standard-reconcile.dfhuynh.user.dev.freebaseapps.com./reconcile&amp;line=168\" target=\"_blank\">//4.standard-reconcile.dfhuynh.user.dev.freebaseapps.com./reconcile</a>: 168\n    </li>\n    <li class=\"internal\">\n\n    acreboot.js: 2180\n    </li>\n    <li class=\"internal\">\n\n    acreboot.js: 2199\n    </li>\n    <li class=\"internal\">\n\n    acreboot.js: 2576\n    </li>\n    <li class=\"internal\">\n\n    acreboot.js: 2726\n    </li>\n    <li class=\"internal\">\n\n    acreboot.js: 2731\n    </li>\n    <li class=\"internal\">\n\n    acreboot.js: 35\n    </li>\n</ul>\n\n\n```\n\n    at com.google.refine.commands.recon.GuessTypesOfColumnCommand.guessTypes(GuessTypesOfColumnCommand.java:189)\n    at com.google.refine.commands.recon.GuessTypesOfColumnCommand.doPost(GuessTypesOfColumnCommand.java:89)\n    at com.google.refine.RefineServlet.service(RefineServlet.java:177)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n    at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n    at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n    at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n    at org.mortbay.jetty.Server.handle(Server.java:326)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:923)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:547)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\n11:36:16.808 [                  command] Exception caught (2ms)\n. Same error. Actually i was using the trunk version\nOn 4 September 2013 13:57, Tom Morris notifications@github.com wrote:\n\nThat service definitely no longer works. Please make a backup of your\nworkspace directory and give the OpenRefine 2.6 beta 1 release a try.\nhttps://github.com/OpenRefine/OpenRefine/releases\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/696#issuecomment-23783698\n.\n. Did it:\nhttps://github.com/OpenRefine/OpenRefine/issues/805\n\nPS. How can i add a new service with the url you gave me?\nOn 4 September 2013 15:19, Tom Morris notifications@github.com wrote:\n\nSounds like an upgrade issue. We should have removed the old recon service\nsince it's broken (ie it's the service itself that doesn't work). If you\ndon't have it listed, you can add the new service using the endpoint:\nhttp://reconcile.freebaseapps.com.\nCan you open an new issue for the upgrade issue and provide a copy of your\nlist of reconciliation services? Thanks!\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/696#issuecomment-23788049\n.\n. It's not an upgrade actually, I am using the last version from github.\nI was able to configure the new service by hands, thanks!\n. I just downloaded the trunk (and cleaned the settings ) and I still have this problem using freebase reconciliation.\nCreating a new reconciliation service using the new url works.\n\nThis the error:\n```\n    Error in //standard-reconcile.freebaseapps.com./reconcile\n<p class=\"msg\">JS exception: acre.errors.URLError: urlfetch failed: 410</p>\n\n```\n. Do you have an advice on how to implement this?\nIn order to start I'd like to know where is the code of the existing wrapper.\nThanks!\n. It does not seems the same bug to me. \nAnd this service is included in the OpenRefine distribution and code.\nWhere I have to report this bug?\n. ",
    "ivankovalenko": "That's ok when all null columns are simple. But in case of 30-column table\nwith sparsed nulls - a bit tricky. So clipboard cut-and-paste goes well\nwith that. Thanks for the tip.\n2013/4/11 Tom Morris notifications@github.com\n\np.s. you can recover the table structure by a) moving or deleting the\nheaders and b) using the Transpose command to unwrap every 3 lines into\ncolumns.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/702#issuecomment-16206478\n.\n. \n",
    "psychemedia": "Hi Herwig - my 'native OpenRefine\" workaround was also to create a project file with just an identifier, and then load the JSON into a from-URL created column and parse it.\n@dracos also suggested this workaround when I asked whether a list version of the data was available: http://dracos.co.uk/temp/mapit-hash-to-list/ (it transforms the object to a list so the records can be straightforwardly loaded in )\nThe issue remains though if OpenRefine is to be used by non-developers...?\n. I went round the houses a couple of times, and it seems to work on \"./refine -p 3334\". I checked there were no other OpenRefine servers running previously, so not sure what the issue was?\n. @Ravenwater The particular use case I have been exploring is the assembly of virtual machines for use in distance education by remote students. I am part of a team writing a distance education course on data management and data analysis for the UK Open University, and we are supplying students with a virtual machine that contains various databases (mongodb, postgresql), an analysis environment (IPython notebooks with a lot of preinstalled python packages), and OpenRefine.\nThe original model was to give students a single VM image configured using vagrant and docker. (As well as getting this particular machine built, I was also interested in ways in which we might support workflows for creating new VM configurations, as well as supporting VM configurations for courses that run once or twice a year for up to 5 years). The particular use case we have at the moment is for students to download a VM image and run it using VirtualBox on their own computer; but I was mindful that there might also be a requirement to support students who want to access the VM running in the cloud or on institutional cloud servers. (Another model might be a traditional university where students use computer labs and need to access software packaged and maintained by the institution on pubic access machines).\nI also started exploring the idea of being able create a VM that was assembled as a combination of docker containers. Part of the attraction of this is that an institution could maintain a set of \"approved containers\" that could be called on by people developing a new course requiring a new set-up.\nThe challenge then becomes one of adding data into the mix eg a set of OpenRefine example projects accessible from an OpenRefine container, eg a database preconfigured with some example database tables, eg IPython Notebook pointing to a directory containing example notebooks.\nA brief timeline of my journey is described in these blogposts:\nhttp://blog.ouseful.info/2013/12/02/packaging-software-for-distance-learners-vms-101/\nhttp://blog.ouseful.info/2014/05/15/confused-again-about-vm-ecology-i-blame-not-blogging/\nhttp://blog.ouseful.info/2014/12/10/thoroughly-confused-about-student-vms-docker/\nhttp://blog.ouseful.info/2015/01/14/using-docker-to-build-course-vms/\n. @Ravenwater Agreed on the commercial software side, though I can imagine (not sure how it would be implemented) something like boot2dockerPro that provides you with metered access to a commercial software container, and that logs the use of said containers, providing billing at an institutional level say. Eg I could imagine an institutional dockerhub that includes commercial as well as free containers, with the institution paying license fees on commercial containers based on a per get basis, or metering via docker runners just logs use of the commercial containers. (Though possibly intrusive, I can also imagine the desirability for corporate types to be able to track usage of all containers using docker on institutional machines.)\nA real issue we have at the UK Open University is the current IT policy that virtual machines are not allowed on trusted/managed computers because they might introduce vulnerabilities (In part, the VM would have access to the core IT network, and arbitrary code can be installed and run in the VM that could then act maliciously within the core network context).\nAs well as the educational context, I have an interest in working with data in general and open data sets in particular, for example in a news context. I first took an interest in VMs after seeing @DataminerUK's Infinite Interns project, which supports the creation of a variety of VM configurations for use in a data journalistic context. In the last few days, I've started pondering how we might create custom configurations based around orchestrated assemblies of containers eg Frictionless Data Analysis \u2013 Trying to Clarify My Thoughts.\n*(You might note that a lot of my interests relate to client side use of containers which I think is underexplored at the moment?)\nWould be great to be able to both work up and generalise some of these ideas further, and also explore the use cases you're looking at.\n. To complement the OpenRefine container, I've started exploring a container for running CSV backed reconciliation services using the Open Knowledge Lab Reconcile-CSV server : Reconcile-CSV container\nWhilst my early tests work fine for calling the reconciliation service from an OpenRefine container by calling the container's IP address on host, I can't seem to register the reconciliation service published by a _--link_ed to Reconcile-CSV server from the OpenRefine container using aliases or IP addresses that exist within docker?\n. Here's another approach that uses a release:\n```\nFROM maven:3.6.0-jdk-8-alpine\nMAINTAINER tony.hirst@gmail.com\nARG RELEASE=3.1\nENV RELEASE=$RELEASE\nRUN wget --no-check-certificate https://github.com/OpenRefine/OpenRefine/releases/download/$RELEASE/openrefine-linux-$RELEASE.tar.gz\nRUN tar -xzf openrefine-linux-$RELEASE.tar.gz  && rm openrefine-linux-$RELEASE.tar.gz\nRUN mkdir /mnt/refine\nVOLUME /mnt/refine\nEXPOSE 3333\nCMD openrefine-$RELEASE/refine -i 0.0.0.0 -d /mnt/refine\nReference:\nto peek inside the container:\ndocker run -i -t psychemedia/openrefinetest /bin/bash\nto run:\ndocker run --rm -d -p 3333:3333 --name openrefine psychemedia/openrefinedemo\n```\nYou can build to a specific version with eg:\ndocker build -t psychemedia/openrefinedemo --build-arg RELEASE=3.1-beta .. @thadguidry Is there a clean step that can be added to your dockerfile to tidy up the container and make it essentially a container for the the clean/distributable app?\nAlso, does OpenRefine require Java JDK to run, or can it get by with JRE?. I've just posted some more thoughts about running reconciliation servers in docker containers that could then be linked to an OpenRefine container: OpenRefine Style Reconciliation Containers.\n. @jackyq2015 I haven't tried any others recently.. will let you know if I find other examples.\n. @tfmorris I'm not sure what the politicking is, but agree that OKF approach tends towards the pragmatic and W3C often sides towards supporting more formal semantic mappings which I think are harder for tinkerers to engage with when trying to implement according to conventions rather than standards.\nIn the world of realpolitik, I'd guess that the W3C approach will have advocacy support from the ODI in its training sessions, whereas OKF style training and products (CKAN) will use the DataPackage approach.\nA problem with both approaches is that it adds complexity - CSV files are nice and convenient, and packages less so, particularly if they use a bespoke suffix rather than .zip for compressed bundles (which means many folk wouldn't know what to do with them..) For me, the packaging is more likely to be of use when it comes to putting together toolchains so that once I fix a datatype I can get it to persist across different applications while still using CSV as the transport.\nIn this respect, I'd personally be looking for signs of other folk using the tools that I use (python/pandas, R, OpenRefine) adopting one or the other, and going with the one that works easiest....\nI'd also be looking for APIs starting to offer support by means of conventionally annotated URLs providing access to metadata files given a CSV URL (so in the W3C example, http://example.org/tree-ops.csv mapping on to http://example.org/tree-ops.csv-metadata.json using the -metadata.json annotation.\nWhilst this is also a 'wait and see' approach, I guess it demonstrates that I don't personally currently value either approach to have opted for one every time I start to play with a new dataset, aside from occasional test cases.\nHowever, if tools at the start of my workflow - such as OpenRefine - were to start producing metadata, I may be tempted to start making more use of it.\nTBH, the biggest win for me at the moment would not necessarily be for my own workflows, but more for basic teaching/education - showing more conceptually how columns can be data-typed, and why this makes sense when you start analysing data using code. (Novices wouldn't necessarily think to cast from one datatype to another...)\n. This makes me wonder: if OpenRefine was a Jupyter client, and supported a Jupyter kernel connection on the back of Edit cells > Transform > Language, you would be opening up support for all manner of things, especially if there was Apache Arrow (https://github.com/OpenRefine/OpenRefine/issues/1469) data transfer in place (which would work with or python/pandas dataframes?).\nI wonder if there are packages on the back of https://github.com/minrk/thebelab that might help with this?. @thadguidry apols for not picking this up sooner, I try to go offline over the w/e.\nMy  R Lang knowledge is sketchy. Thinking a bit more about the Jupyter route as a way of providing different sorts of language support for transformations, this perhaps places an undesirable  requirement of having a Jupyter server running. In which case, 'native' support for R, as with Python/Jython, is perhaps more appropriate.\nThat said, for people working in a Jupyter environment, the ability to connect to a Jupyter kernel (of which there are many) would provide a more general way of supporting transformations using arbitrary languages.\nOne way of doing this might be to provide a way of opening a connection to a Jupyter kernel and somehow (?!) passing state between it and OpenRefine. I'm pitching ideas as a user here, but will try to read up on the docs to see if I can better articulate what I have in mind. But my fieeling is that supporting a Jupyter client as part of OpenRefine would be a big part of that.\nRelated: Jupyter client docs, what looks like it might be an old proof of concept(?) Java Jupyter client;  UI/UX: the ThebeLab Javascript Jupyter client,  or the SageCell server.\ni'm basically just riffing on the idea that Jupyter lets you run code in a \"cell\" against an arbitrary language kernel and then trying to imagine how OpenRefine could leverage it.\n. I left it >10 mins and it seemed to get going itself... \nSo: any ideas why it should take a significantly long time to start running on first run? (It seems to get going okay on later runs.... and then it doesn't: it starts to hang again, or at least, it does if I switch port?)\n[UPDATE]; related? https://github.com/OpenRefine/OpenRefine/issues/1274. Hmmm... experimenting with a Docker container as part of a docker-compose config and once again OpenRefine just hangs, not getting further than:\nrefine_container | /usr/lib/jvm/java-1.8-openjdk/bin/java -cp server/classes:server/target/lib/*   -Xms1400M -Xmx1400M -Drefine.memory=1400M -Drefine.max_form_content_size=1048576 -Drefine.verbosity=info -Dpython.path=main/webapp/WEB-INF/lib/jython -Dpython.cachedir=/root/.local/share/google/refine/cachedir -Drefine.data_dir=/mnt/refine -Drefine.webapp=main/webapp -Drefine.port=3333 -Drefine.host=0.0.0.0 com.google.refine.Refine\nrefine_container | Starting OpenRefine at 'http://0.0.0.0:3333/'\nrefine_container | \nrefine_container | 22:12:34.364 [            refine_server] Starting Server bound to '0.0.0.0:3333' (0ms)\nrefine_container | 22:12:34.368 [            refine_server] refine.memory size: 1400M JVM Max heap: 1419116544 (4ms)\nrefine_container | 22:12:34.393 [            refine_server] Initializing context: '/' from '/openrefine-3.1/webapp' (25ms). Okay, I think the issue is that the hanging was a server memory issue.\nAnyone know what the minimum spec server that OpenRefine will reliably run on and what the start / config parameters for OR are in such a case?\nWith the -1400M default and a 2GB machine I got the hanging situation above. 3GB server seems okay.. @thadguidry I don't have any experience with Java, and don't have a Java environment to work in, so there's a startup cost there that I don't have time to invest in atm... unless maybe you have a VM somewhere I could clone and fire up in an already working state?. There's an example running in MyBinder here:\nhttps://mybinder.org/v2/gh/psychemedia/jupyterserverproxy-openrefine/reconciler\nThis should launch a Jupyter notebook server to s/thing like:\nhttps://hub.mybinder.org/user/psychemedia-jup-roxy-openrefine-RANDOM/tree\nOpenRefine is then available at:\nhttps://hub.mybinder.org/user/psychemedia-jup-roxy-openrefine-RANDOM/openrefine\nThe reconciler is on:\nhttps://hub.mybinder.org/user/psychemedia-jup-roxy-openrefine-RANDOM/proxy/8895/parliament/reconcile/\n(You need the trailing slash. That is perhaps one problem?)\nReconcile:\nhttps://hub.mybinder.org/user/psychemedia-jup-roxy-openrefine-RANDOM/proxy/8895/parliament/reconcile/?queries={%22q1%22:{%22query%22:%22Diana%20Abbot%22}}\nreturns something of the form:\n{\n  q1: {\n    result: [\n      {\n        id: 172,\n        name: \"Ms Diane Abbott\",\n        score: 77,\n        match: false,\n        type: [\n          {\n            id: \"/people/person\",\n            name: \"Person\"\n          }\n        ]\n      }\n    ]\n  }\n}\nThe service is defined in notebooks/Reconciler.ipynb using the Jupyter Kernel Gateway, which allows you to create simple API servers in a Juptyer notebook and serve them using Jupyter machinery.\nI would have expected to also be able to register the service in OpenRefine with:\nhttp://localhost:8895/parliament/reconcile\nbut that seems not to work? (Which is a bit odd, because the connection to Postgres on 127.0.0.1 works here: https://github.com/psychemedia/jupyterserverproxy-openrefine/tree/pgdemo )\nI've also been wondering if I need to set access permissions for the kernel gateway as per config options to handle the Java URL requests? (I've no idea what those permission might need to be tho!)\nI think I had an old flask demo somewhere. I'll see if I can get that to work instead. .... Same with that:\n```\njava.net.ConnectException: Connection refused (Connection refused)\n    at java.net.PlainSocketImpl.socketConnect(Native Method)\n    at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n    at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n    at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n    at java.net.Socket.connect(Socket.java:589)\n    at sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n    at sun.net.www.http.HttpClient.openServer(HttpClient.java:463)\n    at sun.net.www.http.HttpClient.openServer(HttpClient.java:558)\n    at sun.net.www.http.HttpClient.(HttpClient.java:242)\n    at sun.net.www.http.HttpClient.New(HttpClient.java:339)\n    at sun.net.www.http.HttpClient.New(HttpClient.java:357)\n    at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220)\n    at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156)\n    at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1050)\n    at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:984)\n    at sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1334)\n    at sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1309)\n    at com.google.refine.commands.recon.GuessTypesOfColumnCommand.guessTypes(GuessTypesOfColumnCommand.java:174)\n    at com.google.refine.commands.recon.GuessTypesOfColumnCommand.doPost(GuessTypesOfColumnCommand.java:89)\n    at com.google.refine.RefineServlet.service(RefineServlet.java:190)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n    at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n    at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n    at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n    at org.mortbay.jetty.Server.handle(Server.java:326)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:923)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:547)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\n```. I moved to 0.0.0.0 and got the same?. I tweaked an earlier fork of https://github.com/OpenRefine/reconciliation_service_skeleton just now here \u2014  https://github.com/ouseful-backup/reconciliation_service_skeleton \u2014 that I think has the correct/required metadata fields. This gives the same connection error as above?\nIt'd be useful to see a working version of OpenRefine/reconciliation_service_skeleton.. Hmm... generally, I though 127.0.0.1 should work for localhost traffic (eg\nwithin a container) and 0.0.0.0 would accept everything?\nOn Sat, 9 Feb 2019 at 09:59, Antonin Delpeuch notifications@github.com\nwrote:\n\n@psychemedia https://github.com/psychemedia yes, the problem is that\nboth 127.0.0.1, 0.0.0.0, localhost or anything like that is only going to\nbe understood locally, on your Binder instance. You need to use an address\nthat can be understood anywhere, including in your browser.\nOne such address would be\nhttps://hub.mybinder.org/user/psychemedia-jup-roxy-openrefine-RANDOM/proxy/8895/parliament/reconcile/\nfor instance.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1952#issuecomment-462030879,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAFELJBEd3NKexW8oavzQBheE11d3XCiks5vLpwPgaJpZM4awUaX\n.\n. @wetneb Agreed. So I have several test set ups and they're all failing when OpenRefine tries to connect from Java:\n\n\nMyBInder, with OpenRefine and the reconciliation server both running inside the same MyBinder container.\nmy local machine with OpenRefine running in a Docker container with port 3333 exposed to host localhost and my reconciliation server either running on host as a Python flask app, or inside a container with ports exposed to host localhost.\n\nAlso, I did try (in the MyBinder instance) passing the full URL to OpenRefine as the service registration URL.\nI'm maybe doing something wrong with my reconciliation service and messing up the networking, as well as fundamentally misunderstanding that I seem to have got working by other means and in other situations before!\nI've overspent so much time on OpenRefine stuff lately, I maybe need to take a break from the keyboard and come back to it when I've cleared my head a bit.\nFor reference:\n\nlocal working notes here: https://github.com/ouseful-backup/reconciliation_service_skeleton\nMyBinder repo: https://github.com/psychemedia/jupyterserverproxy-openrefine/tree/reconciler\n\nFWIW, I can connect fine to Postgres running in same MyBinder container as OpenRefine using localhost: https://github.com/psychemedia/jupyterserverproxy-openrefine/tree/pgdemo  \nSurely the connection is made to the service from the Java process, not the browser. The browser is just rendering the internal state of the OpenRefine Java process.. @wetneb one of the issues might be how URLs are handled. The Mybinder proxy requires a trailing slash on eh http://URL/reconcile/?queries=.... Is that respected as part of the URL I register the service with or is it stripped?. Also, if I look at browser developer tools reconciling against something like WIkidata (which does work), I see no calls to Wikidata in network traffic? All I see are calls to OpenRefine local services?. @wetneb Ok... I've not got that far. The bit that is broken is the bit where OpenRefine says to the reconciliation server - \"have you got anything that sort of matches this thing?\" \nHmmm, in fact, in MyBinder it may not even get that far, it may break when I try to register the service in the first place. (In some local tests that bit was working, at least. I started a new reconciliation, the name of the new service appeared in the Reconciliation panel, then I stalled on the spinny wheel as OR tried to look up reconciliation matches down a column.). ",
    "dracos": "@tfmorris \"It's a little weird\" <-- it's historic, sorry [1] I agree it should return a list under a key object (to allow for possible expansion too cf. mysociety/mapit#18), but we are where we are :) Perhaps for a v2...\n[1] Boring historical record - it originally (this would be 2003ish) had two calls, one to look up IDs given some parameters, and a separate call to return information on IDs as a ID => data hash. When this was combined into one call (as speed improvements made it just as quick to return the data in the first call), the hash was kept, presumably so the things making the second type of call could continue unchanged, and those making the first type could still get the IDs very easily. But it probably shouldn't have done that, oh well.\n. ",
    "prairieskygal": "I'm currently using Windows 7 and Version 26.0.1410.64 m of Chrome.\nEvery page was rendering this way, but just the paragraph text. Not the headers.\n. ",
    "sckucera": "Here's a screenshot; I'm on XP Version    5.1.2600 Service Pack 3 Build 2600, using Chrome Version 29.0.1547.76 m\nSpecifically, pay attention to tall characters, such as \"l,\" \"i,\" or \"T.\"  This doesn't happen in IE or Firefox.  Also, I know XP is ancient, but this happens on all of my computers.\n. ",
    "biofool": "I have a new spreadsheet that takes forever to load and I sanitized this one so I could share it.  Emailed it to you.\n. I can no longer reproduce the problem.  I would close it I think.\nDon't get Mad\nDon't get Even\nGet the Electors the truth! NOW!\nhttps://secure.avaaz.org/campaign/en/stolenelection21/?cqOrokb\nCreate a Beautiful World http://createabeautifulworld.com/\nOn Wed, Aug 2, 2017 at 9:18 AM, Antonin Delpeuch notifications@github.com\nwrote:\n\n@tfmorris https://github.com/tfmorris could you share the dataset so\nthat any contributor could tackle this bug?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/713#issuecomment-319722877,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA-9rz11ZzNrYZCCIshKYH2tFIgwjGEwks5sUKFZgaJpZM4Ani35\n.\n. \n",
    "KartikaGarg": "hi,\nThanks for your reply. It wasn't just for the text facet. For a custom\nfacet, I had to search for strings like Hyd, hyd ,HYD etc. I had to use\nignore case. A project wide property would have made things easier. Also I\ndidn't really want to change my data to all caps. Most of it was proper\nEnglish pronoun like Hyderabad.\nAnyway, thanks for the tips.\nKartika\nOn Wed, May 1, 2013 at 11:25 PM, Tom Morris notifications@github.comwrote:\n\nThere are two things that you can do with the current version:\n- Use one of the operations from the common transformations menu to change\n  the data to lower/upper/title case\n- Use a custom text facet with the expression value.toLowercase() (for\n  example) to force all the values to the desired case when computing facet\n  values\nOn Wed, May 1, 2013 at 10:50 AM, KartikaGarg notifications@github.comwrote:\n\nHi,\nI used google-refine to clean up some messy data . Its a very good tool\nand quite easy to use. Since I did not have a lot of time, My data had a\nlot of entries which differed only in case. I would have liked GARY and\ngary to be counted under one , when text facet was used. However I had\nto\nmanually add up the counts.\nAdding a global property which says case sensitive or insensitive would\nhelp in this case.\nThanks in advance\nKartika Garg\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/OpenRefine/OpenRefine/issues/714>\n.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/714#issuecomment-17295555\n.\n. Hi,\nI trabsformed my data using totitlecase and now I dont need this anymore. Please close this issue\n. \n",
    "syrinxsean": "Only somewhat related. #556 is looking to make new blank rows that can then be edited directly. I want to make new rows that come from an external dataset. That's different than #556.\n. ",
    "gbonanome": ":+1: \nAs a use case, I have a single huge file of 8.6 GB that is almost impossible to parse as a whole with my computer. I have split the file in small ones and would be nice to import them in the same project.\n. Are you sure it was a .json? I was having the same problem and discovered that the file was a .tsv whit only a field formatted as .json.\n. ",
    "BuhtigithuB": "There is this tricks (though I didn't get it to work) : https://www.google.ca/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjgwYversbKAhXIVD4KHVnHC0sQFggbMAA&url=http%3A%2F%2Fkb.refinepro.com%2F2011%2F12%2Fadd-extra-rows-records-in-google-refine.html%3Fm%3D1&usg=AFQjCNEcv4k5ZKZzUoVplhLgGBSWSpECYQ&sig2=NQMIgVHG0jECngRywojgSg\nAnd there is this solution : https://code.google.com/archive/p/google-refine/issues/556\nThat work if you have combined value into a single cell/row... You can fake a single cell/row content and use a different splitting character to do so... Note caution with using default comma splitting character because it will split any row that contains it... I had hard time find back what was just happen when I splited cell/row into 2 rows, because I was splitting with the wrong character... If you split correctly with a filter you should see poping your new row with the content of the previous splitted row... And as earlier mention you can play Open Refine make it create a new \"empty\" row by just append a special character into a row and invoking cell -> split values -> \"input your special character into the pop alert box\"\n:)\n. I add an index column and the issue goes away very strange... The same number of rows are matching though... And I tried export the filtered rows priviously to index the project and there were 29 rows... I circle only one False, but there was 2 as you can see...\n. Ok, I guess I should read about the difference between rows and records mode, I just start using and learn. Thanks to notice me.\n. ",
    "biiip": "Is there a more recent solution to this issue?. @thadguidry Yes, the main issue is that the database is all the time growing and i need to compare new data with already existing one. To be clear: there are some operations that need to be done only for new data and there are operations (deduplication eg) where i need to compare old and new data. It looks like your solution might help for that. . @thadguidry Do you know approximately how much time this kind of developments might take and when this update might be launched?\n. ",
    "pachevalier": "Thanks, I've finally found a simple solution here : https://github.com/OpenRefine/OpenRefine/issues/590\n. ",
    "whxiyi100829": "Oh, I get it! Thank you very much!\nI wait in hope for the improvement.\nBest regards.\n. ",
    "shawnholt": "FYI - would be great to alert user if record mode is on though!\n. Perhaps a more specific request: Create a new flag type called \"modified\" and apply to any row that is modified.  Then a user can export only those rows if they choose.\n. fyi: https://groups.google.com/d/msg/openrefine/NR4g3hwh1Rs/1m5lQv0LmrgJ\n. ",
    "fils": "Here is my use case...\nI am working with JSON-LD (so in application/json or application/json-ld space).   Mixing into that JSON we are looking to use JSON Schema and the hypertext definitions (ref: http://json-schema.org/latest/json-schema-hypermedia.html ) in order to embed information to enable hypermedia navigation of related JSON-LD resources.  \nSo around a JSON-LD resource would be the data associated with the resource (mostly from RDF triple stores) and then also related URL's (text/html) for humans and then also other REST or RPC type resources for the machines to inspection.  Among those would be the OpenSearch and OpenReconcile end points.\nThe idea being that given a controlled vocabulary from RDF / JSON-LD space..  a machine could know if a resource had a certain concept in it (like sample ID).   It could then ask a reconcile end point about a concept \"sample\" and an ID \"sampleID\".   Or do the same to an openSearch end point. \n. ",
    "mfessler": "Thanks for the update. I did manage to redo my operations as you suggested - the individual edits on particular records were lost, but it wasn't too hard to recreate them. (This is a small dataset of membership information for a nonprofit, so I'm glad I was able to figure this out - wouldn't have been able to share the data with anyone else for privacy reasons.) \nMany thanks for the help, and I've already downloaded the latest development version and will use that instead. \n. ",
    "runjuliet": "I'm having the same issues when attempting to install on a WIN7 machine. Screenshot attached shows in a HTTP 404 error.\n\n. ",
    "iloveitaly": "Gotcha, so facets are parsed right here. Is there any docs on the facets array? Or is the best way to look through the java source?  Thanks\n. ",
    "Aubreymcfato": "Hi all. \nI use Google Refine 2.5, and I downloaded yesterday OpenRefine 2.6.\nUnfortunately, OpenRefine doesn't work for me (it doesn't show some buttons and texts): I switched back to GR, but I lost all my projects. \nI saw here that I have to change the URL for the data, but it's not working. \nI tried:\n- JAVA_OPTIONS=-Drefine.data_dir=/aubrey/home/.local/share/openrefine/\n- JAVA_OPTIONS=-Drefine.data_dir=C:\\Users\\user\\AppData\\Roaming\\OpenRefine (copying the URL from a friend)\nI actually see the projects browsing in my local directory, I even tried to copy them in the right directory, but still not working... \n. ",
    "rufuspollock": "@wetneb awesome \ud83d\udc4d \ud83d\udc4f /cc @pwalsh @vitorbaptista . @psychemedia @tfmorris just letting you know i'm following and ping if any questions. \n. @thadguidry the Data Package specs are also now v1.0 https://blog.okfn.org/2017/09/05/frictionless-data-v1-0/\nThere's a lot of tooling support including JS, Ruby, Python, R etc http://frictionlessdata.io/tools/.\nAlso goodtables (which is python) has developed a lot https://github.com/frictionlessdata/goodtables-py (and there is a partial JS implementation of this in https://github.com/frictionlessdata/tableschema-js).\n/cc @pwalsh @callmealien. @thadguidry i'm not on that mailing list atm so i'm commenting here:\nI contributed to both specs in a significant way: in fact, the CSV on the web started out largely based on Table Schema and Data Packages.\nTable Schema (and Data Package) are a lot simpler and support pretty much all the features i can imagine you wanting.\nThere is also extensive and mature tooling in a lot of languages with a lot of roading testing: http://frictionlessdata.io/software/ - here are some specific examples http://github.com/frictionlessdata/tableschema-js https://github.com/frictionlessdata/tableschema-py https://github.com/frictionlessdata/tableschema-java\nWe've also seen significant community adoption e.g. into pandas https://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.build_table_schema.html - you can read more at http://frictionlessdata.io/articles/\n. @jackyq2015 - you mean examples of the specs in action?\n\nData Package spec: http://frictionlessdata.io/specs/data-package/ - includes some examples\nTable Schema spec: http://frictionlessdata.io/specs/table-schema/ - includes some examples\n\nSome examples of tabular datasets as tabular data packages which include table schemas for the data files: http://datahub.io/docs/data-packages/tabular\nYou can also find many more examples at https://datahub.io/core or https://github.com/datasets (the github datasets are the raw form that get published to https://datahub.io/core)\nSome discussion of comparison of the two: https://discuss.okfn.org/t/w3c-csv-for-the-web-how-does-it-relate-to-data-packages/1715/8\nLet me know if you need more!. > Another question is that the criteria to choice between implementing the \"Data Package spec\" and \"Table Schema spec\" for the OpenRefine. Is it feasible that we start with the \"Table Schema spec\" firstly then move to \"Data Package spec\"? How hard it will be?\nAs @wetneb says Data Package and Table Schema aren't really substitutes:\n\n@jackyq2015 as I explained earlier I don't think there is any discussion to be had about \"Data Package versus Table Schema\". These two things just do not do the same thing at all, and it does not make any sense to compare them: it's like discussing \"XML vs XSD\" or \"JSON vs JSON Schema\"\u2026\n\nThe Table Schema describes the table columns itself.\nThen, if you want it, you have a Tabular Data Resource http://frictionlessdata.io/specs/tabular-data-resource/ which describes a single tabular file and uses the Table Schema to describe the columns.\nFinally, if you need it, you have the (Tabular) Data Package that describes an entire dataset made up of multiple tables.\n@wetneb puts this very well:\n\nSo:\n\nOpenRefine's model should be updated to store metadata following the Table Schema specs\nWe need new importers and exporters following the Data Package specs\n\n\n\n\n@rufuspollock I saw there is a \"rdfType\" in the \"Table Schema spce\". Just wondering if there is any further detail around this? As far as I know, the CSVW covers the RDF.\n\nThis allows you to tie an RDF type to a given column in the table schema. Its super simple to use. In my experience very few data folks use RDF on a regular basis so I think it is relatively less important.\n. @thadguidry \n\nI spoke to Jacky and the approach I think that makes sense is...\n\nAs per @wetneb I'm not sure i follow this approach. As he says:\n\nI don't think an importer for Data Packages should be based on the generic JSON importer - they don't do the same thing at all, and basically no code can be shared between the two.\n\n\nYou also mention re the exporter:\n\nnew exporter for CSVW since this supports the widest Linked Data efforts through JSON-LD format with Schema.org semantics.\n\nI note that CSVW and Data Package aren't especially compatible. Do if you don't also export Table Schema / Data Package stuff people won't be able to consume that.\nOverall, the major benefits of Table Schema and Data Package is they are simple yet relatively powerful -- and the simplicity means they are much easier to support and use (meaning more other consumers and tooling out there). . @thadguidry @jackyq2015 what rdf support do you need? Do your users regularly use RDF types in their work with OpenRefine and how do they do that? That would help get clearer on whether what is there already in TableSchema is sufficient.\nI also note that Data Package / Data Resource etc are all extensible with your own properties if you want.\n@jackyq2015 \n\nMy question is that does it make sense to do the \"table schema\" firstly then move to \"data package\" since it can be put in the \"data package\" later on.\n\nYou can definitely do that and might make a lot of sense as an initial approach.. @thadguidry \n\n@rufuspollock the problem is that there's not good support for Linked Data within Data Package / Table Schema from what I see. I.E. only the rdfType:\n\nWhat exactly are your requirements? Is this an internal modelling system or for import / export (i assumed the latter).\nAnd who do you anticipate using OpenRefine for these use cases. Most data wranglers or data scientists I've worked with rarely use linked data annotations to their tabular data (and if they did i'd guess it would be around the column semantics).. ",
    "pwalsh": "Amazing @wetneb . Happy to support with reviews and discussion on the Data Package Java Library.. > But I really insist: \"CSVW vs Data Packages\" is not the right question to ask. The right question is: what extra data do we want to store for each column in an OpenRefine project, and how is that going to be beneficial to the user.\nBravo.. ",
    "dantexier": "Hi, thanks for you reply.\nThe column represents ID values \u200b\u200bof a person and I want to export to xml, but when I Export --> Templating... makes me an ID number in scientific notation (two ways in last message).\nThe Google Refine version that I have is:\nVersion 2.5 [r2407]\nThanks again,\nJose\n. Hi Tom.\nIt's correct. I used: value.floor().toString()\nSolved it.\nThanks,\nJose\n. ",
    "ovillemain": "Another solution: use language \"Python / Jython\" rather than \"GREL\" with the expression \"return str(value)\". ",
    "DavidLeoni": "\nActually I think this was some kind of timing or ordering issue that only occurred in certain environments, rather than something that was specific the data directory not existing.\n\nI confirm this, I had the same problem on Windows 7 64 bit. In particular, if I run the test singularly it passed, but as part of the suite it failed. I have to try out the fix.\n. The bug appears when MassChange is initialized with changes that successively operate on the same data and read  old values directly from the project (for example, RowFlagChange or ColumnAdditionChange). \nI found the bug while using MassChange to implement my own operations. As far as I know there is no open report regarding this, which is not surprising: Refine uses MassChange only in ReconCopyAcrossColumnsOperation and in RowFlag / RowStarOperation, which change non-overlapping data.\nI adopted Guava Lists and added a test case as requested. \nDavid\n. I tried playing with Refine at that commit and Firefox 36.0.1 on my Windows 7, both with default and different context path, but couldn't reproduce the problem.\n. ",
    "lmsanch": "In my mac, no errors in the console.\nNo Freebase pulldown menu.\nWhen I open Open Refine, I have to manually input 127.0.1:3333 to access\nrefine..\nThanks for your help, and if there is anything else I can do, please, let me\nknow.\nCheers\nFrom:  Tom Morris notifications@github.com\nReply-To:  OpenRefine/OpenRefine\nreply+i-18832607-f49da4158a6ae40e85023c1655605a410ea464ed-3156487@reply.git\nhub.com\nDate:  Sunday, September 1, 2013 3:49 PM\nTo:  OpenRefine/OpenRefine OpenRefine@noreply.github.com\nCc:  Luis M S\u00e1nchez lmsanch@gmail.com\nSubject:  Re: [OpenRefine] Add column from freebase not available  (#797)\nAre there any error messages output on the console/terminal? Do you have a\nFreebase pulldown menu in the Extensions: bar (underneath the Export & Help\nbuttons in the upper right)?\n\u2039\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/797#issuecomment-23631746\n.\n. Thank you Tom\nThis is the link I used:\nhttps://github.com/OpenRefine/OpenRefine/releases/tag/2.6-beta.1\nI have not tried the win-2.6 version,  but all mac and linux have the same\nproblem.\nCheers\nFrom:  Tom Morris notifications@github.com\nReply-To:  OpenRefine/OpenRefine\nreply+i-18832607-f49da4158a6ae40e85023c1655605a410ea464ed-3156487@reply.git\nhub.com\nDate:  Sunday, September 1, 2013 5:00 PM\nTo:  OpenRefine/OpenRefine OpenRefine@noreply.github.com\nCc:  Luis M S\u00e1nchez lmsanch@gmail.com\nSubject:  Re: [OpenRefine] Add column from freebase not available  (#797)\nCan you let me know what URL you downloaded the kit from?  This almost\nsounds like it's one of the old alpha kits.\n\u2039\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/797#issuecomment-23633006\n.\n. Tom:\nI also tried the windows version: same problem.\nNevertheless, I was able to see that there are options under the pulldown\nmenu in the Extension bars, including the input box for Freebase API\u0160the\nproblem seems to be a display issue (the fonts are the same color as the\nbackground, or something like that).\nCheers\nFrom:  Tom Morris notifications@github.com\nReply-To:  OpenRefine/OpenRefine\nreply+i-18832607-f49da4158a6ae40e85023c1655605a410ea464ed-3156487@reply.git\nhub.com\nDate:  Sunday, September 1, 2013 3:49 PM\nTo:  OpenRefine/OpenRefine OpenRefine@noreply.github.com\nCc:  Luis M S\u00e1nchez lmsanch@gmail.com\nSubject:  Re: [OpenRefine] Add column from freebase not available  (#797)\nAre there any error messages output on the console/terminal? Do you have a\nFreebase pulldown menu in the Extensions: bar (underneath the Export & Help\nbuttons in the upper right)?\n\u2039\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/797#issuecomment-23631746\n.\n. Tom:\nI have not seen any issues in the javascript. I'll try later today with\nChrome and Firefox, the only browsers I use. (So, you'' have six cases:\n[Mac, Windows, Ubuntu]x[Firefox, Chrome])\nCheers\nFrom:  Tom Morris notifications@github.com\nReply-To:  OpenRefine/OpenRefine\nreply+i-18832607-f49da4158a6ae40e85023c1655605a410ea464ed-3156487@reply.git\nhub.com\nDate:  Monday, September 2, 2013 3:45 PM\nTo:  OpenRefine/OpenRefine OpenRefine@noreply.github.com\nCc:  Luis M S\u00e1nchez lmsanch@gmail.com\nSubject:  Re: [OpenRefine] Add column from freebase not available  (#797)\nNow that I look more closely, I can see there there is a space in the menu\nwhere the Add Columns from Freebase command would go, so your theory about\nit being a rendering issue fits.\nWhat browser(s) are you using?  Are there any errors on the browser's\nJavascript console?\nAs you may be able to tell, I'm starting to run out of ideas here...\n\u2039\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/797#issuecomment-23675990\n.\n. Cool. Let me know if I can help with anything..maybe run the first test\noutside of your environment.\nCheers\nFrom:  Tom Morris notifications@github.com\nReply-To:  OpenRefine/OpenRefine\nreply+i-18832607-f49da4158a6ae40e85023c1655605a410ea464ed-3156487@reply.git\nhub.com\nDate:  Monday, September 2, 2013 5:40 PM\nTo:  OpenRefine/OpenRefine OpenRefine@noreply.github.com\nCc:  Luis M S\u00e1nchez lmsanch@gmail.com\nSubject:  Re: [OpenRefine] Add column from freebase not available  (#797)\nI've replicated the issue with Chromium on Linux, so I should be able figure\nout what's going on.\n\u2039\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/797#issuecomment-23679353\n.\n. Done..Problem solved. Thank you.\nCheers\nFrom:  Tom Morris notifications@github.com\nReply-To:  OpenRefine/OpenRefine\nreply+i-18832607-f49da4158a6ae40e85023c1655605a410ea464ed-3156487@reply.git\nhub.com\nDate:  Sunday, September 22, 2013 1:16 PM\nTo:  OpenRefine/OpenRefine OpenRefine@noreply.github.com\nCc:  Luis M S\u00e1nchez lmsanch@gmail.com\nSubject:  Re: [OpenRefine] Freebase extension I18N strings missing from\n\"default\" language  (#797)\nAs a workaround, if you run into this problem, use the Language Settings\noption on the main Refine screen to select either English or Italian.\n\u2039\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/797#issuecomment-24886076\n.\n. I tried this this in \"Queries Parameter\" insted of \"Query Parameter\" at (http://reconcile.freebaseapps.com/), and it works. Still having problems with reconciling with OpenRefine.\n{\n  \"q0\": {\n    \"query\": \"Ford\"\n  }\n}\n. Tad, I am confused: Is the reconcile within OpenRefine working? I am referring to the reconciliation service I use in Open Refine http://reconcile.freebaseapps.com/reconcile. Thanks\n. Try typing your localhost IP to bypass the automatic launching.\n. ",
    "kabhinav": "I am running refine from master branch and I have updated the language settings on Windows but I still have the same problem: undefined is displayed in extensions?\n. I am still getting this message in the code checkout from master branch.\n. ",
    "Narendrao9": "Hi tfmorris,\nI am facing the same issue (add columns from free base is not present in the dropdown).\nThe workaround which you have provided doesn't solve the problem.\nCould you help me in this ?\n. ",
    "johntann99": "Thanks for the quick response. I'm looking forward to working with OpenRefine on my other hardware.\n. ",
    "cldwalker": "@tfmorris Fwiw, I installed 2.5 on a mac last night and then when I installed 2.6-beta.1 I saw this issue. The suggested workaround works. Thanks!\n. ",
    "vladan-me": "Eh, I was planning a whole project based on this feature and it broke... Some sort of alternative is to use Fetching URLs From Web Services but that can be slow, limited (single query request) and it creates another column which I will have to rename after deleting previous one? Do you have any other temporary better idea?\n. This is quite useful, I have similar use case as already mentioned - export clusters, use them in a separate program for human verification before merging.. ",
    "magnusmanske": "I did implement one a while ago, partially. Will that do?\nhttps://tools.wmflabs.org/wikidata-reconcile/\n. Plus, git:\nhttps://bitbucket.org/magnusmanske/reconcile\n. ",
    "jithinjayan": "The problem was with my data....a special character in between prevented the full loading of data..\nAwesome Tool\n. ",
    "patdevinwilson": "For some reason, I'm unable to post to that group.\n\uf8ff\nOn Sep 11, 2013, at 5:15 PM, \"Tom Morris\" notifications@github.com<mailto:notifications@github.com> wrote:\nPlease use the mailing list/google group for questions. That way the whole community will see it and whoever's available can answer it, rather than having to wait for a developer. We only use this issue system for bug reports and enhancement requests.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/810#issuecomment-24277002.\n. ",
    "edcorcoran": "I have this same issue and log gives the same output. I can go to http://127.0.0.1:3333/ in my browser and everythign seems to work fine.\n. ",
    "shlomobl": "When I opened the project again, the columns were there. I'm using the last\nversion available for windows and it just automatically opens IE. When I\ncopy-paste the link into Chrome, everything works ok. I'm using windows 7,\nIE 10 and Chrome is updated.\nOn Fri, Sep 27, 2013 at 6:49 PM, Tom Morris notifications@github.comwrote:\n\nThe mailing list https://groups.google.com/forum/#!forum/openrefine is a\nbetter place to ask questions. In addition to the configuration information\nMartin asked for, we also need to know what version of OpenRefine you are\nusing.\nThe most likely cause of the problem 1 is that the separator isn't what\nyou think it is (e.g. an em dash instead of a hyphen). Try cutting and\npasting the single separator character to make sure you've got it right.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/819#issuecomment-25255343\n.\n\n\nDr. Shlomo Blum\nDept. of Bacteriology\nKimron Veterinary Inst.\nPOB 12\nBet Dagan, 50250\nIsrael\nTel.: 972-3-9681680\n. ",
    "eximius313": "If there will be \"coalesce\" method, then I'm fine without 3. First of all - coalesce shoud look something like this:\nnullableField = null;\ncoaleste(nullableField, \"foo\") == \"foo\"\nor\nnullableField = \"bar\";\ncoaleste(nullableField, \"foo\") == \"bar\"\nsecond, coalesce solves only the 3rd point below\n\nConclusion\n~~1)Bullets no. 2 and 3 above should perform exactly the same as no. 4~~\n2)There should be two options: \"Blank out cells\" and \"Null out cells\"\n~~3)Google refine should allow to concatenate nulls treating them as empty strings~~\n4)\"Text facet\" should not treat nulls exactly the same as empty stings - currently both are displayed as (blank) and you cannot distinguish one from another\n. Sounds great! But will this solve:\n1)In [Edit cells -> Common transform] there should be two options: \"Blank out cells\" and \"Null out cells\"\n2)\"Text facet\" should not treat nulls exactly the same as empty stings - currently both are displayed as (blank) and you cannot distinguish one from another\n\n?. That would be awesome!. @thadguidry \n1) Please also have in mind my conclusion 1) from my initial message\n2) isBlank(\" \") should be true (ref)\n3) What would replace(null,null,\"\") do? I have no idea how is it related to concatenating two columns that can have null values. Ok, and what's wrong with:\ncoalesce(cells[\"Column 3\"].value, \"\") + coalesce(cells[\"Column 1\"].value, \"ThisIsNull\") which seems more intuitive to me?. that sounds great!. And by the way - this is the initial reason, that forced me to blank all null cells :D. Thank you very much OpenRefine team!. By the way - when do you plan to release 3.0?. wrong file - sorry. Try this one: testUNID.zip\nI believe it's because UTF-8 with BOM. But it's rather a bug.\nUser doesn't have to know the details of encoding and should not be forced to change the name. @thadguidry how about Making sure BOM is never treated as part of the first column name on import + proper warning that it happened?\nI don't see any scenario why I would want to have invalid characters in my column names. I think that coalesce should take at least 2 arguments, but if it must be at least one, then I think coalesce(null) -> null should be correct behavior. ",
    "daboe01": "same error with a fresh download from today and a ./refine clean ahead.\nare there implicit dependencies?\nmany thanks and best greetings,\ndaniel\nerror message:\n    [javac] /Users/daboe01/Downloads/OpenRefine-2.6-beta.1/main/src/com/google/refine/browsing/facets/ScatterplotFacet.java:396: cannot find symbol\n    [javac] symbol  : method encodeBase64String(byte[])\n    [javac] location: class org.apache.commons.codec.binary.Base64\n    [javac]         String encoded = Base64.encodeBase64String(output.toByteArray());\n    [javac]                                ^\n    [javac] Note: Some input files use or override a deprecated API.\n    [javac] Note: Recompile with -Xlint:deprecation for details.\n    [javac] 1 error\n. ",
    "rahji": "\"The version on the website\" == 2.5\nFor the source build, I was not running ./refine build - it works now, thanks.\nI'm seeing this part of the installation process on this page now, sorry:\nhttps://github.com/OpenRefine/OpenRefine/wiki/Get-Development-Version\n. ",
    "abhillman": "Taking a look at XlsExporter, it looks like the offending line is the call to createCell, Cell c = r.createCell(i), when i>255. Moreover, it looks like there is some code in place to deal with this; in particular, we see the check to see if there were too many columns, if (i == 255 && cells.size() > 256), which sets the last possible cell to \"ERROR: TOO MANY COLUMNS.\" The problem is, however, that after this, i is incremented to 256 and we then make a call to createCell(256), which raises the error. It seems to me there is simply a missing break statement.\n. ",
    "kener": "Don't worry about RLT, the modern Chinese language is LTR. Thanks for remind about the formatting.\nWhat i can't sure is, if there are any functional dependency to the text? As i found some texts haven't been translated in the \"translation-it.json\", check this\n\n. cool~\n. done~ try this https://github.com/OpenRefine/OpenRefine/tree/master/main/webapp/modules/core/langs\n. cool ~ @terry2tan has made it~  \n\n\n\n. ",
    "terry2tan": "Hi, kener~I'm Terry Tan\uff08\u8c08\u548c\uff09, I also want to translate google refine, but don't know how to do it....If   you know how to do , just leave some work with me. Hope I can help~\n. I have saw it. Thx~\n. ",
    "JosefAssad": "Ah, it might be something local, and it might also be me doing something obviously wrong, but I can reproduce this. Openrefine 2.7 on Debian stable, reproducing my dataset at the bottom of this comment.\nSteps to reproduce:\n\nImport the dataset\nTransform the \"rooms\" column to number\nAdd numeric facet on Rooms column\nObserve how histogram looks correct\nClick \"change\" on the facet\nEnter value>2 as GREL\nObserve that the preview assigns the right booleans\nClick ok on the facet expression modal box\nObserve: dataset has not been filtered, the facet box says \"No numeric value present\"\n\nDataset:\nDescription,Rooms,Price\nHello world,1,1000000\nFoo,2,2000000\nBar,2,500000\nBaz,3,3400000\nQux,2,2100000\nWalter,5,760000\nRhonda,1,23000\nMickey Mouse,4,450000. ",
    "rquinn": "thanks, sorry for duplicate. \n. ",
    "annolangen": "Thank for looking at the pull request. I did not file any bugs nor did I\nsearch for them. I am new to OpenRefine, github, and open source software\ndevelopment. Should I file a bug to go along with the proposed fix?\nAnno\nOn Tue, Feb 11, 2014 at 10:21 PM, Pablo Moyano notifications@github.comwrote:\n\nAre there any open bugs for number 1 an number 2 ?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/pull/853#issuecomment-34842475\n.\n. I created https://github.com/OpenRefine/OpenRefine/issues/856 for fix #1.\nFixes #2 and #3 both pertain to\nhttps://github.com/OpenRefine/OpenRefine/issues/799. It mentions both the\nfact of an error as well as the botched formatting of the error message.\n\nI think I am just getting started. I want to work toward a smooth round\ntrip with fusiontables.\nAnno\nOn Wed, Feb 12, 2014 at 10:27 AM, Pablo Moyano notifications@github.comwrote:\n\nplease, try to search for a bug that matches 1 and other for 2, if you\ncan't find already reported bugs, please create them clarifying that you\nare using HEAD version.\nMaybe then we'll need to divide this PR in 3 parts.\nIf you have any questions, please contact me.\nThanks for the fixes!\n\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/pull/853#issuecomment-34899301\n.\n. I may need more help with the process. I intended to request only to pull\nthe code changes, but the pull request shows two commits from my branch.\nThe first one is just for me to use Intellij, which didn't like the same\nsource directory in a module and its parent. I don't want to bother any\neclipse users. I was only thinking of my fork for that commit.\n\nAnno\nOn Wed, Feb 12, 2014 at 11:06 AM, Anno Langen anno.langen@gmail.com wrote:\n\nI created https://github.com/OpenRefine/OpenRefine/issues/856 for fix #1.\nFixes #2 and #3 both pertain to\nhttps://github.com/OpenRefine/OpenRefine/issues/799. It mentions both the\nfact of an error as well as the botched formatting of the error message.\nI think I am just getting started. I want to work toward a smooth round\ntrip with fusiontables.\nAnno\nOn Wed, Feb 12, 2014 at 10:27 AM, Pablo Moyano notifications@github.comwrote:\n\nplease, try to search for a bug that matches 1 and other for 2, if you\ncan't find already reported bugs, please create them clarifying that you\nare using HEAD version.\nMaybe then we'll need to divide this PR in 3 parts.\nIf you have any questions, please contact me.\nThanks for the fixes!\n\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/pull/853#issuecomment-34899301\n.\n. \n\n",
    "ppezziardi": "Got it\n2 messy header rows got Refine fooled. Workaround : remove the messy headers.\nBest,\nPP\n. ",
    "jadient": "Sounds like a problem with the java classpath.  \nTry setting the starting directory to the folder containing the .exe file.\nIf that doesn't work, take a look at the batch file, which shows how the classpath should be set.\n. ",
    "sun2rise": "i unzipped the project into my \"Program Files\" folder... that's the error\nNow i placed it in c:\\ and everything works fine!\n. ",
    "Rookev": "Same error here. Sometimes, after a bit of inactivity, the front end is stuck. I can see graphical changes like hover effects for buttons but the connection seems to be lost and won't come back. Error console shows:\nFailed to load resource: net::ERR_NETWORK_IO_SUSPENDED\n. ",
    "cbdavis": "I was able to get this to work by adding REFINE_MAX_FORM_CONTENT_SIZE=4194304 in the refine.ini file\n. I'm seeing what may be the same problem on openrefine-2.6-beta.1, openrefine-2.6-rc.2, and also a version built today from a fresh git checkout.  You can see a screenshot .\nUnder Chrome everything is indeed fine, and I've verified the problem on Ubuntu 15.10 running Firefox 42.0.  This may also cause confusion about projects not being saved.  In Firefox, clicking on 'Open Project' doesn't show anything even if you have created several projects, while viewing this in Chrome verifies that the projects are indeed there.\n. ",
    "rfc791": "For me the ./refine -p 3334 did not fix it but doing ./refine -i 127.0.0.1 did.. ",
    "adelaide01": "I installed Refine today on my MacOSX. Cannot import any data (Google spreadsheet, directly uploading a CSV from my desktop, or copy and pasting the CSV.\nWhen I copy and paste the CSV this is the error message:\nError uploading data\n.import-temp/1414111018237/raw-data/clipboard.txt (No such file or directory)\n. ",
    "fredinvdg": "Hi, \nThis problem occurs when the software (Refine) cannot create the temporary \".import-temp\" folder within its \"main\" folder.\nTry to move the \"main\" folder, the one containing the executable on which you double-click to start Refine.\nYou have to stop Refine to move the \"main\" folder.\nI hope this help, since it worked for me... (I had the same issue on windows 7)\n. ",
    "zsxwing": "Can someone verify this one? We encountered this concurrency issue about SimpleDateFormat in our application sometimes. Thank you.\n. ",
    "lvca": "This is the fork: https://github.com/lvca/OpenRefine But I should update with last version of OrientDB that is much faster and merge last OpenRefine repo...\n. ",
    "chrinide": "Great work! Lvca.\nIs there any Windows 64bit binary of this OpenRefine? \n. ",
    "chromanna": "I know. I really tried, but I can\u00b4t add new topic in google groups. Don\u00b4t\nknow why:) Use 3 different explorers, use 3 different account. I\ndefinitivly lost my nerves :D\nRomana Velflov\u00e1\n\nMedio Interactive, s.r.o.\nJihlavsk\u00e1 823/78\n140 00 Praha 4-Michle\nwww.medio.cz\nvelflova@medio.cz\nOn Wed, Jul 2, 2014 at 3:23 PM, Tom Morris notifications@github.com wrote:\n\nPlease use the mailing list for questions. The issue tracker is for bug\nreports and feature requests.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/888#issuecomment-47774655\n.\n. \n",
    "HeinrichGit": "It is the Openrefine-win-2.6-beta1 version (not the source control one)\n. ",
    "nmggithub": "You would still have only one set active at a time. You would just tell the program what facets to use without actually activating them. The activation process takes place after you hit export. I'm not a coder (at least in this context) but I what I would think it would be best if it created sheet/tab 1 activated the facets and filters, \"saved\" the sheet/tab, created sheet/tab 2, activated the facets and filters set for that sheet/tab, and so on. An example where this would be useful would be if you have, say, a \"category\" tab, and wanted to create a workbook, with tabs/sheets, one for the rows within each category. This way they only have to open the worksheet, and not go into Refine and then have to go through all the facets and filters. And it saves the user time from having to export each category as a separate workbook, and then have to combine them.\n. http://www.extendoffice.com/product/kutools-for-excel/excel-split-data-into-multiple-sheets.html Here is an example of what I want it to do. This example takes place in Excel, but I want it to take place in the export in Open Refine. I doesn't have to be based off one column. It can be based off of several columns.\n. What do you mean by that?\nOn Thu, Jul 31, 2014 at 5:18 PM, sutt notifications@github.com wrote:\n\n@nmggithub https://github.com/nmggithub This is very helpful, thank\nyou.\nI think I'll only be able to add a one-column-at-a-time solution for\nthough.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/892#issuecomment-50820113\n.\n. Yeah. It was just a suggestion to do it with more than one column.\n. \n",
    "dgbdgb": "OK -\nDone more experimenting and it does work, just takes longer to load into OpenRefine than GoogleRefine. \nThanks,\nDave Brown\n. ",
    "gideonthomas": "Hi @tfmorris,\nI recently started experimenting with Open Refine and found that it is a pretty neat tool.\nI decided that maybe I should begin contributing code as well and thought that it might be a good idea to start with smaller issues like this one.\nMay I work on this issue and submit a pr?\n. Hmm I'm not too sure how to get appveyor to pass\n. ",
    "Pigeo": "I'm using Google Refine version 2.5 (on a Mac). The problem happens \nimmediately after project creation.\nPerhaps this issue has already been corrected in later versions of Open \nRefine ? If so, sorry for the noise.\nLe 02/11/2014 20:41, Tom Morris a \u00e9crit :\n\nWhat version are you using? The line-based text importer shouldn't be \ndoing any interpretation/conversion of the text at all. Does this \nhappen immediately after project creation or after you've applied \nother transformations? Can you provide a small sample project that \ndemonstrates the issue?\n\u2014\nReply to this email directly or view it on GitHub \nhttps://github.com/OpenRefine/OpenRefine/issues/906#issuecomment-61420235.\n. \n",
    "Krienas": "Yesterday I had tried 2.6 beta 1 version on OSX 10.10.2. Had loaded XLS file as data and had created project without problems. Stable 2.5 version was not working, my guess was that too old java version was expected. But that was a guess - didn't try to solve that.\n. Hey, thanks for both of you a lot! Not sure why I didn't find that by myself. It is exactly what I was looking for.\n. ",
    "JobsiteSean": "Yes, it is reasonable.  I have moved jobs since. . ",
    "kevinwong15": "I edited refine.ini, removed the hash and set REFINE_MAX_FORM_CONTENT_SIZE=2097152.\nI am still getting the error form too large when I cluster my massive data. Any ideas?\n. ",
    "martavillegas": "Hi,\nYes, I'm behind a proxy. I can access URL data if I start OpenRefine using \"refine /i IPaddress\" command. But still I cannot use RDF reconciliation services. When choosing any of them (dbpedia, freebase, ...) a pop-up window opens with the message 'Working' and stops there.\nThis is what I get:\nat org.mortbay.jetty.Server.handle(Server.java:326)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:54\n2)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpCo\nnnection.java:923)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:547)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.\njava:228)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n        at java.lang.Thread.run(Unknown Source)\n09:36:30.714 [                  command] Exception caught (50ms)\njava.io.IOException: \n\n\nError 500 com/ibm/icu/text/StringPrepParseException\n\nHTTP ERROR 500\nProblem accessing /extension/rdf-extension/services/dbpedia. Reason:\nCaused by:<\npre>java.lang.NoClassDefFoundError: com/ibm/icu/text/StringPrepParseException\n        at com.hp.hpl.jena.iri.impl.SchemeSpecification.<init>(SchemeSpeci\nfication.java:68)\n        at com.hp.hpl.jena.iri.ViolationCodes$Initialize.<clinit>(Violatio\nnCodes.java:1360)\n        at com.hp.hpl.jena.iri.IRIFactory.<clinit>(IRIFactory.java:111)\n        at org.openjena.riot.system.PrefixMap.add(PrefixMap.java:54)\n        at com.hp.hpl.jena.sparql.util.MappingRegistry.addPrefixMapping(MappingR\negistry.java:33)\n        at com.hp.hpl.jena.query.ARQ.init(ARQ.java:438)\n        at com.hp.hpl.jena.query.ARQ.<clinit>(ARQ.java:456)\n        at com.hp.hpl.jena.sparql.engine.http.QueryEngineHTTP.<init>(Query\nEngineHTTP.java:90)\n        at com.hp.hpl.jena.sparql.engine.http.QueryEngineHTTP.<init>(Query\nEngineHTTP.java:81)\n        at org.deri.grefine.reconcile.rdf.executors.VirtuosoRemoteQueryExecutor.\nsparql(VirtuosoRemoteQueryExecutor.java:20)\n        at org.deri.grefine.reconcile.rdf.endpoints.QueryEndpointImpl.reconcileE\nntities(QueryEndpointImpl.java:38)\n        at org.deri.grefine.reconcile.rdf.RdfReconciliationService.reconcile(Rdf\nReconciliationService.java:69)\n        at org.deri.grefine.reconcile.model.AbstractReconciliationService.reconc\nile(AbstractReconciliationService.java:48)\n        at org.deri.grefine.reconcile.ServiceRegistry.multiReconcile(ServiceRegi\nstry.java:117)\n        at org.deri.grefine.reconcile.GRefineServiceManager.multiReconcile(GRefi\nneServiceManager.java:94)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n        at java.lang.reflect.Method.invoke(Unknown Source)\n        at org.mozilla.javascript.MemberBox.invoke(MemberBox.java:161)\n        at org.mozilla.javascript.NativeJavaMethod.call(NativeJavaMethod.java:24\n7)\n        at org.mozilla.javascript.optimizer.OptRuntime.call2(OptRuntime.java:76)\n        at org.mozilla.javascript.gen.c6._c5(file:/C:/Users/U39306/AppData/Local\n/OpenRefine/extensions/rdf-extension/MOD-INF/controller.js:204)\n        at org.mozilla.javascript.gen.c6.call(file:/C:/Users/U39306/AppData/Loca\nl/OpenRefine/extensions/rdf-extension/MOD-INF/controller.js)\n        at org.mozilla.javascript.ContextFactory.doTopCall(ContextFactory.java:3\n98)\n        at org.mozilla.javascript.ScriptRuntime.doTopCall(ScriptRuntime.java:306\n5)\n        at org.mozilla.javascript.gen.c6.call(file:/C:/Users/U39306/AppData/Loca\nl/OpenRefine/extensions/rdf-extension/MOD-INF/controller.js)\n        at edu.mit.simile.butterfly.ButterflyModuleImpl$Controller.process(Butte\nrflyModuleImpl.java:399)\n        at edu.mit.simile.butterfly.ButterflyModuleImpl$Controller.run(Butterfly\nModuleImpl.java:377)\n        at org.mozilla.javascript.Context.call(Context.java:515)\n        at org.mozilla.javascript.ContextFactory.call(ContextFactory.java:507)\n        at edu.mit.simile.butterfly.ButterflyModuleImpl.processScript(ButterflyM\noduleImpl.java:650)\n        at edu.mit.simile.butterfly.ButterflyModuleImpl.process(ButterflyModuleI\nmpl.java:427)\n        at edu.mit.simile.butterfly.Butterfly.service(Butterfly.java:516)\n        at com.google.refine.RefineServlet.service(RefineServlet.java:200)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511\n)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet\nHandler.java:1166)\n        at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n        at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:155)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet\nHandler.java:1157)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:3\n88)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.jav\na:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:1\n82)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:7\n65)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:1\n52)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:54\n2)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnectio\nn.java:938)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.\njava:228)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n        at java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.ClassNotFoundException: com.ibm.icu.text.StringPrepParseExc\neption\n        at java.net.URLClassLoader$1.run(Unknown Source)\n        at java.net.URLClassLoader$1.run(Unknown Source)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(Unknown Source)\n        at java.lang.ClassLoader.loadClass(Unknown Source)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)\n        at java.lang.ClassLoader.loadClass(Unknown Source)\n        at edu.mit.simile.butterfly.ButterflyClassLoader.loadClass(ButterflyClas\nsLoader.java:65)\n        at java.lang.ClassLoader.loadClass(Unknown Source)\n        ... 57 more\n\nCaused by:\njava.lang.ClassNotFoundException: com.ibm.icu.text.Strin\ngPrepParseException\n        at java.net.URLClassLoader$1.run(Unknown Source)\n        at java.net.URLClassLoader$1.run(Unknown Source)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(Unknown Source)\n        at java.lang.ClassLoader.loadClass(Unknown Source)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)\n        at java.lang.ClassLoader.loadClass(Unknown Source)\n        at edu.mit.simile.butterfly.ButterflyClassLoader.loadClass(ButterflyClas\nsLoader.java:65)\n        at java.lang.ClassLoader.loadClass(Unknown Source)\n        at com.hp.hpl.jena.iri.impl.SchemeSpecification.<init>(SchemeSpeci\nfication.java:68)\n        at com.hp.hpl.jena.iri.ViolationCodes$Initialize.<clinit>(Violatio\nnCodes.java:1360)\n        at com.hp.hpl.jena.iri.IRIFactory.<clinit>(IRIFactory.java:111)\n        at org.openjena.riot.system.PrefixMap.add(PrefixMap.java:54)\n        at com.hp.hpl.jena.sparql.util.MappingRegistry.addPrefixMapping(MappingR\negistry.java:33)\n        at com.hp.hpl.jena.query.ARQ.init(ARQ.java:438)\n        at com.hp.hpl.jena.query.ARQ.<clinit>(ARQ.java:456)\n        at com.hp.hpl.jena.sparql.engine.http.QueryEngineHTTP.<init>(Query\nEngineHTTP.java:90)\n        at com.hp.hpl.jena.sparql.engine.http.QueryEngineHTTP.<init>(Query\nEngineHTTP.java:81)\n        at org.deri.grefine.reconcile.rdf.executors.VirtuosoRemoteQueryExecutor.\nsparql(VirtuosoRemoteQueryExecutor.java:20)\n        at org.deri.grefine.reconcile.rdf.endpoints.QueryEndpointImpl.reconcileE\nntities(QueryEndpointImpl.java:38)\n        at org.deri.grefine.reconcile.rdf.RdfReconciliationService.reconcile(Rdf\nReconciliationService.java:69)\n        at org.deri.grefine.reconcile.model.AbstractReconciliationService.reconc\nile(AbstractReconciliationService.java:48)\n        at org.deri.grefine.reconcile.ServiceRegistry.multiReconcile(ServiceRegi\nstry.java:117)\n        at org.deri.grefine.reconcile.GRefineServiceManager.multiReconcile(GRefi\nneServiceManager.java:94)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n        at java.lang.reflect.Method.invoke(Unknown Source)\n        at org.mozilla.javascript.MemberBox.invoke(MemberBox.java:161)\n        at org.mozilla.javascript.NativeJavaMethod.call(NativeJavaMethod.java:24\n7)\n        at org.mozilla.javascript.optimizer.OptRuntime.call2(OptRuntime.java:76)\n\n```\n    at org.mozilla.javascript.gen.c6._c5(file:/C:/Users/U39306/AppData/Local\n```\n\n/OpenRefine/extensions/rdf-extension/MOD-INF/controller.js:204)\n        at org.mozilla.javascript.gen.c6.call(file:/C:/Users/U39306/AppData/Loca\nl/OpenRefine/extensions/rdf-extension/MOD-INF/controller.js)\n        at org.mozilla.javascript.ContextFactory.doTopCall(ContextFactory.java:3\n98)\n        at org.mozilla.javascript.ScriptRuntime.doTopCall(ScriptRuntime.java:306\n5)\n        at org.mozilla.javascript.gen.c6.call(file:/C:/Users/U39306/AppData/Loca\nl/OpenRefine/extensions/rdf-extension/MOD-INF/controller.js)\n        at edu.mit.simile.butterfly.ButterflyModuleImpl$Controller.process(Butte\nrflyModuleImpl.java:399)\n        at edu.mit.simile.butterfly.ButterflyModuleImpl$Controller.run(Butterfly\nModuleImpl.java:377)\n        at org.mozilla.javascript.Context.call(Context.java:515)\n        at org.mozilla.javascript.ContextFactory.call(ContextFactory.java:507)\n        at edu.mit.simile.butterfly.ButterflyModuleImpl.processScript(ButterflyM\noduleImpl.java:650)\n        at edu.mit.simile.butterfly.ButterflyModuleImpl.process(ButterflyModuleI\nmpl.java:427)\n        at edu.mit.simile.butterfly.Butterfly.service(Butterfly.java:516)\n        at com.google.refine.RefineServlet.service(RefineServlet.java:200)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511\n)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet\nHandler.java:1166)\n        at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n\n```\n    at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:155)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet\n```\n\nHandler.java:1157)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:3\n88)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.jav\na:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:1\n82)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:7\n65)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n\n```\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:1\n```\n\n52)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:54\n2)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnectio\nn.java:938)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.\njava:228)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n        at java.lang.Thread.run(Unknown Source)\n\nPowered by Jetty://\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nat com.google.refine.commands.recon.GuessTypesOfColumnCommand.guessTypes\n(GuessTypesOfColumnCommand.java:189)\n        at com.google.refine.commands.recon.GuessTypesOfColumnCommand.doPost(Gue\nssTypesOfColumnCommand.java:89)\n        at com.google.refine.RefineServlet.service(RefineServlet.java:177)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511\n)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet\nHandler.java:1166)\n        at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\nat org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet\nHandler.java:1157)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:3\n88)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.jav\na:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:1\n82)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:7\n65)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\nat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:1\n52)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:54\n2)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpCo\nnnection.java:923)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:547)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.\njava:228)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n        at java.lang.Thread.run(Unknown Source)\nThanks!!\n. ",
    "Ravenwater": "We (the Data Lake) would be interested in a collaboration mechanism. The use case we are currently working through is working with the Maine CDC on vector-borne disease data and predictive models. We, as a private company, constantly need to issue these FOIA requests, which the folks at the CDC then need to interpret, solve, check for PII leakage, redact, check, double check, export, and email to us. We than need to go through the reverse engineering of what it is that they sent us, check, double check, refine, apply data dictionary, possibly transform, and then use. It would be much more productive if we could collaborate on the data refinement and use the refinement process as a knowledge transfer process, hopefully repeatable and meta-data generating.\nThinking further, would be lovely if we can then create RDF hierarchies for the data and these processes so that they become a true, communicable data assets.\n. I like containers, so I want to think with you. Given the fact that openrefine is already a self-contained widget that minimally inserts itself into a machine, the only workflow that I can see an OpenRefine container would enable is dispatching it in the cloud in a fast and reliable way. To do that properly, the dockerfile would need to architect a storage hierarchy, so that there is some persistence and workspace to get non-trivial things done. If you are working with data sets that reside in a public/private cloud it would be interesting to push the OpenRefine functionality to the data and reduce cost, or increase performance, but that would only kick in for data sets that are over a couple GB. That again would make the storage design more important.\nWhat was the use case that you designed the dockerfile for?\n. @Tony Excellent, thank you for that background: makes perfect sense;\nbeen through that same 'how to package a collection' experiment myself,\nfor us it also included the hardware and software appliance module.\nThe problem that we encountered is that some academic environments are\nvery Microsoft oriented, and some are very Apple oriented, and most of\nthe good server side web application stuff is Canonical oriented. Than\nthrough in a smorgasbord of Java and JVM languages, JS environments for\nweb development, and PSE such as MATLAB, COMSOL, Gaussian, SciLab,\nOctave, and R, and you end up without a good answer.\nWe have been putting some elbow grease into containerizing the server\nside (Hadoop, Cassandra, etc.) guided by the observation that a modern\nSOA is going to be a micro-services platform and containers are the\nright weight for delivering said microservice SOAs. However, that\ndoesn't solve the MATLAB/COMSOL/Gaussian commercial software, which are\nreal workhorses in academia where the cost to use these is minimal.\nI still like the Vagrant path as the tools are more mature and work on\nMicrosoft environments.\nLove to collaborate as we are doing similar things.\nOn 1/28/2015 5:03 AM, Tony Hirst wrote:\n\n@Ravenwater https://github.com/Ravenwater The particular use case I\nhave been exploring is the assembly of virtual machines for use in\ndistance education by remote students. I am part of a team writing a\ndistance education course on data management and data analysis for the\nUK Open University http://www.open.ac.uk/courses/modules/tm351, and\nwe are supplying students with a virtual machine that contains various\ndatabases (mongodb, postgresql), an analysis environment (IPython\nnotebooks with a lot of preinstalled python packages), and OpenRefine.\nThe original model was to give students a single VM image configured\nusing vagrant and docker. (As well as getting this particular machine\nbuilt, I was also interested in ways in which we might support\nworkflows for creating new VM configurations, as well as supporting VM\nconfigurations for courses that run once or twice a year for up to 5\nyears). The particular use case we have at the moment is for students\nto download a VM image and run it using VirtualBox on their own\ncomputer; but I was mindful that there might also be a requirement to\nsupport students who want to access the VM running in the cloud or on\ninstitutional cloud servers. (Another model might be a traditional\nuniversity where students use computer labs and need to access\nsoftware packaged and maintained by the institution on pubic access\nmachines).\nI also started exploring the idea of being able create a VM that was\nassembled as a combination of docker containers. Part of the\nattraction of this is that an institution could maintain a set of\n\"approved containers\" that could be called on by people developing a\nnew course requiring a new set-up.\nThe challenge then becomes one of adding data into the mix eg a set of\nOpenRefine example projects accessible from an OpenRefine container,\neg a database preconfigured with some example database tables, eg\nIPython Notebook pointing to a directory containing example notebooks.\nA brief timeline of my journey is described in these blogposts:\nhttp://blog.ouseful.info/2013/12/02/packaging-software-for-distance-learners-vms-101/\nhttp://blog.ouseful.info/2014/05/15/confused-again-about-vm-ecology-i-blame-not-blogging/\nhttp://blog.ouseful.info/2014/12/10/thoroughly-confused-about-student-vms-docker/\nhttp://blog.ouseful.info/2015/01/14/using-docker-to-build-course-vms/\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/933#issuecomment-71808864.\n. @psychemedia third-party virtualized workloads would have to go through a hardening and security review before they could be introduced and deployed in trusted environments: that unfortunately is just a fact of life for exactly the reasons you describe: arbitrary code could be brought in thus breaching any trust. Interestingly enough, for the DoS security vector, VMs are a better choice as the hypervisor can exert some control over the VM's use of resources: container technologies are not as mature yet and really can take down a machine for other tenants. That implies that you can apply containers where there is a need for efficiency, and VMs where there is a need for control.\n\nIf you have a dockerfile that you can share, I can take a look and debug.\n. Wouldn't mind jumping in, as I want to learn the ins and out of OpenRefine as I want to use it for the Data Lake, a public data hub for public service analytics.\n. ",
    "noamoss": "Thanks,\nI just started to translate front-end without experimenting. I guess I will finish translating it within 2-3 weeks and than will be able to check for RTL issues.\n. more work to be done...Taking it step by step.\n. ",
    "daviddou82": "I think this encoding issue is still persistent.  I though I could fix it by changing the encoding to uft-8.   I can contain the issue with cell level transformation to replace the garbage characters.  Out of 400k rows there always appears about 150 rows with garbage character when i export to csv.  I can share the data if you would like to see it.  \nDavid\n. https://drive.google.com/drive/#folders/0BwLvaFRd3dxEekRCVDVUa01TNzg/0BwLvaFRd3dxES0FMSHl2RVZDNW8\nthe file is in here.  with the name concat6.csv\n. the pandas function is: df.drop_duplicates(['col1','col2','col3'])\nThis function correctly drops rows based on multiple columns.  \nI followed the open refine tutorial on this page:\nhttp://programminghistorian.org/lessons/cleaning-data-with-openrefine.html\nwith no success after tinkers with all the settings.\n. I have tried all the options.  only the first column is null.  There is no way of reading rows with nulls in the first column\n. IT looks like open refine does read the row.  But open refine does not assign a row index.  The row index is blank.  The behavior of these rows are strange.  Total record count at the top is incorrect as well.\n. sure. what is your email?\n. I would really like to see a fix where open refine assigns a index/ID for every row.  This problem have messed me up quite a few times.  I would like to row count number at the top be fixed.\n. okay I never saw that option before.. thanks for you help.\n. ",
    "patd0000": "I am having the same issues.  Is there any update on this bug?  Data seems to export correctly to Excel but there is the limit of 65536 rows.\n. Thanks Tom.  I wonder how anyone can process international data if the\ncharacter sets don't work.\n. ",
    "MattBlissett": "My -e has broken this, there's at least one error in the Bash script.  I'll look into it.\n. It is indeed all under the extension.\nDoes anyone speak Italian?  There are a few translations that need doing (or not, if this is going to need rewriting in a few months anyway).\n. Hi all,\n@thadguidry \u2014 I'm pleased to hear you were using the service on The Plant List!  Some researchers at @RBGKew and elsewhere were also finding it useful.\nI have a new employer; I'm now working for the Global Biodiversity Information Facility (@GBIF).  We're interested in work like this, but it will be several months before there's any time for me to work on OpenRefine or extensions.  By then, I guess the Knowledge Graph API will be released, and there won't be any point keeping MQL.  GBIF's new checklists should also be completed, which would be a good data source.\nThanks.\n. ",
    "martingraham": "I had a poke about and it looks like the POI library is generating massive amounts of xml wrapper objects - up to 4million Xobj$AttrXobj and 3 million Xobj$ElementXObj. Then having a poke around in the .xslx file (by changing the suffix to .zip) I find an example of one of the sheets and lo and behold every cell value is nested in 'v' and 'c' elements, so 3 million elements is actually about right, and the attributes will be about right too, there are multiple of those per cell :-(\nThe problem isn't OpenRefine - the POI library is faithfully turning every attribute and element in the xml files in the xslx into a separate object instance - whether that's an efficient use of space is another question but for the POI authors not Refine. It also explains why switching to .xls works\nMaybe the real wtf is xml :-(\nedit: I suppose one route is to investigate whether POI has an interface for allowing the picking and choosing of certain elements and attributes to be turned into objects - it seems the 's' attribute within cell elements is definitely needed (data type) by OpenRefin, but whether the 'r' (cell index) and 't' attributes are needed too I don't know.\n. I've got an anonymised version now, where would you want it put?\n. aargh, it's not letting me attach anything that isn't a picture file... can you reply to this so I get your email (I deleted the original message)\n. ",
    "ambodi": "Great, guys! \nCan you return the result in  a query too? Sorry if it is too basic, I am a newbie with refine.\n. ",
    "ignacio-chiazzo": "@tfmorris Is there a quick workaround without upgrading the poi?\n. Couldn't reproduce it. I will open the issue if I see it again.. Filling down the cells solved the issue\n. @tfmorris Is there a way to fill dow all the columns with one click? Instead of clicking in each columns and filling down?\n. ",
    "simon1tan": "Nevermind. I watched a video to figure out I had to choose beyond the item.\n. I'm new to OpenRefine as well although it has worked fairly well for me. See if you can remove the freebase extension...unless you use it.\n. Could be a memory issue? Have you increased the memory heap? https://github.com/OpenRefine/OpenRefine/wiki/FAQ:-Allocate-More-Memory\n. What version are you running? Mine(ver2.5 r2407) shows \"Locate one or more files on your computer to upload:\"\n. From my experience, OpenRefine defaults to showing 10 records per page, 50 max? It uses pagination to only get a page of 50 records at a time. So the browser doesn't really get hit with 400,000 records. If your browser is crashing, it maybe due to something else, perhaps try a different browser. All your records would be loaded into the OpenRefine application, the browser is just the interface to it. So if anything were to crash, it would be OpenRefine with so much data. It could just be sluggish if you don't allocate enough memory.\nHave you looked at the memory usage and settings? https://github.com/OpenRefine/OpenRefine/wiki/FAQ:-Allocate-More-Memory\nJust because your computer has 8GB memory does not mean your application is getting to use that. You have to set it up. Good luck.\n. The time needed to code this up verses the time it takes to export the file and then zip it manually seems really small. But if you want to do this, go for it. I can see it for reducing bandwidth and download times if that is a problem. I assume you are using this tool from an outside server...\n. You don't have to run anything from the command line. Mapping a drive letter or starting OpenRefine can all be done in the GUI.\n. I can't tell if this a bug or faulty logic in the code. You are just replacing values with \"\" or the name and reversing the array. Nowhere is there a sort. \n. ",
    "srugano": "I'm on OpenRefine 2.6 beta and the cell.cross(\"results_export\", \"Values\") is returning me an empty array [] . \n. Hi. Am I doing something wrong here  ?\n\n. > .toDate(\"EEE MMM dd h\u24c2\ufe0fs z yyyy\")\nEffectively this works. Maybe it was the cap HH ? Thanks everyone.. ",
    "asael2": "Great idea Nestor I'll be glad to help in that!\n. ",
    "nestorjal": "@magdmartin thanks, as soon as I check the documentation above I'll post in the mailing list my progress and probably asking for help, so @asael2 your offer is highly appreciated!\n. Yes! I'll submit tomorrow a pull request with a parcial translation, 85% or so done. \n. I'd like to precise which strings require translation, however I don't know how to put that in the JSON file, do you have any advice on that? I've a copy of the file with \"TODOs\" but in that way the JSON will be useless.\n. Thanks for the recomendation but \"Retraso de tiempo\" sounds weird, I think \"Tiempo de retraso\" is the appropriate term, since it's the amount of time to wait or \"retraso\" between requests. \n. Actually I'm learning how to use git, thanks for your advice! count me in if any update is necessary or new strings are created.\n. ",
    "avorio": "Ok, I managed to launch OpenRefine using the Mac terminal. Here's the output all the way to when the screen freezes and nothing happens.\n```\n./JavaAppLauncher\n17:40:16.064 [            refine_server] Starting Server bound to '127.0.0.1:3333' (0ms)\n17:40:16.073 [            refine_server] Initializing context: '/' from '/Applications/OpenRefine.app/Contents/Resource/webapp' (9ms)\n17:40:16.420 [                   refine] Starting OpenRefine 2.6-beta.1 [TRUNK]... (347ms)\n17:40:17.853 [                   refine] Sorry, some error prevented us from launching the browser for you.\nPoint your browser to http://127.0.0.1:3333/ to start using Refine. (1433ms)\n17:40:30.295 [                   refine] POST /command/core/load-language (12442ms)\n17:40:30.387 [                   refine] POST /command/core/load-language (92ms)\n17:40:30.571 [                   refine] POST /command/core/get-importing-configuration (184ms)\n17:40:30.586 [                   refine] GET /command/core/get-all-project-metadata (15ms)\n17:40:30.619 [                   refine] GET /command/core/get-version (33ms)\n17:40:41.067 [                   refine] POST /command/core/create-importing-job (10448ms)\n17:40:41.088 [                   refine] POST /command/core/importing-controller (21ms)\n17:40:42.083 [                   refine] POST /command/core/get-importing-job-status (995ms)\n17:40:42.116 [                   refine] POST /command/core/importing-controller (33ms)\n17:40:42.172 [                   refine] POST /command/core/importing-controller (56ms)\n17:40:42.205 [                   refine] POST /command/core/get-models (33ms)\n17:40:42.221 [                   refine] POST /command/core/get-rows (16ms)\n17:40:53.640 [                   refine] POST /command/core/importing-controller (11419ms)\n17:40:53.649 [                   refine] POST /command/core/get-models (9ms)\n17:40:53.666 [                   refine] POST /command/core/get-rows (17ms)\n17:41:02.757 [                   refine] POST /command/core/importing-controller (9091ms)\n17:41:03.767 [                   refine] POST /command/core/get-importing-job-status (1010ms)\n17:41:03.779 [                   refine] POST /command/core/cancel-importing-job (12ms)\n17:41:04.458 [                   refine] POST /command/core/load-language (679ms)\n17:41:04.484 [                   refine] GET /command/core/get-preference (26ms)\n17:41:04.490 [                   refine] GET /command/core/get-preference (6ms)\n17:41:04.495 [                   refine] POST /command/core/load-language (5ms)\n17:41:04.504 [                   refine] POST /command/core/load-language (9ms)\n17:41:04.546 [                   refine] GET /command/core/get-project-metadata (42ms)\n17:41:04.590 [                   refine] GET /command/core/get-models (44ms)\n17:41:04.757 [                   refine] GET /command/core/get-history (167ms)\n17:41:04.769 [                   refine] POST /command/core/get-rows (12ms)\n17:41:04.773 [                   refine] GET /command/core/get-history (4ms)\n```\n. Hi @jackyq2015, thank you for your answer. I appreciate your request \u2014as I would love to share the dataset\u2014 but unfortunately I don't have the permission to do so at this point. Is there any other way you can help me?\n. ",
    "marcobra": "Obviously i use 2.6 now, but please put some note on the download page, people can't guess this issue, i spent 3 hours on 2.5 a various trying to get openrefine working...\nSo the main problem here is the lack of info on the main page... the instructions are:\nhttp://openrefine.org/download.html\nLinux kit, Download, extract, then type ./refine to start.\nAnd also on github there wasn't any clear java related opened issue... so please don't close/hide this\nThanks\n. ",
    "jansalm": "Sorry Tom\nI am very new to this.\nJan\nFrom: Tom Morris [mailto:notifications@github.com]\nSent: Wednesday, 8 April 2015 10:00 AM\nTo: OpenRefine/OpenRefine\nCc: Jan Salmon\nSubject: Re: [OpenRefine] Clustering (#966)\nHi Jan. The email list / Google Group is the best place for questions which aren't actual bug reports.\nThe readers will probably want to see examples of the entries that you thought should have been clustered, but which weren't, along with the parameters that you used for the clustering.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/966#issuecomment-90768028.\n. ",
    "rodalpho": "I actually came here to post about this exact same issue. The Openrefine backend performs great with a ton of options inside a facet, but my browser doesn't-- it locks up for 4-5 minutes every time I refresh the data. This is using 64bit Chrome; firefox is even worse.\nAs I see it, the problem is most likely that Openrefine is trying to display all 90,000 options inside that little table on the left side of the page, and browsers aren't equipped to handle that. One fix would be to only show the first \"X\" options by default, where X should probably be around 25. Another would be to implement paging, similarly to the data paging in the right-side panel. \n. ",
    "akuckartz": ":+1: \n. Please remove the \u2018wontfix\u2018 label.. Is this the best source code to use as a foundation of a replacement of Deri's RDF extension: https://github.com/sparkica/rdf-extension ?\n. ",
    "techtonik": "Done. https://groups.google.com/forum/?fromgroups#!topic/openrefine/UnPRxgk84G4\n. ",
    "amsardesai": "Hi, it seems the option is not getting set properly on Windows. #971 may fix the problem, it enables the max form size option on Windows.\n. ",
    "lispc": "@baditaflorin.  Thanks for the link. I have already read the code on clustering in OpenRefine / Simile-Vicino / SecondString all. My algorithm is particularly optimized for edit-distance clustering, so my target is not to replace most of the existed algorithms  but  only replace the edit-distance module. \n@tfmorris The new algorithm does not use blocking at all. Here is a brief comparison between new lib vs simle-vicino (There seems a bug in simile-vicino , which is mentioned in the gist) : https://gist.github.com/lispc/d4ee8de81e7bfa6bc352. I will provide a git repo to reproduce experiment results and start the fork-pull-request steps. Thanks.\n. @baditaflorin I had a look at the dataset. I know there are two methods used in OpenRefine for clustering : binning and knn. This dataset is very suitable for testing binning method, but it is too large for knn method and I calculated that the dataset would cost several days using the simile-vicino clustering lib. So may I guess that you had not tested the dataset using current knn method (namely, simile-vicino ) in OpenRefine? Even our algorithm may take several hours. So, do you have any smaller dataset ? Or would you please give me some advice to generate smaller dataset that can simulate how OpenRefine is usually used? Thanks.\n. @tfmorris  Thanks. I am improving the multicore version of our lib. It may take a bit longer before I can issue a pull request.\n. ",
    "li-guoliang": "Hi @baditaflorin and @tfmorris, I am the supervisor of @lispc. I have studied data cleaning for several years and I would like to contribute our algorithms to OpenRefine. Our algorithm is super fast and in the completion organized by a premier international conference EDBT'13, our algorithm beats all of other algorithms by an order of magnitude (see http://www2.informatik.hu-berlin.de/~wandelt/searchjoincompetition2013/Results.html). \n@baditaflorin I am very interested in your trial dataset and I want to test our algorithm on it. Could you share a copy for us. Thank you.\n. @baditaflorin  I have downloaded the dataset. We will test it and let you know the result later. Is it possible to have a copy of your 46M data?\nThanks. \n. ",
    "jandiolola": "My spreadsheets on my Google Drive account\n. ",
    "rdoursenaud": "Should have stated that this is using OpenRefine 2.6-beta.1 on Arch Linux.\n. ",
    "karawoo": "I've run into this problem as well on Windows 8. It would be great if OpenRefine could run from a network share. Due to a set of highly annoying factors I don't have privileges to move OpenRefine to the local hard drive, and I am teaching students who are not familiar with command line interfaces and would be scared off by having to run OpenRefine from the command prompt.\n. ",
    "jezcope": "It looks like this has never been resolved \u2014 I've just helped out a colleague who ran into the same problem. I don't even know why the UNC path doesn't work because I thought all the file access primitives in Windows worked with either that or a Local File System path.\nWRT to @simon1tan's suggestion, sadly if you start OpenRefine by double-clicking openrefine.bat, Windows reports the UNC path instead of the (mapped) LFS path to the process as the working directory, which is why the error happens in the first place.. ",
    "kennedyoliveira": "any news on this one? . @thadguidry i had done it already and looks like it's working fine up to now, but would be good to be on the project it self because some things work on 2.7 but not on 2.5, like json.\nHow would i parse json on Jython 2.5.x? Using the 2.7 i can simple do like in python, import json, and use it.\nThanks in advance.. @thadguidry ok, thank you. ",
    "Vanuan": "\nWhat were you attempting to do when you got the error?\n\nJust start it from the console.\n\nCheck if you have the file in place under:\nOpenRefine\\extensions\\jython\\module\\MOD-INF\\lib\\jython-standalone-2.5.3.jar\n\nIs it the same path for Linux?\n. Found under ./webapp/extensions/jython/module/MOD-INF/lib/jython-standalone-2.5.3.jar\n. Additional trace:\nCaused by: java.lang.ClassNotFoundException: org.python.core.PyException\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n    at edu.mit.simile.butterfly.ButterflyClassLoader.loadClass(ButterflyClassLoader.java:65)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n. Which version of Java should I use?\n$ java -version\njava version \"1.7.0_51\"\nJava(TM) SE Runtime Environment (build 1.7.0_51-b13)\nJava HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode)\n\nAre you using the refine shell script to start things?\n\nYes.\n. This is the command executed:\n/usr/bin/java -cp server/classes:server/lib/* -Xms256M -Xmx1400M\n-Drefine.memory=1400M -Drefine.max_form_content_size=1048576\n-Drefine.verbosity=info -Dpython.path=main/webapp/WEB-INF/lib/jython\n-Dpython.cachedir=/home/john/.local/share/google/refine/cachedir\n-Drefine.webapp=main/webapp\n-Drefine.port=3333 -Drefine.host=127.0.0.1 com.google.refine.Refine\n. Seems like prepackaged jython-standalone-2.5.3.jar indeed lacks the class above.\nAfter downloading jython-standalone-2.5.3.jar from official repo I have this error:\n01:13:24.071 [         butterfly.module] Error initializing module freebase by script function init() (1058ms)\norg.mozilla.javascript.EcmaError: ReferenceError: \"ClientSideResourceManager\" is not defined. (file:../openrefine-2.6-rc1/webapp/extensions/freebase/module/MOD-INF/controller.js#88)\n    at org.mozilla.javascript.ScriptRuntime.constructError(ScriptRuntime.java:3654)\n    at org.mozilla.javascript.ScriptRuntime.constructError(ScriptRuntime.java:3632)\n    at org.mozilla.javascript.ScriptRuntime.notFoundError(ScriptRuntime.java:3717)\n    at org.mozilla.javascript.ScriptRuntime.name(ScriptRuntime.java:1692)\n    at org.mozilla.javascript.gen.c4._c1(file:../openrefine-2.6-rc1/webapp/extensions/freebase/module/MOD-INF/controller.js:88)\n    at org.mozilla.javascript.gen.c4.call(file:../openrefine-2.6-rc1/webapp/extensions/freebase/module/MOD-INF/controller.js)\n. Strange. Downloading fresh copy helped. Can't reproduce this anymore.\n. ",
    "mtelesha": "mrScot your work around is that you can have Google Drive sync with your computer into a driver folder.  https://support.google.com/drive/answer/2375083?hl=en\n. ",
    "esiegerman": "@jackyq2015 The thing is that invert is a toggle -- and it remembers its state even if nothing is selected.  Try this:\n1. create a new facet\n2. select one or more choices; they highlight in red\n3. click invert; they turn into struck-through black\n4. click reset\n5. click a choice; it comes up already inverted\nIt's that memory that makes invert different from reset:  clicking reset with nothing selected[1] would be a no-op; clicking invert with nothing selected would still be a meaningful operation, as it would affect what will happen the next time you select something.\n@magdmartin Agreed that the inversion state doesn't need any more graphic treatment; keeping the button visible (and clickable) would be enough.\n[1] assuming the reset button itself stayed visible -- which I'm not asking for btw\n. @jackyq2015 Woops, my description was incorrect.  Step (4) should read:\n4. Deselect all of the selected choices\nreset flips the facet back to include mode, but if you deselect all the choices yourself, it stays in exclude mode.\n(I was writing from memory, and hadn't noticed that the behaviours are different.)\n. Multiple facet panels should definitely be optional -- sorry I didn't make that clear.  Another option would be to keep it as a single panel, but let one:\n- change its width\n- drag the little facet windows into multiple columns within it\nThat said, popup facets would be awesome -- I am indeed using a dual-monitor setup.  Excellent idea!\n. I think I agree with this answer in that thread -- as far as long-lived code is concerned.  But the little scripts one writes in OpenRefine are throwaway -- run once and forget (modulo the history feature of course :-) )\nFor those, the focus should be different: ease of typing (and reading in a small space, i.e. the facet's window) seems to me more important than long-term maintainability.\nBesides, GREL makes \".\" notation available in this context, so whatever the theoretical arguments against that, OpenRefine is already ignoring them.  It would just be nice if it ignored them consistently :-)\n. ",
    "JJIguiniz": "colleague figure it out!!!! edited the refine.bat file as it follows....\nREFINE_PORT=50\nREFINE_HOST=127.0.0.1\nREFINE_WEBAPP=main\\webapp\nREFINE_MEMORY=1024M\nSome sample configurations. These have no defaults.\nANT_HOME=C:\\grefine\\tools\\apache-ant-1.8.1\nJAVA_HOME=C:\\Program Files\\Java\\jre6\nJAVA_OPTIONS=-XX:+UseParallelGC -verbose:gc -Drefine.headless=true\nRan it from refine.bat\n. ",
    "Archaeoknitter": "In Windows Firefox isn't registering the export file as a download.\nIn Windows Chrome a tab is left open after the download completes\n. ",
    "ScientificProgressGoesBoing": "Tried this advice but the program did not work anymore. Would be grateful for another quick fix because the error message keeps popping up and changing tabs which is annoying. Thank you for your work! I appreciate the development effort put into this program!\n. ",
    "dbl001": "The advised update worked for me.  I\u2019m running OS  10.10.4 Yosemite.\nDavid-Laxers-MacBook-Pro:OpenRefine davidlaxer$ ./refine \nStarting OpenRefine at 'http://127.0.0.1:3333/'\n20:19:17.312 [            refine_server] Starting Server bound to '127.0.0.1:3333' (0ms)\n20:19:17.313 [            refine_server] refine.memory size: 1400M JVM Max heap: 1304952832 (1ms)\n20:19:17.333 [            refine_server] Initializing context: '/' from '/Users/davidlaxer/OpenRefine/main/webapp' (20ms)\n20:19:17.428 [            refine_server] Starting autoreloading scanner...  (95ms)\n20:19:18.311 [                   refine] Starting OpenRefine 2.6 [TRUNK]... (883ms)\n20:19:25.589 [                   refine] POST /command/core/load-language (7278ms)\n20:19:25.736 [                   refine] POST /command/core/load-language (147ms)\n20:19:25.827 [                   refine] POST /command/core/get-importing-configuration (91ms)\n20:19:25.858 [                   refine] GET /command/core/get-all-project-metadata (31ms)\n20:19:25.904 [                   refine] GET /command/core/get-languages (46ms)\n20:19:25.933 [                   refine] GET /command/core/get-version (29ms)\n09:20:00.030 [                   refine] POST /command/core/load-language (46834097ms)\n09:20:00.062 [                   refine] GET /command/core/get-preference (32ms)\n09:20:00.068 [                   refine] GET /command/core/get-preference (6ms)\n09:20:00.073 [                   refine] POST /command/core/load-language (5ms)\n09:20:00.099 [                   refine] GET /command/core/get-project-metadata (26ms)\n09:20:00.264 [                  project] Loaded project 2231895062070 from disk in 0 sec(s) (165ms)\n09:20:00.299 [                   refine] GET /command/core/get-models (35ms)\n09:20:00.392 [                   refine] GET /command/core/get-history (93ms)\n09:20:00.435 [                   refine] POST /command/core/get-rows (43ms)\n09:20:00.580 [                   refine] GET /command/core/get-history (145ms)\n09:20:07.455 [                   refine] GET /command/core/get-history (6875ms)\n09:20:07.470 [                   refine] POST /command/core/get-rows (15ms)\n09:20:07.617 [                   refine] POST /command/core/compute-facets (147ms)\n09:22:39.258 [                   refine] GET /command/core/get-starred-expressions (151641ms)\n09:22:39.259 [                   refine] GET /command/core/get-expression-history (1ms)\n09:22:39.261 [                   refine] GET /command/core/get-expression-language-info (2ms)\n09:22:39.289 [                   refine] POST /command/core/preview-expression (28ms)\n09:23:10.473 [                   refine] POST /command/core/preview-expression (31184ms)\n09:23:11.007 [                   refine] POST /command/core/preview-expression (534ms)\n09:23:12.846 [                   refine] POST /command/core/preview-expression (1839ms)\n09:23:15.015 [                   refine] POST /command/core/log-expression (2169ms)\n09:23:15.019 [                   refine] POST /command/core/text-transform (4ms)\n09:23:15.091 [                   refine] GET /command/core/get-history (72ms)\n09:23:15.136 [                   refine] POST /command/core/get-rows (45ms)\n09:23:15.256 [                   refine] POST /command/core/compute-facets (120ms)\n09:23:15.263 [                   refine] POST /command/core/preview-expression (7ms)\n09:23:29.057 [                   refine] GET /command/core/get-history (13794ms)\n09:23:29.076 [                   refine] POST /command/core/get-rows (19ms)\n09:23:29.203 [                   refine] POST /command/core/compute-facets (127ms)\n09:24:19.157 [           ProjectManager] Saving some modified projects ... (49954ms)\n09:24:19.202 [        project_utilities] Saved project '2231895062070' (45ms)\n\nOn Jul 27, 2015, at 11:13 AM, QI notifications@github.com wrote:\nWhat error did you see when you say it is not working? Can you also provide\nthe version of the openrefine and how you run it\nOn Mon, Jul 27, 2015, 03:44 ScientificProgressGoesBoing \nnotifications@github.com wrote:\n\nTried did advice but program did not work anymore. Would be grateful for\nanother quick fix because the error message keeps popping up and changing\ntabs which is annoying. Thank you for your work! I appreciate the\ndevelopment effort put into this program!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1043#issuecomment-125112287\n.\n\u2014\nReply to this email directly or view it on GitHub https://github.com/OpenRefine/OpenRefine/issues/1043#issuecomment-125292624.\n. \n\n",
    "D1no": "odd add-on:\nIf you include in your calculation a meaningless decimal-based devision, than it keeps evaluation to decimals. I guess I am going to add (0.5/1*2) * for a while now...\n\n. Apologies. I did a prior google search before; checked the GREL math operations and did not find any notable behaviour for devisions, float and automatic typing.\nMaybe this could be added, as it is certainly an unexpected ux - as it seems to have popped up before\n. ",
    "nickynicolson": "512 describes storing file source as a column in the data (and a problem whereby this is not populated for JSON from URL). I'd like the file source(s) to be stored as project metadata (not data columns) and output when the project is saved in Open Refine project format, in the metadata.json file.\n. ",
    "joewiz": "Thanks, @jackyq2015! I'm not a java developer, but looking at the source of some of your recent commits on the table exporter, would the fix be on https://github.com/RefinePro/OpenRefine/blob/issue-1015/main/src/com/google/refine/exporters/HtmlTableExporter.java#L108, to the effect of:\nwriter.write(StringEscapeUtils.escapeXml(cellData.text));\nHere I'm using https://commons.apache.org/proper/commons-lang/javadocs/api-3.1/org/apache/commons/lang3/StringEscapeUtils.html#ESCAPE_XML.\nI suggest the XML escaping to limit escaping to just a few characters (as opposed to escaping the full range of HTML entities, which just make these characters more human editable).\nMy only worry is whether the HTML import would similarly need to unescape &amp;?\n. Okay, great! In the meantime I've switched to TSV export/import, so I'm handling the escaping/unescaping outside the application. Thanks for your help!\n. The issue isn't limited to curly/smart quotes. I'm getting it with other characters too.  Here are some example clusters and the character that triggers the problem:\n\u00e1\n- Montagne S\u00e1nchez, Ernesto\n- Sanchez, Ernesto Montagne\n\u00e7\n- Mitterrand, Francois\n- Mitterrand, Fran\u00e7ois\n. Yes, this is the same issue, though I provided some more detail than #1056.  The most concerning aspect of this is that the behavior can result in merges even without the checkbox checked.  In other words, this can lead to merges even when the user has intentionally said \"no\" by unchecking a box.  Less concerning but equally a bug is when the user intends the merge, but the merge isn't made - even though the checkbox is checked..\n. Many thanks again, @jackyq2015 and all!\n. @Opennat Thanks for what you suggested. I agree that this often happened when I had 2 or more filters/facets active. (And sorry I didn't spot your reply sooner.) Were you able to find a reproducible case? It's been a while since I was working intensively with OR, but if you had anything more to add, esp. with 2.6RC2, that would be worth keeping the issue open for.\n. Thanks for correcting my misunderstanding about JSON arrays being ordered.  That insight also helped me track down the cause of the misordering in my reconciliation service.\n. Wonderful! Thank you, @jackyq2015! \n. Appending ?w=1 to the URL of the diff view forces an ignore whitespace diff:\nhttps://github.com/OpenRefine/OpenRefine/pull/1074/files?w=1\nSee https://github.com/blog/967-github-secrets\n. +1!\n. Just spotted this - nice work! Is there a facility here for re-importing the results of a decision set, or does the PR just export the clusters from OR? (I'm not a reviewer - just an interested user.)\n. @corajr @kgrons Congrats! . This reminds me that I saw this too on all XML files when I first began using OR.  I decided to pre-process my XML because I needed to move ahead on the project, so I never reported the bug.  This would be a nice one to fix.  If additional files are needed to reproduce the bug I could dig some up.\n. @jackyq2015 Sorry, what I meant was that I converted the XML to TSV outside of OR before importing into it. I used XQuery - using a script like this one: https://gist.github.com/joewiz/48ce061423aa7d3ada28. \n. Can you provide a specific example?\n. Thanks @jackyq2015! \n. Any luck with Chrome? I've seen other users report problems with Firefox here. \n. Excellent work, @wetneb - looking forward to taking this for a spin. I would be grateful for any tutorial or screencast illustrating how the new feature works.\nTo @thadguidry and other core developers, I'm just curious, since this plugs a big hole left by Freebase, would this open up an opportunity to do a new RC and move toward a formal release of 2.6? . @thadguidry @tfmorris @jackyq2015 Very exciting!. @Archaeo-Programmer Could you try this command?\nbrew tap caskroom/versions\n\nThen do the install again:\nbrew cask install openrefine-dev\n\nPlease let me know if you have any success.. @Archaeo-Programmer Thanks for the report! I've updated the homebrew-based installation instructions with this additional step. I think I had already performed that step on my own system to access other version-specific casks, so I neglected to include it in the steps that first-time users would need to run. Thanks again!. Yes, Caskroom only puts an app's current stable release in the primary caskroom tap. Pre-releases, betas, and version-specific builds go into the versions tap. OpenRefine currently fits into the latter, and as soon as a final release is issued we will submit a PR for a new openrefine cask in the core caskroom tap.. Is the title of this issue a typo - perhaps autocorrect? :-). ",
    "a-blohm": "Sure. I can't attach it to this comment, but you can download the files from http://alexanderblohm.de/jnkdnfkjndfgn3iuu4u3sdf/json.zip. Will be available for at least a month or so.\n. As far as I remember, there was some kind of Exception after some time. \n. ",
    "niconoe": "To complete this ticket, it seems the solution proposed at https://groups.google.com/forum/#!topicsearchin/openrefine/listen/openrefine/km25PNwn-_Q (adding a key to Info.plist) just works fine for me.\nMaybe we should just explain that in the documentation?\n. Done, thanks!\n. ",
    "cameronstewart": "Has there been any further movement on this?. Seeing same issue with same conditions - did you work it out?. Thanks Thad, I changed the System Variables but it didn't seem to make a\ndifference.  I'll try another box.\nOn Fri, Mar 31, 2017 at 1:13 AM, Thad Guidry notifications@github.com\nwrote:\n\nThis isn't a OpenRefine issue, but a local environment issue. The proof is\nin our code here:\nhttps://github.com/OpenRefine/OpenRefine/blob/master/main/\nsrc/com/google/refine/importing/ImportingJob.java#L70\nBut to help you out, Check your TEMP and TMP environment variables for\npossible corruption, or just simply change them to another folder path like\nE:\\TEMP or whatever you want.\nOn my Windows 7 spare laptop, I have my User Environment Variables for\nTEMP and TMP pointing to\n%USERPROFILE%\\AppData\\Local\\Temp\n[image: tempdiroveride]\nhttps://cloud.githubusercontent.com/assets/986438/24508349/bdf4e910-1528-11e7-9282-51b7cea8b9eb.PNG\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1142#issuecomment-290423339,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAZibRCG1-09weWL5JO7wslCsBS9s7v4ks5rq7iJgaJpZM4JNdNI\n.\n. Did you select the file or paste the whole path? The raw data file should be staged under raw-data folder. .  So how would you fix this considering everything will be somewhere on a\ndrive so a drive: will be replicated Irrespective of where it is?\n\nOn Sat, 22 Apr 2017 at 6:20 am, jls4dtna notifications@github.com wrote:\n\nI have this same issue, and it has nothing to do with the value of TMP or\nTEMP. Using the Browse button, I selected the file Librarygood.xml, which\nwas in my Windows 7 home directory C:\\Users\\STAMPEL. When I tried to load\nit, I got this error, much like the others received:\n[image: image]\nhttps://cloud.githubusercontent.com/assets/27868729/25293817/456ad426-2691-11e7-8f60-8bff136d2a79.png\nIt seems clear that OpenRefine was trying to copy the xml file in its own\nprivate temp directory\n'C:\\Users\\STAMPEL\\AppData\\Local\\Temp\\Jetty_127_0_0_1_3333_webapp____4ulpc9\\import\\2\\raw-data',\nreplicating the original file's path - i.e., C:\\Users\\STAMPEL.\nThe problem is that the colon character is not allowed in Windows file\nnames. The colon needs to be removed or replaced with a legal character.\nThat should fix the issue.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1142#issuecomment-296296306,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAZibTkUSEyRaE4L3XHsAxVnLa0Nf0MDks5ryQ99gaJpZM4JNdNI\n.\n. Makes sense but how do you modify it?  The error just pops up in a dialogue\nbox.\nOn Tue, 25 Apr 2017 at 1:16 am, jls4dtna notifications@github.com wrote:\nYou can simply retain the drive letter but remove the colon.\ne.g. C:\\foo\\bat\\bar\\data.xml is mapped to\n...\\raw-data\\C\\foo\\bat\\bar\\data.xml\nFrom: cameronstewart [mailto:notifications@github.com]\nSent: Saturday, April 22, 2017 11:04 PM\nTo: OpenRefine/OpenRefine\nCc: Stamper, Larry (164-Extern-Larry); Comment\nSubject: Re: [OpenRefine/OpenRefine] error uploading data from csv,\nperhaps due to colon included in temp filename (#1142)\nSo how would you fix this considering everything will be somewhere on a\ndrive so a drive: will be replicated Irrespective of where it is?\nOn Sat, 22 Apr 2017 at 6:20 am, jls4dtna notifications@github.com\nwrote:\n\nI have this same issue, and it has nothing to do with the value of TMP\nor\nTEMP. Using the Browse button, I selected the file Librarygood.xml,\nwhich\nwas in my Windows 7 home directory C:\\Users\\STAMPEL. When I tried to\nload\nit, I got this error, much like the others received:\n[image: image]\n<\nhttps://cloud.githubusercontent.com/assets/27868729/25293817/456ad426-2691-11e7-8f60-8bff136d2a79.png>\nIt seems clear that OpenRefine was trying to copy the xml file in its\nown\nprivate temp directory\n'C:\\Users\\STAMPEL\\AppData\\Local\\Temp\\Jetty_127_0_0_1_3333_webapp____4ulpc9\\import\\2\\raw-data',\nreplicating the original file's path - i.e., C:\\Users\\STAMPEL.\nThe problem is that the colon character is not allowed in Windows file\nnames. The colon needs to be removed or replaced with a legal character.\nThat should fix the issue.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\n<\nhttps://github.com/OpenRefine/OpenRefine/issues/1142#issuecomment-296296306>,\nor mute the thread\n<\nhttps://github.com/notifications/unsubscribe-auth/AAZibTkUSEyRaE4L3XHsAxVnLa0Nf0MDks5ryQ99gaJpZM4JNdNI>\n.\n\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub<\nhttps://github.com/OpenRefine/OpenRefine/issues/1142#issuecomment-296421999>,\nor mute the thread<\nhttps://github.com/notifications/unsubscribe-auth/Aak-OcO0NMks1pRQozVBYswBmLishTozks5ryunngaJpZM4JNdNI>.\nIf you are not the addressee, please inform us immediately that you have\nreceived this e-mail by mistake, and delete it. We thank you for your\nsupport.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1142#issuecomment-296701816,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAZibV2Jg1-4mqfqfLSksbJQO2Nr_ABLks5rzLzWgaJpZM4JNdNI\n.\n. Great - didn't even think of that...just post http://127.0.0.1:3333/ into\nChrome address bar!\n[image: Inline image 1]\n\nWorks as expected.  thankyou for the idea!\nCameron\nOn Wed, Apr 26, 2017 at 12:56 PM, Jacky notifications@github.com wrote:\n\nSeems there are some inconsistency between chrome and IE for the file\nselector component. I can put a patch on either front end or back end to\nextract the file name only. But MS is moving to Edge from IE, not sure it\nis still worth fixing it.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1142#issuecomment-297223326,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAZibf24Qu5ZezDi8ynXtlzFb1I2jQjFks5rzrJSgaJpZM4JNdNI\n.\n. \n",
    "FernandoInes": "Solved!!! \nJust replace this line at the file refine.ini:\n       #JAVA_HOME=C:\\Program Files\\Java\\jdk1.6.0_25\nwith the correct java home directory of my computer\n       #JAVA_HOME=C:\\Program Files\\Java\\jre6\nRegards!\n. ",
    "wolxXx": "same here...\n. ",
    "rudygt": ":), I'm glad to help. \n. @nachomezzadra done!\n. I speak spanish as my primary language, but I'm a technical person, maybe we need more feedback XD, I will review and update the pr, \nthanks!\n. Do you have more context about \"QA Facets\", it is intended to stand \"to check the quality of the facets\" or something else, if you can help me with a long english explanation I can find the proper spanish form. Thanks!\n. I believe that we can use \n\"QA Facets\" --> \"Verificar la calidad de las facetas\"   --> \"verify the quality of the facets\n\"QA Results\" --> \"Verificar la calidad de los resultados\" --> \"verify the quality of the results\"\nWhat do you think? @magdmartin \n. \"Aplicar Cambios\" may be another valid option\n. ",
    "pprasqui": "My bad... Because of the example in the reverse function description \"For example, reverse([ 0, 1, 2, 3]) returns the array [ 3, 2, 1, 0 ]\", I somehow inferred that reverse was an inverted sort.\n. 2.6 beta 1 on Win 10\n. ",
    "mgalushka": "(y)\n. ",
    "stundzig": "Hi,\nyou are right, i couldn't upload the file. Hmm. It's the CSV file from https://www.govdata.de/suchen/-/details/stala-sn-service-1679024015 a german open data website.\n. With polymap4 we create a geo location plattform. There you could e.g. upload CSV files. And i use OpenRefine for the parsing of the CSV content. Therefore i've written a server side implementation class, which encapsulates all the refine methods, see https://github.com/stundzig/polymap4-core/blob/develop-rap2.3/plugins/org.polymap.core.data.refine/src/org/polymap/core/data/refine/impl/RefineServiceImpl.java\n. Hi Tom, thanks for your thoughts. I think, increasing the threshold would have only a minimal influence to the other code. And in my file i have a value of 0.1010101, but the check was only <0.1. \n. ",
    "dbolser-ebi": "Thanks Thad,\nThe issue I'm having is that the list can be 1 to n terms, and I want to trim them all... Actually the solution in this case is to split on ' ,' rather than ','.\nI'm guessing it isn't possible to run a function over a list in GREL?\nCheers,\n. Thanks thad,\nLooking here:\nhttps://github.com/OpenRefine/OpenRefine/wiki/GREL-Controls#filterexpression-a-variable-v-expression-test\nthis solution was pretty obvious:\nforEach(split(value, ','), v, trim(v))\nPersonally, I rather just try the answer than work it out for myself ;-)\nThanks for help,\nDan.\n. \u200bMeh ;-P\n. ",
    "danpaulsmith": "Hi Thad,\nI think the issue is more wide-spread that the parsing screen. The Open project page is blank, and buttons and dropdowns on the main page are missing text:\n\n\n\nFirefox 41, OSX 10.10.5\n. I have Version 2.6-rc1 installed on my personal laptop, and it's fine, so it might be my work laptop. I'll check tomorrow.\n. ",
    "kafran": "Same problem here on 2.6-rc2, the openproject page returns \"No existing project. Select 'Create Project' on the left to create a new project.\" but there are projects. Running on Firefox 43.0.4; not tried others.\n. @tfmorris https://github.com/OpenRefine/OpenRefine/issues/1120\n. @magdmartin no, I haven\u2019t tried it with previous version. Its my first time using Open Refine. On Firefox console I get this (which doesn't appear on Chromium):\n\n\n. Problem solved. When I compared the network traffic I saw a lot of things cached on Firefox. I erase then the browser cache and the existing projects showed up in \"Open Project's\" tab.\nSorry for the trouble guys. And thanks for the help.\n@tfmorris \n. ",
    "FigaCZ": "Unfortunatelly, it doesn't show any error message. It crash after few seconds. I can say that RC1 runs great on this OS X build.\n. ",
    "charlie7691": "Thank you so much for this prompt and useful reply. I used facets, as you suggested, and have achieved exactly the results I was hoping for! Awesome stuff - thanks!\n. Thanks! At the moment I am trying to clean massive columns of employer names and job titles, but am limited by the cluster sizes. Looks like 7500 is about the most I can merge at once.\nI did try using Levenshtein distance to normalise them but even with 32 cores and 60GB ram it ran too slow!!\nWould you happen to be able to offer any hints on good approaches to this sort of problem?!\nMany thanks, Charlie\n. ",
    "corajr": "Certainly! @kgrons might be able to say more, as she was hoping to use this at her day job, but I'll try to explain.\nWe wanted to address a situation in which merging clusters and deciding on the canonical representation of a datum requires input from stakeholders beyond just the person using OpenRefine. Higher-ups who would like to be involved in the process would be unlikely to install OpenRefine on their own machines and learn how to use it just for this purpose.\nOur use case actually requires two components: the export functionality in OpenRefine, and a separate tool that could consume this JSON and display it in a separate interface. To this end, we also built a proof-of-concept tool that can work with the data. (Such a tool is certainly out of scope for the main OpenRefine project, but it can't work at all without some way to get the clusters out of OpenRefine.)\n. Thanks for the movement on this! I've tried to update all the translations to ensure that none are blank. Can you please let me know if the text still isn't showing up for you?. ",
    "kgrons": "Thanks for the explanation @corajr - as mentioned, this is our primary use case. The fundamental use case underlying a cluster export feature is the ability to output clustered data with relationships outside of OpenRefine in a portable format, so that people can build whatever they need - for us, a prototype vocabulary selection tool and some basic data viz with freely available tools. \nThe high value of the clusters as decision-making tools is echoed in Enhancement Request 344, which requests a flag on a cluster so that users can \"export the clustered records out of refine to perform some manipulation on a external database\".\nWe believe the cluster export is within the current scope of OpenRefine as a new option that enables more data normalization by not limiting decision-making activities to the tool itself. \n. re: @joewiz - this PR is only to add the cluster export function/button.\nWe hope adding this feature encourages additional PRs to import & merge data from external processes. cc @corajr . ",
    "danbri": "Yes please :) /cc @rtroncy who also suggested this recently\n. The Data Package spec is fine enough for what it is, but it has no interest/capability for mapping from CSVs into entity/relationship graphs (RDF-style, Freebase/KG, schema.org etc etc.). My advice is that you'll find a bigger win with the W3C spec (but I was co-chair, so I have a natural preference). \nRegarding \"in attendance\", unfortunately Rufus had many other calls on his time and did not habitually attend the CSVW WG calls throughout the life of the WG. It became clear very late in the process that many of the points of divergence from Data Package were not appealing to Rufus, and he filed a set of comments that the WG felt they largely couldn't adapt to. There isn't a lot of \"behind the scenes\" here from my p.o.v. - just editorial suggestions that arrived too late in the process - see https://lists.w3.org/Archives/Public/public-csv-wg/2015Aug/0003.html .  That's life, getting complete agreement is hard - I hope there are no hard feelings on either side.\nAnd yes, @gkellogg has done an amazing job with tests - see https://github.com/w3c/csvw/tree/gh-pages/tests\n. See https://github.com/w3c/csvw/blob/gh-pages/experiments/historical-weather-observation-dataset/README.md and nearby for CSVW and Data Cube experiments from @6a6d74\n. Anyone still investigating this?. Thanks. I was thinking that the rdf mappings part of CSVW -- i.e. https://www.w3.org/TR/2015/REC-csv2rdf-20151217/#example-events-listing -- might be interesting if people are using OpenRefine to bring data into Wikipedia, given that Wikipedia's datamodel is a variant on RDFish graphs.. Yeah, I think so. There's a horribly underdocumented js experiment here btw - it reads some CSVW metadata and the .csv itself, then injects the results into the HTML document. At Google we can even consume that. Demo uses schema.org but it could equally be Wikidata properties and types. @thadguidry and co have also been working on mappings between the two. Would be lovely to have an OpenRefine story around such things...\n. https://youtu.be/MsuXqf9wog0. +1 for https://github.com/OpenRefine/OpenRefine/issues/1096#issuecomment-345722334 proposal from @thadguidry . Note that anyone mapping into the Wikidata data model is in \"RDF plus some stuff, minus some stuff\" territory. It would be interesting to get more explicit requirements from the Wikidata side in terms of mapping in not just basic facts but qualifier and sourcing/provenance data too. While a simple CSVW RDF mapping like http://danbri.org/2016/PublicToilets/mapping.json can show how to turn tables into triples, I expect for using OpenRefine to source graph data for Wikidata we'll want to pass along per-factoid provenance too somehow. @thadguidry - have you any thoughts on this?. @thadguidry yes in many case provenance would be at a higher granularity. But I understand Wikidata at least in theory encourages per-fact sourcing references, e.g. https://www.wikidata.org/wiki/Q243 has two references for the Eiffel Tower's height. I didn't mean to suggest that every cell in the original table need have different provenance to pass along.. ",
    "satra": "\nA problem with both approaches is that it adds complexity - CSV files are nice and convenient, and packages less so, particularly if they use a bespoke suffix rather than .zip for compressed bundles (which means many folk wouldn't know what to do with them..) For me, the packaging is more likely to be of use when it comes to putting together toolchains so that once I fix a datatype I can get it to persist across different applications while still using CSV as the transport.\n\nthe problem with CSV files is precisely the missing metadata. i work in a field where our columns come from many different sources of information. some of them are conceptually similar and some of them are the same but have different names, and some that are different and have the same name. whether one is sharing within a lab/project/organization, if the data generators change, it's essential that information exists to indicate what columns refer to, and that this information is machine accessible and not stored in a pdf somewhere. \ni'll take a simple example of a column that arises in many of our CSVs: Age. Since age can be represented in many units, many levels of quantization (days/weeks/months/years/5years), someone somewhere has to understand what that Age column refers to. note that the datatype is not changing, it's really about what is being represented that changes. so, yes CSVs are simple, but they require a significant amount of human understanding and agreement to operate on. by structuring information, we can potentially remove the human in the loop and computational toolchains can benefit from structured information attached to the columns.\nif semantics can be attached in machine accessible form, we will be in a much better position down the road in aggregating and recomputing data. in addition to the tabular data, there is also this:\nhttp://www.w3.org/TR/vocab-data-cube/\nand in someways this maps quite nicely to computational tools as n-d arrays, of which a table is a subset (or a transformation).\n. ",
    "LibrarPotter": "Thank you for your assistance. I did use that documentation before reaching out, but through further experimentation I have realized that it does not cover some of the characters (e.g. * ), which has resolved my issue. \nreplace() does not require the use of * to search within a continuous string whereas match() does.\n. ",
    "PeterJPower": "It must be a MS Edge browser's incompatibility fault, I noticed now with Google Chrome everything of OpenRefine is functioning ok . \n. ",
    "c0gs": "I'll give it a shot, thanks!\n. This worked, thanks!\n. ",
    "radinamatic": "openrefine-2.6-rc.2 not working in Firefox 45 on Ubuntu 15.10. Tried running in safe mode with all the add-ons disabled & erasing cache to no avail:\n\nChromium:\n\njava version \"1.7.0_95\"\nOpenJDK Runtime Environment (IcedTea 2.6.4) (7u95-2.6.4-0ubuntu0.15.10.2)\nOpenJDK 64-Bit Server VM (build 24.95-b01, mixed mode)\n. ",
    "AliND": "Hi Tom,\nThanks a lot for your reply.\nI am using OpenRefine 2.6 Beta1, Windows 7 x64, Chrome v48.\nThe error occurs when I try to cluster & edit a column (method: key collusion). Once I try to merge the data, this message pops up \"Error: Form too large 4467189>1048576\".\nApparently, it has to do with the 1MB limit which I am trying to change. I was happy to know that this parameter can be changed but I am not successful in doing that. \nSince refine.ini & refine.bat have to be altered, I need to run refine.bat, otherwise it has no effects as you stated.\nI tried running refine.bat by double-clicking and it didn't work (command window shows for milliseconds and then disappears). If I try to run it from the command window, I get: The system cannot find the file refine.ini...etc. There is a link with instructions on how to set JAVA_HOME to point at JDK installation, but the site is down!\nCould you please advise if the following is sufficient for overriding the 1MB form limit?\n\nAlter refine.ini & refine.bat as explained in my initial post, and then run refine.bat and not the executable openrefine.exe file.\n\nIf I am correct on that point, then all what I need to do is to learn how to run refine.bat.\nThanks again @tfmorris, Appreciate it.\nAli\n. UPDATE:\nHi again,\nI have installed JDK and set the JAVA_HOME variable. Now, I am able to run OpenRefine through refine.bat.\nI've replaced refine.bat & refine.ini with the update of @amsardesai ( #908 & #971) and changed the value of \"REFINE_MAX_FORM_CONTENT_SIZE\" to 10485760 in refine.ini. Unfortunately, I am still getting the same error.\nIs't that I have to apply the modification mentioned in #876 first so that the new limit is applied? If this is the case, I am unable to find the file refine.java. In case that is the only additional step for the new parameters to become effective, could you please advise on how to do it?\nThanks & Regards,\nAli\n. ",
    "thatbudakguy": "OSX 10.11.6 / OpenRefine 2.6-rc.2\nAlso have this issue. The project looks fine in preview and correctly detects that the option is needed, but ignores it when I create the project.. Just reproduced on Ubuntu 16.04 / OpenRefine 2.7-rc.2 as well.. workaround for me was to upload the .csv I was trying to import to Google Drive/convert to Google Sheets, which parsed the file correctly. Then I could use the \"import from Google\" option in OpenRefine on the newly created sheet and the data came in correctly.. ",
    "ciscoheat": "Perfect, thank you! :)\n. ",
    "joyceho951": "Since the issue report system does not support uploading .json, I change the file extension to .txt for upload.\nI can only import 9 records (13 rows) of this file. It stopped import at the one with Chinese characters in parameter values using Google Refine v2.5\n20160423_210000_shop.txt\n. ",
    "sputnik62": "Anyone working on this?\n. ",
    "zacharynanfelt": "@tfmorris, as the most (thumbs up) voted issue for this project that is currently open, I thought I would look into it.\nI just tried verifying this with the Easy Nested Example.txt from http://jsonlines.org/examples/ and selecting \"Parse data as: JSON Files\" which provided the attached result.  \nCan we close this issue?\n\n. ",
    "ymnder": "Thank you for your reply. Maybe failed reason is that there is no solution file or project in the directory. So if not contain those files, it is necessary to turn off build settings on project settings page. https://www.appveyor.com/docs/build-phase\n. ",
    "lmsurpre": "Looks like a duplicate of #1093 which is already marked for 2.6\n. ",
    "iYefeng": "This failure is not caused by me.\n\u201cBuild started\ngit clone -q https://github.com/OpenRefine/OpenRefine.git C:\\projects\\openrefine\ngit fetch -q origin +refs/pull/1139/merge:\ngit checkout -qf FETCH_HEAD\nSpecify a project or solution file. The directory does not contain a project or solution file.\u201d\nI find that if jython scripts are too long, the status code of the http response is 413 because you put scripts in the url. So I have changed some code again. Please have a check.\n. ",
    "standage": "Well, 2.6-rc2 seems to run fine after restarting the computer. Sorry this might be hard to reproduce, given that I was trying with 2 different versions, with vs without legacy Java, etc. The only clue I can provide is that my last few attempts to launch 2.5 before restarting my computer, an error dialog popped up very briefly (i.e. <2 seconds) referencing some kind of binding exception and stating that the address was already in use.\n. ",
    "WalnerPessoa": "try to downgrade java\n. I uninstalled the newest version of java and install old version of java, so I installed google refine Version 2.5 [r2407].\n. ",
    "evanwill": "There is something clearly broken in the styling on Chrome however that could be an easy fix by looking at why it works in Firefox. Some odd browser prefix thing?\nThe usability issue for both browsers is that the pop up dialog-frame is set to width 1000px, the height is set by the content, but if it is taller than the browser window size, it is not possible to scroll down to use the full pop up since its container dialog-container is set to position: fixed. A quick fix to allow scrolling is to set dialog-container to position: relative, but that breaks the \"draggable\" functionality.\nIt might be an issue with deeper roots, since it looks like when rendering it must query the browser for window size to set the parameters of the page. However, on Win 10 it seems to get the size of the full screen rather than window, since a small portion of the UI is actually below the browser window, where the task bar is, and is inaccessible unless I go to full screen mode with the browser. This maybe an issue specific to HD screens that use scaling to have everything appear normal size on the screen.\n. ",
    "jls4dtna": "I have this same issue, and it has nothing to do with the value of TMP or TEMP. Using the Browse button, I selected the file Librarygood.xml, which was in my Windows 7 home directory C:\\Users\\STAMPEL.  When I tried to load it, I got this error, much like the others received:\n\nIt seems clear that OpenRefine was trying to copy the xml file in its own private temp directory  'C:\\Users\\STAMPEL\\AppData\\Local\\Temp\\Jetty_127_0_0_1_3333_webapp____4ulpc9\\import\\2\\raw-data', replicating the original file's path - i.e., C:\\Users\\STAMPEL.\nThe problem is that the colon character is not allowed in Windows file names. The colon needs to be removed or replaced with a legal character. That should fix the issue.. You can simply retain the drive letter but remove the colon.\ne.g. C:\\foo\\bat\\bar\\data.xml is mapped to ...\\raw-data\\C\\foo\\bat\\bar\\data.xml\nFrom: cameronstewart [mailto:notifications@github.com]\nSent: Saturday, April 22, 2017 11:04 PM\nTo: OpenRefine/OpenRefine\nCc: Stamper, Larry (164-Extern-Larry); Comment\nSubject: Re: [OpenRefine/OpenRefine] error uploading data from csv, perhaps due to colon included in temp filename (#1142)\nSo how would you fix this considering everything will be somewhere on a\ndrive so a drive: will be replicated Irrespective of where it is?\nOn Sat, 22 Apr 2017 at 6:20 am, jls4dtna notifications@github.com wrote:\n\nI have this same issue, and it has nothing to do with the value of TMP or\nTEMP. Using the Browse button, I selected the file Librarygood.xml, which\nwas in my Windows 7 home directory C:\\Users\\STAMPEL. When I tried to load\nit, I got this error, much like the others received:\n[image: image]\nhttps://cloud.githubusercontent.com/assets/27868729/25293817/456ad426-2691-11e7-8f60-8bff136d2a79.png\nIt seems clear that OpenRefine was trying to copy the xml file in its own\nprivate temp directory\n'C:\\Users\\STAMPEL\\AppData\\Local\\Temp\\Jetty_127_0_0_1_3333_webapp____4ulpc9\\import\\2\\raw-data',\nreplicating the original file's path - i.e., C:\\Users\\STAMPEL.\nThe problem is that the colon character is not allowed in Windows file\nnames. The colon needs to be removed or replaced with a legal character.\nThat should fix the issue.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1142#issuecomment-296296306,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAZibTkUSEyRaE4L3XHsAxVnLa0Nf0MDks5ryQ99gaJpZM4JNdNI\n.\n\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/1142#issuecomment-296421999, or mute the threadhttps://github.com/notifications/unsubscribe-auth/Aak-OcO0NMks1pRQozVBYswBmLishTozks5ryunngaJpZM4JNdNI.\nIf you are not the addressee, please inform us immediately that you have received this e-mail by mistake, and delete it. We thank you for your support.\n. I did not copy the full path. I did not copy anything at all. As I stated in my original posting,\nI used the Browse button to select the file\u2026\n[cid:image002.jpg@01D2BCDE.9FFA5370]\nThen I selected the file\u2026\n[cid:image003.jpg@01D2BCDE.9FFA5370]\nThen I clicked the Next button\u2026\n[cid:image010.jpg@01D2BCDE.9FFA5370]\nAnd then I got the error message:\n[cid:image011.jpg@01D2BCDE.9FFA5370]\nFrom: Jacky [mailto:notifications@github.com]\nSent: Monday, April 24, 2017 9:24 AM\nTo: OpenRefine/OpenRefine\nCc: Stamper, Larry (164-Extern-Larry); Comment\nSubject: Re: [OpenRefine/OpenRefine] error uploading data from csv, perhaps due to colon included in temp filename (#1142)\nI cannot reproduce the issue. The file path you selected should not has\nbeen taken the full path. Should be file name only. Did you copy paste the\nfull path\nOn Mon, Apr 24, 2017, 11:16 jls4dtna notifications@github.com wrote:\n\nYou can simply retain the drive letter but remove the colon.\ne.g. C:\\foo\\bat\\bar\\data.xml is mapped to\n...\\raw-data\\C\\foo\\bat\\bar\\data.xml\nFrom: cameronstewart [mailto:notifications@github.com]\nSent: Saturday, April 22, 2017 11:04 PM\nTo: OpenRefine/OpenRefine\nCc: Stamper, Larry (164-Extern-Larry); Comment\nSubject: Re: [OpenRefine/OpenRefine] error uploading data from csv,\nperhaps due to colon included in temp filename (#1142)\nSo how would you fix this considering everything will be somewhere on a\ndrive so a drive: will be replicated Irrespective of where it is?\nOn Sat, 22 Apr 2017 at 6:20 am, jls4dtna notifications@github.com\nwrote:\n\nI have this same issue, and it has nothing to do with the value of TMP\nor\nTEMP. Using the Browse button, I selected the file Librarygood.xml,\nwhich\nwas in my Windows 7 home directory C:\\Users\\STAMPEL. When I tried to\nload\nit, I got this error, much like the others received:\n[image: image]\n<\nhttps://cloud.githubusercontent.com/assets/27868729/25293817/456ad426-2691-11e7-8f60-8bff136d2a79.png>\nIt seems clear that OpenRefine was trying to copy the xml file in its\nown\nprivate temp directory\n'C:\\Users\\STAMPEL\\AppData\\Local\\Temp\\Jetty_127_0_0_1_3333_webapp____4ulpc9\\import\\2\\raw-data',\nreplicating the original file's path - i.e., C:\\Users\\STAMPEL.\nThe problem is that the colon character is not allowed in Windows file\nnames. The colon needs to be removed or replaced with a legal character.\nThat should fix the issue.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\n<\nhttps://github.com/OpenRefine/OpenRefine/issues/1142#issuecomment-296296306>,\nor mute the thread\n<\nhttps://github.com/notifications/unsubscribe-auth/AAZibTkUSEyRaE4L3XHsAxVnLa0Nf0MDks5ryQ99gaJpZM4JNdNI>\n.\n\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub<\nhttps://github.com/OpenRefine/OpenRefine/issues/1142#issuecomment-296421999>,\nor mute the thread<\nhttps://github.com/notifications/unsubscribe-auth/Aak-OcO0NMks1pRQozVBYswBmLishTozks5ryunngaJpZM4JNdNI>.\nIf you are not the addressee, please inform us immediately that you have\nreceived this e-mail by mistake, and delete it. We thank you for your\nsupport.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1142#issuecomment-296701816,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AKCGi0Oxo-J5KOTVRoeeTiIsw9wHRyh1ks5rzLzTgaJpZM4JNdNI\n.\n\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/1142#issuecomment-296728184, or mute the threadhttps://github.com/notifications/unsubscribe-auth/Aak-OV2pEBU2vRlr4BS_joTdjSpr8HYNks5rzMyZgaJpZM4JNdNI.\nIf you are not the addressee, please inform us immediately that you have received this e-mail by mistake, and delete it. We thank you for your support.\n. Right. The problem is in Windows 7. I read in another forum that someone had been using Refine on earlier version(s) of Windows for years with no problem, but when they moved to Windows 7 this problem happened immediately.. Setting Chrome as the default browser fixed the problem for me.. @cameronstewart, read the most recent posts. The root cause of the problem is different than I supposed. It turned out that the full path should never have been replicated in the first place, therefore my proposed solution is not helpful. The workaround is to make Chrome your default browser.. ",
    "bright-sea": "There is no problem in my old 32bit windows7 machines, everything works fine. Today I changed to a new 64bit windows7 machine and  seeing exactly the same issue. Nothing to do with users operation, I just choose a file from dialog. Running as administrator doesn't help.  . One more test found out the problem: It is happens when you use IE browser. If you change to use chrome, it will be OK. This might be caused by IE11 which was installed on my new Windows7.\nSo we can further investigate this at IE11 on other environment.. ",
    "ericjarvies": "Perhaps adding a contextual menu shortcut link, e.g.-\nMove column to beginning\nMove column to end\nMove column left\nMove column right\nRe-order columns\nwherein \"Re-order columns\" opens what can otherwise be found (opened) here;\nAll -> Edit Columns -> re-order / remove columns...\n. Thanks @wetneb and @ettorerizza for the feedback.\nWhat would be a fair bounty for this feature/option  (the ability to set/change User Agent)?. @ettorerizza - Your above mentioned advice/instructions worked, thank you.. @ettorerizza - If I may; what are the \"few more lines\" that allow credentials (username/password) to be used?\nIt seems both the User Agent and Credentials features would not be too terribly difficult to add, right?\nAlthough you've offered -interim- solutions to both of these, wouldn't it would be better if they were included as default menu items/options.\nDo you have an opinion regarding what a fair bounty is (vs. programming time) for each of these tasks?\nEric. i've been fetching records from this site for several years, and have \nnever been blocked, but i only ever fetch their 'free' data, otherwise \ni am a regular/ongoing paying customer for their 'fee' based data.\ni have a handful of other sources/sites that also yield the same \nun-styled XML... but i can't remember which ones they are, but as i do \nmy routine updating i'll make note of them as they occur, and post them \non GitHub so there are some examples to working with.\nthanks,\neric\nOn 7/31/17 2:54 AM, Ettore Rizza wrote:\n\n@ericjarvies https://github.com/ericjarvies It all depends on the \nauthorization system that the site uses. But if it's the kind to ban \nuser-agents and impose a username/password, it must also have terms of \nuse that prohibit scraping. It will not take long for it to detect \nthat your queries are being sent by software. Open Refine can do basic \nscraping, but this is not its primary function.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub \nhttps://github.com/OpenRefine/OpenRefine/issues/1217#issuecomment-319022364, \nor mute the thread \nhttps://github.com/notifications/unsubscribe-auth/AA7qKicL0q-yyfiKmOzGEERnUkMVr3n6ks5sTaRtgaJpZM4On6kU.\n\n\n. I've run into this problem a few times in the past, but do not have the URLs committed to memory (or saved to file).  The current URL is a credential-based website, so I must first log into the website, and then the URL/query looks something like this;\nhttps://someWebsite.com/user/@ericjarvies;pid=111-222-333;fileReference=;selectedDistrictCode=ALL;selectedDistrictName=;includeCancelledTitles=N?t=1485910928995\nThe returned result is entirely in XML like so;\n<TitleSummaries>\n<errors/>\n<itemsPerPage>20</itemsPerPage>\n<titleSearchResultByPidorShortLegal>\n<errors/>\n<pid>FOO</pid>\n<shortLegal>S/4502///24//7 *</shortLegal>\n<titleSummary>\n<errors/>\n<concatShortLegal>\n0000000000000004502 00000000000024 00007 011542641001\n</concatShortLegal>\n<firstOwner>FOO</firstOwner>\n<fullLegalDescription>LOT 7 BLOCK 24 DISTRICT LOT 526 PLAN 4502</fullLegalDescription>\n<index>1</index>\n<landTitleDistrict>VA</landTitleDistrict>\n<pid>FOO</pid>\n<shortLegal>S/4502///24//7 *</shortLegal>\n<status>REGISTERED</status>\n<strataPlanCommonProperty>false</strataPlanCommonProperty>\n<titleNumber>BB300420</titleNumber>\n</titleSummary>\n</titleSearchResultByPidorShortLegal>\n<titleSearchResultByPidorShortLegal>\n<errors/>\n<pid>FOO</pid>\n<shortLegal>S/4502///24//8 *</shortLegal>\n<titleSummary>\n<errors/>\n<concatShortLegal>\n0000000000000004502 00000000000024 00008 011542675001\n</concatShortLegal>\n<firstOwner>FOO</firstOwner>\n<fullLegalDescription>LOT 8 BLOCK 24 DISTRICT LOT 526 PLAN 4502</fullLegalDescription>\n<index>1</index>\n<landTitleDistrict>VA</landTitleDistrict>\n<pid>FOO</pid>\n<shortLegal>S/4502///24//8 *</shortLegal>\n<status>REGISTERED</status>\n<strataPlanCommonProperty>false</strataPlanCommonProperty>\n<titleNumber>CA5139349</titleNumber>\n</titleSummary>\n</titleSearchResultByPidorShortLegal>\n</TitleSummaries>\nThe web browser simply displays the XML -even though it does not have any styling/is not part of an html (or whatever) type of document.  But OpenRefine simply ignores the fetched/returned results/XML\nEric\n. ",
    "javian": "You can submit a PR to https://github.com/OpenRefine/openrefine.github.com to change it.\n. ",
    "packet-rat": "@ettorizza: Many thanks - seconding the question as to which rdf-extension to use with 2.6 (or is advice to revert to Google Refine for RDF related work?)\n. @ettorerizza :   Many thanks for the guidance.  I've had good success with OpenRefine and LODRefine by replacing the deprecated rdf-extension in the source distributions. \nI'm a \"python guy\" (primarily developing on Mac OSX) and looking for some general guidance on how you manage the multiple instantiations. For most scenarios I use Brew and Virtualenvwrapper to control what packages and external libraries are used for a given host environment executed instantiation.  Where required, I use Vagrant and Docker to further control environments (or suppprt things that need Linux or Windows variants). I also use pycharm vs. eclipse as my \"goto\" IDE.\nI have run into issues with unintentional interactions between various OpenRefine and LODRefine instantiations on my base OSX system.  I've built the distributions from source (on Mac OSX).   I've edited the build.xml files to point (via relative references) to the new extensions.  I then clean and rebuild from source.  I then run them from command line in the given instantiations root directory.\nIn the case of LODRefine, I eventually discovered it was actually using a deprecated rdf-extension from an OpenRefine instantiation.  Once I deleted the offending extension from OpenRefine, LODRefine used the \"right\" rdf-extension.\nI could use Vagrant or Docker to isolate instantiations but prefer to run this from my base OSX system to leverage various development/debugging toola.\n. ",
    "viktordickmm": "Hello,\nis there any workaround or plans to correct this?\nMany thanks,\nViktor. ",
    "augusto-herrmann": "For anyone also experiencing the problem, I can confirm that the following workaround for using Java 8 currently works:\nsudo apt install openjdk-8-jre\nexport JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"\n./refine. ",
    "danielsedlacek": "That's amazing, thanks!. Hi Thad, there is a misunderstanding, I have no idea how to allocate more memory - I was unable to follow the FAQ and I don't know what to do. . ",
    "davbre": "I have experienced the same issue. It occurs when I try to export to CSV. Oddly, if I export a second time I get the same error message yet the csv file is successfully exported (i.e. downloaded automatically).\nMy setup:\nopenrefine-2.6-rc.2\nUbuntu 16.04.1 LTS\nI can't reproduce the error at will. It occurs after I have worked on a dataset for a while. I'll update here if/when I have more info.. ",
    "VitoLiao": "@thadguidry yah, I did, but I really forgot the way cause time has passed too long  : ). ",
    "BernardBoureee": "Hello Thad\nThank you so much!\nRegards\nBernard\n\nBernard Bour\u00e9e\n\nDe : Thad Guidry notifications@github.com\nEnvoy\u00e9 : mercredi 15 f\u00e9vrier 2017 15:59:57\n\u00c0 : OpenRefine/OpenRefine\nCc : BernardBoureee; Author\nObjet : Re: [OpenRefine/OpenRefine] Help for building an extraction (#1171)\nWith Chrome and Firefox you can right click on an element on a webpage and\nget lots of details on elements through an Inspector window. The inspector\nwindow highlights elements and you can move your mouse around in the\ninspector window of the HTML code.\nIn Chrome or Firefox -\n1. Right click on the element you are interested in in the page itself and\nchoose Inspect Element.\n2. The inspector window appears with HTML code.\n3. In the inspector window move your mouse around and hover over elements\nto highlight them.\n4. Right click on the  or whatever element that you are really\ninterested in extracting then choose Copy -> Copy Selector (or Css Selector)\n5. You can then use that which is copied to the clipboard and paste into\nthe GREL select() itself, since that just uses Jsoup under the covers.\nmw-content-text > div.bandeau-article.bandeau-niveau-ebauche.plainlinks\nbecomes\nvalue.parseHtml().select(\"\ndiv.bandeau-article.bandeau-niveau-ebauche.plainlinks \")\nMore details here in these links and read the important tip about refering\nto Jsoup's specific Selector command (which is what the GREL select()\nactually exposes:\nhttps://github.com/OpenRefine/OpenRefine/wiki/StrippingHTML\nhttps://jsoup.org/apidocs/org/jsoup/select/Selector.html\n-Thad\n+ThadGuidry https://www.google.com/+ThadGuidry\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/OpenRefine/OpenRefine/issues/1171#issuecomment-280033355, or mute the threadhttps://github.com/notifications/unsubscribe-auth/ASse7W9UYVSwdk4UZMTvyHbd-bucqHp3ks5rcxLtgaJpZM4MByWp.\n. OK thanks. ",
    "scottythered": "Hi Thad, \nThanks for looking into this. The commands are set up so that they can run if a column is generated, or the command can die if a column wasn't generated by a previous command. The issue I'm raising is why in 2.7 the commands generate the NULL values in the \"959$a\" column and the value in the \"link\" column -- they should both be set to blank, as what happens in the 2.6 version, right?. Ah! I had no idea there was a new version. I was under the impression that the RDF extension had been abandoned?. ",
    "cwhits": "Was a different cert used to sign the .app this time? The cert leaf has changed since 2.6.\nCould you look into using a shared codesigning cert if different people are building and signing the app or getting an Apple Dev ID?\nThanks!. ",
    "Shamanou": "Hi thanks for the quick response\ni will let you know when i need any help. ",
    "adehner": "Thank you. Unfortunately the system outputs this wonky data, without config options to improve the format. Sometimes bad OCR results in characters that split records unpredictably. We will probably need to use scripting between system export and OpenRefine import.. ",
    "Tech-Horizon": "when importing to excel the wizard does not ask for row separator. it treats both CRLF and LF as row seperator . if it defaults to only CRLF  then it could work.. ",
    "codeforkjeff": "Hi, thank you for the ping! This definitely sounds useful, and I would be happy to add support for a new endpoint to conciliator for VIAF. I guess the details to be figured out are: how to provide a list of available properties to OpenRefine (should the service metadata be extended?) and making sure that the new endpoint supports a \"multiple query mode\" for better performance.\nI can also try implementing some \"test ideas\" for this new API in a branch, if anyone wants to test alpha/beta code for this feature against a live service. Let's use email, gitter, or slack, to coordinate that?. @wetneb Sounds good. I see what you mean about users needing to set their own properties.Yes, the mailing list works for me, I've just subscribed to it and will keep a look out for developments on this front.\n. @wetneb quick question: in the example response shown in the API spec, there is an \"ids\" key in addition to \"rows\" and \"meta\", but this key isn't described. Should that really be part of the response or is that a copy and paste error?. Thanks for the ping, I haven't forgotten about this, just trying to find time to digest this extension and get a working test implementation in conciliator. Soon, I hope.. ",
    "xiaomj": "@thadguidry hi, I come across this issue when I first revert Flag or Starred rows back to normal rows and then apply back. It does not work of course.. ",
    "adieyal": "This works for me in Version 2.6-beta.1 [TRUNK] so I suspect that the regression happened between those two versions.. This is the stack trace that I get with 2.6-rc.2\n08:27:02.840 [      ImportingParserBase] ModelException adding Filename column (18ms)\ncom.google.refine.model.ModelException: Duplicated column name\n    at com.google.refine.model.ColumnModel.addColumn(ColumnModel.java:120)\n    at com.google.refine.importers.ImportingParserBase.addFilenameColumn(ImportingParserBase.java:169)\n    at com.google.refine.importers.TabularImportingParserBase.readTable(TabularImportingParserBase.java:110)\n    at com.google.refine.importers.ExcelImporter.parseOneFile(ExcelImporter.java:217)\n    at com.google.refine.importers.ImportingParserBase.parseOneFile(ImportingParserBase.java:118)\n    at com.google.refine.importers.ImportingParserBase.parse(ImportingParserBase.java:89)\n    at com.google.refine.importing.ImportingUtilities.previewParse(ImportingUtilities.java:956)\n    at com.google.refine.importing.DefaultImportingController.doUpdateFormatAndOptions(DefaultImportingController.java:185)\n    at com.google.refine.importing.DefaultImportingController.doPost(DefaultImportingController.java:93)\n    at com.google.refine.commands.importing.ImportingControllerCommand.doPost(ImportingControllerCommand.java:62)\n    at com.google.refine.RefineServlet.service(RefineServlet.java:177)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n    at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n    at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n    at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n    at org.mortbay.jetty.Server.handle(Server.java:326)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:938)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:748)\n08:27:02.852 [                   refine] POST /command/core/get-models (12ms)\n08:27:02.863 [                   refine] POST /command/core/get-rows (11ms)\nLooking at the code this is where it breaks in com.google.refine.importers.ImportingParserBase:\n165     protected static int addFilenameColumn(Project project) {\n166         String fileNameColumnName = \"File\";\n167         assert project.columnModel.getColumnByName(fileNameColumnName) == null;\n168         try {\n169             project.columnModel.addColumn(\n170                 0, new Column(project.columnModel.allocateNewCellIndex(), fileNameColumnNa    me), false);\n171             return 0;\n172         } catch (ModelException e) {\n173             // Shouldn't happen: We already checked for duplicate name.\n174             logger.error(\"ModelException adding Filename column\",e);\n175         }\n176         return -1;\n177     }. \nUntitled spreadsheet.xlsx\nSame problem - see below. I also attached a test file\n```\n\u21d2  ./refine\nStarting OpenRefine at 'http://127.0.0.1:3333/'\n22:46:57.781 [            refine_server] Starting Server bound to '127.0.0.1:3333' (0ms)\n22:46:57.784 [            refine_server] refine.memory size: 1400M JVM Max heap: 1304952832 (3ms)\n22:46:57.809 [            refine_server] Initializing context: '/' from '/Users/adieyal/Development/OpenRefine/main/webapp' (25ms)\n22:46:57.903 [            refine_server] Starting autoreloading scanner...  (94ms)\n22:46:58.450 [                   refine] Starting OpenRefine 2.7 [TRUNK]... (547ms)\n22:47:06.405 [                   refine] POST /command/core/load-language (7955ms)\n22:47:06.436 [                   refine] POST /command/core/load-language (31ms)\n22:47:06.597 [                   refine] POST /command/core/get-importing-configuration (161ms)\n22:47:06.622 [                   refine] GET /command/core/get-all-project-metadata (25ms)\n22:47:06.661 [                   refine] GET /command/core/get-languages (39ms)\n22:47:06.699 [                   refine] GET /command/core/get-version (38ms)\n22:49:44.849 [                   refine] POST /command/core/create-importing-job (158150ms)\n22:49:44.881 [                   refine] POST /command/core/importing-controller (32ms)\n22:49:45.864 [                   refine] POST /command/core/get-importing-job-status (983ms)\n22:49:45.934 [                   refine] POST /command/core/importing-controller (70ms)\n22:49:47.575 [                   refine] POST /command/core/importing-controller (1641ms)\n22:49:47.741 [                   refine] POST /command/core/get-models (166ms)\n22:49:47.754 [                   refine] POST /command/core/get-rows (13ms)\n22:50:03.417 [                   refine] POST /command/core/importing-controller (15663ms)\n22:50:03.462 [                   refine] POST /command/core/get-models (45ms)\n22:50:03.468 [                   refine] POST /command/core/get-rows (6ms)\n22:50:05.858 [                   refine] POST /command/core/importing-controller (2390ms)\n22:50:05.903 [      ImportingParserBase] ModelException adding Filename column (45ms)\ncom.google.refine.model.ModelException: Duplicated column name\n    at com.google.refine.model.ColumnModel.addColumn(ColumnModel.java:120)\n    at com.google.refine.importers.ImportingParserBase.addFilenameColumn(ImportingParserBase.java:169)\n    at com.google.refine.importers.TabularImportingParserBase.readTable(TabularImportingParserBase.java:110)\n    at com.google.refine.importers.ExcelImporter.parseOneFile(ExcelImporter.java:217)\n    at com.google.refine.importers.ImportingParserBase.parseOneFile(ImportingParserBase.java:118)\n    at com.google.refine.importers.ImportingParserBase.parse(ImportingParserBase.java:89)\n    at com.google.refine.importing.ImportingUtilities.previewParse(ImportingUtilities.java:956)\n    at com.google.refine.importing.DefaultImportingController.doUpdateFormatAndOptions(DefaultImportingController.java:185)\n    at com.google.refine.importing.DefaultImportingController.doPost(DefaultImportingController.java:93)\n    at com.google.refine.commands.importing.ImportingControllerCommand.doPost(ImportingControllerCommand.java:62)\n    at com.google.refine.RefineServlet.service(RefineServlet.java:177)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n    at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n    at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n    at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n    at org.mortbay.jetty.Server.handle(Server.java:326)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:938)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:748)\n22:50:05.924 [                   refine] POST /command/core/get-models (21ms)\n22:50:05.937 [                   refine] POST /command/core/get-rows (13ms)\n```\nUntitled spreadsheet.xlsx\n. Yep, works fine now. The problem before was that the check for a duplicate name was done using an assert which obviously needs to be explicitly enabled at runtime. I replaced it with and if statement. ",
    "Archaeo-Programmer": "Yes, that did the trick. Everything installed just fine. Thanks!. Great, thanks so much for the reply, and updating the instructions with additional step. I was unaware that this was a necessary step for some casks. Thanks!. ",
    "BobHarper1": "Is this relevant line of code?\ncom.google.refine.expr.functions.booleans.Or.java\nNot a java guy, but should it be if (args.length >= 2 ...??. Ok, so the documentation needs to change then.. CI errors required rewrite of the test conditions so and and or to not error on three conditions.\nAlso added multiple params to xor to be consistent (see http://stackoverflow.com/questions/6222162/in-java-xor-with-three-true-inputs-returns-true-why). I've updated Wiki in anticipation!. ",
    "sheva605": "@thadguidry\nEvery time I use cluster feature in the text facet, basically there are two methods. For the key collision, there are four functions. If I select all and merge them together, this process will be shown up as a mass edit as a part of the Undo/Redo history which means if I copy the whole history and paste into a same format but a different data file, those mass edit steps wouldn't be processed. I want to use the whole Undo/Redo history as a script to fully automate my daily task which is to merge data based on the keying function without human intervention. . ",
    "SonawaneMayur": "Ohhh Got it.. I can upload excel only by Google chrome. \nThanks, Jacky. ",
    "jwillmer": "There is a web server for Chrome so at least it is possible to host it inside the browser.. ",
    "Yuutakasan": "I will attach a sample file for reference.\nimport file\nimport.txt\nexport file\nexport.txt\nThere is a sense that this letter is actually used in the name of the corporation registered in Japan.\n\u6709\u9650\u4f1a\u793e\u306a\u3079\u8336\u5c4b\u3042\u3055\ud844\udf1b\n\u682a\u5f0f\u4f1a\u793e\u77f3\ud84d\ude3a\u7d44\n\u6709\u9650\u4f1a\u793e\ud84f\udcfe\u65b0\u5546\u4e8b\n\ud84f\udcfe\u5e78\uff11\u5408\u540c\u4f1a\u793e. I currently use EmEditor, I will try using Notpad ++.\nEmEditor\nhttps://www.emeditor.com/. I was able to reproduce the same phenomenon.\n\n. thank you. @thadguidry. \nI tried testing with multiple character codes.\nIt seems that a character with a code point of 10000 or more will be garbled.\nimport txt\nimport-test-sample.txt\n\nexport txt\nexport-test-sample-txt.txt\n\n. OK.I will try it!. @jackyq2015 \nCan I test by adding \"-Dfile.encoding = UTF-8\" setting to the openrefine.l4j.ini file?\n. Since there is no pervert, I was worried whether the setting really worked.\nit is executed once, I will share the result later.. @jackyq2015 @thadguidry \nSorry for being late. I tried the settings I got the other day.\n\nI downloaded openrefine - 2.8.\nIt changed to the following setting.\nopenrefine.l4j.zip\nI imported garbled data before.\nimport-test-sample.zip\n\n\nIt is displayed normally\n\u2460\n\n\u2461\n\n4.The exported file is garbled.\nexport-test-sample-txt.zip\n. I think that it is a character string conversion mistake at export timing, not an encoding discrimination bug at import timing.. other export pattern\nExcel\nimport-test-sample-xlsx.zip\n\nHTML\n\nimport-test-sample-html.zip. @jackyq2015 \nThe above processing is executed with the following settings.\nconfigfile\nhttps://github.com/OpenRefine/OpenRefine/files/1492661/openrefine.l4j.zip\nopenrefine.l4j.ini\n\uff0a\uff0a\uff0a\uff0a\uff0a\uff0a\uff0a\uff0a\uff0a\uff0a\uff0a\uff0a\uff0a\uff0a\uff0a\uff0a\uff0a\uff0a\uff0a\uff0a\uff0a\uff0a\uff0a\n-Xms256M\n-Xmx1024M\n-Djava.net.useSystemProxies=true\n-Dfile.encoding=\"UTF-8\". ",
    "felipemouradev": "I'm going through almost the same problem, at the endpoint of creating a new project, the response comes in an html and no ID is provided as stated in the documentation.\nHas anyone managed to solve it?. ",
    "stellasia": "Well, I do not remember exactly but I finally get it to work thanks to the https://github.com/OpenRefine/refine-python as @ostephens  suggests, adapted for python3 in my case.\nRelevant lines are : \nfrom urllib import parse\n\n response = requests.post(self.server + '/command/core/create-project-from-upload',\n                         data=data,\n                         files=files,\n                         headers={\"Accept\": \"application/json\"}\n)\nif response.status_code != 200:\n  # TODO: better error reporting\n  return None\n\nresponse_body = parse.parse_qs(parse.urlparse(response.request.url).query)\n\nif 'project' in response_body:\n  i = response_body['project'][0]\n  return RefineProject(self.server, i, project_name)\n\n. ",
    "jyf1997": "How do you solve it?. @ostephens OpenRefine 3.1\uff1bI tried to run through its source code, but I made a mistake.\njava.lang.ClassNotFoundException: com.google.refine.RefineServlet\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n    at org.mortbay.jetty.webapp.WebAppClassLoader.loadClass(WebAppClassLoader.java:401)\n    at org.mortbay.jetty.webapp.WebAppClassLoader.loadClass(WebAppClassLoader.java:363)\n    at org.mortbay.util.Loader.loadClass(Loader.java:91)\n    at org.mortbay.util.Loader.loadClass(Loader.java:71)\n    at org.mortbay.jetty.servlet.Holder.doStart(Holder.java:73)\n    at org.mortbay.jetty.servlet.ServletHolder.doStart(ServletHolder.java:242)\n    at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n    at org.mortbay.jetty.servlet.ServletHandler.initialize(ServletHandler.java:736)\n    at org.mortbay.jetty.servlet.Context.startContext(Context.java:140)\n    at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282)\n    at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)\n    at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)\n    at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n    at org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:130)\n    at org.mortbay.jetty.Server.doStart(Server.java:224)\n    at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n    at com.google.refine.RefineServer.init(Refine.java:197)\n    at com.google.refine.Refine.init(Refine.java:109)\n    at com.google.refine.Refine.main(Refine.java:103)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at org.codehaus.mojo.exec.ExecJavaMojo$1.run(ExecJavaMojo.java:293)\n    at java.lang.Thread.run(Thread.java:745)\n17:41:23.300 [                        /] Unavailable javax.servlet.UnavailableException: com.google.refine.RefineServlet (6ms)\n17:41:23.302 [          org.mortbay.log] failed refine: java.lang.NullPointerException (2ms)\n17:41:23.339 [          org.mortbay.log] Failed startup of context org.mortbay.jetty.webapp.WebAppContext@41363d79{/,E:\\OpenRefine-3.1\\main\\webapp} (37ms)\njava.lang.NullPointerException\n    at java.lang.Class.isAssignableFrom(Native Method)\n    at org.mortbay.jetty.servlet.ServletHolder.doStart(ServletHolder.java:256)\n    at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n    at org.mortbay.jetty.servlet.ServletHandler.initialize(ServletHandler.java:736)\n    at org.mortbay.jetty.servlet.Context.startContext(Context.java:140)\n    at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282)\n    at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)\n    at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)\n    at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n    at org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:130)\n    at org.mortbay.jetty.Server.doStart(Server.java:224)\n    at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n    at com.google.refine.RefineServer.init(Refine.java:197)\n    at com.google.refine.Refine.init(Refine.java:109)\n    at com.google.refine.Refine.main(Refine.java:103)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at org.codehaus.mojo.exec.ExecJavaMojo$1.run(ExecJavaMojo.java:293)\n    at java.lang.Thread.run(Thread.java:745)\n17:41:23.350 [          org.mortbay.log] failed SocketConnector@127.0.0.1:3333: java.net.BindException: Address already in use: JVM_Bind (11ms)\n17:41:23.351 [          org.mortbay.log] failed RefineServer@1bd84225: java.net.BindException: Address already in use: JVM_Bind (1ms)\n17:41:23.351 [            refine_server] Failed to start server - is there another copy running already on this port/address? (0ms)\n[WARNING] \njava.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at org.codehaus.mojo.exec.ExecJavaMojo$1.run(ExecJavaMojo.java:293)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.BindException: Address already in use: JVM_Bind\n    at java.net.DualStackPlainSocketImpl.bind0(Native Method)\n    at java.net.DualStackPlainSocketImpl.socketBind(DualStackPlainSocketImpl.java:106)\n    at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:387)\n    at java.net.PlainSocketImpl.bind(PlainSocketImpl.java:190)\n    at java.net.ServerSocket.bind(ServerSocket.java:375)\n    at java.net.ServerSocket.(ServerSocket.java:237)\n    at org.mortbay.jetty.bio.SocketConnector.newServerSocket(SocketConnector.java:80)\n    at org.mortbay.jetty.bio.SocketConnector.open(SocketConnector.java:73)\n    at org.mortbay.jetty.AbstractConnector.doStart(AbstractConnector.java:283)\n    at org.mortbay.jetty.bio.SocketConnector.doStart(SocketConnector.java:147)\n    at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n    at org.mortbay.jetty.Server.doStart(Server.java:235)\n    at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n    at com.google.refine.RefineServer.init(Refine.java:197)\n    at com.google.refine.Refine.init(Refine.java:109)\n    at com.google.refine.Refine.main(Refine.java:103)\n    ... 6 more\n[WARNING] thread Thread[Timer-0,5,com.google.refine.Refine] was interrupted but is still alive after waiting at least 15000msecs\n[WARNING] thread Thread[Timer-0,5,com.google.refine.Refine] will linger despite being asked to die via interruption\n[WARNING] NOTE: 1 thread(s) did not finish despite being asked to  via interruption. This is not a problem with exec:java, it is a problem with the running code. Although not serious, it should be remedied.\n[WARNING] Couldn't destroy threadgroup org.codehaus.mojo.exec.ExecJavaMojo$IsolatedThreadGroup[name=com.google.refine.Refine,maxpri=10]\njava.lang.IllegalThreadStateException\n    at java.lang.ThreadGroup.destroy(ThreadGroup.java:778)\n    at org.codehaus.mojo.exec.ExecJavaMojo.execute(ExecJavaMojo.java:328)\n    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)\n    at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)\n    at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)\n    at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)\n    at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)\n    at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)\n    at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] OpenRefine ......................................... SUCCESS [  3.698 s]\n[INFO] OpenRefine - main .................................. SUCCESS [  0.949 s]\n[INFO] OpenRefine - server ................................ FAILURE [ 16.484 s]\n[INFO] OpenRefine - extensions ............................ SKIPPED\n[INFO] OpenRefine - Jython extension ...................... SKIPPED\n[INFO] OpenRefine - Wikidata extension .................... SKIPPED\n[INFO] OpenRefine - Database extension .................... SKIPPED\n[INFO] OpenRefine - Gdata extension ....................... SKIPPED\n[INFO] OpenRefine - PC-axis extension ..................... SKIPPED\n[INFO] OpenRefine - packaging ............................. SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 22.385 s\n[INFO] Finished at: 2019-03-19T17:41:38+08:00\n[INFO] Final Memory: 22M/1963M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.3:java (default-cli) on project server: An exception occured while executing the Java class. null: InvocationTargetException: Address already in use: JVM_Bind -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn  -rf :server\n. @ostephens What's the use of this Refine with extensions. launch file?. @ostephens Thank you for your teaching. I am grateful to you. I just took the chance to debug this project. Thank you very much.. ",
    "felixlohmeier": "Yes, I am sure. Maybe there is a leaner way. I tried to change as little as possible.\ncode\n\nREFINE_AUTOSAVE_PERIOD is loaded from refine.ini and relayed to the server app as parameter refine.autosave\nserver/src/com/google/refine/Refine.java reads and relay refine.autosave to the servlet app\nmain/src/com/google/refine/RefineServlet.java defaults AUTOSAVE_PERIOD to 5 minutes first. Then it reads parameter refine.autosave and overwrites AUTOSAVE_PERIOD if it is present.\nthe service is scheduled with parameter AUTOSAVE_PERIOD (no changes here)\n\ntest\nget the code, build and change autosave to 1 minute in refine.ini\ngit clone https://github.com/OpenRefine/OpenRefine.git\ncd OpenRefine\ngit fetch origin +refs/pull/1202/merge\ngit checkout FETCH_HEAD\n./refine build\nsed -i 's/#REFINE_AUTOSAVE_PERIOD=60/REFINE_AUTOSAVE_PERIOD=1/' refine.ini\nstart refine and load some data\n```\n[09:41 felix ~/.../test/OpenRefine ((0c40e93...) *)]$ ./refine\nStarting OpenRefine at 'http://127.0.0.1:3333/'\n09:41:54.789 [            refine_server] Starting Server bound to '127.0.0.1:3333' (0ms)\n09:41:54.791 [            refine_server] refine.memory size: 1400M JVM Max heap: 1304952832 (2ms)\n09:41:54.804 [            refine_server] Initializing context: '/' from '/home/felix/Desktop/test/OpenRefine/main/webapp' (13ms)\n09:41:54.842 [            refine_server] Starting autoreloading scanner...  (38ms)\n09:41:55.067 [                   refine] Starting OpenRefine 2.7 [TRUNK]... (225ms)\n09:41:59.067 [                   refine] POST /command/core/load-language (4000ms)\n09:41:59.096 [                   refine] POST /command/core/load-language (29ms)\n09:41:59.292 [                   refine] POST /command/core/get-importing-configuration (196ms)\n09:41:59.506 [                   refine] GET /command/core/get-all-project-metadata (214ms)\n09:41:59.600 [                   refine] GET /command/core/get-languages (94ms)\n09:41:59.732 [                   refine] GET /command/core/get-version (132ms)\n09:42:03.436 [                   refine] POST /command/core/create-importing-job (3704ms)\n09:42:03.462 [                   refine] POST /command/core/importing-controller (26ms)\n09:42:04.475 [                   refine] POST /command/core/get-importing-job-status (1013ms)\n09:42:04.570 [                   refine] POST /command/core/importing-controller (95ms)\n09:42:04.649 [                   refine] POST /command/core/importing-controller (79ms)\n09:42:04.674 [                   refine] POST /command/core/get-models (25ms)\n09:42:04.687 [                   refine] POST /command/core/get-rows (13ms)\n09:42:05.725 [                   refine] POST /command/core/importing-controller (1038ms)\n09:42:06.753 [                   refine] POST /command/core/get-importing-job-status (1028ms)\n09:42:07.604 [                   refine] POST /command/core/load-language (851ms)\n09:42:07.629 [                   refine] GET /command/core/get-preference (25ms)\n09:42:07.653 [                   refine] POST /command/core/load-language (24ms)\n09:42:07.706 [                   refine] GET /command/core/get-project-metadata (53ms)\n09:42:07.744 [                   refine] GET /command/core/get-models (38ms)\n09:42:07.918 [                   refine] GET /command/core/get-history (174ms)\n09:42:07.922 [                   refine] POST /command/core/get-rows (4ms)\n09:42:07.951 [                   refine] GET /command/core/get-history (29ms)\n09:42:55.101 [           ProjectManager] Saving some modified projects ... (47150ms)\n09:42:55.121 [        project_utilities] Saved project '2190681525239' (20ms)\n```\n(autosave after one minute). @thadguidry May I ask you to revert my changes to refine.bat? I am going to debug this but need more time to test it thoroughly.. @tfmorris quotes from https://groups.google.com/forum/#!topic/openrefine/7ESn_Xh0tRU. @jackyq2015 Yeah, thanks for the quick solution! I am afk until July, 16. Shall I close the issue now, so you can get your reward at bountysource?. Again, thank you very much @jackyq2015 for the great job!\nI wonder if this is a breaking change that needs to be addressed in upcoming release notes. The original proposal by @tfmorris in 2012 was to... \n\nextend it to take either a cell or a value\n\nThe new cross() function takes a string only, which is fine by me but breaks existing user transformation history and the VIB-BITS OpenRefine extension. Is there a chance to make the new cross() function backward compatible (to take either a cell or a value)?\nI added a note in the wiki to reflect this different behavior between the stable release (2.7) and the development version.. I think of OpenRefine projects that use the old cross function that expects cell.cross() and will be opened someday in a future release of OpenRefine that includes the new cross function that expects value.cross(). In this case the undo/redo function would not work properly.\nAdditionally, there are many tutorials out there (stackoverflow, google forum, ...) that suggest to use the cross function with cell.cross(). If they try them with the new version they will get an error.\n\nIs it possible to extend the cross function one step further to allow both forms of user input (cell and string)?\ncell.cross(\"My Address Book\", \"friend\")[0].cells[\"address\"].value\nvalue.cross(\"My Address Book\", \"friend\")[0].cells[\"address\"].value. I have another issue... maybe there is still a problem in the caching logic. I would like to use the new cross() function together with split and join:\nforEach(value.split(\",\"),v,v.cross(\"My Address Book\",\"friend\")[0].cells[\"address\"].value).join(\",\")\nThis works for \"mary,john\" but not for \"anne,mary\"\n\nIn this example, the value \"anne\" is not part of the recipient column. If I add a row for \"anne\" then \"anne,mary\" works too.\n\nI think this behavior is unexpected. It seems that the new cross() function still expects that the string input value is present as a single cell.. > ... can have the code also support cell if necessary\nI think that would be great to make it backwards compatible.\n\nThe cross function does not and should not aware of the custom operation(in your case, split).\n\nThank you @jackyq2015, now I understand the caching logic. The reason why I raised this issue was the idea that combining the split, cross and join in one expression could reduce total run time. Row operations (split/join multi-valued-cells, delete rows) are very expensive and often amount to more than 75% of total run time in my projects. Do you have an idea how to support custom operations (e.g. split) together with the cross function? I am willing to give another bounty if this helps.. Thank you @jackyq2015, it works.\nWe should update the integrated help too: line 85 in Cross.java\n\nwriter.key(\"params\"); writer.value(\"string s or cell c, string projectName, string columnName\");. Hi @jackyq2015 and ALL, may I ask again if there is a way to support multiple-value-cell-input in the cross function? I am willing to give another bounty. I had a look at the InterProjectModel again and I wonder if it could be possible to add an optional split function before computing the cache? From a user perspective, I would like to call the cross function with a separator as 4th argument, so that multiple values (separated by a custom separator, e.g. \u241f) in an input cell get \"crossed\". Multiple output values should be joined with the same separator.\n\nExample (see screenshots above):\n- Input: anne\u241fmary\n- Expression: cell.cross(\"My Address Book\", \"friend\",\"\u241f\")[0].cells[\"address\"].value\n- Output: 17 Morning Crescent\u241f50 Broadway Ave\nThe reason why I raised this issue was the idea that combining split, cross and join in one (cached) function could reduce total run time. Row operations (split/join multi-valued-cells, delete rows) are very expensive and often amount to more than 75% of total run time in my projects.. result for anne should be \"17 Morning Crescent\"; i am going to open a new issue with example data. issue #1202 (no separate bug ticket). example-data-1289.zip\n. I agree that it is quite hacky, but it is very useful and it works for all cases i can imagine. Before I have realized that the pull request got reverted I extended the documentation of the cross() function in the wiki, see https://github.com/OpenRefine/OpenRefine/wiki/GREL-Other-Functions (cross, note for dev version)\n@ostephens, I think there are many possibilities with the new operator and combined forEach, forNonBlank or value.split() operations. What do you think as a GREL pro :-) ?\nExample for splitting multi-value field values in from column and extract more than one value from the results:\nforEach(value.cross(\"My Address Book\", \"friend\", \",\"),r,forNonBlank(r.cells[\"address\"].value,v,v,\"\")).join(\"|\")\nExample for splitting multi-value field values in from column, extract only the first value from the results and return a custom string if there is a match in foreign key (\"friend\") but no value in target column (\"address\"):\nforEach(value.split(\",\"),v,forNonBlank(v.cross(\"My Address Book\", \"friend\", \",\")[0].cells[\"address\"].value,x,x,\"!\")).join(\"|\")\nSee example data attached: example-data-1289.zip\nI have tested the cross-func-split branch with quite \"big\" data: project A with 1.5 million rows and 100 columns, project B with 500.000 rows and 50 columns. Keys in project A are separated by a string. Without the new cross() function I need to split multi-valued cells first (and join after doing the cross). This process (split, cross, join) takes about 30 minutes on my machine. If I use the new cross() function with integrated split possibility it takes only seconds!. @ostephens write:\n\nAlso worth noting that with the change to use a string rather than just a 'cell' object for the lookup, the desired outcome can be achieved in GREL:\nforEach(value.split(\",\"),v,forEach(v.cross(\"My Address Book\",\"friend\"),r,r.cells[\"address\"].value).join(\",\")).join(\",\")\n\nThe old cross() function builds a \"cache\" containing all cell values in from and to column. The GREL above does not produce the desired outcome if there are keys in the from column that are only present in multi-value form (e.g. mary, anne in one cell but no cell containing mary alone -> mary will not be \"cached\"). See this example: https://github.com/OpenRefine/OpenRefine/issues/1204#issuecomment-316012444. Many thanks to ALL that you dive into my use case! I am happy with a more general approach and a S L O W D O W N :-). @claussni developed a solution that works for me, so it is not urgent for me anymore. But maybe the extended cross() functionality in #1294 is useful for someone else right now and could be merged until a more general approach is discussed and developed?\n@thadguidry: here are some stats...\nsystem\n\nVM with 4 XEON CPU E5-2660 v3 @ 2.60 GhZ and 98GB RAM\nOpenRefine 2.7\nXms: 70G ; Xmx: 70G \n\ninitial load base project:\n\n1521345 rows, 138 columns\nloaded project from disk in 50 sec(s)\nRSS after initial load: 32483896\n\ninitial load target project:\n\n457966 rows, 34 columns\nloaded project from disk in 3 sec(s)\n\nthe \"old\" way\nsplit multi-valued cells\n\ncommand: edit cells / split multi-valued cells\nrun time: 4m39s\nRSS after split: 38782472\n1864858 rows\n\ncross\n expression: forEach(cross(cell,\"A\",\"B\"),v,v.cells[\"C\"].value)[0]\n duration: 0m12s\njoin multi-valued cells\n command: edit cells / join multi-valued cells (for key column AND new column)\n run time: 4m43s + 4m33s = 9m16s\n RSS after join: 45576532\n 1521345 rows\ntotal run time: 4m39s + 0m12s + 9m16s = 14m7s\nhighest memory load: 45576532\nThere are several columns with foreign keys in the base project which multiplies run time in my use case by ~10.\nwith cross-func-split branch\ncross\n expression: forEach(value.split(\"\u241f\"),v,forNonBlank(v.cross(\"A\",\"B\",\"\u241f\")[0].cells[\"C\"].value,x,x,\"\u241e\")).join(\"\u241f\")\n run time: 0m09s\n* RSS after cross: 33681272 \nsplit into several columns\n\nsplit by separator: \u241f\nguess cell type and remove column unselected\nrun time: 4m02s\n40 new columns. @thadguidry There are up to 40 keys in the from column in the example above. If I use \"split into several columns\" I will get up to 40 new columns. I would have to apply the cross function 40 times (one transformation for each column) and concat resulting values into one column afterwards, right? Performance is only slightly better (see above).. Thank you @thadguidry, @jackyq2015, @ostephens and @wetneb. I understand your hesitation to put functionality into OpenRefine that is out of it's main scope. I am grateful for your comments.\n\nI work with library data (books, articles, etc.) and in this particular use case we built a discovery tool based on existing data that is only available in proprietary structured fixed-width text files. The project team (including non-tech-librarians) loves the opportunity to explore and experiment with their data in OpenRefine. We built an simple ETL workflow from these fixed-width text files to Apache Solr with OpenRefine, a python client for OpenRefine and some shell scripts (openrefine-batch.sh). This works great overall. Librarians can explore and manipulate data and transformation rules in a daily updated OpenRefine instance. Administration for the IT department in the library is easy. The templating feature allows us to build custom library metadata formats (e.g. dublin core xml). I am not keen to introduce more tools to the workflow. I want to keep it simple.\nI am aware of real ETL tools like Pentaho, although I have not much experience using it. I have used Pandas recently, which provides merge and join functions that would also handle this use case. There are library specific tools (without GUIs) like catmandu or metafacture. But as I said, I prefer to stick with OpenRefine. The code from @claussni works very well for this use case and solves the performance bottleneck in the workflow.\nWell, works for me != works for us... I thought that other people might be interested in this extended cross() function. I did not get the full picture why it is exactly troublesome to add #1294 to the code base but I understand the general approach to keep OpenRefine straightforward and maintainable.. Sounds great @thadguidry ! In the meantime i will maintain a fork that includes #1294 here:\n fork: https://github.com/felixlohmeier/OpenRefine\n documentation: https://github.com/felixlohmeier/OpenRefine/wiki\nI have reverted my changes to the documentation of the cross() function in the official OpenRefine wiki (and added a recipe from previous discussions from the mailing list).\nI will close this issue now. Thank you all for your help!. Following discussion results in https://github.com/OpenRefine/OpenRefine/issues/1289#issuecomment-340135928 I suggest to reject/close this pull request.\nI will maintain a fork that includes this pull request from @claussni here:\n fork: https://github.com/felixlohmeier/OpenRefine\n documentation: https://github.com/felixlohmeier/OpenRefine/wiki. Oops, looking at the screenshots again, I've just realized that I mxied up true/false in the Facet \"Blank rows\". Fixed with commit 9c55c7e\n\n. \n. good question... this is a leftover from my tests, i vote for 60. Yes, but the width of the menu (2nd level) seems to be fixed. The (longer) string \"Count non-blank values per column\" would cross the border line.. Alright, fixed with 2c266d3. ",
    "tom-h": "I also have this bug, assuming the same cause. Any incomplete/invalid regular expression causes the front end to hang (i.e., grey with \"waiting\"). The form of the data is irrelevant to trigger this. I think this has been a bug at least since the last RC. \nFor instance, while \"regular expression\" is ticked, typing an open parenthesis \"(\", square bracket \"[\"  (i.e., without content a closing parenthesis/bracket) will trigger this error. Similarly, typing an invalid regex and then clicking \"regular expression\" will also trigger the bug.\nGiven that regular expressions are frequently invalid while composing them, a simple fix might be to not send the regex with every keystroke, but rather after clicking \"submit\" or similar. Slightly better might be to use javascript to validate the regex before sending to the backend (Note that there are minor difference between the form of regexs in Javascript and Java).. ",
    "hpiedcoq": "@thadguidry Sorry Thad for my late answer to your message.\n@tom-h answered for me by repeating the bug. \nThat's exactly it.. I didn't make any change to this file.\nLine is commented. But to be sure, I'm going to make a fresh install (downloading from github is pretty slow today). Very strange indeed.. Yes you were right! fresh install did the trick!\nThanks jacky!. ",
    "claussni": "Implemented as optional 4th parameter to cross() function: https://github.com/claussni/OpenRefine/tree/cross-func-split\n@felixlohmeier can you test this, please? Here is the expression I used:\nforEach(value.cross(\"My Address Book\", \"friend\", \",\"),r,r.cells[\"address\"].value).join(\";\")\nThe result should be:\n\n. @thadguidry fixed it. This PR is in a limbo state now. I already deleted my branch and fork. Will see if I can reconnect a new branch.... @wetneb I started a new fork and branch, gradually fixing the issues you raised: https://github.com/claussni/OpenRefine/tree/cross-func-split-2\nI'm currently trying to improve unit test coverage for the new use case - which appears to be more challenging than the actual fix. @felixlohmeier provided more sample data for testing.\nA new PR is coming up and we can continue the discussion there, if that is ok?\n. ",
    "cyberandy": "I remove it with AppCleaner, and got it started again.. ",
    "charmygarg": "We have updated the script now the user will also have the choice to add their own IP address otherwise default [127.0.0.1] will be used. Please take a look now. We have removed open-refine.log and added entry for the same in .gitignore. Let me know if anything is needed to be changed.\nThanks . ",
    "shubhra02": "Thanks a lot @Thad\nOn Wed, Jul 26, 2017 at 7:46 PM, Thad Guidry notifications@github.com\nwrote:\n\nMerged #1208 https://github.com/OpenRefine/OpenRefine/pull/1208.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/1208#event-1179971502, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AX5CqKNg9l11yM31Z2tZseRVpmQYmuTkks5sR0oxgaJpZM4OYG6x\n.\n. \n",
    "nehaljwani": "I also found:\nhttp://repository.pentaho.org/content/groups/omni/com/oracle/appbundler/1.0/appbundler-1.0.jar\nhttp://repo.boundlessgeo.com/main/com/oracle/appbundler/1.0ea/appbundler-1.0ea.jar\nAlthough, both of them gave some error (Maybe I was using them wrong. I only edited refine and build.xml). ",
    "fmaali": "@thadguidry I guess it is an env problem... seeing this error on Travis:\nError: Error while uploading main/tests/data/food.csv to OpenRefine\ntests run fine locally. Is there a way to re-trigger the build?. thanks for looking after this @thadguidry . ",
    "paregorios": "FWIW, some sites prohibit anonymous and library-default user-agent strings because of past bad behavior by bots that don't identify themselves and that violate robots.txt directives or scrape so aggressively that site performance suffers. This has been our experience, prior to a recent upgrade, with the website of the Pleiades gazetteer (see intersecting discussion at #1265).\nIf there is interest, I can see if I can find Pleiades-related funds for a bounty for resolving this OpenRefine issue. Please advise.. Hmmm. I installed from DMG per docs. If there's java config that needs doing for non-developer users, I missed it in the docs. Pointer?. I have Java 1.8.0_144 installed.. Using keytool to install root cert in question, as implied by the link cited by @wetneb above, has made no difference for me. I am still seeing same errors, even after full restart and ensuring that \"cache results\" is turned off in OR. I note that the discussion on twitter seems to involve people running OR from the command line, presumably on linux. \nI have tried launching OR using the JavaAppLauncher distributed inside the Mac application bundle in the hopes of seeing an error notification or traceback at the command line when the error occurs. OR runs fine this way, and I do see application messages, but the errors still occur without any corresponding output on the command line.. @ostephens, regarding the 403s: yes, I believe you are correct. We have apache config that rejects certain default and blankish user agent strings because of bad behavior by some bots in the past. \n~~I'm going to undertake to remove (temporarily?)~~ I have removed this restriction from the Pleiades server. I will attempt to follow the steps you outline with the JCE to see if I can get things working. And I'll report back here to inform any follow-on software or documentation actions the OR development community wants to take. \nRegarding the apache rewrite rules: If we don't see significant performance degradation tied to anonymous bots, we might leave the exception in place, but ideally I'd like to see automated agents using specific user agents and so will follow the discussion on #1217. . @ostephens unfortunately I cannot yet duplicate your success. A diagnostic question:\n\nAre you running the OSX production 2.7 distro of OR, or are you building locally and running from the command-line or an IDE?. Yeah, so it looks like the OR 2.7 distro for OSX bundles jdk 1.8.0_60 and ignores the $JAVA_HOME variable. It seems to be hard-coded in OpenRefine.app/Contents/Info.plist.. According to this, the \"DST Root CA X3\" certificate, which is the parent to the \"Let's Encrypt\" certificate was only added to the default JDK/JRE keystore with versions 7u111 and 8u101. So, bundling jdk 1.8.0_101 or later might take care at least of the certificate problem (and not just for Pleiades). . @ostephens thanks for the roadmap. I am now fetching Pleiades JSON with my hacked version of the OSX OR 2.7 distro. Adding the pleiades cert (which was part of your debugging process, I realize) wasn't necessary, so long as the root cert from Let's Encrypt has been added. \n\nThis resolves my immediate issue, but -- if I may be so bold -- would seem to leave open some steps for the OR development community to explore with respect to the (next) OSX release. This workaround is probably too techno-fiddly for most of the users to whom I'd like to be able to recommend OpenRefine for Pleiades reconciliation. I am happy to help troubleshoot and/or test, if that would be useful, but I don't have java development skills. . ",
    "denim2x": "@wetneb PLease clarify: do you require PR-tabular-metadata conformance for this issue?. @wetneb Are you considering using Apache Arrow in OpenRefine?. @thadguidry @wetneb Any further information for working on this feature would be very appreciated. @ettorerizza It'd be nice if you could provide some mockups with the improved UI (with steps). ",
    "akbertram": "I see there are a few language implementation now - can anyone point me to how these are defined? Would the jython extension be a good model?. @ettorerizza N.B. Renjin tries to provide the best of both worlds - platform independence and full support for modules with C, C++ and Fortran code. We have a tool chain that compiles these languages to JVM bytecode so they can be used as normal JVM libraries without having to help your users navigate the setup of a Fortran compiler. Builds of the CRAN packages are published to http://packages.renjin.org. ",
    "narendravardi": "@thadguidry can i work on this?. @thadguidry any suggestions on where to make the changes?\nI have gone through the classes EngineDependentMassCellOperation and TextTransformOperation based on the message printed on the gif file. But I couldn't find out. Please help me out! :)\n. ",
    "codacy-bot": " Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 20\nAdded 13\n\nComplexity increasing per file\n\nmain/webapp/modules/core/scripts/dialogs/extend-data-preview-dialog.js  4\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 1\nAdded 5\n\nCoverage increased per file\n\nmain/src/com/google/refine/importers/TabularImportingParserBase.java  1\nmain/src/com/google/refine/model/Recon.java  2\nmain/src/com/google/refine/model/recon/StandardReconConfig.java  34\nmain/src/com/google/refine/model/recon/ReconJob.java  33\nmain/src/com/google/refine/importers/ImporterUtilities.java  4\n\nComplexity increasing per file\n\nmain/src/com/google/refine/importers/TextFormatGuesser.java  2\nmain/src/com/google/refine/model/recon/StandardReconConfig.java  2\nmain/webapp/modules/core/scripts/reconciliation/recon-manager.js  2\n\nComplexity decreasing per file\n\nmain/webapp/modules/core/scripts/index/parser-interfaces/preview-table.js  -1\n\nClones added\n\nmain/webapp/modules/core/scripts/index/parser-interfaces/preview-table.js  1\n\nClones removed\n\nmain/tests/server/src/com/google/refine/tests/recon/DataExtensionTests.java  -2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/logging/IndentingLayout.java  3\n\nComplexity decreasing per file\n\nmain/webapp/modules/core/scripts/project.js  -3\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/importers/WikitextImporter.java  1\n\nComplexity increasing per file\n\nmain/tests/server/src/com/google/refine/tests/importers/WikitextImporterTests.java  2\nmain/src/com/google/refine/importers/WikitextImporter.java  20\nmain/webapp/modules/core/scripts/index/parser-interfaces/wikitext-parser-ui.js  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/logging/IndentingLayout.java  1\nmain/src/com/google/refine/clustering/binning/FingerprintKeyer.java  3\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 5\n\nComplexity increasing per file\n\nmain/webapp/modules/core/scripts/index.js  4\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/clustering/binning/FingerprintKeyer.java  3\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 3\nAdded 5\n\nComplexity increasing per file\n\nmain/src/com/google/refine/commands/cell/SplitMultiValueCellsCommand.java  2\nmain/webapp/modules/core/scripts/views/data-table/menu-edit-cells.js  5\nmain/src/com/google/refine/operations/cell/MultiValuedCellSplitOperation.java  9\n\nClones added\n\nmain/webapp/modules/core/scripts/views/data-table/menu-edit-cells.js  9\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 5\n\nComplexity decreasing per file\n\nmain/webapp/modules/core/scripts/project/process-panel.js  -1\nmain/webapp/modules/core/scripts/dialogs/clustering-dialog.js  -2\nmain/webapp/modules/core/scripts/util/misc.js  -3\n\nClones removed\n\nmain/webapp/modules/core/scripts/views/data-table/menu-edit-column.js  -3\nmain/webapp/modules/core/scripts/views/data-table/data-table-view.js  -1\nmain/webapp/modules/core/scripts/views/data-table/menu-edit-cells.js  -1\nmain/webapp/modules/core/scripts/index/parser-interfaces/fixed-width-parser-ui.js  -1\nmain/webapp/modules/core/scripts/index/parser-interfaces/separator-based-parser-ui.js  -1\nmain/webapp/modules/core/scripts/views/data-table/cell-ui.js  -2\nmain/webapp/modules/core/scripts/dialogs/expression-preview-dialog.js  -2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 1\nAdded 1\n\nComplexity increasing per file\n\nmain/src/com/google/refine/expr/functions/Cross.java  1\nmain/src/com/google/refine/InterProjectModel.java  8\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nmain/webapp/modules/core/scripts/project.js  1\nmain/webapp/modules/core/scripts/project/browsing-engine.js  2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 1\nAdded 1\n\nComplexity increasing per file\n\nmain/src/com/google/refine/expr/functions/Cross.java  4\nmain/src/com/google/refine/InterProjectModel.java  7\nmain/tests/server/src/com/google/refine/tests/expr/functions/CrossFunctionTests.java  6\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/logging/IndentingLayout.java  2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 6\nAdded 20\n\nComplexity increasing per file\n\nmain/src/com/google/refine/importers/TabularImportingParserBase.java  1\nmain/src/com/google/refine/importers/ImportingParserBase.java  2\nmain/src/com/google/refine/ProjectMetadata.java  32\nmain/src/com/google/refine/io/FileProjectManager.java  9\nmain/webapp/modules/core/scripts/index/open-project-ui.js  15\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 3\n\nComplexity decreasing per file\n\nmain/webapp/modules/core/scripts/index/open-project-ui.js  -6\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 11\nAdded 2\n\nComplexity increasing per file\n\nmain/webapp/modules/core/scripts/facets/text-search-facet.js  3\n\nClones added\n\nmain/webapp/modules/core/scripts/facets/list-facet.js  2\nmain/webapp/modules/core/scripts/facets/text-search-facet.js  3\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 20\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nmain/webapp/modules/core/scripts/preferences.js  2\nmain/webapp/modules/core/scripts/index/open-project-ui.js  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nmain/src/com/google/refine/importers/ExcelImporter.java  1\nmain/src/com/google/refine/importers/OdsImporter.java  2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 2\n\nComplexity increasing per file\n\nmain/src/com/google/refine/ProjectManager.java  15\n\nComplexity decreasing per file\n\nmain/src/com/google/refine/io/FileProjectManager.java  -12\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 9\n\nComplexity increasing per file\n\nmain/src/com/google/refine/commands/project/DeleteProjectCommand.java  3\n\nComplexity decreasing per file\n\nmain/tests/server/src/com/google/refine/tests/operations/cell/JoinMultiValuedCellsTests.java  -5\nmain/tests/server/src/com/google/refine/tests/browsing/facets/TextSearchFacetTests.java  -7\nmain/webapp/modules/core/scripts/index/edit-metadata-dialog.js  -10\nmain/src/com/google/refine/importing/ImportingUtilities.java  -145\nmain/webapp/modules/core/scripts/index/default-importing-controller/parsing-panel.js  -14\nmain/tests/server/src/com/google/refine/tests/ProjectManagerTests.java  -19\nmain/src/com/google/refine/ProjectMetadata.java  -56\nmain/src/com/google/refine/io/FileProjectManager.java  -53\nmain/webapp/modules/core/scripts/index/default-importing-controller/controller.js  -24\nmain/webapp/modules/core/scripts/index/open-project-ui.js  -25\nmain/src/com/google/refine/ProjectManager.java  -58\n\nClones added\n\nmain/webapp/modules/core/scripts/index/open-project-ui.js  4\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/logging/IndentingLayout.java  2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/logging/IndentingLayout.java  2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity decreasing per file\n\nmain/src/com/google/refine/browsing/facets/TextSearchFacet.java  -14\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/logging/IndentingLayout.java  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 1\n\nComplexity decreasing per file\n\nmain/src/com/google/refine/model/Recon.java  -29\nmain/webapp/modules/core/scripts/views/data-table/menu-reconcile.js  -7\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity decreasing per file\n\nmain/src/com/google/refine/commands/row/GetRowsCommand.java  -15\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 1\nAdded 1\n\nComplexity decreasing per file\n\nmain/tests/server/src/com/google/refine/tests/operations/cell/JoinMultiValuedCellsTests.java  -5\nmain/tests/server/src/com/google/refine/tests/browsing/facets/TextSearchFacetTests.java  -7\nmain/src/com/google/refine/io/ProjectMetadataUtilities.java  -9\nmain/src/com/google/refine/expr/functions/ToDate.java  -3\nmain/tests/server/src/com/google/refine/tests/ProjectManagerTests.java  -19\nmain/tests/server/src/com/google/refine/tests/exporters/CsvExporterTests.java  -20\nmain/src/com/google/refine/ProjectMetadata.java  -56\nmain/src/com/google/refine/util/JSONUtilities.java  -68\nmain/src/com/google/refine/model/recon/StandardReconConfig.java  -2\nmain/tests/server/src/com/google/refine/tests/util/ParsingUtilitiesTests.java  -5\nmain/src/com/google/refine/model/Project.java  -22\nmain/src/com/google/refine/commands/expr/PreviewExpressionCommand.java  -19\nmain/src/com/google/refine/ProjectManager.java  -58\nmain/src/com/google/refine/util/ParsingUtilities.java  -22\nmain/src/com/google/refine/model/Cell.java  -22\nmain/src/com/google/refine/history/HistoryEntry.java  -16\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 2\n\nComplexity decreasing per file\n\nmain/tests/server/src/com/google/refine/tests/model/UrlFetchingTests.java  -4\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 54\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 3\nAdded 16\n\nComplexity increasing per file\n\nmain/webapp/modules/core/scripts/index/default-importing-controller/file-selection-panel.js  1\nmain/src/com/google/refine/model/Column.java  1\n\nComplexity decreasing per file\n\nmain/src/com/google/refine/importing/ImportingUtilities.java  -9\nmain/src/com/google/refine/commands/GetPreferenceCommand.java  -1\nmain/src/com/google/refine/commands/SetPreferenceCommand.java  -1\nmain/src/com/google/refine/io/ProjectUtilities.java  -2\n\nClones added\n\nmain/tests/server/src/com/google/refine/tests/browsing/facets/TextSearchFacetTests.java  1\nmain/tests/server/src/com/google/refine/tests/operations/cell/TransposeTests.java  1\nmain/tests/server/src/com/google/refine/tests/exporters/HtmlExporterTests.java  1\nmain/webapp/modules/core/scripts/index/default-importing-sources/sources.js  3\nmain/tests/server/src/com/google/refine/tests/exporters/XlsExporterTests.java  1\nmain/tests/server/src/com/google/refine/tests/exporters/TemplatingExporterTests.java  1\nmain/webapp/modules/core/scripts/project/exporters.js  6\n\nClones removed\n\nmain/tests/server/src/com/google/refine/tests/commands/project/SetProjectMetadataCommandTests.java  -2\nmain/webapp/modules/core/scripts/index/edit-metadata-dialog.js  -2\nmain/tests/server/src/com/google/refine/tests/importers/JsonImporterTests.java  -2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/logging/IndentingLayout.java  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/logging/IndentingLayout.java  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/logging/IndentingLayout.java  1\n\nComplexity decreasing per file\n\nmain/src/com/google/refine/logging/IndentingLayout.java  -4\nmain/webapp/modules/core/scripts/dialogs/extend-data-preview-dialog.js  -49\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 54\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 4\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 2\n\nComplexity increasing per file\n\nmain/src/com/google/refine/commands/project/GetModelsCommand.java  1\nmain/src/com/google/refine/operations/column/ColumnAdditionByFetchingURLsOperation.java  3\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 3\nAdded 2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 44\nAdded 5\n\nCoverage increased per file\n\nmain/src/com/google/refine/preference/PreferenceStore.java  7\nmain/src/com/google/refine/RefineServlet.java  9\nmain/src/com/google/refine/browsing/Engine.java  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/logging/IndentingLayout.java  2\n\nComplexity decreasing per file\n\nextensions/database/test/com/google/refine/extension/database/cmd/ExecuteQueryCommandTest.java  -1\nextensions/database/test/com/google/refine/extension/database/cmd/TestQueryCommandTest.java  -1\nextensions/database/test/com/google/refine/extension/database/mysql/MySQLConnectionManagerTest.java  -1\nextensions/database/test/com/google/refine/extension/database/DatabaseServiceTest.java  -1\nextensions/database/test/com/google/refine/extension/database/cmd/TestConnectCommandTest.java  -1\nextensions/database/test/com/google/refine/extension/database/pgsql/PgSQLDatabaseServiceTest.java  -1\nextensions/database/test/com/google/refine/extension/database/cmd/ConnectCommandTest.java  -1\nextensions/database/test/com/google/refine/extension/database/mysql/MySQLDatabaseServiceTest.java  -1\nextensions/database/test/com/google/refine/extension/database/mariadb/MariaDBConnectionManagerTest.java  -1\nextensions/database/test/com/google/refine/extension/database/pgsql/PgSQLConnectionManagerTest.java  -1\nextensions/database/test/com/google/refine/extension/database/mariadb/MariaDBDatabaseServiceTest.java  -1\n\nClones added\n\nextensions/database/test/com/google/refine/extension/database/cmd/TestConnectCommandTest.java  1\nextensions/database/test/com/google/refine/extension/database/cmd/ConnectCommandTest.java  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/importers/WikitextImporter.java  1\nmain/src/com/google/refine/importers/TabularImportingParserBase.java  1\n\nComplexity increasing per file\n\nmain/webapp/modules/core/scripts/index/parser-interfaces/wikitext-parser-ui.js  1\n\nClones added\n\nmain/tests/server/src/com/google/refine/tests/importers/WikitextImporterTests.java  4\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 1\n\nCoverage increased per file\n\nmain/src/com/google/refine/importers/SeparatorBasedImporter.java  3\n\nClones added\n\nmain/tests/server/src/com/google/refine/tests/importers/TsvCsvImporterTests.java  16\n\nClones removed\n\nmain/webapp/modules/core/scripts/index/parser-interfaces/separator-based-parser-ui.js  -1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nmain/src/com/google/refine/expr/functions/ToDate.java  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 4\n\nCoverage increased per file\n\nmain/src/com/google/refine/importing/ImportingUtilities.java  5\nmain/src/com/google/refine/util/JSONUtilities.java  1\nmain/src/com/google/refine/model/ColumnModel.java  1\nmain/src/com/google/refine/operations/column/ColumnAdditionByFetchingURLsOperation.java  5\nmain/src/com/google/refine/importing/ImportingJob.java  2\nmain/src/com/google/refine/model/Column.java  12\nmain/src/com/google/refine/importing/ImportingManager.java  6\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/logging/IndentingLayout.java  2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/model/recon/StandardReconConfig.java  6\n\nComplexity increasing per file\n\nmain/src/com/google/refine/model/recon/StandardReconConfig.java  10\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/importing/ImportingUtilities.java  5\nmain/src/com/google/refine/util/JSONUtilities.java  1\nmain/src/com/google/refine/model/recon/StandardReconConfig.java  6\nmain/src/com/google/refine/model/ColumnModel.java  1\nmain/src/com/google/refine/importing/ImportingJob.java  2\nmain/src/com/google/refine/model/Column.java  12\nmain/src/com/google/refine/model/Cell.java  3\nmain/src/com/google/refine/importing/ImportingManager.java  6\n\nCoverage decreased per file\n\nmain/src/com/google/refine/util/ParsingUtilities.java  -1\n\nComplexity increasing per file\n\nmain/tests/server/src/com/google/refine/tests/exporters/CsvExporterTests.java  1\nmain/src/com/google/refine/util/ParsingUtilities.java  2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 2\nAdded 4\n\nCoverage increased per file\n\nmain/src/com/google/refine/importers/WikitextImporter.java  1\nmain/src/com/google/refine/importing/ImportingUtilities.java  5\nmain/src/com/google/refine/logging/IndentingLayout.java  1\nmain/src/com/google/refine/importers/TabularImportingParserBase.java  1\nmain/src/com/google/refine/importers/SeparatorBasedImporter.java  3\nmain/src/com/google/refine/expr/functions/ToNumber.java  2\nmain/src/com/google/refine/util/JSONUtilities.java  1\nmain/src/com/google/refine/model/recon/StandardReconConfig.java  6\nmain/src/com/google/refine/model/ColumnModel.java  1\nmain/src/com/google/refine/importing/ImportingJob.java  2\nmain/src/com/google/refine/model/Column.java  12\nmain/src/com/google/refine/importing/ImportingManager.java  6\nmain/src/com/google/refine/expr/ExpressionUtils.java  12\n\nCoverage decreased per file\n\nmain/src/com/google/refine/io/FileProjectManager.java  -1\n\nComplexity increasing per file\n\nextensions/gdata/src/com/google/refine/extension/gdata/TokenCookie.java  1\nextensions/gdata/src/com/google/refine/extension/gdata/GDataImportingController.java  2\n\nComplexity decreasing per file\n\nextensions/gdata/src/com/google/refine/extension/gdata/UploadCommand.java  -4\nextensions/gdata/src/com/google/refine/extension/gdata/GDataImporter.java  -3\n\nClones removed\n\nmain/webapp/modules/core/scripts/project/exporters.js  -3\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nmain/webapp/modules/core/scripts/reconciliation/standard-service-panel.js  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nmain/src/com/google/refine/operations/column/ColumnAdditionByFetchingURLsOperation.java  3\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/model/Cell.java  3\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nClones removed\n\nmain/webapp/modules/core/scripts/views/data-table/cell-ui.js  -1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 13\n\nComplexity increasing per file\n\nmain/src/com/google/refine/exporters/CustomizableTabularExporterUtilities.java  12\nmain/src/com/google/refine/commands/project/ExportRowsCommand.java  4\n\nClones added\n\nmain/webapp/modules/core/scripts/project/exporters.js  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nmain/src/com/google/refine/expr/functions/strings/Contains.java  5\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/importers/RdfTripleImporter.java  3\nmain/src/com/google/refine/logging/IndentingLayout.java  5\n\nComplexity increasing per file\n\nmain/src/com/google/refine/importers/RdfTripleImporter.java  2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/model/changes/MassCellChange.java  25\nmain/src/com/google/refine/operations/recon/ReconJudgeSimilarCellsOperation.java  41\nmain/src/com/google/refine/InterProjectModel.java  6\nmain/src/com/google/refine/operations/EngineDependentMassCellOperation.java  73\nmain/src/com/google/refine/model/recon/StandardReconConfig.java  1\nmain/src/com/google/refine/model/changes/CellChange.java  13\nmain/src/com/google/refine/model/Column.java  1\nmain/src/com/google/refine/model/changes/ReconChange.java  19\nmain/src/com/google/refine/model/ReconStats.java  3\n\nComplexity increasing per file\n\nmain/src/com/google/refine/operations/recon/ReconJudgeSimilarCellsOperation.java  2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/expr/functions/ToDate.java  6\nmain/src/com/google/refine/expr/util/CalendarParserException.java  50\nmain/src/com/google/refine/expr/util/CalendarParser.java  3\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/importers/RdfTripleImporter.java  3\nmain/src/com/google/refine/browsing/util/FilteredRecordsAsFilteredRows.java  100\nmain/src/com/google/refine/expr/functions/strings/Contains.java  53\nmain/src/com/google/refine/model/changes/MassCellChange.java  26\nmain/src/com/google/refine/browsing/util/RowVisitorAsRecordVisitor.java  90\nmain/src/com/google/refine/model/RecordModel.java  2\nmain/src/com/google/refine/InterProjectModel.java  6\nmain/src/com/google/refine/operations/cell/BlankDownOperation.java  70\nmain/src/com/google/refine/operations/EngineDependentMassCellOperation.java  84\nmain/src/com/google/refine/browsing/Engine.java  4\nmain/src/com/google/refine/browsing/util/ConjunctiveFilteredRecords.java  68\nmain/src/com/google/refine/operations/cell/FillDownOperation.java  71\nmain/src/com/google/refine/model/changes/CellChange.java  13\n\nComplexity increasing per file\n\nmain/src/com/google/refine/operations/cell/BlankDownOperation.java  2\nmain/src/com/google/refine/operations/EngineDependentMassCellOperation.java  1\nmain/src/com/google/refine/browsing/Engine.java  3\nmain/src/com/google/refine/operations/cell/FillDownOperation.java  2\n\nClones removed\n\nmain/src/com/google/refine/operations/cell/BlankDownOperation.java  -1\nmain/src/com/google/refine/operations/cell/FillDownOperation.java  -1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/logging/IndentingLayout.java  2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nClones removed\n\nmain/webapp/modules/core/scripts/project/exporters.js  -2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nmain/webapp/modules/core/scripts/views/data-table/menu-reconcile.js  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nmain/src/com/google/refine/expr/ExpressionUtils.java  16\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nmain/src/com/google/refine/commands/recon/PreviewExtendDataCommand.java  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/logging/IndentingLayout.java  3\nmain/src/com/google/refine/expr/functions/date/DatePart.java  83\n\nComplexity increasing per file\n\nmain/src/com/google/refine/expr/functions/date/DatePart.java  3\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nextensions/wikidata/src/org/openrefine/wikidata/schema/WbQuantityExpr.java  1\nextensions/wikidata/src/org/openrefine/wikidata/schema/WbLocationVariable.java  1\nextensions/wikidata/src/org/openrefine/wikidata/schema/WbDateVariable.java  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/browsing/util/FilteredRecordsAsFilteredRows.java  100\nmain/src/com/google/refine/expr/functions/strings/ParseJson.java  28\nmain/src/com/google/refine/model/changes/MassCellChange.java  26\nmain/src/com/google/refine/expr/functions/date/Inc.java  72\nmain/src/com/google/refine/browsing/util/RowVisitorAsRecordVisitor.java  90\nmain/src/com/google/refine/expr/functions/strings/Diff.java  5\nmain/src/com/google/refine/model/RecordModel.java  2\nmain/src/com/google/refine/exporters/XlsExporter.java  8\nmain/src/com/google/refine/operations/recon/ReconJudgeSimilarCellsOperation.java  41\nmain/src/com/google/refine/grel/Parser.java  10\nmain/src/com/google/refine/util/JSONUtilities.java  2\nmain/src/com/google/refine/InterProjectModel.java  6\nmain/src/com/google/refine/operations/cell/BlankDownOperation.java  70\nmain/src/com/google/refine/operations/EngineDependentMassCellOperation.java  84\nmain/src/com/google/refine/model/recon/StandardReconConfig.java  2\nmain/src/com/google/refine/browsing/Engine.java  4\nmain/src/com/google/refine/browsing/util/ConjunctiveFilteredRecords.java  68\nmain/src/com/google/refine/expr/functions/Get.java  29\nmain/src/com/google/refine/util/StringUtils.java  2\nmain/src/com/google/refine/exporters/CustomizableTabularExporterUtilities.java  10\nmain/src/com/google/refine/expr/functions/date/Now.java  22\nmain/src/com/google/refine/operations/cell/FillDownOperation.java  71\nmain/src/com/google/refine/model/changes/CellChange.java  13\nmain/src/com/google/refine/expr/util/CalendarParser.java  9\nmain/src/com/google/refine/util/ParsingUtilities.java  2\nmain/src/com/google/refine/model/Column.java  1\nmain/src/com/google/refine/grel/Scanner.java  6\nmain/src/com/google/refine/model/changes/ReconChange.java  19\nmain/src/com/google/refine/grel/ast/FieldAccessorExpr.java  24\nmain/src/com/google/refine/model/ReconStats.java  3\n\nCoverage decreased per file\n\nmain/src/com/google/refine/model/Cell.java  -1\n\nComplexity increasing per file\n\nmain/src/com/google/refine/expr/functions/date/Inc.java  6\n\nComplexity decreasing per file\n\nmain/src/com/google/refine/sorting/DateCriterion.java  -1\nmain/src/com/google/refine/expr/functions/ToDate.java  -2\nmain/src/com/google/refine/expr/functions/strings/Diff.java  -5\nmain/src/com/google/refine/browsing/filters/ExpressionTimeComparisonRowFilter.java  -1\nmain/src/com/google/refine/expr/functions/Type.java  -1\nmain/src/com/google/refine/exporters/XlsExporter.java  -1\nmain/src/com/google/refine/exporters/OdsExporter.java  -1\nmain/src/com/google/refine/browsing/util/TimeBinIndex.java  -3\nmain/src/com/google/refine/util/JSONUtilities.java  -1\nmain/src/com/google/refine/expr/functions/ToString.java  -3\nmain/src/com/google/refine/util/StringUtils.java  -3\nmain/src/com/google/refine/sorting/NumberCriterion.java  -1\nmain/src/com/google/refine/browsing/util/ExpressionTimeValueBinner.java  -1\n\nClones added\n\nmain/tests/server/src/com/google/refine/tests/expr/functions/CoalesceTests.java  1\nmain/tests/server/src/com/google/refine/tests/exporters/CsvExporterTests.java  1\nmain/tests/server/src/com/google/refine/tests/expr/functions/strings/ToFromConversionTests.java  1\nmain/tests/server/src/com/google/refine/tests/expr/functions/strings/DiffTests.java  1\n\nClones removed\n\nmain/tests/server/src/com/google/refine/tests/expr/functions/date/DatePartTests.java  -10\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/logging/IndentingLayout.java  2\nmain/src/com/google/refine/expr/functions/date/Inc.java  72\nmain/src/com/google/refine/model/changes/DataExtensionChange.java  44\nmain/src/com/google/refine/expr/functions/strings/Diff.java  5\nmain/src/com/google/refine/exporters/XlsExporter.java  8\nmain/src/com/google/refine/model/recon/StandardReconConfig.java  1\nmain/src/com/google/refine/util/Pool.java  2\nmain/src/com/google/refine/util/StringUtils.java  2\nmain/src/com/google/refine/model/ReconType.java  34\nmain/src/com/google/refine/expr/functions/date/Now.java  22\nmain/src/com/google/refine/model/Row.java  27\nmain/src/com/google/refine/expr/util/CalendarParser.java  9\nmain/src/com/google/refine/util/ParsingUtilities.java  2\nmain/src/com/google/refine/model/Cell.java  34\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/expr/EvalError.java  8\nmain/src/com/google/refine/operations/column/ColumnAdditionByFetchingURLsOperation.java  5\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/model/changes/MassCellChange.java  9\nmain/src/com/google/refine/model/Recon.java  11\nmain/src/com/google/refine/commands/recon/ReconJudgeOneCellCommand.java  66\nmain/src/com/google/refine/util/Pool.java  22\nmain/src/com/google/refine/process/ProcessManager.java  4\nmain/src/com/google/refine/process/QuickHistoryEntryProcess.java  5\nmain/src/com/google/refine/model/Cell.java  5\nmain/src/com/google/refine/history/HistoryEntry.java  13\nmain/src/com/google/refine/model/changes/ReconChange.java  5\n\nComplexity increasing per file\n\nmain/src/com/google/refine/operations/recon/ReconMarkNewTopicsOperation.java  1\nmain/src/com/google/refine/commands/recon/ReconJudgeOneCellCommand.java  2\n\nClones removed\n\nmain/src/com/google/refine/commands/recon/ReconJudgeOneCellCommand.java  -1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nextensions/wikidata/tests/src/org/openrefine/wikidata/qa/MockConstraintFetcher.java  1\nextensions/wikidata/src/org/openrefine/wikidata/qa/scrutinizers/SingleValueScrutinizer.java  1\nextensions/wikidata/src/org/openrefine/wikidata/schema/WbItemVariable.java  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity decreasing per file\n\nextensions/wikidata/src/org/openrefine/wikidata/qa/scrutinizers/RestrictedPositionScrutinizer.java  -3\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity decreasing per file\n\nmain/src/com/google/refine/importing/ImportingUtilities.java  -1\nmain/src/com/google/refine/importers/ExcelImporter.java  -4\nmain/src/com/google/refine/ProjectManager.java  -2\nmain/src/com/google/refine/importers/LineBasedImporter.java  -6\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 6\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nmain/src/com/google/refine/browsing/DecoratedValue.java  1\n\nComplexity decreasing per file\n\nmain/src/com/google/refine/operations/cell/MassEditOperation.java  -10\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nmain/src/com/google/refine/browsing/facets/ListFacet.java  6\nmain/webapp/modules/core/scripts/facets/list-facet.js  9\nmain/src/com/google/refine/browsing/util/ExpressionNominalValueGrouper.java  2\n\nClones added\n\nmain/tests/server/src/com/google/refine/tests/browsing/util/ExpressionNominalValueGrouperTests.java  2\nmain/webapp/modules/core/scripts/facets/list-facet.js  2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/browsing/DecoratedValue.java  10\nmain/src/com/google/refine/expr/EvalError.java  8\nmain/src/com/google/refine/operations/column/ColumnAdditionByFetchingURLsOperation.java  4\n\nComplexity increasing per file\n\nmain/src/com/google/refine/operations/column/ColumnAdditionByFetchingURLsOperation.java  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 1\n\nComplexity increasing per file\n\nextensions/wikidata/module/scripts/menu-bar-extension.js  2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/logging/IndentingLayout.java  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/logging/IndentingLayout.java  1\nmain/src/com/google/refine/browsing/DecoratedValue.java  10\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/exporters/sql/SqlInsertBuilder.java  2\nmain/src/com/google/refine/exporters/sql/SqlCreateBuilder.java  5\nmain/src/com/google/refine/operations/row/RowReorderOperation.java  54\nmain/src/com/google/refine/sorting/Criterion.java  46\nmain/src/com/google/refine/model/changes/RowReorderChange.java  28\nmain/src/com/google/refine/sorting/SortingRowVisitor.java  100\nmain/src/com/google/refine/exporters/CustomizableTabularExporterUtilities.java  1\nmain/src/com/google/refine/sorting/NumberCriterion.java  71\nmain/src/com/google/refine/sorting/BaseSorter.java  80\n\nComplexity decreasing per file\n\nmain/src/com/google/refine/sorting/NumberCriterion.java  -13\nmain/src/com/google/refine/sorting/BooleanCriterion.java  -3\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nmain/src/com/google/refine/exporters/sql/SqlInsertBuilder.java  3\n\nClones added\n\nmain/tests/server/src/com/google/refine/tests/exporters/sql/SqlExporterTests.java  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/exporters/sql/SqlCreateBuilder.java  5\nmain/src/com/google/refine/exporters/sql/SqlCreateBuilder.java  5\nmain/src/com/google/refine/operations/row/RowReorderOperation.java  54\nmain/src/com/google/refine/operations/row/RowReorderOperation.java  54\nmain/src/com/google/refine/sorting/Criterion.java  46\nmain/src/com/google/refine/sorting/Criterion.java  46\nmain/src/com/google/refine/model/changes/RowReorderChange.java  28\nmain/src/com/google/refine/model/changes/RowReorderChange.java  28\nmain/src/com/google/refine/exporters/HtmlTableExporter.java  9\nmain/src/com/google/refine/exporters/HtmlTableExporter.java  9\nmain/src/com/google/refine/exporters/CustomizableTabularExporterUtilities.java  2\nmain/src/com/google/refine/exporters/CustomizableTabularExporterUtilities.java  2\nmain/src/com/google/refine/expr/functions/strings/Unescape.java  47\nmain/src/com/google/refine/expr/functions/strings/Unescape.java  47\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/logging/IndentingLayout.java  3\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/exporters/CustomizableTabularExporterUtilities.java  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/model/metadata/ProjectMetadata.java  16\nmain/src/com/google/refine/model/metadata/AbstractMetadata.java  35\nmain/src/com/google/refine/grel/controls/IsEmptyString.java  25\nmain/src/com/google/refine/expr/functions/Coalesce.java  43\nmain/src/com/google/refine/expr/functions/strings/Range.java  5\nmain/src/com/google/refine/expr/functions/strings/Find.java  33\nmain/src/com/google/refine/expr/functions/strings/EndsWith.java  46\nmain/src/com/google/refine/operations/row/RowReorderOperation.java  24\nmain/src/com/google/refine/expr/functions/math/Floor.java  60\nmain/src/com/google/refine/expr/functions/strings/Match.java  27\nmain/src/com/google/refine/expr/functions/math/ATan.java  60\nmain/src/com/google/refine/operations/OperationRegistry.java  75\nmain/src/com/google/refine/expr/functions/math/Degrees.java  60\nmain/src/com/google/refine/operations/recon/ReconMatchSpecificTopicOperation.java  52\nmain/src/com/google/refine/expr/functions/booleans/And.java  47\nmain/src/com/google/refine/expr/functions/strings/Contains.java  30\nmain/src/com/google/refine/expr/functions/math/LeastCommonMultiple.java  31\nmain/src/com/google/refine/io/ProjectMetadataUtilities.java  25\nmain/src/com/google/refine/expr/functions/html/ParseHtml.java  50\nmain/src/com/google/refine/preference/PreferenceStore.java  5\nmain/src/com/google/refine/operations/column/ColumnSplitOperation.java  30\nmain/src/com/google/refine/expr/functions/arrays/Reverse.java  20\nmain/src/com/google/refine/expr/functions/math/Tanh.java  60\nmain/src/com/google/refine/expr/functions/ToDate.java  9\nmain/src/com/google/refine/expr/functions/math/Ln.java  60\nmain/src/com/google/refine/expr/functions/strings/ParseJson.java  43\nmain/src/com/google/refine/expr/functions/strings/SHA1.java  50\nmain/src/com/google/refine/model/Recon.java  36\nmain/src/com/google/refine/expr/functions/math/Odd.java  50\nmain/src/com/google/refine/history/History.java  10\nmain/src/com/google/refine/expr/functions/math/Max.java  50\nmain/src/com/google/refine/expr/functions/date/Inc.java  18\nmain/src/com/google/refine/browsing/facets/NominalFacetChoice.java  67\nmain/src/com/google/refine/expr/functions/HasField.java  33\nmain/src/com/google/refine/expr/functions/math/Pow.java  50\nmain/src/com/google/refine/operations/recon/ExtendDataOperation.java  17\nmain/src/com/google/refine/expr/functions/ToNumber.java  32\nmain/src/com/google/refine/operations/recon/ReconMarkNewTopicsOperation.java  27\nmain/src/com/google/refine/expr/functions/strings/Diff.java  13\nmain/src/com/google/refine/expr/functions/strings/Split.java  24\nmain/src/com/google/refine/expr/functions/math/Sinh.java  60\nmain/src/com/google/refine/expr/functions/booleans/Not.java  50\nmain/src/com/google/refine/operations/row/RowRemovalOperation.java  32\nmain/src/com/google/refine/model/RecordModel.java  5\nmain/src/com/google/refine/expr/functions/strings/NGramFingerprint.java  26\nmain/src/com/google/refine/expr/functions/math/Abs.java  60\nmain/src/com/google/refine/process/LongRunningProcess.java  42\nmain/src/com/google/refine/expr/functions/booleans/Or.java  47\nmain/src/com/google/refine/expr/functions/math/Min.java  50\nmain/src/com/google/refine/expr/functions/strings/SplitByCharType.java  46\nmain/src/com/google/refine/expr/functions/Type.java  26\nmain/src/com/google/refine/grel/controls/If.java  37\nmain/src/com/google/refine/expr/functions/Jsonize.java  21\nmain/src/com/google/refine/expr/functions/FacetCount.java  22\nmain/src/com/google/refine/preference/TopList.java  9\nmain/src/com/google/refine/operations/recon/ReconJudgeSimilarCellsOperation.java  30\nmain/src/com/google/refine/expr/functions/Cross.java  34\nmain/src/com/google/refine/expr/functions/strings/Partition.java  14\nmain/src/com/google/refine/grel/controls/IsError.java  33\nmain/src/com/google/refine/expr/functions/strings/ToLowercase.java  54\nmain/src/com/google/refine/operations/cell/KeyValueColumnizeOperation.java  8\nmain/src/com/google/refine/grel/controls/ForRange.java  12\nmain/src/com/google/refine/expr/functions/math/Radians.java  60\nmain/src/com/google/refine/io/FileProjectManager.java  5\nmain/src/com/google/refine/expr/functions/math/FactN.java  27\nmain/src/com/google/refine/operations/column/ColumnRemovalOperation.java  61\nmain/src/com/google/refine/expr/functions/strings/LastIndexOf.java  46\nmain/src/com/google/refine/expr/functions/strings/NGram.java  24\nmain/src/com/google/refine/operations/row/RowStarOperation.java  46\nmain/src/com/google/refine/util/JSONUtilities.java  2\nmain/src/com/google/refine/operations/cell/BlankDownOperation.java  27\nmain/src/com/google/refine/grel/controls/ForNonBlank.java  26\nmain/src/com/google/refine/expr/functions/math/Cosh.java  60\nmain/src/com/google/refine/browsing/DecoratedValue.java  46\nmain/src/com/google/refine/operations/recon/ReconClearSimilarCellsOperation.java  48\nmain/src/com/google/refine/operations/column/ColumnMoveOperation.java  66\nmain/src/com/google/refine/expr/functions/strings/Unicode.java  40\nmain/src/com/google/refine/model/recon/StandardReconConfig.java  14\nmain/src/com/google/refine/grel/controls/ForEach.java  11\nmain/src/com/google/refine/expr/functions/html/HtmlText.java  43\nmain/src/com/google/refine/expr/functions/strings/IndexOf.java  46\nmain/src/com/google/refine/expr/functions/strings/StartsWith.java  46\nmain/src/com/google/refine/expr/functions/math/Ceil.java  60\nmain/src/com/google/refine/expr/functions/strings/MD5.java  50\nmain/src/com/google/refine/expr/functions/Length.java  27\nmain/src/com/google/refine/expr/functions/strings/UnicodeType.java  12\nmain/src/com/google/refine/expr/functions/math/Tan.java  60\nmain/src/com/google/refine/expr/functions/strings/RPartition.java  14\nmain/src/com/google/refine/expr/functions/strings/Reinterpret.java  18\nmain/src/com/google/refine/expr/functions/arrays/Uniques.java  22\nmain/src/com/google/refine/operations/column/ColumnAdditionOperation.java  35\nmain/src/com/google/refine/process/ProcessManager.java  30\nmain/src/com/google/refine/browsing/Engine.java  10\nmain/src/com/google/refine/expr/functions/math/Quotient.java  60\nmain/src/com/google/refine/expr/functions/strings/Fingerprint.java  47\nmain/src/com/google/refine/expr/functions/strings/Replace.java  31\nmain/src/com/google/refine/expr/functions/math/Cos.java  60\nmain/src/com/google/refine/expr/functions/ToString.java  29\nmain/src/com/google/refine/expr/functions/strings/SplitByLengths.java  29\nmain/src/com/google/refine/expr/functions/math/Exp.java  60\nmain/src/com/google/refine/operations/row/DenormalizeOperation.java  28\nmain/src/com/google/refine/expr/functions/Get.java  8\nmain/src/com/google/refine/grel/controls/IsBlank.java  33\nmain/src/com/google/refine/operations/row/RowFlagOperation.java  46\nmain/src/com/google/refine/expr/functions/math/ATan2.java  60\nmain/src/com/google/refine/operations/cell/MassEditOperation.java  32\nmain/src/com/google/refine/operations/cell/MultiValuedCellJoinOperation.java  22\nmain/src/com/google/refine/expr/functions/html/HtmlAttr.java  36\nmain/src/com/google/refine/model/ReconType.java  33\nmain/src/com/google/refine/process/QuickHistoryEntryProcess.java  31\nmain/src/com/google/refine/expr/functions/strings/ToTitlecase.java  32\nmain/src/com/google/refine/expr/functions/arrays/Join.java  15\nmain/src/com/google/refine/expr/functions/strings/Phonetic.java  16\nmain/src/com/google/refine/operations/recon/ReconDiscardJudgmentsOperation.java  33\nmain/src/com/google/refine/model/recon/ReconConfig.java  54\nmain/src/com/google/refine/expr/functions/date/Now.java  55\nmain/src/com/google/refine/operations/cell/TransposeRowsIntoColumnsOperation.java  25\nmain/src/com/google/refine/operations/cell/FillDownOperation.java  29\nmain/src/com/google/refine/expr/functions/math/GreatestCommonDenominator.java  54\nmain/src/com/google/refine/grel/controls/Filter.java  10\nmain/src/com/google/refine/operations/column/ColumnAdditionByFetchingURLsOperation.java  9\nmain/src/com/google/refine/grel/controls/With.java  27\nmain/src/com/google/refine/expr/functions/strings/Trim.java  50\nmain/src/com/google/refine/operations/recon/ReconOperation.java  11\nmain/src/com/google/refine/expr/functions/math/Even.java  50\nmain/src/com/google/refine/expr/functions/math/Combin.java  24\nmain/src/com/google/refine/expr/functions/Slice.java  11\nmain/src/com/google/refine/expr/functions/strings/ToUppercase.java  54\nmain/src/com/google/refine/operations/column/ColumnReorderOperation.java  77\nmain/src/com/google/refine/expr/functions/math/Log.java  60\nmain/src/com/google/refine/expr/functions/math/ASin.java  60\nmain/src/com/google/refine/expr/functions/date/DatePart.java  9\nmain/src/com/google/refine/expr/functions/booleans/Xor.java  70\nmain/src/com/google/refine/expr/functions/strings/Chomp.java  46\nmain/src/com/google/refine/expr/functions/math/Fact.java  60\nmain/src/com/google/refine/grel/controls/IsNumeric.java  17\nmain/src/com/google/refine/expr/functions/strings/SmartSplit.java  23\nmain/src/com/google/refine/history/HistoryProcess.java  60\nmain/src/com/google/refine/expr/functions/math/Round.java  60\nmain/src/com/google/refine/operations/recon/ReconCopyAcrossColumnsOperation.java  42\nmain/src/com/google/refine/grel/controls/ForEachIndex.java  10\nmain/src/com/google/refine/grel/controls/IsNonBlank.java  33\nmain/src/com/google/refine/expr/functions/math/ACos.java  60\nmain/src/com/google/refine/expr/functions/arrays/Sort.java  22\nmain/src/com/google/refine/grel/controls/IsNull.java  33\nmain/src/com/google/refine/util/ParsingUtilities.java  6\nmain/src/com/google/refine/model/Column.java  20\nmain/src/com/google/refine/model/Cell.java  12\nmain/src/com/google/refine/expr/functions/math/Sin.java  60\nmain/src/com/google/refine/history/HistoryEntry.java  17\nmain/src/com/google/refine/expr/functions/html/SelectHtml.java  36\nmain/src/com/google/refine/expr/functions/math/Sum.java  22\nmain/src/com/google/refine/expr/functions/math/Mod.java  50\nmain/src/com/google/refine/expr/functions/math/Multinomial.java  33\nmain/src/com/google/refine/expr/functions/strings/Escape.java  20\nmain/src/com/google/refine/model/ReconCandidate.java  66\nmain/src/com/google/refine/grel/controls/IsNotNull.java  33\nmain/src/com/google/refine/expr/functions/strings/Unescape.java  23\nmain/src/com/google/refine/grel/controls/IsTest.java  40\nmain/src/com/google/refine/expr/functions/html/OwnText.java  43\nmain/src/com/google/refine/operations/recon/ReconMatchBestCandidatesOperation.java  28\nmain/src/com/google/refine/model/ReconStats.java  28\nmain/src/com/google/refine/operations/cell/MultiValuedCellSplitOperation.java  18\nmain/src/com/google/refine/expr/functions/html/InnerHtml.java  43\nmain/src/com/google/refine/expr/functions/strings/ReplaceChars.java  40\nmain/src/com/google/refine/operations/cell/TextTransformOperation.java  4\nmain/src/com/google/refine/model/ColumnGroup.java  51\nmain/src/com/google/refine/operations/column/ColumnRenameOperation.java  66\n\nComplexity increasing per file\n\nmain/tests/server/src/com/google/refine/tests/util/TestUtils.java  1\n\nClones removed\n\nmain/tests/server/src/com/google/refine/tests/operations/cell/TransposeTests.java  -1\nmain/tests/server/src/com/google/refine/tests/operations/cell/KeyValueColumnizeTests.java  -1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/operations/recon/ReconMatchSpecificTopicOperation.java  1\nmain/src/com/google/refine/browsing/util/NumericBinRowIndex.java  100\nmain/src/com/google/refine/operations/EngineDependentOperation.java  24\nmain/src/com/google/refine/operations/recon/ReconMarkNewTopicsOperation.java  1\nmain/src/com/google/refine/browsing/facets/RangeFacet.java  78\nmain/src/com/google/refine/browsing/util/ExpressionNumericValueBinner.java  69\nmain/src/com/google/refine/browsing/facets/TimeRangeFacet.java  78\nmain/src/com/google/refine/browsing/util/TimeBinIndex.java  64\nmain/src/com/google/refine/browsing/facets/ListFacet.java  40\nmain/src/com/google/refine/operations/row/RowStarOperation.java  2\nmain/src/com/google/refine/operations/recon/ReconClearSimilarCellsOperation.java  2\nmain/src/com/google/refine/browsing/util/TimeBinRowIndex.java  100\nmain/src/com/google/refine/browsing/facets/ScatterplotFacet.java  66\nmain/src/com/google/refine/browsing/facets/TextSearchFacet.java  26\nmain/src/com/google/refine/operations/column/ColumnAdditionOperation.java  1\nmain/src/com/google/refine/exporters/HtmlTableExporter.java  9\nmain/src/com/google/refine/operations/row/RowFlagOperation.java  2\nmain/src/com/google/refine/browsing/util/NumericBinIndex.java  60\nmain/src/com/google/refine/operations/cell/MassEditOperation.java  1\nmain/src/com/google/refine/commands/Command.java  1\nmain/src/com/google/refine/operations/recon/ReconDiscardJudgmentsOperation.java  1\nmain/src/com/google/refine/exporters/CustomizableTabularExporterUtilities.java  1\nmain/src/com/google/refine/operations/recon/ReconOperation.java  1\nmain/src/com/google/refine/operations/recon/ReconCopyAcrossColumnsOperation.java  1\nmain/src/com/google/refine/browsing/util/ExpressionBasedRowEvaluable.java  100\nmain/src/com/google/refine/browsing/util/ExpressionTimeValueBinner.java  69\nmain/src/com/google/refine/operations/recon/ReconMatchBestCandidatesOperation.java  1\n\nCoverage decreased per file\n\nmain/src/com/google/refine/browsing/Engine.java  -1\n\nComplexity decreasing per file\n\nmain/src/com/google/refine/operations/EngineDependentOperation.java  -3\nmain/src/com/google/refine/browsing/facets/RangeFacet.java  -3\nmain/src/com/google/refine/browsing/facets/TimeRangeFacet.java  -3\nmain/src/com/google/refine/browsing/facets/ScatterplotFacet.java  -6\nmain/src/com/google/refine/browsing/facets/TextSearchFacet.java  -1\nmain/src/com/google/refine/browsing/Engine.java  -14\n\nClones added\n\nmain/src/com/google/refine/operations/recon/ReconJudgeSimilarCellsOperation.java  1\nmain/src/com/google/refine/operations/recon/ReconClearSimilarCellsOperation.java  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/clustering/binning/FingerprintKeyer.java  4\nmain/src/com/google/refine/clustering/binning/BinningClusterer.java  87\nmain/src/com/google/refine/clustering/knn/kNNClusterer.java  85\nmain/src/com/google/refine/clustering/Clusterer.java  87\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 2\n\nClones added\n\nmain/tests/server/src/com/google/refine/tests/importers/TsvCsvImporterTests.java  8\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 9\n\nComplexity increasing per file\n\nmain/webapp/modules/core/scripts/views/data-table/menu-edit-cells.js  3\n\nClones added\n\nmain/webapp/modules/core/scripts/views/data-table/menu-edit-cells.js  6\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/expr/EvalError.java  8\nmain/src/com/google/refine/operations/column/ColumnAdditionByFetchingURLsOperation.java  6\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/logging/IndentingLayout.java  1\nmain/src/com/google/refine/importers/SeparatorBasedImporter.java  9\n\nComplexity increasing per file\n\nextensions/wikidata/src/org/openrefine/wikidata/editing/EditBatchProcessor.java  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 4\n\nCoverage increased per file\n\nmain/src/com/google/refine/logging/IndentingLayout.java  4\n\nClones added\n\nmain/webapp/modules/core/scripts/views/data-table/menu-reconcile.js  2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 1\nAdded 11\n\nComplexity increasing per file\n\nmain/src/com/google/refine/browsing/facets/RangeFacet.java  2\nmain/tests/server/src/com/google/refine/tests/util/TestUtils.java  6\nmain/src/com/google/refine/commands/row/GetRowsCommand.java  5\nmain/src/com/google/refine/io/ProjectUtilities.java  1\nmain/src/com/google/refine/importing/ImportingUtilities.java  8\nmain/src/com/google/refine/model/recon/ReconciledDataExtensionJob.java  1\nmain/src/com/google/refine/commands/cell/EditOneCellCommand.java  2\nmain/src/com/google/refine/commands/history/ApplyOperationsCommand.java  1\nmain/src/com/google/refine/commands/SetPreferenceCommand.java  2\n\nComplexity decreasing per file\n\nmain/tests/server/src/com/google/refine/tests/util/ParsingUtilitiesTests.java  -2\nmain/src/com/google/refine/commands/lang/GetLanguagesCommand.java  -2\nmain/src/com/google/refine/commands/GetPreferenceCommand.java  -3\nmain/src/com/google/refine/sorting/Criterion.java  -1\nmain/src/com/google/refine/expr/functions/Jsonize.java  -8\nmain/src/com/google/refine/model/Cell.java  -5\nmain/src/com/google/refine/commands/HttpUtilities.java  -1\nmain/src/com/google/refine/importing/DefaultImportingController.java  -2\nmain/src/com/google/refine/browsing/filters/ExpressionEqualRowFilter.java  -1\nmain/src/com/google/refine/sorting/StringCriterion.java  -2\nextensions/wikidata/src/org/openrefine/wikidata/schema/WikibaseSchema.java  -4\nmain/src/com/google/refine/commands/GetVersionCommand.java  -1\nmain/src/com/google/refine/commands/importing/GetImportingJobStatusCommand.java  -2\nmain/src/com/google/refine/expr/functions/arrays/Sort.java  -1\nmain/src/com/google/refine/commands/project/GetProjectMetadataCommand.java  -1\nmain/src/com/google/refine/operations/OperationRegistry.java  -2\nmain/src/com/google/refine/commands/project/SetProjectTagsCommand.java  -1\nmain/src/com/google/refine/commands/history/GetProcessesCommand.java  -1\nextensions/gdata/src/com/google/refine/extension/gdata/GDataImportingController.java  -2\nmain/src/com/google/refine/model/ReconType.java  -1\nmain/src/com/google/refine/expr/functions/arrays/Uniques.java  -1\nmain/src/com/google/refine/grel/controls/ForEachIndex.java  -1\nmain/src/com/google/refine/browsing/facets/TimeRangeFacet.java  -7\nmain/src/com/google/refine/exporters/TemplatingExporter.java  -6\nmain/src/com/google/refine/model/recon/StandardReconConfig.java  -12\nmain/src/com/google/refine/grel/ast/FieldAccessorExpr.java  -1\nmain/src/com/google/refine/grel/controls/ForEach.java  -1\nmain/tests/server/src/com/google/refine/tests/importers/FixedWidthImporterTests.java  -1\nmain/src/com/google/refine/expr/functions/Get.java  -3\nmain/src/com/google/refine/expr/functions/HasField.java  -1\nmain/src/com/google/refine/commands/project/GetModelsCommand.java  -3\nmain/src/com/google/refine/expr/functions/arrays/Join.java  -1\nmain/src/com/google/refine/expr/functions/arrays/Reverse.java  -1\nmain/src/com/google/refine/grel/controls/Filter.java  -1\nmain/src/com/google/refine/expr/functions/Slice.java  -1\nmain/src/com/google/refine/util/JSONUtilities.java  -4\nmain/src/com/google/refine/model/changes/ColumnReorderChange.java  -1\nmain/src/com/google/refine/model/changes/ColumnMoveChange.java  -1\nmain/src/com/google/refine/commands/GetAllPreferencesCommand.java  -1\nmain/src/com/google/refine/process/QuickHistoryEntryProcess.java  -2\nmain/src/com/google/refine/commands/browsing/GetScatterplotCommand.java  -18\nmain/src/com/google/refine/commands/recon/PreviewExtendDataCommand.java  -3\nmain/src/com/google/refine/exporters/CustomizableTabularExporterUtilities.java  -11\nmain/src/com/google/refine/model/recon/DataExtensionReconConfig.java  -3\nmain/src/com/google/refine/browsing/facets/ListFacet.java  -11\nmain/src/com/google/refine/commands/importing/GetImportingConfigurationCommand.java  -2\nmain/src/com/google/refine/operations/column/ColumnReorderOperation.java  -1\nmain/src/com/google/refine/preference/PreferenceStore.java  -1\nextensions/wikidata/src/org/openrefine/wikidata/commands/PreviewWikibaseSchemaCommand.java  -1\nmain/src/com/google/refine/commands/expr/GetExpressionHistoryCommand.java  -4\nmain/webapp/modules/core/scripts/index/default-importing-controller/file-selection-panel.js  -1\nmain/src/com/google/refine/operations/cell/MassEditOperation.java  -2\nextensions/wikidata/src/org/openrefine/wikidata/commands/LoginCommand.java  -1\nmain/src/com/google/refine/commands/recon/GuessTypesOfColumnCommand.java  -2\nmain/src/com/google/refine/operations/recon/ReconMatchSpecificTopicOperation.java  -1\nmain/src/com/google/refine/model/Row.java  -2\nmain/src/com/google/refine/commands/expr/GetStarredExpressionsCommand.java  -1\nmain/src/com/google/refine/commands/project/SetProjectMetadataCommand.java  -1\nmain/src/com/google/refine/commands/workspace/GetAllProjectTagsCommand.java  -1\nmain/tests/server/src/com/google/refine/tests/exporters/CsvExporterTests.java  -1\nmain/src/com/google/refine/browsing/filters/ExpressionNumberComparisonRowFilter.java  -1\nmain/src/com/google/refine/model/ColumnGroup.java  -3\nmain/src/com/google/refine/model/changes/ColumnChange.java  -1\nmain/src/com/google/refine/preference/TopList.java  -3\nmain/src/com/google/refine/commands/workspace/GetAllProjectMetadataCommand.java  -4\nmain/src/com/google/refine/commands/history/GetOperationsCommand.java  -3\nmain/src/com/google/refine/commands/column/SplitColumnCommand.java  -1\nmain/src/com/google/refine/model/changes/ColumnRemovalChange.java  -1\nmain/src/com/google/refine/model/recon/ReconConfig.java  -2\nmain/src/com/google/refine/commands/history/GetHistoryCommand.java  -1\nmain/src/com/google/refine/commands/expr/ToggleStarredExpressionCommand.java  -1\nmain/src/com/google/refine/importers/ExcelImporter.java  -1\nmain/src/com/google/refine/browsing/EngineConfig.java  -16\nmain/src/com/google/refine/browsing/facets/ScatterplotFacet.java  -8\nmain/src/com/google/refine/commands/cell/SplitMultiValueCellsCommand.java  -1\nmain/src/com/google/refine/model/changes/ColumnAdditionChange.java  -1\nmain/src/com/google/refine/commands/browsing/ComputeClustersCommand.java  -3\nmain/src/com/google/refine/io/FileProjectManager.java  -6\nmain/src/com/google/refine/browsing/filters/ExpressionStringComparisonRowFilter.java  -1\nmain/src/com/google/refine/model/Column.java  -3\nmain/src/com/google/refine/model/changes/ColumnSplitChange.java  -2\nmain/src/com/google/refine/model/Recon.java  -10\nmain/src/com/google/refine/commands/expr/GetExpressionLanguageInfoCommand.java  -2\nmain/src/com/google/refine/exporters/CsvExporter.java  -5\n\nClones added\n\nmain/tests/server/src/com/google/refine/tests/importers/XmlImporterTests.java  2\nmain/tests/server/src/com/google/refine/tests/commands/project/SetProjectMetadataCommandTests.java  2\nmain/tests/server/src/com/google/refine/tests/importers/JsonImporterTests.java  2\nmain/tests/server/src/com/google/refine/tests/exporters/sql/SqlExporterTests.java  3\nmain/webapp/modules/core/scripts/index/edit-metadata-dialog.js  2\n\nClones removed\n\nmain/src/com/google/refine/commands/HttpUtilities.java  -1\nmain/src/com/google/refine/commands/recon/ReconClearOneCellCommand.java  -1\nextensions/gdata/src/com/google/refine/extension/gdata/GDataImportingController.java  -2\nmain/src/com/google/refine/expr/functions/arrays/Uniques.java  -1\nmain/src/com/google/refine/operations/recon/ReconJudgeSimilarCellsOperation.java  -1\nmain/src/com/google/refine/operations/recon/ReconClearSimilarCellsOperation.java  -1\nmain/src/com/google/refine/expr/functions/arrays/Reverse.java  -1\nmain/webapp/modules/core/scripts/project/exporters.js  -3\nmain/src/com/google/refine/process/LongRunningProcess.java  -3\nmain/src/com/google/refine/operations/cell/TextTransformOperation.java  -1\nmain/src/com/google/refine/operations/cell/MassEditOperation.java  -1\nmain/src/com/google/refine/operations/recon/ExtendDataOperation.java  -3\nextensions/database/src/com/google/refine/extension/database/DatabaseImportController.java  -2\nmain/src/com/google/refine/operations/column/ColumnAdditionByFetchingURLsOperation.java  -4\nmain/src/com/google/refine/operations/column/ColumnAdditionOperation.java  -1\nmain/src/com/google/refine/commands/cell/EditOneCellCommand.java  -1\nmain/src/com/google/refine/operations/recon/ReconOperation.java  -2\nmain/src/com/google/refine/commands/Command.java  -1\nmain/src/com/google/refine/operations/cell/FillDownOperation.java  -2\nmain/src/com/google/refine/commands/recon/ReconJudgeOneCellCommand.java  -1\nmain/webapp/modules/core/scripts/index/default-importing-sources/sources.js  -3\nmain/src/com/google/refine/operations/cell/BlankDownOperation.java  -2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nCoverage increased per file\n\nmain/src/com/google/refine/exporters/XlsExporter.java  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nmain/src/com/google/refine/importers/RdfTripleImporter.java  1\n\nClones added\n\nmain/tests/server/src/com/google/refine/tests/importers/RdfTripleImporterTests.java  4\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nextensions/wikidata/src/org/openrefine/wikidata/schema/WbStringVariable.java  2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nextensions/wikidata/src/org/openrefine/wikidata/qa/WikidataConstraintFetcher.java  1\nextensions/wikidata/src/org/openrefine/wikidata/qa/scrutinizers/DistinctValuesScrutinizer.java  1\nextensions/wikidata/src/org/openrefine/wikidata/qa/scrutinizers/SnakScrutinizer.java  2\n\nComplexity decreasing per file\n\nextensions/wikidata/src/org/openrefine/wikidata/qa/scrutinizers/QuantityScrutinizer.java  -1\nextensions/wikidata/src/org/openrefine/wikidata/exporters/QSValuePrinter.java  -1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nmain/tests/server/src/com/google/refine/tests/operations/cell/BlankDownTests.java  2\nmain/tests/server/src/com/google/refine/tests/operations/cell/FillDownTests.java  2\n\nClones added\n\nmain/tests/server/src/com/google/refine/tests/operations/cell/BlankDownTests.java  1\nmain/tests/server/src/com/google/refine/tests/operations/cell/FillDownTests.java  1\nmain/src/com/google/refine/operations/cell/BlankDownOperation.java  1\nmain/src/com/google/refine/operations/cell/FillDownOperation.java  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity decreasing per file\n\nmain/src/com/google/refine/expr/functions/ToDate.java  -12\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nmain/webapp/modules/core/scripts/views/data-table/menu-edit-cells.js  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nmain/tests/server/src/com/google/refine/tests/expr/functions/html/ParseHtmlTests.java  3\n\nClones added\n\nmain/tests/server/src/com/google/refine/tests/expr/functions/html/ParseHtmlTests.java  3\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 2\nAdded 2\n\nClones added\n\nextensions/wikidata/module/scripts/menu-bar-extension.js  1\nextensions/database/module/scripts/index/database-import-controller.js  2\nextensions/gdata/module/scripts/index/importing-controller.js  2\nmain/webapp/modules/core/scripts/index.js  1\nmain/webapp/modules/core/scripts/project.js  1\nextensions/gdata/module/scripts/project/exporters.js  1\nmain/webapp/modules/core/scripts/preferences.js  1\n\nClones removed\n\nmain/webapp/modules/core/scripts/views/data-table/menu-edit-column.js  -1\nmain/webapp/modules/core/scripts/views/data-table/data-table-view.js  -1\nmain/webapp/modules/core/scripts/views/data-table/menu-edit-cells.js  -3\nmain/webapp/modules/core/scripts/index/parser-interfaces/fixed-width-parser-ui.js  -1\nmain/webapp/modules/core/scripts/index/parser-interfaces/separator-based-parser-ui.js  -1\nmain/webapp/modules/core/scripts/views/data-table/cell-ui.js  -1\nmain/webapp/modules/core/scripts/dialogs/expression-preview-dialog.js  -2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity decreasing per file\n\nmain/src/com/google/refine/expr/functions/Cross.java  -1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nextensions/database/src/com/google/refine/extension/database/DatabaseService.java  4\n\nComplexity decreasing per file\n\nextensions/database/src/com/google/refine/extension/database/pgsql/PgSQLDatabaseService.java  -1\nextensions/database/module/scripts/index/database-source-ui.js  -1\n\nClones removed\n\nextensions/database/src/com/google/refine/extension/database/pgsql/PgSQLDatabaseService.java  -1\nextensions/database/src/com/google/refine/extension/database/mysql/MySQLDatabaseService.java  -3\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nClones added\n\nmain/tests/server/src/com/google/refine/tests/exporters/sql/SqlExporterTests.java  2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 3\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 19\n\nComplexity increasing per file\n\nmain/webapp/modules/core/scripts/dialogs/clustering-dialog.js  1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nextensions/wikidata/src/org/openrefine/wikidata/updates/ItemUpdate.java  2\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nextensions/wikidata/src/org/openrefine/wikidata/editing/NewItemLibrary.java  3\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 3\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nextensions/wikidata/src/org/openrefine/wikidata/schema/WbDateConstant.java  3\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity increasing per file\n\nextensions/wikidata/module/scripts/dialogs/schema-alignment-dialog.js  3\nmain/webapp/modules/core/scripts/views/data-table/menu-reconcile.js  3\nmain/webapp/modules/core/scripts/views/data-table/cell-ui.js  3\n\nClones removed\n\nextensions/wikidata/module/scripts/dialogs/schema-alignment-dialog.js  -4\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nSolved 3\nAdded 1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nIssues\n\nAdded 1\n\n```\nSee the complete overview on Codacy\n        . \n Here is an overview of what got changed by this pull request:\n```diff\nComplexity decreasing per file\n\nmain/webapp/modules/core/scripts/facets/list-facet.js  -9\nmain/src/com/google/refine/browsing/util/ExpressionNominalValueGrouper.java  -2\n\nClones removed\n\nmain/webapp/modules/core/scripts/facets/list-facet.js  -2\nmain/tests/server/src/com/google/refine/tests/browsing/util/ExpressionNominalValueGrouperTests.java  -2\n\n```\nSee the complete overview on Codacy\n        .  Issue found: 'label' is already defined. (no-redeclare).  Issue found: 'input' used outside of binding context. (block-scoped-var).  Issue found: 'input' is already defined. (no-redeclare).  Issue found: 'tr' used outside of binding context. (block-scoped-var).  Issue found: 'label' used outside of binding context. (block-scoped-var).  Issue found: 'input' used outside of binding context. (block-scoped-var).  Issue found: 'label' used outside of binding context. (block-scoped-var).  Issue found: Expected '===' and instead saw '=='. (eqeqeq).  Issue found: 'tr' used outside of binding context. (block-scoped-var).  Issue found: 'label' used outside of binding context. (block-scoped-var).  Issue found: 'label' used outside of binding context. (block-scoped-var).  Issue found: Generic Object Injection Sink (security/detect-object-injection).  Issue found: 'input' used outside of binding context. (block-scoped-var).  Issue found: Do not use 'new' for side effects. (no-new).  Issue found: The function setTimeout can be unsafe (scanjs-rules/call_setTimeout).  Issue found: Function has a complexity of 13. (complexity).  Issue found: The function hide can be unsafe (scanjs-rules/call_hide).  Issue found: Function Call Object Injection Sink (security/detect-object-injection).  Issue found: You may have misspelled a JUnit framework method (setUp or tearDown).  Issue found: Method names should not start with capital letters.  Issue found: Perhaps 'servlet' could be replaced by a local variable..  Issue found: You may have misspelled a JUnit framework method (setUp or tearDown).  Issue found: Method names should not start with capital letters. ",
    "tentonsofhours": "\n Here the full screen shot, sorry for the wrong one previously.. ",
    "tmmcub": "Hi Thad,\nThanks tons for all the suggestions!  I pretty much had to do all the above in order to get the json file loaded, but that's fine by me.  Allocating more memory was the key.\nReally appreciate your help!  I learned a lot today.\nHave a great week!. ",
    "ViktorHolas": "@wetneb yes, the URL is http://dumazahrada.eu/sitemap_index.xml. @wetneb Im using version 2.7 on Windows, but now I tried it on my OSX and works fine. . ",
    "msaby": "Thank you Ettore for migrating my questions to github ;-)\nI think I found an other few strange things, Should I open issues directly on github ?\nM. Saby. Fingerprint in Openrefine has always treated \"ecole ecole ecole ecole\" as the same as \"ecole\", so please don't change it. Some people may rely on that... But if you think it is needed you can always implement a new algo more specific.\nNormalisation/translitteration is tricky, I know that, I work in a library ;-) But this case is not a tricky one, this is a consistance issue. For me, if the \"\u00e9\" is supposed to be processed as a \"e\" by fingerprint, it has to be true in every circumstance.. Hi \nI don't understand half of your conversation ;-), but as someone wrote, I spent some time wrangling with java certificates in OR some days ago. \nI believe Mac version do NOT ignore $JAVA_HOME. I had issues with missing certificate in cacerts, but my $JAVA_HOME  was void. When I put the path of my system java in $JAVA_HOME, the issue was resolved\n. Wow... I spend hours working around this, and just discover it is a feature ;-)\nI really need use a filtered duplicate facet for a project, and don't know how to do that :/. I disabled caching because I remember there were a bug in 2.7. I thought it was linked, but no...\nUnfortunately I don't have anymore the history, and I can't reproduce the bug easily, because I updated the JDK of my system, in order to fix the SSL issue.. I have made a new issue. Thank you all for your answers.\nMathieu\nLe mer. 10 oct. 2018 \u00e0 17:21, Ettore Rizza notifications@github.com a\n\u00e9crit :\n\nNice catch, @ostephens https://github.com/ostephens. In summary, the\nparser called \"RDF/N3\" cannot read n3, just Turtle, but is able to parse\nN-triple, which is not indicated. It should at least be renamed.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1560#issuecomment-428615116,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA47xPLdPMRXJ6JvBNfmdTsZnX1-KBDWks5ujhCTgaJpZM4TLGkg\n.\n\n\n-- \nMathieu Saby\n. Hi\nI am not sure it is working as it should. When I blank down on column2, it creates new records... Is it the wanted behavior? If so, it is weird\nHere is a screen capture\n\n. Well I don't understand why blanking down on col2 would change the number of the records...\nHave you written documentation for that new behavior? I How would you explain it clearly in a training session? \nImagine I have a list of persons with 2 columns : city / gender\nParis - Male\nParis - Male\nParis - Male\nParis - Female\nBerlin - Female\nBerlin - Female\nBerlin - Female\nLondon - Male\nLondon - Male\nNow I want to have this : one line for each different \"couple\" of data\nParis - Male\nParis - Female\nLondon - Male\nBerlin - Female\nWith the old behavior, in records mode, blanking down col2 would have suppress all information for Berlin. It was bad...\nWith the new behavior, I get that : 8 records \n- record 1 : \"Paris\" -> 1 value : \"Male\"\n- record 2 : null -> 1 value :  null\n- record 3 : null -> 2 values : null and \"Female\"\n- record 4 : Berlin -> 1 values : null\n- record 5 : null -> 1 value :  null\n- record 6 : null -> 1 value :  null\n- record 7 : London -> 1 value :  Male\n- record 8 : null -> 1 value :  null\n\nI would expect that instead :  3 records\n\n\nrecord 1 : \"Paris\" -> 4 values : \"Male\", null, null, \"Female\"\nrecord 2 : \"Berlin\" -> 3 values : \"Female\", null, null\nrecord 3 : \"London\" -> 2 values : \"Male\", null . I did the operation in record mode!\n. ok but note that the initial message \"Records are respected if the operation is run in records mode. In rows mode, the behavior is unchanged.\" does not describes what OR is now doing : records are not respected... And as Owen wrote, there is probably a bug in the \"Paris/Berlin/London example\"\nI will train some people in a few days, and I think I won't show them this new behavior in records mode, as I cannot really explain it.. Easy ;-)\n\nI try with an other example\n```\n[\n  {\n    \"op\": \"core/blank-down\",\n    \"description\": \"Blank down cells in column Paris\",\n    \"engineConfig\": {\n      \"mode\": \"row-based\",\n      \"facets\": []\n    },\n    \"columnName\": \"Paris\"\n  },\n  {\n    \"op\": \"core/blank-down\",\n    \"description\": \"Blank down cells in column H\",\n    \"engineConfig\": {\n      \"mode\": \"record-based\",\n      \"facets\": []\n    },\n    \"columnName\": \"H\"\n  }\n]\n```\nWith Openrefine 3.0\nBefore blanking down in records mode\n\nAnd after...\n\nStrange : The result seems inconsistent with my previous example, because \"F\" is preserved in line 3\nI retry with my previous example:\n```\n[\n  {\n    \"op\": \"core/column-split\",\n    \"description\": \"Split column Column 1 by separator\",\n    \"engineConfig\": {\n      \"mode\": \"row-based\",\n      \"facets\": []\n    },\n    \"columnName\": \"Column 1\",\n    \"guessCellType\": true,\n    \"removeOriginalColumn\": true,\n    \"mode\": \"separator\",\n    \"separator\": \" - \",\n    \"regex\": false,\n    \"maxColumns\": 0\n  },\n  {\n    \"op\": \"core/blank-down\",\n    \"description\": \"Blank down cells in column Column 1 1\",\n    \"engineConfig\": {\n      \"mode\": \"row-based\",\n      \"facets\": []\n    },\n    \"columnName\": \"Column 1 1\"\n  },\n  {\n    \"op\": \"core/blank-down\",\n    \"description\": \"Blank down cells in column Column 1 2\",\n    \"engineConfig\": {\n      \"mode\": \"record-based\",\n      \"facets\": []\n    },\n    \"columnName\": \"Column 1 2\"\n  }\n]\n```\nbefore blank down : \n\nafter:\n\nSo to sum up, I am lost...\nWhat is the expected behavior ?. I add the complete project for the Paris/Berlin/London example, if it can help\nclipboard.openrefine.tar.gz\n. Merged ;-) Thanks folks!\nWhen a new feature is added, does someone update the documentation in the wiki ?. Hi\nNo I did not, but I gladly do it. Where should it be described in the wiki\narborescence?\nLe mer. 7 nov. 2018 \u00e0 17:57, Thad Guidry notifications@github.com a\n\u00e9crit :\n\nMerged ;-) Thanks folks!\nWhen a new feature is added, does someone update the documentation in the\nwiki ?\n@msaby https://github.com/msaby Did you add this feature into our Wiki\n? :-)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1742#issuecomment-436696371,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA47xEKdO83hf00CZVUjPwTCdL_yq6vwks5usxDtgaJpZM4W1YNR\n.\n\n\n-- \nMathieu Saby\n. Done in the wiki ;-) Can you take a look ? I am not fluent in english\nBut I have just discovered a bug... Will need a fix but I won't have time until the end of the month. The \"regular expression\" option escapes \\ where it must not do :\n\n. closes #1742 . The UI must look like that :\n\nclipboard.csv.zip\nTo test :\nTry to search & replace strings and patterns and play with the options\n I join a csv file with some weird strings, but you can craft your owns.\n. ok for me. I updated my branch https://github.com/msaby/OpenRefine-devms/commits/replaceGUI\nCan you tell me if I need to update the pull request? or is it automatic?. thanks @ostephens . So you can test with the new commit and see if it works as planned.. Managing the escaping of newlines and tabs in textareas gave me a headache. So Ok for a single line input.\nI propose to keep the escaping I made for both fields.\nI the user need to search for a tab or a newline, he must check \"regular expression\", and use \\n and \\t\nI must try to find something less obscure than \"do not escape backlash automatically\". Maybe \"interpret \\n as newline, \\t as tab, \\ as \\\". Maybe something like the options in the search & replace box of Notepad++ would be nice ?\n\nIn Notepadd++ you can choose to find whole words only (could be implemented by using regexp under the hood and adding \\b before and after the word).\nYou can also use escaped characters like \\n \\t but without using regexp (list http://docs.notepad-plus-plus.org/index.php/Searching_And_Replacing ). If you don't check the box, \"\\n\" is interpreted as 2 characters : \\ and n. I added a followup commit with the requested changes, and I added a new option for matching whole words (like in Notepad++ and Atom), and changed the behavior of the escaping option for replacement text.\nCould you test it please?\nIt seems a conflict happened with translation-en.json file. Should I do something ? I don't know how to manage it.... Well, the \"semantic data\" idea is probably a bad one... Implementing JSON-LD import is sufficient. I cannot test : Openrefine master code gives me Maven and Java error, \"impossible de trouver ou charger la classe principale com.google.refine.Refine\" etc.\n(when I launch . refine run). Thanks, it worked!\nSo, I suppose it's ok :\nFrom this project with 14 lines and 3 columns, with \"x\" in the 3rd column to force OR to keep the record number at 3\n\nI can get 3 records, and apply the blanking down on column 2. In row mode it works like before\n\nBut in record mode the operation is applied on each record\n\nNow I try the filling down, after editing manually some cells\n\nIt works well : \n\nSo the only disturbing point is the behavior of record mode when a row contains only null values: a record is created for that row. But I believe this behavior is not new, even if I discover it today... So this has to link with your patch. Can you confirm it?\n\n. I let the java guys writing something, I don't know the language ;-)\nNote that Jsoup version used in Openrefine seems very old, so maybe it is necessary to update it to use the xml parser.. Hi\nWhat do you think of this valid XML for testing?\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<root xmlns:foaf=\"http://xmlns.com/foaf/0.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n    <foaf:Person>\n        <foaf:name>John Doe</foaf:name>\n        <head>head1</head>\n        <head>head2</head>\n        <BODY>nice body</BODY>\n        <foaf:homepage rdf:resource=\"http://www.example.com\"/>\n    </foaf:Person>\n    <foaf:Person>\n        <foaf:name>H\u00e9lo\u00efse Dupont</foaf:name>\n        <head>head3</head>\n        <BODY>nice body</BODY>\n        <foaf:title/>\n    </foaf:Person>\n</root>\nI am not sure about the tests, but it could be something like this:\nvalue.parseXML().select(\"foaf|Person\")  -> an array with the 2 elements foaf:Person\nforeach(value.parseXML().select(\"foaf|Person foaf|name\"),x,innerHTML(x) ).join(|) -> \"John Doe|H\u00e9lo\u00efse Dupont\"\nforeach(value.parseXML().select(\"foaf|Person head\"),x,innerHTML(x) ).join(|) ->\"head1head2head3\"\nvalue.parseXML().select(\"BODY\")[1]-> \"nice body\"\nvalue.parseXML().select(\"foaf|homepage[rdf|resource]\")[0] -> \"http://www.example.com\"\nNote that the name of innerHTML() could be strange if parseXML() is created. Maybe it is possible to create innerXML() that would just be an alias of innerHTML? Same for other ...HTML functions?\n. For the pipe after the namespace, I saw that in Jsoup documentation, so I thought it was already the case for GREL functions based on Jsoup.... ah ok. And it works already like that with parseHtml(). I have just tested. Oh I forgot I asked about it in Stackoverflow...\nThe code of the function waits only a String :\n                ( v instanceof String || v instanceof WrappedCell ) &&. Yes ;-)\nI made a PR, but it does not seem to link itself with the issue. To test and check nothing is broken : try to redo all the steps of this project\ntest.openrefine.tar.gz\n. sorry, no time before next week\nLe mer. 21 nov. 2018 \u00e0 17:03, Owen Stephens notifications@github.com a\n\u00e9crit :\n\n@msaby https://github.com/msaby are you able to test this?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/1845#issuecomment-440719223,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA47xMMhPpNAfCZZe-PQnQe5W2cvSC5Cks5uxXlPgaJpZM4Yta_R\n.\n\n\n-- \nMathieu Saby\n. I can give you access to the projects via box.com if you want. I also had problems with 2 project with the same name... I don't remember the result but it was not what I expected... Could it be possible to raise an error (or to fill the created column with an error) when this situation happens?. Addition : \nvalue.parseXml().select(\"datafield[tag=035]\")[0].innerXml() is working well and gives the same result as value.parseXml().select(\"datafield[tag=035]\")[0].innerHtml()\n. Ok. So it is more precise than innerHtml().\nCould someone add an entry describing the behavior of this new function innerXml() in the wiki, I see no reference in https://github.com/OpenRefine/OpenRefine/wiki/GREL-Other-Functions  ?. ",
    "Gucci1986": "Sure, here : \n{\n  \"data\": [\n    {\n      \"created_time\": \"2017-05-31T15:53:45+0000\",\n      \"from\": {\n        \"name\": \"JJ Nina\",\n        \"id\": \"10203499588585347\"\n      },\n      \"message\": \"Or\u00e9lie Or\u00e9lie ... mdrrr j en peux plus \ud83d\ude02\ud83d\ude02\",\n      \"id\": \"1551460621555337_1551461834888549\"\n    },\n    {\n      \"created_time\": \"2017-05-31T15:55:03+0000\",\n      \"from\": {\n        \"name\": \"Emilie Duthoy\",\n        \"id\": \"10203735457525161\"\n      },\n      \"message\": \"\\\" tu veux voir mon clitoris de 15cm ?\\\"\",\n      \"id\": \"1551460621555337_1551462631555136\"\n    },\n    {\n      \"created_time\": \"2017-05-31T15:56:50+0000\",\n      \"from\": {\n        \"name\": \"Mi Stick\",\n        \"id\": \"133169370361082\"\n      },\n      \"message\": \"Trol Lex \u00e7a te rappelle qqn ? \ud83d\ude02\ud83d\ude02\",\n      \"id\": \"1551460621555337_1551464234888309\"\n    },\n    {\n      \"created_time\": \"2017-05-31T15:53:46+0000\",\n      \"from\": {\n        \"name\": \"Seb Seb\",\n        \"id\": \"123755004648034\"\n      },\n      \"message\": \"Mdr\",\n      \"id\": \"1551460621555337_1551461858221880\"\n    }\n  ]\n}. @wetneb Here's the screenshot of the lasts utterances of 1) the json file and 2) the Openrefine result\n\n\nAs you may see, the file is cut from the original. I know I don't explain very well, as I said, i'm very new in this.\n@ettorerizza What do you mean? Is it a problem of censorship?\nThank you for your responses.. Ok, I'll find another way to do what I want. It seems very complicated and I don't have the competences to expose my problem. i don't want to bother you ahaha.\nThank you very much, I'll close the topic. . @ettorerizza Oh ok, I'm glad to here that. I can't send the whole file because there is confidentiality problem, I can send you a sample but not the whole file I'm sorry. This is why I said it will be complicated. \nI think the file has been truncated during the process in Open Refined, I used another device to organize my file into a proper spreadsheet and it worked, so I think maybe OpenRefine is not what I needed in the first place.\nThank you very much for your response . ",
    "richyvk": "Done a bit more digging. Expression box is resizable in everything but IE because it is a textarea, and by default there are resizable in none IE browsers, via the resize css property (not supported by IE).\nI think getting the whole dialog box to resize might be hard, certainly outside of my skills. But, it looks to me like adding css to the expression box to limit resize to vertical would at least offer a possible fix for this: https://github.com/OpenRefine/OpenRefine/issues/421\nI will do a PR to do that.\n. @thadguidry Thanks for that. Do you know when this big change might happen? I'm wondering if it's worth my PR, but if the big change is a way off, maybe it still is?. @thadguidry Tested 3.2 beta and working - thanks very much for clarifying :). ",
    "darrenfoong": "@jackyq2015 has added types and constraints to columns to support Data Packages.\nBut if precision values are not part of the Data Package specification, then there may be a need to create a more encompassing type/constraint model of which the Data Package requirements form a subset.. ",
    "ValentinChCloud": "Something like that ?\n```\nroot@galaxy-e-v1:~/OpenRefine# iostat \nLinux 4.4.0-87-generic (galaxy-e-v1)    10/20/2017      x86_64        (8 CPU)\navg-cpu:  %user   %nice %system %iowait  %steal   %idle\n           0.91    0.00    0.07    0.01    0.00   99.02\nDevice:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn\n```\nthere is no difference before and after \n. >can you use a regular user to download a new copy and try again? The idea is to try to make it running locally, not from remote storage.\nI used to have Open refine on my VM with virtual box and it works well. root@galaxy-e-v1:~# mount | grep $(df -P ~/.local/share/openrefine | tail -1 | awk {'print $1'})\n/dev/vda1 on / type ext4 (rw,relatime,data=ordered)\n/dev/vda1 on /var/lib/docker/overlay type ext4 (rw,relatime,data=ordered)\nOpenstack from Genouest http://www.genouest.org/outils/genostack. It makes me think,, this happens after the 2~3 the launch of openrefine. But if I don't use  it for few days and come back,, same thing happens. It doesn't happn locally.. Done, nothing changes :/ . it's really weird,  the first time it's work and after get slower\n. There is ./refine test server if it could helps : \nhttps://github.com/ValentinChCloud/Logs/blob/master/out_server_test.txt. >You said that the first time after you removed the folder it's work though?\nNo I said, remove the folder didn't change anything.I said this issue happens when, I create a new VM, I download Openrefine from github, I build and run refine, and after many launches it gets slower.\nI don't use refine at all, I just want first to launch many server on different ip and port ( finnaly to use Openrefine as container). Very interesting, I guess I have to use openjdk-9 to work with containers right? I tried but as mentioned  #1162 didn't work ^^\n. Of course, I don't have any problems locally, so back to openstack issue :/. . ",
    "aadrian": "@wetneb thank you for the information.\n\nI don't think OpenRefine supports JDBC natively. \n\nThis looks quite unusual to me, since the server is based on Java, and Java's main advantage, purpose and selling point with businesses is mostly JDBC .\n\nYou can try exporting your dataset from JDBC to a format that OpenRefine supports.\n\nHmm, using those formats would also mean to loose information (metadata, constraints,  relationships etc), so it would pretty defy the whole purpose of \"refining\" the data  :( .\n. ",
    "kidehen": "@thadguidry,\nWhat's the status of JDBC support? . @thadguidry ,\nGreat! I will give it a whirl. \nKingsley. @thadguidry,\nI've looked at the DBMS connection, which I assume is JDBC-based, but it only offers PostgreSQL, MySQL, and MariaDB as options. How do I make generic JDBC connection? \nKingsley . Little note to self, re connecting these JDBC threads.\n{\n<http://linkeddata.uriburner.com/about/id/entity/https/github.com/OpenRefine/OpenRefine/issues/1277>\nskos:related <http://linkeddata.uriburner.com/about/id/entity/https/github.com/OpenRefine/OpenRefine/issues/1909> .\n}. ",
    "remram44": "Fixing this would mean all translation files have to be adapted (though I suspect the program is not very usable in non-English locales anyway), so a commitment from the project leads is necessary before work can happen here.. > Feel free to invalidate existing translations, people will go to Weblate to fix them if they use the translation.\nSounds good! I just don't want to do work to then hear that \"that would break translations\" and have the patch rejected. But if that's acceptable then it's fine.\nI will see if I can do that but I'm short on time at the moment, so if somebody else wants to grab this go ahead.. It doesn't look like you are using jquery.i18n (from wikipedia) but recurser/jquery-i18n. Commit adding it is 817c6cc8d4468c5bf8e776cbe2f384385e74bf69, which doesn't mention the source.\nFurthermore, although the whole source is reproduced, no attribution is done to Dave Perrett, the author; this violates the terms of the MIT license (quite a feat...)\ncc @Blakko. It is a bit long to type, but the issue here is that this returns a single string, and you get no string interpolation or custom plural machinery, necessary for proper i18n.\nFor example this is problematic:\nhttps://github.com/OpenRefine/OpenRefine/blob/e5fdf48680a6f810c84706671344dc93d9a4fb75/main/webapp/modules/core/scripts/dialogs/clustering-dialog.js#L246\nUsing jquery.i18n, gettext or another state-of-the-art i18n library, you do $.i18n('core-clusters-filtered', clusters.length, this._clusters.length) or similar instead of concatenations, allowing the translator to see the full sentence in one go, reorder the inserted values if appropriate, and properly pluralize words in the translation.. It works, but I haven't gone over the extensions. In particular, do we want some kind of namespacing for the string IDs? Such as core-discard, gdata-discard, etc.. Can someone configure @codacy-bot to ignore external libs?. I will do that then: core-msg, gdata-msg. Thanks!. I've been really busy but I'll come back to this soon! Sorry about the delay.. I can update this patch for current OpenRefine. Note that once this is merged, translation files will have to be updated.. Thanks @wetneb! Sorry I couldn't find time to finish this!. ",
    "schignel": "Thanks@jackyq2015! Switching to Chrome fixed the problem. \nSo something to with Firefox? I have a number of add-ons enabled. I have listed them below in case this is useful for bug fixing (listed below):\n\n. ",
    "olleolleolle": "In order to allow the reflective access, some Java 9-specific settings need to be applied as JAVA_OPTS.\n[testng] WARNING: An illegal reflective access operation has occurred\n   [testng] WARNING: Illegal reflective access by com.google.refine.expr.util.CalendarParser (file:/home/travis/build/OpenRefine/OpenRefine/main/webapp/WEB-INF/classes/) to method sun.util.calendar.ZoneInfo.getAvailableIDs()\nTo hint better at what I mean, here's one --add-opens example for some other module names:\nexport JAVA_OPTS='--add-opens java.base/java.util.zip=ALL-UNNAMED --add-opens java.base/java.security.cert=ALL-UNNAMED --add-opens java.base/java.security=ALL-UNNAMED --add-opens java.base/java.io=ALL-UNNAMED'\n. ",
    "stkenny": "@ettorerizza I had to modify Open Refine to remove the dependency on xerces-j. It was the only way I could find to resolve classpath problems with the extension. I'll look at bringing the forks up to date to see if the extension works with the latest.. ",
    "joanneong": "I will try to work on this in the next few days :). Although I am technically no longer a teen, I am interested in giving this issue a shot :P However, just to clarify, what do you mean by \"the new logo\"? I can only see this:\n\nwhich does not say \"Google Refine\" so I thought that it has already been updated. Has another logo been created for OpenRefine to replace this? . Can I get some directions on how to update the website? I can't seem to find the relevant files in the Git repository so I am not too sure how to proceed.... I've tried to edit the logo a little by enlarging the \"OpenRefine\" in the logo. After making that change, the various pages now look like this:\nIndex page\n\nProject page\n\nPreferences page\n\nAbout page\n\nFor the index page I have decided to revert to the original text slogan because I realised that the slogan can actually be translated into a different language. I also left the favicon and the second diamond logo in the index page untouched from when I first opened my pull request yesterday.\n@wetneb @ettorerizza @thadguidry What do you make of this?. One last question: should I keep the older logos (Google Refine, the old Open Refine logos) around, or delete them?. Here's the README.md before the update:\n\n...and the README.md after the update:\n\nThe new logo is narrower but longer so I didn't try to enlarge it that much. Let me know if this is fine :). @thadguidry nope, I believe this PR is ready for review and merging. Meanwhile, I will be opening another pull request for task 3 (Add new logo to website) here instead.. Thanks for the response @ettorerizza ! This is because I am working on a new range function (which should return an array) so I was looking more into this issue. Hmm seems like I have to return a string for now then, thanks!. Got it, thanks for the prompt responses @thadguidry and @ettorerizza ! I will update this pull request soon :P. UPDATE SUMMARY\n1. The range function now returns an array and requires users to use .join(\",\") if they want to see it in the cells.\n\n\nThe range function now has a step parameter (optional).\n\n\nThe range function can accept 1 to 3 string/integer arguments:\n\n\n(a) 1 argument: 0 becomes the default range start, and the given argument is the range end\n      e.g. range(5) --> [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]\nrange(\"5\") --> [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]\n(b) 2 arguments: the first argument is the range start, and the second argument is the range end. If the range start is smaller than the range end, the sequence generated is an increasing sequence (increments by 1 each time). Otherwise, it is a decreasing sequence (decreases by 1 each time).\n      e.g. range(0, 5) --> [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]\nrange(5, 0) --> [\"5\", \"4\", \"3\", \"2\", \"1\", \"0\"]\n(c) 3 arguments: the first argument is the range start, the second argument the range end, and the third argument the step. Steps are all positive numbers, and the sequence generated is a decreasing one only if the range start is larger than the range end. Step 0 will produce an empty array. Steps that are bigger than the range will produce only the range start.\n      e.g. range(1, 5, 2) --> [\"1\", \"3\", \"5\"]\nrange(5, 1, 2) --> [\"5\", \"3\", \"1\"]\nrange(1, 5, 0) --> [\"\"]\nrange(1, 5, 100) --> [\"1\"]\n\n\nAny permutation of strings and integers is possible when using > 1 arguments for the range function (e.g. range(\"1\", 2) --> [\"1\", \"2\"]), except when the first argument is an integer and the last two arguments are strings (e.g. range(1, \"2, 3\") --> EvalError) (can be fixed if wanted).\n\n\nWhitespaces are still ignored as long as the values (string/ integer) are valid.\n\n\nBoth given range start and range end are included in the generated sequence.\n\n\nOnly the comma separator is supported to reduce code complexity. Dash operators can be replaced with commas by using a simple replace transformation prior to using the range function.\n\n\nLet me know your thoughts about this!\nP/S Woops! I realised I still forgot to support negative numbers. Will do it in a bit after I shower.\nPP/S Since arrays are now returned instead, the tests became a little unwieldy since I had to use the java String's join method to check against the expected output :/ Not sure if there's a better way to go about doing this...? Particularly since I notice that OpenRefine's testing seems to be quite lacking at the moment :P\n. @thadguidry Just to clarify, does that mean that for something like range(2, 4) you should get [\"2\", \"3\"] instead of [\"2\", \"3\", \"4\"] (i.e. exclude the STOP for all range operations? \nWhat about range(2, 2)? . UPDATE SUMMARY\nThe range function has been updated to follow Python's range function closely.\n\n\nThe range function returns an array and requires users to use .join(\",\") if they want to see it in the cells.\n\n\nThe range function can accept 1 to 3 string/integer arguments. Sequences generated include any given range start and exclude any given range end.\n\n\n(a) 1 argument: 0 becomes the default range start, and the given argument is the range end\ne.g. range(5) --> [\"0\", \"1\", \"2\", \"3\", \"4\"]\nrange(\"5\") --> [\"0\", \"1\", \"2\", \"3\", \"4\"]\nrange(-1) --> [\"\"]\n(b) 2 arguments: the first argument is the range start, and the second argument is the range end. If the range start is smaller than the range end, the sequence generated is an increasing sequence (increments by 1 each time). Otherwise, an empty array is returned.\ne.g. range(-1, 5) --> [\"-1\", \"0\", \"1\", \"2\", \"3\", \"4\"]\nrange(5, -1) --> [\"\"]\n(c) 3 arguments: the first argument is the range start, the second argument the range end, and the third argument the step. Steps that do not make sense (e.g. a negative step for a sequence where start < stop) will produce an empty array. Step 0 will produce an empty array. Steps that are bigger than the range will produce only the range start.\ne.g. range(1, 5, 2) --> [\"1\", \"3\"]\nrange(5, 1, -2) --> [\"5\", \"3\"]\nrange(1, 5, -2) --> [\"\"]\nrange(1, 5, 0) --> [\"\"]\nrange(1, 5, 100) --> [\"1\"]\n\n\nAny permutation of strings and integers is possible when using > 1 arguments for the range function.\ne.g. range(\"1\", 5, \"2\") --> [\"1\", \"3\"]\nrange(1, \"5, 2\") --> [\"1\", \"3\"]\nrange(\"1\", \"5, 2\") --> [\"1\", \"3\"]\netc.\n\n\nWhitespaces are ignored as long as the string value(s) is/are valid.\n\n\nOnly the comma separator is supported to reduce code complexity. Dash operators can be replaced with commas by using a simple replace transformation prior to using the range function.\n\n\nTests have been updated to check for these functions. Do let me know if there is still something else that I have missed out!\nP/S Do I have to refactor createRangeWithTwoGivenArguments? Codacy is complaining about it :/. @wetneb @ettorerizza \nI am aware that I could have returned either a string array or an integer array, so I just picked string because I felt that most of the inputs are likely to be strings (i.e. most likely from the data/cells rather than directly inputting integers in the transform box) so I might as well keep the output as a string array...\nAh but in that case Assert.assertArrayEquals seems to be more convenient for comparing integer arrays :P  \nWhat do you mean by an empty list? The reason why the array is empty but no error is thrown is because all the arguments are valid integers (not decimal etc.) but the step is negative even though the range start is smaller than the range end. The equivalent example in my unit test is range(1, 5, -1) -> [\"\"]. Also, Python's range function exhibits this behaviour too (or at least when I tested here)\n. @wetneb \nI went to think about it again, and I think another possible reason for using string arrays might be to improve performance. If I were to use an integer array, I have to use an Integer array rather than an int array because Integer is an Object but int is a primitive. This will require auto-boxing and auto-unboxing operations which can possibly make things slower, but I'll have to test this to be sure.\nIn any case, I do agree about the consistency, so I have updated the range function accordingly. So yes, now range(1, 5, -2) will return [ ]. Always ready for more suggestions!. Haha I just checked it out, it looks like this:\n\nWould you prefer it as a comment? I can fix it now.... I will work on this this weekend and get a PR up by next week :). My apologies for the long period of inactivity, @ostephens - had examinations over this period. I will get a PR up and running by this Sunday.. @ostephens I am not too familiar with the translation side of matters - when I create the new facet, do I manually add in a translation for each of the language supported by Open Refine as well (using Google Translate)?. A few hours ago I thought I received some email notification on keeping the \"Facet by blank\" command. Can I just check whether this facet should or should not be kept? Personally I think having all three facets (blank, null, empty string) can be confusing for new users, but I'm not sure about the perspective of seasoned users so.... Right now the simple solution is to put them like this:\n\nBut I think what everyone has said so far makes sense because this is indeed quite nested. Would shifting the blank, null, and empty string facets to a section in the same level as the scatterplot facet, but below that section work?. Update: \nThe English UI now looks like this:\n\nI have also updated the other translations using a mix of Google Translate and my own judgement as per whatever is already available. (And I can verify English and Chinese using my own knowledge haha)\nI noticed a minor UI problem prevalent across several languages. Here is an example:\n\nBasically, words protrude out of the menu because they are too long.\nBut the most severe issues come in for the Deutsch language, with the UI looking like this:\n\nSince I am aware that this is a work in progress, I will not touch it. Just raising a flag again anyway as a reminder :P \nI will be updating the tests for these facets soon.. My apologies for the delayed update. I was looking into the appropriate location to add tests for these facets, and so far I have found facets.js which seems to be testing a very small subset of the facets using some predefined data. I also realised that testing seems to be done using Windmill even though the page says that it is no longer being actively maintained and that Selenium should be used instead. \nIn light of this, should I (a) write tests following the format of the old tests using Windmill, (b) write tests using Selenium (but I am not sure if OpenRefine has taken up Selenium yet), or (c) not touch tests for the time being. What do you think?. @wetneb Got it. Give me some time to look into testing for Open Refine, and I will open a new issue to initiate some further discussion when I am ready :P. Oh I see, once I verify that the logo is working I will switch it back to an absolute link then :). Can I double check what you mean by this?. @jackyq2015 Thanks for bringing that up. I can understand Chinese so this did not occur to me. @wetneb What do you make of @jackyq2015 's suggestion to use \"\u6309null\u5f52\u7c7b\" instead? That is fine with me if it helps users.. Ah I see! One more question, do you think we should just keep the word \"null\" across all the different languages for consistency and in case such a scenario applies to other languages as well?. ",
    "nijel": "Weblate can also generate JSON file with empty translations for new language, would that work better for you?\nIf this is desired, just set \"Base file for new translations\" under File settings for your translation to translation to use a a template (translations will be removed when using it).. Weblate can also create pull requests with the changes, maybe that would be better fit for your project?\nI thought there is way to configure GitHub to dismiss checks for certain users for protected branch, but I've never used this feature myself, so I really can't help here.. Apparently the docs was wrong (I've just fixed that). It just has to be configured manually.. Here is first PR: https://github.com/OpenRefine/OpenRefine/pull/1375. ",
    "rezabj": "No. I tried it. The same problem.. I use OpenRefine in Docker container. Dockerfile attached.\nDockerfile.txt\nI think the container use UTF8.. It's help me. Thx.. ",
    "xseris": "@ettorerizza I know that could be dangerous, but in my personal implemetation is a \"tag-based remove all\". The upper bar contains all defined project tags, and clicking on one of them returns and shows a filtered list of projects that contain that specific tag. Then can be all removed in block (Of course with a warning message).  Answering your question: Yes, I can share this tag-system implementation, but i need some time to isolate the feature's code from other junk personal implemented stuff.. @ettorerizza I have isolate the tag system code (without \"delete all\" option for now), the following is a fast screencast of the feature...\n\nI created a PR ( #1357 ) if you want to test it more deeply. I implemented the feature quite long time ago when the metadata structure, introduced in OpenRefine 2.8, did not exist yet. That's why tags are treated as metadata in the code. I am not expert but if it is logically correct and/or it is better for future developments, we can keep tags separated from metadata. For the visualization part of the discussion: you focused pros and cons, I cannot say which solution is better.. @jackyq2015 I solved the null pointer exception in tests, but now I get another error: \"org.mockito.exceptions.verification.NoInteractionsWanted\" that i don't know how to fix it. Can you help me?. @jackyq2015 I tried locally but just moving tag column right after the name one does not solve the issue that you raised. This because the setAnyField method seems not to be able to interpret the tag string into an array (tags are actually stored as string array). \nI already solved it (and also moved the column right to name) with a workaround. Also the filter bar needed to be refreshed and the tags into the specific cell rebuild. I still have to solve a little GUI problem, but it will not take too much time (max 1 day).\nThen you are free to merge all modifications into your repo.. @jackyq2015 You can merge into your repo now. @jackyq2015 This is strange, I don't see those tags. There are not even tags injected in the code... . @thadguidry I think the problem is that there is no auto-update on form change. If you click outside the  form, or the update preview button, column names should change. Can you confirm this?. @wetneb you see it disabled because the \"Parse next\" checkbox is checked. The logic is either you parse next n lines as header or specify a custom one.. @wetneb I think one feature does not exclude the other. We can have a custom column name on different importers on project creation time, and an option to bulk change column names in the project page itself, if necessary.\nAgree that changing column names can be done in a prettier way, but usually char-separated strings are the faster way to bulk define or change column names of datasets with lots of fields. And, in my opinion, a char-separated string is also more reusable when datasets are similar.. Hi @thadguidry. Following @wetneb guidelines I've already moved all stuff on one single line and enabled/disabled options properly. This last commit produced the single \"Column names (comma separated)\" label. Check the box size if it is good for you.. @thadguidry now input area is below the label. @wetneb said to keep it on the same line, it would be good if you decide :smile:  I also think below is better.. @thadguidry What do you mean exactly? When the column names input changes, the preview automatically updates. there is no need to click the \"Update Preview\" button.. ",
    "quocvu": "Work thanks. . Just to confirm the special is not a problem. I tried w/ a different column and getting the same stacktrace\n[\n  {\n    \"op\": \"core/column-addition\",\n    \"description\": \"Create column item no at index 1 based on column Item # using expression grel:value\",\n    \"engineConfig\": {\n      \"mode\": \"row-based\",\n      \"facets\": []\n    },\n    \"newColumnName\": \"price2\",\n    \"columnInsertIndex\": 1,\n    \"baseColumnName\": \"Price\",\n    \"expression\": \"grel:value\",\n    \"onError\": \"set-to-blank\"\n  }\n]\n. @ostephens it is rather surprising that you where able to use -d with curl because the documentation says we need to post w/ multipart form data which is the -F \"operations=@test.json\" or -F \"operations={...some json here...}\"\nhttps://github.com/OpenRefine/OpenRefine/wiki/OpenRefine-API#apply-operations. $ curl --version\ncurl 7.56.0 (x86_64-pc-linux-gnu) libcurl/7.56.0 OpenSSL/1.1.0f zlib/1.2.11 libpsl/0.18.0 (+libicu/59.1) libssh2/1.8.0 nghttp2/1.23.1\nRelease-Date: 2017-10-04\nProtocols: dict file ftp ftps gopher http https imap imaps pop3 pop3s rtsp scp sftp smb smbs smtp smtps telnet tftp \nFeatures: AsynchDNS IPv6 Largefile GSS-API Kerberos SPNEGO NTLM NTLM_WB SSL libz TLS-SRP HTTP2 UnixSockets HTTPS-proxy PSL. I figured it out after trying different combo.  The documentation is inaccurate.\n\nThe project id parameter is passed as query params (current implementation) while document as a form param (which is cleaner to not mix query and form params in one request)\nthe form params are not multipart. Please see curl docs of -d vs -F (https://ec.haxx.se/http-postvspost.html).  Multipart make sense since we allow uploading the operations file\n\nThe quick fix is the documentation although I would prefer to change the code to match the documentation.\n$ /usr/bin/curl -s -d @test.json 'http://127.0.0.1:3333/command/core/apply-operations?project=2381194801492\nwith the test.json looking like this\noperations=[\n  {\n    \"op\": \"core/column-addition\",\n    \"description\": \"Create column item no at index 1 based on column Item # using expression grel:value\",\n    \"engineConfig\": {\n      \"mode\": \"row-based\",\n      \"facets\": []\n    },\n    \"newColumnName\": \"price2\",\n    \"columnInsertIndex\": 1,\n    \"baseColumnName\": \"Price\",\n    \"expression\": \"grel:value\",\n    \"onError\": \"set-to-blank\"\n  }\n]\n. after further examination, the -d @test.json is still not working as expected.  Because data is no longer multipart, thus must be url encoded.  For example, the + signs in the JSON file are not properly preserved.  I had to replace them w/ %2B to get things to work.  I don't think is what we wanted.. ",
    "Museumdatacleaner": "Hey, \nI am not importing a new project (have just started using openrefine on this machine). Below are example projects.\n\nWhen I try to open them it sits on this screen.\n\n\nNo errors that I can see.\n. Success! Must be IE browser, as it opens on Chrome. I should stop using IE in general...\nThanks!. ",
    "andriaz": "no errors, simply any change has be applied. I imported a csv in UTF8 (OR 2.8  under Ubuntu Linux). good, i've just discovered that i can edit directly from cells. The issue is when you try to edit cells from text facet. Here is my procedure. i imported this csv:\nv1,v2,v3\n\ud83d\ude4f\ud83c\udffd, \ud83d\udd34sardinia\ud83d\udd34, \ud83c\uddef\ud83c\uddf5\ud83c\uddf0\ud83c\uddf7\ud83c\uddec\ud83c\udde7\ud83c\uddef\ud83c\uddf5 \n\n. ",
    "ddu1": "I encountered this problem in a customized reconciliation service.. ",
    "tiagofernandez": "There you have it. Cheers. @ostephens are you sure importOptionMetadata was already available at the endpoint /command/core/get-project-metadata?project=<id>? Prior to my patch I couldn't find a way to retrieve them, so if you're right I was certainly missing a query parameter, configuration or something else (I forked and build the project from scratch). I also looked into ~/.local/share/openrefine/ and couldn't find that info in any of the saved files.\nWould any of you be able to clarify how to fetch the importOptionMetadata after creating a project?. @jackyq2015 That was probably it then. I'll revert that change, but keep and adapt the UT. Cheers.\n. cf. https://github.com/OpenRefine/OpenRefine/pull/1397. ",
    "lafaurie": "This is the xml file xml it dosen't import completed . ",
    "isaomatsunami": "please ignore the weblate version, which I made it to 100% just before I find the second file.. Why did not anyone find that we need \"find\" until 2.8?\nGreat improvement!!!!!!!. OK, I try to merge them, I will try and try.\nAnd I will recheck in other languages.. I am very sorry to bother dev teams. Please discard #1458 #1459 #1460 #1461 #1462 #1463 \nBasically it seems good in other languages. but I will reconsider more robust one. \nI will send it as a single PR, I will try. . Very sorry. My knowledge was obsolete.. What a new year gift! Thank you all dev-team for this extension!\nLet me explain my original request;\nBelieve it or not, these Japanese characters, for example, are same in one context, different in another context (in some cases it is the only key to distinguish people's names). Some share same unicode number, Some do not!\n(And there are unicode numbers even for non-existent characters and duplicated? characters. You will understand why Japan invented \u7d75\u6587\u5b57(emojis)!!!)\nPresumably each language has its own \"mysterious\" usages and it's the responsibility that each language user groups should take.\nThis is the original intention. But I hope this allows English-speaking users to invent the now-unimaginable ways of using OpenRefine.\nThank you @wetneb \n. ",
    "zhouyao1994": "@wetneb  oh thank you very much to remind me . i just find that there is no support for the keyboard shortcuts support~. ",
    "AlexandruAmarandei": "Hello, I think I've solved the main issue. It still needs the proper string id in https://github.com/OpenRefine/OpenRefine/tree/master/main/webapp/modules/core/langs. Until then I've just used Select Files to Import. Should I also apply this to the import project html?. @dbutlerdb  @ostephens @magdmartin , I wrote a solution but didn't have time to implement the rest of the code for the other browse buttons. Exams started so I'll see when I have time to finish it. Feel free to use/add to my solution.. ",
    "dbutlerdb": "Is this one solved?. Hi Thad, \nI just tested this in 3.0 and its doing the same thing. Below is before I transform the cells to a string\n\nThis is after. You'll notice that the (blanks) turned to null\n\n. Hi Owen,\nI'm having a hard time understanding why the default action when converting a numeric cell with no values into a string is to add 'null' into the cell. If operations like .toString[0,2] didn't lop off part of the 'null' string it would make more sense to me.  I am not the expert here like you guys so I'm sure there is a reason that I am missing. \nA work around it to exclude all nulls before making a transformation but that can be easily forgotten or unknown to a new user. Nulls/blanks were not handled this way in version 2.5 and lower.\n@thadguidry \nI was following David Huynh's tutorial and noticed the difference in \"3. Cell Editing\" on page 6.. Thank you for changing the functionality! . ",
    "zazi": "The project has been created after applying the mappings (JSON history) to the main project. After removal of the main project the project folder disappeared and the only things that has been left was the id of the main project in the workspace.json. Hence, the existence of this id causes (from my understanding) the creation of the (empty) project folder (after OpenRefine restart).. ",
    "YannBrrd": "Hello @magdmartin,\nOK, did not see that before. I'll have a look a it, code wise, but I'm a bit rusty TBH. Do you have a view about how to do that ? RefineOnSpark approach is something you'd accept ? Or a completely different architecture ? . The way I see things, we should implement various things : \n1. Loading datasets from various Data Sources, like HDFS, S3, ... (with a sampling option)\n2. Working locally on the loaded dataset, this should be the same as today, as far as I understand things\n3. Generate a Spark job when done locally, which is where the trick is...\n(side talk, woring in parquet locally is definitely a thing to consider https://tech.blue-yonder.com/efficient-dataframe-storage-with-apache-parquet/)\nSo, in the end, Using Spark would be :\n1. A matter of connectivity \n2. Translating GREL to Spark \nAm I wrong ? . OK. Regarding architecture \n\n. @fpompermaier maybe the right way to map history would be doing it to Beam, which will then be a bridge to many other frameworks (Spark, Flink, ....).\nWDYT ?. @magdmartin if we follow @fpompermaier suggestion, we could tackle the 2 tasks in parallel : \n1.  Separation of front/back \n2. Map History objects and GREL to target framework \nAnd because I count in ternary : \n3. Add a s3, hdfs, ... getter. ",
    "Bowiemb": "Is Refine-on-Spark (https://github.com/andreybratus/RefineOnSpark) currently the only way to run open-refine on spark? If so, has anyone had success with it?. Thank you @wetneb. I was afraid so. \nWith the new database connection feature, it's foreseeable to potentially query an Hbase in the future. Then, it would be wonderful if we could translate OpenRefine operations into Scala code.\nIf the backend was disconnected from the front end, this would make this easier since you could include OpenRefine more or less as a package, call the commands directly, and parse (and pass) the JSON as parameters. . @magdmartin That's a great idea. This would allow much easier batch processing of large tables. \nMoreover, would be great if there was a way to pull in new queries of data without having to create a new project. Maybe an extension that works something like:\n\nloop:\nquery next batch of data in preset batch size\nautomatically apply saved operations\ngenerate update statements\nclear data in project\n\nThen, you could spin up multiple nodes that pull from a single database, and these nodes could start crunching through the table in batches. That's not bad. . ",
    "pachamaltese": "@magdmartin at least my repo adds an icon and install dependencies\nin the future I shall create a PPA for upgrades. Sure! I'll do it tomorrow\nOn Tue, Jun 19, 2018, 10:07 PM Antonin Delpeuch notifications@github.com\nwrote:\n\nHi @pachamaltese https://github.com/pachamaltese - thanks for this PR!\nHowever, there is a problem: you deleted most files from our repository.\nFor OpenRefine to work, we need these files to stay.\nCan you try making a new pull request without deleting any files? Thanks.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/1660#issuecomment-398601121,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AJn6OY0Jc_0ZVpxKmowAdQoMHHqbeNttks5t-a5ogaJpZM4UtxcD\n.\n. Hi Martin\nNot yet. Work has been hectic lately. I'll try to have this next Sat.\n\nOn Sat, Jul 7, 2018, 8:21 AM Martin Magdinier notifications@github.com\nwrote:\n\n@pachamaltese https://github.com/pachamaltese any update on the new PR?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/1660#issuecomment-403211864,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AJn6OQV_fFhSPN9QYDWaBc2B0XWdT5YCks5uEKfNgaJpZM4UtxcD\n.\n. @magdmartin are you ok with a PPA or just a package that works for any Debian based distro?. @magdmartin @thadguidry @wetneb\nI created PR #1679 that keeps all the yml files for continuous integration. In order to create this Debian generic package (not limited to Ubuntu, I made it with generic Debian tools to make it compatible with Mint, Pop, etc) you should have build-essentials installed and then do this\n\ncd YOUR-GITHUB-FOLDER/OpenRefine\n./deb-create.sh\nthat script will create a subfolder titled deb-package that by using rsync contains a part of master branch files in order to create the final deb titled openrefine.deb\nin general maintaining a PPA is cooler in a sense that is easier for end users to update their OpenRefine version, but a more general (and harder) solution would be to get into a branch of the original Debian project (just like R and other stats software that have official deb packages via apt-get) or maybe provide a deb repository that is way better than a PPA (for example, R has its own repo that contains newer versions than those in the official Debian tree). do you have an svg version? the transparency is quite tricky for the icon\n\u2014\u2014\u2014\u2014\u2014\nMauricio Vargas Sep\u00falveda \u5e15\u590f\nDo you like Data Science? visit pacha.hk\n\u4f60\u7231\u79d1\u5b66\u6570\u636e\u4e13\u5417\uff1f\u4f60\u8d70pacha.hk\n2018-07-13 17:26 GMT-04:00 Codacy Bot notifications@github.com:\n\n[image: Codacy]\nhttps://camo.githubusercontent.com/b72992711a855bd9a03bfbdfc5ef7fcf528482f5/68747470733a2f2f6170702e636f646163792e636f6d2f6173736574732f696d616765732f66617669636f6e2e706e67\nHere is an overview of what got changed by this pull request:\nIssues\n======- Added 1\nSee the complete overview on Codacy\nhttps://app.codacy.com/app/OpenRefine/OpenRefine/pullRequest?prid=1902034&bid=8008957\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/1679#issuecomment-404958215,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AJn6OY_czk26S2Lww4_o8jCV84cJrQ62ks5uGRB0gaJpZM4VPdX8\n.\n. @thadguidry all set, new icon!. @thadguidry Hi. I was thinking about moving the bin to /opt just like chrome and other popular software. Give me a few minutes to edit the PR.. @thadguidry I had another idea that I just pushed. Now the package integrates better with Debian, it creates ~/.openrefine containing the default settings and the tools are stored there too.\nAlso, the full package is stored in /opt/openrefine and a link is added to /usr/bin in order to avoid touching the user PATH\n. IMO now it's ready to be reviewed\n\n\u2014\u2014\u2014\u2014\u2014\nMauricio Vargas Sep\u00falveda \u5e15\u590f\nDo you like Data Science? visit pacha.hk\n\u4f60\u7231\u79d1\u5b66\u6570\u636e\u4e13\u5417\uff1f\u4f60\u8d70pacha.hk\n2018-07-18 15:03 GMT-04:00 Thad Guidry notifications@github.com:\n\n@pachamaltese https://github.com/pachamaltese awesome. Let us know when\nyou think its fairly \"ready to ship\" and we'll review again.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/1679#issuecomment-406040035,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AJn6OfINdDsTib221tYUdhnWhbK15fmoks5uH4Z5gaJpZM4VPdX8\n.\n. no worries\n\ncool changes btw\nand yes, on debian java version > 8 works but sometimes creates problems\nbecause recent versions do not provide a javah path\n\u2014\u2014\u2014\u2014\u2014\nMauricio Vargas Sep\u00falveda \u5e15\u590f\nDo you like Data Science? visit pacha.hk\n\u4f60\u7231\u79d1\u5b66\u6570\u636e\u4e13\u5417\uff1f\u4f60\u8d70pacha.hk\n2018-08-07 5:40 GMT-04:00 Antonin Delpeuch notifications@github.com:\n\n@wetneb requested changes on this pull request.\nHi @pachamaltese https://github.com/pachamaltese, sorry for the delay\nin reviewing this. Because Travis build is failing, I was not expecting you\nto be waiting for feedback. The priority should be to fix the Travis build\n- I think we need to reconsider your changes to the refine script.\nI have left a couple of comments on the changes.\nIn DEBIAN/postrm\nhttps://github.com/OpenRefine/OpenRefine/pull/1679#discussion_r208162082\n:\n\n@@ -0,0 +1,11 @@\n+#!/bin/bash\n+\n+case \"$1\" in (remove)\n+ rm /usr/bin/openrefine /usr/share/applications/openrefine.desktop\n+ rm -rf /opt/openrefine\n+ rm -rf $HOME/.openrefine\n\nShould we really be deleting user data here? I think this should only\nhappen with the --purge option.\n\nIn refine\nhttps://github.com/OpenRefine/OpenRefine/pull/1679#discussion_r208162434\n:\n\n@@ -4,6 +4,18 @@\n #               OpenRefine Control System             #\n ##########################################################\n\n+# ---------------- user settings ------------------------\n+\n+if [ ! -d $HOME/.openrefine ]; then\n+ echo \"Creating user directory\"\n+ mkdir $HOME/.openrefine\nWe currently use .local/share/openrefine by default - why change to\n.openrefine?\n\nIn deb-exclude\nhttps://github.com/OpenRefine/OpenRefine/pull/1679#discussion_r208163059\n:\n\n@@ -0,0 +1,10 @@\n+.git\n+.github\n+.gitignore\n+.gitattributes\n+.travis.yml\n+appveyor.yml\n+DEBIAN\n+refine.bat\n+deb-exclude\n+deb-creation.sh\n\nCould we move this file to .deb-exclude maybe, so that it is hidden by\ndefault, just like .gitignore?\n\nIn DEBIAN/control\nhttps://github.com/OpenRefine/OpenRefine/pull/1679#discussion_r208163466\n:\n\n@@ -0,0 +1,12 @@\n+Package: openrefine\n+Version: 2.8\n+Maintainer: Mauricio Vargas mvargas@dcc.uchile.cl\n+Section: utils\n+Source: OpenRefine\n+Homepage: http://openrefine.org/\n+Architecture: amd64\n\nI think the architecture should not be restricted to amd64, as it should\nrun independently anywhere with a JVM, no?\n\nIn DEBIAN/control\nhttps://github.com/OpenRefine/OpenRefine/pull/1679#discussion_r208163555\n:\n\n@@ -0,0 +1,12 @@\n+Package: openrefine\n+Version: 2.8\n+Maintainer: Mauricio Vargas mvargas@dcc.uchile.cl\n+Section: utils\n+Source: OpenRefine\n+Homepage: http://openrefine.org/\n+Architecture: amd64\n+Priority: optional\n+Depends: default-jre\n\nShould we enforce Java 8 in the dependencies?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/1679#pullrequestreview-143913816,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AJn6Obq-mNrTqI8tTejcuuxPPzhvHxjYks5uOWCLgaJpZM4VPdX8\n.\n. and about the architecture I am not sure because that is more openrefine\nrelated\n\nlet me know your thoughts\n\u2014\u2014\u2014\u2014\u2014\nMauricio Vargas Sep\u00falveda \u5e15\u590f\nDo you like Data Science? visit pacha.hk\n\u4f60\u7231\u79d1\u5b66\u6570\u636e\u4e13\u5417\uff1f\u4f60\u8d70pacha.hk\n2018-08-07 9:10 GMT-04:00 Mauricio Vargas mauriciovargas@ug.uchile.cl:\n\nno worries\ncool changes btw\nand yes, on debian java version > 8 works but sometimes creates problems\nbecause recent versions do not provide a javah path\n\u2014\u2014\u2014\u2014\u2014\nMauricio Vargas Sep\u00falveda \u5e15\u590f\nDo you like Data Science? visit pacha.hk\n\u4f60\u7231\u79d1\u5b66\u6570\u636e\u4e13\u5417\uff1f\u4f60\u8d70pacha.hk\n2018-08-07 5:40 GMT-04:00 Antonin Delpeuch notifications@github.com:\n\n@wetneb requested changes on this pull request.\nHi @pachamaltese https://github.com/pachamaltese, sorry for the delay\nin reviewing this. Because Travis build is failing, I was not expecting you\nto be waiting for feedback. The priority should be to fix the Travis build\n- I think we need to reconsider your changes to the refine script.\nI have left a couple of comments on the changes.\nIn DEBIAN/postrm\nhttps://github.com/OpenRefine/OpenRefine/pull/1679#discussion_r208162082\n:\n\n@@ -0,0 +1,11 @@\n+#!/bin/bash\n+\n+case \"$1\" in (remove)\n+    rm /usr/bin/openrefine /usr/share/applications/openrefine.desktop\n+    rm -rf /opt/openrefine\n+    rm -rf $HOME/.openrefine\n\nShould we really be deleting user data here? I think this should only\nhappen with the --purge option.\n\nIn refine\nhttps://github.com/OpenRefine/OpenRefine/pull/1679#discussion_r208162434\n:\n\n@@ -4,6 +4,18 @@\n #               OpenRefine Control System             #\n ##########################################################\n\n+# ---------------- user settings ------------------------\n+\n+if [ ! -d $HOME/.openrefine ]; then\n+    echo \"Creating user directory\"\n+    mkdir $HOME/.openrefine\nWe currently use .local/share/openrefine by default - why change to\n.openrefine?\n\nIn deb-exclude\nhttps://github.com/OpenRefine/OpenRefine/pull/1679#discussion_r208163059\n:\n\n@@ -0,0 +1,10 @@\n+.git\n+.github\n+.gitignore\n+.gitattributes\n+.travis.yml\n+appveyor.yml\n+DEBIAN\n+refine.bat\n+deb-exclude\n+deb-creation.sh\n\nCould we move this file to .deb-exclude maybe, so that it is hidden by\ndefault, just like .gitignore?\n\nIn DEBIAN/control\nhttps://github.com/OpenRefine/OpenRefine/pull/1679#discussion_r208163466\n:\n\n@@ -0,0 +1,12 @@\n+Package: openrefine\n+Version: 2.8\n+Maintainer: Mauricio Vargas mvargas@dcc.uchile.cl\n+Section: utils\n+Source: OpenRefine\n+Homepage: http://openrefine.org/\n+Architecture: amd64\n\nI think the architecture should not be restricted to amd64, as it should\nrun independently anywhere with a JVM, no?\n\nIn DEBIAN/control\nhttps://github.com/OpenRefine/OpenRefine/pull/1679#discussion_r208163555\n:\n\n@@ -0,0 +1,12 @@\n+Package: openrefine\n+Version: 2.8\n+Maintainer: Mauricio Vargas mvargas@dcc.uchile.cl\n+Section: utils\n+Source: OpenRefine\n+Homepage: http://openrefine.org/\n+Architecture: amd64\n+Priority: optional\n+Depends: default-jre\n\nShould we enforce Java 8 in the dependencies?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/pull/1679#pullrequestreview-143913816,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AJn6Obq-mNrTqI8tTejcuuxPPzhvHxjYks5uOWCLgaJpZM4VPdX8\n.\n\n\n. @wetneb Hi. Sorry about the long delay, do you think is ok to put the DEB scripts in a separate repo?. @wetneb I moved everything to a new repo with standard deb packaging https://github.com/pachamaltese/openrefine-deb. @wetneb in any case the deb is here: https://github.com/pachamaltese/openrefine-deb/releases. @wetneb Hi. Pls let me know if I can help retaking the DEB package.. \n",
    "adamhadani": "@wetneb I think what I'm saying is simpler. Something like this:\n\nWhen I check a box to signify column should be included, It should be required to provide a non-empty value in the textbox next to it, since otherwise it just silently doesn't get sent which is not great UX. Right now I'm free to just check boxes and go about reconciling, which results in OpenRefine actually not sending any data to do with the checked columns.\nOn top of this, I suggest further that a sensible default value for that textbox, when I click the checkbox, is simply the name of the column (which is what appears to the left of the checkbox). Of course different use cases might call for customizing this, but this is a great starting point/default value to use when I check a box to indicate I wish a specific column should be passed along to backend. \n",
    "jcpuzs1": "Sir thank you. What I wanted to do is that I want to translate some files from your good open source and request a pull request from you sir so that I can present it to utopian.io. @wetneb thank you sir now I get it. Great help.. Sir good eve, sorry to bother you but I don't know how to pull request again. I want to pull request the 100% of my English to Filipino translation. I promised to finished it and here it goes. But my problem is I cant do pull request. Do u have guides of which I can follow ? Thank you sir. Wow, thank you sir God bless.. Sir Good day, Since I finished translation in Filipino. Can I also translate it to my dialect? which is Cebuano.. No, I should be the one to say thank you, Thank you sir. I promise to finish it all. More power and Godbless. Sir I'm sorry for the trouble and thank you for granting my request. God bless.. @wetneb,  sir can you help me. I accidentally press merge and I thought that I am not be the one to mwrge my own work. Im sorry sir I dont know what to do. please help.. ",
    "Cyyy1998": "Or can I translate it to Cebuano dialect ?. ",
    "dbucci24": "@wetneb hey, does this project need translation into Spanish? I would like to collaborate. ",
    "vinzruzell": "HI. I have already completed the file. I would like to ask  if there is another that needs translation for filipino and tagalog?. Thank you @wetneb. I will.. @wetneb I have created a new pull request with the right file folder. Thank you\n. Sorry for that @wetneb . I will resend. Thank you for the comments.\n. Thank you @wetneb . Thank you sir.. Hi sir. I have a concern because there is a conflict of file when making same file name. Thank you.. ",
    "kerim020": "@wetneb \nI can volunteer for the Turkish translation team.. ",
    "fokky": "Thanks, everyone, for your prompt and enthusiastic responses.\nAlthough I'm kind of a Wikidata veteran editor, I did not know about P1963 - I will surely have a look at that more often.\nRe: including the most basic metadata, I think nothing at the moment can beat Autodesc, so I think supporting this is a good move for the time being.\nI also applaud all other efforts that make disambiguation easier (i.e. needing less clicks and eliminating browser tab switches).. ",
    "Stolyassa": "Hello @thadguidry , Thank for the answer. I do not know why the error shows % symbol because there was not any in the actual path. I changed the path name with what you suggested as C:\\openrefine2.8\\\nThe issue contiunes just like before. Here is the error.\nYou have 4101M of free memory.\nYour current configuration is set to use 1400M of memory.\nOpenRefine can run better when given more memory. Read our FAQ on how to allocat\ne more memory here:\nhttps://github.com/OpenRefine/OpenRefine/wiki/FAQ:-Allocate-More-Memory\n23:27:21.294 [            refine_server] Starting Server bound to '127.0.0.1:333\n3' (0ms)\n23:27:21.297 [            refine_server] refine.memory size: 1400M JVM Max heap:\n 1468006400 (3ms)\n23:27:21.306 [            refine_server] Initializing context: '/' from 'C:\\open\nrefine2.8\\webapp' (9ms)\n23:27:21.805 [            refine_server] Failed to use jdatapath to detect user\ndata path: resorting to environment variables (499ms)\n23:27:21.806 [            refine_server] Failed to use jdatapath to detect user\ndata path: resorting to environment variables (1ms)\n23:27:21.815 [                   refine] Starting OpenRefine 2.8 [TRUNK]... (9ms\n)\nException in thread \"main\" java.lang.ExceptionInInitializerError\n        at org.python.util.PythonInterpreter.(PythonInterpreter.java:100)\n        at org.python.util.PythonInterpreter.(PythonInterpreter.java:94)\n        at org.python.util.PythonInterpreter.(PythonInterpreter.java:71)\n        at com.google.refine.jython.JythonEvaluable.(JythonEvaluable.jav\na:83)\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Nativ\ne Method)\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Native\nMethodAccessorImpl.java:62)\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(De\nlegatingMethodAccessorImpl.java:43)\n        at java.base/java.lang.reflect.Method.invoke(Method.java:564)\n        at org.mozilla.javascript.MemberBox.invoke(MemberBox.java:161)\n        at org.mozilla.javascript.NativeJavaMethod.call(NativeJavaMethod.java:24\n7)\n        at org.mozilla.javascript.optimizer.OptRuntime.callProp0(OptRuntime.java\n:119)\n        at org.mozilla.javascript.gen.c4._c1(file:/C:/openrefine2.8/webapp/exten\nsions/jython/module/MOD-INF/controller.js:46)\n        at org.mozilla.javascript.gen.c4.call(file:/C:/openrefine2.8/webapp/exte\nnsions/jython/module/MOD-INF/controller.js)\n        at org.mozilla.javascript.ContextFactory.doTopCall(ContextFactory.java:3\n98)\n        at org.mozilla.javascript.ScriptRuntime.doTopCall(ScriptRuntime.java:306\n5)\n        at org.mozilla.javascript.gen.c4.call(file:/C:/openrefine2.8/webapp/exte\nnsions/jython/module/MOD-INF/controller.js)\n        at edu.mit.simile.butterfly.ButterflyModuleImpl.scriptInit(ButterflyModu\nleImpl.java:636)\n        at edu.mit.simile.butterfly.ButterflyModuleImpl.init(ButterflyModuleImpl\n.java:94)\n        at edu.mit.simile.butterfly.Butterfly.initializeModule(Butterfly.java:47\n6)\n        at edu.mit.simile.butterfly.Butterfly.configure(Butterfly.java:451)\n        at edu.mit.simile.butterfly.Butterfly.init(Butterfly.java:308)\n        at org.mortbay.jetty.servlet.ServletHolder.initServlet(ServletHolder.jav\na:440)\n        at org.mortbay.jetty.servlet.ServletHolder.doStart(ServletHolder.java:26\n3)\n        at com.google.refine.RefineServer.configure(Refine.java:296)\n        at com.google.refine.RefineServer.init(Refine.java:208)\n        at com.google.refine.Refine.init(Refine.java:114)\n        at com.google.refine.Refine.main(Refine.java:108)\nCaused by: java.lang.IllegalArgumentException: Cannot create PyString with non-b\nyte value\n        at org.python.core.PyString.(PyString.java:57)\n        at org.python.core.PyString.(PyString.java:70)\n        at org.python.core.PyString.(PyString.java:74)\n        at org.python.core.Py.newString(Py.java:647)\n        at org.python.core.PyJavaType.init(PyJavaType.java:585)\n        at org.python.core.PyType.createType(PyType.java:1523)\n        at org.python.core.PyType.addFromClass(PyType.java:1462)\n        at org.python.core.PyType.fromClass(PyType.java:1551)\n        at org.python.core.PyObject.(PyObject.java:93)\n        at org.python.core.PySingleton.(PySingleton.java:9)\n        at org.python.core.PyNotImplemented.(PyNotImplemented.java:10)\n        at org.python.core.Py.(Py.java:94)\n        ... 27 more\n. The output is in below. ( I have no idea where i installed the python or if its in multiple directories.)\nMicrosoft Windows [Version 6.3.9600]\n(c) 2013 Microsoft Corporation. T\u00fcm haklari saklidir.\nC:\\Windows\\system32>python -V\nPython 3.6.3\nC:\\Windows\\system32>\n. ",
    "adambako": "thanks a lot for your fast answer.. \nfiles are there :) \ud83d\udc4d . ",
    "quan17": "@ettorerizza Thank you. \nFrom the API page, I guess the new find operation can help but I never test it. Since I am also afraid to update the Open Refine to 2.9 version, since we are developing based on 2.6 version. Is that possible to only get the new find function rather than upgrading the whole package? If I directly change the partition function return value form \"\" to null, based on your experience, will there more issues come out?\nI totally understand the reason you don't want to change partition here, however I believe another parameters can help us in this case.\nThe case I have is to distinguish the regex part has been matched or not, so split may not give me the result as expected.\n. @ostephens  Thank you.. @ettorerizza Unfortunately, the contains function does not support regular expression, the \"e\"here, sometimes can be a regex.. ",
    "gobertm": "@ettorerizza Sorry for late reply . Didn't  see the notification.\nI think the best solution is to insert null.\nINSERT INTO clipboard (Col1,Col2,Col3) VALUES \n( 'A','B','B' ),\n( 'C',null,'D' ),\n( 'D','E','E' )\nThis will be correct in regards of the input. Replacing by an empty string is not the same. An empty string is not null. \nIf I perform a Select * from clipboard where Col2 is null I wont' get the line if it's replaced with an empty string.\nMoreover it may be useful to add a \"Not null\" option in the export columns options. Because by default a CREATE TABLE t (  \nScore INT(2) ); allows null values. It will therefore be CREATE TABLE t (\n Score INT(2) NOT NULL); or even allowing the user to enter a default value in case of undesired null values in the input CREATE TABLE t (\n Score INT(2) NOT NULL DEFAULT 0 );. Hi , \nAs my first issue, I'd like to take this one. \nI already tried to implement it, this looks like this : \n\nBefore rushing into (my also first) PR I'll check the whole process before doing so (test code, documentation? ) \nAnd is it what you requested ? \nIt will work like the partition method second argument, meaning that contains(string s, string sub) will become contains(string s, string or regex sub)\nThank!\nMaxime. Hi,\n(had a few days off).\nI created PR #1558 .\nI can not find any particular test on this function.\nI tried creating mine, with simple line such as :\nAssert.assertEquals(invoke(\"contains\", \"it contains\", \"con\"),true);\nAssert.assertEquals(invoke(\"contains\", \"123contains\", \"\\\\d{3}\"),true);\nbut it seems I have to include a special evaluation function (something with Evaluable object) in order to interpret the regular expression.  I do not fully get this mechanism yet. So I created the PR as is. And look forward your input.\nRegards,\nMaxime. @ettorerizza thanks! This is better. I rushed with the partition function implementation..\nTest case works now.. @ettorerizza It depends what is considered string mode and regexp mode. \nIn OpenRefine do you accept regular expression inside double quotes or regular expression must only be in / /  ? \nBecause it seems that currently the behavior is not consistent throughout all functions : \n- find  (as you also mentioned here. \nbut also match\ninterprets regular expression in double quotes. \n\n\nreplace and partition only consider regexp when in / /\n\nMy original commit of contains this morning worked like replace and partition. That's why I couldn't write tests easily (regexp can not be passed as String).\nI guess you have to choose between the two interpretation.\n. @thadguidry \nI just checked all String functions mentionning regular expression.\nThose that accepts regex in \" \" are : find and match . Others are ok.\n@ettorerizza They indeed act like that currently. If I only keep the initial commit of this PR the contains function works like partition. . Hi,\nIs there something else to do in order to have this one completed?\nCheers,\nMaxime. Hi,\nI added tests based on @ettorerizza examples.\nString input , regular expression pattern injected as string are interpreted as string and for the tests of regular expression I tested by directly innjecting a Pattern.. ",
    "quirkiest": "Hi Antonin,\nActually I think both would be useful. The virtualenv definitely the way to go for preloading of custom classes etc, but also prefs available in UI so that we can parameterize modules so that non-coders can configure them. E.g. a character strip file called via pref(\u2018stripfile\u2019) with a value of c:\\foo\\bar\\somefile.txt. User can then define & populate the file, and call functions which reference it from classes put into the virtualenv.\nWarwick\nSent from my iPad Mini\n\nOn Mar 27, 2018, at 20:12, Antonin Delpeuch notifications@github.com wrote:\nCan you give a bit more detail about how that would work? What sort of python expressions would you be able to write with this feature?\nIntuitively I would be more in favor of exposing a clean virtualenv inside OpenRefine's folder where users could directly write their python modules (or install existing ones) that would be accessible from the scripts.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Exactly correct!\n\nCheers,\nWarwick\nOn Wed, Mar 28, 2018 at 1:05 AM, Thad Guidry notifications@github.com\nwrote:\n\nNot against this issue, but... this issue has come up before where there's\nautomation needs for scripting especially around batch processing. Most\ndata architects do this already in other tools. I personally use Apache\nNiFi for that, where my users can actually modify the data flow even at\nruntime via Python scripts and change environment variables on a whim.\nI'm curious about your less technical users, since that's exactly the user\nbase we tried to design for with OpenRefine. When you say \"cleanse a file\",\nI guess you mean there's an already developed Python script that the team\nshares and it takes arguments, and you want to store those arguments as a\npreference or environment variable ? So that they can just choose the\ncustom Python script and quickly configure it ?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1555#issuecomment-376537322,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AkExxcJLdLIWPEzE8xUtDUe_fFa071c6ks5tikcMgaJpZM4S8XSU\n.\n. Awesome!\nCheers,\n\nWarwick\nOn Thu, Feb 28, 2019 at 1:38 AM Thad Guidry notifications@github.com\nwrote:\n\nfixed above\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1677#issuecomment-467885679,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AkExxeo11kRf09DClEdX4uaOpFNnpSpoks5vRphcgaJpZM4VOdtR\n.\n. \n",
    "Vern1erCa11per": "Thank you for  reviewing.  So, what can I do for it ? Only wait? . Thank you, @wetneb,  That's what I want !!!  I also require what sort of change it made.\n. ",
    "Gautamshahi": "Hi,\nI tried with that also but its giving \"internal error\", If possible can you\nplease share a link or sample code?\nRegards,\nOn Thu, Apr 19, 2018 at 12:30 AM, Thad Guidry notifications@github.com\nwrote:\n\nYes, just choose Python or Clojure as your scripting language...rather\nthan GREL. It will be easier for you.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1569#issuecomment-382494057,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ADyFFp8rHUHBljsPJyUk2zq5mxb7bS4jks5tp41QgaJpZM4TagGc\n.\n\n\n-- \nGautam Kishore Shahi,\nMaster Student,\nDISI- University of Trento,\nItaly\n. Thank you.\nGREL doesn't allow command like toDate(value.match(/.*?\\/+(.+)/)[0]) Any suggestion.\n. Thank you.. Hi,\nPlease read the query properly, Everytime I get a same answer.\nThe SPARQL endpoint is working but reconcilation is not working which is a\npart of open refine.\nAnyway thanks a lot.\nOn Wed, Jul 25, 2018, 02:02 Thad Guidry notifications@github.com wrote:\n\n@Gautamshahi https://github.com/Gautamshahi Our documentation for\nReconciliation\nhttps://github.com/OpenRefine/OpenRefine/wiki/Reconciliation\nI've added a special NOTE now to highlight this now:\n\"NOTE: OpenRefine Reconciliation does NOT work against a SPARQL endpoint\nat this time (#1212 https://github.com/OpenRefine/OpenRefine/issues/1212),\nbut instead only a Reconciliation Service API.\"\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1686#issuecomment-407590007,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ADyFFqb8iCGA0VJtbQPnuspIJ6boX9JMks5uJ7WVgaJpZM4VfM-h\n.\n. Thanks a lot.. \n",
    "joniarroba": "Thanks for your prompt response! Looking forward to see it! For now we added manually the JSON file. . ",
    "lucaswerkmeister": "Sorry, that doesn\u2019t really help me, there\u2019s some big misunderstanding between us \u2013 I\u2019m definitely not interested in limiting anything :)\nSay I have a data file like this:\nBundestag ID    Wikidata ID date of birth   place of birth  date of death\n11000001    Q109221 20.10.1930  Stuttgart   17.01.2008\n11000002    Q214251 09.04.1909  Siegen  02.12.1991\n11000003    Q566240 26.05.1913  Parabutsch  18.02.1994\n11000004    Q109322 06.11.1933  Berlin  \n11000005    Q96630  09.06.1950  Teterow, Kr. Teterow, Bezirk Neubrandenburg \n11000007    Q2171984    10.11.1919  Masburg / Kreis Cochem  25.05.2013\n11000008    Q1500263    28.09.1912  D\u00fcsseldorf  25.01.1992\n11000009    Q2492   05.01.1876  K\u00f6ln    19.04.1967\n11000010    Q916089 22.06.1944  Drangstedt / Krs. Weserm\u00fcnde    25.10.2004\n11000011    Q1287863    31.10.1920  M\u00fcnchen 28.12.2000\n\u2026\nI want to import it into OpenRefine and use column 2 as Wikidata IDs directly, without going through the lengthy reconciliation process. I suppose this could also be phrased as \u201cset the match for each cell based on an expression\u201d, where in my case the expression would simply be value. Does that make more sense?. Note that, as far as I\u2019m aware, it\u2019s not yet clear what this will mean for the instances we\u2019re not hosting\u00a0:) I currently see two possibilities:\n\nKeep username/password authentication and use that if no OAuth consumer has been configured.\nAutomatically register an owner-only consumer (which does not require review by Wikidata administrators) and use that if no other OAuth consumer has been configured.. (The advantage of option 2 would be that we don\u2019t need to store the user\u2019s password indefinitely, we only need it once to register the owner-only consumer and then store the consumer/access key/secret instead. And I suppose this would automatically group all edits made from that instance via the OAuth revision tag.). BTW the Phabricator task for a hosted OpenRefine instance is T194767.. Yeah, and apparently owner-only tokens don\u2019t support the usual \u201cthree-legged OAuth flow\u201d, you have to use the \u201cone-legged OAuth flow\u201d instead, so we won\u2019t even be able to reuse all the code from full OAuth\u2026 it\u2019s probably much simpler to just stick to normal username/password authentication for local instances.. I think you have to explicitly set the bot parameter in action=edit or action=wbeditentity, and have the bot right (usually: be part of the \u201cBots\u201d group).. Are you sure that\u2019s not a bug in OAbot? You\u2019re setting data['bot'] = '', and it looks like that might cause Python requests to completely drop the parameter:\n\n```\n$ python3\nPython 3.6.5 (default, Apr  1 2018, 05:46:30) \n[GCC 7.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport requests\ndata = {'action':'edit'}\nif 'yes':\n...     data['bot'] = ''\n...   \nr = requests.post(\"http://httpbin.org/post\", data=data)\nprint(r.text)\n{\"args\":{},\"data\":\"\",\"files\":{},\"form\":{\"action\":\"edit\"},\"headers\":{\"Accept\":\"/\",\"Accept-Encoding\":\"gzip, deflate\",\"Connection\":\"close\",\"Content-Length\":\"11\",\"Content-Type\":\"application/x-www-form-urlencoded\",\"Host\":\"httpbin.org\",\"User-Agent\":\"python-requests/2.18.4\"},\"json\":null,\"origin\":\"158.109.94.211\",\"url\":\"http://httpbin.org/post\"}\n``\nNotice how theformkey only hasaction: edit, but nobot..data = (('action', 'edit'), ('bot', ''))(list of tuples instead of dict) works, though. (That is, it sends an emptybotparam in the request \u2013 I don\u2019t know whether MediaWiki will interpret it correctly! Probably safest to simply usebot=1.). Thought: I think it would be best to use a consumer with a fixed callback URL (OpenRefine just sendsoauth_callback=oobtoSpecial:OAuth/initiate`), so that OpenRefine doesn\u2019t need to know its public hostname. That way, only two configuration variables should be necessary: consumer key and consumer secret.. \n\n\n",
    "mungewell": "I am attempting to validate/cleanup data provided by a 3rd party. The data is from 2 sheets of a XLS which I have imported as separate projects, they have already (poorly) cross referenced it.\nNot sure where I might find the 'backend' info, didn't see any log files under:\nC:\\Users\\swood\\AppData\\Roaming\\OpenRefine. ",
    "martinjrobins": "Can do, created a pull request here #1605. At the moment it looks like you are testing against Oracle JDK 8 on travis. it would be simple enough to add Oracle JDK 9 and Open JDK 6-8, as travis seems to support these (https://docs.travis-ci.com/user/reference/trusty/#JavaScript-and-Node.js-images). Travis does not currently support version 10, although this looks to be in the works (https://github.com/travis-ci/travis-ci/issues/9368)\nThe reason why I ran into this issue is that Ubuntu 18.04 uses open JDK 10.0.1 as the default version, so  I would suggest that blocking OpenRefine use on this platform is a bad idea. OR worked fine for me using Open JDK 10, just playing around applying filters to a single dataset, although this was certainly not thorough testing. Perhaps the error could be changed to a warning for newer, untested versions of java? Then once travis supports Open/Oracle JDK 10 start testing and update the warning?. Added a warning for Java versions > 9, see what you think.. ",
    "yaeln": "No, did not help\nMacBook-Pro-3:MacOS yaeln$ ls\nJavaAppLauncher\nMacBook-Pro-3:MacOS yaeln$ ./JavaAppLauncher \n17:06:24.474 [            refine_server] Starting Server bound to '127.0.0.1:3333' (0ms)\n17:06:24.489 [            refine_server] Initializing context: '/' from '/Applications/OpenRefine.app/Contents/Resource/webapp' (15ms)\n17:06:25.099 [                   refine] Starting OpenRefine 3.0-beta [TRUNK]... (610ms)\n17:06:25.099 [                   refine] initializing FileProjectManager with dir (0ms)\n17:06:25.099 [                   refine] /Users/yaeln/Library/Application Support/OpenRefine (0ms)\n17:06:26.605 [       database-extension] Initializing OpenRefine Database... (1506ms)\n17:06:26.608 [       database-extension] Database Extension Mount point /extension/database/ [*] (3ms)\n17:06:26.608 [       database-extension] Registering Database Extension Commands...... (0ms)\n17:06:26.625 [       database-extension] Database Extension Command Registeration done!! (17ms)\n17:06:26.625 [       database-extension] Database Operations Registered successfully... (0ms)\n17:06:26.625 [       database-extension] Database Functions Registered successfully... (0ms)\nException in thread \"main\" java.lang.LinkageError: loader constraint violation: loader (instance of edu/mit/simile/butterfly/ButterflyClassLoader) previously initiated loading for a different type with name \"org/slf4j/Logger\"\n    at java.lang.ClassLoader.defineClass1(Native Method)\n    at java.lang.ClassLoader.defineClass(ClassLoader.java:760)\n    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n    at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\n    at java.net.URLClassLoader.access$100(URLClassLoader.java:73)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:368)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:362)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:361)\n    at edu.mit.simile.butterfly.ButterflyClassLoader.loadClass(ButterflyClassLoader.java:51)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n    at com.google.refine.extension.database.DatabaseModuleImpl.readModuleProperty(DatabaseModuleImpl.java:93)\n    at com.google.refine.extension.database.DatabaseModuleImpl.init(DatabaseModuleImpl.java:68)\n    at edu.mit.simile.butterfly.Butterfly.initializeModule(Butterfly.java:476)\n    at edu.mit.simile.butterfly.Butterfly.configure(Butterfly.java:451)\n    at edu.mit.simile.butterfly.Butterfly.init(Butterfly.java:308)\n    at org.mortbay.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:440)\n    at org.mortbay.jetty.servlet.ServletHolder.doStart(ServletHolder.java:263)\n    at com.google.refine.RefineServer.configure(Refine.java:296)\n    at com.google.refine.RefineServer.init(Refine.java:208)\n    at com.google.refine.Refine.init(Refine.java:114)\n    at com.google.refine.Refine.main(Refine.java:108)\n. Still the same behavior.\nThe question is how can I import all of my 2.7/ 2.8 projects to the 3.0 (it runs well on an empty directory)?. Here is a link to my (quite large) library\nhttps://www.dropbox.com/s/p69fph1qrt0tpv8/OpenRefine.zip?dl=0\nattached also my ls -ld before and after  + error of running\nOn Thu, Jun 7, 2018 at 4:33 AM Jacky notifications@github.com wrote:\n\n@yaeln https://github.com/yaeln The old projects should be\nautomatically imported.Also I noticed that you installed OR under\n/Applications/OpenRefine3.app/, the default installation folder should\n/Applications/OpenRefine.app/.\nCan you do a ls -ld /Applications/OpenRefine* before and after your\ninstallation? or you can upload somewhere as suggested by @thadguidry\nhttps://github.com/thadguidry so I can take a look.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1636#issuecomment-395264449,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AE4Oplc9aB4lsHLa_CPbQ_wrtS-0OAYiks5t6IL4gaJpZM4UX8yy\n.\n. Version 2.8 works fine!\nHowever, I don't know where the installation/refine.ini file is..\n\nThanks a lot!\nOn Thu, Jun 7, 2018 at 10:01 PM Jacky notifications@github.com wrote:\n\n@yaeln https://github.com/yaeln Can you install version 2.8 and see if\nit's working? Seems it is related to the location of the path which has a\nspace in between. I cannot find any code change might be affecting this.\nAlso my version of MacOS Mavericks works fine and the old projects can be\nimported automatically.\nAlso you can try to move your project to:\nmkdir /Users/yaeln/.openrefine/\nmv /Users/yaeln/Library/Application Support/OpenRefine/* /Users/yaeln/.openrefine/\nThen edit the refine.ini under installation folder:\nJAVA_OPTIONS=-Drefine.data_dir=/Users/yaeln/.openrefine/\nLet me know the result.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1636#issuecomment-395529638,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AE4OppUeyySWdTucE3vuoy4THl0u5B2Jks5t6XiQgaJpZM4UX8yy\n.\n. Thank you very much\n\nOn Fri, Jun 8, 2018 at 5:38 AM Jacky notifications@github.com wrote:\n\n@yaeln https://github.com/yaeln Please ignore it won't work. Seems\nrelated to the data itself instead of installation. will do some debug on\nthe projects you shared and get back to you.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1636#issuecomment-395627858,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AE4OpoKBunTZzg6RK6i9H7N66iF13GVtks5t6eO5gaJpZM4UX8yy\n.\n. Here is the log for current run, quite strange.\n\nI need to start it all over again: I work with openrefine almost on a daily\nbasis. Would love to use the 3.0 version - what would you suggest?\nMacBook-Pro-3:MacOS yaeln$ pwd\n/Applications/OpenRefine.app/Contents/MacOS\nMacBook-Pro-3:MacOS yaeln$ ./JavaAppLauncher\n09:32:04.500 [            refine_server] Starting Server bound to '\n127.0.0.1:3333' (0ms)\n09:32:04.527 [            refine_server] Initializing context: '/' from\n'/Applications/OpenRefine.app/Contents/Resource/webapp' (27ms)\n09:32:05.823 [                   refine] Starting OpenRefine 3.0-beta\n[TRUNK]... (1296ms)\n09:32:05.823 [                   refine] initializing FileProjectManager\nwith dir (0ms)\n09:32:05.823 [                   refine] /Users/yaeln/Library/Application\nSupport/OpenRefine (0ms)\n09:32:05.848 [       FileProjectManager] Failed to load workspace from any\nattempted alternatives. (25ms)\n09:32:07.768 [       database-extension] Initializing OpenRefine\nDatabase... (1920ms)\n09:32:07.809 [       database-extension] Database Extension Mount point\n/extension/database/ [*] (41ms)\n09:32:07.809 [       database-extension] Registering Database Extension\nCommands...... (0ms)\n09:32:07.857 [       database-extension] Database Extension Command\nRegisteration done!! (48ms)\n09:32:07.858 [       database-extension] Database Operations Registered\nsuccessfully... (1ms)\n09:32:07.859 [       database-extension] Database Functions Registered\nsuccessfully... (1ms)\n09:32:07.886 [       DatabaseModuleImpl]  Database Extension Module\nInitialization Completed!! (27ms)\n09:32:18.998 [       database-extension] receiving request for\nstyles/jquery.contextMenu.css (11112ms)\n09:32:18.998 [       database-extension] receiving method for GET (0ms)\n09:32:19.015 [       database-extension] receiving request for\nstyles/pure.css (17ms)\n09:32:19.015 [       database-extension] receiving method for GET (0ms)\n09:32:19.053 [       database-extension] receiving request for\nstyles/bootstrap.css (38ms)\n09:32:19.053 [       database-extension] receiving method for GET (0ms)\n09:32:19.276 [       database-extension] receiving request for\nstyles/database-import.less (223ms)\n09:32:19.276 [       database-extension] receiving method for GET (0ms)\n09:32:20.465 [                   refine] POST /command/core/load-language\n(1189ms)\n09:32:20.525 [                   refine] GET /command/core/get-preference\n(60ms)\n09:32:20.552 [                   refine] POST /command/core/load-language\n(27ms)\n09:32:20.565 [                   refine] POST /command/core/load-language\n(13ms)\n09:32:20.862 [                   refine] POST\n/command/core/get-importing-configuration (297ms)\n09:32:20.899 [                   refine] GET\n/command/core/get-all-project-tags (37ms)\n09:32:20.940 [                   refine] GET\n/command/core/get-all-project-metadata (41ms)\n09:32:21.047 [                   refine] GET /command/core/get-languages\n(107ms)\n09:32:21.124 [                   refine] GET /command/core/get-version\n(77ms)\n09:32:21.532 [       database-extension] receiving request for\nscripts/index/database-import-form.html (408ms)\n09:32:21.533 [       database-extension] receiving method for GET (1ms)\n09:32:21.551 [                   refine] GET\n/command/database/saved-connection (18ms)\n^Z\n[1]+  Stopped                 ./JavaAppLauncher\nMacBook-Pro-3:MacOS yaeln$ bg\n[1]+ ./JavaAppLauncher &\nMacBook-Pro-3:MacOS yaeln$ cd '/Users/yaeln/Library/Application\nSupport/OpenRefine'\nMacBook-Pro-3:OpenRefine yaeln$ ls\ndbextension\nMacBook-Pro-3:OpenRefine yaeln$ cd dbextension/\nMacBook-Pro-3:dbextension yaeln$ ls\nMacBook-Pro-3:dbextension yaeln$\nOn Fri, Jun 8, 2018 at 2:29 PM Jacky notifications@github.com wrote:\n\nI can open the projects you shared using the same version. so it is not a\ndata issue.\nDo you still see the exception after install, if so please share the whole\nlog as you did.\nCould you please click \"open project\" then click the \"Browse workspace\ndirectory\" at the bottom and let me know the folder location? you should be\nable to see all of you projects in its folders and at least a file called\nworkspace.json etc.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1636#issuecomment-395733127,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AE4OpkGRetC0q1NMhjMEa8og0EX_0fudks5t6mAhgaJpZM4UX8yy\n.\n. Jacky, thanks for your answers: the reason the workspace was empty because\nI tried again to run openrefine3\nbut when the directory is not empty still the same problem\n\nMacBook-Pro-3:openrefine-3.0-beta yaeln$ ./refine\nYou have 229M of free memory.\nYour current configuration is set to use 1400M of memory.\nOpenRefine can run better when given more memory. Read our FAQ on how to\nallocate more memory here:\nhttps://github.com/OpenRefine/OpenRefine/wiki/FAQ:-Allocate-More-Memory\nStarting OpenRefine at 'http://127.0.0.1:3333/'\n11:58:49.918 [            refine_server] Starting Server bound to '\n127.0.0.1:3333' (0ms)\n11:58:49.920 [            refine_server] refine.memory size: 1400M JVM Max\nheap: 1407188992 (2ms)\n11:58:49.941 [            refine_server] Initializing context: '/' from\n'/Users/yaeln/Dropbox/OpenRefine3/openrefine-3.0-beta/webapp' (21ms)\n11:58:50.880 [                   refine] Starting OpenRefine 3.0-beta\n[TRUNK]... (939ms)\n11:58:50.881 [                   refine] initializing FileProjectManager\nwith dir (1ms)\n11:58:50.881 [                   refine] /Users/yaeln/Library/Application\nSupport/OpenRefine (0ms)\n11:58:52.570 [       database-extension] Initializing OpenRefine\nDatabase... (1689ms)\n11:58:52.574 [       database-extension] Database Extension Mount point\n/extension/database/ [*] (4ms)\n11:58:52.574 [       database-extension] Registering Database Extension\nCommands...... (0ms)\n11:58:52.591 [       database-extension] Database Extension Command\nRegisteration done!! (17ms)\n11:58:52.591 [       database-extension] Database Operations Registered\nsuccessfully... (0ms)\n11:58:52.592 [       database-extension] Database Functions Registered\nsuccessfully... (1ms)\nException in thread \"main\" java.lang.LinkageError: loader constraint\nviolation: loader (instance of\nedu/mit/simile/butterfly/ButterflyClassLoader) previously initiated loading\nfor a different type with name \"org/slf4j/Logger\"\nat java.lang.ClassLoader.defineClass1(Native Method)\nat java.lang.ClassLoader.defineClass(ClassLoader.java:760)\nat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\nat java.net.URLClassLoader.defineClass(URLClassLoader.java:455)\nat java.net.URLClassLoader.access$100(URLClassLoader.java:73)\nat java.net.URLClassLoader$1.run(URLClassLoader.java:367)\nat java.net.URLClassLoader$1.run(URLClassLoader.java:361)\nat java.security.AccessController.doPrivileged(Native Method)\nat java.net.URLClassLoader.findClass(URLClassLoader.java:360)\nat\nedu.mit.simile.butterfly.ButterflyClassLoader.loadClass(ButterflyClassLoader.java:51)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\nat\ncom.google.refine.extension.database.DatabaseModuleImpl.readModuleProperty(DatabaseModuleImpl.java:93)\nat\ncom.google.refine.extension.database.DatabaseModuleImpl.init(DatabaseModuleImpl.java:68)\nat edu.mit.simile.butterfly.Butterfly.initializeModule(Butterfly.java:476)\nat edu.mit.simile.butterfly.Butterfly.configure(Butterfly.java:451)\nat edu.mit.simile.butterfly.Butterfly.init(Butterfly.java:308)\nat\norg.mortbay.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:440)\nat org.mortbay.jetty.servlet.ServletHolder.doStart(ServletHolder.java:263)\nat com.google.refine.RefineServer.configure(Refine.java:296)\nat com.google.refine.RefineServer.init(Refine.java:208)\nat com.google.refine.Refine.init(Refine.java:114)\nat com.google.refine.Refine.main(Refine.java:108)\n^CMacBook-Pro-3:openrefine-3.0-beta yaeln$ cd\n'/Users/yaeln/Library/Application Support/OpenRefine'\nMacBook-Pro-3:OpenRefine yaeln$ cd -\n/Users/yaeln/Dropbox/OpenRefine3/openrefine-3.0-beta\nMacBook-Pro-3:openrefine-3.0-beta yaeln$ ls\n'/Users/yaeln/Library/Application Support/OpenRefine'\n1446580682893.project 1666186142080.project 1793693898613.project\n1918520131795.project 2020263392516.project 2136998469473.project\n2259939700194.project 2402041926607.project\n1489218330389.project 1670302201997.project 1799042444002.project\n1923689059576.project 2031728960862.project 2139101206950.project\n2264253659541.project 2409171240385.project\n1489364242205.project 1677344519612.project 1800007913009.project\n1924573721718.project 2032311738138.project 2144524865786.project\n2272088438167.project 2414008925260.project\n1501796329251.project 1687143955008.project 1805021170114.project\n1932591736306.project 2041547207055.project 2159414752919.project\n2273768868611.project 2414378949082.project\n1509132215183.project 1688064656376.project 1818391931890.project\n1937502301779.project 2041555231399.project 2175974709736.project\n2276183242047.project 2417684623211.project\n1530528190323.project 1688773663886.project 1841913137661.project\n1942523210146.project 2043587037229.project 2182249138150.project\n2282262430452.project 2418843375818.project\n1532242424112.project 1702286269219.project 1842698519929.project\n1945899236298.project 2050449891603.project 2199657297131.project\n2286341239207.project 2421289426309.project\n1538911124566.project 1716419152512.project 1850797696343.project\n1946853425096.project 2053801477002.project 2200290950472.project\n2290251681469.project 2426789312953.project\n1549921280123.project 1717878707118.project 1851159164491.project\n1952727197627.project 2057187941191.project 2203235113420.project\n2302081944383.project 2427499263716.project\n1553937602167.project 1719178273314.project 1854933002698.project\n1966260829138.project 2064936534813.project 2205995292637.project\n2303846768545.project 2431993061547.project\n1565172332019.project 1724547469113.project 1858018257345.project\n1975702946577.project 2065958278745.project 2208441215130.project\n2305460339776.project 2440100340100.project\n1567203187209.project 1732489578292.project 1858077236625.project\n1976009328081.project 2067034566918.project 2213216606268.project\n2318516316786.project 2445864230447.project\n1571978127561.project 1733972861179.project 1869203724432.project\n1977709691762.project 2067863863631.project 2216638548473.project\n2327000760945.project 2455159320072.project\n1587825598164.project 1743386847322.project 1878880344349.project\n1982646960318.project 2075642828002.project 2222643026646.project\n2339788063806.project 2462081905625.project\n1588840134378.project 1746472567069.project 1880802807866.project\n1986019738472.project 2085520177557.project 2223058456840.project\n2347538514479.project 2464898644573.project\n1599936691371.project 1751135287300.project 1881060793986.project\n1989339977948.project 2089572327315.project 2223467505826.project\n2351930441513.project 2468646542835.project\n1610771101153.project 1765375004976.project 1884146487283.project\n1994193289483.project 2096614283835.project 2235555532353.project\n2357055151319.project 2474040628970.project\n1614869370298.project 1769290954109.project 1885136041021.project\n1995697485713.project 2099572626764.project 2241416007337.project\n2372871461418.project 2513584509629.project\n1615028782997.project 1773964629200.project 1885362571233.project\n1997538651930.project 2111856832805.project 2242409430241.project\n2376358755012.project cache\n1620294021385.project 1778048960086.project 1889307503617.project\n2013137542546.project 2124493779831.project 2243309603691.project\n2385830905655.project extensions\n1648032973528.project 1780825948647.project 1893484483544.project\n2017480276292.project 2127151184698.project 2253906528238.project\n2392043806733.project workspace.json\n1648050396339.project 1783378714077.project 1903575896504.project\n2019052398606.project 2129382286096.project 2254693892092.project\n2400494862316.project workspace.old.json\nMacBook-Pro-3:openrefine-3.0-beta yaeln$\nOn Sun, Jun 10, 2018 at 6:18 PM Jacky notifications@github.com wrote:\n\n@yaeln https://github.com/yaeln Your default workspace is empty. I\nthink for some reason you 2.8 version points to different workspace other\nthan /Users/yaeln/Library/Application Support/OpenRefine. You may be did\nthe change and forgot.\nThe solution is simple. you can just find the original workspace and copy\nall to the above folder. If you don't know where the original workspace is,\nyou can just startup the 2.8 the same way you did above and you will find\nthe location in the log by searching the line:\ninitializing FileProjectManager with dir\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1636#issuecomment-396057164,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AE4OpsOLKpG4g57U60tRs0rkYNiff8amks5t7TiogaJpZM4UX8yy\n.\n. Yap, I did it, still error. Installation package - do you mean the\nopenrefine.app dir? can be downloaded from here\nhttps://www.dropbox.com/s/9qz3p47kqinxu1h/OpenRefine.app.zip?dl=0\n\nAgain, this is the log\nMacBook-Pro-3:OpenRefine.app yaeln$ cd ..\nMacBook-Pro-3:Applications yaeln$ cd OpenRefine.app/\nMacBook-Pro-3:OpenRefine.app yaeln$ cd Contents/MacOS/\nMacBook-Pro-3:MacOS yaeln$ ./JavaAppLauncher\n23:07:21.265 [            refine_server] Starting Server bound to '\n127.0.0.1:3333' (0ms)\n23:07:21.287 [            refine_server] Initializing context: '/' from\n'/Applications/OpenRefine.app/Contents/Resource/webapp' (22ms)\n23:07:22.226 [                   refine] Starting OpenRefine 3.0-beta\n[TRUNK]... (939ms)\n23:07:22.226 [                   refine] initializing FileProjectManager\nwith dir (0ms)\n23:07:22.226 [                   refine] /Users/yaeln/Library/Application\nSupport/OpenRefine (0ms)\n23:07:23.902 [       database-extension] Initializing OpenRefine\nDatabase... (1676ms)\n23:07:23.905 [       database-extension] Database Extension Mount point\n/extension/database/ [*] (3ms)\n23:07:23.905 [       database-extension] Registering Database Extension\nCommands...... (0ms)\n23:07:23.920 [       database-extension] Database Extension Command\nRegisteration done!! (15ms)\n23:07:23.920 [       database-extension] Database Operations Registered\nsuccessfully... (0ms)\n23:07:23.920 [       database-extension] Database Functions Registered\nsuccessfully... (0ms)\nException in thread \"main\" java.lang.LinkageError: loader constraint\nviolation: loader (instance of\nedu/mit/simile/butterfly/ButterflyClassLoader) previously initiated loading\nfor a different type with name \"org/slf4j/Logger\"\nat java.lang.ClassLoader.defineClass1(Native Method)\nat java.lang.ClassLoader.defineClass(ClassLoader.java:760)\nat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\nat java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\nat java.net.URLClassLoader.access$100(URLClassLoader.java:73)\nat java.net.URLClassLoader$1.run(URLClassLoader.java:368)\nat java.net.URLClassLoader$1.run(URLClassLoader.java:362)\nat java.security.AccessController.doPrivileged(Native Method)\nat java.net.URLClassLoader.findClass(URLClassLoader.java:361)\nat\nedu.mit.simile.butterfly.ButterflyClassLoader.loadClass(ButterflyClassLoader.java:51)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\nat\ncom.google.refine.extension.database.DatabaseModuleImpl.readModuleProperty(DatabaseModuleImpl.java:93)\nat\ncom.google.refine.extension.database.DatabaseModuleImpl.init(DatabaseModuleImpl.java:68)\nat edu.mit.simile.butterfly.Butterfly.initializeModule(Butterfly.java:476)\nat edu.mit.simile.butterfly.Butterfly.configure(Butterfly.java:451)\nat edu.mit.simile.butterfly.Butterfly.init(Butterfly.java:308)\nat\norg.mortbay.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:440)\nat org.mortbay.jetty.servlet.ServletHolder.doStart(ServletHolder.java:263)\nat com.google.refine.RefineServer.configure(Refine.java:296)\nat com.google.refine.RefineServer.init(Refine.java:208)\nat com.google.refine.Refine.init(Refine.java:114)\nat com.google.refine.Refine.main(Refine.java:108)\nOn Wed, Jun 13, 2018 at 4:58 PM Jacky notifications@github.com wrote:\n\n@yaeln https://github.com/yaeln I am got confused. From this\nhttps://github.com/OpenRefine/OpenRefine/issues/1636#issuecomment-396040135post,\nI did not see you had exception java.lang.LinkageError as you did in\nprevious post. You may only have this one issue and the project loading is\nthe consequence of the failure.\nSo please do a refresh install by choose \"replace\" when you see the\ndialog box then start it to see if you still see the LinkageError\nexception. If you do, please zip your whole installation package(not the\nprojects package) and share with me. Thanks a lot\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1636#issuecomment-396946787,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AE4OprPb1FXv4iyX48qPNpIFSIl8daTwks5t8RppgaJpZM4UX8yy\n.\n. MacBook-Pro-3:MacOS yaeln$ java -version\n\njava version \"1.8.0_25\"\nJava(TM) SE Runtime Environment (build 1.8.0_25-b17)\nJava HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode)\nMacBook-Pro-3:MacOS yaeln$\nOn Thu, Jun 14, 2018 at 3:37 PM Owen Stephens notifications@github.com\nwrote:\n\nDownloaded and ran successfully on macOS High Sierra.\n@yaeln https://github.com/yaeln can you check what version of Java you\nhave installed?\nrun java -version on the command line and paste the results\n(I'm clutching at straws a bit here but just trying to think of things\nthat might be difference across setups)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1636#issuecomment-397278505,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AE4Opu5foHuKjTJ4hIscoxHULKBtf0ITks5t8ljxgaJpZM4UX8yy\n.\n. MacBook-Pro-3:openrefine-3.0-beta yaeln$ bash -x ./refine\n\n++ dirname ./refine\n\n\ncd .\n\n\nOPTS=\n\n\n++ uname\n\n\nSYSTEM=Darwin\n\n\ncase \"$SYSTEM\" in\n\n\nOS=macosx\n\n\nSEP=:\n\n\n'[' macosx = windows ']'\n\n\nload_configs refine.ini\n\n\n++ mktemp -t refine.XXXXXXX\n+\nTEMP_CONFIG=/var/folders/x8/mv_d4_n55xbgplfd146hj27c0000gn/T/refine.XXXXXXX.nO0V9Eh5\n\n\n'['\n/var/folders/x8/mv_d4_n55xbgplfd146hj27c0000gn/T/refine.XXXXXXX.nO0V9Eh5 =\n'' ']'\n\n\ncat refine.ini\n\n\negrep '^[A-Z]'\n\n\nsed 's/^(.*)$/export \\1/'\n\n\n. /var/folders/x8/mv_d4_n55xbgplfd146hj27c0000gn/T/refine.XXXXXXX.nO0V9Eh5\n\n\n++ export REFINE_MEMORY=1400M\n++ REFINE_MEMORY=1400M\n++ export REFINE_MIN_MEMORY=1400M\n++ REFINE_MIN_MEMORY=1400M\n\n\nrm\n/var/folders/x8/mv_d4_n55xbgplfd146hj27c0000gn/T/refine.XXXXXXX.nO0V9Eh5\n\n\n'[' macosx = macosx ']'\n\n\n'[' -z '' ']'\n\n\n++ /usr/libexec/java_home\n\n\nexport\nJAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_25.jdk/Contents/Home\n\n\nJAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_25.jdk/Contents/Home\n\n\n'[' /Library/Java/JavaVirtualMachines/jdk1.8.0_25.jdk/Contents/Home ']'\n\n\n+\nJAVA=/Library/Java/JavaVirtualMachines/jdk1.8.0_25.jdk/Contents/Home/bin/java\n\n\n'[' '!' -x\n/Library/Java/JavaVirtualMachines/jdk1.8.0_25.jdk/Contents/Home/bin/java ']'\n\n\ncheckJavaMajorVersion\n\n\n++ /Library/Java/JavaVirtualMachines/jdk1.8.0_25.jdk/Contents/Home/bin/java\n-version\n++ grep version\n++ cut -d ' ' -f 3\n++ tr -d '\"'\n\n\njava_ver=1.8.0_25\n\n\n'[' 1. == 1. ']'\n\n\n++ echo 1.8.0_25\n++ sed -E 's/1.([0-9])[0-9_.]{2,6}/\\1/g'\n\n\nmajor=8\n\n\n((  8 < 7  ))\n\n\n((  8 > 9  ))\n\n\n'[' 0 -ne 0 ']'\n\n\n'[' 0 -ne 0 ']'\n\n\n'[' -z '' ']'\n\n\nACTION=run\n\n\n'[' -z '' ']'\n\n\nJAVA_OPTIONS=\n\n\nadd_option ''\n\n\nOPTS=' '\n\n\n'[' -z 1400M ']'\n\n\n'[' -z 1400M ']'\n\n\nadd_option '-Xms1400M -Xmx1400M -Drefine.memory=1400M'\n\n\nOPTS='  -Xms1400M -Xmx1400M -Drefine.memory=1400M'\n\n\nfreeRam=UNKNOWN\n\n\n'[' macosx = macosx ']'\n\n\n++ top -l 1\n++ grep PhysMem\n++ awk '{print $6}'\n++ tr -d M\n\n\nfreeRam=92\n\n\necho You have 92M of free memory.\n\n\nYou have 92M of free memory.\n\necho Your current configuration is set to use 1400M of memory.\n\nYour current configuration is set to use 1400M of memory.\n\necho OpenRefine can run better when given more memory. Read our FAQ on\nhow to allocate more memory here:\n\nOpenRefine can run better when given more memory. Read our FAQ on how to\nallocate more memory here:\n\necho\nhttps://github.com/OpenRefine/OpenRefine/wiki/FAQ:-Allocate-More-Memory\n\nhttps://github.com/OpenRefine/OpenRefine/wiki/FAQ:-Allocate-More-Memory\n\n\n'[' -z '' ']'\n\n\nREFINE_MAX_FORM_CONTENT_SIZE=1048576\n\n\nadd_option -Drefine.max_form_content_size=1048576\n\n\nOPTS='  -Xms1400M -Xmx1400M -Drefine.memory=1400M\n-Drefine.max_form_content_size=1048576'\n\n\n'[' -z '' ']'\n\n\nREFINE_PORT=3333\n\n\n'[' -z '' ']'\n\n\nREFINE_HOST=127.0.0.1\n\n\n'[' -z '' ']'\n\n\nREFINE_WEBAPP=main/webapp\n\n\n'[' -z '' ']'\n\n\nREFINE_TEST_DIR=main/tests\n\n\n'[' -z '' ']'\n\n\nREFINE_CLASSES_DIR=server/classes\n\n\n'[' -z '' ']'\n\n\nREFINE_LIB_DIR=server/lib\n\n\n'[' -z '' ']'\n\n\nREFINE_BUILD_DIR=build\n\n\n'[' -z '' ']'\n\n\nREFINE_TOOLS_DIR=tools\n\n\n'[' -z '' ']'\n\n\nREFINE_DIST_DIR=dist\n\n\n'[' -z '' ']'\n\n\nREFINE_VERBOSITY=info\n\n\nadd_option -Drefine.verbosity=info\n\n\nOPTS='  -Xms1400M -Xmx1400M -Drefine.memory=1400M\n-Drefine.max_form_content_size=1048576 -Drefine.verbosity=info'\n\n\n'[' '!' -z '' ']'\n\n\n'[' -z '' ']'\n\n\nJYTHONPATH=main/webapp/WEB-INF/lib/jython\n\n\nadd_option -Dpython.path=main/webapp/WEB-INF/lib/jython\n\n\nOPTS='  -Xms1400M -Xmx1400M -Drefine.memory=1400M\n-Drefine.max_form_content_size=1048576 -Drefine.verbosity=info\n-Dpython.path=main/webapp/WEB-INF/lib/jython'\n\n\nadd_option\n-Dpython.cachedir=/Users/yaeln/.local/share/google/refine/cachedir\n\n\nOPTS='  -Xms1400M -Xmx1400M -Drefine.memory=1400M\n-Drefine.max_form_content_size=1048576 -Drefine.verbosity=info\n-Dpython.path=main/webapp/WEB-INF/lib/jython\n-Dpython.cachedir=/Users/yaeln/.local/share/google/refine/cachedir'\n\n\ncase \"$ACTION\" in\n\n\nrun\n\n\nFORK=\n\n\ncheck_running\n\n\ncheck_downloaders\n\n\n++ which curl\n\nCURL=/Users/yaeln/anaconda/bin/curl\n\n++ which wget\n\n\nWGET=\n\n\n'[' -z /Users/yaeln/anaconda/bin/curl ']'\n\n\nURL=http://127.0.0.1:3333/\n\n\nCHECK_STR='OpenRefine'\n\n\n'[' /Users/yaeln/anaconda/bin/curl ']'\n\n\ncurl -s -S -f http://127.0.0.1:3333/\n\n\n'[' 7 = 7 ']'\n\n\nNOT_RUNNING=1\n\n\n'[' -z 1 ']'\n\n\nRUNNING=\n\n\n'[' '' ']'\n\n\n'[' '!' -d server/classes ']'\n\n\n++ ls server/lib\n++ grep openrefine\n\n\nIS_JAR=openrefine-3.0-beta-server.jar\n\n\n'[' -z openrefine-3.0-beta-server.jar ']'\n\n\n'[' -d server/classes ']'\n\n\n'[' macosx = macosx ']'\n\n\nadd_option -Xdock:icon=graphics/icon/openrefine.icns\n\n\nOPTS='  -Xms1400M -Xmx1400M -Drefine.memory=1400M\n-Drefine.max_form_content_size=1048576 -Drefine.verbosity=info\n-Dpython.path=main/webapp/WEB-INF/lib/jython\n-Dpython.cachedir=/Users/yaeln/.local/share/google/refine/cachedir\n-Xdock:icon=graphics/icon/openrefine.icns'\n\n\n'[' '' ']'\n\n\n'[' main/webapp ']'\n\n\nadd_option -Drefine.webapp=main/webapp\n\n\nOPTS='  -Xms1400M -Xmx1400M -Drefine.memory=1400M\n-Drefine.max_form_content_size=1048576 -Drefine.verbosity=info\n-Dpython.path=main/webapp/WEB-INF/lib/jython\n-Dpython.cachedir=/Users/yaeln/.local/share/google/refine/cachedir\n-Xdock:icon=graphics/icon/openrefine.icns -Drefine.webapp=main/webapp'\n\n\n'[' 3333 ']'\n\n\nadd_option -Drefine.port=3333\n\n\nOPTS='  -Xms1400M -Xmx1400M -Drefine.memory=1400M\n-Drefine.max_form_content_size=1048576 -Drefine.verbosity=info\n-Dpython.path=main/webapp/WEB-INF/lib/jython\n-Dpython.cachedir=/Users/yaeln/.local/share/google/refine/cachedir\n-Xdock:icon=graphics/icon/openrefine.icns -Drefine.webapp=main/webapp\n-Drefine.port=3333'\n\n\n'[' 127.0.0.1 ']'\n\n\nadd_option -Drefine.host=127.0.0.1\n\n\nOPTS='  -Xms1400M -Xmx1400M -Drefine.memory=1400M\n-Drefine.max_form_content_size=1048576 -Drefine.verbosity=info\n-Dpython.path=main/webapp/WEB-INF/lib/jython\n-Dpython.cachedir=/Users/yaeln/.local/share/google/refine/cachedir\n-Xdock:icon=graphics/icon/openrefine.icns -Drefine.webapp=main/webapp\n-Drefine.port=3333 -Drefine.host=127.0.0.1'\n\n\n'[' '' ']'\n\n\n'[' '' ']'\n\n\nCLASSPATH='server/classes:server/lib/*'\n\n\n+\nRUN_CMD='/Library/Java/JavaVirtualMachines/jdk1.8.0_25.jdk/Contents/Home/bin/java\n-cp server/classes:server/lib/*   -Xms1400M -Xmx1400M -Drefine.memory=1400M\n-Drefine.max_form_content_size=1048576 -Drefine.verbosity=info\n-Dpython.path=main/webapp/WEB-INF/lib/jython\n-Dpython.cachedir=/Users/yaeln/.local/share/google/refine/cachedir\n-Xdock:icon=graphics/icon/openrefine.icns -Drefine.webapp=main/webapp\n-Drefine.port=3333 -Drefine.host=127.0.0.1 com.google.refine.Refine'\n\necho 'Starting OpenRefine at '\\''http://127.0.0.1:3333/'\\'''\n\nStarting OpenRefine at 'http://127.0.0.1:3333/'\n\n\necho ''\n\n\n'[' -z '' ']'\n\n\nexec\n/Library/Java/JavaVirtualMachines/jdk1.8.0_25.jdk/Contents/Home/bin/java\n-cp 'server/classes:server/lib/*' -Xms1400M -Xmx1400M -Drefine.memory=1400M\n-Drefine.max_form_content_size=1048576 -Drefine.verbosity=info\n-Dpython.path=main/webapp/WEB-INF/lib/jython\n-Dpython.cachedir=/Users/yaeln/.local/share/google/refine/cachedir\n-Xdock:icon=graphics/icon/openrefine.icns -Drefine.webapp=main/webapp\n-Drefine.port=3333 -Drefine.host=127.0.0.1 com.google.refine.Refine\n\n\n16:22:57.717 [            refine_server] Starting Server bound to '\n127.0.0.1:3333' (0ms)\n16:22:57.719 [            refine_server] refine.memory size: 1400M JVM Max\nheap: 1407188992 (2ms)\n16:22:57.739 [            refine_server] Initializing context: '/' from\n'/Users/yaeln/Dropbox/OpenRefine3/openrefine-3.0-beta/webapp' (20ms)\n16:22:58.658 [                   refine] Starting OpenRefine 3.0-beta\n[TRUNK]... (919ms)\n16:22:58.658 [                   refine] initializing FileProjectManager\nwith dir (0ms)\n16:22:58.658 [                   refine] /Users/yaeln/Library/Application\nSupport/OpenRefine (0ms)\n16:23:00.270 [       database-extension] Initializing OpenRefine\nDatabase... (1612ms)\n16:23:00.275 [       database-extension] Database Extension Mount point\n/extension/database/ [*] (5ms)\n16:23:00.276 [       database-extension] Registering Database Extension\nCommands...... (1ms)\n16:23:00.297 [       database-extension] Database Extension Command\nRegisteration done!! (21ms)\n16:23:00.297 [       database-extension] Database Operations Registered\nsuccessfully... (0ms)\n16:23:00.297 [       database-extension] Database Functions Registered\nsuccessfully... (0ms)\nException in thread \"main\" java.lang.LinkageError: loader constraint\nviolation: loader (instance of\nedu/mit/simile/butterfly/ButterflyClassLoader) previously initiated loading\nfor a different type with name \"org/slf4j/Logger\"\nat java.lang.ClassLoader.defineClass1(Native Method)\nat java.lang.ClassLoader.defineClass(ClassLoader.java:760)\nat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\nat java.net.URLClassLoader.defineClass(URLClassLoader.java:455)\nat java.net.URLClassLoader.access$100(URLClassLoader.java:73)\nat java.net.URLClassLoader$1.run(URLClassLoader.java:367)\nat java.net.URLClassLoader$1.run(URLClassLoader.java:361)\nat java.security.AccessController.doPrivileged(Native Method)\nat java.net.URLClassLoader.findClass(URLClassLoader.java:360)\nat\nedu.mit.simile.butterfly.ButterflyClassLoader.loadClass(ButterflyClassLoader.java:51)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\nat\ncom.google.refine.extension.database.DatabaseModuleImpl.readModuleProperty(DatabaseModuleImpl.java:93)\nat\ncom.google.refine.extension.database.DatabaseModuleImpl.init(DatabaseModuleImpl.java:68)\nat edu.mit.simile.butterfly.Butterfly.initializeModule(Butterfly.java:476)\nat edu.mit.simile.butterfly.Butterfly.configure(Butterfly.java:451)\nat edu.mit.simile.butterfly.Butterfly.init(Butterfly.java:308)\nat\norg.mortbay.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:440)\nat org.mortbay.jetty.servlet.ServletHolder.doStart(ServletHolder.java:263)\nat com.google.refine.RefineServer.configure(Refine.java:296)\nat com.google.refine.RefineServer.init(Refine.java:208)\nat com.google.refine.Refine.init(Refine.java:114)\nat com.google.refine.Refine.main(Refine.java:108)\nOn Thu, Jun 14, 2018 at 4:07 PM Thad Guidry notifications@github.com\nwrote:\n\nButterfly loads things from the Classpath. Classpath is figured out\nthrough the start script. Maybe the Classpath is munged for some reason ?\n@jackyq2015 https://github.com/jackyq2015 has he tried to just run\nusing the Linux start script.... ./refine ? Because keep seeing\nJavaAppLauncher being used in his replies.\nDebug the entire start flow with\nbash -x ./refine\nand then let's see what it looks like.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1636#issuecomment-397289266,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AE4Opucda5uF3j0aufcuXJTr7Sjd6gVxks5t8mAagaJpZM4UX8yy\n.\n. I tested - and not - obviously --  I usually run the openrefine from the\n/Applications path (this is the linux installation)\n\nMy own solution  can be manually 'exporting' all projects from 2.8 and then\n'importing' them into the 3.0 one after the other.\n..\nOn Thu, Jun 14, 2018 at 4:31 PM Thad Guidry notifications@github.com\nwrote:\n\n@yaeln https://github.com/yaeln Can you move your OpenRefine install\noutside of a DropBox path ? I'm wondering if something with a Dropbox\nclient might also be causing some conflict ? dunno....\n/Users/yaeln/Dropbox/OpenRefine3/openrefine-3.0-beta/webapp\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1636#issuecomment-397296814,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AE4Oplk19UMO_-87XEaK6c870_q0rAJGks5t8mW-gaJpZM4UX8yy\n.\n. It worked!\nThanks a lot!\nI have this extension 'prefine-rdf-extension-0.9' but once I moved them one\nafter the other it worked!\nI am most grateful for you assistance.\nOpenRefine is a very meaningful tool for me, I teach it almost like a\nmissioner in my DH classes, and I will do an effort to advance the Hebrew\ntranslation.\n\nBest regards,\nYael\nOn Fri, Jun 15, 2018 at 6:51 PM Jan Ruehling notifications@github.com\nwrote:\n\nI had a similar problem (macOS High Sierra - OpenRefine3 stuck at\n\"Butterfly still initializing\" in the browser) and solved it like this:\nIn your workspace directory (for me ~/Library/Application\nSupport/OpenRefine):\n\nRename \"extensions\" to \"_extensions\"\nCreate a new folder \"extensions\"\nCopy one extension folder after the other from \"_extensions\" to\n   \"extensions\" and restart OpenRefine\n\nThe problematic extension for me was \"orefine-rdf-extension-0.9\"\nGood luck\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1636#issuecomment-397663917,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AE4OpnsW9vr0zWZo98_3wEjMSR3WY4OEks5t89gAgaJpZM4UX8yy\n.\n. Thanks.\nThanks for finding a solution to this problem as well\n\nOn Sat, Jun 16, 2018 at 3:34 AM Ettore Rizza notifications@github.com\nwrote:\n\n@yaeln https://github.com/yaeln This version of the RDF extension\nhttps://github.com/stkenny/grefine-rdf-extension/releases/tag/v1.0.0-rc2\nshould work with OR 3.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/OpenRefine/OpenRefine/issues/1636#issuecomment-397773058,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AE4Opg5bpUNhoy1odPGWAZnV6KexPenOks5t9FJ5gaJpZM4UX8yy\n.\n. \n",
    "janruehling": "I had a similar problem (macOS High Sierra - OpenRefine3 stuck at \"Butterfly still initializing\" in the browser) and solved it like this:\nIn your workspace directory (for me ~/Library/Application Support/OpenRefine):\n\nRename \"extensions\" to \"_extensions\"\nCreate a new folder \"extensions\"\nCopy one extension folder after the other from \"_extensions\" to \"extensions\" and restart OpenRefine\n\nThe problematic extension for me was \"orefine-rdf-extension-0.9\"\nGood luck. ",
    "susannaanas": "I removed the rows by selecting All -> Edit rows -> Remove all matching rows. After deleting the green bar still did not display 100% coverage.\nAnd super excited about the tool! Hope to be able to use it properly soon!. I would find use for it immediately, but I would not describe it as super urgent. There is a growing number of Wikibase instances, listed in this Wikibase https://wikibase-registry.wmflabs.org/. I could not do it myself, but I wonder if the component could be configured by a skilled developer to point to another Wikibase?. Sounds much more complex than I imagined!\nBTW, I think also Structured Data for Commons qualifies as one such Wikibase.. Wow, we'll start testing it (soonish)!\nThanks!. Good point, I will create further issues there!. ",
    "praveene04": "Hi jackyq2015,\nI copy and pasted the openrefine folder directly in C drive and  tried opening openrefine.exe file and received below message.\n09:25:30.543 [            refine_server] Starting Server bound to '127.0.0.1:3333' (0ms)\n09:25:30.559 [            refine_server] Initializing context: '/' from 'C:\\openrefine-2.8\\webapp' (16ms)\n09:25:31.387 [            refine_server] Failed to use jdatapath to detect user data path: resorting to environment variables (828ms)\n09:25:31.387 [            refine_server] Failed to use jdatapath to detect user data path: resorting to environment variables (0ms)\n09:25:31.403 [                   refine] Starting OpenRefine 2.8 [TRUNK]... (16ms). I am getting this error message \n\n. Used \"http://localhost:3333\" instead of \"http://127.0.0.1:3333\" which worked.. ",
    "yarl": "Same here, also Win 10, 3.0 beta.. ",
    "SannitaSSJ": "@wetneb it seems to be working now, thanks :) I'll do some more tests and let you know asap. Fantastic, thanks for updating the FAQs. :). ",
    "visch": "It's obviously tied to https://tools.wmflabs.org/openrefine-wikidata/en/api taking too long to respond, but OpenRefine probably should fail more gracefully or not have this url baked in? I'm not sure what the right solution is . ",
    "GiantCrocodile": "I think that is what I did get, yes. But what I wold expect is that the columns at least have the correct name, for example: Column 5 should be 2015 because that's the sub-columns name - it's not a data row. If I'm mistaken, just tell me.. Also a huge exception if you click on \"RDF/XML files\" with this URL.. ",
    "michaelavoigt": "dear @wetneb wow, you were quick! Thanks for looking into it. To be honest, I have never installed patches for OR before and would need some help. But since somebody confirmed that your fix is currently working - it's maybe not necessary for me to try? Again, thank you.... ",
    "whanley": "Here's the error:\nwhanley$ ./JavaAppLauncher\n11:11:02.568 [            refine_server] Starting Server bound to '127.0.0.1:3333' (0ms)\n11:11:02.577 [            refine_server] Initializing context: '/' from '/Applications/OpenRefine.app/Contents/Resource/webapp' (9ms)\n11:11:02.937 [          org.mortbay.log] failed SocketConnector@127.0.0.1:3333: java.net.BindException: Address already in use (360ms)\n11:11:02.937 [          org.mortbay.log] failed RefineServer@5ae63ade: java.net.BindException: Address already in use (0ms)\n11:11:02.937 [            refine_server] Failed to start server - is there another copy running already on this port/address? (0ms)\nException in thread \"main\" java.net.BindException: Address already in use\n    at java.net.PlainSocketImpl.socketBind(Native Method)\n    at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:387)\n    at java.net.ServerSocket.bind(ServerSocket.java:375)\n    at java.net.ServerSocket.<init>(ServerSocket.java:237)\n    at org.mortbay.jetty.bio.SocketConnector.newServerSocket(SocketConnector.java:80)\n    at org.mortbay.jetty.bio.SocketConnector.open(SocketConnector.java:73)\n    at org.mortbay.jetty.AbstractConnector.doStart(AbstractConnector.java:283)\n    at org.mortbay.jetty.bio.SocketConnector.doStart(SocketConnector.java:147)\n    at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n    at org.mortbay.jetty.Server.doStart(Server.java:235)\n    at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n    at com.google.refine.RefineServer.init(Refine.java:202)\n    at com.google.refine.Refine.init(Refine.java:114)\n    at com.google.refine.Refine.main(Refine.java:108)\nAddress already in use? It looks like OpenRefine is in fact running, but is not responding. When I force quit the non-responsive OpenRefine, this is the result:\nwhanley$ ./JavaAppLauncher\n11:13:49.666 [            refine_server] Starting Server bound to '127.0.0.1:3333' (0ms)\n11:13:49.675 [            refine_server] Initializing context: '/' from '/Applications/OpenRefine.app/Contents/Resource/webapp' (9ms)\n11:13:50.016 [                   refine] Starting OpenRefine 3.0-beta [TRUNK]... (341ms)\n11:13:50.016 [                   refine] initializing FileProjectManager with dir (0ms)\n11:13:50.016 [                   refine] /Users/whanley/Library/Application Support/OpenRefine (0ms)\n11:13:50.649 [       database-extension] Initializing OpenRefine Database... (633ms)\n11:13:50.651 [       database-extension] Database Extension Mount point /extension/database/ [*] (2ms)\n11:13:50.652 [       database-extension] Registering Database Extension Commands...... (1ms)\n11:13:50.657 [       database-extension] Database Extension Command Registeration done!! (5ms)\n11:13:50.657 [       database-extension] Database Operations Registered successfully... (0ms)\n11:13:50.657 [       database-extension] Database Functions Registered successfully... (0ms)\nException in thread \"main\" java.lang.LinkageError: loader constraint violation: loader (instance of edu/mit/simile/butterfly/ButterflyClassLoader) previously initiated loading for a different type with name \"org/slf4j/Logger\"\n    at java.lang.ClassLoader.defineClass1(Native Method)\n    at java.lang.ClassLoader.defineClass(ClassLoader.java:760)\n    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n    at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\n    at java.net.URLClassLoader.access$100(URLClassLoader.java:73)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:368)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:362)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:361)\n    at edu.mit.simile.butterfly.ButterflyClassLoader.loadClass(ButterflyClassLoader.java:51)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n    at com.google.refine.extension.database.DatabaseModuleImpl.readModuleProperty(DatabaseModuleImpl.java:93)\n    at com.google.refine.extension.database.DatabaseModuleImpl.init(DatabaseModuleImpl.java:68)\n    at edu.mit.simile.butterfly.Butterfly.initializeModule(Butterfly.java:476)\n    at edu.mit.simile.butterfly.Butterfly.configure(Butterfly.java:451)\n    at edu.mit.simile.butterfly.Butterfly.init(Butterfly.java:308)\n    at org.mortbay.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:440)\n    at org.mortbay.jetty.servlet.ServletHolder.doStart(ServletHolder.java:263)\n    at com.google.refine.RefineServer.configure(Refine.java:296)\n    at com.google.refine.RefineServer.init(Refine.java:208)\n    at com.google.refine.Refine.init(Refine.java:114)\n    at com.google.refine.Refine.main(Refine.java:108)\nBrowser still shows \"Butterfly is still initializing...\", and the app itself won't open.\nThanks for your help.. Yes. RDF Refine--and maybe others though I can't remember.... I checked 2.8, and the RDF extension is the only one I have installed. (I'm upgrading, which I guess I should have mentioned).. Thank you @jackyq2015 and @thadguidry and @ettorerizza. You've solved my problem, which appears indeed to have been a problem with the extension.\nAs @janruehling suggested in #1636:\n\nIn your workspace directory (for me ~/Library/Application Support/OpenRefine):\n\nRename \"extensions\" to \"_extensions\"\nCreate a new folder \"extensions\"\nCopy one extension folder after the other from \"_extensions\" to \"extensions\" and restart OpenRefine\n\n\n\nI put this compiled 3.0 version of the RDF extension in my extensions folder, and everything works well now.\n. ",
    "waldyrious": "Good point, @jackyq2015. Should the change be made instead on this line? https://github.com/OpenRefine/OpenRefine/blob/f1416ae50c4bd170f3225936e206ca79af885484/main/webapp/modules/core/scripts/index/parser-interfaces/separator-based-parser-ui.js#L137\nI'm not really sure how to do it there, any hints welcome.. ",
    "omkarnix": "@thadguidry Fixed this issue in following PR. Could you please take a look? https://github.com/OpenRefine/OpenRefine/pull/1761. @thadguidry, if I am not mistaken, while working on this issue I figured that functions don't have test coverage. So do we have any plans to add them in one go or should we just add them incrementally as we encounter the low test coverage? e.g in this fix, should I add test for SmartSplit in SmartSplitTests directly by invoking the new SmartSplit().call(...) ?. @thadguidry Thanks for the response. I would certainly like to contribute in unit tests for not only SmartSplit but other functions as well. \nfor my https://github.com/OpenRefine/OpenRefine/pull/1761, should I create new issue for adding test in SmartSplitTests or amend the same PR? Also, should I create new issue to add other functions' tests?. @wetneb. Sure Will add the tests and update my PR tomorrow. Thanks!. @wetneb, Sure Thanks for reviewing. Actually, I realized that low test coverage after creating PR and started the discussion on https://github.com/OpenRefine/OpenRefine/issues/1674. Will amend my PR with tests tomorrow and update! Thank you for quick responses and encouragement! . @wetneb Thank you very much for your support and encouragement on contribution. The community is very supportive. I would certainly try to contribute as much as possible!. Yeah. Actually I thought we should move the invoke method to some test util class. Also could you please help me understand what is RefineTest and should we extend our control function test classes with RefineTest?. Ok. Thanks. I would certainly like to refactor it and move the invoke method to some util class. \nI noticed that travis CI build failed due to assertion failure in serializeProcessManager whereas appveyor build succeeded. Could it be a glitch while building on travis-ci? Can we trigger the build manually?. > Fantastic, thanks a lot! As a small note, I think invoke should not need to take the function name as parameter, as it is always FUNCTION_NAME in this class (and the method is private). But that is a cosmetic concern.\n@wetneb I agree. It was really not required to create constant for the local static value. So made the change suggested by yo .Also added one more misc change. Can you review it?. ",
    "fgibaux": "Hi, I got what looks like the same issue when trying to preview an SQL query from database connection:\njava.lang.NoSuchFieldError: metadata\n        at com.google.refine.extension.database.DatabaseImportController.doParsePreview(DatabaseImportController.java:201)\n        at com.google.refine.extension.database.DatabaseImportController.doPost(DatabaseImportController.java:101)\n        at com.google.refine.commands.importing.ImportingControllerCommand.doPost(ImportingControllerCommand.java:62)\n        at com.google.refine.RefineServlet.service(RefineServlet.java:178)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n        at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n        at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:938)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n        at java.lang.Thread.run(Thread.java:748)\nand my java version is: \njava -version\nopenjdk version \"1.8.0_181\"\nOpenJDK Runtime Environment (build 1.8.0_181-b13)\nOpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)\n. Oops forgot to mention it's on Linux fedora 27\nLe 7 sept. 2018 \u00e0 02:20, \u00e0 02:20, Jacky notifications@github.com a \u00e9crit:\n\n@fgibaux what's your OS and version?\n-- \nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub:\nhttps://github.com/OpenRefine/OpenRefine/issues/1707#issuecomment-419290458\n. \n",
    "macfraser": "Sorry I was late to respond. Here is my java version info:\njava -version\njava version \"10\" 2018-03-20\nJava(TM) SE Runtime Environment 18.3 (build 10+46)\nJava HotSpot(TM) 64-Bit Server VM 18.3 (build 10+46, mixed mode)\n. I uninstalled, re-installed and received the same error message as before. Here is the stacktrace.\n11:45:16.168 [          org.mortbay.log] Error for /command/core/importing-controller (10ms)\njava.lang.NoSuchFieldError: metadata\n    at com.google.refine.extension.gdata.GDataImportingController.doParsePreview(GDataImportingController.java:245)\n    at com.google.refine.extension.gdata.GDataImportingController.doPost(GDataImportingController.java:71)\n    at com.google.refine.commands.importing.ImportingControllerCommand.doPost(ImportingControllerCommand.java:62)\n    at com.google.refine.RefineServlet.service(RefineServlet.java:178)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n    at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\n    at org.mortbay.servlet.UserAgentFilter.doFilter(UserAgentFilter.java:81)\n    at org.mortbay.servlet.GzipFilter.doFilter(GzipFilter.java:132)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:418)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n    at org.mortbay.jetty.Server.handle(Server.java:326)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:938)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n. ",
    "LibrErli": "\nColumn deDesc contains text values. @wetneb\nAfter same try and error - i can reproduce how the error occues.\nFirst i add a Description term, set the language and type a string in the input field. The schema will be previewed. Afterwards i delete the string in the description field and drag and drop the column deDesc in the inputfield - the error occures.\nIf i remove the description term, add a new one, select the language and add the column immediately everything works fine!. @wetneb i am running open refine 3 rc 1 on ubuntu 18.04 firefox 61. ",
    "bohy": "Yea, got that. Now I have an issue that I would like to get also in the \nexport the information about the username who made that comment. I can get \nonly the text of the comment, the the time and that is basically it..\n---------- P\u016fvodn\u00ed e-mail ----------\nOd: Thad Guidry notifications@github.com\nKomu: OpenRefine/OpenRefine OpenRefine@noreply.github.com\nDatum: 25. 8. 2018 22:56:02\nP\u0159edm\u011bt: Re: [OpenRefine/OpenRefine] Only 100 rows displaying (#1716) \n\"\nYou are probably only looking at the 1st preview stage.... look to the top \nright corner for the \"Create Project\" button on the preview grid.....then it\nwill take you create the Refine Project and leave you with the main data \ngrid with all your rows (not the preview stage of only 100 rows)\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\n(https://github.com/OpenRefine/OpenRefine/issues/1716#issuecomment-415992824)\n, or mute the thread\n(https://github.com/notifications/unsubscribe-auth/Alfl8k9KL2W20ftHj5erwuuOVkT0FmGyks5uUavIgaJpZM4WMTw6)\n. \n\". Yes, sure. For example I am trying to extract the data from this post on Facebook: https://web.facebook.com/fortebet.ug/photos/a.1452160851664455/2193264900887376/?type=3 \nI need to extract to excell who commented on this post, what time and what is the comment. And the problem is that I am only getting the time and the comment, but not who commented.. Also the problem I am facing is that as you can see on the facebook page it is showing there are more than 800 comments, but I am getting only around 500.. \nany help would be appreciated. Many thanks.. Not really getting any help from the other site.. Would you be so kind to \nmaybe mail that to me? petrrak@seznam.cz\nthank you\n---------- P\u016fvodn\u00ed e-mail ----------\nOd: Thad Guidry notifications@github.com\nKomu: OpenRefine/OpenRefine OpenRefine@noreply.github.com\nDatum: 26. 8. 2018 13:09:12\nP\u0159edm\u011bt: Re: [OpenRefine/OpenRefine] Only 100 rows displaying (#1716) \n\"\nAh, you are doing webscraping, and these are webscraping questions actually.\nI know the answers but our Github issues are for specific OpenRefine \nquestions. The right place to ask general web scraping questions is probably\non Stackoverflow with tag \"web-scraping\". https://stackoverflow.com/\nquestions/tagged/web-scraping\n(https://stackoverflow.com/questions/tagged/web-scraping) Someone there \nshould be able to help you with all your webscraping questions.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\n(https://github.com/OpenRefine/OpenRefine/issues/1716#issuecomment-416030831)\n, or mute the thread\n(https://github.com/notifications/unsubscribe-auth/Alfl8j5QWDsmARVasqwLfpQKNcZLw0P-ks5uUoHPgaJpZM4WMTw6)\n. \n\". ",
    "wentianq": "Thx for your reply. Here is the console. Can you please help me to fix these error?\n\n. I jump to this screen.\n\n. \n. It is version 68.0.3440.106.. It is the default package.. After I forbade the extensions in Chrome, it still didn't work. I also tried IE, but I got the same result.. - Version Downloaded: openrefine-win-3.0-rc.1\n- Windows Version: Microsoft Windows 10 Home\n- Java Version: 10.0.2. I reinstalled Java with version 8, but I get the same result for OpenRefine.\n\n. I downloaded JRE of Java SE 8u181 from http://www.oracle.com/technetwork/java/javase/downloads/index.html.\n\nI also set JAVA_HOME variable like this:\n\nI tried to run refine.bat, but it quit immediately.\n. I have tried http://localhost:3333, but I still get the following page:\n\nI read your FAQ about browser and checked all bullet points except the third one since I had no idea about how to use a different IP address and Port. Still, I got the same result.. I think the Port is correct by default. If not, What should I change it into?\n\n. I tried Firefox on this computer, as well as IE. None of these works.\n\n\n. It also shows on my computer.\n\n. I just double clicked refine.bat. I don't know how to pause it in the end. I tried to add PAUSE at the end of refine.bat, but it didn't work either.\n\n. Well... Assuming something is wrong, what should I do? Frankly speaking, I just wanna give up using OpenRefine. Fixing here and there without any fruit is so exhausting to me.. - I installed it again in a different folder and I did not help.\n- I don't know how to change the path of the workspace folder to another one.\nI don't think the problem comes from the installation package since it is directly download from the OpenRefine website without any change. In addition, the zip file is too large to upload here.\n. C:\\Users(user id)\\AppData\\Local\\OpenRefine is exactly the path of the workspace at present.. Thx. OpenRefine 2.8 also works on my computer.. ",
    "pollyhunter": "i have the same problem. who can solve it ?\n. if apache ant is necessary?. > @wentianq @pollyhunter have you tried running older versions of OpenRefine, like 2.8 or even 2.7? Does it make any difference?\nthank u \ni have tried openrefine 2.8. it succeed!. ",
    "AlexGuo1998": "Hello? Same issue on my Windows PC. \nIt seems that they (we) all are using a Chinese Windows 10. Could it be some issue related to locale? \nPlease check that, thanks!. BTW, @wetneb \n\n      can you scroll down to line 63726 and see what is there?\n\n\nLine 63726 shows something like this:\n\n. Quick fix: Replace webapp\\modules\\core\\externals\\jsoneditor\\jsoneditor.js with this file jsoneditor.zip (unzip first).. ",
    "lapoisse": "Some complementary informations  : \n- Jre 1.8.0_181\nIt is working fine on a windows 10 machine with the same VM.\n. You will find the content from the server under the windows server 2016 OS. \ntest.txt\n. Same problem. Though the saving error is not here now. The new result is attached. \ntest.txt\n. Sure :\n1- Go on https://regex101.com/\n2- type in the regexp part : \\w+ (word matching)\n3- The text part : \"\u00c9abc\"\n3- You can change the modifier on the right on the regexp part to get U for unicode.\nYou will see as a word \u00c9abc with the modifier, and abc without the modifier. The same thing happens inside openrefine with UTF8/16 file. It is not a bug but an interpretation of the regexp. In fact, the problem is far more complex than this simple example. If you want a complete explanation on unicode regexp, please this document is exceptionnal : https://www.regular-expressions.info/unicode.html. I invite you to read on the dot operator to understand why it could be useful to have a U modifier. \nI find a way to get good results with the \\p{} tag, but sometimes it would be far more simple to do it with the modifier. By the way, a very interesting problem for my own experience. \nThank you for your interest. . ",
    "jenyoung": "I'd also like to see the list of the Common Transforms replicated here. Thanks!. Yes please! I had already gone through my data and changed empty strings to null and to have to change it back when I just want to merge or concatenate columns is not ideal.. Thanks - I thought I was the only one confused by that wording!. ",
    "bbonza": "@wetneb yes, thanks :). ",
    "skyarchers": "web site http://127.0.0.1:3333/ cannot be opened after run the new version openrefine.exe.\nstop at \"receiving method for GET\"\nwin 10\njava version 8. \nAt 2018-09-18 23:48:40, \"Antonin Delpeuch\" notifications@github.com wrote:\n@skyarchers could you provide a screenshot of the issue?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.. \n\n. \n. Thanks\uff0c waiting for the V3.1. Hope the problem will be solved, and I can used the new version.. The excel file  is 10MB, and has 25 columns, 33971 rows.   I have increase the maximun available memory to 2048M, and the openrefine runs a little faster. Can I set the max available memory to 3072M.\n. Thanks\uff01. ",
    "pranavjadhao": "\n@wetneb Please refer the above image. Here I have imported a csv file with default options, but the output was not as expected so I have changed the options and the preview is showing correct results.\nAfter that I have tried same by calling the API through \"postman\" with the required form data (with modified options) as per the documentation. But still I am getting the table as if the default options were applied.\nPlease let me know if I am doing something wrong here or is it the problem with API?. @wetneb Sorry I couldn't get it. Which GET parameters you are talking about?. @wetneb Can you please provide me a sample request so that I can modify my request and try?. \n\n@wetneb I am unable to find the request similar to \"create-project-from-upload\" in above log. can you please point me to correct request which should be referred. or at least the GET parameters which should be passed. . ",
    "ancore": "The options parameter is taken from the URL GET parameters and ignored in form fields.\nCreateProjectCommand Line 139:\nString optionsString = request.getParameter(\"options\");\nI think the line should rather be:\nString optionsString = parameter.getProperty(\"options\");\n. Sure. I think I need permissions to create a branch for this.. @wetneb Are you able to review the PR or is this done in another way?\nhttps://github.com/OpenRefine/OpenRefine/pull/1764. ",
    "orr721": "Can you please look for dot separated date values in the format string as well?\nthis does not work: \"21.12.2012\".toDate(\"dd.MM.yyyy\")\nthis works: \"21.12.2012\".toDate(false,\"dd.MM.yyyy\"), also\nthis works: \"21.12.2012\".replaceChars(\".\",\"-\").toDate(\"dd-MM-yyyy\")\nOpenRefine v3.1-beta, MacOS 10.13.6, Safari 12.0.1, Slovak locale (lots of countries use dots, really).\nIt worked previously in v2.8, doesn't work in 3.0 as well.\nThank you.. yes, dates with dots as separators are fixed in v3.1 when you put them in the format string like \"21.12.2012\".toDate(\"dd.MM.yyyy\").\nit doesn't work with an empty format string, however I am not sure that worked in 2.8 as well. is it supposed to work with \"well formated\" strings only? (i.e. in my case Edit Cells ->\u00a0Common Transforms -> To Date still results with an error / zero transforms). anyway i can live with it.. ",
    "nsiebler-dm": "Any possibility to finalize this? Would be much appreciated.. ",
    "ralcazar-oeg": "Thank you for your answer. \nI think that the solution could be adding a variable in refine.ini file that resolve between 127.0.0.1 and the subdomain, but not sure how to code it, and some attemps that I tried did not solve the problem\nI'll open this issue in the rdf repo extension. If I find a solution I'll post it here to help others with this issue.\nPS: FYI https://github.com/stkenny/grefine-rdf-extension/issues/9. ",
    "vineetharkut": "@ostephens thanks for reply\nYes you are right it was working for me also but suddenly it is showing me \"internal error\" from past week do not know what happen and I am using version 3 of open refine only. For more clarity i am sharing a google drive link which contain video, what i am doing if i does any mistake then correct me. Basically in video i am showing the version which i am using then uploading one csv list from project list section and then previously i am made first cell of event column empty and then using transform to fill that empty cell by using custom expression. \n. @ostephens sorry for late reply\nI am using Window 10 operating system\nThere is no error in command line of open refine i have check. I am sharing the video link let me if i have done any mistake.. @ostephens Thanks for such quick response. ",
    "anchardo": "Thank you for your help @ettorerizza and @wetneb. I have completely removed my extension folder (I'll reinstall only the last version of the ones I actually use) and the 3rd version is finally working on my computer, thank you very much!. ",
    "mlhale7": "@wetneb - Is there a particular update number we should focus on? The staff member's permanent computer (not the one she was working on) has Java 8 Update 191. It is also extremely slow.. @thadguidry - I can confirm that the Java is 64 bit. Unfortunately I have been unable to locate the support.log file. Can you provide any additional instructions on locating it (Googling has failed me so far)?. @wetneb - We have also tried 2.8. I was going to have her install 2.7 since that works best on my computer, but I'm not sure if this will help or not. I can easily work with the data on my computer with 2.7, but her computer using 2.8 is incredibly slow.\n@thadguidry - I did increase the memory allotted to OpenRefine but that also didn't help. I'm not able to access this staff member's computer today to check on the file you mentioned and she is not confident in retrieving it herself. I'll see if I can try again tomorrow.. ",
    "DimEvil": "I double checked with this one from EOL, the GBIF one indeed does not seems to work on both versions.\nhttp://iphylo.org/~rpage/phyloinformatics/services/reconciliation_eol.php\nWorking EOL reconciliation\n \nnot working EOL reconciliation\n\nChecklist_Opiliones.txt\n. I could, but I don't understand that the service is working in 2.7 and not in 3.0.  So the service should be written differently?. I see what happened, EOL just updated their complete site (on Nov 17 th). I was probably the last one ever who could make use of the reconciliation service in the old form.\nThis one is working in 3.0 (http://iphylo.org/~rpage/phyloinformatics/services/reconciliation_ncbi.php)\n\nAnd in 2.7\n\nThnx for the effort! I agree it's not a bug in OpenRefine \n. ",
    "mdbaehre": "Is the CVE-2018-19859 vulnerability fixed in the 3.2-beta?  The history above and the bug fix summary for 3.2-beta seems to indicate it has - but you indicate above that you've allocated some work to 3.5.  As a result the CVE record (https://nvd.nist.gov/vuln/detail/CVE-2018-19859) indicates that it won't be fixed until 3.5.  My work won't let me use the software until the NIST vulnerabilities are fixed\nthanks  . ",
    "salgo60": "My vote for this error field to. I did an upload yesterday and got no feedback of an error. \nI just found that out when starting matching and after that I tried a Quick statement with missing records and got an error message - that I had duplicated labels...\nMy ref: T215603#4964368. ",
    "bizKOr": "Perhaps a good idea to add such option when exporting a CSV.. @lucacanella is possible to show where you put the breakpoint and the code. And I only find \"project-bundle.js\".. Ok. And where I put the new script please. Thank you.. ",
    "lucacanella": "Sorry, I've been missing the notifications.\nBack to the issue: Yes, I put the breakpoint on http://127.0.0.1:3333/project-bundle.js, row 70710. \nHere is where i put the breakpoint:\n```js\nExporterManager.handlers.exportRows = function(format, ext) {\n  / BREAKPOINT / \n  var form = ExporterManager.prepareExportRowsForm(format, true, ext);\n  $('')\n  .attr(\"name\", \"contentType\")\n  .attr(\"value\", \"application/x-unknown\") // force download\n  .appendTo(form);\ndocument.body.appendChild(form);\nwindow.open(\"about:blank\", \"refine-export\");\n  form.submit();\ndocument.body.removeChild(form);\n};\n```. ",
    "SineadM": "Thank you very much!. ",
    "abartov": "Oh, I guess I should file it against the Wikidata extension.  Doing that now.. Ah, thanks for clarifying!  I'll file future tickets here.. ",
    "davidabian": "Thanks! :D. ",
    "webieseo": "HI All,\nThank you for responses I have tried both suggestions but still getting same problem just wont build ? I have no idea how to fix \n```\n Building OpenRefine - server 3.2-SNAPSHOT\n[WARNING] The POM for edu.mit.simile:butterfly:jar:1.0.1 is missing, no dependency information available\n[INFO] OpenRefine - server ................................ FAILURE [  0.058 s]\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 13.063 s\n[INFO] Finished at: 2019-01-01T20:13:53+00:00\n[INFO] Final Memory: 41M/559M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal on project server: Could not resolve dependencies for project org.openrefine:server:jar:3.2-SNAPSHOT: Failure to find edu.mit.simile:butterfly:jar:1.0.1 in http://repo.maven.apache.org/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn  -rf :server\nError: Error while running maven task 'compile test-compile'\n```\nRegards. HI All,\nThat fixed it all working now excellent work can I just say big thank you to this project saved me hours of work. HAPPY NEW YEAR\nRegards. ",
    "tuukka": "The latest release still fails with this same error I suppose: Failure to find edu.mit.simile:butterfly:jar:1.0.1. No worries, and thanks in advance!. ",
    "nluedtke": "This was assigned CVE-2019-3580.. ",
    "browncow5": "\n@browncow5 Thanks, Ryan! BTW, how did you find out about OpenRefine and choosing this issue?\n\n@thadguidry \nI was looking for a project that I could contribute to, so that I could keep up to date on my skills. This project just happened to meet my criteria. Which were:\n-Implemented in Java\n-Data oriented project\n-Most importantly it was extremely well documented and easy to jump into, and had a set of issues that were mark as \"Good First Issue\". @wetneb\nI agree with what you are saying, it would be more logical if the date time was in the same order on both sides. Do you have any opinion on the date being bold? I was torn over whether or not I thought it was needed but in general I think it helps with readability.\n2010-07-11 12:21:20 -- 2010-07-11 12:21:20\n. ",
    "marlara": "\n@marlara so I think I have a fix in #1937 - but unfortunately it is not going to get you your data back by itself.\nI am sure it is very frustrating to lose hours of work (apologies for this on behalf of the rest of the team!).\nIf I the data loss is a serious problem for you I can try to hack something around get the data back.\n\nHi, sorry for answering so late. It's great that you have understood the issue so quickly and I'm happy for have been useful. \nI would like to have the data back, if you coul manage something it's fine. But I don't want to bother too much, so if I can do the work with some tips I could try.\nYou can write me some instruction, if you like, also by mail.. Ok, I'll try, hope I can manage. @wetneb thanks a lot, I'll you a mail. ",
    "MiconSchorsij": "I would mostly like for it not to happen again, so I can proceed with my task. Is an update for OR expected soon?. I use the LoC reconcile script by mphilli: https://github.com/mphilli/LoC-reconcile. Apparently a broken project stays broken. Or something else fails. I followed the following steps:\n- updated my OpenRefine from 3.0 to 3.1.\n- integrated the new fork of LoC reconcile in this new version of OR\n- opened my project (it had 59 changes in it's history)\n- performed the reconciliation (it worked) and then did some other changes too\n- closed the project (it had now I believe 87 changes)\n- Completely restarted my computer\n- reopened the project and it once again has 59 changes. \nIn the History folder all changes are there (it has 89)\nShould I start a complete new project (which I have done many times before I found out that this was a bug) or am I running in circles?\n. ",
    "rogargon": "No, I tried first with the .dmg but it does not start property and if I go to localhost:3333 the only message is something like \"Butterfly still starting...\". Also tried with the last commit from master, then the exception is: \nException in thread \"main\" java.lang.NoClassDefFoundError: org/json/JSONException\nDownloaded java-json.jar and placed it in webapp/WEB-INF/lib, but then the error is:\nException in thread \"main\" java.lang.NoClassDefFoundError: com/google/refine/Jsonizable. Solved for version 3.1 by renaming the user folder that was kept from a previous install some time ago. For Mac the folder renamed so 3.1 can recreate it is: ~/Library/Application Support/OpenRefine\nSo no way to reuse old projects. It works. It seems the problem was with a previous version of the RDF Extensions that was messing it all. Now I'm able to load the old projects with the latest versions of Open Refine and the RDF Extension. ",
    "AndreyR777": "\u0423\u0436\u0435 \u043c\u043d\u0435 \u043f\u043e\u043c\u043e\u0433\u043b\u0438 \u0437\u0430\u043f\u0443\u0441\u0442\u0438\u0442\u044c \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0443. \u0421\u043f\u0430\u0441\u0438\u0431\u043e.\n\n\u0427\u0435\u0442\u0432\u0435\u0440\u0433, 31 \u044f\u043d\u0432\u0430\u0440\u044f 2019, 18:16 +03:00 \u043e\u0442 Thad Guidry notifications@github.com:\n@ AndreyR777 \u041c\u043e\u0436\u0435\u0442\u0435 \u043b\u0438 \u0432\u044b \u0432\u043b\u043e\u0436\u0438\u0442\u044c \u0432 \u0444\u0430\u0439\u043b \u043f\u043e\u043b\u043d\u044b\u0439 \u043a\u043e\u043d\u0441\u043e\u043b\u044c\u043d\u044b\u0439 \u0436\u0443\u0440\u043d\u0430\u043b?\n\u0412\u044b \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0435 \u044d\u0442\u043e, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0432\u044b \u0431\u044b\u043b\u0438 \u0443\u043f\u043e\u043c\u044f\u043d\u0443\u0442\u044b. \n\u041e\u0442\u0432\u0435\u0442\u044c\u0442\u0435 \u043d\u0430 \u044d\u0442\u043e \u043f\u0438\u0441\u044c\u043c\u043e \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e,  \u043f\u0440\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u0442\u0435 \u0435\u0433\u043e \u043d\u0430 GitHub \u0438\u043b\u0438  \u043e\u0442\u043a\u043b\u044e\u0447\u0438\u0442\u0435 \u0442\u0435\u043c\u0443 .\n\n\u0421 \u0423\u0432\u0430\u0436\u0435\u043d\u0438\u0435\u043c, \u0410\u043d\u0434\u0440\u0435\u0439 \u0421\u0435\u0440\u0433\u0435\u0435\u0432\u0438\u0447.\n. ",
    "richirikken": "Clicking on the candidates is not possible in this configuration. ",
    "nanobrad": "@thadguidry, I think you got it. The structure inside the zip is stored, but I have a bunch of identical structures where the zip filename itself is what differentiates them.\nIn my case, these are log files taken from several devices. Each device produces files that have the same name (e.g., eventlog.txt and python.log). The zip filename incorporates the serial number of the device.. ",
    "TeraBlitz": "Thanks for the help/explanation!. ",
    "jfeng43": "Let me see if I can help on this.. @thadguidry I am quite a newbie to open source project. I thought it was a straightforward fix but when I look into the codebase, I realized it was not the case.\nI want to have some context here: why do we want to remove this link from the scripts?. ",
    "only1chunts": "Many thanks for the advice. I've finally found the issue, 4 instances of spurious quotation marks! they made some of the rows get treated as single values within 1.\nPlease go ahead and close this ticket, you can mark it up as user-error! sorry to have bothered you.. ",
    "Daniel-KM": "O\u00f9 ?\n. Done.\n. ",
    "nachomezzadra": "There is a small typo here, the accent is not needed.  It should be \"Aceptar\".\n. I like this one better (\"Convertir Columnas a Filas\").  It's simpler and easier to understand.\n. I also like this one better.\n. What's the context of this one?  I mean, \"Ejecutar operaciones\" sounds better to me, but I'm not sure about the context it is displayed in.\n. I like \"Coincidir\" better, as well.\n. I think \"Crear Columnas a partir de Columnas Clave/Valor\" is more intuitive than the new suggestion.  But that's my personal impression.\n. "
}
{
    "steffentchr": "Thanks -- but who would ever forget such as thing? ;-)\n. The main bit in the feature check is:\n(!!Blob.prototype.webkitSlice||!!Blob.prototype.mozSlice||Blob.prototype.slice||false)\nFrom what I can see, recent versions of Opera should support Resumble.js. I haven't tested it though. I'm more dubious about Internet Explorer, since the relevant documentation does not mention .slice() at all. Has anyone tested?\n. With the recent community activity on improvement on the Resumable.js, I have scheduled some time to generally check in with browser support. As @Takawari and @matiaslarsson rightly say, both Opera and IE10 ought to be supportable, so I will have a look to see if there are easy solutions there as a part of that project. Closing this thread for now.\n. Nice catch: We're using Resumable.js in IE10 ourselves, and I've updated the README to reflect this. The future is here ;-)\n. Yup, I absolutely agree -- and I'm collecting sample server-side implementation to include in the repository at the moment. Hopefully, I'll have a few of them online this weekend.\nA sample server in Node.js would be a good idea traction-wise. Will make sure to do that as well. Will close when done.\n. I have just added a set of samples, including Tcl, PHP and, most relevantly for this issues, a full server implementation for Node.js. \nI'll be closing this issue -- but will be setting of a web site for this project for live testing based on the Node.js.\n. Oh, and @ibjhb, yes the example does support large files.\n. @ibjhb Nope, the sample code doesn't reassemble chunks. The reason is that it probably shouldn't spend time concatenating chunks, but rather make the upload available as a Readable Stream. Just haven't written the code yet.... Reopening.\n. Have committed 0c913a5, handling file downloads in the Node.js example.\n. Does this happen when simply set up two Resumable objects:\nvar r1 = new Resumable(...);\nvar r2 = new Resumable(...);\n... or only when interacting with both?\n. Sorry, this thread should have been marked as close a while a go -- doing that now. \nAs a side-note for posterity, work is being done in #45 to make a single Resumable object more flexible so as to avoid having multiple objects in a page.\n. @zz85: Great idea getting a .clean() in there, but the pull request seem rather verbose with a lot of added logging and even unrelated code. Will you push a clean pull request for me to merge in?\n. Looking great, a few questions (just to be annoying): \n- Should the done() callback function be named differently, just to differentiate it from the end boolean (named so to be consistent with pipes)? Could just be onDone() or something like that?\n- I'm wondering if the error handling alongside unlink() makes senses, since the function is asyncronous?\n. @zz85 I agree on the end bit, but that's the naming convension within Node.js -- so I thought I'd follow it.\n. I love having some one fix my typos -- and amazing to have multiple objects work on the same page. Merging and closing.\n. Hey jarodium, \nI don't know much about PHP, but I'm wondering if this might be a configuration thing for the allowed upload size. \nIn either case, try firing up the sample and check Firebug (or equivalent). Look for errors from the script and more relevantly from the XHR calls by the uploader. Finally, inspect the actual uploads to see a) if they have content, and b) what the response is from the PHP server?\nSteffen\n. Before each chunk is uploaded, a test GET request is send to the upload endpoint -- and if it returns 200, the chunk is considered to have been already uploaded. To disable this behaviour, add this...\ntestChunks:false\nWhen I make this change in your sample though, I get this error after the chunks is POSTed (which might be by design):\n04.05.2012: Error saving (move_uploaded_file) chunk 1 for file Sk\u00e6rmbillede 2012-05-02 kl. 23.25.56.png\n. Cool -- and thank you for all the pull reqs. Curious to hear what you're working on with Resumable?\n. Looks great -- accepted with gratitude and merged ;-)\n. Beautiful, merging with thanks ;-)\n. Looks good, merged with thanks ;-)\n. Actually this is expected behaviour: That the last chunk is bigger than other chunks -- but smaller than 2*chunkSize. This is the case for two reason: To avoid uploads of byte-size chunks and more relevantly to allow for meta detection in the last chunk of a file. This is something we use ourselves to detect whether an upload is a video file, based only on the first and the last chunk of a resumable upload.\n. If so, the trick is simply to have set the chunk size to be 1 MB. This will also be efficient enough, even at large files. Not really sure what you mean about verifying server reponse?\n. I think the quick answer is that the core concept of chunking up files for upload within the browser can be used with S3's partial uploads -- but that Resumable.js itself is on a different mission.\nThe library requires a server-side component because it prescribes a kind of API itself: A way to upload a part, a way to know whether the upload completed, a way to check whether a part has already been upload etc. These features a also seem present in S3, but in a different way.\nYou will be able to abstract some of the Resumable.js code for S3 part uploads if you want though.\n. Resumable.js is three things:\n- 70% the server communications I described above.\n- 26% browser stuff for selecting, dropping files etc.\n- 4% logic for chopping up files through a built-in browser feature.\nThe latter two would apply for S3, but the first wouldn't -- so I'm not sure how to set the foundation.\n. Excellent! If you want to contribute the code as a sample for using Resumable.js, be sure to send me a pull request. Would be a great addition.\n. Perfect, well-timed -- and merged. Thank you so much.\n. I'm wondering is the best course of action for this one isn't to implement a maxFiles option, which might be null for no limit, 1 to approach your non-multiple option and any other number which would simply check and possibly error during add?\nThis would also include an option much like your callback, called something like maxFilesErrorCallback, to handle the error case. \nWould you want to implement such behaviour and submit as a pull request -- otherwise I'll have a look at it?\n. Guess it would be max number of files uploaded in a single session. So you'll check the number of un-cancelled files in the queue and match against that?\n. That's perfect, Bert! Thank you so much..\nThere are a few recent commits to the repos meaning that our changes cannot be auto-merged. I can recreate pretty easily, but would you want to resubmit a new pull request and get the credit? ;-)\n. Hadn't really considered the need for these myself, but it's a good and usable addition. Merged.\n. Good catch, pretty sure that was needed -- thanks!\n. Perfect, merged...\n. Thank Bert -- and see also https://github.com/23/resumable.js/issues/27 which you might want to weigh in on.\n. Wow, lovely ... merging.\n. My quick thoughts on maxFiles are on https://github.com/23/resumable.js/pull/17 and https://github.com/23/resumable.js/pull/25 -- and #25 was just merged in. \nI think maxFiles can be handled as a special case, because we might want to clear the multiple option in the file input when the value is set to 1. For any other kind of validation/verification (whether total upload sizes, file type, names, or even reading of the files) I like your idea of having a callback. I would implement it by sending an array of file objects to the function -- which in turn would manipulate that data and return an array of the files to be included in the request. So basically and filter.\nWould that work for your case?\n. Looks good; can't be auto-merged though. Do you want to resubmit for the commit credit -- or should I simply merge the diff?\n. That's a pretty good addition actually, hadn't thought about that use case -- since we're actually just doing this by using a non-persistant load balancer. But absolutely, submit the pull request and I'll merge.\n. Not in our case, it's merely a matter of having the servers follow along with the demand. I'm guessing though that the 6 simultaneous connections allowed by most browsers should be enough to throttle a consumer connection?\n. With the crashing, can you try increasing the chunk size from 1 MB to something like 50 MB -- I'd be interested to see if it's just a matter of the number of events crashing Firebug?\n. That's great. It's actually quite similar to a problem (which may since have been fixed) in Chrome where the entire file being uploaded was read in to memory -- and kept there. \n. What's the pattern -- or rather, what does \"large\" mean? How much logging do you do to Firebug?\n. I am closing this thread. \nNoting, that I am not entirely sure that this issue has been solved, but I have been un-able to verify with recent Firefox and Firebug. If this recurs or is still a relevant concern, please re-open with specific debug information.\n. Thanks Buffy -- great addition. In fact, I had to look at the code a few times because I could have sworn it had already been added. Merged.\n. Hi @EnTeQuAk, \nThis is a neat idea and I would love to merge the request, but the configured chunk size is a valuable piece of information, not withstanding the actual chunk size -- for example, we use the size to provision space on our storage system, and I would guess there are other assumptions in play based on this parameter elsewhere. In that sense, this merge request would be a breaking change.\nIn theory, the piece of information you're looking for is available through the Content-Length header, but I'm happy to add en entirely new header with this value (for example, resumableCurrentChunkSize) -- if you submit a new pull request.\n. @EnTeQuAk The reason for the chunkSize header very generally is to allow clients to define the chunk size itself and require no config on the server-side to be able to determine how to store individual chunks -- and how many chunks to expect overall.\n. Yeah, it's my bad in the original comment: There's obviously overhead in the multipart body, so the Content-Length HTTP header will be larger than the chunk size. The content length indication on the specific body part will be correct though.\nBut as I said, I'm happy to include a resumableCurrentChunkSize header if you submit a pull request?\n. @elksson \nSorry that I entirely missed this bug report when you posted it. I haven't used Resumable with 20+ MB chunk sizes myself before testing it just now. In my very quick test, it did work.\nMy guess is that this is related to the server not accepting files larger than the specific chunk size -- because the last chunk will be larger than chunkSize. Can this be the case? (see #56 for context)\n. @elksson \nThis can be implemented with the new query-as-a-function option added by #48. Since there will be a server-side component to this, CRC probably shouldn't be included in the core library -- but if you find the chance to write it up, I would love to include an samples of this under examples.\n. Very important, thanks ;-)\n. @xaiki \nIt's a great idea to make Resumable.js more plug-playable with Node.js, but seeing as the main purpose of the project is not to provide server-side infrastructure and since the Node sample is Express-specific I wouldn't want to promote the sample to a core part of the library.\nIf you were to make a node-resumable repository submoduling in the repos, I would happily link it though.\n. Looks great -- would it be possible for it to use the straight Github-version of Resumable.js though? Is there anything that would need to change in the repository to keep the dependencies clean in that way -- because then we'll be able to list this in the implementations list?\n. Looks good, thanks for contributing.\n(And if you're willing to jump directly on a train over the bridge I will show my appreciation in beer and sushi: http://findvej.dk/Slotsgade2,2200)\n. Makes sense, hope it worked out for your project as well?\n. @jinora \nIt's an interesting question -- and just to make sure I understand you right: You want to use Resumable.js for a file manager of some sort. The manager will have multiple dropzones (for example different folders), each of which should have slightly different upload preferences. And since the uploads would all run in simultaneously through the same Resumable object, query options etc must be attached to individual files rather than to the object itself.\nIs that correct?\n. @jinora \nYou're probably right -- that this would be a valuable addition, and that there's not other good way of doing this apart from file-level options. Am worried that it will make the inner workings of the project slightly less transparent, but of course it won't break existing implementations, only add a new options. \nI guess the idea would be to have each ResumableFile have its own .opts, which is set up on creation from a merge of Resumable.opts and Resumable.fileOptions. The latter is empty by default, but can be overwritten by the implementation as needed.\nThen the last question becomes whether your proposed method is best:\nvar r = new Resumable(....);\nvar r.setFileOptions({...});\nOr whether to opt for a simpler:\nvar r = new Resumable(....);\nr.fileOptions = {...};\nFirst is easier to document; latter is less opaque.\nWould you be interested in doing a fork and attempt an implementation?\n. Hey guys,\nSorry for checking out a bit from this conversion, but work has been busy for the past weeks -- all the usual excuses and all that. Even now, I'm hanging out on the most flaky train wifi ever. \n@srijs, I really like the simple implementation done here; it makes for a lot of internal consistency, and there's a clear way of adding in new options. You actually made good on the promise to remove complexity with your addition. I'm all for cleaning this up and merging it in. \n(Notes on actual code are small and petty: Add a few extra braces in $.getOpt for the sake of legibility, and similarly to opt for local helper vars instead of called getOpt three times in some of the (... ? ... : ....). Also, to keep in line with the rest of the code base, use $ or $this instead of self Anyways.)\n\nI'm unsure about the suggestion regarding the way to set chunk options. But I think the moment one cares about fine-tuning options for individual chunks it is justifiable to require a bit more code for that...\n\nAgree on that one. It makes a lot of sense actually to make that explicit in the code, even at the cost of the extra lines in the callbacks. Yes, these callbacks will be called a lot of time if you add a lot of files, but performance won't be hurt much, and it keeps the core code much cleaner.\nAnyone dissent?\n. Merged and solved with #59 \n. Looks awesome, merged with pleasure -- thank you so much for contributing.\n. Hi @eway0815 \nI'm not really sure this is specifically related to Resumable.js, but generally you can pass objects between frames if they're hosted on the same domain -- and this would need to be done with the Resumable object. \nSteffen\n. Generally, you user frameObject.contentWindow.resumableObject, but a Google search for something like \"cross frame scripting\" would probably help.\n. Perfect, merged with gratitude ;-)\n. Interesting -- would love to hear more about what you're working on with Resumable.js btw?\nMerging.\n. ... here I was cleaning up the issue list, and you beat me to it. Thanks ;-)\n. The reason for this is related to the prioritizeFirstAndLastChunk option. This lets you receive the first and the last part of a file for verification of meta data, and in those cases it's necessary to know at both of these parts are at least chunkSize big. This makes it handy for files with meta data in the end of the file (think id3 tags on an mp3 file).\n(Resumable.js was originally written for us to handle video uploads to 23 Video and in those cases we actually review whether a file is a valid video file already when the first and last chunk are uploaded. For this, we need all meta data for the file to be available -- and not just a last bit of only a few bytes.)\n. I don't actually disagree about the configurability, but opted against it in the implementation -- for the simple reason that having it configurable on the frontend would also mean that it had to be configurable in any and all backend implementations (the node/express lib, the python/django lib, the php lib and so on). To me, this was too high a price to pay for the feature.\nAs a side-note, you're absolute right: 8 MB chunks are probably too big; I usually opt for 1 MB chunks, making the trailing meta data chunk between one and two megs.\n. \"Could you elaborate in which way this would have to be configured server-side?\"\nBecause the server needs to verify the chunk size -- so if the size varies by client-side configuration we would also need to have a server-side configuration. This is something to avoid. I agree with you that there are alternative defaults, but not that these would make enough of a difference to warrant breaking existing implementations of server-side implementations.\n. We can change behaviour, sure -- but let me try to understand the issue in having a bigger-than-chunkSize chunk before then. Is it merely a size validation issue or are there more pressing concerns?\nOriginally, I reasoned that the *chunks are always chunkSize or bigger\" made good sense for optimizing upload times because you could have cases of an upload of only a few bytes, but I agree that the potential gains of that are negligible. \n. > Alongside with that -- more ideologically -- you \n\nhave to admit the current behavior is rather not \nwhat one would expect when explicitly setting a \nchunk size, don't you?\n\nOf course, you're exactly right about that. I agree that the original choice could have been made better; only question is whether the mistake is big enough to warrant breaking changes, or just needs better documentation.\n. That wouldn't be breaking, only question is whether to set it to true by default to make for a logical future.\n. Lets go with your suggestion. So:\n- Keeping the current behaviour by default, even if prioritizeFirstAndLastChunk is set to false.\n- Then we'll add forceChunkSize (defaulting to false). In the default mode, the last chunk will be between chunkSize and 2*chunkSize bytes. When set to true  the last chunk will be sized between 0 bytes and chunkSize.\nWould you want to make the changes and send a pull request?\n. It's a very important default for our use case -- we need a sizeable chunk of the file being uploaded to determine whether it's a supported video file. So wouldn't call it very bad.\n. Awesome -- really looking forward to seeing how S3 fits into the picture. It's something that a lot of people have been wondering about and asking for.\n. You guys are more than welcome to contribute ;-)\nIt's a bit esoteric to include support for a specific service like S3 in a JS lib that's designed to be generic, http and all of that -- but I do see the potential, even if I'm not using S3 for anything myself at the moment.\n. From how I read the S3 docs, it's kinda the same thing: In order to upload chunks according to the specs, Resumable would need to be changed. I don't disagree that there's great potential -- only that it's a different potential to the mission of this particular project.\nIf anyone has taken up this specific problem though, I'm happy to reference it in docs etc.\n. Looks great, and all extremely worth-while improvements. \nNot to be too pedantic though: Would you be able to contribute a pull request without massive changes to the indentation? (In its current form 0fdd405 would touch modify every line in the main script, making it harder for people to merge and maintain their forks and modifications.)\n. Wow, that made it even worse and non-mergable. It's pretty tricky to diff out the merge itself -- would you be able to throw in a clean pull request? Otherwise, I'll make sure to patch up this featureset to the library over the weekend.\n. Just added both features in commits 0976a38486 and 1d905235b2:\n- The event uploadStart fires just before uploading begins.\n- minFileSize and maxFileSize are now supported settings. They have individual callbacks (ugly) for error handling, and file sizes in error messages are nicely formated. I took a slightly different approach to limiting error alerts, so all error callbacks now receive an errorCount argument, which can be used by implementors to limit prompts.\n. Ah yes, that's an easy fix though...\n. Hi @Guthur \nIs this only happening for the GET request or also for the POST ones? Is there a certain way of reproducing, because it does seem to be a bug.\n. Makes perfect sense not to pollute the main object ;-) \nThe jQuery approach to cloning an object by-value rather than by-reference would be $.extend, but I loved this over-kill suggestion from Stack Overflow:\nvar y = JSON.parse(JSON.stringify(x));\nAnyways, the solution as prescribed by @srijs is to copy create new instances of the query object in lines 290 and 364. I'll get to it later this week.\n. Pretty sure it's solved with some refactoring in be4e647394a3acc4cb14619170a6f921f341512d, please be sure to verify though.\n. Awesome feature, much appreciated. Merging.\nIf you find the opportunity to add a good example of how this might used to the docs, that would be a big help.\n. Nice and merged.\nFor reference to this pull request, see #51.\n. Love the implement commit + then the \"implement for real\" commit ;-)\nMerged.\n. Solved in be4e647394a3acc4cb14619170a6f921f341512d, please refer to #54.\n. Merged with gratitude.\n. An error code on .test() might just indicate that the testing methods haven't been implemented -- which is designed to be a optional add-on. That at least is the reason for the current behaviour.\nAs for the HTTP status code resulting in permanent errors, there's another really good reason for limiting the number: We actually want to retry in many cases. In that way, Resumable is designed to be a really, really annoying client that will basically keep hitting the server until it either accepts or finally rejects a chunk.\n. Sorry: I couldn't auto-merge this issue so it took me way too long to handle it manually. \nAnyways, thank you so much for the contribution.\n. Adding a bit onto this the optimal strategy for how to handle this varies a bit by which server side software you're using. For example, we're using AOLserver for some if our core application server. On upload, the contents of a multipart form is automatically spooled to either ram or to a temp file -- and then made available to our code in a single, final chunk. In the ram scenario, nothing is on disk yet and the full buffer is available, so a good strategy is probably to assign a single file for the entire upload and seek-write the content with an exclusive lock. In the latter, the small chunk file is already in disk -- so the better strategy seems to be background concatenation once the upload is complete. (For weird reasons we need to use the disk method, which was originally a motivation for writing Resumable)\nThese thoughts also illustrate other scenarios. In Node, you might be reviving data on 10 chunks of the same file simultaneously and seeks on a multi-GB file might be expensive. So there multiple small files could be good - and you could even choose never to concat the files, just abstracting the file open and constructing the buffer from chunks. \nFinally, a number of reverse proxies (nginx, pound from my own experience) do not offer steaming uploads and instead buffers the entire request before forwarding it (again, a good motivation for node). In those cases, you'll have full data immediately upon request -- and the big file + seek approach is probably good. \n(Please correct me if these assumptions don't hold, but it is how I've been thinking about this up to now)\n. Awesome, really awesome in fact. I'm wondering though if you can change to pull request to have the ful script in the samples/ folder with a note that the coffeescript version is not maintained alongside the JS version. Just to make sure we don't need to make them match commit-by-commit?\n. Had missing this issue and pull request entirely, so sorry about that. In either case, It's been merged into master now. \nThanks!\n. This is luckily configurable, so you can easily remove 500 from the list of permanent error codes when initing Resumable. After this, we should apply a good policy for retry intervals for all non-permanentErrors and non-200 status codes. There are some pull request for this already, but would really appreciate all input on that issues.\n. My point is that 500 in that case is not really different from 404 or something similar -- you want some kind of retry interval and threshold for just giving up. As such, this issue should probably merge with the only conversation about retries etc.\n. Ah, it's unfortunately not possible -- and in fact the rapid retries at the moment is a source of some annoyance in my own projects using Resumable.\n. Happily merged, thanks!\n. Very good, reviewed and merged with thanks ;-)\n. Thanks David -- I've merged the .js file in.\nAm intrigued by the idea, but how would the Resumable client-side code be useful in Node?\n. Simple and beautiful, love it -- thanks\n. Looks good, hadn't actually realized that the PHP examples didn't support resuming. Nicely done.\n. Thanks man -- love having people love it ;-)\n. Good that someone is proof reading ;-)\n. Just talking this through with @AidasK. \nBasically, the current ResumableFile has a retry() method that a) queue all chunks in the file for (re-)upload and b) starts upload the global uploader. \nThis issue mentions that this has the side-effect of also restarting uploads of files that have been \"paused\" using the internal ResumableFile.abort() method (and no, it's not really internal -- because the Resumable object needs to be able to call it..).\nAm closing this issue because retry() is working as expected, but added #92 as a suggestion for later improvement instead.\n. @AidasK Would happily accept a pull request implementing this behaviour, closing until then though.\n. @AidasK This seems like a corner case, but feel free to submit a pull request modifying this behaviour in an API-compliant fashion. Closing until then.\n. Merged with pleasure, thanks ;-)\n(I'll be rolling back the added semicolons as the project won't be following that particular style choice going ahead.)\n. I like the idea, and have actually had this feature in the custom Flash uploaders I've been using before. The reason I didn't include it in Resumable originally was that the actual calculation tended to be inaccurate at best -- mostly because HTTP uploads tend not to perform linearly for bigger files. And in this case there's the additional complexity of connect times, seek times, test times, resume times to consider. \nMy approach would probably be to:\n- Create a timer firing every few seconds which indexes the currently uploaded file size. This index should probably be trimmed over time.\n- Create functions $.remainingFileSize() and $.currentUploadSpeed(). This latter would look at the above index to see the average upload speed in bytes/sec over the past minute or two. It would also consider if the upload was started or resumed more recently that this (starting/pausing/resuming could just clear the index, I guess). \n- Create one or two helper functions using the above to calculate and format the time remaining.\n. I like the idea of the changing chunk size -- but wondering if it's really applicable in any real use case? On mobile, would you even know what the available bandwidth is ahead of sending the chunk?\nThe last uploaded byte can be a bit of a moving target with Resumable: Since multiple chunks are uploaded at once, a file is not uploaded linearly -- in fact the first chunk set to upload might be the last to finish.\n. Yup -- absolutely doable, although it would require a bit of extra complexity on the server side to stitch together chunks. The real way of doing it would be do simply send the start and end bytes I gather and then have the server stream bytes to a fully assigned file.\nThe question remain though if it's worth the bother -- is the real life use cases justify the change?\n. Resolves, it should have been -- anyways.\n. Let's call this 1.0. And work on a 2.0, since the scope would be breaking changes.\nWe will tag up the repository and do this after having gone through the outstanding issue list for this version. Will also be adding in:\nthis.version = 1.0;\n. This follows from changes and inconsistencies in the original drafts and implementations of the HTML5 File API. Basically, in early browser implementations the File object had fileName as its property. In theory, you're right -- but in practise the upside from changing the attribute name is vastly outweighed by the downside of a breaking change.\n. Added a suggestion that would resolve this issue as well: https://github.com/23/resumable.js/issues/92\n. Let's keep it simple: fileParameterName is forced in some libraries and contexts, which is why it's configurable. The other set of parameters require not only a parameter name but also Resumable-specific logic. Basically, you'll be writing code already -- so there's no case when you won't be able to read out the hardcoded parameter name. \nIf we went the other way and made this configurable, it would put an extra burden on server-side implementators who would need to add additional code.\n. Interesting. Most likely you won't be able to remove the line entirely without causing problems with old versions of Safari -- but I'll make a fix for it and commit.\n. Resolves, it should have been -- anyways.\n. Beautiful, it's merged with thanks.\n. I agree about the problems of the server log; but the reason for having the info in the request is that backends aren't able to stream the parsings of the multipart body -- meaning that it can be helpful for implementations to be able to read the request outside of the body. And in trying to keep the interface between client and server side clean, I would hate to add a setting on the client-side that forces more implementation work on the server-side.\n. If you're using this sample simply instantiate the resumable module with the folder.\n. Nice addition, really appreciated. Is it easy for you to create a new pull request that can be auto-merged? (If not, I'm happy to make the changes manually.\n. You're right -- and I've updated the code. This behaviour can also be controlled using the permanentErrors option.\n. Ah, so the idea is to have the JS file object be available and expose it. Sounds good. If you write up a pull request, I'll merge it.\n. Awesome, closing.\n. It's should actually work as is now -- the simultaneousUploads parameter controls how many, well, simultaneous chunk uploads you want to run. I'll usually opt for 3-4 workers to boost speeds.\n. @davidchase This blog post did some basic testing of that premise, so you may be able to use it as a starting point? http://kufli.blogspot.dk/2012/06/speedier-upload-using-nodejs-and.html\n. Makes sense, merged with thanks ;-)\nSteffen\n. @davidchase, there's no such concept at the moment -- although I can see a hypothetical use for it. One question to answer is this: Which use case would have you rewinding a subset of chunks rather than rerunning the entire upload? (I'm asking mainly because the current model already allows for validation of the full file and individual chunks.)\n. @davidchase Alright; I'll close this issue for now -- but feel free to throw in thoughts and reopen. It's an interesting idea, so something to consider and pursue.\n. Sorry about that -- merged #139 with thanks. \nHappy new year,\nSteffen\n. Should be nicely registered now...\n. @binarykitchen Just tested this myself, although on 10.9. Can you set up a page where I can test and reproduce?\n. @davidchase You're right that there's a viable path to implementing limited chunk re-uploading, where the server controls which chunks are sent. However, I still cannot wrap my head around the use case for the re-upload of only a subset of chunks -- just curious to hear how you would be using it?\nSteffen\n. @davidchase But that's still something you can do on individual chunks: You'll checksum each chunk and then simply send back an error if the chunk isn't valid. After this, Resumable.js will automatically reupload the chunk. \nThe question is if there's a case where an error in, say, the 15th chunk will require you to upload the 13th and 12th chunks -- but not everything before? \nThe real complexity comes in because Resumable.js is running multiple simultaneous uploads to optimize upload speed, which means that you won't be receiving chunks in order.\n. Yeah, it's actually a great reason for chunk uploads: Because the chunks are wholly independent of each other (apart from actually coming from the same file). So if a chunk fails, just have it re-uploaded through the standard mechanism in Resumable. There's no need to worry about rewinding because chunks aren't assumed to come in in sequence.\n. @davidchase It's hard to say without seeing the implementation -- any way you can post an address of your test implemanttion?\n. Think you're right -- looking forward to merging this one along with the other changes you made.\n. Perfect -- and thanks. The pull request has been merged now.\n. I'm guessing it's path.join() then as well?\n. Yeah, we don't run Node for upload handling -- which is why the Express module is provided mostly as an examples. Which means that any debugging you can do would be appreciated.\n. What are you uploading -- and to which endpoint? (remember, there needs to be a server-side component handling the uploaded chunk and sending back a status code)\nIf you post the address of your test, I can have a look.\n. Are the any errors posted in the JS console?\n. Awesome -- that kinda explains it at least ;-)\n. It's a fascinating idea, but one for which there are no current facilities in Resumable.js. This is the case mostly because we want to handle chunks quickly and have the only real communication from the server be in HTTP status codes. This make's it extremely easy to implement server-side logic for Resumable -- but it also means that these kinds of features may not fit in the model. \nFrom experience, video conversion will take a long time -- and users will most likely browser away from the upload page before the full conversion is done. This means that you'll need to post fileConversionProgress events through other mechanisms to other pages as well -- so most likely it would make better sense to lean on that implementation in the uploader as well?\n. I'm interested to hear how to tweak avconv that tightly though ;-)\nIf the 500 MB video is 20 seconds, I could probably match that -- but not beyond that?\n. Looks awesome -- and I see that you make good use of knowing the content codec coming in as well. But you're right, let's take this conversation offline ;-)\nSteffen\n. I guess the ResumableFile object had been added before the fileProgress event fired, which is why I never had a problem with this before. In either case though, you're right and I've merged in a fix to close this issue.\n. Sounds great -- absolutely make the pull request.\n. @bantan That's a pretty hefty pull request, modifying whitespace throughout the repository. Is there any way I can ask you to contribute a clean pull request so this doesn't mess with people's merge histories?\n. Thanks man -- it's really appreciate.\n. Yeah, it might seem weird -- but it's mainly because of the different kinds of events have different scopes: for fileError the primary trigger is a file object, and there will in fact always be a file object to related the event to. That's not necessarily the case for error; we might find case where we want to throw error events that are not related to files at all (for example during load).\n. Looks good, merged -- and thanks ;-)\n. That happens ;-)\n. Awesome! Merged with thanks.\n. Yup, you're exactly right -- and the inconsistencies come from two different source: My idiosyncrasies is one. The other one is merges over time from contributor, and often I have opted for merging features rather than enforcing style. And it's because I really prefer not pre-maturely enforcing a code style on contributors.\nYou're right though that the library is due for a quick run-through aligning code and patterns. I appreciate the offer, but will figure it's better that I prioritise this myself?\n. Alrighty. Did a quick run-through of this. Again, this isn't meant to be a style guide, but generally:\n- I've used very little whitespace conditional clauses, and square brace openings and kept inline.\n- Mostly, one-line if clauses are kept in blocks for legibility and consistency. \n- I've used single quotes throughout because I find that it help code readability. That might nothing more than a personal preference though?\n- Indentation is done using two spaces rather than with tabs. Throughout indentation is done using the excellent TypeScript mode for Emacs.\nA few exceptions:\n- In a few complex conditions, I've kept a little extra space, specifically this and this.\n- I really, really like the $ shorthand; but it could be changes to $resumable and $file I guess?\n. ... and nor do I. @bamtam would you know?\n. I wondered why that didn't work ;-)\n. Yeah, the real problem here is that click-to-browse is going to be a hack in any case -- because (as you know) browsers don't offer a way of doing this in script only. So Resumable and most other libraries take the shortcut of adding <input type=\"file\"> to the dom, styling it and then hiding it. \nThis works, but there are huge downsides. For example how the cursor changes as you hover over the hidden file form element in Safari. \nIn this specific case it's very logical that you shouldn't be able to have <button><input type=\"file\"></button> so kudos to Firefox. I guess the solution would be to check the container when calling .assignBrowse() and then showing a warning?\n. I agree that this would be a nice feature to add -- and in fact would fit very nicely into the receive-and-reject scheme presented by Resumable.js. Would you be interested in adding this as an option with the library and contributing in a pul request?\n. The checksum mechanism should probably be implemented in such a way that it can be used for both the GET verification and the POSTed upload of the chunks. Basically, this will have you doing:\n- Have a property for the Resumable object that sets a checksumFunction or similar. Also there should be a checksumScheme string identifying the scheme to the server, for example \"md5\".\n- Each ResumableChunk object has a .checksum property that contains the computed sum using the function above.\n- These sum are computed asynchronously and in parallel as soon at the file/chunk is created. Some kind of queueing is probably needed to ensure that neither disk i/o nor cpu becomes a problem.\n- No chunk starts any interaction with the server before the .checksum property is non-empty if the checksumFunction option is set.\n- The .checksum property is forwarded to the server in both GET and POST requests as resumableChecksum along with a resumableChecksumScheme.\nIf the computation of a full-size chunk becomes a problem, it could be a viable option to simply calculate the checksum for a subset of the chunk.\n. That actually looks really nice. Did you have a concrete set of problems causing you to start using the feature -- and any sense of how often is happens?\n. Great write-up -- and quite interesting. I figured it would be low, but at every 20K uploads it's actually a worth-while exercise. Might also be slightly different, for example, if you're uploading a lot of files through 3G...\n. Are similar properties available in other browsers, or is this a Mozilla-specific feature? Alternatively, the data can be read from the file object (available through the Resumable object) and rendered to canvas as a bitmap.\nSteffen\n. Thanks @samtny -- and sorry for taking a bit of time to come back with an answer on this. You can now set xhrTimeout as an option as a request time limit before the request is canceled -- and then retried.\n. It's a good point -- and something I've been wanting to add to my own deployments of Resumable.js for a while. I'm away from any kind of decent connectivity for the next few weeks though. Would you want to contribute a pull request?\n. Is this limited to some browsers? Because usually, Resumable.js is setting \ntop = right = botttom = left = 0;\n... which should be the same as 100px in CSS-complaint browsers. I think the main reason for this approach though was that it saved a line of code.\n. The whole injection of input is so horribly hacky anyways -- but apparently it's still the only way to do this. The container will probably always need to be styled, because the input is ugly and non-generic across browsers, so I'm leaving this in place. But great comment.\n. This should be a easy as delete'ing the Resumable object and removing any dom object you've run assignBrowse to.\n. Yeah, sorry -- you'll need to call .cancel() as well.\nSteffen\n. I think this is a pretty common issue with browsers -- and the standard limit of 6 connections per server is probably something that you should design your preferences for. \nFrom what I recall, most browsers use the hostname/domain to determine connection limit, so you may be able to have 6 workers for upload1.example.com along with another 6 for upload2.example.com. This would give you cookie and authentications problems, but could be a viable path if you really, really want to get around this limit.\n. Simple and nice; thanks @DevinTan \n. Looks nice -- merging with thanks.\n. Thank man -- any concrete pull request are appreciated as well. I'll close the issue, since there are no concrete actions to be taken.\n. I've always hated that hack; and you're right that this cleans it up nicely. I'm pretty sure I included the previous version because of Safari issues. Your approach works well in Safari 7, but I just want to verify it in Safari before merging. \nA fiddle to test with -- if anyone finds the browser before I remember by password to Browserstack ;-)\nhttp://jsfiddle.net/PKmtT/\n. You're right -- work really well in Safari 6, and I also ran the test through a few other browsers to test. Merging with thanks!\n. Looks like good additions. From how I read the commit about configurable test targets though, the default behaviour is to require both to be specified -- otherwise tests will go to / even if target is set differently (which it will be in 99% of all cases). The default needs to be pointing to the same script. \nAlso, would you mind changing the parameter from httpMethod to uploadMethod -- and update the README file with docs for the new options? From there I'd be delighted to merge.\nSteffen\n. Sorry -- lost track of this entirely. Problem is that it's a pretty old change set, so hard to merge well. Would you want to do a clean PR; or is someone doing this at the moment?\n. You're nice -- thanks man.\n. @vadimkaz90 You can use the preprocess option for this -- but it seems like something that will make uploads to be significantly larger ahead of them being uploaded?\n. @vadimkaz90 \nIn the code above you overwrite the chunk object before calling a method on it -- so that's why there's no method left after the operation.\n. Resumable.js checks whether the chunks have already been uploaded to the server -- so they won't need to be uploaded again. (In fact, this is the trick to resumability.) These checks return 404 if the chunk hasn't been uploaded before -- which is what you're seeing.\n. Looks awesome @oetiker. I just read and tested the code, and to me these new internal methods (and public events) aren't breaking any interfaces for the library. Am I right about that? (in which case I'll hit merge and all that good stuff...)\n. Awesome! Merging, closing and thanking very much...\n. If I'm reading this correctly, you will probably need to tweak the code yourself to manage the feature you're looking for. maxFiles is designed mainly to cap the number of files being uploaded; not to cap it for number of files uploaded at the same time. I figure you can make the relevant modification to reset counters within the cancel() function?\n. Hi @dmauro -- the resumableChunkSize parameter is not related to the the currently uploading chunk (that's you'll get in the size of the body), but rather it's the configured target size of chunks. So if you configure chunks to be 1 MB all requests related to the upload, also the last one, will have resumableChunkSize: '1048576'.\nSteffen\n. Are the files byte-to-byte different in the two cases? What is meant by sanitizing?\n. You should be able to change this by updating Resumable.opts.chunkSize, but since this is a global setting applied dynamically to the chunks as they're being uploaded you should be careful in doing this.\n. The problem is that the instance of ResumableFile is created (for obvious reasons) before fileAdded is called. So by then, chunkSize cannot be changes. So to do this, you'll need to hook in to the process before then.\nBy the way, 50MB chunks seem pretty damn high?\n. Looks good, @edtechd  -- there are a few indentation issues in the suggested pull request. If you want to update the pull request you'll get credit for the lines as they're merged -- otherwise I can do it manually with an extra commit on top of the lines.\nLet me know.\nSteffen\n. Beautiful -- and thanks!\n. Changed now ;-)\n. Hi @neonnds \nYou're right that the console warnings are extremely annoying; but I do really like the extreme simplicity of only communicating through HTTP codes -- since it makes no assumption about preferred messaging formats on the server side. \nAny idea for a semantically nice HTTP status code to replace 404. One that would cause console logs to be made, and preferable one that doesn't break existing implementations?\n. I agree with you almost entirely: Settling for a JSON transport for messaging instead of flat HTTP status code could be helpful in cases. Avoiding red console log entries for something that's by-design would be welcome as well. I even tend to agree on the everything-is-200 approach for API design; even if there are compelling arguments against it.\nThe reason I'm holding back on this suggestion is that I'm worries about having two different supported ways of doing the exact same thing. I makes implementation, debugging and maintenance harder. \nIf the main intent here is to avoid console logs, we can find a way of doing so (for example by allow the check to return 204 No Content to signify the same thing as a 404 Not Found). If this is a wider argument for more detailed messaging between the server- and client-side, that's a welcome discussion to have as well ;-)\n. This is exactly what the retry() method is supposed to do, so I'm wondering if there are other factors cause this to fail. \nI quickly tried running this after a file has been uploaded: \nresumableObj.files[0].retry();\n... and indeed the file was uploaded a second time with progress and all. \nCan you set up a test page where I can have a deeper look?\n. Alright, so this recent update causes the issue: Basically chunking is being done \"in background\" using window.setTimeout, meaning that the immediately fired.upload()` doesn't have any chunks to upload, yet.\nThis is fixed with the most recent commit and with an updated jsfiddle to illustrate it.\n. That is very true ;-)\n. Thanks @babyzone2004 -- but this behaviour is by design:\nhttps://github.com/23/resumable.js/issues/51\n. Sorry -- I missed that entirely. I've merged the request now -- thanks, man ;-)\n. @demrks In general, I agree with this practise -- but it's something that should be implemented server-side and is outside of the domain of a client-side library. \n. Absolutely: There's an ajax response object attached to the fileSuccess callback. This will give you the context directly from the server.\n. Thanks @dmauro for the contribution. The main reason that this isn't in Resumable.js already is that there are two ways to adding files to the upload queue: Through the <input> or through drag and drop. Enabling the file type to be specified on former is misleading, because it doesn't apply to the latter. If not aware of a way to replicate the exact behaviour of each browser with regard to file mime type and file extensions -- but could be that I'm mistaken?\nIf not, the better way to do this is implementing a custom filter when each file is added, unfortunately...\n. This hasn't been implemented in the library yet, although it would be a valuable feature if you're looking to contribute ;-)\n. Think it's very unlikely that the required browser features for Resumable are available in the Android browser, but hopefully the initially feature test reveals that?\n. I'm fascinated that this might feasibly work -- but good at least that there's a bit of backlash standing in our way. Just to make it interesting ;-)\nThe debugging information is good: The client library flags how much data is supposed to be sent; but the resulting xhr request from the browser to the server misstates the data to be 0 rather than the correct number of bytes. For that reason the chunk is denied. Does that sound just about right?\nIs there actual data in the payload, or is the multipart body body empty?\n. That's great to hear -- would you want to contribute the change as a pull request here?\n. Do you see any error in the console -- any notices at all?\n. If you're getting a 500 Server Error returned from the backend then that's your problem -- and you'll need to review your backend log for indication about the issue.\n. Pretty sure that this is the exception detailing the server-side problem -- maybe because the 500 returns som xml/html -- but I simply don't know. In either case, if you're getting a server error that's where you'll need to investigate first...\n. There are sample in the repository touching a few different languages; apart from this I'm happy to look at your implementation if you send me a link.\n. I'll need to full app somewhere where i can be tested -- otherwise it's impossible to get a sense of what's going wrong.\n. Yeah -- I can't offer support in setting stuff up. I'm happy to look at a working example to see of there's anything noticeable. \n. Put it on the web. Send me a URL. ;-)\n. Nope, unfortunately not. \n. This repository is the most recent version of Resumable -- just grab the javascript file from github.\n. The script handling the upload.\n. Alrighty -- might be good to start by refering to the documentation at https://github.com/23/resumable.js/blob/master/README.md. \nBasically, the target is a URL pointing to the script that handles the server-side part of the upload. \n. Is it enough to fix this error simply to try {...} catch(e){} the f.container assignment?\n. Just tried uploading the image through my own Resumable-based uploader -- and it seems to work well:\n- Looking at the console, is this third file being added to the list of files to upload?\n- Looking at the console, is the photo being sent by Resumable to the server?\n- Is there a URL where I'd be able to test this myself (with the backend in place)?\n. Just tried uploading the offending image along with a screenshot of my desktop -- and both came through without a hitch. Tested on Mac/Chrome.\n. Most likely this doesn't come down to Phonegap, but to the platform and browser where you're using Phonegap: Resumable.js is browser-independent in the sense that it will work on every browser that supports the HTML5 File API along with slicing those files (there are more requirements, but those are the major ones). \nIf this is present in the browser being wrapped by Phonegap, then Resumable will work. Otherwise not.\n. From the docs it seems to be designed to mirror the W3C API: If you enable the File feature in your build, how far along on the code do you get?\n. Perfect -- thanks and merged.\n. This has been asked a few times before, so you'll find good guides in the issues list on how to handle this: The quick answer is the the standard lib makes assumptions on file name + file size, assuming that any real change to a file will also change the byte size. \nThe are hooks into the lib though to build MD5 signatures and more; so it's an easy extension to roll.\n. It's mostly that checksumming needs are pretty varied in terms of computation and external dependencies (including in the server side) -- combined with the fact that we wouldn't use the feature in production ourselves and wouldn't have too much of a footprint with it in practise. Having said this, I've encourage stable and well-tested PRs for the feature and would be happy to accept.\n. Easily done. Thanks.\n. Did you close this one on purpose -- or is there still value merging the pull request?\n. No problem -- let me know when you've had a chance to verify and I'll be happy to merge.\n. Yup, the test is a pretty big part of the method uploads are made resumable, you can read more about this at:\nhttps://github.com/23/resumable.js/issues/152\nhttps://github.com/23/resumable.js/issues/160\nThe latter also prescribes a way of avoiding the message in consoles by returning 204 No content instead of 404 Not found\n. Thanks man, merged and closed.\n. Please refer to https://github.com/23/resumable.js/issues/51 for a discussion of and explanation for this behaviour. There are reasons.\n. Thanks @jmonma -- I've merged and appreciate the contribution ;-)\n. Perfect -- I've also merged #182 into the main repository.\n. There are a few other threads/issues on this issue -- and notwithstanding the virtues of such a scheme there are compelling reasons for the current setup. Having said that, it's an easy thing to configure when bootstrapping Resumable.\n. Files are divided into chunks of a size that you choose with Resumable -- and you specify how many chunks are uploaded at the same time. By default three chunks are uploaded in parallel to make better use of the user's internet connection, and these chunks may cover different files at the same time.\n. Chunks are being uploaded in parallel. When the upload process start, a number of upload workers are started and handling their work load simultaneously:\nhttps://github.com/23/resumable.js/blob/master/resumable.js#L743-L751\n. Yes, that's true -- and by design. The behaviour can be change pretty easily by modifying the uploadNextChunk(). Such a change wouldn't make much sense to bring into Resumable proper; there's no performance gain from doing this and it would often increase the disk space required on the server-side while making for a poorer user experience (since the files would become available collectively at the same time as now, but individually only much later that the current case).\n. That's a good point -- hadn't thought of it since our own file storage is not tied up this way. \n- Two theoretical questions: Since you backbone connection (i/o on the file) will be much faster that the upload, how much is actually gained form writing multiple files at the same time?\n- Is there no good way of buffering chunks and then flushing on the web server? This seems like it would be needed in either case for resiliency, and a local 1 MB buffer for each chunk wouldn't be much of a drain on local memory -- or am I missing something?\n. Yeah, sorry -- missed this one the first time around.\nFrom how I read the pull request, it would a) make uploads workable on legacy Android by circumventing missing features; but b) require dual implementations of Resumable.js on the server side in order to support old Android browsers specifically?\n. The lib is design to see fairly infrequent changes, at least after the set of community-inspired improvements and bug fixes added late last year. For that reason, I've frankly not been too diligent in keeping version numbers. \nI'm happy to roll the current repository into a 1.0.1 -- and keep to that track if it's helpful for anyone?\n. I'm fascinated that Resumable even works on iPad, so hey...\nIs this mostly an issue on the server side though because the files are written by file name, or is this causing problems for the JavaScript as well? (From https://github.com/23/resumable.js/blob/master/resumable.js#L142-L150 it seems that this should only cause trouble if files are exactly the same size AND have the same file name?)\n. The .assignBrowse method attaches to any DOM element and is not limited to an <input type=file ...>. When the assignment is done, the library will create an input element of its own and run all of the behaviour internally without any UI hitting the user. This also means that the Resumable library doesn't pay attention to when a DOM node is a file input or not.\nIn this specific case, I would suggest assigning an <input type=\"button\" ...> as the browse button.\n. (like)\n. I'd be happy to include resumable-node.js is a form very similar to what you have here as a part of Resumble.js proper. Wondering what would be the best way -- taking over something from @mrawdon with the help from the community. And shipping it as a formal npm module?\n. As is stand the sample is just that: a sample. It illustrates how the communication might flow, but doesn't get you a full solution without reworking it for your context. Most likely the better approach is to use something like @mgcrea's Express.js middleware for Resumable -- you can find it at https://github.com/mgcrea/express-resumablejs. \n. (This is the meat of the thing if you're not using Express in your project: https://github.com/mgcrea/express-resumablejs/blob/master/lib/resumablejs.js)\n. Easy enough, thanks for pushing.\n. If you have reference to the individual file upload, file then you can do:\nfile.cancel()\n. Looks clean and simple -- merged with pleasure.\n. Hi @digitalnature \nThe intension is actually uphold the min/max constraints pretty strictly as is done now; since people are able to write custom constraints in easily enough, for example if you want different file size limitation in different contexts. I like your approach though, so will accept a pull request with the implementation.\nSteffen\n. Sure.. You should be able to start uploading immediately after the first file is added -- and then further added files will just be managed by the queue. In fact, you can simply call start every time a file is added to get the expected behaviour.\n. Looks great -- but would you be able to add a few more lines of comments to the code? Mainly notes about the method for folder support and about the current Chrome-only status?\n. Awesome! Merging with thanks!\n. Cool @piotrpawlaczek  -- can you submit a pull request though without the extra indents and spaces to make for a cleaner merge?\n. The temporary folder is controlled by the server-side implementation -- which isn't actually part of Resumable.js itself. We do provide some sample implementations, for example this one for Node which takes temporaryFolder as an argument when the library is inited. In other words: You'll need to have a look at the code being run server-side to figure out this mystery.\nSteffen\n. Hi @donut, \nThis status codes used by default are mainly chosen based on what created notices in browser consoles. You can see the discussion here: https://github.com/23/resumable.js/issues/160\nHaving said that, you can configure the response to error codes when setting up resumable (and yes 201 would be reasonable as an ok status)\n. You're right that it isn't easy to white-list replies -- you could send a pull request that make change in and around this line.\nAre you looking to add different targets for different files or for individual chunks? I can see some use case, but wondering what you have in mind?\n. A good approach to that would be to allow the target option to be a function. If that's the case, then the function is expected to return the chunk for the URL for the chunk. This would updated getTarget with something like this:\ngetTarget:function(params){\n  var target = $.getOpt('target');\n  if(typeof(target)=='function') {\n    return target(params);\n  } else {\n    if(target.indexOf('?') < 0) {\n      target += '?';\n    } else {\n      target += '&';\n    }\n    return target + params.join('&');\n  }\n}\n. (I would like to add that I have a special interest in video services, upload apis and that kind of stuff -- considering that it's my day job and the reason Resumable was created. So if you want to and can share more about what you're building I'm very interested.)\n. @axschech I never made a change based on this, but if you're looking into it please submit a pull request and I'm happy to bring it into core.\n. query is currently just a flat hash, can you provide an example of how you would want to extend functionality?\nSteffen\n. Yup, I did so now.\n. Sounds like a reasonable enough addition -- thank you for the contribution, it's been merged.\n. Can you be more detailed in the report?\n. That's a great feature -- merged with thanks.\n. Looks good, merging. Thanks @benjamincarp.\n. @psaxena What was your chunk size when seeing the problem?\n. Ah okay; weird that this is causing issues, but in either case optimal chunk size is likely much less that 50 MB -- at least considering stability.\n. Great bug report, annoying bug.\n- Is there a consistent size on the folder, for example 4 bytes or whatever?\n- I don't really like the 0-as-permanent-error, since a 0 status might actually happen in cases where a backend is down -- in which cases you of course want retries to happen.\n. That last one is unlikely to work across languages though (don't know if error messages are localized in Safari?)\nThere's this beautiful option? http://stackoverflow.com/questions/4857846/html5-drag-and-drop-detect-folders-in-safari-filelist-file\n. I can only find that code twice in the lib, but neither on line 848:\nhttps://github.com/23/resumable.js/blob/master/resumable.js#L910\nhttps://github.com/23/resumable.js/blob/master/resumable.js#L1004\n. Yeah, it's a good point -- but I'm wondering if there are consequences to doing this. The usual patterns will be that this input isn't displayed at all (at least that's the case in all implementation I've seem of resumable in the wild) and i'm wondering if it can become confusion for the input field to show a subset of selected files (in the cases where people do selects more than once).\n. It made my life easier as well ;)\nGreat about the pull request; am merging it now.\n. Ah, it wasn't a pull request. If you submit the change as a PR, I'll happily merge.\n. No problem -- thank you for taking the time!\nSteffen\n. Thanks @thegreenpizza  -- it's been merged now.\n. Looks great -- merged with thanks.\nSteffen\n. Looks nice -- merging with gratitude ;)\n. Hard to debug based on the description -- do you have a link where this is running?\n. Nice addition, thanks!\n. Good one, have been promising this one for a while -- so good that you've come to my rescue. \nCurious about how you'll be using Resumable at Atlassian?\n. Hi @psaxena \nThere's currently no way of undoing assignBrowse, but unAssignDrop() is available for use in the other case.\nFor the former though, removing all <input> nodes within the objects should do the trick in almost all cases.\nSteffen\n. Hey guys -- I'm merging the pull request to revert this with apologies. I simply messed up the check on 204 especially. Going back to the old behaviour at least is the preferable compromise. \n. Hi @saipavanvutukuri \nResumable.js itself doesn't do this, since all uploads are individual -- even if they're handled by the library at the same time. This is also reflected in the fact that you can add more files to the upload queue while the process is ongoing, so the size of the queue is in flush.\nWhen the full queue has been processed and completed though, an event is fired within Resumable -- which in turn can be used for you purpose:\nr.on('complete', function(){\n  // Tell the server that all queued files have been delivered\n  ....\n});\n. This seems to be a problem related to server-side communication outside of Resumable. Even if I'd love to, I unfortunately cannot help well on such issues.\n. @TeaSeaLancs Does this actually complete the full chain -- and get the folder support working as expected?\n. Perfect, tested a few things now myself -- so merging with thanks ;)\n. Thanks @davidpowerkneat -- it certainly does look like an error from Resumable, but I can't seem to reproduce it myself. \nI did some quick checking in both browsers (in Win 8.1) and with the test page from the repository: http://prototypes.labs.23company.com/resumable.js/test.html\nWould you mind checking an upload there and copying back contents from the developer console?\n. We're using the library in production on IE11 ourselves and I haven't seen this issue reproduced or reported elsewhere. Do you have more information and I'd be happy to review?\n. Hi @nickpeck -- is this a problem with Resumable.js or related to something behind the scenes? Because in our own usage of Resumable.js this hasn't come up and I can't force the error with a double-period file name.\n. Which browser is this? And does it work when using the browse option instead for drop?\n. Only on drop -- or also on browse?\n. Also, this code only checks that there's a number of files present -- not that all files have been delivered in full (which @cpnielsen also alludes to). The problem is very likely related to chunks being assembled before being fully uploaded.\n. Good point, although the samples/example are community provided -- and provided mostly as guidance. Will have a look to see if anyone has a better PHP snippet.\n. @saipavanvutukuri Thanks for using Resumable.js. The core library is JavaScript and I'm happy to help along on that; unfortunately though I'm not able to debug server-side issues and specifics like handling temp files and storing chunks as in this case. Really am sorry about this, but I wouldn't know where to begin...\n. Hi Stephen, \nThere's no out-of-the-box way to do this unfortunately -- that is: there isn't a way to have a single Resumable object with that is managing multiple different buttons or drop zones with different settings. If you have a reasonable number of buttons/zones, I would simply create a Resumable object for each and be done with it. \nOtherwise, you should handle buttons/zones outside of the Resumable library (you can copy out the code as needed) and then call r.addFile() for each file you need uploaded.\nSteffen\n. I don't think it would be added as a part of the standard library -- since there are two ways to add files, browse buttons and file drop zones; and since the latter don't have a name to pass on ;)\nHaving said that you can find the button in question within appendFilesFromFileList(...) using the event argument:\nvar buttonName = '';\n    try {\n      buttonName = event.target.parentNode.getAttribute('name')\n    } catch(e){};\nSteffen\n. Yup -- at least since you'd need to do custom overrides to use the button name in either case ;)\n. We use it for IE11 upwards along with recent versions of Opera. \n. From the readme:\n\n... support is widely available in to Firefox 4+, Chrome 11+, Safari 6+ and Internet Explorer 10+.\n. Yeah, sorry about that -- the site is actually just the README, but needs to be manually updated. Which hasn't happened in a while ;)\n. @keyboSlice Unfortunately there's no really good way to do this with mimetypes as you suggest: \n- Either you rely on the browser's built-in support for file type accept, which doesn't consistently allow JavaScript callbacks and a good UI for hidden input elements.\n- Or you maintain your own list.\n\nNone of these are particularly helpful: The first one would have Resumable be unusable when unaccepted files are dropped in. The second one would force us and ultimately the end-developers to keep a map of extension-to-mime-type up to date. So instead of doing that, we just let you specify the extensions instead.\nIn short: fileTypes should be something like ['mp4', 'wmv', ...] instead of mimes.\n. Looks nice -- thank you and merged.\n. I know @kkabell has been looking into this as well. \nKalle, how did it end up?\n. @jmesh Can you provide a test link for this issue -- just did a bit of testing myself and couldn't reproduce?\n. Thanks man -- merged with thanks.\n. Have a look at #127 for background, but essentially you can send back any status code from you server and add that to the permanentErrors array. My suggestion would be 204 No Content.\n. Thanks @mspanc!\n. Looks nice, merging with thanks.\n. Looks good in the PHP file, but any chance you can submit a pull request without the dlfk file? ;)\n. Beautitful, thanks man!\n. Hi @shawhu -- the use case here is out of scope for Resumable.js proper; it adds a significant amount on complexity to the library to consider different dropzones with different timelines etc. Considering the low footprint of setting up a new Resumable instance, I would simply do so for each dropzone and then write the wrapper to manage the pool around the library.\n. @sam1111 If the server supports of library's resumable chunk scheme, uploads will catch up from session to session. In the next session, the library with ask the server whether individual chunks are already in place and thus shouldn't be uploaded. The consequence is that uploading starts from 0%, but catches up quickly to the point where the process was stopped -- without uploading any data to achieve this.\n. The sample code seems to do writing and checking from the local store -- you may want to debug your code to ensure that...\n- files are being written to disk\n- file checks against those filenames/chunks actually find those files.\n. That's a great point -- and a great idea for the cleanup bit (even if you probably need to trigger cleanup on the server in other ways as well). Do you want to contribute with a pull request -- or should I simply add it in?\nSteffen\n. The maxFileSize parameter does that ;)\n. Hi @cpaulet -- can you help me with an example url where I can see this happen?\n. Hi @singlecheeze \nI tried to follow the referenced issues, but I may have missing something: Are you looking to add per-file properties to be forwarded with queries for that specific file?\nWithout having actually tested, I'm pretty sure you can achieve this by having query be a function:\nvar resumableQuery = {\n    on_progress: 'mono.upload.on_progress',\n    session: sess.id\n}\nr = new Resumable({\n    target: 'upload',\n    chunkSize: 1024 * 512,\n    forceChunkSize: true,\n    simultaneousUploads: 4,\n    testChunks: true,\n    query: function(file){\n        return jQuery.extend({}, resumableQuery, file.query);   // this can be improved\n    }\n});\nr.on('fileAdded', function (file) {\n    file.query {\n        finish_extra: JSON.stringify({\n            SR: currentSR,\n            extra_info2: \"blah\"\n        });\n    }\n});\n. Sure, the message variable in the error callback returns the response text from the server. Something like this will work:\nr.on('fileError', function(file, message){\n  alert(message);\n});\nThis requires that you're sending information back with the 404/500 response from the server.\nSee also https://github.com/23/resumable.js/blob/master/resumable.js#L799-L801\n. Test link, details?\n. Alright, without being able to access a concrete demo it's hard to help along. I have verified that both properties are working on my own production systems with Resumable.\n. @donut Yeah, the problem is that the browse option and the drop option uses two different method -- and only the former has an <input> that would be able to use the accept parameter. Also accept with filename is only formally support by Firefox according to the Mozilla dev center. So using this option would create a pretty skewed user experience where stuff would act differently only when users browse rather than drop; and only in Firefox.\nSteffen\n. Yeah, when I last tested video/* was unusable -- I didn't know about the extension thing though.\n. Awesome -- what's the case for lower casing the full filename though?\n. Not really sure what this would -- it at least seems that extension.toLowerCase() isn't being assigned to anything?. Pretty sure you can do that right now -- provided that the file name and file size is the same on all connections.\n. Yup, I think this is addressed in the README -- and discussed here: https://github.com/23/resumable.js/issues/160\n. Was just looking at that, and I actually don't know why the code wasn't updated to reflect the recommendation -- I have done so now. \n. Wonderful -- thanks for pushing!\n. I guess technically the two could co-exist -- but you're absolutely right that its a very bad idea to do so. Bad judgement on my part and rolled back. Thanks you for bring this up.\n. @drahgon55 -- the library itself is set up to using the browser DOM and the HTML5 File API. I guess that you can technically emulate the latter in Node -- and then not test assignDrop and assignBrowse, but the test coverage would be limited in that case.\n. Have a look at the query and header options in the docs. Generally, the same options are sent with every chunk, but both properties can take a function as well -- which then returns dynamic queries or headers.\n. That makes sense -- thanks for updating here, because I couldn't really figure out a good reason why this would be failing.\n. Use a function(){...} for the query option.\n. The function takes two parameters:\nquery: function(resumableFile, resumable) {\n  return {\n    md5: calcMD5(resumableFile)\n  };\n}\nThis means that you'll have raw access to the file in order to do any computation needed.\n. @IgorZiegler want to do a PR for this?\n. Thank @guzmanfg -- you even updated the .coffee sample code. Nice.\n. Wow yeah -- sorry, I never managed to push my fix for this post-merge. So sorry!\n. Fixed now. Really sorry for the regression in #292. It was certainly merged to optimistically on my part, won't happen again!\n. Thanks @guzmanfg -- I actually prefer the previous version with a flatter structure. \nOr rather: In the current incarnation of Resumable there are A LOT of optional parameters forwarded to the Resumable object; and they are all forwarded in a key-value style. (The only exception is the query parameters, but that doesn't have pre-defined indexes so the current setup is logical.)\nAdd to that the naming fileParameterName that's been part of the library from the beginning -- and I think we should keep the somewhat clunky (yet consistent) chunkNumberParameterName etc. \nI will say that the current way of configuration the Resumable object was probably not intended to scale a far as it has -- and it would be nice to have options-with-suboptions for  paramNames and for limits, to keep everything cleaner. \nYeah?\n. The sample itself write to /tmp/resumable.js/ and the node binary is called node on unix systems at least?\n. Yeah, the constant chunk size is an assumption in how Resumable works: A key part of the method is that there's no assumed order of the uploads. In fact you can upload many chunks at the same time, and some implementations even have the lib upload the last chunk as quickly as possible for meta data inspection. \nSay you have a chunk size of a 1 MB, then it's clear that chunk number five will be start at 4 MB and end at 5 MB -- which means that this chunk can be triggered even before chunk number 4 (for example). \nIf the chunk size was variable, none of this would be possible -- because chunks 1 through 4 would need to have clearly specified chunk sizes ready before chunk 5 can fire. \n. Did you update the jsfiddle for this as well?\n. Absolutely -- seems like a huge oversight from my end.\n. Yeah, it happens because the lib checks if the file is already loaded/queued/uploading. If that's the case, then it isn't added again -- and the files array may be empty even if a file has been selected.\nTo avoid the behaviour where filesAdded this line could be update to check if files.length>0. However, I kinda like the behaviour where filesAdded is fired to indicate user actions -- and an empty array argument would indicate that no new files where added as a part of the interaction.\n. No problem at all.\n. Is this after reloading the page/resumable? Or before, because I can't seem to reproduce this one.\n. Is there any network traffic happening? Because you shouldn't even be able to add the same file twice to begin with...\nAlso, are you getting duplicates of the same file in the filesAdded event?\n. But what happens on fileAdded -- is that fired with content?\n(Otherwise, I think it's simply an over-communication of the fact that the upload IS done. You'll see the complete event trigger a few times depending on flow and how many times files are added in either case.)\n. Can you try updating with something in this vein, just to see if that improves things:\nuploader.on('filesAdded', function(files) {\n  alert('file added');\n  if(files.length>0) uploader.upload();\n});\n. There's a recent ticket on the same issues here. I like the idea of firing the event since it makes it explicit that there was user interaction -- but could go either way, I agree.\n. Not directly from the library, unfortunately -- but you can easy use the progress events to estimate upload speeds.\n. Hi @nickdatum -- I'd accept a pull request to improve this performance, but since Resumable.js at its core is designed to handle large files resiliently, the scale of file handling seems somewhat out of scope. Obviously, if you're looking at it and find good solutions, please share.\n. It's very likely that -- combined with the fact that the library does a lot of operations on \"all files at the same time\". Adding 10K files will create as many instances of ResumableFile immediately, and all calls to methods like .progress() will loop through all and continuously poll the progress of each items. Which is going to be a pain.\nThe problem is certainly solvable; I'm worried that it's a pretty sizeable rewrite of lib though. On hacky solution would be to manage drag/drop/files array outside of Resumable with a queue that would feed files to Resumable in drips rather than all at once.\n. That look great -- very solid improvements from the look of the code, and it seems to be doing all the optimisations in a transparent manner that isn't even breaking backwards-compatibility. Any outstanding things or do you want to merge this in to the core liv?\n. Awesome -- do you want to submit as a pull request?\n. This one, right? https://github.com/23/resumable.js/pull/313\n. Is it possible to do this well on IE?\n. This sounds like a Java issue, not something related to Resumable?\n. This seems to be a server-side question -- which we unfortunately cannot help out too much with given that Resumablejs is a client-side lib.\n. You should be able to bind to the error event and cancel the upload process from there?\n. nice and tidy -- thanks ;)\n. @sachinwalia2k8 Yeah, that description holds for us as well: We're running multiple web frontends, which resumable chunks being uploaded to any and all of them. Behind the scenes all web servers share some storage (via NFS) and chunks are stored and concatenated there.\n. Is the problem that the file isn't uploading for this time? Or that there's no status for the upload happening? \nIs there a place when I can test this in practise?\n. Without access I won't be able to help much. Generally, note that Resumable won't actually upload many files at the same time -- rather, it will upload many chunks at the same time. This means that a second file won't start uploading before the former is almost done (the exception to this is prioritizeFirstAndLastChunk as described in the docs).\n. Yes, that should be done -- and I basically wanted to hold off with cutting the release until the most recent set of PRs had been merged. Any more coming, or should we call this a 1.1?\nThanks, \nSteffen\n. Someone else published the project to npm originally, and I'm waiting for ownership to be transferred in order for me to publish. So yeah, annoying to hard-reference a commit ;). Absolutely -- done.. Hey guys, just chiming in here quickly -- and there's also another thread/issue covering this exact same question elsewhere.\nResumable.js is set up to manage uploads of multiple chunks of files, since this will heavily optimize the use of bandwidth. For example you can run five upload processes at the same time, each uploading a one megabyte chunk of data. Each of these chunks can fail resiliently, and they can conclude asynchronously etc etc. \nThis is different from uploading multiple files at the same time, and I consider it bad practise to try to do this -- because if Resumable is set up correctly, you're maxing out the upload capacity already with the chunked uploads; and effectively you would just be upload each file slower if you run file uploads in parallel. The would cannibalise from each other, and certainly in the setup with multiple resumable instances at once managing each of these would make for poorer performance.\n. Hi @Marthyn \nI just upgraded to the most recent iOS 10.1 beta (beta 5 I think) and verified that Resumable.js is working as expected. I wonder if this is related to a specific file, or is there's any other information that you can give to shine a light on the issue?\nSteffen\n. No, didn't set that in the initial report - in guessing this is Chrome for iOS?\nSteffen Fagerstr\u00f6m Christensen\nCTO & Co-founder, TwentyThree \u2122\n+45 2532 7058 [tel:004525327058]\nsteffen@twentythree.net [steffen@twentythree.net]\nTwentyThree\u2122. The Video Marketing Platform.\ntwentythree.net [http://twentythree.net/]\nOn tir., okt. 25, 2016 at 7:34 AM, Marthyn Olthof notifications@github.com wrote:\n@steffentchr [https://github.com/steffentchr] We tested again with beta 4, it works indeed again in Safari. Chrome is still an issue. Did you test in Chrome too?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub [https://github.com/23/resumable.js/issues/336#issuecomment-255940783] , or mute the thread [https://github.com/notifications/unsubscribe-auth/AAA_uSxxj6EVc5xLnzMbQpazaRZ8CsJdks5q3ZTagaJpZM4Kbu6q] .\n. Keeping this open until a GM of 10.1 -- to see how it shakes out.\n. nice, merged with thanks...\n. @mbuehrig Unfortunately its slightly out of the scope of what Resumable.js does -- since it's a server-side thing with the permissions to match. So you'll need to have some kind of API/ajax method to delete the file, maybe using the same token that you're uploading with if that works within your security model.\n. This seems like a problem wit the server-side implementation -- because uploading the exact same chunk twice for the same file shouldn't cause an issues; and two clients uploading the same file, with the same token should actually improve performance.\n. Yeah, so that's probably the problem then, but that's mainly about then sharding the upload folder per user -- or allowing for simultaneous file uploads. We use the latter to great effect at TwentyThree, but the former may be a better option.\n. @thewilli Yup, that sounds well-reasoned and very valuable. Happy to accept the pr.\n. Thanks man -- that's beautiful.\n. Good one, thanks...\n. This is on me for missing this in the original PR that introduced this.\n@thewilli you introduced this check, but no other references to jQuery apart from this?. Pinging @thewilli again -- otherwise planning to remove it over the weekend.. Thanks for the contribution @thewilli. \nIs the queueLength = 0; correct? It seems that this variable should be in/decremented elsewhere.\n. Makes sense. \nI think you're right though that the variable should be removed entirely -- instead of staying around in a limbo state. Would you want to update the PR or should I go ahead and merge+augment?\n. Looks great -- will be able to run the manual merge some time later this week.\n. Hey -- that made it easier, thank you ;)\n. You're right -- that's just stupid. Closing.\n. Thanks -- look good and merged!\n. You can add 401 to the list of permanentErrors status codes when loading Resumable. Should do the trick ;). Hi @adavidof -- thanks for the contribution, it's much appreciated. cb3926f and fee5902 are great, and I think the intention in 0139815 is good, happy to accept a contribution adding that feature. In the current form though, testTarget is set by default, which adds confusion and effectively mean that you won't be able to set the test target to testTarget. I suggest that the default is null, and then testing against that -- would that work for you?\nStyle changes such as ff48aa7 and 205fc97 since they explicitly undo our own style guides, hope that makes sense.\n. Just leave it as null in the PR and it should be alright ;). Perfect, did you want to remove some of the styling commits -- or should I merge individual parts manually?. Yup.. Merged with thanks ;). Yes, absolutely right -- or rather it's a consequence of maxChunkRetries being set to undefined by default, which in effect means that there's never an error occuring/registering. I've changed the default value to 100 to avoid this case.. They should -- and from what I can see they are?\n\n. Any reason for not just setting that limitation as an option client side?\nSteffen Fagerstr\u00f6m Christensen\nCTO & Co-founder, TwentyThree \u2122\n+45 2532 7058 [tel:004525327058]\nsteffen@twentythree.net [steffen@twentythree.net]\nTwentyThree\u2122. The Video Marketing Platform.\ntwentythree.net [http://twentythree.net/]\nOn Mon, Feb 6, 2017 at 13:06, rmhubley notifications@github.com wrote:\nI have a similar issue with file size limitations. When a GET is issued to check on a chunk the client should be able to discern between \"Chunk not found\" and a cancelable error state i.e \"Upload file size too large\".\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub [https://github.com/23/resumable.js/issues/358#issuecomment-277813379] , or mute the thread [https://github.com/notifications/unsubscribe-auth/AAA_uW93ovDZIR7y4B0f4A9iukYHpC1uks5rZ4tegaJpZM4LlJbt] .. Sure, you should absolutely check chunks during upload - and return forbidden and errors accordingly. But that had nothing to do with the chunk checks.\nSteffen Fagerstr\u00f6m Christensen\nCTO & Co-founder, TwentyThree \u2122\n+45 2532 7058 [tel:004525327058]\nsteffen@twentythree.net [steffen@twentythree.net]\nTwentyThree\u2122. The Video Marketing Platform.\ntwentythree.net [http://twentythree.net/]\nOn Mon, Feb 6, 2017 at 14:36, rmhubley notifications@github.com wrote:\nSure. Because that would be pretty easy to manipulate to bypass what should be a server concern.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub [https://github.com/23/resumable.js/issues/358#issuecomment-277837117] , or mute the thread [https://github.com/notifications/unsubscribe-auth/AAA_ufMUgzq_RpdXubYNRgqDRs7og7eHks5rZ6BxgaJpZM4LlJbt] .. Yeah, it's validating the chunk on disk to make sure it's fully uploaded. So it's similar, but different ;). @Larryun I might be misunderstanding the question, but ahead of every chunk being POST'ed to the server, there's a preflight/test GET request using the same information. So by that logic, tests are run before the actual data is sent.. There has been a few requests about this before -- and the consensus has been that even if it's an awesome feature, it's a feature that's out of scope for this library. It would be us building (and supporting!) interaction with services that we don't use ourselves.. Awesome, thanks.. Alternative is to force maxFiles in those browsers -- did you guys end up do some magic testing to handle this case?. Yeah, if you have Resumable nested within a form, that form logic will apply to the form elements created by Resumable. In some cases this can be used to create features; although the usual advise is not to have Resumable within a form. . This is out of scope for the library, so won't be able to help much along ;). First, I don't know of such a project -- and I would add that it might be pretty tricky, since persisting large amounts of data on the client-side from session to session can't/shouldn't be done.\nConsider the case of a dropped internet connection that Resumable.js is set up to handle: Someone uploads the first 2 GB of a 3 GB files. In this case, chunks for the the first 2 GB will be stored on the web server -- and when the client comes into work the next morning and uploads again, then the upload will catch up and only upload the last gigabyte of data. \nThat's different in the download scenario: Here you would want the browser to store 2 GB in a temp folder somewhere, waiting for the download to maybe be resumed later on. That's a pretty wastful pattern, even if it were possible to do.\nSome notes on implementation if anyone is thinking about building:\n- It's perfectly possible to stitch together a file in a browser using multiple XMLHttpRequests.\n- Probably a library like this should not have server-side requirement -- but ought to simply use HTTP 1.1's HTTP Range Requests. This would make support pretty much built-in to anything and make deployment easier.. Sure -- you can use the query option on Resumable for that exact thing.. Thank you for a great write-up of the issue. Am curious to hear though about the specific negative consequences of multiple complete triggers?. @afukada The mode for Resumable is to deliver chunks for files successfully, and if any of the chunks fail either retry that chunk or ultimately fail the upload. \nA common pattern, for example, is to run verifications on parts coming in to the server side. And then void the entire upload if the verifications fail. \nIn your case however, you're looking for the entire upload to reboot -- which isn't a built-in behaviour. You can do it easily enough though by listening to the error event handler and retriggering the upload.. Thanks man!. The core idea of Resumable is to shoot of A LOT of requests for the same file -- and to make each of these chunk requests fairly disposable. If they fail, let's retry. Etc. \nThis also means that there isn't a lot of utility built into the system around the chunks themselves. If you want to, however, you can find header through the file object that's being pushed in event. Specifically, file.chunks is an array of all file chunks; and each item in the array will have a .xhr property which should let you .getAllResponseHeaders() or whatever you need.. I can't say exactly what's happening here -- but my input would be that this seems like a bug on the server-side of things which isn't covered by the Resumable library.. Sounds good, sorry for missing in the first go-around.. Yeah, use the fileType option when setting up Resumable.. Start with https://github.com/23/resumable.js/.. You should be able to configure a property in your setup for Resumable for this one:\nvar r = new Resumable({\n  ....\n  fileTypeErrorCallback: function(file, errorCount) {\n     alert(file.fileName||file.name +' has type not allowed, please upload files of type ' + $.getOpt('fileType') + '.');\n   }\n });\n\nFrom here, simply change the behaviour -- whether by omitting the alert() entirely or by changing the text.. Awesome -- thanks. I actually never knew about void 0 so had to read a bit. . We test for File being available, so this should work well. Merged ;). Metadata in Resumable's sense is transferred through the request itself, not in the data of the file -- and that's part of every chunk request being sent off. For video transcoding and to transcode chunks individually you're out of Resumable's territory -- for that's you'd need to split into more logical chunks follow GOP structures, not just to split by size of the chunks. . Thanks Marc.. Thanks - appreciated.. @Lehakos @nickdatum Thanks for pinging on this. I won't have time to look at this in detail with the matter of urgency that you probably need. Realistically, it may be a few weeks before anything can happen. If you're able to spare time and thought before then, I'm very happy to review and accept a PR with a solution.. Looks good -- thanks.. Specific server-side implementation is slightly outside the scope of what w can help with in issues -- one hint though: Try Googling \"resumablejs django\".. Kinda helps ;). Makes sense. Can't figure out if this will break existing conflict handling, but guess we'll see.... Done -- sorry I missed this one in Dec.. That's not as instant -- any reason for not just specifying the 409 in code?. Was thinking more about something like this:\nvar r = new Resumable({\n  permanentErrors:[400, 404, 409, 415, 500, 501],\n  ....\n});\n\nThis would have the same effect.. Beautiful -- appreciate it. \nMove the merged file to to samples/ folder though: https://github.com/23/resumable.js/commit/be4d3175130b271976c7d2a563560d056025aed5. Think you should be able to achieve this using the fileTypeErrorCallback property.. Use .progress() and fileProgress(file) respectively.. Is this different from the simultaneousUploads upload configuration option?. simultaneousUploads is the number of processing running uploads; that's for chunks.. This is only for docs, or?. Merged with thanks.. Great addition, thanks!. try:\nvar r = new Resumable({\n        target:'/resumable_upload',\n    maxFileSize:'8000000000'\n});. There's probably a reason, and my guess is that it's done because the same code is used for the preflight GET and for the upload POST. Obviously it would make sense split these two by the request type of it's breaking server-side behaviour.. Not using the library directly -- but you can encrypt chunks before upload using the callback hooks I guess.. Depends on your setup for Resumable, I guess -- but not a known issue.. There's currently no way of doing this automatically -- although you'll likely easily be able to iterate through chunks to flag off \"done\" bits. I agree on the problem of the overhead; while on the other hand the asynchronicity is one of the core upsides of Resumable. Anyways, would receive a well-designed PR handling this case.. Looks neat. Can you make one change for me: Have the function name be markChunksCompleted() instead of the slightly misleading setStartChunk()?. Awesome -- merged with thanks!. Sounds like a pretty good idea -- if you want to contribute with a PR for this, I'll happily accept.. The core logic is that I do more extensive testing and review of the code base before upgrading the version number -- and suggest people simply reference the github master branch if they want to live on master.. Merged with thanks ;). Looks pretty neat -- and cool that you're closing your own issues as well ;). Wow, this is awesome! If you're interested, we could talk about getting you on as a straight-on committer to the project?. It'll be a bit before I'll have the chance to run through testing well enough to mark this up as a release. In the meantime, referencing master is a better bet.. @upMHuhn Did you you delete your reply here? I loved it:\n\nNo, it uses its own \"protocol\". Sure, you can have {query:function(){...}} and do whatever.. \n",
    "Takawari": "Thanks for the lightweight library. I have tested it for some days on a site where user upload a lot of files and it turned out that Opera 12.01 is not supported. But it should probably: http://caniuse.com/xhr2\nThis is the line where I get the following error message:\nformData.append($.resumableObj.opts.fileParameterName, $.fileObj.filefunc);\nUncaught exception: DOMException: NOT_SUPPORTED_ERR\nI tried all day long to fix that but I did not have any luck. Has anyone solved this already or could take a quick look?\n. No, it's not revoked by the feature test. According to this link, it may be possible that all JS functions are given for Android browsers:\nhttp://caniuse.com/#search=blob\nThis is an extract from my logs. It seems, the file_size is not transferred correctly on Android. And I'm testing with:\nif ( $number_of_chunks == 1 && $filter['resumableTotalSize'] != $filter['file_size'] )\nFile chunk size invalid. Chunk must be file size in this case.\nPOST PAYLOAD: '\nresumableChunkNumber=1&resumableChunkSize=1048576&resumableCurrentChunkSize=758433&resumableTotalSize=758433&resumableType=application%2Fpdf&resumableIdentifier=758433--changedpdf&resumableFilename=-changed.pdf&resumableRelativePath=-changed.pdf&resumableTotalChunks=1&upload_token=5212345678901234567895b' \nFILE PAYLOAD: '\nfile%5Bname%5D=Blobcchanged&file%5Btype%5D=application%2Foctet-stream&file%5Btmp_name%5D=%2Ftmp%2FphprVBCtb&file%5Berror%5D=0&file%5Bsize%5D=0'\n. ",
    "matiaslarsson": "Since IE10 also has the slice method I guess this should work in IE10 too. Will test this the upcoming month.\n. ",
    "craigmarvelley": "Hi - did anyone ascertain whether the library works in IE10? Would be nice to update the project readme to indicate that if so.\n. Great to hear, thanks!\n. ",
    "ravindermahajan890": "Please let me know if resumable.js is supported in IE10+.\nplease refer to this url as it does not support the same in IE10.\nhttp://jsfiddle.net/ravindermahajan890/rbd7yx00/\n. As observed in the code, it always looks for the next available chunk from the file already being uploaded.\nit moves to the next file only when the file chunk is completed.\nPlease provide a way wherein i can upload my files in parallel.\n. if i have three files file1, file2 and file 3 of  200 mb each and my chunk size is 5 mb... as per my observation 40 chunks of file1 would be uploaded first, then 40 chunks of file 2 and so on...\nPlease confirm?\nhttps://github.com/23/resumable.js/blob/master/resumable.js#L648-l669\n. ",
    "ibjhb": "Will the node.js example support large file uploads?\n. @steffentchr Wonderful!  I need to be able to upload 1-3 GB files and this looks like it might be the solution.\n. @steffentchr Thanks for your replies and work.  I have the upload working and I see the chunks being saved by node.js but I don't see it actually putting the chunks together into one file.  Is this something I still need to write or am I missing something?\n. ",
    "maacl": "This looks brilliant.\nOn Tue, Feb 7, 2012 at 11:48 AM, Steffen Tiedemann Christensen\nreply@reply.github.com\nwrote:\n\nOh, and @ibjhb, yes the example does support large files.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/23/resumable.js/issues/3#issuecomment-3845887\n. \n",
    "jzelez": "Only when interacting.\nWhen both are initially set up on page load, no errors occur. But when the upload should start, the error is given. \nfileAdded event is processed ok, and the error is triggered somewhere in r.upload().\n. Can I possibly help somehow in diagnosing the problem ?\n. ",
    "blocka": "+1\nI have this issue two. Seems to be that the $ in $h.uploadNextChunk(); is not referring to the $ that it came from.\n. ",
    "zz85": "opps sorry it was left over from a project i was trying to implement. added some cleanup\n. actually, i'm not really a fan of that option parameter - would prefer kind of a signature like\nclean(id, success, error)\nwrite(id, stream, success, error)\netc..\nit was a quick hack, so when i found that the option parameter was there, i just added it in :S\n. i've cleaned up and make changes from .error -> .onError, .done -> .onDone, see if you're okay with it?\n. ",
    "jarodium": "a) \nGET requests only\nhttp://dev.phelios.org/up/go.php?upload_token=rrrr&resumableChunkNumber=1&resumableChunkSize=1048576&resumableTotalSize=11926015&resumableIdentifier=11926015-clubvoyeurptzip&resumableFilename=clubvoyeur.pt.zip\nb) PHP receiver is not receiving $_FILES or $_POST, since Firebug only shows the $_GET requests, \nMy environment is: \njquery.1.7.2\nUsing no form tag.\nI'm not using your sample in the distro as it is, since it does not have the resumable-uploader.js in there so i've stripped it out. \nJust using initialization routines like instructed in the readme file and a auto-upload feature on \"File added\"\nTesting small files, so upload limit does not trigger.\nThe javascript is indeed chunking the file as I see those multiple requests pouring out of FF, but since there are no POST's and FILE's the PHP does not get there...\nI had not had the change to lookup your source yet, but does the script require a form tag?\nThanks\nPedro\n. On line 146 you have:\nvar relativePath = file.webkitRelativePath||file.fileName||file.name; // Some confusion in different versions of Firefox\nFirefox 25.0.1 exposes a mozFullPath variable when inspecting the file. Maybe Resumable.js can expose the fullpath or have a method for retrieving the image binary data?\n. From what Chrome tells me there isn't. Only relative path is present. Other browsers I really can't tell right now. I must install Safari and Opera.\nCan you point to me where is the data? \nUpon inspection I can only find the proto property.\nI think its the boostrap method I should be using but I don't know if I use the File Reader api or not...\nThank you for answering Steffen.\n. I see... I have Windows here. I'll try using a single upload with that image and report the results.\nThank you for you time.\n. Hello steffen.\nI closed on purprose. At a first glance, I thought it was working properly, because the Firebug console was messing with my view. After I tried again, without the Firebug, I verified the code was not working. But feel free to try. \nLogically I think the patch is correct, but I need to spend more time around this.\nI really enjoy this script and don't want to mess it up by adding stuff which is not working properly.\n. Hmmm... I doubt they should have the same filesize. Since pixel information is different ( hopefully i'm saying this properly ) the image shouldn't be the same size as the next. I have tried to select 3 different photos, and the third ( which is the last one ) gets uploaded. \nI'm wondering if the file.webkitRelativePath is returning proper values on Ipad ( since it's safari... )\nI would apply a random number (5-6 digits in length) at the end, just to make sure there isn't any duplicates... With numbers is safer...\n. ",
    "garjitech": "The problem is that the actual file input still contains the selected file after cancel is called.  An easy fix is to set input.value='' after calling appendFilesFromFileList inside assignBrowse.  \nI've submitted a pull request with the fix.\n. I haven't actually started using it in production.  I'm currently working on using it as a replacement uploader in our Chrome extension (https://chrome.google.com/webstore/detail/transferbigfilescom-gmail/lajnjaghjodocddaglgghffgacnoepgf).  \nThanks for all the work you've done!\n. I was also considering adding an option which would allow the user to specify an external validation function (e.g. for possible server side validation).\nYour thoughts?\n. Sorry for the delay.  I'll resubmit tonight.  Thanks!\nOn Oct 8, 2012, at 11:42 PM, Steffen Tiedemann Christensen notifications@github.com wrote:\n\nLooks good; can't be auto-merged though. Do you want to resubmit for the commit credit -- or should I simply merge the diff?\n\u2014\nReply to this email directly or view it on GitHub.\n. Do the load balances allow you to get around the max connections per host name limit?\n\nOn Oct 8, 2012, at 11:30 PM, Steffen Tiedemann Christensen notifications@github.com wrote:\n\nThat's a pretty good addition actually, hadn't thought about that use case -- since we're actually just doing this by using a non-persistant load balancer. But absolutely, submit the pull request and I'll merge.\n\u2014\nReply to this email directly or view it on GitHub.\n. I agree that 6 is enough to throttle... In our case we are not trying to throttle but instead trying to reduce the total upload  time. \n. I think it is probably firebug that is crashing the browser.  Have you tried uploading the same file with firebug closed?\n. Interesting.  Thanks for the heads up.   Ill take a look and see if I can repro it.\n\nOn Oct 15, 2012, at 9:44 PM, hugohuynh notifications@github.com wrote:\n\nYes, I have tried, I rarely happens but sometime it still happens\n\u2014\nReply to this email directly or view it on GitHub.\n. So I'm definitely able to make the browser crash with firebug open but not when it is closed.\n\nI've added a speed calculation as well (it hasn't been submitted as a pull request).  I'm curious to see what you come up with.  If you want to checkout what I have just let me know and I can make it available.\n. No problem.  I'll try to get it up tomorrow.  Glad to see you using resumable.js but all the credit goes to @steffentchr \n. Sure, just fork the repo and then submit a pull request.\n. Uploading a large file with firebug open quickly causes the memory use by Firefox to shoot up and that's what caused it to crash in my case.\n. ",
    "EnTeQuAk": "Wow, this was fast. Thanks a lot!\n. @steffentchr Thanks, for the feedback, as I said I also was kinda unsure how this parameter was used. But I do not really understand how you use the chunk size to provision the space as I thought totalSize would be used for this kind of use case. This would be very interesting.\nBut you are totally right with Content-Length somehow I did not think about that, I will adapt my code properly, I don't think that a separate header carrying exactly the same information is useful at all.\nThanks!\n. @steffentchr I just checked using Content-Length and magically it's not identical to the actual chunk size but always some byte larger. From my understanding this shouldn't happen so I I'd love to hear your idea about that if you have some.\nIf not I'd suggest to add a separate header to transmit the real file size.\nEDIT: Alright, found the problem... resumable.js is adding the parameters to the POST body so that they are added to the Content-Length. This way I cannot properly use it :-/\n. ",
    "sarvigalava": "if file has 1,5MB it is uploaded entirely\n. fixed with : \nvar lst=0;\n      if($.file.size/$.resumableObj.opts.chunkSize>0)\n        lst=1;\nfor (var offset=0; offset<Math.floor($.file.size/$.resumableObj.opts.chunkSize)+lst; offset++) {\n    $.chunks.push(new ResumableChunk($.resumableObj, $, offset, chunkEvent));\n  }\n. seems to work but md5sums are different. Need to know if it is a php issue or javascript :(\n. fixed : \nif ($.fileObjSize-$.endByte < 0) {\n      // The last chunk will be  less than chunkSize\n      $.endByte = $.fileObjSize;\n    }\n. my php max file size limitation is 2MB so max chunk size must be 2MB in order to be efficient. I must upload big archive 10-12GB on server with limitation without using ftp. Thanks for script it works awesome. One only question : how to verify  server response ?\n. I want to send to client error messages (permission denided, already uploaded etc etc) \n. ",
    "inconduit": "thanks for the quick reply.  i haven't had the time to go in depth in resumable.js nor the s3 multipart api, but in your opinion would i be able to modify resumable.js to function with the s3 api? i'm just wondering if there are any showstoppers that i'm not seeing at first glance.\n. okay, thanks a lot for the input.  i'll revisit this in-depth when i have the time.  imagine a drag and dropping large files to the browser that are multipart and concurrently uploaded direct to s3.  a dev can dream, right?\n. ",
    "devatwork": "I guess that is better but how would I check for it? Is it max number of files? Or is it max number of files uploading at once?\n. Hi Steffen,\nI updated the code accordingly, please have a look and let me know what you think.\nBert\n. ",
    "hugohuynh": "Yes, I have tried, I rarely happens but sometime it still happens\n. another things is, on the on progress event should return the speed of upload and time remaining. I'm modifying a bit to calculate the speed.\n. Yes, It rarely happens when you close firebug. I still debugging.\nYour lib is quite good, I am using in my project so I am going to modify a bit to add speed calculation ability But if you have already implemented that is very good news. Might you make it available?\nMany thanks,\nHuynh.\n. Thanks, but in case If I can improve something, Might I send code to you and how?\nHuynh\n. Ok, thanks. I'm very glad to see you.\n. ",
    "sprzen": "Hello, google search brought me here while searching for firefox crash caused by file uploading. I have written a gwt component that uses xhr2 / iframe to upload files and it seems that I'm experiencing the same problem like you are - firefox is crashing  while uploading a large file but only with firebug open and when using iframe as the upload method. Do you have any additional insights? Do you perhaps know if this is a reported firebug problem?\n. I tried to upload a 60 MB large file using iframe and Firefox crashed, but when I use xhr upload for the same file it works normally. If i try to upload a bit larger file (150 MB) using xhr it also makes Firefox crash. From what I see, when upload is nearly finished Firefox memory usage starts to climb rapidly and it crashes. All this happens ofc only when Firebug is opened.\n. ",
    "2xyo": "It works like a charm :-)\n. ",
    "JindrichPilar": "@steffentchr \nAllmost Correct, but i want to use only one dropzone (given to AngularJS two way data binding)\nThe main issue is Resumable.js doesn't provide API to customize options for files separatarately. (e.g. query option). Right now it is kind of black box, you create instance, give it some option, and then all you have is callback when user try to upload more files then allowed. \nTo be specific, I am working on private web app (AngularJS client + REST API server), there is simple file manager (user folders, group folders). And also a photo gallery, but the gallery is stored in Google Picasa. So when a file is uploaded i want to call API to move image from server to Picasa. And it would not be good to have server API like\n/file/upload/?action=picasa&title=Summer&description= so posibility to specify callback (also for individual groups of files) would be much helpful.\nSure, i can have different Resumable object for album and file manager, but file manager still will have problem with folders and e.g. maxFiles will be hard to make shared by them.\nWell, to be clear as you said query options etc must be attached to individual files rather than to the object itself. is the issue.\n. @steffentchr \nSure, I would gladly attempt an implementation.\nI think setFileOptions is better because it simply suggest getFileOptions in case somebody need to retrieve current options, which could be helpful in large apps.\n. @guillemsalas \nHi,\nI dont want to set only query for file, but almost any option (which make sense for file... e.g. not simultaneousUploads)\nThis way you can have one Resumable.js and handle all uploads (to multiple targets and etc.) with only one object. \nNice example may be this:  You have multiple servers and you want to upload to the one with best connection to user. And with this, you can create Resumable object, let user add files, set file specific options and when your tests are finished you will add target, simultaneousUploads etc. \nI am not saying this example couldn't be done before, but only query is able to have function parameter, you couldn't upload to multiple targets with one object etc. etc.\nWith this modifiction you do not need building any extra code, it is nice way and easy to read.\nThis is backward compatible change, so you can solve query option with passed function if you wish.\nIn my pull request there is new sample, showing how easy it is.\nI made pull request but didn't bind it to this issue, sorry about that.  https://github.com/23/resumable.js/pull/50\n. @srijs \nGood idea. I like this more general way. And agree that some options makes sence for chunk.\nThere are two things I am unclear about.\n- getOpt would be implemented as a helper function, traversing hiearchy itself. Or each object would have implemented its own getOpt metod, traversing hiearchy by calling getOpt of parent?\n  - First variant would be easier to maintain the second more self descriptive.\n- Right now the only way to set options for chunk I see is listening for fileadded event. Then from fileobject passed to callback figure out what options should be set to which chunk. I find this way unfortunate, beause it requires additional code, which I would almost for sure use in every project with Resumable. \n  - Right now, the only solution i can think of is passing chunkOpts array to file object and when chunks are being creted set chunk[i].opts = chunkOpts[i]; (ofcourse, the first way would be still posible)\n. $h was defined in global scope, in one my experiment it overridden my actually intended global variable. \n. This is really better than hard saving options to file object. For example, you can create resumable, let user add files (and customize options for each) and in the mean time check nearest upload server and set target then.\n. $.opts are accessible directly, but just passing object is more elegant. And there may be added some validation (for setFileOptions  too). \ne.g. simultaneousUploads must be a number > 0, if not use default. \n. ",
    "guillemsalas": "Hi,\nI think this is a not-really-needed feature since, from documentation:\n\n- query: Extra parameters to include in the multipart POST with data.\n  This can be an object or a function. If a function, it will be passed a\n  ResumableFile object (Default: {});\nSo it's not hard to implement a external object with appropiate functions\n(I usually simply use a plain js object) using the\nresumableUniqueIdentifier as index to get done the job done.\n2013/3/3 jinora notifications@github.com\n\nSure, I would gladly attempt an implementation.\nI think setFileOptions is better because it simply suggest getFileOptions\nin case somebody need to retrieve current options, which could be helpful\nin large apps.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/23/resumable.js/issues/45#issuecomment-14354205\n.\n. @jinora \n\nIn my last comment I didn't see the real point of this issue. It can be a very useful feature for a lot of situations, so please go on!\nI agree with @srijs that also ResumableChunk should have its own options, so you can adapt resumablejs to any kind of deployment enviroment.\n. ",
    "srijs": "I'm sorry to bring in my opinion rather late in the process, but I have a few remarks/suggestions regarding that feature.\nAs I see it, we have the chance to implement this in a more general way so that you can set options with custom granularity. It could work a little bit like this:\n- There currently exists an obvious hierarchy between classes, namely: Resumable (R) -> ResumableFile (RF) -> ResumableChunk (RC)\n- Each of these classes could have the possibility to contain options and implement a getOpt method.\n- This method walks up the chain RC->RF->R->defaults or RF->R->defaults or R->defaults, depending from which object in the hierarchy it was called, executing functions and appending/merging arrays/objects along the way.\nThe result of this would be that you could set options at any level (Global, File or Chunk) depending on the intended scope of the setting. I would assume this wouldn't add significantly more complexity (actually, removing a bit) from the current state of the implementation.\nWhat do you think about that?\n. I'd implement getOpt as a helper function to avoid redundancy, but bind it to the three objects individually to make for a nicer interface. I'll see if I can do a rough implementation of my idea later this week...\nRight now I'm unsure about the suggestion regarding the way to set chunk options. But I think the moment one cares about fine-tuning options for individual chunks it is justifiable to require a bit more code for that...\n. Hi all!\nI've finished my draft implementation.\nPlease have a look at it and tell me your thoughts!\nCheers -- http://github.com/srijs/resumable.js/compare/feature-getopt\n. I find this behavior highly confusing, too. Could you elaborate in which way this would have to be configured server-side?\nWouldn't this be solvable by:\na) Really just triggering this when prioritizeFirstAndLastChunk is set and document this along with the option.\nb) Add a lastChunkSize option with a sane default to make this behavior more explicit.\nc) When prioritizeFirstAndLastChunk is set and the last chunk is smaller than max, upload the two last chunks. This would allow for analysis while the server would still be able to store this in a chunk-sized storage system without breaking the chunks apart himself.\nI personally would opt for b) or c), but I'm sure there are many more possibilities to solve this.\nIn the current state this really can be confusing and also unnecessarily complicate some use cases when this is not configurable.\nSo let's please discuss this.\n. I understand you don't want to break existing server-side implementations, could we add an opt-in to make this more sane (e.g forceChunkSize)?\n. The issue we are having specifically are a server-side enforcement of the maximum chunk size along with a chunk-based storage system with a fixed block size. Server-side chunk splitting or usage of a greater block size are unfortunately not possible.\nAlongside with that -- more ideologically -- you have to admit the current behavior is rather not what one would expect when explicitly setting a chunk size, don't you?\n. I don't understand. How would e.g. adding a forceChunkSize option with default false be a breaking change?\nAnd yes -- better documentation about that should be done anyway.\n. When aiming for a logical future interface, I still think we should send the last chunk >=chunkSize only when prioritizeFirstAndLastChunk is set. It's hard to imagine an existing server-side implementation would assume the last chunk to be >=chunkSize except when using the prioritizeFirstAndLastChunk. Yes, this would theoretically be a breaking change, but I expect the percentage of implementations it would possibly break to be very low.\n. See https://github.com/23/resumable.js/pull/56\n. This is likely due to the fact that the send method uses the configured query object and inserts properties right into it:\nhttps://github.com/23/resumable.js/blob/master/resumable.js#L364\nThe next call to the test-method then pushes all of these (now contained in the query object) onto the params array and then adds its own (duplicate) properties again:\nhttps://github.com/23/resumable.js/blob/master/resumable.js#L290\nThe fix is to use a copy of the query-object instead of the original in the send method.\nI have to admit this may be my fault ;)\n. This kind of fixes the issue, but unfortunately in a still-not-very-stable way. Proper fix would be (additional to your changes) to create var query = {} in the first place, then do var extra = (typeof $.resumableObj.opts.query... and then loop over extra and copy its contents into query.\n. A few thoughts about your problem:\n1. resumable.js uploads the chunks in a kind-of first-to-last order, so the whole issue is not that bad.\n2. It is theoretically impossible to guarantee an absolute first-to-last order with parallel uploading.\n3. Amazon S3 does exactly the same thing with their multi-part upload and it seems this is not too bad performance-wise.\nTo avoid the performance-overhead of concatenating all the received chunks, you have some possibilities:\n1. Make the chunk-size a multiple of the server-side file system block size (which it should be in most cases, already). That way, the file system does not have to concatenate anything, just combine the correct i-nodes in order to combine the received chunks (of course, this doesn't work with a simple cat, but with special fs utilities).\n2. Allocate a large-enough file upfront to which you write the received chunks in the right places. resumable.js sends you the total file size and the chunk's number with each chunk, so no matter which chunk comes in first, you'll be able to allocate the file-size upfront.\n. ",
    "Roisholto": "What I did in my case In  order to change parameters on the fly was to do something like this, \nR=resumable ({}) \nThen R. opts['query'] = {name=\"somevalue\"} \nR. upload() \nI believe other Params can be manipulated before calling the upload method. ",
    "tpodom": "Thanks!  You guys are awesome, so far in testing Resumable is amazing.  Our flash upload is so unreliable, this is going to be so much better.\n. Sorry about that. I could have merged with the latest master for you.\nThanks though!\nOn Aug 10, 2013, at 7:00 AM, Steffen Tiedemann Christensen \nnotifications@github.com wrote:\nSorry: I couldn't auto-merge this issue so it took me way too long to\nhandle it manually.\nAnyways, thank you so much for the contribution.\n\u2014\nReply to this email directly or view it on\nGitHubhttps://github.com/23/resumable.js/pull/61#issuecomment-22437959\n.\n. I know this is a tad late but the retries are configurable with two config options that I failed to document in the PR I did a while back.  I'll submit a PR with updated README but they are:\n- maxChunkRetries - maximum number of retry attempts before treading the upload as a failure\n- chunkRetryInterval - number of milliseconds to wait before retrying a chunk\n. It was actually renamed to ResumableFile.name\n. ",
    "eway0815": "I don't know how to pass an object to Resumble object, could you give a sample?\n. I get binary file by html File System API, how Resumable object deal with the binary file?\n. ",
    "sreuter": "Guessed so and truly understand, as we're also handling video uploads / encoding for our service. Still, I think this should be more configurable. I don't think that I need 8MB of the file end for the proper (MOOV/id3 tag) detection. Also, this routine should only be enforced when prioritizeFirstAndLastChunk is set to true and should be well documented, don't you think?\n. @davekiss Hey buddy - unfortunately, It's kinda hard to follow the changes because of the additional chore being done. Can you do a quick check if https://github.com/23/resumable.js/pull/232 and https://github.com/23/resumable.js/pull/233 does the job for what you wanted? Thanks!\n. @jmonma Quick heads up that this should be superseded by https://github.com/23/resumable.js/pull/233 .. Can you confirm?\n. @axschech @donut Just a quick heads up. Everything discussed here should be addressed by now. See https://github.com/23/resumable.js/pull/231 and https://github.com/23/resumable.js/pull/233 ..\n. While I still think the new behaviour is more \"correct\" and endusers shouldn't look into developer console, I don't have a strong opinion on this. As long as 201 is still considered a positive response, I'm good.\n. @steffentchr As this is still a change in behaviour, can you keep that in mind when choosing the next release/version number?\n. Gotcha, thanks for reaching out!\n. @vsivsi Sweet - Glad to hear it's valuable!\n. Just to be clear, the side-effects were made intentionally. That's why I proposed to use a version number that makes this \"change of behaviour\" clear. As far as I know, the merged changes have not being part of any official release yet. I'm still fine with this being merged, even tho I think it's a bad idea ;)\n. @vsivsi @steffentchr Thanks guys. I wasn't aware of issue #127. Glad it has been sorted out :)\n. ",
    "talha-asad": "Impressive work, however a very bad default behaviour regarding last chunkSize in my opinion. Perhaps slowly we can fix this. :)\n. I do understand your use case, but this library is standalone and it seems unreasonable to have that behaviour as its default. I think the general idea is vice versa.\n. ",
    "pacificsoftdevld": "Hi all. \nin r.on('fileAdded', (file, event) => {});\nI want get content to create md5 hash and add it into query of  this file.\nPlease help me get content file.. I have a sane problem likw @henno.\n@cpnielsen can you show me an example to solve this problem?\nThanks!!!\n. Thanks for your support.\nIt works well. :+1: . ",
    "cpnielsen": "@pacificsoftdevld Please don't hijack other issues with unrelated comments (I can see you also created an issue, which is the right approach).. @piotrpawlaczek Although it makes sense, it makes for a very \"unclean\" pull request. Is it possible you could rebase against the current version and undo the indent/white space cleanup? If so, we can do a separate version where we do a clean up to avoid confusion between lines changes.\nThanks for your input :+1: \n. Hi @szelcsanyi \nSorry for the late answer. Though the example is working for an \"optimal\" situation, there are a number of concerns:\n- You replace missing resumable* field values with \"error\" - it should short-circuit and simply return an error if the variables are missing (especially since this is a resumable-only example).\n- Chunks are not guaranteed to be in order, which means that you are not guaranteed to have all available chunks when the last (resumableChunkNumber) chunk arrives. Instead, you should check if all chunks have been correctly written and are all available, pretty much on every chunk.\nI am aware that a lot of these might be out of scope for a simple example, and I would be happy to merge this if you add warnings / comment on the drawbacks of your method.\n. Could you try and upload a text file and set the chunk size very low (something like 512 bytes) and see if there are any obvious breaks, line breaks inserted, etc.?\nThe library should transmit raw bytes, so it may be an error when re-assembling the pieces.\n. It might be that you are expecting the last chunk to be the same size as the others (which is most often not the case).\nAre you using any of the sample backends or your own code? Is it possible you can share the \"file-reassembly\" code?\n. The following line is (potentially) wrong:\nif ($total_files * $chunkSize >=  ($totalSize - $chunkSize + 1)) {\nDepending on whether forceChunkSize is true or false, the last chunk size will either be in the range of 1 - $chunkSize or $chunkSize - (2*$chunksize-1). This could potentially mean you assemble the file before you have received the last chunk.\nA better way would be to just use the resumableTotalChunks and compare that to the number of files OR to tally the file sizes of all the chunks and compare it exactly against the resumableTotalSize. There is no reason to be \"fuzzy\" when it comes to determining whether the upload has finished.\nIn cases where the files are corrupted, could you check the log for the writing chunk X statements and see if the number of chunks match the actual total?\n. Hi ChadTaljaardt \nI am not seeing any open pull requests from you - can you link the one have created? We'll be happy to merge any contributions :+1:\n. Fork this repository, create a new branch with the changes and open a pull request from that - see https://help.github.com/articles/using-pull-requests/.\nThe changes looks good, I'll be happy to test and merge when it's set up properly via GitHub.\n. Looking good, just a bit of cleanup. Could you remove this (line 90-92):\nif (stripos($file, $fileName) !== false) {\n        $total_files++;\n    }\nIt's not used with the new check and is just clutter. Also the indentation on this line is one tab too far (line 96):\nif ($total_files_on_server_size >= $totalSize) {\nOnce that is fixed, I'll merge it. FYI: Just commit the new changes and they will automatically be included in this pull request.\n. Looks good, merged!\n. Hi @NewPlayer2 \nCould you provide us with a set of sample code for how you are using resumable.js so we can debug this further?\n. It seems fairly odd. Are you using the latest version of resumable? The relevant lines are here: https://github.com/23/resumable.js/blob/master/resumable.js#L179-L183 and as you can see we check to see if the target is a function and then pass the params along to that.\nThe URL it requests seems to indicate that the target passed is a string that basically says: function () {\"use strict\";return 'backend.php';}\nIs it possible you can give us the actual script you use, then I can test and debug locally.\n. Basically there are two methods for uploading:\n- octet which creates a list of parameters it sends to $h.getTarget() and sends the actual file data as the request POST body.\n- multipart which packs all the parameters + file data in the POST body, and just sends it to the \"raw\" target parameter.\nWe have previously made the assumption (prior to supporting a target function) that the target parameter would be the endpoint and the above method would then define whether metadata was sent as query parameters or as part of the POST body.\nIn your example, it will most likely work if you pass method: 'octet' as part of the options when you instantiate Resumable, ie.:\nvar getUploadTarget = function () {return 'backend.php';}\nvar r = new Resumable({\n    target: getUploadTarget,\n    method: 'octet'\n});\nAt this moment, there is no way to support a target function as well as sending the parameters in the POST body. Our assumption (which may be faulty) has been that the target function is a way to create the target URL from all the parameters, hence it would be irrelevant to send them along in the POST body as well. \n. The things is, we use it as-is now, it would send the parameters both in the URL and in the POST body for POST requests if target is not a function, so some kind of rewrite is necessary.\nIf you need to pass along some extra token, you could make use of the headers parameter, which also takes a function that returns a key-value object that inserts it into the headers of the AJAX call. It would still require some minor changes to your server-side code, but you would get around the whole target issue.\nA potential change that could fix this, is to pass along the method to get the getTarget method and use that to determine whether to return the non-function URL with or without query parameters.\n. @aarpon Are you using the latest version of resumable.js (from this repo)? And can you post the PHP code you are using (pastebin, gist, or similar with a link here)?\nThis could happen, if for some reason the server was not sending back the correct responses to let resumable know that a chunk already exists. We have never internally experienced an error like you say, so I am a bit puzzled.. @anniefzh \n\n\nsimultaneousUploads is how many chunks are being uploaded at the time, resumable does not think in terms of files per se.\n\n\nResumable will happily work on multiple files at a time, if they are added to the same instance. If simultaneousUploads is 3 (default), you might have the last chunk of file1 being uploaded simultaneous with the first two chunks for file2 - if you want them \"separated\", you should create multiple resumable instances.\n\n\nA quick look at the PHP code seems to show it's working correctly (I haven't tested it). However, when it merges chunks into the final file, it will append to the file if it already exists. This means, if you upload an identical file twice (same identifier, same size), it will create the final file when the first upload finishes and then append to that file for each consecutive upload (instead of overwriting it). This also makes sense if you say that the file size is an integer multiplier of the expected size (x2, x3, etc.)\nCode in question: https://github.com/dilab/resumable.php/blob/master/src/Resumable.php#L277-L280\nIt creates a file if it does not exist (but does not delete or truncate it first if it does) and then appends every chunk to the file.. With the default settings, it is likely to improve speeds when set to 2-5, but more than that will probably give diminishing returns (all the chunks are sharing the bandwidth after all).\nIf you are using Chrome for upload, try setting simultaneousUploads to 3, open the Network tab, start the upload, log everything and save it as a HAR file, share it here and I can take a quick look at it. It might also be that you can see what is happening from your side.. First off, did you assign the button to call your resumableobject.cancel()?\nThe cancel() method does not actually block any workflow from finishing, but it does cancel any ongoing file upload. If you click cancel right as the last chunk finishes uploading, the complete() callback will still fire.\nIf you have some sample code I'd be happy to help you verify / debug.\n. The node.js sample is fairly old (dating back to 0.10), so I am not entirely surprised it crashes. Could you provide the crash dump and/or errors?\n. I installed a fresh install and experienced an error when uploading. It has been fixed in the latest commit and should be working as expected now (check updated README.md / app.js for instructions on enabling CORS).\n. @ps613 Could you make the change against the latest version of resumable.js and fix the line I commented on? Then we should be good to go :). @tesujimath (and others) We have been granted access to the repository, so you can expect a new release within the new few weeks.\nCurrently, we have to decide what to officially bump the version to (based on changes), and then get it packaged correctly and shipped off. I know this makes management of the package much easier, especially with the latest changes. Now that it is possible it's definitely something we are going to push.. @henno You can easily add 40 files at once, or add one at a time to the same Resumable instance, and it will start from the first file and continue to the last one until finished.\nPlease refer to the documentation and discussion on this and other issues, and refrain from commenting on old (closed) issues.. To both of you, there is no obstacles to overcome - as long as you don't have maxFiles set (defaults to un-set), you simply add all the files (drag-and-drop, choose via file open dialog, etc.) and the files will be transferred one after the other, with simultaneousUploads number of chunks (not files) being uploaded at a time (see steffentchr's comment earlier).\nIf you want to identify all the files as belonging to \"one\" session, you can generate a unique ID and pass it along to the query parameter when creating the Resumable.js instance, and it will be sent along with all chunks for all the files. The backend then needs to be able to recognize them and \"group\" them together again, but that is not something resumable.js can solve for you.. Hi Daniel\nWhile it's true that it's \"inefficient\" to check all chunks every time, we cannot guarantee the order of chunks in resumable.js - it's quite likely that the \"end chunk\" is not the last to get uploaded. A better solution is probably to have a server-side cache/counter adds each chunk as it is completed.\nSay a file with 4 chunks (1, 2, 3, 4) is sent. Because of glitchy internet (or perhaps a switch from 3G to WiFi), chunks are sent 1, 2, 3, 4 but are received 2, 1, 4, 3. Suddenly chunk 3 is the \"final\" chunk, but the server will assume that all chunks have been sent when chunk 4 is received and an incomplete file will be assembled.\nCreating an efficient server-side check is beyond the scope of this library, but that should be the focus instead of assuming the chunk order.. The code currently skips files that does not have the expected file extension, as seen here: https://github.com/23/resumable.js/blob/master/resumable.js#L364-L367\nAs such, it does work as a file extension mask. Make sure you pass an array of file extensions (without the dot), such as [\"pdf\",\"gif\",\"jpg\", \"jpeg\"].\nThe default implementation of fileTypeErrorCallback simply outputs a warning that the file is not valid, which you can overwrite with your own implementation (to give visual feedback, cancel the upload, etc.). Hi! I can't seem to find the documentation for your specific endpoint, but I suggest you use one of the provided (third-party) javascript libraries for Dropbox found here: https://www.dropbox.com/developers-v1/core/sdks/other.\nWe cannot offer support for third-party APIs, only for the library itself.. @vladejs There is an official, updated (although work-in-progress) SDK from Dropbox for v2 here: https://www.dropbox.com/developers/documentation/javascript#overview\nThe documentation for Dropbox also specifically says not to use /files/upload for files over 150 MB (see https://www.dropbox.com/developers/documentation/http/documentation#files-upload), but to use /files/upload_session/start instead.\nA quick read of the documentation indicates that their chunk-based upload is akin to Resumable.js, but simpler and you would have to modify what data is sent with each chunk (session_id + offset calculation). That is left as an exercise for the reader.. I am afraid resumable.js is not the right tool for this job without some rewrites - by the looks of it, Dropbox seems to expect sequential uploads (Resumable.js performs uploads as fast as possible and does not care about order) and only one at a time. Resumable.js can help you chunk the files, but the upload mechanism and chunk-checking mechanism would have to be completely rewritten.\nThe Dropbox SDK does have a javascript upload example (in the example folder) showcasing how to use it, and it looks fairly straightforward. I am sorry we cannot help you any further.. You should be able to turn your blob into a File-object (in modern browsers):\nvar fileOfBlob = new File([blob], 'aFileName.whatever');\nOnce that is done and we assume you have initiated resumable into the variable res, you can add it directly as a file list:\nres.loadFiles([fileOfBlob])\nI haven't tested this, but something like this should work. Ofcourse, if you have multiple blobs you can add them simultaneously (loadFiles takes a list of File objects).. @allansli I think you might be right, it was me being a bit too fast.\n@afukada There is an addFile function (https://github.com/23/resumable.js/blob/master/resumable.js#L1019-L1021), that you can use instead. It takes a single file at a time, but otherwise works the same.. The simultaneousUploads refers to how many parallel uploads are happening, but in terms of chunks, not files. The default, 3, means we will attempt to upload 3 chunks at a time, but all chunks from a single file will be uploaded before the next file is attempted.\nIf you want to do 4 files at a time, the easiest way is to start create 4 resumable instances and upload a file with each.. Does the network requests contain data and the right number of chunks?. We do not provide support on any backend/receiving code, only the frontend library, as it can any of a 1000 constellations, code, server setup, etc.. Can you verify, in a browser or via network sniffer (WireShark, Fiddler, etc.) that the requests contain no data?. @vaskaloidis Again, I cannot tell why the data is not reaching your backend and I cannot help you debug that part. Did you update to a new version of Resumable.js that caused this change? Otherwise it sounds like it's an issue with your server setup.. @publicocean0 I am unsure what your issue is, can you clarify?\nAre you receiving chunks from Resumable via web socket or are you receiving chunks from elsewhere via web socket and wish to pass them on to Resumable? In the latter case, I am afraid we do not support chunk-by-chunk uploads, as we need the entire file to determine file size, unique identifier and more.\nYou can rewrite the library and/or take out essential functions but it is outside of the scope of what we can support.. @shijiantian This is purely a backend issue and we cannot help you debug that. My suggestion is to split up the operations (create a new File, check if it is succesful, then try renaming, check again).. @shijiantian HTTP is stateless, so pressing pause during a chunk upload will cause the upload to stop, and resuming will cause a NEW chunk upload to occur.\nIf you never resume, nothing further happens.. In the fileAdded event function, you can access the file object itself - the file parameter is an instance of ResumableFile:\nr.on('fileAdded', function(file, event){\n    var fileObject = file.file;\n    var md5sum = <do something here>;\n});\nThere is not a browser-native MD5 function (that I know of), but you can use something like SparkMD5 to calculate it - check their documentation.\nIf you want to calculate / pass the MD5 value along with the request to the server for all chunks, you can also implement the MD5 calculation (and cache it) in a function you pass in as the query option when initiating Resumable:\n```\nfunction md5Query(file, chunk) {\n    \n}\nvar r = new Resumable({query: md5Query });\n```. @tesujimath Good find. We'll be happy to accept a PR for the homepage / README. If you could expand on what browsers are affected (and what the underlying cause might be) that would be great :+1:. Looks good, merging!. Hi @BenVS \nWe generally do not support server-side issues, since we cannot help debug and it is outside the scope of our codebase. Are you using any of the code from our examples?\nAs far as I can tell with a bit of googling, ASP.NET will not recognize individual chunks as files when posted through an AJAX request (which is what Resumable.js makes use of). I assume the data will be available in Request.Form along with the rest of the sent parameters (under \"file\" if the defaults are used).. Simple, straightforward, looks good \ud83d\udc4d  Thanks @zzarcon!. @majames We are looking to make a general resumable.js release this week (hopefully), likely bumping the version number to 1.1.0, but I cannot say exactly when this will happen.. Hey. I assume you have included a Promise return because we support custom functions? If not, please enlighten me.. You are completely right. Merging \ud83d\udc4d . @nickdatum @tawfiqAlbatsh Any chance I can get any of you to test it out to see how processItem() registers the incoming items and what it does (breakpoints + inspection)?. I am a bit strapped for time, so it will be a while before I can set up a test case. What I meant in my previous test is whether you can do some quick inspection (running with a test setup) to see what happens to each directory, if we accidentally overwrite a variable or Chrome changed the File API to return a promise somewhere (they did something with video.play() recently as well).. @jreijn Thanks. Take note that the samples are provided as-is (and not regularly updated), as this is is a purely client-side library, and the examples only serve to illustrate possible (naive) server-side implementations.. @jreijn I hope resumable.js can help you out. Implementing your own server-side part is fairly straightforward, but will vary depending on whatever underlying caching / file system is facilitating the uploads.. @dgeswein-dlx The java example is not to be seen as best practice nor even a good example - it is included for \"completeness\" and community-provided.\nIn the specific case you are right that a simple integer would be faster, as intereger-comparison is not identity-based (like strings are) and hence there's no need for a special compare function.. ",
    "davidchase": "@garjitech was that pull request ever submitted ? really interested in seeing how resumable.js works with s3\n. @steffentchr thank you for the quick reply :) , if i may ask did you notice significant performance gains when using multi chunk uploads with web workers?\n. @steffentchr That blog post is definitely a good starting point, but I was hoping for something more in the lines of implementing web workers with your library or something along those lines... thank you\n. @steffentchr Thanks for the quick reply, the project we are currently working to upload videos lets say we have a 100 chunks and 1-49 uploaded fine but chunk 50 was bad on the server side so it tells the front-end hold up re-upload the bad chunk and then if all is well continue on with 51-100... im trying to figure how can we accomplish that method... hopefully it makes sense\n. @steffentchr couldnt this concept be attained perhaps by getting how many bytes have been already uploaded and then using that as a starting point to re-upload from that chunk instead the whole file?\nSo the server sends an error message and file size, we capture the event on a chunk fail and using the number of bytes already uploaded resume only partial uploading of the file... granted the file is already uploaded\n. @steffentchr We are trying to create if you will a reliable uploader by allowing the server-side control what is done with the chunks and then if it deems necessary send back a JSON error to front-end saying waiting a minute theres a problem with one of these chunks go back and re-upload it from that chunk on... maybe the client-side uploaded more chunks than the server could proof, or the md5 check didnt match, etc\n. @steffentchr Can you give me and example with the checksum and re-upload?\nIn my understanding you only want to upload the chunk that is at fault so stopping the uploading going back to the faulty chunk re-uploading it then continuing, in essence using the uploaded bytes would allow \"resumablity\" on aborted uploads since you could send a GET request to see if the file existed and then respond with a file data which could say how much of the file  was uploaded(uploaded bytes) before an error occured... \nI guess thats another reason for chunk uploads instead of full files.\n. ",
    "roundrobin": "@garjitech Any update on the S3 pull request?\n. ",
    "edu2004eu": "@roundrobin looks like we got to the same place :) Resumable.js looks good, but without S3 support it's not right for us.\n. ",
    "neilchaudhuri": "Well, I don't think anyone is advocating changing Resumable to work specifically with S3. Rather, I think people are looking for samples of using Resumable with S3--uploading chunks according to S3 specs, generating the signatures without a serverside component, etc. As you say, there is great potential.\n. ",
    "devotox": "Alright that's no problem at all. Yeah i figured that it may be a problem when i saw the diff from Github i had changed it on sublime when i started using it. ok will get to that and update it tomorrow morning. \nOn 18 Mar 2013, at 17:23, Steffen Tiedemann Christensen wrote:\n\nLooks great, and all extremely worth-while improvements.\nNot to be too pedantic though: Would you be able to contribute a pull request without massive changes to the indentation? (In its current form 0fdd405 would touch modify every line in the main script, making it harder for people to merge and maintain their forks and modifications.)\n\u2014\nReply to this email directly or view it on GitHub.\n. Just looked through the code and yh what you did is a lot better only problem is that you have created a format size function but you still add bytes after the size so bytes will be written twice\n\nOn 25 Mar 2013, at 19:40, Steffen Tiedemann Christensen wrote:\n\nJust added both features in commits 0976a38 and 1d90523:\nThe event uploadStart fires just before uploading begins.\nminFileSize and maxFileSize are now supported settings. They have individual callbacks (ugly) for error handling, and file sizes in error messages are nicely formated. I took a slightly different approach to limiting error alerts, so all error callbacks now receive an errorCount argument, which can be used by implementors to limit prompts.\n\u2014\nReply to this email directly or view it on GitHub.\n. definitely was going to fix it and send a pull request but it seemed useless to do that and i would rather just email ya anyway :)\n\nOn 25 Mar 2013, at 20:45, Steffen Tiedemann Christensen wrote:\n\nAh yes, that's an easy fix though...\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "Guthur": "I can confirm that what srijs mentions seems to be the issue, if i remove the following line and the map function below it there is then no issue.\nhttps://github.com/23/resumable.js/blob/master/resumable.js#L290\n. ",
    "frank-fan": "@srijs sounds good. \nThanks.\n. I just move coffeescript to samples/coffeescript. \nBtw, I just use resumable.js in my own project. It is amazing!  In a wifi network, it can speed up 200kb/s - 300kb/s for uploading.\nNext time, I will push my server code in \"Java Servlet\" to samples. I use RandomAccessFile to save file in server side, avoiding to merge chunks. \n. It is me again. I just finished Java DEMO.\n. ",
    "faller": "thanks @steffentchr , this help me a lot\n\"Finally, a number of reverse proxies (nginx, pound from my own experience) do not offer steaming uploads and instead buffers the entire request before forwarding it (again, a good motivation for node). In those cases, you'll have full data immediately upon request -- and the big file + seek approach is probably good.\"\n. ",
    "thenewguy": "@steffentchr I read about the permanentErrors option and it appears simple enough to allow 500 errors always.  But what hooks would I use to allow X sequential 500 errors and stop trying on the (X+1)th?\n. @steffentchr I agree with your point.  I just thought that you meant it was already possible but I couldn't figure out where I could hook in to do it.\n. I found https://github.com/23/resumable.js/issues/135\nI haven't put together the workaround listed in the ticket but it seems like the method presented doesn't block when the chunk hash isn't computed does it?\nIs there an interface available to block the upload until the hash is computed for a particular chunk?\n. I also came across https://github.com/flowjs/flow.js from reading the comments in #135 and I guess that is why they added the preprocess callback function.\n. ",
    "guilhermewop": "Thanks, merge so fast :open_mouth:\n. ",
    "danimt": "This feature isn't working for me.\njs\nvar uploader = new Resumable({\n  fileType: ['zip', 'rar', 'tar', '7z']\n});\nWhen dragging/selecting a file it always returns the invalid alert.\n\nI've modified the code to make it work. https://github.com/23/resumable.js/pull/110\n. May I ask if your library includes the latest code from the original Resumable.js? Or if you forked it a while ago and have been improving it on your own.\n. ",
    "edmellum": "My main goal with the patch was to get it working with Component, the Node.js thing was mostly an interesting side-effect. I can't really figure out a good use-case for Resumable in Node, except if someone wanted to make it platform independent and basically turn it into a resumable upload library working both on the server and browser. The issue with that approach of course is the cruft that you get when trying to support both. Component forgoes that utopia and instead focuses on making tight, small, browser-focused components.\n. ",
    "hatsch": "thanks for merging! \nbtw: i love this library! having resumable uploads is fantastic!\n. ",
    "gprasant": "No problem. Was trying out the in-browser git flow\n. ",
    "AidasK": "Hi, \ni have prepared a sample implementation at https://github.com/AidasK/resumable.js/compare/23:master...master\nResumableFile now is going to have two additional params: \n- $.currentSpeed - current file upload speed based on \"measureSpeedInterval\" parameter\n- $.averageSpeed - this is implemented based on formula: averageSpeed = SMOOTHING_FACTOR * lastSpeed + (1-SMOOTHING_FACTOR) * averageSpeed;  http://stackoverflow.com/questions/2779600/how-to-estimate-download-time-remaining-accurately\nI have also added two params: measureSpeedInterval and speedSmoothingFactor. I think they need a better naming.\n. Hey, have you seen my implementation? I would like to finish this feature and move on to new ideas.\n. Hi, thanks for using this library.\nYou got this issue because you used private library variable $ which is not exposed publicly.\nTo fix this error you should assign resumable instance to a variable and try to call this method.\njavascript\nvar resumable =new Resumable({\n    maxFilesErrorCallback: function (files, errorCount) {\n       var maxFiles = resumable.getOpt('maxFiles'); \n       alert('Please upload ' + maxFiles + ' file' + (maxFiles === 1 ? '' : 's') + ' at a time.');\n    },\n});\nHope this helps\n. I think there will be much more breaking changes and this is not an issue. This fix could be made in a new version of resumable.js.\n. same as https://github.com/23/resumable.js/issues/80\n. You can:\n```\n$ bower search resumable\nSearch results:\nresumablejs git://github.com/23/resumable.js.git\nng-resumable git://github.com/AidasK/ng-resumable.git\nresumable.js git://github.com/resumable2/resumable.js.git\nmaybe-resumable.js git://github.com/resumable2/maybe-resumable.js.git\nmaybe-resumable-js git://github.com/AidasK/maybe-resumable.js\n\n```\nuse \"resumablejs\" to install from https://github.com/23/resumable.js\nor  \"resumable.js\" for development version from http://github.com/resumable2/resumable.js.git\n. I have implemented this function on my library: https://github.com/resumable2/resumable.js\nLibrary has timeRemaining method https://github.com/resumable2/resumable.js/blob/master/src/resumable.js#L877\naverageSpeed and currentSpeed properties https://github.com/resumable2/resumable.js/blob/master/src/resumable.js#L633\nAlso look at sizeUploaded method. https://github.com/resumable2/resumable.js/blob/master/src/resumable.js#L859\n. yes it does, for more information read changelog https://github.com/resumable2/resumable.js/blob/master/CHANGELOG.md\n. I have wrote library for php for concating chunks https://github.com/resumable2/resumable.js-php-server/blob/master/src/Resumable/File.php#L148.\nThe process should be similar in node.js.\nJust iterate through all of them in order and write to a destination file.\nIn ssh we could use command cat chunk1 chunk2 > file\n. Can you share your node.js implementation?\n. > chunkSize The size in bytes of each uploaded chunk of data. The last uploaded chunk will be at least this size and up to two the size, see Issue #51 for details and reasons. (Default: 1_1024_1024)\nIn your configuration chunk can be up to 10mb of size.\n. I have implemented image preview on a angular framework extension:\nhttps://github.com/flowjs/ng-flow/blob/master/src/directives/img.js\nWhat you basically need is ResumableFile( or in my case FlowFile) instance and it has file property. Use it in the file reader.\njavascript\nvar file = resumable.files[0];\nvar fileReader = new FileReader();\nfileReader.readAsDataURL(file);\nfileReader.onload = function (event) {\n           attrs.$set('src', event.target.result); //set image element src property\n };\nWorking example at the bottom (Image example): http://flowjs.github.io/ng-flow/\n. You are probably looking for https://github.com/flowjs/flow.js, which was renamed from resumable2js to flowjs.\n. ```\n$ bower search resumable\nSearch results:\nresumablejs git://github.com/23/resumable.js.git\nresumable.js git://github.com/resumable2/resumable.js.git\nmaybe-resumable.js git://github.com/resumable2/maybe-resumable.js.git\nng-resumable git://github.com/resumable2/ng-resumable.git\nResumable.js git://github.com/23/resumable.js.git\n\n```\nInstall resumablejs or Resumable.js and write a complaint to bower to fix it.\n. Yes it is, will be fixed in next commit. Thanks.\n. ",
    "PunKeel": "Hi, [up]\nI think it could be cool to store last uploaded byte in a variable, so that chunkSize can be modified while upload (so, if someone uploads from a mobile device, and gets 4G ... It can adjust speed and not do useful requests)\n. I thought of some way to have the chunk size increased if it took <1 second to upload, else decrease it. But you're right, there's no way to know for the first chunk  (worse) when upload speed is lowered, resumable's main feature \"fails\", as it can't be really paused.\nAs for uploading multiple chunks at one, I were not aware and don't know if it's a good point (and I supposed it to be ran linearly, appending chunk to a file ..)\n. ",
    "bertsinnema": "You could force the first chunks to be small and have Resumable upload them non-simultaneously. when speed is high increase the chunks to what is configuredm switch to the simoultaneous number  of uploads that is configured. When the chunks need to shrink, just first have the uploader fall back to 1 upload, wait for all other chunks to complete and then decreas the chunksize.   the only tricky part is to identify the order of the chunks in the backend because you might end up slicing chunks that persist somewhere in the middle of a file.\n. If you are going to have a lot of mobile users I would probably just set the chunksize to 512k and have the number of simultaneous uploads shrink when upload speeds are low. \n\nOn 23 sep. 2014, at 16:53, Steffen Tiedemann Christensen notifications@github.com wrote:\nYup -- absolutely doable, although it would require a bit of extra complexity on the server side to stitch together chunks. The real way of doing it would be do simply send the start and end bytes I gather and then have the server stream bytes to a fully assigned file.\nThe question remain though if it's worth the bother -- is the real life use cases justify the change?\n\u2014\nReply to this email directly or view it on GitHub.\n. No problem, i just fell in love with Resumable.js :-)\n. Why exactly would you need that functionality?. \n",
    "AnkurThakur": "Ooops... My bad... Thanks a lot.. It worked... :)\n. ",
    "jamescarlos": "I added a getTarget helper method to normalize the target url and query parameters.\n. ",
    "tobiastom": "I understand your point. But if you make it an optional thing (which is disabled by default) and someone uses it, he would be aware of the server implications. \n. Excellent. Thank you.\n. After looking into the code, I do not think this is necessary at all. I can just put the URL of the image into the response for the last chunk. The event message parameter will then contain my URL, which it got from xhr.responseText.\nIf you don't think this feature is needed anyway, it would be fine with me if you close this issue.\n. It's @Bantam, just for highlights sake. :)\n. I can confirm that it works fine after this fix.\n. ",
    "Walfou": "Can you explain more what should be done to instantiate the module with the folder?. ",
    "winya": "Fine, i create a new pull request.\n. ",
    "binarykitchen": "Wicked. Thanks so much for the super fast response. You probably want to mention that somewhere in the README.md?\n. Ummm, also, there is no version of ResumableJS in bower. Can you add it?\n. Hello?\n. Alright, here is a PR\nhttps://github.com/23/resumable.js/pull/139\n. Thanks! But it looks like you haven't registered the package with bower yet ... bower ls still shows no version number.\n. I am afraid, the version number is still not correct.\n$ bower ls\nbower check-new     Checking for new versions of the project dependencies..\nsigndna#0.0.15 /home/michael.heuberger/projects/binarykitchen/code/signdna\n...\n\u251c\u2500\u2500 resumablejs#e4bcccb241\n...\nNot sure what the problem is. Somehow bower is not recognizing the new version and using the git commit hash instead.\n. PS: It does not happen when I use <input type=\"file\" /> instead of a link.\n. Fair enough but sorry, cannot give you a test right now.\nI can confirm, it happens on Firefox only. On Chrome my code works like a char. Hmmm ...\n. It does not complain about that. But probably yes. You better update your node version and see it for yourself.\n. Uh, you don't use node.js? I am surprised. You should.\nAnyway, path.join() is fine like that:\nhttp://nodejs.org/api/path.html#path_path_join_path1_path2\nTested it on my machine. Just fix path.exists, cheers\n. Ha, sure I know it needs server-side code. I copied yours and fixed express routing.\nFor each chunk posted a 200 is returned. And for chunks that do not exist on the server yet, a 404 is returned. I assume that's the correct behavior?\n. Nope, none ... let me debug your script on the client-side and see where it is stuck.\n. LOL, i found the mistake. a progress of 1 means 100%. i misinterpreted that as 1%. all good now.\n. all sweet, my node.js implementation works. thanks\n. nothing special, most of it is taken from the node.js sample you have here\n. good points but i do not want to give the user the ability to save (submit) a video until the conversion is really done. with a highly optimized, self-compiled avconv code and correct options I can convert a 500 MB video within seconds.\nbut you're right, this is out of scope of resumable. will figure this out myself. cheers.\n. compile avconv only with the minimum without any other crap. following is taken from my gist at https://gist.github.com/binarykitchen/5230096\ninstall x264\n\ncd [YOUR PROJECTS FOLDER]\ngit clone git://git.videolan.org/x264 x264\nsudo ./configure --enable-static\nsudo make\nsudo make install\nx264 --version\n\nshows something like\n\nx264 0.130.31 c832fe9\n\ninstall libvpx\n\ncd [YOUR PROJECTS FOLDER]\ngit clone http://git.chromium.org/webm/libvpx.git\nsudo ./configure\nsudo make\nsudo make install\nvpxenc\n\nshould show command line options = OK\ninstall avconv\n\ncd [YOUR PROJECTS FOLDER]\ngit clone git://git.libav.org/libav.git avconv\nsudo ./configure\nsudo ./configure --enable-gpl --enable-libx264 --enable-libvpx\nsudo make\nsudo make install\navconv -v\n\nshows something like\n\navconv version v9-823-g352dbdb, Copyright (c) 2000-2013 the Libav developers\n\nat this point, you have everything ready for local video encoding at maximum performance.\nthen encode videos with https://npmjs.org/package/avconv ... but with the correct parameters! this is the most important thing. i've been working very hard on this for www.videomail.io but am happy to help you out. can send you some examples. but not here, sorry. email me!\n. aye, thx!\n. Wonderful, it works. Thanks @Bantam !\n. No, I am not looking for that. It's your work I want to install via bower!\n. Alright, then you need to fix bower.json and rename it from resumable.js to resumablejs\n. forget it, this ticket is too old\n. nice @mrawdon but will you update it whenever 23/resumable.js makes changes affecting its logic?\n. cool but still, it's more safe when the makers update this themselves. otherwise things might get broken when not synced properly.\n. It is not on Bower yes @steffentchr .... did you run register the new version with the bower command?\n. thanks, worked!\n. ",
    "sun2rise": "This would be great!!\n. ",
    "mspanc": "I've created npm package: https://www.npmjs.com/package/resumablejs although it's for client side.\n. Seems to be duplicate of #113 \n. I've created npm package: https://www.npmjs.com/package/resumablejs\n. No problem. @steffentchr unless you want me to be a bottleneck while publishing updates to NPM  packages, can you maybe make a login on npmjs.com so I could add you as a package co-owner?\n. I passed ownership to @steffentchr and it seems mine is already revoked, so I am unable to publish 1.0.3\n$ npm publish\nnpm ERR! publish Failed PUT 403\nnpm ERR! code E403\nnpm ERR! you do not have permission to publish \"resumablejs\". Are you logged in as the correct user? : resumablejs. ",
    "davekiss": "I think the problem is here:\nhttps://github.com/23/resumable.js/blob/master/resumable.js#L456\nThe $.preprocessState needs to be set to 1 before calling preprocess($). Otherwise, the $.preprocessState gets reset to 1 after the preprocessing occurs, even after calling .preprocessFinished()\n. I see what you're saying about the test target - have any ideas on how to do this cleanly?\nWill resubmit the other two shortly.\n. Added a few commits, let me know what you think\n. ",
    "aaronleesmith": "This issue desperately needs to be fixed. I will be issuing a PR today.\n. Thanks for the quick turnaround on this. This little library has been very helpful for us.\n. ",
    "jmonma": "PR #182 was just merged, which fixes the issues with the node example.\n. ",
    "Bantam": "I applied two spaces inside the encapsulation as you use in the project but i will resubmit without\n. According to https://groups.google.com/forum/#!searchin/google-chrome-developer-tools/filter$20console$20messages/google-chrome-developer-tools/X1fzw52Ci9Y/P2IfjHBLoE0J you can open find and filter out with the filter checkbox but its not ideal.\n. I just thought of a way to fix this by using a non-error status code.\nI added 204 to my permanentErrors array and started sending 204 No Content back from my server when the chunk is not uploaded.\nhttp://tools.ietf.org/html/rfc2616#section-10.2.5\n. After reading the r.js documentation it tells me that:\nIf the file does an existence check for define, in the following form typeof define === 'function' && define.amd, then it will prefix the define references with \"namespace.\".\nSince we don't use r.js optimizer here can you try out this version and let me know if the problem is fixed? https://github.com/Bantam/resumable.js/blob/master/resumable.js\nI will submit a proper pull request if you can confirm it works!\n. ",
    "AnSavvides": "Yup that makes absolute sense. I completely understand your thought rationale, being very strict on coding styles can put people off from forking the repository and opening a pull request.\nThanks for the speedy response!\n. :+1: \nConsistently using single or double quotes is a good idea, it definitely helps make code more readable - I think it does come down to a personal preference, I am a double quotes fan but single quotes do the trick too ;)\nI would be in favour of using $resumable and $file over $ (it's too jQuery-ish and gives the wrong message), but it's your call!\n. ",
    "evanworley": "Hi @steffentchr - Yes, I'm interested in adding it. As I have not contributed to this project before, I have a few questions.\n1) How is testing done? I don't see anything formal, is it just manually done?\n2) How should we depend on the 3rd party libraries crypto-js and operative? If they are defined, then we will use them, otherwise we don't provide MD5 checksums?\n3) Any plans to make the library more modular, and/or written in a higher level language? Right now it appears to be a single large JS file, which is the right deployment target but might not be the best for source management. Modular coffeescript would be my vote, but I know not everyone loves coffee like I do\n4) Any plans to integrate grunt? I suppose this greatly relies on #3 \nCheers,\nEvan\n. Hi @k0d3d,\nFirst of all, this feature will be optional. Secondly, we won't be computing checksums on large files, we will be computing them on the small \"chunks\". If the computation is done within web workers, multiple checksums can be computed in parallel, feeding the \"ready to upload\" chunk pipeline. However, browsers with extremely slow javascript interpreters may slow down quite a bit. In a project of mine, we use this approach to upload terabytes of data, and we can compute MD5s fast enough to support 20+ MB/s, using Chrome in Ubuntu with one of the first generation core i7s.\nDoes that resolve any of your concern?\n. @k0d3d,\nYes, the approach you mentioned is possible, and more secure than relying exclusively on the name and size of the file. If the file was updated after some parts were uploaded, but the size remained the same, this can be caught by comparing the MD5s of the chunks.\nHowever, if you are trying to resume a file there are a few approaches to verifying the integrity of the upload.\n1) You can re-compute and verify the md5s of all the previously uploaded chunks, though this will be very expensive if you have already uploaded a large number of chunks. You may be able to do this \"in the background\" depending on your connection speed and how many remaining chunks you have.\n2) At the end of the upload you send a final MD5 for the whole file. This can also be very expensive depending on the size of the file.\n3) Assume the previously uploaded parts are safe to use without checking. This works ok if you know the file on the clients machine has not changed. Some folks use \"mtime, name, and size\" as a proxy for this. If any of those 3 change, they abort the upload (or at least do #1).\n. @samtny thank you for the SparkMD5 link! That's going to be incredibly helpful for me.\n. I just dropped it in and it's 40% faster for me in the average case (when computing the checksum for 10MB chunks).\n. ",
    "k0d3d": "i use resumable.js on my app www.i-x.it.  I read around the web that\nimplementing  checksums on large files is extremely slow.  how do you plan\non achieving client side check sum esp.  cross-browser compatibility.\nOn 18 Nov 2013 23:41, \"Evan Worley\" notifications@github.com wrote:\n\nHi @steffentchr https://github.com/steffentchr - Yes, I'm interested in\nadding it. As I have not contributed to this project before, I have a few\nquestions.\n1) How is testing done? I don't see anything formal, is it just manually\ndone?\n2) How should we depend on the 3rd party libraries crypto-js and\noperative? If they are defined, then we will use them, otherwise we don't\nprovide MD5 checksums?\n3) Any plans to make the library more modular, and/or written in a higher\nlevel language? Right now it appears to be a single large JS file, which is\nthe right deployment target but might not be the best for source\nmanagement. Modular coffeescript would be my vote, but I know not everyone\nloves coffee like I do\n4) Any plans to integrate grunt? I suppose this greatly relies on #3https://github.com/23/resumable.js/issues/3\nCheers,\nEvan\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/23/resumable.js/issues/135#issuecomment-28745747\n.\n. it really does. my original idea was to use checksums to verify if a\npartially uploaded file is a \"valid\" resumable upload from another location\n/ browser/ session instead of using the default  size-name file unique id.\nif i was to use this small chunk method, it means every  chunk checksum is\nalso validated individual with the \"test chunk\"  GET request.  is this\npossible and also performance efficient.  im a mid level programmer so\nforgive any ignorance in my question/ assumptions.\nOn 19 Nov 2013 00:01, \"Evan Worley\" notifications@github.com wrote:\nHi @k0d3d https://github.com/k0d3d,\nFirst of all, this feature will be optional. Secondly, we won't be\ncomputing checksums on large files, we will be computing them on the small\n\"chunks\". If the computation is done within web workers, multiple checksums\ncan be computed in parallel, feeding the \"ready to upload\" chunk pipeline.\nHowever, browsers with extremely slow javascript interpreters may slow down\nquite a bit. In a project of mine, we use this approach to upload terabytes\nof data, and we can compute MD5s fast enough to support 20+ MB/s, using\nChrome in Ubuntu with one of the first generation core i7s.\nDoes that resolve any of your concern?\n- Evan\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/23/resumable.js/issues/135#issuecomment-28747313\n.\n. I would have asked this next question on another thread but its kinda\nrelated to chunks and  checksums.  Is there a way to calculate the bit\nrate/ transfer speed of an upload? Internet connection in Nigeria where i\ndo business is slow.  i want the chunk size to be a variable based on the\nInternet speed.  so the file is uploaded in smaller chunks when the speed\nis slow?  is this possible?\nOn 19 Nov 2013 00:31, \"Steffen Tiedemann Christensen\" \nnotifications@github.com wrote:\nThe checksum mechanism should probably be implemented in such a way that\nit can be used for both the GET verification and the POSTed upload of the\nchunks. Basically, this will have you doing:\n- Have a property for the Resumable object that sets a checksumFunctionor similar. Also there should be a\n  checksumScheme string identifying the scheme to the server, for\n  example \"md5\".\n- Each ResumableChunk object has a .checksum property that contains\n  the computed sum using the function above.\n- These sum are computed asynchronously and in parallel as soon at the\n  file/chunk is created. Some kind of queueing is probably needed to ensure\n  that neither disk i/o nor cpu becomes a problem.\n- No chunk starts any interaction with the server before the .checksumproperty is non-empty if the\n  checksumFunction option is set.\n- The .checksum property is forwarded to the server in both GET and\n  POST requests as resumableChecksum along with a resumableChecksumScheme\n  .\nIf the computation of a full-size chunk becomes a problem, it could be a\nviable option to simply calculate the checksum for a subset of the chunk.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/23/resumable.js/issues/135#issuecomment-28750600\n.\n. \n",
    "samtny": "We are doing checksums per chunk now by hooking \"fileAdded\" and passing the \"file\" to a function that looks more or less like this;\n```\nvar computeHashes = function (resumableFile, offset, fileReader) {\n    var round = resumableFile.resumableObj.getOpt('forceChunkSize') ? Math.ceil : Math.floor,\n      chunkSize = resumableFile.getOpt('chunkSize'),\n      numChunks = Math.max(round(resumableFile.file.size / chunkSize), 1),\n      forceChunkSize = resumableFile.getOpt('forceChunkSize'),\n      startByte,\n      endByte,\n      func = (resumableFile.file.slice ? 'slice' : (resumableFile.file.mozSlice ? 'mozSlice' : (resumableFile.file.webkitSlice ? 'webkitSlice' : 'slice'))),\n      bytes;\nresumableFile.hashes = resumableFile.hashes || [];\nfileReader = fileReader || new FileReader();\noffset = offset || 0;\n\nif (resumableFile.resumableObj.cancelled === false) {\n  startByte = offset * chunkSize;\n  endByte = Math.min(resumableFile.file.size, (offset + 1) * chunkSize);\n\n  if (resumableFile.file.size - endByte < chunkSize && !forceChunkSize) {\n    endByte = resumableFile.file.size;\n  }\n  bytes  = resumableFile.file[func](startByte, endByte);\n\n  fileReader.onloadend = function (e) {\n    var spark = SparkMD5.ArrayBuffer.hash(e.target.result);\n    console.log(spark);\n    resumableFile.hashes.push(spark);\n\n    if (numChunks > offset + 1) {\n      computeHashes(resumableFile, offset + 1, fileReader);\n    }\n  };\n\n  fileReader.readAsArrayBuffer(bytes);\n}\n\n};\n```\nOne key here is https://github.com/satazor/SparkMD5 for computing the checksum; way faster than CryptoJS which we had been using for a while.\nThen on Resumable.js \"query\" callback we return the correct hash thus;\nvar query = function (resumableFile, resumableChunk) {\n  return { 'checksum': resumableFile.hashes[resumableChunk.offset] };\n};\nFun times.\n. Thanks.  The checksum-per-chunk is simply a requirement of the (proprietary) file service we are building, which is part of a larger asset-management service.\nHowever your question, \"Did you have a concrete set of problems...\" did cause me to research TCP error rates a bit just now;\nMy previous assumption was that TCP error rates are \"rather high\", and we must compensate for this with an application-level integrity check (MD5 checksum).\nThis assumption does seem to be supported by the following (often referenced) paper; \"When The CRC and TCP Checksum Disagree\" (http://www.ir.bbn.com/documents/articles/crc-sigcomm00.pdf).  The paper ultimately proposes a more-or-less real-world error rate of between one in 16 million and one in 10 billion TCP packets.  These errors are not caught by TCP, and are simply passed on to the next \"layer\" as good.\nTaking this a step further, assuming an average packet size of 1,400 bytes, and an average file size of 1,048,576 bytes (one Megabyte), and using the \"worst case\" one in 16 million packets assumed to contain an error not caught by TCP, I arrive at;\n(16 million * 1,400 bytes) / 1,048,576 bytes\nequals;\n1 in every 21,362 uploaded files may contain an error.\nSo I guess this is \"how often it happens\"!  :)\n. @evanworley np.  SparkMD5 about 80% faster than CryptoJS in my testing.\n. Thanks!\n. ",
    "pawski": "Hello @samtny I have similar implementation (hook for fileAdded and SparkMD5), but I calculate MD5 for whole file (any size). \nWhen checksum is computed I run Resumable.upload() (from inside fileReader.onloadend function). It works for one file, but for more, where first is relatively small and second large it obviously fails. Resumable uploads second file before it's checksum is computed. \nI am curious when / where are You triggering Resumable.upload() ?\n. No problem :)\n. ",
    "sbagri": "Based on the labels, it looks like this feature will be released in v2. Is there any timeline of when it will be released. Also is there a branch where this feature is already included. In the main branch, I can't see this code.\n. My mistake, my apache server is prepending own headers which I didn't notice earlier. Can delete this issue\n. ",
    "lpgeiger": "+1 any updates on incorporating sparkmd5 in here?\n. @steffentchr I've seen several issues regarding including checksum in the metadata. Seems like it's a feature you don't want to include. What's the reasoning behind this?\n. ",
    "hyperknot": "Is there a way to calculate MD5 for the whole file here, which might not even fit in the memory? It'd be great to \"prove\" that the full chunking - uploading - server side combining process all went right. Individual chunk hashing only proves the uploading went right, with no way to check if the server could combine the files back into the original one.. It's great to see that the newest commit is about bumping version number to 1.0.3! Is there anything what is blocking it from being a release on GitHub and NPM?\nAt the moment, I'm using\n\"resumablejs\": \"23/resumable.js#4a8ceb8\"\nAs a workaround in package.json, but it'd be great to just use 1.0.3!. Wow, that's quite a reason! I wish you good luck with the transfer of the\nownership and thanks for the new release!\nOn 23 February 2017 at 19:00, Steffen Tiedemann Christensen \nnotifications@github.com wrote:\n\nSomeone else published the project to npm originally, and I'm waiting for\nownership to be transferred in order for me to publish. So yeah, annoying\nto hard-reference a commit ;)\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/23/resumable.js/issues/328#issuecomment-282070385,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAeKjwf1QKDSnkSGGhHvmdYkMALPeyv1ks5rfclMgaJpZM4JwrJ2\n.\n. But still, shouldn't fileRetry-s be fired?. In the master version from which I opened the ticket there were not fired.. \n",
    "gamb": "+1 good idea\n. ",
    "tomi-roam": "+1\n. ",
    "trawick": "I'm trying to implement that in a retry callback by updating the chunkRetryInterval (doubling up to a max, resetting when progress is made) but I can't figure out how to update the options of my Resumable object.  Any hints?\nThe basic idea:\n```\n    r.on(\"fileRetry\", function (file) {\n        var newRetryInterval = Math.min(r.opts.chunkRetryInterval * 2, maxChunkRetryInterval);\n    console.log(\"Increasing retry interval from\", r.opts.chunkRetryInterval, \"to\", newRetryInterval);\n    r.opts.chunkRetryInterval = newRetryInterval;   <=== not taking effect AFAICT\n});\nr.on(\"fileProgress\", function (file) {\n    if (r.opts.chunkRetryInterval > minChunkRetryInterval) {\n        console.log(\"Resetting retry interval after progress\");\n        r.opts.chunkRetryInterval = minChunkRetryInterval;\n    }\n});\n\n```\nUpdate:\nThe progress function is called even if no progress was made, so you need to check for actual progress before resetting chunkRetryInterval.  I don't think the check below is quite correct, but it works well enough to verify that the sliding timer is workable.\nif (file.progress() !== file._prevProgress) {\n. ",
    "nickjohnson": "While you may be right about the 100px bit, it seems a bit odd as a browser default. What's more many websites, including ours, use css resets which set things at zero, thus giving an expected default behavior. I suppose this is what we were seeing on our site.\n. ",
    "kpachbiu88": "steffentchr\nvar dropbox = $('#dropbox'),\nmessage = $('.message', dropbox);\nself.resumable = new Resumable({\n                    target:'/server.php',\n                    query: {\n                        method: 'uploadReplay',\n                        match_id: self.match_id,\n                        user: appSession\n                    },\n                    resumableFilename: self.match_id,\n                    fileParameterName: 'replay',\n                    withCredentials: true,\n                    maxFiles: 1,\n                    maxFileSize: 100000000,\n                    fileType: ['dem'],\n                    maxFileSizeErrorCallback: function(file, errorCount) {\n                        noty({type: 'error', text: '\u0424\u0430\u0439\u043b '+file.name+' \u0434\u043e\u043b\u0436\u0435\u043d \u0431\u044b\u0442\u044c \u043d\u0435 \u0431\u043e\u043b\u0435\u0435 100 \u041c\u0431.'});\n                    },\n                    fileTypeErrorCallback: function(file, errorCount) {\n                        noty({type: 'error', text: '\u0420\u0430\u0441\u0448\u0438\u0440\u0435\u043d\u0438\u0435 \u0444\u0430\u0439\u043b\u0430 '+file.name+' \u0434\u043e\u043b\u0436\u043d\u043e \u0431\u044b\u0442\u044c .dem'});\n                    }\n                });\nself.resumable.assignBrowse(message);\n                self.resumable.assignDrop(dropbox);\n.....etc.\nI start upload, and after delete Resumable:\ndelete self.resumable;\n$('#dropbox').remove();\nBut upload NOT STOP!\n. ",
    "45sound": "This would seem relevant(table of per browser connections): \n2013: https://stackoverflow.com/questions/985431/max-parallel-http-connections-in-a-browser/14768266#14768266\n2011:  https://stackoverflow.com/questions/8404464/increasing-google-chromes-max-connections-per-server-limit-to-more-then-6   \n2009: http://stackoverflow.com/questions/985431/max-parallel-http-connections-in-a-browser\n. Perfect, thank you. 3 connections is fine! :)\nVinny\n. ",
    "Rameshvolkar": "How to fix error, \u201cwaiting for available sockets\u201d in Google Chrome\nWaiting for available sockets error on chrome basically happens due to overload images and data saved on chrome.Cookies and images saved on chrome once gets in huge it creates this error.Solution for http://www.fixotip.com/how-to-fix-error-waiting-for-available-sockets-in-google-chrome/ is very easy just open previous this url and follow the steps.\nWhenever we do open any site it saves some cookies and images as temporary on browser.For the time being it reaches into overload and creates available socket error.and it will show in the bottom left corner of the screen.. ",
    "DevinTan": "Resumable.js is an amazing project, thank you.\n. ",
    "CmdrMoozy": "I don't have a version of Safari older than 7 to test with on hand, but for what it's worth the MDN seems to indicate that this code should be supported in virtually all versions of Safari (see this and this).\nIf we do find a browser that this doesn't work in, I'm more than willing to setup a test environment and debug it.\nThanks!\n. ",
    "adavidof": "@steffentchr @davekiss \nHello, what about this pull-request?\nIt will be cool if you add the testTarget functionality\nThank you!\n. @steffentchr \nI can do the clean PR\n. @steffentchr \nLook at this PR, please\nhttps://github.com/23/resumable.js/pull/355. Cool idea, I think the devs can you it for the upload chunks with different upload links for every chunk. @steffentchr The test function will be invoke only if testChunks options is enabled.\nSo may be disable this option by default?. ff48aa7 and 205fc97?\nI can remove them from PR today. @steffentchr was fixed, check please. ",
    "vadimkaz90": "when i write code like this:\n            preprocess :function(chunk){\n                chunk = $.base64.encode(chunk);\n                chunk.preprocessFinished();\n            },\nI get error:\nUncaught TypeError: Object W29iamVjdCBPYmplY3Rd has no method 'preprocessFinished' \nwhere is my mistake?\nI need encode file chunks before upload, and in the server i will decode it.\n. so how i can encode to base64 chunk content? Is it impossible?\n. anyone can help me?\n. ",
    "grzchr15": "No need to do it. Binary uploads work fine\n. ",
    "hellais": "We are discussing how to implement a feature that would allow this in a library that is a fork of this one in this ticket: https://github.com/flowjs/flow.js/issues/42.\nOnce done it would be quite simple to back-port it to this library too.\nComments and feedback from the resumable.js community would be very useful and appreciated.\n/cc @AidasK \n. ",
    "demrks": "Good to know, thanks\n. Thanks, i will take a look into the code to see if i can figure it out. \nThe cancel() function actually resets the counters, but if i use it, every additional file i upload afterwards, won't upload at all (fileProgress() ist not being fired, although fileAdded() is). I don't think that's what people might expect, am i wrong?\n. @steffentchr Sure, I made a quick JSfiddle (http://jsfiddle.net/56rna/) with a basic setup of Resumable.js. The server side part is missing, but i guess that shouldn't matter, since this is just a test. You'll have to check the developer tools console for the output:\nThe first time I upload a file everything seems to work as expected: fileAdded(), uploadStart(), fileProgress() and fileSuccess() are being fired.\nBut if i press the \"Retry\" button once the file has been \"uploaded\", only uploadStart() is being fired and then nothing happens anymore (not even fileProgress gets fired).\nExcactly the same happens with my code, although i use PHP to handle the upload on the server side (which works perfectly).\n. @steffentchr Thanks that fixes the problem. Just one last question: Is there any reason why fileRetry() is not fired if i use .retry()?\n. ",
    "oetiker": "@steffentchr yes, not breaking existing code was my intention ... As far as I understand the behaviour of window.setTimeout(fn,0), this will cause the respective code to be added to the end of the execution queue. I have therefore wrapped the fileAdded and filesAdded events too, so that they only trigger once all the chunking work is completed.\n. ",
    "dmauro": "Ahh, sorry about that. Thanks for clearing it up.\n. ",
    "wlitwa": "Exact same file. All I mean by sanitizing is running the filename through a regex to remove characters. In the interests of simplicity, I ran it through preg_replace('/[^A-Za-z0-9]/','',$filename);. This removed the extension. If I just did this, the file would be corrupted. If I added this: preg_replace('/[^A-Za-z0-9]/','',$filename).'.jpg';, then it didn't corrupt the file.\n. ",
    "cholubin": "Thanks for your answer!\nI applied your advice to my code, but it is not working ^^;\nr.on('fileAdded', function(file, e){\n  r.opts.chunkSize = 50_1024_1024;\n  ...\nr.upload();\n}\nIn Resumable opts chunkSize value is correctly changed but chunk size in\nserver side temporary folder was not changed.\n. Thank you!\nI'll check your advice.\n50MB chunk size test is..\nIn my  project, I'm trying to upload to openstack the individual chunk\nfiles in parallel before putting together chunk files.\nFor reason of performance of openstack upload API, I'm trying to change\nchunk size of a specific file that have large size over 5GB (limit of 5GB\nlarge object in openstack) )\n2014-03-05 19:32 GMT+09:00 Steffen Tiedemann Christensen \nnotifications@github.com:\n\nThe problem is that the instance of ResumableFile is createdhttps://github.com/23/resumable.js/blob/master/resumable.js#L228-L234(for obvious reasons) before\nfileAdded is called. So by then, chunkSize cannot be changes. So to do\nthis, you'll need to hook in to the process before then.\nBy the way, 50MB chunks seem pretty damn high?\n\nReply to this email directly or view it on GitHubhttps://github.com/23/resumable.js/issues/157#issuecomment-36729127\n.\n. Solved this problem!  Thanks for your advice.\n\nBy modifying the $.bootstrap and ResumableChunk function, I adjust the\nchunking process of newly dropped individual files.\nThank you once again!\ncholubin.\n2014-03-06 11:57 GMT+09:00 \u00b1\u00e8\u00c7\u00f6\u00bc\u00f6 cholubin@gmail.com:\n\nThank you!\nI'll check your advice.\n50MB chunk size test is..\nIn my  project, I'm trying to upload to openstack the individual chunk\nfiles in parallel before putting together chunk files.\nFor reason of performance of openstack upload API, I'm trying to change\nchunk size of a specific file that have large size over 5GB.limit of 5GB\nlarge object in openstack) )\n2014-03-05 19:32 GMT+09:00 Steffen Tiedemann Christensen \nnotifications@github.com:\nThe problem is that the instance of ResumableFile is createdhttps://github.com/23/resumable.js/blob/master/resumable.js#L228-L234(for obvious reasons) before\n\nfileAdded is called. So by then, chunkSize cannot be changes. So to do\nthis, you'll need to hook in to the process before then.\nBy the way, 50MB chunks seem pretty damn high?\n\nReply to this email directly or view it on GitHubhttps://github.com/23/resumable.js/issues/157#issuecomment-36729127\n.\n. \n\n",
    "edtechd": "Fixed.\n. ",
    "neonnds": "I would allow for non-200 (for backwards compatibility) and a json response. \nMajor players using REST APIs (Vimeo, Facebook, Twitter) return 200 status codes for every request. Status code 200 means the request itself completed with no errors (the act of accessing the API and returning a result). \nWithin a specific call, an error may occur, and this is where a specific error code can be returned.\nReturning something similar to below would do the job for most situations:\n```\nStatus - Ok or Fail\nData (optional) - Any data returned\nCode (optional) - HTTP code associated with the error\nMessage (optional) - Message associated with the error\nExplanation (optional) - Additional error info\n{status: \"Fail\", data: {}, code: 404, message: \"Chunk not found\", explanation: \"\"}\n{status: \"Ok\", data: {}, code: 200, message: \"Chunk found.\", explaination: \"\"}\n```\nAlso in the future its probably a good idea to allow for communication in XML and not just JSON.\n```\n                var testHandler = function(e) {\n                    $.tested = true;\n\n                    var status = $.status();\n\n                    if(status == 'success') {\n\n                            var response = JSON.parse($.xhr.responseText);\n\n                            if(response.status == \"Ok\") {\n                                    $.send();\n                            } else {\n                                    $.callback(status, $.message());\n                                    $.resumableObj.uploadNextChunk();\n                            }\n\n                    //NON-200 Returned so did not find chunk\n                    } else {\n                            console.log(\"HTTP Error.\");\n\n                            $.callback(status, $.message());\n                            $.resumableObj.uploadNextChunk();\n                    }\n            };\n\n```\n. Well 204 will do the job to remove the errors, so that solves my problem. And your right that supporting two different methods will be problematic. Probably best just to use HTTP codes for now and just keep the other approach for future releases / branches. Also if people really need more detailed responses, hopefully our conversation above will help them implement their own changes. Cheers.\n. ",
    "meden": "While this bug has already been closed, here are my 2cents: to be semantically correct, any 2xx status codes should be interpreted as a successful upload. The difference between success codes are only to notify the client about particular conditions, so that it could behave accordingly (i.e. not trying to build a DOM if it receives 204 No Content).\n~~Maybe a good response for a missing chunk test could be 100 Continue. But it has the limitation of being a HTTP/1.1-only response code (it is not defined in HTTP/1.0).\nFor further informations, read https://en.wikipedia.org/wiki/List_of_HTTP_status_codes.~~\nUPDATE: I read the RFC2116 Section 8.2.3 and I was wrong. So, to be semantically correct, the only way would be a JSON response. Sorry for the noise...\nThanks for you work!\n. ",
    "babyzone2004": "Yes I konw, but the demo of java is not correct, please check it again.\n if upload a large file , the server won't change the filename from \"xxx.temp\" to \"xxx\".\n@steffentchr \n. ",
    "dstoichev": "Steffen perhaps you can check if something like this will do the job.\n``` javascript\nfunction ResumableFile(resumableObj, file){\n      var $ = this;\n      $.opts = {};\n      $.getOpt = resumableObj.getOpt;\n      $._prevProgress = 0;\n      $.resumableObj = resumableObj;\n      $.file = file;\n      $.fileName = file.fileName||file.name; // Some confusion in different versions of Firefox\n      $.size = file.size;\n      $.relativePath = file.webkitRelativePath || $.fileName;\n      $.uniqueIdentifier = $h.generateUniqueIdentifier(file);\n      $._pause = false;\n      $.container = '';\n  // For calculating time left and last speed\n  $.bytesUploaded = 0;\n  $.estimatedSecondsLeft = 0;\n  $.speedLast = 0;\n  $.timeStart = 0;\n\n\n  var _error = false;\n\n...\n$.progress = function(){\n        if(_error) return(1);\n        // Sum up progress across everything\n        var ret = 0;\n        var error = false;\n        $h.each($.chunks, function(c){\n          if(c.status()=='error') error = true;\n          ret += c.progress(true); // get chunk progress relative to entire file\n        });\n        ret = (error ? 1 : (ret>0.999 ? 1 : ret));\n        ret = Math.max($._prevProgress, ret); // We don't want to lose percentages when an upload is paused\n        $._prevProgress = ret;\n    // Calculate time left\n    var currentTimestampInMilliseconds = Date.now(); // requires polyfill for IE < 9\n    $.bytesUploaded = Math.round($.size * ret);\n    var millisecondsEllapsed;\n    if (0 == ret) {\n      $.timeStart = currentTimestampInMilliseconds;\n    } else {\n      millisecondsEllapsed = currentTimestampInMilliseconds - $.timeStart;\n      if (0 < $.bytesUploaded && 0 < millisecondsEllapsed) {\n        $.speedLast = Math.floor(1000 * $.bytesUploaded / millisecondsEllapsed); // bytes / sec\n        $.estimatedSecondsLeft = Math.round(($.size - $.bytesUploaded) / $.speedLast);\n      }\n    }\n\n    return(ret);\n  };\n\n```\nPause and continue needs development.\n. ",
    "jimdoescode": "You don't need to do this in Resumable. You can get a decent calculation using the progress value that Resumable exposes:\n``` javascript\n    /*\n     * Calculates the remaining upload time by executing on a 1 second interval and\n     * monitoring the progress of the upload then updating a global time variable.\n     * Also fill the progress bar based off the progress reported by the uploader.\n     /\n    var progressIterations = 0;\n    var ONE_SECOND = 1000;\n    function calculateRemainigUploadTime()\n    {\n        var step = 1; //Change this to alter how often this check is run. It corresponds to seconds.\n        var progress = resumable.progress();\n        //Progress is a float from 0 - 1 so we can calculate the remainder by subtracting from 1.\n        var remainingProgress = 1.0 - progress;\n        //Calculate how much relative progress has been made then multiply by the iteration count.\n        var estimatedCompletionTime = Math.round((remainingProgress / progress) * progressIterations);\n        var estimatedHours, estimatedMinutes, estimatedSeconds, displayHours, displayMinutes, displaySeconds;\n        //Important! Make sure we increment the iteration count.\n        progressIterations += step;\n    //If progress is complete then quit\n    if(progress >= 1.0)\n        return;\n\n    //If the estimated time is valid then calculate the time values.\n    //NOTE: It might take 1 or 2 iterations to get a valid estimate.\n    if(isFinite(estimatedCompletionTime))\n    {\n        //Convert estimated completion time to hours, minutes,\n        //and seconds and append 0's if they are single digits\n        estimatedHours = Math.floor(estimatedCompletionTime / 3600);\n        displayHours = estimatedHours > 9 ? estimatedHours : '0' + estimatedHours;\n        estimatedMinutes = Math.floor((estimatedCompletionTime / 60) % 60);\n        displayMinutes = estimatedMinutes > 9 ? estimatedMinutes : '0' + estimatedMinutes;\n        estimatedSeconds = estimatedCompletionTime % 60;\n        displaySeconds = estimatedSeconds > 9 ? estimatedSeconds : '0' + estimatedSeconds;\n\n        //TODO: DISPLAY THE ABOVE VARIABLES\n    }\n    //NOTE: If you want to adjust how often this timeout runs then change the step variable.\n    setTimeout(calculateRemainigUploadTime, step * ONE_SECOND);\n}\n\n```\nThe code above will iterate once a second and take the progress resumable reports at each second and estimated the total number seconds until the upload is complete. The estimation should get more accurate as more progress is consistently made. It will then break up the estimated completion to hours minutes and seconds. Lastly it will set a timeout to execute the function again. You could do a setInterval as well but I like that the function can terminate the timeout if resumable reports complete progress.\nThere are probably better ways to do this but it seems to work pretty well for me and gives a decent estimate. I don't really think time calculation should be in resumable. As long as you can get access to a progress value.\n. ",
    "kuldipem": "Hi, I am facing same problem with Android (4.0) Native browser.  I have tested on Android's Chrome 34.0 , on same phone , It is working proper. \nI am working on to fix it...\nhttps://code.google.com/p/android/issues/detail?id=39882\n. Hi ,@steffentchr \nI found that lower than Android 4.2 have a platform bug for Stock Browser (native browser), Whenever we try to  send a file using resumable.js its send empty payload/body which creates a problem. So I used a script in Resumable to detect  \"Is Android lower than 4.2 and Is Native Browser ? \", If I found true than I used ArrayBuffer to the send file a file.... Now It is working good.. :-)\n. Hi @steffentchr ,\n  Sorry for late replay...I was busy with my master degree education. \nI have tested with Android Stock Browser in 4.0.1 and works good. Lack of time I used Header instead of Parameters....Pls do something if you can.\nThnx  in advance,\n@steffentchr \n. Hello @steffentchr ?\n. Ya, you are right it needs too implement in server side and also complicated for that. Ok I will find a way  to do things make simple as much as possible.\n. ",
    "nishantsvyas": "No Error on Console, but in debugger mode under file object file.chunks[0].xhr.status: [Exception:DOMExceptuon]\nStatusText:[Exception:DOMExceptuon]\nis showing. Image is attached.\nChange in Target:\n var r = new Resumable({\n            target: 'http://localhost/VumaxDR-WebViewer/Upload/frmUpload.html', // Default is \"/\"\n            query: {},\n            maxChunkRetries: 2,\n            maxFiles: 3,\n            prioritizeFirstAndLastChunk: true,\n            simultaneousUploads: 4,\n            chunkSize: 50 * 1024 * 1024\n        });\nyour help in this regard will be highly appreciated.\non IIS server side I have given permission to Upload folder (web.config)\nWeb.config file\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n\n\n\n\n\n\n\n\n. After running the whole code I got this Error on Console\nFailed to load resource: the server responded with a status of 500 (Internal Server Error) http://localhost/VumaxDR-WebViewer/Upload/frmUpload.html?resumableChunkNumb\u2026ableFilename=Dhruv%20Closeup.jpg&resumableRelativePath=Dhruv%20Closeup.jpg\nFailed to load resource: the server responded with a status of 500 (Internal Server Error) \n. could you please provide me the sample code for upload files\nMy Setup:\nI have Upload folder (VumaxDr Web3\\TestProject-Kendo\\TestProject-Kendo\\Upload)\nI have set the resmable variable as below\nvar r = new Resumable({\ntarget: 'http://localhost/VumaxDR-WebViewer/Upload/frmUpload.html', \nquery: {},\nmaxChunkRetries: 2,\nmaxFiles: 3,\nprioritizeFirstAndLastChunk: true,\nsimultaneousUploads: 4,\nchunkSize: 50 * 1024 * 1024\n});\nIS initialization is ok? what should I write in target option?\nI have made upload as application in iis. by doing this I am not getting any error as such. But unable to upload (copy) file.\nI am attaching my whole code:\ndebugger;\n        var r = new Resumable({\n            //target: 'http://localhost/VumaxDR-WebViewer/Upload/frmUpload.html', // Default is \"/\"\n            target: 'http://localhost/VumaxDR-WebViewer/Upload/fileUpload.html', // Default is \"/\"\n            query: {},\n            maxChunkRetries: 2,\n            maxFiles: 3,\n            prioritizeFirstAndLastChunk: true,\n            simultaneousUploads: 4,\n            chunkSize: 50 * 1024 * 1024\n        });\n        var results = $('#results'),\n            draggable = $('#dragHere'),\n            uploadFile = $('#uploadFiles'),\n            browseButton = $('#browseButton'),\n            nothingToUpload = $('[data-nothingToUpload]');\n```\n    // if resumable is not supported aka IE\n    if (!r.support) location.href = 'http://browsehappy.com/';\nr.assignBrowse(browseButton);\nr.assignDrop(draggable);\n\nr.on('fileAdded', function (file, event) {\n    debugger;\n    var template =\n        '<div data-uniqueid=\"' + file.uniqueIdentifier + '\">' +\n        '<div class=\"fileName\">' + file.fileName + ' (' + file.file.type + ')' + '</div>' +\n        '<div class=\"large-6 right deleteFile\">X</div>' +\n        '<div class=\"progress large-6\">' +\n        '<span class=\"meter\" style=\"width:0%;\"></span>' +\n        '</div>' +\n        '</div>';\n\n    results.append(template);\n});\n\nuploadFile.on('click', function () {\n    debugger;\n    if (results.children().length > 0) {\n        r.upload();\n    } else {\n        nothingToUpload.fadeIn();\n        setTimeout(function () {\n            nothingToUpload.fadeOut();\n        }, 3000);\n    }\n});\n\n$(document).on('click', '.deleteFile', function () {\n    var self = $(this),\n        parent = self.parent(),\n        identifier = parent.data('uniqueid'),\n        file = r.getFromUniqueIdentifier(identifier);\n\n    r.removeFile(file);\n    parent.remove();\n});\n\n```\n. why xhr is showing status as [Exception :DOMException] ?\n. in console I am not getting any error. But under file object of Resumable.js code\nXhr.Status is showing Exception: DOMException. you can see the picture file in earlier post marked with red. \n500 Error is not coming this time.\n. could you send me one simple code for upload file. what should I give in target?\n. Entire file fileUpload.html is below:\nHTML\n\n\n\n\n  <script src=\"Scripts/Kendo/js/jquery.min.js\">\n\n\n\n    <ul id=\"filelist\"></ul>\n<br />\n\n<div id=\"container\">\n    <a id=\"browse\" href=\"javascript:;\">[Browse...]</a>\n    <a id=\"start-upload\" href=\"javascript:;\">[Start Upload]</a>\n    <br />\n<pre id=\"console\"></pre>\n</div>\n<div class=\"row\">\n<div class=\"large-12 columns\">\n    <p class=\"lead\">Select files to upload</p>\n    <button class=\"inverse small\" id=\"browseButton\">+ Add Files</button>\n    <button class=\"danger small\" id=\"uploadFiles\">Start Upload</button>\n  <!--  <div class=\"alert-box alert\" data-nothingToUpload>Error Nothing To Upload, Please Add Some Files</div>-->\n    <div id=\"dragHere\" class=\"panel drop-zone\">Drag &amp; Drop Here</div>\n    <div id=\"results\" class=\"panel\"></div>Status:\n    <div class=\"alert-box secondary\"></div>\n</div>\n\n```\n",
    "tetzank": "Yes, that will work. if(event != undefined) or using typeof, like it is done some lines above, will work too and is probably better than some exception handling. The question is: How to set f.container (which isn't used as far as i can tell)? What event to send with fileAdded?\n. ",
    "celebvidy-owner": "Phonegap doesn't support the file in put. They have their own File Transfer APIs, which work across device platforms. So you could code support for phonegap and it would work across all platforms phonegap can build to. \n. ",
    "riaan53": "Hi,\nWill you accept a PR with support for cordova/phonegap? Its a smallish change needed. \nRegards,\nRiaan\n. Hi @steffentchr ,\nAny change on merging this PR? It looks like some good improvements to resumable.js.\nRegards,\nRiaan\n. I see I can use the .addFile(file) method and just convert my base64 to a blob and file obj. Will give it a try and let you know the outcome.\n. I have found a simple solution for image resize/manipulation with resumable.js. Also now you can use resumable.js as is on Cordova as well! See https://github.com/vsivsi/meteor-file-collection/issues/29\n. ",
    "kkabell": "@riaan53 Sounds good! You are more than welcome to submit the pull request, then we'll make sure to take a look at it.\n. ",
    "mjgchase": "Sorry should say used php sample from download.\n. ",
    "eklann": "Using forceChunkSize: true fixes the size of the second last chunk, but with this option disabled the last chunk is sent even though the data in it is already sent in the second last chunk.\n. I cannot see a reason for uploading the same data twice...\n. ",
    "udisun": "+1\n. ",
    "meonkeys": "Everything works if I use the patch from #182 \nI also upgraded to Node v0.10.30.\n. Thank you! :cake: \n. ",
    "Billybobbonnet": "+1 :+1: \n. ",
    "icruces": "@steffentchr Sorry for commenting on this old ticket but I just hit this issue. I see your point but I disagree that \"there's no performance gain\" as it strongly depends on the server implementation/file system.\nIn my case, my backend streams the data received from the browser to an external distributed system. This distributed system then creates the file in one of its nodes.\nSetting simultaneousUploads options higher than 1 is useless in this case because the distributed system API doesn't allow either, creating a partial file for each chunk and then merging them or having multiple threads writing different chunks to the same file.\nHowever, it is possible to write different files at the same time, actually it is very efficient, as different files are hosted by different nodes. So having the option to send chunks from different files would drastically improve the performance in this case.\nI am actually using ng-flow (the angular wrapper) but saw it adequate commenting here.\nCheers.\n. ",
    "mgcrea": ":+1: \n. ",
    "mrjoelkemp": "Understood. A new release would be great. Thanks!\nOn Sep 9, 2014 6:20 AM, \"Steffen Tiedemann Christensen\" \nnotifications@github.com wrote:\n\nThe lib is design to see fairly infrequent changes, at least after the set\nof community-inspired improvements and bug fixes added late last year. For\nthat reason, I've frankly not been too diligent in keeping version numbers.\nI'm happy to roll the current repository into a 1.0.1 -- and keep to that\ntrack if it's helpful for anyone?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/23/resumable.js/issues/191#issuecomment-54950047.\n. \n",
    "manojLondhe": "I see same issue. \nWill someone please fix this?\n. ",
    "jeremycook": "Case matters to bower: bower install resumable.js does not equal bower install Resumable.js. Hint, use the one with the uppercase \"R\".\n. ",
    "vsivsi": "This is a niche answer, but if you use the Meteor environment, the file-collection package has built-in support for Resumable on the server-side. (Disclaimer, I wrote that package). If someone wants to dig into that code and turn it into a more general express.js middleware npm package, go to town.  \nThe code is here:  https://github.com/vsivsi/meteor-file-collection/blob/master/resumable_server.coffee  \nThat's probably 90% of the way to solving this issue, although it's in coffeescript, so please don't hate me.\nThe only wrinkle is that code stores the uploaded file data in a MongoDB gridFS file, instead of in the filesystem, which is what I imagine many people would want out of an npm package. (Although if you use MongoDB server-side, gridFS a nice option to consider).  Should be straightforward to remove the Meteorisms and manage the uploaded chunks in a filesystem directory instead of Mongo.\n. @sreuter  @steffentchr \nIn my opinion this change is too broad.  I'm fine with adding 201, but this lumps all 2XX into the same category.\nMy package has been using 204 as a response code (literally meaning: \"No content\"), and this PR blatantly breaks that by flipping the meaning of 204.\nThe reason I don't use 404 or some other non-2XX response code is because most browsers log such responses to the console as failures, which leads to an endless stream of issues reported for my package because people assume that something is wrong because of all the 404s.  See for example:\nhttps://github.com/vsivsi/meteor-file-collection/issues/51\nI'm happy to file a separate issue on this, but I wanted to check in here first since this is the context that introduced this change.\n. FYI, I just created a PR maintaining 201 as success, as was requested in this PR, but reverting 202-2xx back to their previous meaning.  https://github.com/23/resumable.js/pull/236\n. It wasn't end-users reporting errors, it was developers attempting to use my package in their apps (under Meteor.js) and reporting issues against the package because they were alarmed by all the 404s when uploading large files during testing.\nAnyway, thanks for responding, I'm glad you're okay with the change.\n. +1  Just noticed this. Fantastic, I've been wanting to use HEAD requests for the upload chunk tests for a long time!\n. I meant \"unintended\" in the sense that the justification for your PR was specifically that 201 should be a \"successful\" response (which I agree with), but the code submitted flipped the meaning of all other 2XX codes, which led to the unintended consequence of breaking implementations that were counting on 204 as a semantically meaningful negative response.\nI didn't mean to imply that you didn't understand the scope of your change, just that it exceeded what you were actually specifically asking for (201). \n. Just to complete the circle... The 404 failure logging issue has been reported as an issue for resumable.js here: https://github.com/23/resumable.js/issues/127, and the final resolution was to use 204 as the response instead of 404.\n. Fixed\n. ",
    "eliias": ":+1: \n. ",
    "dortzur": "+1\n. ",
    "mrawdon": "I began this for my own purposes if anyone is interested, it is available here. https://github.com/mrawdon/resumable-node\n. Right now we're using 23/resumable.js on our front end, so the plan would be to keep the backend in check with the front end library.\n. I'm not questioning that, but it doesn't seem like there are any plans to do so.\n. I think that makes the most sense to me.  I only started the project so I could include it as an npm module\n. ",
    "darkterra": "Ok. Thank you , I did my own piece of code to make one final file.\n. +1\nResumable.js I use in a project with notament Node.JS, so it would be perfect if Resumable.JS was published an npm module ! ^^\nDarkTerra\n. ",
    "milad-kh": "dear @steffentchr \nhow about pause and resume an individual file object?\ni tested this but not worked\nfile.pause();\npause() only work on whole object not an individual file reference. ",
    "Roanvdploeg": "Hi @steffentchr \nWill you still accept a pull request for this issue?. ",
    "homerjam": "Pertinent part of above\n. ",
    "tarnfeld": "That is fantastic, thanks so much for the speedy and helpful reply!!\n. ",
    "benjamincarp": "I added several more comments as well as function headers (I can remove these if you think they add too much clutter).\nI also did more testing with more complicated sub folder structures and fixed a couple of bugs I found.\n. Dang you're fast!\n. The folder is not of a consistent size. My guess is that it is the size of the folder object (the folder name and the data needed to store the memory locations of its content.\nThat's probably a good call on both of my options being a bad idea.  I didn't realize that the status would be 0 in the case of a down server.  We should retry in that case (and that is more likely). The documentation on XmlHttpRequest made it look like the status should never be 0 after a request was sent off but obviously that is not the case.\nInterestingly, the xhr object seems to throw an error to the browser console  in this case that specifically says it is a directory. I tried wrapping $.xhr.send() in a try catch but it doesn't seem to catch the error at all. The text in the browser console is \"Failed to load resource: The operation couldn't be completed. Is a directory\"\n. ",
    "piotrpawlaczek": "@steffentchr I did remove extra indents and clean the white spaces. I think they were added by someone else.\n. ",
    "donut": "I see where I can set permanent error codes but I don't see where I can change the action taken when receiving 201. Any pointers?\n. The reason this is a pain is that the API is setup to take chunks from locations other than the web and I'm trying to avoid making all these exceptions for Resumable. Another pain point is not being able to customize the URL on a per-chunk basis and the inability to customize the default query parameter names. That said, I am really appreciating the ease of use that Resumable provides.\n. The API I have setup has an endpoint like /videos/{video_id}/chunks/{chunk_number}. It expects GET requests to have the chunk number as part of the endpoint. I tried using the preprocess setting to change the target option before each chunk is uploaded, but that caused problems (if I remember correctly, Resumable would never move on to actually upload the chunk). I ended up adding an exception to the API to also accept resumableChunkNumber query parameter.\nI've already added the exception to send a 200 OK code when Resumable is the client. If I wasn't on such a tight deadline I'd love to do the pull request. I'll add it to my list. It looks like it'd be pretty easy to do.\nThanks for the responses!\n. Sent you an email.\n. I was running into the same issue. Luckily, my API was already setup to accept the octet method. The documentation for the target option should probably be updated to state that this must be used with method set to \"octet\".\n. Along these lines, it would be nice to be able to set the accept parameter through the configuration. Currently, I'm setting it after I run .assignBrowse().\n. @steffentchr Looks like MDN is out of date, accept is supported in IE 10 and 11, Firefox, Chrome, Safari, and Opera. Also, I just tested this in iOS 9.2 Safari and it worked as well.\nAnyway, I don't see the feature difference being noticed by users. When it's supported and the user clicks, the browse dialog will filter out non-relevant files from being able to be selected. If it's not supported, all files will be selectable. In that situation, it will fall back to the extension filter done in JavaScript. Seems like it degrades pretty gracefully and I doubt it would add any confusion.\nI mostly just wanted to bring awareness to the feature. It's trivial to add it retrospectively after the <input> has been inserted without any special support from resumable.js.\n. @steffentchr So, I scrapped using the accept attribute. Safari on OS X didn't see a .mp4 file as a video file. Never mind about all of this. You're right, it's too early.\n. ",
    "axschech": "@steffentchr were any changes ever made here? I'm experiencing the same issue.\n. ",
    "sveri": "I was thinking about the possibility to add a callback function as a value in that map.\nHowever, I just found out that I can mutate the opts.query map when adding a file, which is sufficient in my case, it just was not obvious to me.\n. ",
    "psaxena": "I had the same problem.\nWhat worked for me is, Just before calling upload i.e. r.upload(), I modified query parameter. You can try below:-\nvar r = new Resumable({\n    target: \"somefile.php\",\n    testChunks : false,\n    chunkSize : 1 * 1024 * 1024,\n    query : {\n        email: \"abcd\"\n    }\n});\nr.opts.query.email= document.getElementById('email').value;\nr.upload();\n. ",
    "jdq22": "Thanks, @psaxena. That worked for us!\n. ",
    "mishra1947": "I had the same problem.\nWhat worked for me is, i defined query as a function\nvar r = new Resumable({\n        target: \"somefile.php\",\n        testChunks: false,\n        chunkSize: 1 * 1024 * 1024,\n        query: function () {\n            return{\n                email: document.getElementById('email').value\n            }\n        }\n    });\nr.upload();. ",
    "topgun743": "I was also having the same issue and my resolve to this is that keep the chunk size not bigger than 10 MB. Chunk size bigger than this does cause issue in Firefox.\n. The problem causing chunk size was 50 mb\n. OK. Thanks for replying. I will check it and tell you the feedback.\n. ",
    "rogue780": "+1\n. ",
    "zpchavez": "+1 npm is for browsers too, not just node, as @steffentchr suggested in #40\n. ",
    "jonrimmer": "@mspanc That issue is about an npm package for the server-side code. This is about an npm package for the client code.\n. ",
    "dagomar": "Sadly I am experiencing the same issue :( At least I know I am not the only one. \nI did find the line in which this behaviour is created; it is on line 848 where:\ne.target.value = '';\nWhen I comment this out it works the way I would like it to, I cannot vouch for any negative side-effects. Perhaps this could be added as a setting?\nSincerely,\nDagomar\n. Sorry I think I was working with an older version of resumable, it is found on line 911. I am creating a pull request now for the change, I have made it into an option. Hold on!\n. Hm, I didn't notice there is also that on line 1004. However, that is for the dropzone implementation, which probably makes less sence to change? Let me know if I should change it there as well, then I can update the pull request!\n. I can see your point, and I am also not sure if this could cause any side-effects. In my case (and I am guessing @bung87), I would like to use a default file input, and have it behave like that. In my implementation I would then empty it (or actually hide it) once the upload is completed, then show the uploaded file in a list. Right now I am using it for a single upload, in which case it makes sense, I noticed the behaviour of the multiple file input is to show something like '3 files selected'. In that case I guess it would make more sense to add them to a list and remove them from the input. However, if there are no negative functional side effects to this option, it would be very nice to have as an option at least. I am curious to your thoughts.\nAnd by the way, thank you for an excellent piece of software! Made my life so much easier :)\n. Hi @steffentchr,\nThis will be my first pull request on Github, I didn't realise I wasn't creating the request to the right repository :) Should be there now!\nSincerely,\nDagomar\n. You're welcome! \n. So... I have fixed it! It turns out that the file input element I was using was actually being uploaded to the server. This caused a sort of timeout that caused the redirect back to the form. I cleared the value of the file input after the upload with resumable is done, and that fixes my problem. So, not an issue with resumable after all! I close this now :)\n. ",
    "bung87": "@dagomar it's last year things,I just knew resumable.js and thought use it in my project simply,so I just use the original file input,your suggestion is more modern way,and avoid this problem,I think I will implement it as that way at next version,thank you for you explanation.enjoy coding!\n. ",
    "BorisMorel": "Hello,\nHere is an amazing article that you may find really useful, olease read it here http://mestitraxi.whonue.info/xyvhoyy\nRushing, morel.boris@orange.fr\n. Hey,\nWhat a wonderfull news from our best friend I've just read, you may read it yourself here http://mentongatry.howardandpilar.com/xyliu\nThx, morel.boris@orange.fr\n. Hi,\nLook what they have for you, I guess it's worth reading, more info here http://ndezibome.usatop10lists.com/xyldwdf\nmorel.boris@orange.fr\n. ",
    "szelcsanyi": "Hi @cpnielsen \nThank you for reviewing my example script! I have made some changes. I hope it is acceptable now.\n. ",
    "saipavanvutukuri": "Basing on your comment I tried to do something like this .But this isn't working .I am unable to access isuploadcompleted param on the server side. Please let me know any mistakes I made in this code.\nclient side\n   r.on('complete', function(){\n              // Hide pause/resume when the upload has completed\n              $('.resumable-progress .progress-resume-link, .resumable-progress .progress-pause-link').hide();\n//Calling server to say upload has been completed\n```\n          $.ajax({\n            type: \"POST\",\n            url: \"UploadServlet/Helper\",\n            data: \"isuploadcompleted=\"+\"true\",\n            success: function(data) {\n                                 }\n    });\n\n```\n});  \nServer side \n public void Helper(HttpServletRequest request)\n    {\n          System.out.println(\"Param user side\"+request.getParameter(\"isuploadcompleted\")); \n    }\n. ",
    "TeaSeaLancs": "@steffentchr I believe so. I dragged a folder with about fifteen files in (Including from sub-directories), and everything seems to have been added fine.\nThe first call to readDir reads all the direct children of the folder dropped, then calls readDir again, which in my tests does no more reading but passes the files back to loadFiles. It then adds any direct file children for uploading, and any direct directory children get passed round their own readDir function, and so on and so on.\nFeels like it works to me?\n. ",
    "tylermauthe": "Any updates on this? I am evaluating this library and browser support is an important consideration.\n. Nope, thanks for the info.\n. ",
    "davidpowerkneat": "We used a different library in the end. Thanks for following up.\n. ",
    "nickpeck": "The simplest test case for me that produces this condition is as follows:\n```\n<!DOCTYPE html>\n\n\n\n    <div id=\"dropbox\" style=\"height:400px;border:1px black dashed;\">\n\n        Drop files here\n\n    </div>\n\n</body>\n\n<script type=\"text/javascript\" src=\"./resumable.js\"></script>\n<script type=\"text/javascript\">\nvar r = new Resumable();\nr.assignDrop(document.getElementById(\"dropbox\"));\nr.on('fileAdded', function(file, event){\n    alert('you dropped a file ' + file.fileName);\n}); \n</script>\n\n\n```\nIf I drag filename..ext into the space, the user-defined fileAdded callback is fired (message box is shown).\nIf I rename the file to filename.ext and drag it into the space, the callback does not fire, although I can see from the debugger that the resumable onDrop handler is invoked (line 187).\n. Hi there, I'm seeing the same behaviour with Firefox 40.0.2 and Chrome 44.0.2403.155\n. Apologies, I forgot to test that : yes the same thing seems to happen with an element that has the assignBrowse binding.\n. ",
    "ChadTaljaardt": "The file i was using when i saw this issue was a zip file of like 500mb\nThe first test i done i kept the chunk size as default which was (1 * 1024 * 1024) and i uploaded a text file of 182kb full of loreum ipsum, both checksums where correct and matched. i then lowered the chunk size to 512 and then uploaded the same file and it ended up completing on the client side but wasnt reassembled on the server side and the file is just split into multiple parts without being reassembled.\n. Im using Laravel Framework \nThe code im using is from another laravel project found here : https://github.com/rajivseelam/laravel-resumable/blob/master/app/controllers/UploadController.php\nHere is the specific code tho. \n``` PHP\n    /\n\n     * Check if all the parts exist, and\n     * gather all the parts of the file together\n     * @param string $dir - the temporary directory holding all the parts of the file\n     * @param string $fileName - the original file name\n     * @param string $chunkSize - each chunk size (in bytes)\n     * @param string $totalSize - original file size (in bytes)\n     /\n    function createFileFromChunks($temp_dir, $fileName, $chunkSize, $totalSize) {\n        // count all the parts of this file\n        $total_files = 0;\n        foreach(scandir($temp_dir) as $file) {\n            if (stripos($file, $fileName) !== false) {\n                $total_files++;\n            }\n        }\n        // check that all the parts are present\n        // the size of the last part is between chunkSize and 2*$chunkSize\n        if ($total_files * $chunkSize >=  ($totalSize - $chunkSize + 1)) {\n            // create the final destination file\n            if (($fp = fopen('public/temp/'.$fileName, 'w')) !== false) {\n                for ($i=1; $i<=$total_files; $i++) {\n                    fwrite($fp, file_get_contents($temp_dir.'/'.$fileName.'.part'.$i));\n                    $this->_log('writing chunk '.$i);\n                }\n                fclose($fp);\n            } else {\n                $this->_log('cannot create the destination file');\n                return false;\n            }\n        // rename the temporary directory (to avoid access from other\n        // concurrent chunks uploads) and than delete it\n        if (rename($temp_dir, $temp_dir.'_UNUSED')) {\n            $this->rrmdir($temp_dir.'_UNUSED');\n        } else {\n            $this->rrmdir($temp_dir);\n        }\n    }\n}\n\n```\n. I see, i looked at the sample code in this repositry and it is the same code i am using, if the code im using is problematic, the sample needs updating to working code?\n. Where you able to find any better example's?\n. I made a pull request to fix this issue.\n. Hello, probably have something wrong, still learning how to use github.\nHere is a image of what i have so far\nhttps://i.imgur.com/oIYhT1C.png\n. Its probably not nearly the best code or anything but i have tested it on my server and im getting the same checksums on both of the file now. \n. Think I have done it this time \nhttps://github.com/23/resumable.js/pull/247\n. Here is the update :+1: \n. ",
    "spficklin": "Even if the button id were passed in as part of the fileAdded event... that would help tremendously. Is that a feature request that would be considered?\nCan the issue be re-opend for discussion?\n. Yes, you're right, no source element on a drag-and-drop.   Thanks for the sample code.  So, just to clarify, is the suggestion that I override the Resumable object's appendFilesFromFileList() function so that I can then get the button name from the event?\n. Okay, I'll give that a try.  Thanks much for your help :-) \n. ",
    "keyboSlice": "Apologies, missed that I was just looking at the docs on the site, thanks\n. extensions works for me, thanks for your help :)\n. ",
    "rvillas": "+1\n. ",
    "SubhaPrince": "Same Problem i am also fetching when internet is disconnected mode than 2min that time no such file or directory for some part missing error is coming   \n. ",
    "jmesh": "No sites up that are public facing for demo.\nThe following code (line 909 in resumables.js) was the problem and we\nremoved it:\ne.target.value = '';\nProject is up and running now.\nOn Mon, Nov 2, 2015 at 6:38 PM, Steffen Tiedemann Christensen \nnotifications@github.com wrote:\n\n@jmesh https://github.com/jmesh Can you provide a test link for this\nissue -- just did a bit of testing myself and couldn't reproduce?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/23/resumable.js/issues/253#issuecomment-153193768.\n\n\nJoe Moschler\n. ",
    "NBallaney": "This is great!\n. ",
    "8ivek": "Ok :)\n. @steffentchr  Done :)\n. I am new to github contributing to repositories so please correct me If I have done anything wrong.\n. Cheers thank you steffentchr\n. :+1:  my email is: bivek_j AT yahoo.com and meetbivek AT gmail.com\n. ",
    "basarat": "Note : rename the file to index.d.ts and it will just work when published to NPM :rose: No further configuration will be needed \n. ",
    "matthewdenobrega": "I've made basart's suggested change and some small updates - this should be good to go. \n. ",
    "NewPlayer2": "Thank you for a very fast reply.\nHere's what I'm doing:\nvar getUploadTarget = function () {return 'backend.php';}\nvar r = new Resumable({target: getUploadTarget});\nI also pass chunkSize, simultaneousUploads, parameterNamespace, fileType, fileTypeErrorCallback, testChunks, withCredentials and throttleProgressCallbacks to Resumable.\nFirefox tries to send request to \"./function%20()%20%7B%22use%20strict%22;return%20'backend.php';%7D\" instead of \"./backend.php\"\n. See line 752: https://github.com/23/resumable.js/blob/master/resumable.js#L752\nThe variable target is passed to xhr.open. Target gets assigned on line 731. \nThe getTarget function you referred to is getting called on line 741 inside the if block but not in the else block. I got my test script working by (without considering any consequences) quickly adding a getTarget call in the else block too.\nMight I be doing something wrong or is this a bug?\nIf this is still unclear, I'll be happy to make a test page for you, but it'll have to wait until later.\nThank you for the help.\n. OK, thanks, I understand.\nI was indeed hoping to use the target function to produce the URL for the POST requests too. I needed to fix a security issue and tried to fetch a token with AJAX and manipulate the target URL between the fileAdded event and Resumable's upload() call. It's rather ugly, but something had to be done fast. :)\nChanging multipart to octet will most likely require changes to my server-side code, but I'll give it a thought.\nPerhaps it would be good to mention this method issue in the documentation where it says that you can pass a function to as target.\n. ",
    "sam1111": "@steffentchr does the example provided in samples for node.js doesn't support this at server end? Because I was running that and found out that after a reload every chunk from first to last was uploaded again. Please correct me if I am wrong.\n. Yeah Thanks!!! I realized that after writing the question. My bad!!\n. ",
    "RenzoF": "Thanks @steffentchr just created a pull request, also modified the readme.\nCheers\nRenzo\n. ",
    "cpaulet": "Hi, unfortunately, i work in local, but if you try with this code : \n```\n    Select files\n\n</div>\n<script type=\"text/javascript\" src=\"/js/resumable.js\"></script>\n\n",
    "markope": "Did you check it out? \nThe fileupload takes whatever chunk_extra or finish_extra values are posted per chunk and reflects this in the WAMP event. So there should be no change needed in crossbar here.\n. ",
    "venture758": "local java demo\nvar r = new Resumable({\n            target:'/java-example/upload',\n            chunkSize:1_1024_1024,\n            simultaneousUploads:1,\n            testChunks: false,\n            throttleProgressCallbacks:1,\n            fileType: [\"jpg\"],\n            maxFileSize: 2_1024_1024,\n            method: \"octet\"\n            //  method: 'multipart'\n          });\n. ",
    "atrauzzi": "Does the library currently accept 204?\n. Awesome.  Sorry, I had seen the discussion and gave the code a glance but couldn't see anything that sniffed out 204, which is why I asked.\nGlad I did! :)\n. ",
    "guzmanfg": "+1 \n. You're welcome! I was working on a #184 way parameter naming just in case somebody prefers that way. \nIn addition, I have a .NET sample, wich might be interesting to be added to sample folder, if you want.\n. My fault again, sorry. How about adding some tests? \n. My fault. Sorry \n. Nice, it is a matter of choices. Maybe I must make a pull request just the .NET sample \n. Are you using any server side example found in this repo? May you share your resumable configuration and file sizes? Same files are being received broken and successfully or it depends on the file? \n. ",
    "luxio": "@dagomar How do you have integrated resumable.js in Drupal 8? Have you written a module?\n. ",
    "anniefzh": "Happened to me as well, I set simultaneousUploads to 1, then the issue was gone.. @cpnielsen it keeps happening to me, especially when the file is more than 1GB or when I upload a couple files tgt, like starting the next one when the previous one is still being processed. I used PHP code from this link: https://github.com/dilab/resumable.php. Please take a look if you got a moment.\nAlso, I have a question about the simultaneousUploads property. Does it mean several files got uploaded from one user simultaneously or several users doing the upload simultaneously ? \nThanks, any help is appreciate.\nAnnie. @cpnielsen Thank you for the quick reply. \n\nsimultaneousUploads is how many chunks are being uploaded at the time, resumable does not think in terms of files per se.\n\nGood to know. Just to clarify, is simultaneousUploads supposed to make it faster ? Cuz..when I set the number to 1, the issue seems gone.\n\nA quick look at the PHP code seems to show it's working correctly (I haven't tested it). However, when it merges chunks into the final file, it will append to the file if it already exists. This means, if you upload an identical file twice (same identifier, same size), it will create the final file when the first upload finishes and then append to that file for each consecutive upload (instead of overwriting it). This also makes sense if you say that the file size is an integer multiplier of the expected size (x2, x3, etc.)\n\nI noticed this, and it's an issue indeed, but I had this issue even when the file is new. During debug, I noticed isFileUploadComplete($filename, $identifier, $chunkSize, $totalSize) returned true a couple times, and due to the naming issue, it doubles or triples the size. I suppose fixing the naming issue would help, but it doesn't seem to be the root cause. \nThanks,\nAnnie. ",
    "ylgu": "function(){...}? Can be more details, for example I want to bind a file's MD5 to it's self ,how to write my query function? Thanks very mush.\n. yeah, this is good for me , thank you very much!\nBelow is a part of my code:\nopts.query = function(resumableFile){\n                    return angular.extend(scope.queryData,resumableFile.query);\n                };\nfile.query.md5=getMd5(resumableFile);\n. $scope.options = {\n            target: $scope.baseUrl + '/ecupload/upload.do',\n            chunkSize: 2*1024*1024,\n            simultaneousUploads: 4,\n            testChunks: true,\n            throttleProgressCallbacks: 1,\n            method: \"octet\",\n            maxFiles: 20,\n            headers: {'tokenId': $scope.userTokenId}\n        };\nThe above is my configuration and the file size is about 80M, and the server side is just the same with the demo, I will get some logs from the server soon! \n. ",
    "briankulp": "Just noticed this issue this morning as well. Are you planning on PR'ing that fix @IgorZiegler?\n. I don't, but I would be happy to look into it if you need.\n. ",
    "IgorZiegler": "@briankulp do you know about solution of this problem?\n. ",
    "TheTesla": "No, the binary is /usr/bin/nodejs, but on some systems there is a softlink node to it. Unfortunatelly on Ubuntu 14.04 systems there may appear a /usr/sbin/ax25-node with node as softlink to it. So it is better to use nodejs instead of node. \n. ",
    "Pixelairport": "testChunks: false was missing :( now it works ... \n. ",
    "nickdatum": "I believe I've been able to fix this now...I've created a pull request\n. Will do now. Just realised I forgot to copy over a line of code as well, which I'll also do now and submit a new pull request!\n. https://jsfiddle.net/008qb6gm/5/\n. Pull request now made\n. No reloading of the page or resumable in between uploads, literally just uploading one file and then uploading the same file again\nSo if I did\nuploader.on('complete', function() {\n    alert('Complete');\n}\nThe second time I tried uploading the file, it would alert 3 times\n. No network traffic happening on the second upload. Basically, it doesn't upload on the second attempt (which is expected behaviour) but it still fires the complete event anyway, 3 times\n. Ok, so I've created a jsFiddle here: https://jsfiddle.net/w5zp6cqL/10/\nAlthough it won't work as /upload would need changing to a working path for uploading! Here's what basically happens if I do the following:\n1. I click the upload button and select a file (e.g. readme.txt)\nIt alerts saying \"file added\" and then alerts saying \"complete\" (and uploads the file)\n1. I click the upload button and select the same file (readme.txt)\nIt alerts saying \"file added\" and then alerts 3 times saying \"complete\" (although no upload is performed this time)\nDoes this make sense? Sorry if I'm wording it a bit confusingly.\n. This does the trick, thank you. I wonder if the filesAdded event should fire from the code in the first place, if no files? I'll leave that choice to you to make!\n. I have an idea with this... I think it's because the files array is getting too large and filling up the memory. I also suspect there is some duplicates being put into the memory, but I'll have to debug further first to figure this all out.\nI may have a play later in the week and potentially try to defer some of the processing so it doesn't iterate through directories all in one go before starting the upload.\nDo you think this could work?\n. I'm going to spend the day today doing some memory profiling and seeing if I can make any sort of performance improvements...could end up being a waste of a day or a really productive one, I'll keep you updated!\n. @steffentchr I've rewrote the loadFiles function and it works a lot more efficiently now. I also removed the incorrect paths patch I'd applied before as this also fixed this as part of the rewrite (issue #304)\nPlease let me know what you think, I think you'll be pretty impressed with the performance increase! The main benefit is it doesn't cause the browser to completely hang (and sometimes crash) so you can still use whatever web application you're integrating resumable into.\nhttps://github.com/nickdatum/resumable.js/blob/performance-relative/resumable.js\nI've also added a new \"beforeAdd\" event, as quite often users will want to show a message or something to say files are currently being loaded in for processing (or other possible actions).\n. Thanks, glad it's working for you now.\nThere was one thing breaking with backwards compatibility (missing forward slash at beginning of relative path) but I've sorted this now in my latest push.\nHappy for you to merge in now and glad to help out. I made use of the project for my app, so it felt good to contribute something back :)\n. Ok, just submitted the pull request\n. That's the one! Thanks\n. The issue has come back for me with a more recent version!. ",
    "juneight": "Got it. Thank you @steffentchr .\n. Looks like in the loadFiles() function, changing the success call back for reader.readEntries() call as below fixes this problem. \nCan anybody else reproduce this problem, and validate the fix?\n`\n          reader = entry.createReader();\n      //  var newEntries = [];\n\n      //wrap the callback in another function so we can store the path in a closure\n      var readDir = function(path) {\n        reader.readEntries(\n            //success callback: read entries out of the directory\n            function (entries) {\n              if (entries.length == 0) {\n                // handle empty folders here if necessary.\n              } else {\n                loadFiles(entries, event, queue, path);\n              }\n              updateQueueTotal(-1, queue);\n            },\n            //error callback, most often hit if there is a directory with nothing inside it\n            function (err) {\n              //this was a directory rather than a file so decrement the expected file count\n              updateQueueTotal(-1, queue);\n              console.warn(err);\n            }\n        );\n      }\n\n      readDir(entry.fullPath);`\n\n. ",
    "thewilli": "\nIs it possible to do this well on IE?\n\nno, it isn't. This would require ActiveX \ud83d\ude1c . Blame MS for (still) ignoring web standards since they won the Browser Wars...\n. would be great to especially publish a new npm version as well\n. sorry, I had some private, project-related content included when creating the MR. @steffentchr \n\nIs the queueLength = 0; correct?\n\nImho it is. And here's why:\nFiles get into resumable.js by eitiher dropping or browsing. Both kinds are finally processed by appendFilesFromFileList(fileList, event), which I've slightly modified in an earlier PR. As the queue part happens before, and files chosen in a file browse dialog (-> $.assignBrowse()) are directly passed to appendFilesFromFileList let's only concentrate on dropped files.\nWhen files are dropped the onDrop(event) helper function is invoked which checks for the event type and then lets loadFiles(items, event) do the further work. Here is where some iteration occurs and for each dropped item the queue length is increased and the item is passed to enqueueFileAdditon (here and here).\nThere comes the relevant part for this PR\njs\nif (queueFiles.length == queueLength) {\n   appendFilesFromFileList(queueFiles, event);\n}\nWe've added some files to the queue and now want to wait for the last (synchronous) enqueueFileAddition invocation, where the last item has been added to the queue. In this case we need to reset the queue, in order to not re-add the files again, whenever we drop files again.\njs\nif (queueFiles.length == queueLength) {\n  // process files\n  appendFilesFromFileList(queueFiles, event);\n  // flush the queue\n  queueFiles = [];\n  queueLength = 0;\n}\nSo what happens if we omitted queueLength = 0? We would just never pass the condition queueFiles.length == queueLength again (can be easily tested).\n\nIt seems that this variable should be in/decremented elsewhere.\n\nWell, I don't think this variable is needed at all (neither is the queue), as in all use cases the iteration occurs synchronously, by using some kind of e.g. map, but I didn't want to mess with the code style :wink: And why wouldn't you want to reset the length (count) of a queue together with its content?\nhth, thewilli\n. you're welcome :wink:\n. ",
    "adalga": "@oleilei \nThe problem is the block below. Since there is no try catch when you stop and continue to upload the last raf isn't closed.Therefore, you can't rename the file since there is an open raf already. You can figure this out by simply put try catch block and close raf when you catch exception too.\n`RandomAccessFile raf = new RandomAccessFile(info.resumableFilePath, \"rw\");\n    //Seek to position\n    raf.seek((resumableChunkNumber - 1) * (long)info.resumableChunkSize);\n\n    //Save to file\n    InputStream is = request.getInputStream();\n    long readed = 0;\n    long content_length = request.getContentLength();\n    byte[] bytes = new byte[1024 * 100];\n    while(readed < content_length) {\n        int r = is.read(bytes);\n        if (r < 0)  {\n            break;\n        }\n        raf.write(bytes, 0, r);\n        readed += r;\n    }\n    raf.close();`.\n",
    "falkmueller": "Hi,\nin Firefox 46, the drop-event return a file-list instead of dataTransferitems. in the funtion 'loadFiles' must be add a else-if-Confition:\nelse if(item instanceof File) {\n            queueLength++;\n            enqueueFileAddition(item, event);\n        }\nAlso the edited function is:\nvar loadFiles = function (items, event) {\n$.fire('beforeAdd');\n  var entry, item, i, ii;\n  for (i = 0, ii = items.length; i < ii; i++) {\n    item = items[i];\n    if ((item.webkitGetAsEntry != null) && (entry = item.webkitGetAsEntry())) {\n      if (entry.isFile) {\n        queueLength++;\n        enqueueFileAddition(item.getAsFile(), event);\n      } else if (entry.isDirectory) {\n        processDirectory(entry, entry.name, event);\n      }\n    } else if (item.getAsFile != null) {\n      if ((item.kind == null) || item.kind === 'file') {\n        queueLength++;\n        enqueueFileAddition(item.getAsFile(), event);\n      }\n    } else if(item instanceof File) {\n        queueLength++;\n        enqueueFileAddition(item, event);\n    }\n  }\n};\n. ",
    "alejodaraio": "Hello, please fix the conflict file and push on master to fix the issue.. I did this component on react https://www.npmjs.com/package/react-resumable-js\nCheers. ",
    "mindnektar": "I would love for this to be merged as well.. ",
    "ashudson23": "```\nvar r = new Resumable({\n ...\n});\nr.on('fileAdded', function(file, event){\n    r.upload();\n});\nr.addFile(BLOBHERE);\n```\n. ",
    "snajjar": "Thank you !\n. ",
    "jossef": "check out http://flowjs.github.io/ng-flow/\n. ",
    "ettoredn": "Network/cluster filesystem or the likes (e.g. object stores) is the solution I adopted.\n. ",
    "sosojustdo": "@steffentchr ,@sachinwalia2k8  iphash with load balancer?\n. ",
    "khmukid": "Hi, Thanks for reply.\nfile start upload after 1min+ with progress and it's complete upload successfully. My problem is it's not starting upload instantly after add file and calling \"upload()\", it take much time to start upload and show progress for second file. I use resumable.js for concurrent upload.\nAnd sorry i can't give you any access on my application, cause it's high level business application.\n. ",
    "k-schneider": "Is this still a thing? Why hasn't the npm package been updated to 1.0.3 yet?. ",
    "niklr": "I'm interested in this as well. What can we do to make it happen? ;). I had excatly the same issue and came to the same conclusion. This problem only occurs if a custom preprocess function has been defined.\nInstead of deleting the chunk.preprocessState === 0 check entirely it could be replaced with chunk.preprocessState !== 1 because the chunk should not be sent while preprocessing.. yeah forgot about this in the first pull request ;). ",
    "maxhbr": "I am currently trying to bump the version of resumable.js which is published via webjars at https://github.com/webjars/resumable.js. For this it would be very helpful if you could tag the commit https://github.com/23/resumable.js/commit/4a8ceb834f455629d73f9a00e24ab99bd7e48f81 as v1.0.3.. Thank you.. ",
    "tesujimath": "I'd also really like to see 1.0.3 available on npm.\n6 months after your comment about the ownership transfer of the npm package, I wonder if there is still a blockage getting ownership transferred from @mspanc?  If so, would it be sensible to choose another package name, that could be the official resumablejs package on npm?. ",
    "HardlyMirage": "There's a simultaneousUploads option you can use\n. Show me how you're initializing the resumable object\n. Looks like you already have simultaneousUploads set to 5, but looking at resumable's code, that only controls the chunks being uploaded, not the file containers. I don't see a way to do this directly. You'll likely need to maintain as many resumable instances as you need files uploaded in parallel.\n. You have this code:\nr = new Resumable({\n  target:'upload_receiver-promo.php',\n  maxFiles: 1,\n  simultaneousUploads: 5,\n  maxFileSize: <?php print constant(\"MAX_SIZE_MEDIA\"); ?>,\n  fileType: ['<?php print implode(\"','\",GetCategoryPromoFileTypes($category)); ?>']\n});\nYou essentially need to instantiate and maintain 5 of those variables / instances. I suggest maintaining them in an object or array.\nvar resumables = [];\nfor ( var i = 0; i < 5; i++ ) {\n    var r = new Resumable({\n      target:'upload_receiver-promo.php',\n      maxFiles: 1,\n      simultaneousUploads: 5,\n      maxFileSize: <?php print constant(\"MAX_SIZE_MEDIA\"); ?>,\n      fileType: ['<?php print implode(\"','\",GetCategoryPromoFileTypes($category)); ?>']\n    });\n    resumables.push(r)\n}\nThings will get complicated when handling events. Your code is messed up enough as is. I suggest you try and understand what you're doing instead of copying-pasting from various sources.\n. ",
    "jimski777": "Could you be so kind and provide a short example how to use it.\n. if (typeof Resumable == 'undefined') \n    {\n        r = new Object();\n        r.support = false;\n    } \n  else\n    {\n        r = new Resumable({\n          target:'upload_receiver-promo.php',\n          maxFiles: 1,\n          simultaneousUploads: 5,\n          maxFileSize: <?php print constant(\"MAX_SIZE_MEDIA\"); ?>,\n          fileType: ['<?php print implode(\"','\",GetCategoryPromoFileTypes($category)); ?>']\n        });\n    }\n. Without paying any attention to my code, can you provide an example.\nI mean how would you do it from scratch in a simplest possible way.\n. Ok, now I understand. Thank you.\n. Can you tell me how is my code messed up.\n. Thanks steffentchr. Your comments make perfect sense. Also running multiple chunks rather than multiple instances of resumable.js will make a cleaner code. The reason I wanted to implement multiple uploads is because the end users often feel like the uploads are going faster if they see several of them progressing in the same time.\n. ",
    "henno": "My client wants to select 40 files and upload them in one go. Currently he has to either compress the files beforehand into one zip or upload files one at ta time (40 times). He doesn't want to compress because he thinks it would be easier for his customers if they don't have to do the zipping before uploading. So will I have to find a replacement for ResumableJS  or can it do it?. ",
    "trujaytim": "I had this issue in resumablejs v1.0.2 (chrome only) - can not reproduce after upgrading to v1.1.0 so suspect this issue is resolved. Have also hit this issue in Chrome after upgrading to resumablejs v1.1.0 (from v1.0.2). Some potentially useful details:\n- I did not have this issue when using resumablejs v1.0.2 with Chrome 62.0.3202.94 (though it did have a different/unrelated issue that upgrading to v1.1.0 has resolved)\n- I do not have this issue with Edge browser. ",
    "braco": "What do you mean by \"can't use\"?\n. ",
    "krishbhattacharyya": "I am changing \"Can't use\" to \"How to use\" okay.\nActually I want to use the \"resumablejs\" in react native. Is there any way to use it?\nI need video upload in 23 video with react native.\n. ",
    "sampurcell93": "Hi, I'm not entirely sure what the problem with using this in react native is. My preliminary testing shows that the support property is true when an object is instantiated in RN. And events seem to fire just fine. I'll keep developing and see if I can break the lib on RN, but it looks good for now.. You'll want to use the Blob polyfill from react-native-fetch-blob as your {file} input to Resumable. If/when that PR lands, you can pass the file in like so:\nconst file = await RNFetchBlob.polyfill.Blob.build(\n        RNFetchBlob.wrap(uriString), {type: getMIMEType(uriString)});\nnew Resumable({file: {\n        ...file,\n        slice: async (...args) => {\n          return new Promise((resolve, reject) => {\n            const slice = file.slice(...args);\n            slice.onCreated((slicedBlob) => {\n              const path = slicedBlob.getRNFetchBlobRef();\n              fs.readFile(path, 'utf8').then((bytes) => {\n                resolve(bytes);\n              });\n            });\n          });\n        },\n});\nIn this example, fs can be imported one of two ways: \nimport fs from 'react-native-fetch-blob/fs';\nOR\nconst fs = require('react-native-fetch-blob/fs').default;\nFollowing these steps seems to work in my testing. Still having trouble accessing file data on the server side, but I can see that bytes are indeed being sliced correctly.. This begins to address https://github.com/23/resumable.js/issues/334. Usage example in React Native: \nconst file = await RNFetchBlob.polyfill.Blob.build(\n        RNFetchBlob.wrap(uriString), {type: getMIMEType(uriString)});\nnew Resumable({file: {\n        ...file,\n        slice: async (...args) => {\n          return new Promise((resolve, reject) => {\n            const slice = file.slice(...args);\n            slice.onCreated((slicedBlob) => {\n              const path = slicedBlob.getRNFetchBlobRef();\n              fs.readFile(path, 'utf8').then((bytes) => {\n                resolve(bytes);\n              });\n            });\n          });\n        },\n});. ",
    "Marthyn": "Hi @steffentchr \nIt happened with the feature that you can upload multiple files at once. It was with beta 3 i think, so maybe it is fixed now, will try again. \n. @steffentchr We tested again with beta 4, it works indeed again in Safari. Chrome is still an issue. Did you test in Chrome too?\n. @steffentchr Yes so the issue used to be\niOS beta 10.1 beta 3\nChrome iOS upload does not work.\nSafari iOS upload does not work.\nNow the issue is \niOS beta 10.1 beta 4\nChrome iOS upload does not work.\nSafari iOS upload DOES work.\nSo it seems there's just an issue with the beta ad chrome now. \n. ",
    "prajwalmr62": "I'm deleting the temp files once the upload is complete. This causes an issue because file.1 gets deleted as soon as first upload is complete, thus making file.1 unavailable for second client.\n. ",
    "znap026": "Why has it become a dependency? it wasn't needed before. I'm trying to work out what requires it?. Exactly why was this added? . ",
    "sjelfull": "I don't know anything about the history of the project, but it's obviously required?\nhttps://github.com/23/resumable.js/blob/master/resumable.js#L12. ",
    "Arood": "This is hilarious. Not only does it not use jQuery, but Resumable seems to use $ for it's own thing https://github.com/23/resumable.js/blob/master/resumable.js#L39. ",
    "shabushabu": "Any news on this?. Cheers!. ",
    "myfoxtail": "R = new Resumable({})\nR.on('fileProgress', (file, ratio) => {\n   console.log(file.progress());\n});. ",
    "kuangYIN1217": "@myfoxtail  How to get multiple files uploaded at the same time, the progress of each file?. I want to upload 3 files at the same time and initialize 3 instances, but when I get the progress of each file, I can only get the progress of the first instance. I want to get each progress.. ",
    "danielnutu": "Thank you for the clarification! . ",
    "rmhubley": "I have a similar issue with file size limitations.  When a GET is issued to check on a chunk the client should be able to discern between \"Chunk not found\" and a cancelable error state i.e \"Upload file size too large\".. Sure.  Because that would be pretty easy to manipulate to bypass what should be a server concern.. That's a good point.  I guess what confused me is sample server implementation ( node + express ).  It appeared that the GET implementation validates the size of the chunk.  Probably should not be implemented that way.  . ",
    "Salketer": "I understand, thank you for the fast feedback.\n2017-02-23 18:45 GMT+01:00 Steffen Tiedemann Christensen \nnotifications@github.com:\n\nClosed #364 https://github.com/23/resumable.js/issues/364.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/23/resumable.js/issues/364#event-974782722, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AKrNZlrkEFDq1-XRFhXHTNpERJnlYhZmks5rfcXRgaJpZM4MKHEK\n.\n. \n",
    "lambiase": "Unfortunately I discovered that Internet Explorer and Microsoft Edge don't support the File constructor (see http://caniuse.com/#feat=fileapi).\nI will create another pull request with a fix.. Created pull request #366 . ",
    "vladejs": "@cpnielsen , You are pointing me on the wrong direction. Dropbox's SDK v1 is deprecated. Here are all the endpoints documented for the SDK v2: \nhttps://www.dropbox.com/developers/documentation/http/documentation#files-upload\nAs stated in my first comment, I can successfully upload files with the ol' simple XMLHttpRequests, but I got in trouble with big files. I think I should use session or one of the endpoints that accept multiple chunks. \nAgain, the problem is I don't know how to use resumable.js with those endpoints.\nIn general, I am looking any solution that allows me to upload more than 300 MB without gettin the CONNECTION_RESET error\n. Problem is, that being the SDK v2 a work in progress, they don't have examples to do it with their SDK, I found a SO question which gives an example, but it doesn't work for me. Also, it doesn't show a way to get the upload progress.\nThat's why I'am trying to use Resumable.js. Could you give me some help in here @cpnielsen ?. Thank you for your reply. \nLast night I tried to use the Dropbox API manually but it seems that my chunking algorithm doesn't worked as expected (I modified and used this). \nThe upload example on dropbox repo is for simple uploads (files lower than 150mb), and they don't have a clear explanation in how to upload in chunks using the sdk. I created this issue. \nI'll keep looking.. ",
    "afukada": "Wow, a great suggestion! I'll give it a try and report back. Thanks a lot!. I have the following test script, but am getting \"resm.loadFiles is not a function.\" What am I doing wrong?\n```\n\n",
    "allansli": "@cpnielsen I think loadFiles(items, event) function is not exposed in resumable object.\n. ",
    "vaskaloidis": "Its not because its not iterating over the data from the input stream.\nInputStream pis = request.getPortletInputStream(); //This must be empty now\nlong readed = 0;\nlong content_length = request.getContentLength();\nlog.info(\"Content Length: \" + content_length); //Always -1\nbyte[] bytes = new byte[1024 * 100];\nwhile(readed < content_length) {\n// Not getting into the loop even\n}\nThis used to work - I am not sure what I changed to cause the input stream to be empty. Its not backend - the request from resumable.js used to contain an input stream with the file chunks, now its not. Resumable.js is not sending data to the backend anymore (it used to).. When I write the contents of the request.getPortletInputStream() to console - its empty now. Again, this previously worked\nInputStream pis = request.getPortletInputStream(); //This is empty now\nbr = new BufferedReader(new InputStreamReader(pis));\nwhile ((line = br.readLine()) != null) {\n    sb.append(line);\n}\nThis is printing out nothing.. ",
    "the-specialist": "The workaround/solution described in https://github.com/vimeo/player.js/issues/144 worked for me.\nimport * as Resumable from 'resumablejs/resumable.js';. ",
    "Igmat": "@the-specialist, unfortunately, your fix breaks type inference.\nWhile #397 isn't merged (or if it'll be rejected) there is another fix for angular CLI users (as @n00b1234). It will bring correct types but requires some additional gymnastics and unfortunately won't be up-to-date with this library.\nAdd this code to typings.d.ts:\n```TypeScript\n/ SystemJS module definition /\ninterface NodeRequireFunction {\n    (id: string): any;\n}\ninterface NodeRequire extends NodeRequireFunction {\n    resolve(id: string): string;\n    cache: any;\n    extensions: any;\n    main: NodeModule | undefined;\n}\ndeclare var require: NodeRequire;\ninterface NodeModule {\n    exports: any;\n    require: NodeRequireFunction;\n    id: string;\n    filename: string;\n    loaded: boolean;\n    parent: NodeModule | null;\n    children: NodeModule[];\n}\ndeclare var module: NodeModule;\n// Type definitions for Resumable.js v1.0.2\n// Project: https://github.com/23/resumable.js\n// Definitions by: Daniel McAssey https://github.com/DanielMcAssey\n// Definitions: https://github.com/DefinitelyTyped/DefinitelyTyped\ndeclare namespace resumablejs {\n    interface ConfigurationHash {\n        /\n         * The target URL for the multipart POST request. This can be a string or a function that allows you you to construct and return a value, based on supplied params. (Default: /)\n         /\n        target?: string;\n        /\n         * The size in bytes of each uploaded chunk of data. The last uploaded chunk will be at least this size and up to two the size, see Issue #51 for details and reasons. (Default: 110241024)\n         /\n        chunkSize?: number;\n        /\n         * Force all chunks to be less or equal than chunkSize. Otherwise, the last chunk will be greater than or equal to chunkSize. (Default: false)\n         /\n        forceChunkSize?: boolean;\n        /\n         * Number of simultaneous uploads (Default: 3)\n         /\n        simultaneousUploads?: number;\n        /\n         * The name of the multipart POST parameter to use for the file chunk (Default: file)\n         /\n        fileParameterName?: string;\n        /\n         * The name of the chunk index (base-1) in the current upload POST parameter to use for the file chunk (Default: resumableChunkNumber)\n         */\n        chunkNumberParameterName?: string;\n        /\n         * The name of the total number of chunks POST parameter to use for the file chunk (Default: resumableTotalChunks)\n         /\n        totalChunksParameterName?: string;\n        /\n         * The name of the general chunk size POST parameter to use for the file chunk (Default: resumableChunkSize)\n         */\n        chunkSizeParameterName?: string;\n        /\n         * The name of the total file size number POST parameter to use for the file chunk (Default: resumableTotalSize)\n         /\n        totalSizeParameterName?: string;\n        /\n         * The name of the unique identifier POST parameter to use for the file chunk (Default: resumableIdentifier)\n         */\n        identifierParameterName?: string;\n        /\n         * The name of the original file name POST parameter to use for the file chunk (Default: resumableFilename)\n         /\n        fileNameParameterName?: string;\n        /\n         * The name of the file's relative path POST parameter to use for the file chunk (Default: resumableRelativePath)\n         */\n        relativePathParameterName?: string;\n        /\n         * The name of the current chunk size POST parameter to use for the file chunk (Default: resumableCurrentChunkSize)\n         /\n        currentChunkSizeParameterName?: string;\n        /\n         * The name of the file type POST parameter to use for the file chunk (Default: resumableType)\n         */\n        typeParameterName?: string;\n        /\n         * Extra parameters to include in the multipart POST with data. This can be an object or a function. If a function, it will be passed a ResumableFile and a ResumableChunk object (Default: {})\n         /\n        query?: Object;\n        /\n         * Method for chunk test request. (Default: 'GET')\n         /\n        testMethod?: string;\n        /\n         * Method for chunk upload request. (Default: 'POST')\n         /\n        uploadMethod?: string;\n        /\n         * Extra prefix added before the name of each parameter included in the multipart POST or in the test GET. (Default: '')\n         /\n        parameterNamespace?: string;\n        /\n         * Extra headers to include in the multipart POST with data. This can be an object or a function that allows you to construct and return a value, based on supplied file (Default: {})\n         /\n        headers?: Object | ((file: ResumableFile) => Object);\n        /\n         * Method to use when POSTing chunks to the server (multipart or octet) (Default: multipart)\n         /\n        method?: string;\n        /\n         * Prioritize first and last chunks of all files. This can be handy if you can determine if a file is valid for your service from only the first or last chunk. For example, photo or video meta data is usually located in the first part of a file, making it easy to test support from only the first chunk. (Default: false)\n         /\n        prioritizeFirstAndLastChunk?: boolean;\n        /\n         * Make a GET request to the server for each chunks to see if it already exists. If implemented on the server-side, this will allow for upload resumes even after a browser crash or even a computer restart. (Default: true)\n         /\n        testChunks?: boolean;\n        /\n         * Optional function to process each chunk before testing & sending. Function is passed the chunk as parameter, and should call the preprocessFinished method on the chunk when finished. (Default: null)\n         /\n        preprocess?: (chunk: ResumableChunk) => ResumableChunk;\n        /\n         * Override the function that generates unique identifiers for each file. (Default: null)\n         /\n        generateUniqueIdentifier?: () => string;\n        /\n         * Indicates how many files can be uploaded in a single session. Valid values are any positive integer and undefined for no limit. (Default: undefined)\n         /\n        maxFiles?: number;\n        /\n         * A function which displays the please upload n file(s) at a time message. (Default: displays an alert box with the message Please n one file(s) at a time.)\n         /\n        maxFilesErrorCallback?: (files: ResumableFile, errorCount: number) => void;\n        /\n         * The minimum allowed file size. (Default: undefined)\n         /\n        minFileSize?: boolean;\n        /\n         * A function which displays an error a selected file is smaller than allowed. (Default: displays an alert for every bad file.)\n         /\n        minFileSizeErrorCallback?: (file: ResumableFile, errorCount: number) => void;\n        /\n         * The maximum allowed file size. (Default: undefined)\n         /\n        maxFileSize?: boolean;\n        /\n         * A function which displays an error a selected file is larger than allowed. (Default: displays an alert for every bad file.)\n         /\n        maxFileSizeErrorCallback?: (file: ResumableFile, errorCount: number) => void;\n        /\n         * The file types allowed to upload. An empty array allow any file type. (Default: [])\n         /\n        fileType?: string[];\n        /\n         * A function which displays an error a selected file has type not allowed. (Default: displays an alert for every bad file.)\n         /\n        fileTypeErrorCallback?: (file: ResumableFile, errorCount: number) => void;\n        /\n         * The maximum number of retries for a chunk before the upload is failed. Valid values are any positive integer and undefined for no limit. (Default: undefined)\n         /\n        maxChunkRetries?: number;\n        /\n         * The number of milliseconds to wait before retrying a chunk on a non-permanent error. Valid values are any positive integer and undefined for immediate retry. (Default: undefined)\n         /\n        chunkRetryInterval?: number;\n        /\n         * Standard CORS requests do not send or set any cookies by default. In order to include cookies as part of the request, you need to set the withCredentials property to true. (Default: false)\n         **/\n        withCredentials?: boolean;\n    }\ninterface Resumable {\n    new(options: ConfigurationHash): Resumable;\n\n    /**\n     * A boolean value indicator whether or not Resumable.js is supported by the current browser.\n     **/\n    support: boolean;\n    /**\n     * A hash object of the configuration of the Resumable.js instance.\n     **/\n    opts: ConfigurationHash;\n    /**\n     * An array of ResumableFile file objects added by the user (see full docs for this object type below).\n     **/\n    files: ResumableFile[];\n\n    defaults: ConfigurationHash;\n\n    events: Event[];\n    version: number;\n\n    /**\n     * Assign a browse action to one or more DOM nodes. Pass in true to allow directories to be selected (Chrome only).\n     **/\n    assignBrowse(domNode: Element, isDirectory: boolean): void;\n    assignBrowse(domNodes: Element[], isDirectory: boolean): void;\n    /**\n     * Assign one or more DOM nodes as a drop target.\n     **/\n    assignDrop(domNode: Element): void;\n    assignDrop(domNodes: Element[]): void;\n    unAssignDrop(domNode: Element): void;\n    unAssignDrop(domNodes: Element[]): void;\n    /**\n     * Start or resume uploading.\n     **/\n    upload(): void;\n    uploadNextChunk(): void;\n    /**\n     * Pause uploading.\n     **/\n    pause(): void;\n    /**\n     * Cancel upload of all ResumableFile objects and remove them from the list.\n     **/\n    cancel(): void;\n    fire(): void;\n    /**\n     * Returns a float between 0 and 1 indicating the current upload progress of all files.\n     **/\n    progress(): number;\n    /**\n     * Returns a boolean indicating whether or not the instance is currently uploading anything.\n     **/\n    isUploading(): boolean;\n    /**\n     * Add a HTML5 File object to the list of files.\n     **/\n    addFile(file: File, event: Event): void;\n    /**\n     * Cancel upload of a specific ResumableFile object on the list from the list.\n     **/\n    removeFile(file: ResumableFile): void;\n    /**\n     * Look up a ResumableFile object by its unique identifier.\n     **/\n    getFromUniqueIdentifier(uniqueIdentifier: string): void;\n    /**\n     * Returns the total size of the upload in bytes.\n     **/\n    getSize(): number;\n    getOpt(o: string): any;\n\n    // Events\n    /**\n     * Change event handler\n     **/\n    handleChangeEvent(e: Event): void;\n\n    /**\n    * Drop event handler\n    **/\n    handleDropEvent(e: Event): void;\n\n    /**\n     *  A specific file was completed.\n     **/\n    on(event: 'fileSuccess', callback: (file: ResumableFile) => void): void;\n    /**\n     *  Uploading progressed for a specific file.\n     **/\n    on(event: 'fileProgress', callback: (file: ResumableFile) => void): void;\n    /**\n     *  A new file was added. Optionally, you can use the browser event object from when the file was added.\n     **/\n    on(event: 'fileAdded', callback: (file: ResumableFile, event: DragEvent) => void): void;\n    /**\n     *  New files were added.\n     **/\n    on(event: 'filesAdded', callback: (files: ResumableFile[]) => void): void;\n    /**\n     *  Something went wrong during upload of a specific file, uploading is being retried.\n     **/\n    on(event: 'fileRetry', callback: (file: ResumableFile) => void): void;\n    /**\n     *  An error occurred during upload of a specific file.\n     **/\n    on(event: 'fileError', callback: (file: ResumableFile, message: string) => void): void;\n    /**\n     *  Upload has been started on the Resumable object.\n     **/\n    on(event: 'uploadStart', callback: () => void): void;\n    /**\n     *  Uploading completed.\n     **/\n    on(event: 'complete', callback: () => void): void;\n    /**\n     *  Uploading progress.\n     **/\n    on(event: 'progress', callback: () => void): void;\n    /**\n     *  An error, including fileError, occurred.\n     **/\n    on(event: 'error', callback: (message: string, file: ResumableFile) => void): void;\n    /**\n     *  Uploading was paused.\n     **/\n    on(event: 'pause', callback: () => void): void;\n    /**\n     *  Triggers before the items are cancelled allowing to do any processing on uploading files.\n     **/\n    on(event: 'beforeCancel', callback: () => void): void;\n    /**\n     *  Uploading was canceled.\n     **/\n    on(event: 'cancel', callback: () => void): void;\n    /**\n     *  Started preparing file for upload\n     **/\n    on(event: 'chunkingStart', callback: (file: ResumableFile) => void): void;\n    /**\n     *  Show progress in file preparation\n     **/\n    on(event: 'chunkingProgress', callback: (file: ResumableFile, ratio: number) => void): void;\n    /**\n     *  File is ready for upload\n     **/\n    on(event: 'chunkingComplete', callback: (file: ResumableFile) => void): void;\n    /**\n     * Listen to all the events listed above with the same callback function.\n     **/\n    on(event: 'catchAll', callback: () => void): void;\n    /**\n     * Listen for event from Resumable.js (see below)\n     **/\n    on(event: string, callback: Function): void;\n}\nclass Resumable implements Resumable {\n    constructor(options: ConfigurationHash);\n}\n\ninterface ResumableFile {\n    /**\n     * A back-reference to the parent Resumable object.\n     **/\n    resumableObj: Resumable;\n    /**\n     * The correlating HTML5 File object.\n     **/\n    file: File;\n    /**\n     * The name of the file.\n     **/\n    fileName: string;\n    /**\n     * The relative path to the file (defaults to file name if relative path doesn't exist)\n     **/\n    relativePath: string;\n    /**\n     * Size in bytes of the file.\n     **/\n    size: number;\n    /**\n     * A unique identifier assigned to this file object. This value is included in uploads to the server for reference, but can also be used in CSS classes etc when building your upload UI.\n     **/\n    uniqueIdentifier: string;\n    /**\n     * An array of ResumableChunk items. You shouldn't need to dig into these.\n     **/\n    chunks: ResumableChunk[];\n\n\n    /**\n     * Returns a float between 0 and 1 indicating the current upload progress of the file. If relative is true, the value is returned relative to all files in the Resumable.js instance.\n     **/\n    progress: (relative: boolean) => number;\n    /**\n     * Abort uploading the file.\n     **/\n    abort: () => void;\n    /**\n     * Abort uploading the file and delete it from the list of files to upload.\n     **/\n    cancel: () => void;\n    /**\n     * Retry uploading the file.\n     **/\n    retry: () => void;\n    /**\n     * Rebuild the state of a ResumableFile object, including reassigning chunks and XMLHttpRequest instances.\n     **/\n    bootstrap: () => void;\n    /**\n     * Returns a boolean indicating whether file chunks is uploading.\n     **/\n    isUploading: () => boolean;\n    /**\n     * Returns a boolean indicating whether the file has completed uploading and received a server response.\n     **/\n    isComplete: () => boolean;\n}\n\ninterface ResumableChunk { }\n\n}\ndeclare module 'resumablejs' {\n    export = resumablejs.Resumable;\n}\nMostly all of it is a copy of https://github.com/23/resumable.js/blob/master/resumable.d.ts but with several changes that was introduced to avoid problems with `new Resumable` constructor. Library imports also have to be changed to look like following:TypeScript\nconst Resumable: resumablejs.Resumable = require('resumablejs');\n``\nUnfortunately, I haven't found a way to use this library with normalimport from` syntax by rewriting only typings. It seems that small part of code have to be rewritten for this (see #397).. ",
    "drazenp": "I'm relying on the complete event to show which files are uploaded. I cannot rely on fileSuccess since the uploading process can be canceled anytime. So I have to track successfully uploaded files and on the complete event show them.\nThis is not really stopper since with a flag it can be easily forced to work, but I guess upload should be completed only once :)\nIs there a reason to call $.resumableObj.uploadNextChunk(); for each chunk while canceling upload?. ",
    "calebhskim": "I am also seeing this issue. In my setup I eventually pass my chunks to AWS' multipart upload API. When the complete event is fired I send a request to my server to send a request to AWS to stitch the chunks together. A premature/unexpected firing of the complete event causes AWS to stitch all the current chunks together for an existing upload. But since the request was premature not all the chunks are there meaning an incomplete file gets saved to S3. I can get around this by checking the progress of the file. \nI agree with @drazenp and was wondering what the status on this was. Thanks!. ",
    "adamthedeveloper": "+1. ",
    "shijiantian": "@cpnielsen Thank you for your reply.  Can you tell me what happened on the sever when I click on pause button on browser ? If the chunk is uploading when I click the pause button,can it finish the upload and close the RandomAccessFile? Or  it will pause on the server and never resume, when I click resume button on browser ,server will create a new progress/thread to upload the chunk.\n. So kind of you ,thank you very much. . ",
    "joyanf": "Actually it was sort of my fault..\nI used the resumable.js file from this location:\nhttps://github.com/23/resumable.js/blob/master/samples/java/src/main/webapp/resumable.js\nWhereas i had to use this file..\nhttps://github.com/23/resumable.js/blob/master/resumable.js\nMaybe the file in the samples is a older version...\nMy Bad..!! . ",
    "saberone": "Pretty please with sugar on top. Can this be merged. I currently have a dependency on the github reop of @Igmat (props for the fix), and it seems to work fine.. ",
    "Yamakaky": "It's not on http://www.resumablejs.com/. Ok, thanks. It's outdated ?. ",
    "jessp01": "Hi @bertsinnema,\nI am testing resumeable JS as means to upload onto the Kaltura Server, see:\nhttps://github.com/kaltura/kaltura-parallel-upload-resumablejs/\nOn the server side, we check whether resume was set to true:\nhttps://github.com/kaltura/server/blob/Mercury-13.3.0/alpha/apps/kaltura/lib/kUploadTokenMgr.php#L70\nif so, we assume chunks of this file are already in place and we need to call the handleResume() method. If the first chunk is not the first to arrive on the server side, we'll hit:\nhttps://github.com/kaltura/server/blob/Mercury-13.3.0/alpha/apps/kaltura/lib/kUploadTokenMgr.php#L221\nand the operation will fail.\nHere:\nhttps://github.com/kaltura/server/blob/Mercury-13.3.0/alpha/apps/kaltura/lib/kUploadTokenMgr.php#L78\nwe check whether finalChunk was set to true and if so, we set the status for this upload token [which basically represents an upload \"session\"] to UPLOAD_TOKEN_FULL_UPLOAD. Once the status is set to that, we assume the upload was completed and therefore do no allow additional chunks to be uploaded with that particular token.\nAnd so, if the last chunk arrived before all the previous chunks were processed, the operation fails here:\nhttps://github.com/kaltura/server/blob/Mercury-13.3.0/alpha/apps/kaltura/lib/kUploadTokenMgr.php#L53\nAnd that's basically the reason why I need this.\nThanks,. ",
    "pioh": "It may be safer to do a check for null instead of a instaceOf File\njavascript\nif(item !== null) {\n  ...\n}. ",
    "chase-moskal": "Would love to see this merged! :100:\n\nMy only worry is polyfilling Promise itself, and I am open to making changes / including a polyfill for Promise.\n\nI think it's fair to let consumer applications provide their own Promise polyfills. Resumable should just use whatever Promise is globally available, application authors have the responsibility to ensure promises exist in their environment (for all of the libraries they include, not just resumable).\nI like this PR because, without adding any polyfills, it just makes Resumable more robust by allowing it to work seamlessly with either the browser or the react-native APIs \u2014 so that Resumable \"just works\". ",
    "faraz-purelogics": "Hi @steffentchr / @bmaland / @jamescarlos / @2xyo / @Bantam ,\nCan some one reply on this?\nThanks,\nFaraz. ",
    "zzarcon": "@steffentchr @cpnielsen any chance to have a look into this? :crossed_fingers: . @cpnielsen awesome! thanks \ud83d\ude4c . ",
    "majames": "@cpnielsen when will this be released? which version for resumable.js will it go into?. @cpnielsen could you please give us an update on this? really looking forward to the next release! \ud83d\ude04 . @cpnielsen could I please get your eyes on this \ud83d\ude42 . As stated in the docs, consumers can pass a generateUniqueIdentifier that returns a \"then-able\".\n\ngenerateUniqueIdentifier(file, event) Override the function that generates unique identifiers for each file. May return Promise-like object with then() method for asynchronous id generation. Parameters are the ES File object and the event that led to adding the file. (Default: null) \n\nYou can see in this code snippet that passing a then-able is supported: https://github.com/23/resumable.js/blob/master/resumable.js#L404-L418 \ud83d\ude42 . ",
    "zachcowell": "Here is a solution I ended up using to programmatically call the upload method. Basically, I just utilized the fileAdded event\n``javascript   \nstatic upload(files){\n        const uploadEndpoint =/upload`;\n        const r = new Resumable({\n            target: uploadEndpoint,\n            simultaneousUploads : true,\n            headers : {\n                Authorization : Auth.getToken()\n            }\n        });\n        if(!r.support) alert('Your browser does not support this feature'); //TODO: do better error handling\n        r.on('fileSuccess', function(file){\n            console.log('fileSuccess',file);\n        });\n    r.on('cancel', function(){\n        console.log('cancel');\n    });\n\n    r.on('fileAdded', function(file, event){\n        r.upload();\n    });\n\n    r.on('error', function(e){\n        console.log('ERROR')\n    })\n\n\n    _.each(files, file => r.addFile(file));\n    return r;\n}\n\n```. ",
    "mikemorton": "It's very strange that the API works this way.  Have the maintainers ever commented on why it is like this?. ",
    "tawfiqAlbatsh": "It's happening with me too, and I can't find where is the problem.\nIt's only happening on Chrome ONLY and sometimes while I'm debugging the upload it works correctly so can any one help us??\nThanks . @cpnielsen \nThe processItem() code:\n```\nfunction processItem(item, path, items, cb) {\n            var entry;\n            if (item.isFile) {\n                // file provided\n                return item.file(function (file) {\n                    if (path) {\n                        if (path[0] == '/') {\n                            path = path.substring(1, path.length);\n                        }\n                        file.relativePath = path + '/' + (file.name[0] == '/' ? file.name.substring(1, file.name.length) : file.name);\n                    }\n                    else {\n                        file.relativePath = path + (file.name[0] == '/' ? file.name.substring(1, file.name.length) : file.name);\n                    }\n                    items.push(file);\n                    cb();\n                });\n            } else if (item.isDirectory) {\n                // item is already a directory entry, just assign\n                entry = item;\n            } else if (item instanceof File) {\n                items.push(item);\n            }\n            if ('function' === typeof item.webkitGetAsEntry) {\n                // get entry from file object\n                entry = item.webkitGetAsEntry();\n            }\n            if (entry && entry.isDirectory) {\n                // directory provided, process it\n                return processDirectory(entry, path + entry.name + '/', items, cb);\n            }\n            if ('function' === typeof item.getAsFile) {\n                // item represents a File object, convert it\n                item = item.getAsFile();\n                if (item) {\n                    if (path) {\n                        if (path[0] == '/') {\n                            path = path.substring(1, path.length);\n                        }\n                        item.relativePath = path + '/' + (item.name[0] == '/' ? item.name.substring(1, item.name.length) : item.name);\n                    }\n                    else {\n                        item.relativePath = path + (item.name[0] == '/' ? item.name.substring(1, item.name.length) : item.name);\n                    }\n                items.push(item);\n            }\n        }\n        cb(); // indicate processing is done\n    }\n\n```\nAnd the processDirectory() code:\n```\nfunction processDirectory(directory, path, items, cb) {\n            var directoryEntries = [];\n            var dirReader = directory.createReader();\n        var readDir = function (path) {\n            dirReader.readEntries(\n                function (entries) {\n                    if (entries.length > 0) {\n                        for (var i = 0; i < entries.length; i++) {\n                            directoryEntries.push(entries[i]);\n                        }\n                        readDir(directory.fullPath);\n                    }\n                    else {\n                        if (!directoryEntries.length) {\n                            // empty directory, skip\n                            return cb();\n                        }\n                        // process all conversion callbacks, finally invoke own one\n                        processCallbacks(\n                          directoryEntries.map(function (entry) {\n                              // bind all properties except for callback\n                              return processItem.bind(null, entry, path, items);\n                          }),\n                          cb\n                        );\n                    }\n                },\n                //error callback, most often hit if there is a directory with nothing inside it\n                function (err) {\n                    //this was a directory rather than a file so decrement the expected file count\n                    updateQueueTotal(-1, queue);\n                    console.warn(err);\n                }\n            );\n        }\n\n        readDir(directory.fullPath);\n    }\n\n```\nBut as I think when the code reaches processDirectory() and try to read the entries inside the directory the other directory is skipped. It's really weird thing cause in Firefox/Edge it's working fine. @cpnielsen \nAfter 2 hours of debugging and inspection I figure the following:\nWhen I try to upload 2 folders each folder have 1 file the following scenario happen:\n1) In loadFiles(items, event) it received 2 items as expected.\n2) Then it goes to processCallbacks(items, cb) with the same 2 items and a call back from loadFiles(items, event) function.\n3) Inside processCallbacks It will call processItem(item, path, items, cb) recursively with the following parameters:(item(first directory), \"\", [], call back from the processCallbacks method).\n4) Inside processItem method it will check the type of item and send it to processDirectory(directory, path, items, cb) method.\n5) Inside processDirectory method it will read the entries recursively then it will send the item inside the directory to processCallbacks method then it will send it again to processItem method and it will push it inside the items parameter then it will call the call back method which is to slice the item and call again processCallbacks with this parameters ([], call back is the slice for the beginning first items) and cause the first parameters is empty it will call the call back.\n6) Here the weird thing cause it must slice the other directory and send it to processCallbacks method again to process it in the processItem method which it is do but in the processItem method the item \nis an object with as the following: DataTransferItem {kind: \"\", type: \"\"} so it cant process this directory and it upload only one file.\nI'm sorry if i made it so long to you that is what I figure out, and if you need me to do more things please let m know.\nAlso if you figure where is the problem please let me know cause this issue is too important to fix it ASAP.\nThis is the processCallbacks function code:\n function processCallbacks(items, cb) {\n            if (!items || items.length === 0) {\n                // empty or no list, invoke callback\n                return cb();\n            }\n            // invoke current function, pass the next part as continuation\n            items[0](function () {\n                debugger;\n                processCallbacks(items.slice(1), cb);\n            });\n        }\nThanks in advance . ",
    "Lehakos": "Ping. @steffentchr Do you plan to fix it?. I have created PR for this. @steffentchr Can we update version to 1.1.1?. @bmaland @jamescarlos @2xyo @Mestika . @steffentchr why you didn't update version in package.json after merging?. @steffentchr what about merging? onDrop doesn't work in Firefox now. I think it is critical bug. Duplicate https://github.com/23/resumable.js/pull/504. ",
    "jreijn": "Thanks for the quick fix. I'm currently evaluating solutions for large file uploads in our application, hence the reason that I was looking at the Java backend.. ",
    "LaurensRietveld": "It appears this is simply unsupported. I created a rewrite of resumable.js for my personal use here: https://github.com/LaurensRietveld/resumable-node\nIt runs on nodejs but not the browser, is written in typescript and does not fully cover all of resumablejs' features. \nMight be useful as a boilerplate for others who:\n- Want to migrate resumablejs to typescript\n- Want to make resumablejs support both html5 and fs. \nJust don't use it in production: this implementation is tested for my own usecase, but it might (probably wont) work for yours ;). ",
    "mayeaux": "Looksl ike I need to send back a 500, thanks!. ",
    "flmg": "Any news on this guys? I need this as well and it doesn't seem to be a hard merge \ud83d\ude09 \n  . ",
    "OzoTek": "Thanks! Could you publish it on NPM? :). Do you mean in my side?\nI wanted to, but as 409 is not considered an error, it's entering the fileProgress event, and I can't seem to find the status code in the response, so I can't handle this myself. But maybe I missed something?\nAnd from a conceptual POV, I think handling 409 as an error makes sense, and we let the user decide how we wants to handle it (override, cancel, add a suffix to the name...).. Yes indeed, I must have missed that one! Thanks.. ",
    "WooDzu": "Found it! method:octet and simultaneousUploads:1 did the trick nicely\njs\nconst job = new Resumable({\n  // ...\n  method: 'octet',\n  simultaneousUploads: 1\n});. ",
    "mp-ffx": "take a look at the samples for backend implementations. ",
    "futzlarson": "Note that the PHP example (https://github.com/23/resumable.js/blob/master/samples/Backend%20on%20PHP.md) actually deletes the final file for some reason. In createFileFromChunks(), just after the \"create the final destination file\", adding something like this:\nrename($temp_dir . '/' . $fileName, \"files/$fileName\");\nOf course, the files directory will need to exist and be writable.. ",
    "fjh1997": "in node.js backend\uff0cyou can download the file by using GET /download/:identifier\uff0cit will automatically merge the trunks.. ",
    "AlinCiocan": "Thank you! And sorry for missing that event.. ",
    "Xatenev": "+1 Would love to know, too.. @pacificsoftdevld Pass true as 2nd param to assignBrowse:\n\n.assignBrowse(domNodes, isDirectory) Assign a browse action to one or more DOM nodes. Pass in true to allow directories to be selected (Chrome only). See the note above about using an HTML span instead of an actual button.\n\nIt should work by-default for assignDrop\n. ",
    "dxdleikai": "resumableRelativePath: The file's relative path when selecting a directory (defaults to file name in all browsers except Chrome).. ",
    "ADovgiy": "Yes, simultaneousUploads  means number of simultaneous files uploads, and I'm asking for chunks.. ",
    "cooltoast": "@steffentchr is this PR good to go?. @steffentchr yes only updating the docs.\nthe docs say undefined when it should be 1 (https://github.com/23/resumable.js/blob/master/resumable.js#L79)\nmy application allowed for empty files and I was confused why errors were being thrown, until I realized I made an incorrect assumption from the docs after reading the code. hi @steffentchr can we merge this PR?. ",
    "cjgibson": "Error also observed in Chrome v64.0.3282.167 - isolated to an apparent extra space on line 378.. ",
    "calebkiage": "I have the same question. This seems to cause problems on Spring Boot since Tomcat adds each parameter twice separated by a comma. Once from the query string and once from the form data. This means that if the filename is test.png, then resumableFilename will have a value of test.png,test.png on the server.. ",
    "shuntian": "I have the same question too.\nWhen i use POST method, the url will has some need not params. Is this a issue? \nor my problem;. ",
    "dansoper": "Looks like this is fixed by 02e87a75dfebb036d522f029a3701ef0aa7497f9, so this could be closed.. ",
    "xiaolongge904913": "merge to next pr. ",
    "pcata6": "Ok thks, i will check my config. ",
    "depiction": "Closing. Duplicate of https://github.com/23/resumable.js/issues/51. ",
    "tobias74": "I needed to be able to distinguish between fileProgress and a file being canceld. ",
    "keepRunning": "Updated to markChunksCompleted(). ",
    "User1000": "Have you found a good solution for dotnet core?. ",
    "techburgher": "Well....I think I see the problem, but I'm testing again. One of the backend sample scripts does not return 'OK' until after processing all of the chunks:\n```\n    # combine all the chunks to create the final file\n    if upload_complete:\n        target_file_name = os.path.join(temp_base, resumableFilename)\n        with open(target_file_name, \"ab\") as target_file:\n            for p in chunk_paths:\n                stored_chunk_file_name = p\n                stored_chunk_file = open(stored_chunk_file_name, 'rb')\n                target_file.write(stored_chunk_file.read())\n                stored_chunk_file.close()\n                os.unlink(stored_chunk_file_name)\n        target_file.close()\n        os.rmdir(temp_dir)\n        app.logger.debug('File saved to: %s', target_file_name)\nreturn 'OK'\n\n```\nI'm guessing if I move that OK before the upload_complete block, all will be well.\n. Moving the return 'OK' corrects the issue.. So...yeah -- this is still an issue, but it's a problem with the server-side script and not the resumable.js script, per say. If I move the return 'OK' to above the upload_complete block, then it never processes the chunks...duh.\nSuggestions on how to correct this?. ",
    "SuperPat45": "I found lz-string who is a browser friendly implementation of the LZW algorithm:\nhttp://pieroxy.net/blog/pages/lz-string/index.html\nIt seems a lot more interesting than gzip or brotly for javascript compression. I filled an issue on the Firefox Bug tracker https://bugzilla.mozilla.org/show_bug.cgi?id=1477745. Can you validate this pull request? This bug is very annoying.... ",
    "emreesirik": "I am using 1.1.0 version but it is work on source code. When will you put new release. ",
    "TheTFo": "I'm interested in this also. Wondered if forking and replacing the html5 bits may be worth while. ",
    "gggitpl": "Resolved, when no block is detected, the 404 string should not be returned and a 404 response should be returned.. ",
    "elunty": "All the server-side check is really doing is seeing if the chunk file exists (unless you added some custom logic), so if you have fewer chunks to check (use a larger chunkSize), it should be much faster. Not sure if that answer is good enough for you.. ",
    "upMKuhn": "Can we get this thing merged :) . Yes, you can attach query parameters. \nExtra parameters to include in the multipart POST with data. This can be an object or a function. If a function, it will be passed a ResumableFile object (Default: {}). just a word of warning. There is a bug with this method. If you pass: \n{ \"bla\": true, **\"lentgh\"**: \"anyNumber\" }\nYour query will be treated as an array. Therefore it will pass 0: undefined, 1: undefined. Checkout line: 145 --> $h.each method. ",
    "c3gdlk": "Actually it's can be used only outside of File.open(\"#{dir}/#{params[:resumableFilename]}\",\"a\") block. In other way sometimes file becomes broken. need if user trying to upload the same file twice. "
}
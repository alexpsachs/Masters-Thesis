{
    "dominichamon": "Fixed in 77cd9803ce6bcf1b1e2167e7c115cbaad01af8bb\n. The first error is fixed as of 4e21f5e18052a46f068e36cb0e5e66bf9bae260e.\nThe second error is curious as State is a friend so should be able to access the private member (and can in my non-clang Linux build).\n. This is really useful, thank you. I don't have an OSX machine to test on right now.\nI just pushed commit c44662e that should help with some (or all) of those errors. Please let me know.\n. Commit e2633b9 has the warning(error) fixes.\nIf you want to submit a patch for cmake i'm happy to take it. The idea is that it should be creating bin and lib in the project root so you have bin, lib, include, src in the project root. Are you seeing something different?\nThere may be a runtime issue with osx that i don't know about.. it sounds like either one of the clocks isn't ticking or there's a thread issue. I'll try to get an osx test machine going to check it out.\n. You only need to sign one of them, I believe. Use the individual CLA.\n. Confirmed - seems to work on OSX. I still need to test on linux before merging.\n. LGTM for ubuntu too. some timings were a bit different, but nothing significant. i have some comments on the change that i'll add as review comments now.\n. Thank you - I agree that having the examples in source is important (and they are, in fact: https://github.com/google/benchmark/blob/master/test/benchmark_test.cc) but having the examples in the comment in the main header makes it easier for users to find.\nIt's a difficult balance.\n. #64 has landed which gives us a working mingw port. does that help?\n. before i can merge this: have you signed a Google CLA? https://developers.google.com/open-source/cla/individual\n. should 'make install' not also copy the headers from include/ to /usr/local/include (on linux)?\n. Another 'fix' is to typedef the std::map. Then you remove the ',' from the argument list.\n. I don't know of a workaround for this in the preprocessor that will work, so I think the typedef is going to stay as the expected method. Closing this issue.\n. Have you signed the CLA as per https://github.com/google/benchmark/blob/master/CONTRIBUTING.md ? I don't see you in the CONTRIBUTOR list.\n. Can you sign the contributor agreement as per https://github.com/google/benchmark/blob/master/CONTRIBUTING.md ?\n. https://drone.io/github.com/google/benchmark/32\nfails to extract googletest tar\n. perfect - i also added 'make test' to the drone.io config.\n. That's curious. It seems that the instances.begin() and instances.end() are not being interpreted as InputIterator or ForwardIterator.\n. This misreports the number of iterations for the mean and stddev lines in the report as the total iterations. The change you reference ensures that the number of iterations column is the mean and stddev of the number of iterations, which is much more useful.\nIn my local testing I'm not seeing any issues with the mean or stddev output, so perhaps you could provide more information.\n. Apologies - it's been difficult for me to dedicate the time I want to take to discuss this.\nYou're absolutely right that the initial version used the assumption that the iterations in mean/stddev columns are irrelevant, but I found the initial version misleading. I expect the iterations column for a mean row to be the mean number of iterations. Also, if the std dev of the number of iterations is far from zero, it might indicate that there's something non-deterministic about the test.\nI'd like to keep the current approach for the output, but fix the issues that you've highlighted: The divide by zero and the behaviour under repetition.\n. I've found that cmake generates perfectly good visual studio projects, and allows the setting of build flags and defines (such as OS_WINDOWS) perfectly well. I'd much rather continue using cmake as the de facto project description file as then all information is in one place.\nThis also removes the requirement for including port.h unless you need the symbols therein, which is much less error-prone.\n. @pleroy I just saw a flurry of activity here so I'll go through again. My biggest concern is adding the MSVC project and solution files as the whole point of using cmake is to avoid platform specific build and project management systems. I'm afraid this policy is firm for me.\nIf you don't want to learn it, then I ask that you drop those files from the pull request and i'll accept the build-specific changes. Someone else can then complete the work by ensuring cmake generates the correct project/solution. I don't have a windows machine so I can't take that on, I'm afraid.\n. Fixed in #104 \n. I agree with @ckennelly: I think having tests that exercise correctness alongside the benchmarks that exercise performance is the right path to take.\n. before i can merge this: have you seen https://github.com/google/benchmark/blob/master/CONTRIBUTING.md ?\nPlease sign the CLA and add yourself to AUTHORS/CONTRIBUTORS.\n. Thanks, this is a good change. I'm testing it now on linux and OSX.\n. It might be worth adding VERSION and SOVERSION target properties so we can track breaking changes. maybe a future patch.\n. confirmed this builds/installs .dylib on osx.\n. Perfect. I was able to build with static and dynamic linking. I had to specify LD_LIBRARY_PATH to run the dynamic version, but that's a configuration issue on my end.\nThanks so much for the patch.\nNow I suppose I have to think about versioning...\n. I don't have a preference when it comes to rewriting history. Except that I'll probably mess it up and spend several hours trying to fix the mess :D\nIf you don't mind, i'd rather leave well enough alone.\n. The {{benchmark_filter}} option was originally meant to match the googletest {{gtest_filter}} option, but that is more of a glob than a regex, iirc. I'm actually ok with anything here that works consistently across versions.\nThe most common filter I use is \"prefix_.\" to run a subset of benchmarks. That should work everywhere.\n. It might be the default which would need to be '.' now.\nOn Aug 7, 2014 9:07 AM, \"Matt Clarkson\" notifications@github.com wrote:\n\nHold on before merging - the regular expression is matching correctly but\nthe benchmark_test is filtering everything out.\n\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/pull/41#issuecomment-51493427.\n. This is expected by users coming from Google test so I think that would be\na good change.\nOn Aug 7, 2014 9:37 AM, \"Matt Clarkson\" notifications@github.com wrote:\nI guess the regexec is actually doing regex_search not regex_match. Would\nyou like me to change the C++11 engine to regex_search? Not sure how to\nchange the regexec without going for re_match GNU regular expression\nfunction.\n\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/pull/41#issuecomment-51497366.\n. Great! That's the problem that I had when i did the naive move to c++11 regex :)\n\nchecking the patch now.\n. there's a bug without your change: \"*Calculate*\" matches nothing but \"Calculate*\" matches the BM_CalculatePi benchmarks. This latter is as expected.\nwith your change, none of the following match BM_CalculatePi:\n$ test/benchmark_test --benchmark_filter=\"Calculate*\"\n$ test/benchmark_test --benchmark_filter=\"*Calculate*\"\n$ test/benchmark_test --benchmark_filter=\"Calculate.*\"\n$ test/benchmark_test --benchmark_filter=\".*Calculate.*\"\n$ test/benchmark_test --benchmark_filter=\".*Calculate\"\n$ test/benchmark_test --benchmark_filter=\".*Calculate.*\"\n. #47 is merged\n. Output from test/re_test:\n```\n$ cmake .\n-- Check compiler flag -Wall\n-- Check compiler flag -Wall -- works\n-- Check compiler flag -Wshadow\n-- Check compiler flag -Wshadow -- works\n-- Check compiler flag -Werror\n-- Check compiler flag -Werror -- works\n-- Check compiler flag -pedantic-errors\n-- Check compiler flag -pedantic-errors -- works\n-- Check compiler release flag -fno-strict-aliasing\n-- Check compiler release flag -fno-strict-aliasing -- works\n-- git Version: v0.0.0\n-- Version: 0.0.0\n-- Performing Test HAVE_STD_REGEX\n-- Performing Test HAVE_STD_REGEX - Success\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX - Failed\n-- Performing Test HAVE_POSIX_REGEX\n-- Performing Test HAVE_POSIX_REGEX - Success\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/dominic/git/benchmark\ndougie:~/git/benchmark [mattyclarkson-regex] $ ninja\n[14/14] Linking CXX executable test/re_test\ndougie:~/git/benchmark [mattyclarkson-regex] $ test/re_test\nRunning main() from gtest_main.cc\n[==========] Running 7 tests from 1 test case. \n[----------] Global test environment set-up.\n[----------] 7 tests from Regex\n[ RUN      ] Regex.RegexSimple\ntest/re_test.cc:23: Failure\nValue of: re.Match(\"a\")\n  Actual: false\nExpected: true\ntest/re_test.cc:24: Failure\nValue of: re.Match(\"aa\")\n   Actual: false\nExpected: true\ntest/re_test.cc:25: Failure\nValue of: re.Match(\"baa\")\n  Actual: false\nExpected: true\n[  FAILED  ] Regex.RegexSimple (0 ms)\n[ RUN      ] Regex.RegexWildcard\ntest/re_test.cc:33: Failure\nValue of: re.Match(\"\")\n  Actual: false\nExpected: true\ntest/re_test.cc:34: Failure\nValue of: re.Match(\"a\")\n  Actual: false\nExpected: true\ntest/re_test.cc:35: Failure\nValue of: re.Match(\"aa\")\n  Actual: false\nExpected: true\n[  FAILED  ] Regex.RegexWildcard (0 ms)\n[ RUN      ] Regex.RegexAny\ntest/re_test.cc:45: Failure\nValue of: re.Match(\"a\")\n  Actual: false\nExpected: true\ntest/re_test.cc:46: Failure\nValue of: re.Match(\"aa\")\n  Actual: false\nExpected: true\n[  FAILED  ] Regex.RegexAny (0 ms)\n[ RUN      ] Regex.RegexExact\ntest/re_test.cc:54: Failure\nValue of: re.Match(\"a\")\n  Actual: false\nExpected: true\n[  FAILED  ] Regex.RegexExact (0 ms)\n[ RUN      ] Regex.RegexComplicated\ntest/re_test.cc:60: Failure\nValue of: re.Init(\"([0-9]+ )?(mon|low)key(s)?\", NULL)\n  Actual: false\nExpected: true\ntest/re_test.cc:62: Failure\nValue of: re.Match(\"something monkey hands\")\n  Actual: false\nExpected: true\ntest/re_test.cc:63: Failure\nValue of: re.Match(\"1 lowkey\")\n  Actual: false\nExpected: true\ntest/re_test.cc:64: Failure\nValue of: re.Match(\"19 monkeys\")\n  Actual: false\nExpected: true\n[  FAILED  ] Regex.RegexComplicated (3 ms)\n[ RUN      ] Regex.InvalidNoErrorMessage\n[       OK ] Regex.InvalidNoErrorMessage (0 ms)\n[ RUN      ] Regex.Invalid\n[       OK ] Regex.Invalid (0 ms)\n[----------] 7 tests from Regex (3 ms total)\n[----------] Global test environment tear-down\n[==========] 7 tests from 1 test case ran. (3 ms total)\n[  PASSED  ] 2 tests.\n[  FAILED  ] 5 tests, listed below:\n[  FAILED  ] Regex.RegexSimple\n[  FAILED  ] Regex.RegexWildcard\n[  FAILED  ] Regex.RegexAny\n[  FAILED  ] Regex.RegexExact\n[  FAILED  ] Regex.RegexComplicated\n5 FAILED TESTS\n```\nLooking into it a bit further.\nEnvironment:\n$ uname -a\nLinux dougie 3.13.0-34-generic #60-Ubuntu SMP Wed Aug 13 15:45:27 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux\n$ $CXX --version\nUbuntu clang version 3.4-1ubuntu3 (tags/RELEASE_34/final) (based on LLVM 3.4)\nTarget: x86_64-pc-linux-gnu\nThread model: posix\nSame errors with:\n$ $CXX --version\nUbuntu clang version 3.5-1ubuntu1 (trunk) (based on LLVM 3.5)\nTarget: x86_64-pc-linux-gnu\nThread model: posix\n. Also:\n```\n$ test/benchmark_test \nReading /proc/self/cputime_ns failed. Using getrusage().\nStarting new interval; stopping in 500000\nPer-iteration overhead for doing nothing: 1.92423e-08\nSkipping BM_Factorial\nSkipping BM_CalculatePiRange\nSkipping BM_CalculatePi\nSkipping BM_CalculatePi\nSkipping BM_CalculatePi\nSkipping BM_SetInsert\nSkipping BM_Sequential>\nSkipping BM_Sequential>\nSkipping BM_StringCompare\nSkipping BM_SetupTeardown\nSkipping BM_LongTest\nBenchmarking on 4 X 2501 MHz CPUs\n2014/08/20-16:04:00\nCPU scaling is enabled: Benchmark timings may be noisy.\nDEBUG: Benchmark    Time(ns)    CPU(ns) Iterations\n\n$\n```\n. Merged! Thanks!\nNow to dig into why clang doesn't like std::regex :(\n. libstdc++ vs libc++, apparently.\n. It is, but libstdc++ isn't. Or something.\nWe could use libc++ if it is available but I'm not sure it is worth it.\nOn 23 Aug 2014 05:52, \"Matt Clarkson\" notifications@github.com wrote:\n\nI haven't actually used clang for a while but I was under the impression\nthey were fully c++11 and c++14 compliant? Guess it depends on version. At\nleast it will fallback to one of the other engines!\nOn 22 Aug 2014 19:34, \"Dominic Hamon\" notifications@github.com wrote:\n\nlibstdc++ vs libc++, apparently.\n\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/pull/41#issuecomment-53103162.\n\n\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/pull/41#issuecomment-53152034.\n. I think just the tag, sha, and dirty is enough. Nice work putting this together so quickly.\n. Thanks, @predmond. As much as I want to move to  we just can't abandon support for older gcc compilers.\n\nWhen g++-4.9 is available everywhere, this is exactly what we'll need though. I'll close for now but will resurrect when we can.\n. Is LTO going to work when building a static/dynamic library? I would have thought it would only work at the executable level once all used symbols are known.\nThe full 'strict' set would be -pedantic -Wall -Wshadow -Wpointer-arith -Wcast-qual -Wstrict-prototypes -Wmissing-prototypes but i think that might be too strict.\n. Merged this by hand but it seems not to have triggered. Closing manually.\n. Ah, thanks. I had to merge with the flags change so that'd be why.\n On Aug 6, 2014 10:55 AM, \"Pierre Phaneuf\" notifications@github.com wrote:\n\nFYI: for automatic closing of PRs, it has to be a straight fast-forward\nmerge of what's in the branch.\n\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/pull/45#issuecomment-51371758.\n. checking if drone.io offers an upgraded cmake.\n\nOn Thu, Aug 7, 2014 at 10:23 AM, Chris Kennelly notifications@github.com\nwrote:\n\nURL_HASH isn't supported by all versions of CMake 2.8.x, namely the one\nused by drone.io for automated builds:\nhttps://drone.io/github.com/google/benchmark/latest\nI added this in 6087edd\nhttps://github.com/google/benchmark/commit/6087edda9dbf6fcd91e319d3167a0f7a3f96dcc8\nbut then removed it in 92cd2e8\nhttps://github.com/google/benchmark/commit/92cd2e82af8563b828f331a1b1af7b2eab901de3\nbecause of this issue.\n\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/pull/46#issuecomment-51503457.\n. nope. damn.\n\nOn Thu, Aug 7, 2014 at 10:26 AM, Dominic Hamon dma@stripysock.com wrote:\n\nchecking if drone.io offers an upgraded cmake.\nOn Thu, Aug 7, 2014 at 10:23 AM, Chris Kennelly notifications@github.com\nwrote:\n\nURL_HASH isn't supported by all versions of CMake 2.8.x, namely the one\nused by drone.io for automated builds:\nhttps://drone.io/github.com/google/benchmark/latest\nI added this in 6087edd\nhttps://github.com/google/benchmark/commit/6087edda9dbf6fcd91e319d3167a0f7a3f96dcc8\nbut then removed it in 92cd2e8\nhttps://github.com/google/benchmark/commit/92cd2e82af8563b828f331a1b1af7b2eab901de3\nbecause of this issue.\n\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/pull/46#issuecomment-51503457.\n. pulled change out - also added a second build to drone.io that uses gcc4.8\nso we can enable c++11 features in that build.\n\n\nOn Thu, Aug 7, 2014 at 10:27 AM, Dominic Hamon dma@stripysock.com wrote:\n\nnope. damn.\nOn Thu, Aug 7, 2014 at 10:26 AM, Dominic Hamon dma@stripysock.com wrote:\n\nchecking if drone.io offers an upgraded cmake.\nOn Thu, Aug 7, 2014 at 10:23 AM, Chris Kennelly <notifications@github.com\n\nwrote:\nURL_HASH isn't supported by all versions of CMake 2.8.x, namely the one\nused by drone.io for automated builds:\nhttps://drone.io/github.com/google/benchmark/latest\nI added this in 6087edd\nhttps://github.com/google/benchmark/commit/6087edda9dbf6fcd91e319d3167a0f7a3f96dcc8\nbut then removed it in 92cd2e8\nhttps://github.com/google/benchmark/commit/92cd2e82af8563b828f331a1b1af7b2eab901de3\nbecause of this issue.\n\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/pull/46#issuecomment-51503457.\n. Hmm. when trying to merge:\n\n\n\nCMake Error at cmake/GetGitVersion.cmake:41 (string):\n  string does not recognize sub-command CONCAT\ni have version 2.8.12.2 (default available Ubuntu 14.04) and this was added http://www.cmake.org/pipermail/cmake-commits/2013-October/016563.html which might be later.\n. This doesn't match with the current expectation that users can supply their own reporters. Then a command line flag on the library doesn't make sense at all.\nInstead, it might be worth making the ConsoleReporter part of the public API and have it default to CSV output, with a boolean constructor argument to enable pretty-printing. Then library users can construct their own ConsoleReporter object and pass it in.\nAlternatively, the support methods that ConsoleReporter uses could be part of the public API for the benchmark results enabling much easier custom reporter creation.\n. It sounds like your use case is one where you want multiple possible reporters in the user code (as opposed to the library code) and then the end user to select with an option? That can be done with the current code.\nI'm wary about going down the path of providing multiple reporters in the core library. Even now the existence of the ConsoleReporter is something that I'd rather didn't need to exist, but I recognise it as a useful support class.\nPerhaps if the reporter was non-optional it would make the API expectations more obvious.\n. I agree it is a useful format, however I'm concerned that it sets a precedent for having other useful formats out-of-the-box. HTML would be great for CI, and some might prefer tab-delimited.\nI think having a CSV reporter is useful, I'm less convinced that it should be part of the core library.\nHaving said that, googletest does offer a --gtest_output option that outputs an XML report alongside the console output. Maybe that is the model to use:\n- add --benchmark_report option that takes a value <type>[:file]\n  - type can be 'xml', 'csv', 'html', etc\n  - file is optional and defaults to 'benchmark_report.<type>'\n- write to the given or default reporter and the report\n. See PR #95 \n. JSON has been added. It should be trivial to add CSV along similar terms.\n. Closed by #107 \n. I think this means that our test for regex is failing with zero-as-a-null-pointer-constant. Could you attach the CMakeOutput.log and CMakeError.log files please?\n. We have to compile the code to test for support. We could add the warning and fix the test to compile successfully under that warning too.\n. I just tested this and the tests pass with that warning flag enabled, so something else is going on.\n. and i reproduced it now.. using g++-4.8.\n. adding more details to the cxx feature check:\n-- Version: 0.0.0\n-- Performing Test HAVE_STD_REGEX\n-- Performing Test HAVE_STD_REGEX -- compiled but failed to run\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile\n-- Performing Test HAVE_POSIX_REGEX\n-- Performing Test HAVE_POSIX_REGEX -- failed to compile\n. Fixed in #86 \n. I think https://github.com/google/benchmark/blob/master/cmake/AddCXXCompilerFlag.cmake needs to be updated to support debug vs release. That's the only thing I can see (that a release-only flag is set after pedantic-errors).\n. Thank you for opening this issue!\nFor the record: http://stackoverflow.com/questions/8649828/what-is-the-correct-link-options-to-use-stdthread-in-gcc-under-linux\nI don't think the README is the right place for known issues with user code, but I think having this issue here and searchable will help people who run into the same problem.\n. @ckennelly can you review this?\n. @ckennelly can you review this?\n. @ckennelly can you review this?\n. @ckennelly can you confirm @zjx20 has signed the CLA?\n. Thanks for the patch!\nHave you checked the contributing documentation? You should at least have signed the CFA before we can merge the patch.\n. @niklashofmann can you make the change to only include it for re_test?\n. Why did you remove the extra compiler flag?\n. @niklashofmann Could you open an issue with the compile error and platform information? I haven't seen an issue with it compiling before.\n. turns out we do want googletest in benchmark_test so the CHECK macros can be removed from the public API.\nThanks!\n. Ah.. could an admin (@ckennelly) please flip the switch on benchmark at https://travis-ci.org/profile/google ?\n. Build is set up and running.\n. I'm trying to think what the right solution is here.\nMaybe benchmark should explicitly depend on glog and use the macros from there directly.\nAnother option is to check if they're defined before defining them.\nI think I prefer the first, but I'm open to other thoughts!\n. They are used in benchmark_test.cc which doesn't (yet!) depend on googletest. Maybe it should ;)\n. I have no way to test this so I can only trust that it's an improvement over the current state (and not a regression for other platforms).\nIt seems to build for me, but I'm going to wait for #66 to land to get this PR run through the full matrix, if you don't mind.\n. I wonder if I comment if it will bump the PR and kick off a travis build...\n. i don't know what appveyor is, but anything that builds windows continuously would be great.\n. Nice work! I'll take a look at the patch.\nI don't know if I have permission either - it may have to be a Google org admin. I'll poke around.\n. I also don't have access to the google-level appveyor. i'll ask around but in the meantime we should get this merged (assuming you're happy with it).\n. It's going to depend on your version of MinGW's gcc i imagine:\nhttps://stackoverflow.com/questions/16136142/c11-functionality-with-mingw\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Fri, Jan 12, 2018 at 4:52 AM, Tamar notifications@github.com wrote:\n\nIm currently trying to compile google benchmark for MingW and failing for\nc++11 features as mutex and threads, do you know how to resolve that?\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/pull/64#issuecomment-357230773, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMhxOX1DS8uDpOb2OND7PKgb_977kks5tJ1WFgaJpZM4C2SLZ\n.\n. Wonderful! Thank you!\n\nShould we also add:\ncompiler:\n  - clang\n  - gcc\n. could you also make the README.md change in this PR?\n. As per the CONTRIBUTING.md doc, could you please add yourself to the various contributor things and ensure you sign the CLA?\n. @ckennelly Can you confirm the CLA signing please? Once you do I'll merge this and finally kill drone.io.\n. I believe this is also fixed along with the fix for #65.\nIf you are still seeing it, please let me know the platform and compiler, and if you can attach a debugger for a stack trace, even better :)\n. Tracking here is fine. Thanks so much for the detailed stack.\nInterestingly I'm on exactly the same system and compiler and don't see the same issue.\n. If the example is compiled with clang and the library with g++ you're going to have issues.\nIf you change the compiler, you may have to reconfigure (rerun cmake) from a clean setup. Can you try that please?\n. I've tested this in Debug and Release on the following system:\n```\n$ cat /etc/lsb-release \nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=14.04\nDISTRIB_CODENAME=trusty\nDISTRIB_DESCRIPTION=\"Ubuntu 14.04.1 LTS\"\n$ cmake --version\ncmake version 2.8.12.2\n$ echo $CXX ':' $CC\nclang++ : clang\n$ clang++ --version\nUbuntu clang version 3.5-1ubuntu1 (trunk) (based on LLVM 3.5)\nTarget: x86_64-pc-linux-gnu\nThread model: posix\n```\ncmake output (from clean):\n$ cmake .\n-- The C compiler identification is Clang 3.5.0\n-- The CXX compiler identification is Clang 3.5.0\n-- Check for working C compiler using: Ninja\n-- Check for working C compiler using: Ninja -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working CXX compiler using: Ninja\n-- Check for working CXX compiler using: Ninja -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Performing Test HAVE_FLAG_CXX_14\n-- Performing Test HAVE_FLAG_CXX_14 - Failed\n-- Performing Test HAVE_FLAG_CXX_11\n-- Performing Test HAVE_FLAG_CXX_11 - Success\n-- Performing Test HAVE_FLAG_CXX_0X\n-- Performing Test HAVE_FLAG_CXX_0X - Success\n-- Performing Test HAVE_WALL\n-- Performing Test HAVE_WALL - Success\n-- Performing Test HAVE_WSHADOW\n-- Performing Test HAVE_WSHADOW - Success\n-- Performing Test HAVE_WERROR\n-- Performing Test HAVE_WERROR - Success\n-- Performing Test HAVE_PEDANTIC_ERRORS\n-- Performing Test HAVE_PEDANTIC_ERRORS - Success\n-- Performing Test HAVE_FNO_STRICT_ALIASING\n-- Performing Test HAVE_FNO_STRICT_ALIASING - Success\n-- git Version: v0.0.0-dirty\n-- Version: 0.0.0\n-- Performing Test HAVE_STD_REGEX\n-- Performing Test HAVE_STD_REGEX -- compiled but failed to run\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile\n-- Performing Test HAVE_POSIX_REGEX\n-- Performing Test HAVE_POSIX_REGEX -- success\n-- Looking for include file pthread.h\n-- Looking for include file pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE  \n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/dominic/git/benchmark\nand ninja (i don't use make, though i doubt it would matter) doesn't have any output really.\nthen:\n```\n$ test/67_test \nReading /proc/self/cputime_ns failed. Using getrusage().\nBenchmarking on 4 X 2501 MHz CPUs\n2014/11/14-21:43:00\nCPU scaling is enabled: Benchmark timings may be noisy.\nDEBUG: Benchmark           Time(ns)    CPU(ns) Iterations\n\nDEBUG: BM_StringCreation          2         22   23032372                                \nDEBUG: BM_StringCopy             12         32   15833581           \n```\nwhere 67_test is:\n```\n$ cat test/67_test.cc \ninclude \ninclude \"benchmark/benchmark.h\"\nstatic void BM_StringCreation(benchmark::State& state) {\n  while (state.KeepRunning())\n    std::string empty_string;\n}\n// Register the function as a benchmark\nBENCHMARK(BM_StringCreation);\n// Define another benchmark\nstatic void BM_StringCopy(benchmark::State& state) {\n  std::string x = \"hello\";\n  while (state.KeepRunning())\n    std::string copy(x);\n}\nBENCHMARK(BM_StringCopy);\n// Augment the main() program to invoke benchmarks if specified\n// via the --benchmarks command line flag.  E.g.,\n//       my_unittest --benchmark_filter=all\n//       my_unittest --benchmark_filter=BM_StringCreation\n//       my_unittest --benchmark_filter=String\n//       my_unittest --benchmark_filter='Copy|Creation'\nint main(int argc, char argv) {\n  benchmark::Initialize(&argc, (const char ) argv);\n  benchmark::RunSpecifiedBenchmarks();\n  return 0;\n}\n```\nI don't know what else to suggest. What version of g++ do you have installed? Just in case it's using the std lib from there and it's old (pre 4.6 or something)?\n$ g++ --version\ng++ (Ubuntu 4.8.2-19ubuntu1) 4.8.2\n. Ah yes, the std::thread needs pthread issue. I will add something to the documentation.\n. That's interesting... I just had an issue where the build was failing because Factorial was defined everywhere but ONLY called in DEBUG (it's called within the Factorial benchmark, DEBUG only, and in asserts, !NDEBUG).\nMaybe your build is not stripping asserts on NDEBUG?\nAnyway. thanks for the fix.\n. @ckennelly Could you take a look when you get some time? I'd appreciate your thoughts.\n. rebased. can we get this in?\n. That's odd. The assertion on 1129 should have failed first. This means that total_overhead was negative.\n1128  const double total_overhead = overhead * iterations_;\n1129  CHECK_LT(pause_real_time_, accumulated_time);\n1130  CHECK_LT(pause_real_time_ + total_overhead, accumulated_time);\n. we don't calculate overhead any more.\n. I don't have a 32-bit or iOS build to test against. Could you drop a log of the errors into this issue please?\n. i'm happy to drop the option from the cmake. we should make sure it doesn't also happen with c++11 though as the standard did drop ::gets.\n. Why did you close this? The issue still happens and will until we remove c++14.\n. Oh! I didn't see that commit go through. Thanks!\nOn Sat, Feb 21, 2015 at 3:13 PM, Eric notifications@github.com wrote:\n\nI did remove c++14 from the CMakelists.txt. did I miss something?\nOn Feb 21, 2015 6:02 PM, \"Dominic Hamon\" notifications@github.com wrote:\n\nWhy did you close this? The issue still happens and will until we remove\nc++14.\n\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/issues/74#issuecomment-75398086.\n\n\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/issues/74#issuecomment-75398561.\n. no reason to prefer anything in particular. i've always used _build.\n. added both, and added comments.\n. This should be the last one. Can someone take a look?\n. i don't have a bsd machine to test on. could you send a pull request with the right header in place for BSD? And any cmake changes necessary to detect bsd.\n. We can, but I'd rather not make these broad changes. Just updating when we touch the files is fine.\n. Can this be closed?\n. LGTM modulo failing build\n. if we do this, we only depend on googletest for a single EXPECT_NE in the factorial test to avoid an optimization. I wonder if we can do better ... ;)\n. LGTM modulo tests passing.\n. Looks good\n. this also speeds up tests, even though more have been added.\n- before: Total Test time (real) =  94.58 sec\n- after: Total Test time (real) =  79.25 sec\n. i checked your patch before writing this and didn't notice.\n\nhappy for yours to go in instead :)\n. It looks like we don't have tests for UseRealTime so it may be an oversight. I'll prepare a patch (unless you want to?).\nThe SetLabel issue will be fixed with PR #86\n. Agreed.\n. Do you have a sense if this adds overhead to short running benchmarks?\nYou can't use unique_ptr for the imp pointer, right?\n. If you could check the benchmark numbers before and after the change, that'd be great. LGTM.\n. LGTM\n. LGTM\n. superseded by #97 \n. I can only reproduce this in release builds.\n. a release build with CHECK enabled also doesn't show it, so that suggests there's some timing issue perhaps...\n. This line occasionally results in a negative walltime:\nreal_time_used_ += walltime::Now() - start_real_time_;\n. It must be due to the assumptions made about drift. We could update the drift more often, but for very short benchmarks it may be overwhelming.\nMy concern is that it suggests the non-negative results are inaccurate too.\n. I think the extra (minor) overhead is worth the consistency and reuse of std library.\n. I haven't been able to reproduce this in a long time either. I wonder if it's something unique to the OSX libs in 10.10.5.\n. Well, we could. But I thought this was a quick way to extend it to support a new format, then we can look at the surface area of the changes (printing the header and run data) and then bump the ConsoleReporter to a base class with TableReporter and CSVReporter (and JSONReporter) as subclasses.\nDo you think it's worth doing that jump now?\n. I could ... or we could wait until Monday when I'll be a Google employee again :P\n. closing as a CSVreporter along the lines of #100 will be better.\n. LGTM. Waiting for the build before merging.\n. LGTM\n. Woo #100!\nreviewing...\n. This is great - can you add something to the README.md to document it, maybe with some example output?\n. pull out into a separate file and then LGTM.\n. LGTM\n. this should go in if you still want to land it.\n. Can you add some of your test results to the PR here so we have them for posterity?\n. How can the names have commas? We generate the names from the benchmark functions.\ni'll try to fix up the labels.\n. This would have to fundamentally change the API I think. Currently, we just use macros to register user-defined static methods. With fixtures, we may have to change to the googletest model of defining classes from benchmarks.\nUnless... Perhaps a 'fixture' can inherit from benchmark::State and be passed to the benchmark instead of the state. Ie, you'd have something like:\n```\nstatic void BM_Basic(benchmark::State& state) {\n  while (state.KeepRunning()) {\n    ...\n  }\n}\nBENCHMARK(BM_basic);\nclass MyState : public benchmark::State {\n protected:\n  MyState() : _count(0) {}\n  ~MyState() final {\n    std::stringstream ss;\n    ss << _count;\n    SetLabel(ss.str());\n  }\nint _count;\n};\nstatic void BM_Fixture(MyState& state) {\n  while (state.KeepRunning()) {\n    ...\n    ++_count;\n  }\n}\nBENCHMARK_F(MyState, BM_Fixture);\n```\nWe still need a second version of all the top-level registration macros that take a fixture, and we'd want to check that the given fixture is-a state. This will also require more templating on State...\nWhat do you think, @EricWF?\n. _count isn't in scope - that was a bug :P\nWhy would the fixture classes have static lifetime? RunInThread would need to create the state on the heap as a pointer using the correct fixture type (instead of on the stack as it is now) but it would have the same per-run lifetime. no?\n. I've tooled around and can't find a syntax i like. I was hoping to have either State be subclassed (but registering and instantiating that correctly is hard) so then I tried to have State contain an 'inner state' of some user-defined type, but that was equally disruptive.\nIt's possible now for users to create an object with static lifetime and call methods on it outside the KeepRunning loop so it's not a very high priority to implement.\nMy ideal would be that the user defines a type that they pass in through registration and an instance of that type lives for the lifetime of a benchmark run.\n. This might end up putting overhead on the benchmark (due to the extra function call per iteration). To be tested.\n. i do think that fixtures have more boilerplate, however i also think that it's expected boilerplate. the 'timer doesn't run outside the loop' is a tricky concept for people to get out of the box.\nhaving two ways of doing setup/teardown makes me sad.\n. yes of course, that's why i have the comment above about the extra function call per iteration.\nmaybe we just need clear documentation around when to use a fixture and when to use the space around the loop. ie, state.\n. this doesn't seem to change anything for  Linux (which is good) but i'm surprised that the dll has to be in the runtime directory. i trust your windows know-how :)\n. I'm not sure how often you need to build both libraries.. what use case do you have?\ngiven BUILD_SHARED_LIBS is a builtin variable, we probably should respect it.\n. LGTM\n. I checked and this doesn't seem to make a difference to how pthread is linked in to static or shared libraries (checking that pthread symbols are weakly linked in both cases).\nAs such, i'm happy for this to land as soon as #117 does.\nThanks so much for the cleanup!\n. Merged manually. travis should pass - watching.\n. LGTM\n. This is a broader change than I expected for this feature. I'll take a look, but I expected just to track the min and max after all the runs when we do the mean/stddev calculations and output them in the report.\n. I think you need to do much less. At the point that we report the runs, we already have all the information for every run. All you need to do is, at the point we calculate the mean, also calculate the min and max and report them.\nThe rest of the changes are unnecessary as far as i can see.\n. LGTM\nWould the syntax for registering be any cleaner if we allowed a std::function as a third parameter?\nBENCHMARK_F(BaseClass, BenchmarkName, [&state] {\n  while(state.KeepRunning()) {\n  }\n})\nand then we just pass a std::function around internally.\nI know this might restrict the minimum compiler (though not by much) but does it block you from your work and do you think it's cleaner?\n. Your example isn't very compelling as I can do the find_in benchmark without value parameterising the benchmark, I think. Create the vector on the stack and pass in 42 as an Arg.\nOr did I miss something?\n. I'm still not seeing it. Unlike googletest, you have a preamble to set up local values for the benchmark. Now we have fixtures as well, i'm not seeing why you'd need the values to be passed in.\nI'm sorry, i'm clearly missing something.\n. https://github.com/google/benchmark/blob/master/test/benchmark_test.cc#L97\nThat tests push_back for different container types and inputs. It would probably be cleaner with value benchmarks, but i'm not sure what you're not getting.\nThe only reason i'm pushing back is to try to keep the API simple and to avoid adding things that are C++11 only.\n. why is 'test' being checked for coverage? i thought the point was that 'test' is used to cover 'src' and 'include'...\n. LGTM but @EricWF is the expert so i'll let them merge.\n. I'm fine with using coveralls and am looking into how to enable it now.\n. Badge is ready and coveralls is waiting.\n. I haven't been able to get permission to give access to the repo from appveyor. I will try again some day.\n. It seems that we may have permission after all. Want to put up the pull request again?\n. Done, i think.\n. Hm. Looks like CMAKE_BUILD_TYPE was empty?\ncan you retry with 'cmake -DCMAKE_BUILD_TYPE=Release' and let me know what happens?\n. I agree completely. I wonder if we never saw this because we set the build type explicitly in the travis build, and each of us has built at least once which has cached the build type. It's definitely worth looking in to but I'll drop the priority a little as you have a workaround.\n. Thanks so much!\n. @nickhutchinson are you still working on this?\n. That sounds reasonable to me. Would you like to prepare a pull request as per the CONTRIBUTING doc?\n. Please sign the CLA and reply and I'll take a look.\n. thanks so much!\n. Your fork works for me on linux/clang. Mind putting up a PR?\n. Agreed. Better to clean up the types than hide the warning behind casts.\nsee https://github.com/google/benchmark/issues/141\n. Looking at the delta between the current release and HEAD, we have 43 commits. Broadly speaking, they cover some minor bug fixes and documentation fixes, filters, fixtures, and cmake improvements. Oh, and coverage support.\nThis seems to me like it's enough for a release.\nLooking at the outstanding issues, we have this one, one to deprecate the KeepRunning loop, and windows support. I'm happy to release v1.0 with these outstanding. Anyone have an alternative opinion?\n. waiting a few more days for dissension. \n. tagging and creating 1.0.0 now\n. We might. If StartTimer is called by a thread during termination we might try to access it.\nA better fix would be to allocate the WallTimeImp on the heap as a pointer and never free it.\n. this is really weird... @EricWF has committed in the past without any problems. I wonder if it's because his email changed so he has to resign the CLA.\n. @EricWF are you planning more changes to this as suggested in #134, or is this fine to go in?\ni'm concerned that we might access Now during termination as it is called from StartTimer() which is called by each thread. The only case this might be a problem is if the user sigkills so maybe we need to catch that and wait for phase_condition_?\n. i'll look into the CLA issue.\nnow there's no size_t in reporter.h you'll need to remove that overload of FormatKV from json_reporter for travis to be happy. \n. Your email must be a member of bloomberg-contributors@googlegroups.com, and the commit in the PR must be associated with that exact email.\nIf this has been done and you're still experiencing issues, let me know.\n. i can't see it either .. i'm parroting what i'm being told. as long as the author field has the bloomberg address, that bit's sorted. we just need to match you up with bloomberg now.\nstay tuned! thank you for the patience, i really appreciate it.\n. ok .. more information: \"[the system is] unable to verify that https://github.com/jll63 and jleroy9@bloomberg.net are the same person.  This could be fixed by [them] adding their @bloomberg.net email address as an alternate email on their GitHub account.\"\nhave you? can you?\n. Thank you!\n. The author on the commit is set to eliben@users.noreply.github.com which presumably has not signed the CLA.\nCan you fix the author and repush the PR?\n. nice catch. how do you feel about adding a test for this? :)\n. blech. that looks awful. i wonder if it's going to bust master if we merge this.\nclearly not related to your change though.\n. looks like travis updated clang but didn't include the libstdc++ 4.6 patches. \nhttp://llvm.org/bugs/show_bug.cgi?id=12893\n. it's just not something that occurred to me might happen, given we're git-hosted. I'm more than happy to consider a PR. How do you intend to fix it, ie, what will you return for the version?\n. perfect. i look forward to the PR.\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Thu, Oct 1, 2015 at 9:33 AM, Ulvg\u00e5rd notifications@github.com wrote:\n\nUsing find_package which sets GIT_EXECUTABLE. If Git is found then proceed\nto check the tags, otherwise define version to v0.0.0 as the scripts do\ntoday. I tried this today and made it work.\nhttps://cmake.org/cmake/help/v2.8.12/cmake.html#command:find_package\nhttps://cmake.org/cmake/help/v2.8.12/cmake.html#module:FindGit\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/issues/144#issuecomment-144780672.\n. @EricWF it's one less piece of maintenance. we release through git with a version tag (signed) and it becomes the de facto version. a minor convenience. \n. perhaps there's a way to pull out the release version when tagging so we don't tag the wrong version.\n\ni agree it's cleaner and more obvious to have it in the code (and it can be logged on startup easier!), but i'm old and forgetful and prefer to automate whatever i can.\n. i'd rather not exit from within a library as i think that's unexpected. the documentation should be updated to reflect that.\nreturning the number of benchmarks run is fine, but may have an edge case if a filter is applied that accidentally filters all the benchmarks. then you'll have the tests run anyway unexpectedly.\ncan the application add its own flag to run benchmarks or tests or both?\nor perhaps it's best to just keep the mains separate and build two different binaries referencing the same tests/benchmarks, again at the application level.\n. maybe this isn't the right solution. we have max_iters in State as a size_t, but maybe it should be uint64_t to support high iteration counts on 32-bit machines. consistency is great, and i usually prefer to use size_t for sizes because, y'know, it's in the name. But I wonder if this will cause surprises in cross-platform benchmarks.\nwhat do you think?\n. /home/travis/build/google/benchmark/src/benchmark.cc: In function \u2018void benchmark::{anonymous}::RunBenchmark(const benchmark::internal::Benchmark::Instance&, benchmark::BenchmarkReporter*)\u2019:\n/home/travis/build/google/benchmark/src/benchmark.cc:683:21: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]\n. @EricWF wiser? you mean older, right? you have a better grasp on the constraints for the API and have the stricter limitations. size_t is fine if it's not causing issues.\n@ulvgard for items and bytes processed i can see uint64_t being the right measure. What do you think, @EricWF?\n. Sounds great. Something needs to change in this PR, right?\n. yes please\n. bytes and items processed should remain (u)int64_t. the rest can go, especially taking into account the overhead of 64-bit math on 32-bit systems.\n. At some point we started replacing size_t with int64_t explicitly, and I think we're in a position where everything is correct, and we shouldn't monkey around with it further.\nIf you agree, @EricWF, please close this.. please make sure the author email on the commits matches the expectations from the corporate CLA.\n. possibly. we shall wait and see and i'll poke the bear on this side if nothing happens.\n. @ulvgard sounds good in the case that git isn't available.\n. @googlebot he signed it. check again.\n. @ulvgard whenever you can, please reply \"I signed it!\" so googlebot updates.\n. the bot still thinks you didn't. maybe the commit author doesn't match expectations?\n. can you poke googlebot please? just reply 'I signed it!' when you have.\n. change looks good. when the CLA is signed i'll pull it in.\nthanks!\n. should we be linking against libc++ instead when compiling with clang? (as per the askubuntu site)\ni'm happy for this change to land as nothing breaks, but i'm curious if there's a better long-term fix.\n. nice catch. thanks.\n. We could change to size_t, or int64_t, but we'd end up truncating in some cases as timespec ultimately takes int.\nI'm not sure that choosing int64_t for the constants was a good move in the first place as even `kNumNanosPerSecond comfortably fits in 32 bits. The number of nanoseconds required to overflow the 32-bits would be about an hour, if i did my math correctly, which i'm comfortable with as a max sleep.\nWe should check for overflow, or at least potential overflow, before the multiplications.\n. see, this is why i shouldn't be allowed to write anything on the internet before coffee. thanks, @pleroy.\nin SleepForMicroseconds we take an int (32-bit) and mod/multiply by 1000 for the nanoseconds. We divide by 1000000 for the seconds. The longest sleep time we can have is, then, 2^32/1000000, which is indeed 4.3 seconds.\nThere's a subtle bug in SleepForSeconds which takes a double (ie, 64-bits) and silently truncates to a 32-bit int.\nI think the correct approach is to take int64_t, do the math in 64-bit, and then check for overflow before truncating to 32-bits right before calling the library functions.\n. @guanqun can you make the changes from the above discussion, or would you rather abandon this PR and i'll create an issue for the above?\n. Sounds reasonable to me.\n. Thanks!\n. This looks reasonable, but what is the warning?\n. wait.. double isn't 64-bit on your machine? i think maybe the compiler is being overly conservative and not checking bit-lengths before warning, but ok. it should be safe on every sane platform to do this conversion and therefore the cast is fine.\n. what was your command line for building? did you -lpthread as per the documentation?\n. SG. Patches welcome :)\n. @RYOBG https://help.github.com/articles/fork-a-repo/ is a good starting point. if it's too much, let me know and i'll make the changes.\n. Please also add yourself to CONTRIBUTORS and AUTHORS in this patch\n. thank you!\n. I think the work you've done is great for your use case, and I appreciate you documenting it so well.\nI don't think there's any need to bring that into the library. If we did, we'd either bring in a dependency that I'd rather avoid, or add complexity to an API that is, frankly, already too complex for what it's doing. \n. when you think about how much work memcpy is doing, the number of bytes being copied is the length passed in, which in this case is state.range_x().\n. ob SO post: http://stackoverflow.com/questions/14777040/extending-the-stdchrono-functionality-to-deal-with-run-time-non-compile-time\n. oh ffs. thank you, @EricWF.\n. i prefer it to be per benchmark to cover your example: to get the CPU time for a baseline, and real time numbers for comparison. ie, you can register the same benchmark twice, once under each timing mode, and get everything you need.\ndocumentation is a good point though. i'm surprised it's not in the readme already. i'll add something (unless you want to)!\n. thanks!\n. were the commits made with that email as the author?\n. looks like the commits are from your bloomberg.net address.. the address is still in the github settings, right?\ncan you try poking cla-bot by claiming you have signed (as above)?\n. Are you still a member of bloomberg-contributors@googlegroups.com as jleroy9@bloomberg.net?\n. i can't check... i think you go to groups.google.com and look for it. if you're in it, you can see it.. maybe? i really don't know.\n. (i'm working with the cla developers to figure out what's going on)\n. Nice catch! Once you sign the CLA and let googlebot know you have, I'll get this merged.\n. @alycm can you confirm that you signed with the same email address as you used on these commits?\n. @alycm i'd really like to take this but i can't until clabot says it's ok. can you double check the email address you signed as, and the one on all the commits in the branch?\n. thank you!\n. Policy \"CMP0054\" is not known to this version of CMake.\n. we use 2.8.11 due to broad availability. you'll have to work around it, i'm afraid.\n. appveyor should work on pull requests and comments, just like travis.\n. I saw that. I have the settings the same as other hooks. I'll keep digging.\nOn 14 Feb 2016 10:26, \"Arkady Shapkin\" notifications@github.com wrote:\n\nAppVeryour does not want to build new commits\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/pull/175#issuecomment-183905271.\n. odd. https://ci.appveyor.com/project/google/benchmark/history shows that\nall PRs are being included, but not shown in the README or in the github UI.\n\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Tue, Feb 16, 2016 at 9:28 PM, Arkady Shapkin notifications@github.com\nwrote:\n\nCan you try start build manually?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/pull/175#issuecomment-184863431.\n. readded webhook. let's see.\n\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Wed, Feb 17, 2016 at 12:08 PM, Dominic Hamon dominic@google.com wrote:\n\nodd. https://ci.appveyor.com/project/google/benchmark/history shows that\nall PRs are being included, but not shown in the README or in the github UI.\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Tue, Feb 16, 2016 at 9:28 PM, Arkady Shapkin notifications@github.com\nwrote:\n\nCan you try start build manually?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/pull/175#issuecomment-184863431.\n. LG. I'll merge when travis gets around to finishing.\n. LG. I'll merge once travis completes.\n. cmake -DCMAKE_BUILD_TYPE=Release works for me to get rid of the warning. Could you try that and let me know?\n. added a section to README to cover this. hopefully :)\n. This might be a good use for fixtures. You can generate your keys in SetUp and store them in the fixture object as state.\n\n\nsee test/fixture_test.cc for an example.\n. does your benchmark do one lookup per 'keep running' iteration? do you want to look up every key?\none thing that comes to mind is that if we passed the state to 'SetUp' you could generate a number of keys based on state::range_x() and then loop through them in the keeprunning loop (like BM_SetInsert in the benchmark_test.cc file). Then you just need to set the number of processed items to state::iterations() * state::range_x(). \nAlternatively, you could pick a random key from your set of keys every time through the loop, but you might get more variation between benchmark runs.\n. I did some testing (see #181) which required passing a const version of the state to SetUp. I'm ok with this change, by the way.\nIt's clear from the results that there is a significant impact for small maps. So much so that I'm thinking of deprecating PauseTiming and ResumeTiming now we have the fixtures. I think they cover every use-case for pausing the timer. At least the ones I expected.\nIs my test similar to what you had in mind?\n. @pleroy I think the cost is likely in the mutex and barrier to handle multiple threads pausing/resuming. Though yes, the CHECK doesn't help.\n. Merged. I think we'll remove PauseTiming and RemoveTiming.\nRegarding the iterations -- there's an old version that allowed number of iterations to be specified but trying to create an API that supports time-based and iteration-based constraints is complicated. Given they're benchmarks, in general we're interested in doing as many iterations as possible to get good statistics. As such, setting time-based constraints is a more reasonable API.\nI understand your specific requirement, but I hope that you can get the results you need with the existing set up.\n. I'm not sure there's much we can do here. I'm happy to have the conversation on record but I'm going to close the issue.\n. It sounds reasonable. Can you add a test that uses the new parameter, both to test it and serve as an example for others?\n. Thanks!\n. @EricWF does this seem reasonable to you? See the linked issue #179 for the background.\n. @pleroy great feedback... can you explain the use-case you have that uses this? Or is it that you don't see the impact so you're happy with the simpler API?\n. And both are already taking long enough that you can discount the overhead, or you are less interested in the absolute numbers rather than the relative numbers over time so the overhead is ignorable, i assume.\nOK, sounds good. Dropping the PR.\n. Thanks!\n. just waiting for travis...\n. I would really like to see this happen too! I know enough about cmake to be\ndangerous so a PR would be delightful.\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Tue, Mar 14, 2017 at 1:11 PM, Austin Deric notifications@github.com\nwrote:\n\nI would also like to see this repo work with cmake's find_package function:\nhttps://cmake.org/cmake/help/v3.0/command/find_package.html\nThis way we can reference this package in other cmake packages. If this is\nsomething y'all are interested in, i can submit a PR for it. I can do\nsomething similar to this:\nhttps://cmake.org/Wiki/CMake/Tutorials/How_to_create_a_\nProjectConfig.cmake_file\nThanks and great work!\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/188#issuecomment-286544893,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMkfMu6lXRXV-ZElXJQ1ci0LfQXVnks5rlvSLgaJpZM4HjjU9\n.\n. I don't think any of the methods we use for timing will get more precision. I doubt we get out of the way enough to allow 2ns to be a reasonable measure.\n\nCan you force your benchmark to do the work N times per run and then divide the time measured by N?\n. How would it work? output an HTML file (+JS/CSS) in some temporary folder that someone can use with python -m SimpleHTTPServer?\n. oh, neat.\nif you'll allow me to bikeshed: having a chart per benchmark would be great... would the x/y ranges be shown along the x-axis? how about threaded benchmarks?\ni think this is a really neat idea and i'd love to see more examples or thoughts about how the charts could be displayed.\n. are you generating this from the existing output?\nbenchmark already supports multiple reporter types.. you could try reading the json version, for example.\nalternatively, you could add an HTML reporter that has full access to the raw data as it is generated and can write out whatever you need.\n. well... that can change. it's a matter of passing b->arg1 and b->arg2 to the BenchmarkReporter::Run instance created for a given run.\nthat might actually be a better way of managing the reporting anyway, instead of changing the benchmark name.\n. I'd go with 3 as the end goal and 2 as a reasonable interim.\n. I'd like to see it land still.. I'd really like it to happen, but as an addon to the JSON output. quick-bench.com has done some of the work already, but i think there's something that could be done locally to generate an HTML report given a number of (dated?) results.. company allegiance, and ease of reintegration into google's source. ie, having a parallel build setup is overhead that i was trying to avoid by using cmake, but there are benefits to having a bazel build, but they're entirely selfish :)\nthe benefit of cmake over a script or makefile are two-fold: generation of native \"projects\" for MSVC users, XCode, etc, and dependency enforcement. as such, i don't think we can have just non-cmake.\ni don't actually know if bazel has something similar or relies on all dependencies being integrated into the project so it's hermetic. ie, if we'd need to have submodules/subtrees for any libraries on which we depend. if it does, then maybe a simpler script is the way forward?\n. That's an interesting compromise.\nI have a strong preference for not requiring configuration, so i'd refactor things to run all the runs for a benchmark arg set and then decide which unit to present for the set.\nHowever, if @NewProggie wants to add this as an option and see how it plays out, i'd love to see it.\n. Thank you!\n. yes, please. i'd like to avoid having inline javascript. calls out to cdns from the html reporter should be fine.\n. as you change the code, it should rerun anyway. restarted the build.\n. when i run this, there's no output for ages and then:\n$ test/benchmark_test --benchmark_format=html\n\n3D plotting is not available at the moment!\nand that's it.\nno idea where the HTML files are or how to see them. maybe some output would be useful?\n. it'd be nice to have some output other than warnings while the benchmarks are running (ie, when the reporter gets a request to add a report) so the user knows something is happening.\nnow i have output (yay!) i have some questions.\ni'm surprised that each benchmark is on the same graph. i expected a section for each benchmark name, with a graph for each metric.\nit doesn't really make sense to combine the graphs for all the benchmarks given the wide range of times and arguments, and the different meanings of the arguments, across benchmarks.\n. > > it'd be nice to have some output other than warnings while the benchmarks are running\n\nI agree. However, if this is an issue, don't the JSON and CSV reporters have the same issue? (Assuming the use in all cases is via redirection to file.)\n\nThey do not, precisely because they output to stdout.\n\nMy opinion: do one of\nCreate a general progress report output on stderr\nHave json, csv, and html write to a specified file and do progress reporting on stdout via the console reporter\nBoth solutions seem like a separate issue to solve.\n\nHTML should probably output to stdout and then need to be redirected, if we're following the pattern of the JSON and CSV reporters.\n\n\nit doesn't really make sense to combine the graphs for all the benchmarks given the wide range of times and arguments, and the different meanings of the arguments, across benchmarks.\n\nThis seems hard / impossible to solve in a general way. The best presentation depends a lot on the benchmark itself. Consequently, one would need many configuration options to influence the way the data is presented. The motivation behind the current HTML reporter was to have a quick and easy path to some kind of charting. If you want fully flexibility use CSV or JSON and a dedicated charting program. This extra step is too much for the quick and easy charting use case.\n\nis it a significant change to append a 'set' of charts for every benchmark name encountered? if you're building up the HTML as the runs are sent to the reporter it should be fairly easy to start a new set of charts whenever the name changes. Unless your templates assume one set of charts, which is unfortunate.\n\nWhat is really handy is to use the HTML reporter together with the regex filter to generate separate charts of the results you want to compare.\n\nThat's a good use-case, but out of the box not obvious. I'd like users who try it to get something less surprising when they see it.\n\nThat said, there sure may be a better one size fits all default than the current one.\nGiven e.g. three benchmarks without x/y ranges, should those really have their own graph? They would each have a single bar, no?\n\nyes. and it might be that the HTML reporter isn't a good fit for the single benchmark case. having a single bar is fine as it still shows the magnitude. i think the real power of the HTML reporter is comparing across x/y ranges or threads.\n\nSo it seems the HTML reporter must combine multiple benchmarks with different names into the same chart. Can you think of any discriminator that the reporter could use to split benchmarks into different charts? Currently, it only splits via existence of an x range into line chart vs. bar chart.\n\nas above, per benchmark name. \n. I think this is much better solved by using JSON as an intermediate format and building charts around that (with some processing). This should then be a separate tool to the core library.. @OwenArnold can you take a look at the pull request to see if it is satisfactory?\n. Hello\nAre you trying to use it as a library for your own application, or just test the library out?\ncmake generates build configurations. it may default to 'Makefile' which means you just need to write 'make' afterwards. If it is using ninja, then make sure you have the right ninja package (https://ninja-build.org/)\n. It's a very interesting use-case.. short-term, you could just use state.SetLabel to set the total time spent on the GPU, or divide it yourself outside the KeepRunning loop to get the mean. something like:\nc++\nstatic void BM_GPU(benchmark::State& state) {\n  double mean = 0.0\n  while (state.KeepRunning()) {\n    mean += ExecuteAndMeasureShader(state.range_x());\n  }\n  state.SetLabel(\"mean time: \" + std::to_string(mean/stat.iterations()));\n}\nThis should work without any changes, though it doesn't get you access to the stats breakdown of course.\nWhy do you want to disable the manual timing? A call to SetIterationTime could just override whatever the automated timing would return. It also might allow to supplement it, so in the case of something that uses both CPU and GPU, you would get both times.\n. alright.. start with the simpler approach (maybe you can just revert your local branch to that version, and avoid creating a new PR?) and then i may take a crack at extending it.\n. that call should probably be conditional on the lengths of rate, items, and report_label so it only prints them if they exist.\n. if benchmarks call the SetBytesProcessed or SetItemsProcessed the benchmark will report that alongside the timing. There's an example at https://github.com/google/benchmark/blob/master/test/benchmark_test.cc#L84\nSetLabel is useful for appending arbitrary information. Maybe the output of a test: https://github.com/google/benchmark/blob/master/test/benchmark_test.cc#L64 \n. i'm not sure what you've tried, but this is a good starting point: https://cognitivewaves.wordpress.com/cmake-and-visual-studio/\nonce you have a solution file, you can build as normal in VS.\nwhat are the symbols? it might be our code :)\n. this is a very intrusive change. it's the sort of change i'd like to support without any changes to the implementation. for example, can we define a custom main in whatever binary you link the benchmarks in? similarly, can you use a fixture or a custom reporter to calculate the world max time for reporting?\n. that would be fantastic. if not a null reporter, being able to register your own reporter would be even better :)\n. wonderful!\nif you could write up something somewhere about how to benchmark MPI, i'd be happy to include it in the documentation here, or point to it if it's hosted elsewhere.\n. I think it'd be a fine addition as README.mpi.\n. CHECK should do it\n. maybe i'm misunderstanding, but this seems like a lot of overhead for something that can already be done using custom arguments.\n``` c++\ntemplate\nstatic void CustomArgs(benchmark::internal::Benchmark b) {\n  for (size_t i = FROM; i < TO; i = TICK) {\n    b->Arg(i);\n  }\n}\nBENCHMARK(BM_StringCompare)->Apply(CustomArgs<1, 100>);\nBENCHMARK(BM_StringCompare)->Apply(CustomArgs<1, 100000>);\n```\nBM_StringCompare/1              9 ns          9 ns   74041950\nBM_StringCompare/2             10 ns         10 ns   69762806\nBM_StringCompare/4             10 ns         10 ns   69729450\nBM_StringCompare/8             10 ns         10 ns   71866370\nBM_StringCompare/16            10 ns         10 ns   69295267\nBM_StringCompare/32            11 ns         11 ns   66399803\nBM_StringCompare/64            13 ns         13 ns   55563934\nBM_StringCompare/1              9 ns          9 ns   73928564\nBM_StringCompare/2             10 ns         10 ns   69824043\nBM_StringCompare/4             10 ns         10 ns   69811509\nBM_StringCompare/8             10 ns         10 ns   71892203\nBM_StringCompare/16            10 ns         10 ns   69032850\nBM_StringCompare/32            11 ns         11 ns   66380283\nBM_StringCompare/64            13 ns         13 ns   55726716\nBM_StringCompare/128           15 ns         15 ns   46087197\nBM_StringCompare/256           20 ns         20 ns   35720664\nBM_StringCompare/512           28 ns         28 ns   25162568\nBM_StringCompare/1024          38 ns         38 ns   18331230\nBM_StringCompare/2k            65 ns         65 ns   10713356\nBM_StringCompare/4k           118 ns        118 ns    5947071\nBM_StringCompare/8k           219 ns        218 ns    3210317\nBM_StringCompare/16k          500 ns        500 ns    1399524\nBM_StringCompare/32k         1087 ns       1086 ns     638995\nBM_StringCompare/64k         2169 ns       2167 ns     322625\n. I can imagine calling some method on state when the error occurs.\nwould you want the single iteration to be dropped from the results with a warning, or would you expect the entire benchmark to be dropped?\n. i think this is a really good idea. it should be easy to add something to the run such that the reporters can handle errors appropriately. having it per-iteration is probably best, though it will be spammy in a critical situation (network down). i think this is ok.\n. Perhaps just 'ignore iteration' is the best option as in critical failure cases it will run no iterations. we could potentially have a test if we ran 0 iterations? or annotate the run as containing an error?\n. IgnoreIteration() would skip the current iteration (which is actually just a return.. but it could append an error somewhere.\nIf every iteration has an error, then we end up skipping all iterations. We may want to append some error information to the report somewhere.\n. if appveyor would run against pull requests it would make me so happy.\n. so.. this looks fine to me, but i'm mostly flying blind. given travis doesn't break, and appveyor doesn't want to run until things are merged i guess i'll just go ahead and merge it :D\n. perfect. thanks, @BillyONeal \n. wonderful! thank you!\n. is this ready to merge or were you going to add the 'on iteration x' to the error output?\n. resolve yo conflicts\n. merging now\n. running the skip_with_error_test doesn't allow for benchmark_format override:\n$ test/skip_with_error_test --benchmark_format=json\nRun on (12 X 3501 MHz CPU s)\n2016-05-25 09:04:15\n***WARNING*** CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead.\n***WARNING*** Library was built as DEBUG. Timings may be affected.\nBenchmark                                    Time           CPU Iterations\n--------------------------------------------------------------------------\nBM_error_before_running             ERROR OCCURRED: 'error message'\nBM_error_during_running/1/threads:1 ERROR OCCURRED: 'error message'\n. i think the issue is the test. other tests allow override.\nmaybe something in your custom main is funky? \u00af_(\u30c4)_/\u00af\n. can you make sure your commit authors are set correctly so the cla bot stops complaining?\n. can you resolve conflicts so i can merge please?\n. I like it.\n. it's showing me diffs including the error reporting change. shouldn't those be present on the left side too?\nare you all rebased?\n. attempting manual merge...\n. i've been unable to persuade appveyor to run consistently, which makes it hard to support windows. also, myself and @EricWF have limited access to windows machines.\nI'd say it's supported at a best effort level. I accept all patches to improve/fix VS support (and specifically MSVC) but i can't generate them.\n. LGTM .. feel free to merge if you're happy with it.\n. @BillyONeal any chance you can try this to see if it works for VS? I have no access to a VS machine.\n. @BillyONeal ok. i know i fixed something, at least. but i think there's more to do with the noexcept macro and cmake.\n. slowest. edit compile loop. ever.\n. I confirm that these commits were authored by me.\n. @EricWF do you mind confirming that you're ok with commits being contributed? i managed to rebase and pull your commits into here somehow, even though they've already landed in master.\n. i wish i could clean up the commit history here... @EricWF whenever you can, please persuade cla-bot that i'm not stealing your work :)\n. yeah, i was seeing that on appveyor too. haven't managed to come up with a good solution yet that's clean.\nif you have the time, go for it. otherwise i'll keep hacking on this.\n. @BillyONeal right, that's something close to what i had. the const_cast bothered me, and the version check (instead of BENCHMARK_OS_WINDOWS or COMPILER_MSVC) but it may be that this is what we need to do.\nyou can land that independently of this PR, right?\n. @EricWF no problem. can you convince cla-bot?\n. @BillyONeal please do. i could do it but you wrote the commit already :)\n. iirc, this is a libstdc++ issue.. fixed in newer versions than that which is on every travis machine. \n. https://ci.appveyor.com/project/google/benchmark/build/205 agrees!\n(i can't make it comment on the PR for some reason)\n. thanks!\n. starting the manual merge\n. it's not exactly the same, but it is related. your solution sounds reasonable.\n. @EricWF please take a look\n. the travis failure is the clang/gcc 4.6 issue. @EricWF you're best placed to know what if anything we can do about that.\n. i always thought it was quite obvious that the setup/teardown would be around the entire benchmark method. The use of a while loop should be a clear signal that the code inside can be called more than once.\ni don't think there's any violation of expectations here: your method is being called within the setup/teardown wrapper. the only issue is that you didn't expect the while loop to loop, as far as i can tell.. SkipWithError is relatively new and we didn't think about applying it to fixtures. I'm a little concerned with making the State mutable during fixture creation. The other alternative is to return a boolean, but that puts boilerplate on everyone using fixtures.\nWhat sorts of errors are you seeing at fixture creation that aren't catastrophic?\n. you can use SkipWithError within the test itself, or you can just assert(..) in the fixture's SetUp. That's what I mean by catastrophic: if the fixture SetUp can't complete, there's no point in running any of the benchmarks, right?\n. it sounds like this might be a generalisation of the manual timing feature (https://github.com/google/benchmark#manual-timing) .. is that accurate?\nIf all you want is a double counter with a name,  that should be straight-forward enough and flexible enough for many uses. I assume you'd also plumb it through to the mean/stddev calculations for multiple runs.\nPlease also think about how these would be reported by each of the different (and custom) reporters.\nIt sounds good though.. I look forward to seeing a patch.\n. Thanks for so much detail! Here are some thoughts:\nmake the user counters a std::string and double as it's going to be more general. If people need to do something in the hot loop (they shouldn't as the counter should be one per benchmark invocation, not per loop) they should accumulate it in some local variable and set it outside the KeepRunning loop.\nyou probably want to add the counters to ThreadStats instead of passing state in to RunInThread. Take a look at how the items_processed thing is handled as i think this is along the same lines.\nin fact, it might be possible to replace items_processed and bytes_processed with the general form that you're proposing here. what do you think? \n. yes, i think that would work. it feels like the items processed is a subset of the user counters, though we do currently report it as a rate given a count, which isn't going to apply to every counter. i'd suggest extending counters to allow reporters to also calculate rates, but that may make this too complicated.\n. 1. put a new header line before every benchmark.\nlet's go through each reporter in turn, actually:\n1. json: easy. benchmarks have fields if they have the counters\n2. console/tabular: easy-ish. we can have a header per benchmark, as long as we know the user counters at the time we start the benchmark... worst case, only output the benchmark data once we do know the user counters (after the first run?)\n3. csv: hard. maybe we don't output any csv data until we have it all, and then we have the header row contain all counters?\n. thanks!\n. thank you!\n. i'd be happy to see this as part of the project, absolutely.\nbeing able to name a run (or provide a filename to which to write) would help simplify it even further.\ni'm ok with python.. i prefer go, but it requires compilation which for this case isn't that helpful.\nsome careful attention is required to the definition of 'faster than' if we want to aggregate over ranges.\nthere was some work done on an HTML reporter in #193 that seems to have stalled, but i really liked where that was going.\n. you can avoid the files entirely here... just run a subprocess with the flag to output json, capture stdout, and you're done.\ncan you start to put a PR together? we should probably \"install\" this script somewhere too as we do the header/library.\n. If you specify 'repetitions' we will calculate the mean and std dev across each repetition. Ie, we take the average per-iteration time, and then find the mean/std dev of those averages.\nit sounds like you're suggesting we should have the std dev, max, and min, of the iterations over a single run. i think that's a great idea, and with the more flexible reporters it should be something that can be easily included (the tabular reporter has limitations due to laying out columns).\n. i really like this, thank you. however, there may be a limitation on using std::vector in the api header. @EricWF for more details on this.\n. i'm going to have a crack at resolving conflicts and do some local testing before merging.\n. thank you so much!\n. appveyor failure is some weirdness with ctest. i'm working on a fix (slowly) but not making much progress.\n. no, i have no issue with the coverage drop or trying to test code that writes files. i'm still wary of the command line flag confusion. \n. LGTM\n. no matter how long it has been, we have users who do not have C++11 support. dropping them would be unfortunate.\nhaving said that, if we state that as of version X we are only supporting C++11, that just leaves them on an old (working) version. What do you think?\n. oh, poor gcc 4.6. only around due to the long term ubuntu support, iirc.\ni completely agree with everything you've written.\n. trying to find a way to kick travis into building a new version with the latest travis...\n. ah, it passed the 4.8 anyway.\n. Thank you for sticking with us on this. I understand you've gone back and forth on the API, but it's a big thing to introduce and I want to make sure it's going in the right way.\n. A strong preference for the latter with 'User counters' as the header. For CSV it's a little trickier, but we can start with user counters containing the list of user counters as tabular. JSON is easy.\n. paging @EricWF and @pleroy .. do you have any thoughts?. @biojppm are you still planning on getting the tests passing in debug?. dooo eeett. to confirm: we don't need to install cmake or clang any more?\n. i'm not sure about a script deciding to run executables to get the output. it seems to me that one could do that directly from a shell and this script is much simpler when it can ignore being passed executable files.\nand yes, my unix is showing.\n. LFTM\n(Looks fine to me)\nEvery cell in my unix body is screaming but it is more useful in its current form.\n. can this be more explicit: --benchmark_aggregates_only\n. appveyor now failing because http://stackoverflow.com/questions/14191566/c-mutex-in-namespace-std-does-not-name-a-type\nLGTM\n. set your include path to the include directory too. you might want to check the build flags and platform definitions.\n. and GCC 4.7 has fundamentally broken C++11 support. IIRC it mishandled the feature macro in some key cases.\n. LGTM\n. having custom benchmark prefixes makes it much harder to post process the json files though, because you have to then remove them.\nyou should output to different json files with the prefix as this will also allow you to use the processing script that @EricWF recently added to the project, but also will allow you to keep things obvious at the json file level. the contents of the json files will be much more readily comparable.\nwhy is this not an option for you?\n. i don't think it's your change. it looks like complexity test has been broken for a while (https://ci.appveyor.com/project/google/benchmark/build/361/job/jq6e4wp072gtu90i)\n. so much cleaner :)\n. LGTM\n. LGTM\n. thank you so much for bothering with this, even though it's a pain.\nthere is another alternative: should we replace cmake with bazel or something else?\n. LGTM assuming CI passes\n. the direction looks great. it's very clean and clearly more correct.\n. this lgtm. have you run a comparison for all the benchmark tests to see how the timing changes?\n. wonderful!\n. nice :)\n. can you share the project you're actually running? i have a feeling you're missing a library in your linker command line (maybe pthread).\n. Can you post the link command line that it ends up generating? it may be that you need to change the order of the link libraries. from test/CMakeLists.txt in the benchmark repo:\ntarget_link_libraries(${name} output_test_helper benchmark\n          ${BENCHMARK_CXX_LIBRARIES} ${CMAKE_THREAD_LIBS_INIT})\n. iirc, the 2.8 cmake module just creates a small c++ file and tries to build it. it then exposes the libraries and headers in variables. Later versions of cmake might be smarter?\nour library tests have things in the right order. it's when external binaries want to use our library that they have to do the right thing.\n. if we're going to use u for micro (and n for nano, etc) then why would we not use m for milli?\ni'm not sure this makes things clearer.\n. Purely short on time. i'll take another look.\n. threshold 0.01 looks better for this case at least:\nHumanReadableNumber(1.05e-3)=  1101u   |  0.00105   |    1101u\n. thank you! and sorry for the delay.. Using clang++-3.5 on a linux desktop, (12 x 3501 MHz CPU s) with your code, i can't reproduce:\n$ clang++-3.5 ./vpush.cc -lbenchmark -O3\nSUCCESS\nxoanon:~/git/benchmark/test [master *] $ ./a.out --benchmark_repetitions=50\nRun on (12 X 3501 MHz CPU s)\n2016-10-05 11:50:00\n***WARNING*** CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead.\n***WARNING*** Library was built as DEBUG. Timings may be affected.\nBenchmark                     Time           CPU Iterations\n-----------------------------------------------------------\nBM_VectorPush                89 ns         89 ns    7808372\nBM_VectorPush                89 ns         89 ns    7808372\nBM_VectorPush                89 ns         89 ns    7808372\nBM_VectorPush                89 ns         89 ns    7808372\nBM_VectorPush                89 ns         89 ns    7808372\nBM_VectorPush                89 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                89 ns         89 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                89 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                89 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                89 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                89 ns         88 ns    7808372\nBM_VectorPush                89 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                89 ns         89 ns    7808372\nBM_VectorPush                89 ns         89 ns    7808372\nBM_VectorPush                89 ns         89 ns    7808372\nBM_VectorPush                89 ns         88 ns    7808372\nBM_VectorPush                88 ns         88 ns    7808372\nBM_VectorPush                89 ns         88 ns    7808372\nBM_VectorPush                89 ns         89 ns    7808372\nBM_VectorPush                89 ns         88 ns    7808372\nBM_VectorPush_mean           88 ns         88 ns    7808372\nBM_VectorPush_stddev          0 ns          0 ns          0\nit's possible this is an OSX issue with the timer. maybe related to #292?\n. Is it possible to share the code for BM_graph?\n. From the original message: \"Did you forget to call SetComplexityN?\"\nI don't see a call to that in your example.\n. i don't know how we didn't catch that earlier. thank you!\n. assuming the json format is for machine reading, i'd go with 0.12.\n. From what i can see, this is fixed in the most recent code. At least, the regex being used for csv_report now uses a regex that doesn't assume a decimal point.\nAre you on the latest?\n. Thanks!\n. Thanks!\n. i'd expect some sort of error at least... given i don't have a centos system to test on, can you run in a debugger and see what happens when you hit a print statement.\n. i reproduced this with grpc. the regex is failing to match even when set to the default '.'\nmy guess is that grpc is building with make so we're not setting the regex machine correctly (we do some hoops in cmake to select it).\nstill debugging.\n. grpc hard-codes STD_REGEX. looks like POSIX_REGEX works better.\n. if this also works for you @Dekken, we should consider how we can make this obvious for non-cmake users.\ni don't believe it's possible to pick a reasonable default for all platforms, but we could maybe start by documenting the issue?\n. great.\ni think then either we need to do a better job at exposing the regex machine or have different defaults for the std regex. it's possible that some other default would work with std where '.' fails.\n. playing around with this, and i can't actually make the 'std' regex machine run tests at all under gcc 4.8.4.\n. it's expected to fail some matches, though we could warn if we end up with no benchmarks to run.\n. circling back again, checking a clean 'cmake' run for a g++ 4.8.4 system:\n-- Performing Test HAVE_STD_REGEX\n-- Performing Test HAVE_STD_REGEX -- compiled but failed to run\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile\n-- Performing Test HAVE_POSIX_REGEX\n-- Performing Test HAVE_POSIX_REGEX -- success\nso this is \"expected\".\n. Thanks!\n. v1.1.0 released\n. and annotated:\n~/git/benchmark [master] $ git describe\nv1.1.0\n~/git/benchmark [master] $ git describe --tags\nv1.1.0\n. I agree, especially as the google style guide says no exceptions.\n. https://github.com/google/benchmark/blob/721c9224b96ad6b6559bf1695ec633a35e69027c/src/sysinfo.cc#L211\nacknowledged that we're not happy about using bogomips, but it's only used if we didn't find anything better.\nwhat would you suggest we do instead in that case? we can use a slow estimation method and drop bogomips altogether, but i'm not sure that it's necessarily better.\n. I'd be ok with dropping bogoMIPS and just using estimation. it doesn't add that much overhead to the overall benchmark run.. should we start using gflags directly?. that seems like a reasonable start, yes.. this is #262 i think?. perhaps it can be extended to cover your use cases (once it lands.. longest PR ever)?. I don't think it's intrusive. In fact, calculating the mean, stddev, median, etc at report time is probably the right thing to do.. I think so. Stat1 is only used after the benchmark has run so any concerns about vector performance should be minimal. I would like to see some thought put into is how large the vectors could be as the number of runs increase. . using cpupower works for me:\n$ sudo cpupower frequency-set --governor performance\n...\n$ test/basic_test \nRun on (12 X 3900 MHz CPU s)\n2017-10-30 16:20:52\n***WARNING*** Library was built as DEBUG. Timings may be affected.\n--------------------------------------------------------------------------------------\nBenchmark                                               Time           CPU Iterations\n--------------------------------------------------------------------------------------\n...\n$ sudo cpupower frequency-set --governor powersave\n...\n$ test/basic_test \nRun on (12 X 3900 MHz CPU s)\n2017-10-30 16:21:02\n***WARNING*** CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead.\n***WARNING*** Library was built as DEBUG. Timings may be affected.\n--------------------------------------------------------------------------------------\nIf this is common (and not some quirk of my set up) then maybe that's what should be linked either in the README or the warning.. so we warn, and we have documentation. closing this.. i think the human readable printer shouldn't be applied to anything outside the results (ie, not the labels). so this is a bug.\ni think https://github.com/google/benchmark/blob/56336e70f151f9eb828176e795f7c5dfe6d6bb59/src/benchmark_register.cc#L166 is the line that needs to be changed.. it looks like the pull request was made on @BRevzin's local fork. if it gets made against this repo with this issue linked i'd be happy to take a look.. Thank you!. on its one year anniversary i plan on reverting it.\nactually i should probably just do it now. noone is screaming out for it, and this closed PR will be in the history for anyone that wants to do it.. i speak to the abseil folks quite regularly, and they're considering alternatives to bazel to make integration with existing projects easier. Ie, this issue isn't as high priority as i originally expected.. There are a few patches in progress adding BUILD files, but none of them\nare complete. Feel free to work with the authors to help them finish the\njob :)\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Sun, Feb 25, 2018 at 4:26 PM, Keir Mierle notifications@github.com\nwrote:\n\nNow landed: ceres-solver/ceres-solver@22fa21c\nhttps://github.com/ceres-solver/ceres-solver/commit/22fa21c6f824ebefae12a9a2fa459714907acc57\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/pull/329#issuecomment-368359724, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMl4nmJXb6zIZjGagpE53254gnCEtks5tYfoYgaJpZM4LVY2M\n.\n. thank you!. LGTM. (travis error looks like a travis failure... cmake started failing to build a test program).. we already do: https://github.com/google/benchmark/blob/master/src/benchmark.cc#L320\n\nperhaps something else is going on?. thanks!. this should already be fixed in the latest from master.. just above the lines you quote we have\n24  find_library(LIBRT rt)                                                                                                                                                                                                                                                                                        \n  1 if(LIBRT)\n  2   target_link_libraries(benchmark ${LIBRT})\n  3 endif(). You're right, that's not part of the library. Perhaps it could be a\nsupplemental tool, much like the 'compare_bench.py' tool in tools that\ncompares two sets of benchmarks.\nThe library is really focused on the benchmark registration and running\npart. Any extra things like this, or like continuous benchmarking services,\nwould be better served as separate tools i think. However, having them in\nthe repository is welcome!\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Thu, Feb 2, 2017 at 3:56 PM, WilliamTambellini notifications@github.com\nwrote:\n\nGood morning all\nI've read as much as I could the doc and list of issues on github but\ncould nt find a reply regarding this question.\nIf I understand right this benchmark library offers nice way to register\nand benchmark different functions.\nBut I did nt find a way for the library to :\n\ncompare the timing (real, cpu, ...) with given threshold values\nreports which benchmark passed and which one failed if the time is\n   higher than the threshold\n   I have tested this following solution but there should be better :\nrun the benchmark and output to json\nuse for example the jq command line tool in order to test that each\n   benched functions are within the desired threshold (benchresults.json\n   contains the json example as given in the main .md doc) :\n   jq -e '.benchmarks[] | select(.name == \"BM_SetInsert/1024/1\").real_time\n   < 30000' ~/tmp/benchresults.json\n\nThis is working but I m wondering if these tests should be better done\ninside the library.\nFor instance, the user could provide via the commandline a \"threshold\"\nfile (--treshold_file or whatever) giving the thresholds (time limits) for\neach benched functions : for example, in csv format :\n\"BM_SetInsert/1024/1\", 30000\n\"BM_SetInsert/1024/8\", 33000\n\"BM_SetInsert/1024/10\", 32000\nRFC time.\nCheers\nW.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/339, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMtskiQ61_y_VS91mL1fkaZK18rjXks5rYm0zgaJpZM4L1y-f\n.\n. see https://github.com/google/benchmark/pull/262\n\nit looks like the work stalled, which is a shame.\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Fri, Feb 17, 2017 at 3:01 PM, Prasun Gera notifications@github.com\nwrote:\n\nFrom the README and the API header, it looks like the supported metrics\nare bytes processed and items processed. Is it possible to extend these\nwith custom metrics? For example, one might be interested in GFLOPS, or\nother derived metrics such as raw bandwidth v/s effective bandwidth.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/344, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMn_eOTIQ2GRQ_Tv7s4bM_m2mvrImks5rdibIgaJpZM4ME2tj\n.\n. I think the way you've described it would be confusing for users and would\nmake it harder to write scripts to process the output.\n\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Wed, Mar 1, 2017 at 6:12 PM, jpmag notifications@github.com wrote:\n\nI'd like to do this. I was thinking of the following:\n\nDuring processing we print only regular data to the console (without\n   user counters). This allows immediate feedback, and saves the trouble of\n   figuring out which counters go on and off in each benchmark.\nAfter all benchmarks are finished, with the full knowledge of the\n   counters which were set, we print again, but this time both regular\n   data AND counters.\n\nWhat do you think?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/349, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMje1wMU3HeCMLh5inkHMhfw7nUIyks5rhiWEgaJpZM4MQdbw\n.\n. Looks like one of the tests is failing for MinGW relating to console output. Could you take a look?. nice catch!. the best thing to do is probably to disable the compiler flag warnings for\nICC but with a longer-term goal to fix the code where it's a real issue.\n\ni'd say the SetUp overload is also a real issue and we should finish the\ndeprecation.\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Thu, Mar 23, 2017 at 2:15 PM, rolandschulz notifications@github.com\nwrote:\n\nThe build fails with ICC17 because of warnings and Werror. What is the\ncorrect solution to fix it?\nShould a patch\n\ndisable Werror for ICC (or maybe all non known compilers)\ndisable the false postive warnings for all files. This could be\n   done using:\n   add_cxx_compiler_flag(-wd2102) #ICC17u2: Many false positives for\n   Wstrict-aliasing\n   add_cxx_compiler_flag(-wd2259) #ICC17u2: non-pointer conversion from\n   \"long\" to \"int\" may lose significant bits (even for explicit static cast,\n   sleep.cc(44))\n   add_cxx_compiler_flag(-wd654) #ICC17u2: overloaded virtual function\n   \"benchmark::Fixture::SetUp\" is only partially overridden (because of\n   deprecated overload)\ndisable warnings at file level or some other granularity\n\nAnother warning which isn't clearly a false positive is\n../test/output_test_helper.cc(34): error #3280: declaration hides\nvariable \"::dec_re\" (declared at line 67 of \"../test/output_test.h\")\nShould the local dec_re be renamed or should that warning be suppressed\ntoo for 2/3?\nIf I know which solution is preferred I'm happy to create a pull request.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/354, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMsLkVABVWKPJ9hAHQec80ew9_0kmks5rouDkgaJpZM4MnX-l\n.\n. that's clearly broken, but i don't see what's going on at first glance. @biojppm may have some insights.. thanks!. LG but please do add something to a test.. i'll take the fix. if you can follow up with a test, that would be great, but we can wait.. This installs the headers into /usr/local/include/benchmark/benchmark so i removed the one extra benchmark subfolder before merging.\n\nthank you so much!. There are windows users, so i believe it should. When you run cmake to\ngenerate the VS solution i believe it will check for the pthread\ndependency... perhaps you don't have pthread available?\nI'm afraid i haven't kept up with Windows development so i can't help much.\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Mon, Apr 10, 2017 at 8:49 AM, navia1991 notifications@github.com wrote:\n\nI am currently using VS2017 and i ran into errors when i tried to build in\nVS.\nFor example, LINK : fatal error LNK1104: cannot open file 'pthread.lib'.\nI d like to ask is the 'compiler support' a necessary for using this\nlibrary?\nSuch as i need GCC, clang, VS, Intel 2015 Update 1 ALL intalled?\nThanks!\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/368, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMjo7T6xu-0YcV795tRv8ZrQIlLQSks5ruk93gaJpZM4M497J\n.\n. what happens if you invoke cmake with:\n\ncmake -DBUILD_SHARED_LIBS=FALSE\n(https://cmake.org/cmake/help/v3.0/variable/BUILD_SHARED_LIBS.html)\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Wed, Apr 12, 2017 at 6:30 AM, Sergey Lyubimov notifications@github.com\nwrote:\n\nhello.\ni build benchmarklib as cmake subproject\nif(DARIADB_ENABLE_BENCHMARKS)\n   set(BENCHMARK_ENABLE_TESTING OFF)\n   add_subdirectory(extern/benchmark)\n   include_directories(extern/benchmark/include)\n   set_target_properties(benchmark PROPERTIES FOLDER Extern)endif(DARIADB_ENABLE_BENCHMARKS)\nwhere 'extern/benchmark' is a git submodule. But when i build in msvc\n2015, google.benchmark always build as shared dll, not as static. this is a\nbug? i see you build scripts, and you not set library type of benchmark lib.\nadd_library(benchmark ${SOURCE_FILES})\ni think, that should see be like this\nadd_library(benchmark STATIC ${SOURCE_FILES})\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/369, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMoabiQ9BGa_jmV9ME7CbY7eO8nPKks5rvNIIgaJpZM4M7bUk\n.\n. There is not.\n\nAn early version of the API had both time controls and iteration count\ncontrols, but the way they worked together was very complicated, and it\nturned out that when users were changing iteration counts they almost\nalways were doing it to ensure a minimum time for the benchmark to run.\nWhat is your use-case?\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Thu, Apr 13, 2017 at 11:40 PM, Prasun Gera notifications@github.com\nwrote:\n\nIs there a way to control the iteration count directly instead of\n--benchmark_min_time ?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/370, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMg82U4JR37Ca3NJV8DA8kCsNoGF0ks5rvxTugaJpZM4M9gDt\n.\n. ITYM #373 . For approach 1, you can initialize the queue inside the loop while things are paused with a size passed in to the benchmark, and then use the same parameter to dequeue inside the loop. you can also pause the timing while checking for the queue being empty. something like:\n\n```c++\ninclude \ninclude \nvoid BM_dequeue(benchmark::State& state) {\n  for (auto _ : state) {\n    state.PauseTiming();\n    std::deque q(state.range(0), \"moo\");\n    state.ResumeTiming();\n    while (true) {\n      q.pop_back();\n      state.PauseTiming();\n      if (q.empty()) {\n        state.ResumeTiming();\n        break;\n      }\n      state.ResumeTiming();\n    }\n  }\n}\nBENCHMARK_REGISTER(BM_dequeue)->Range(100,1000);\n```\noutput:\n```\n\nBenchmark                Time           CPU Iterations\nBM_dequeue/8          3644 ns       3645 ns     192098\nBM_dequeue/512      203395 ns     204625 ns       3500\nBM_dequeue/8192    3188800 ns    3205079 ns        219\nBM_dequeue/100       38780 ns      38881 ns      17858\nBM_dequeue/512      199370 ns     200510 ns       3517\nBM_dequeue/1000     389285 ns     390608 ns       1797\n```\nwhile the pausing and resuming does have some small overhead, it is consistent so comparisons between implementations are still going to be useful.\n. Measuring the time to stop would then also add to the time, so we'd have to\nmeasure the time it takes to measure...\nthe overhead should be minimal compared to the operations you're doing. if\nnot, maybe increase the range to force more work per iteration so that it\nis.\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Wed, Nov 8, 2017 at 4:02 PM, frankist notifications@github.com wrote:\n\n\"it is consistent so comparisons between implementations are still going\nto be useful\"\nThat's great! Continuing on that approach, would it be possible in\npractice to compute its overhead separately and subtract it in the final\nresults? My wild guess would be yes, but computers are weird and a million\ndifferent variables can affect the speed of your code.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/370#issuecomment-343002812,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMnQISwi8IrDXlRWfcq9GMxncEkrqks5s0kEPgaJpZM4M9gDt\n.\n. That seems reasonable. We want to make sure the measured part is run enough\nto get a good signal out. Obviously this leads to the slow part running\nmore often.\n\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Wed, Nov 8, 2017 at 5:43 PM, frankist notifications@github.com wrote:\n\nYes, you are right. The last question was just a personal curiosity.\nOne more question, I think that if the code between \"state.PauseTiming()\"\nand \"state.ResumeTiming()\" (non-measured) takes much more time than the\ncode I am measuring, google benchmark becomes very slow. My guess is that\ngoogle benchmark is using the \"measured\" part of the iteration to compute\nthe number of iterations. If it is fast, the number of iterations will be\nhigh. This is causing the slow non-measured part to be computed way too\nmany times. Would this be the expected behavior or a bug?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/370#issuecomment-343020623,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMiFMA8XFD_W1sFMoIjY7Ha0T87Zpks5s0ljXgaJpZM4M9gDt\n.\n. The reason I originally used the population standard deviation is to avoid\ngeneralizing the findings to the population of runs. Ie, what we have is a\nsample, but we're interested in the statistics for the sample.\n\nIf you think this thinking is in error, please feel free to develop a patch\nand show how the data changes under the sample standard deviation. I'm\nhappy to be wrong!\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Mon, Apr 17, 2017 at 7:37 AM, Yixuan Qiu notifications@github.com\nwrote:\n\nHi all,\nThe current calculation for standard deviation uses the population\nversion, i.e., sqrt(sum((x_i - x_mean)^2)/n). But by using repetitions,\nit's more sensible to use the sample version sqrt(sum((x_i -\nx_mean)^2)/(n - 1)). This is especially important when computing the\nconfidence interval for the mean values.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/371, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMpe-wURLo7nNpkk1Wq1zMcEwhmp-ks5rw3lFgaJpZM4M_MyE\n.\n. makes sense to me! i look forward to the patch.\n\nalso, if you think it makes sense to add confidence intervals to the output\ni'm happy to support a patch for that too.\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Mon, Apr 17, 2017 at 10:15 AM, Yixuan Qiu notifications@github.com\nwrote:\n\nHi Dominic, thanks for the quick response. According to your explanation,\nI think it is exactly where the sample standard deviation can be used. The\nlogic is as follows:\nWe are interested in the overall elapsed time of one benchmark, whose\nvalue is mu but unknown. To estimate mu, we run the benchmark n times and\ncollect a sample of n data x = [x_1, x_2, ..., x_n]. The sample mean of x\nis x_mean and the sample standard deviation is x_sd. With these two\nstatistics, we can construct a 95% confidence interval of mu as x_mean \u00b1\n1.96 * x_sd / sqrt(n) (1.96 is obtained from the normal distribution for\nconvenience, a better one is to use the t-distribution). To use this\nequation, x_sd has to be the sample version.\nIf this makes sense to you, I'm glad to submit a PR later. Thanks.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/371#issuecomment-294532041,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMiUi1TVBQC4vOcLIwotVGR4wPju1ks5rw54ggaJpZM4M_MyE\n.\n. It's been a while since I've looked at the stat classes, but iirc it's\nmeant to be ensuring the mean is a per-iteration mean. Ie, we pass in the\naverage time per iteration (yes, we're doing means of means. sorry).\n\nIn your example, real_accumulated_time_stat would end up with a value of\n(0.002 + 0.003) and a sample size of 2, meaning the mean would be 0.0025.\nIn the current code, it would be a value of 0.005 and a sample size of\n2000, giving a mean of 2.5e-6. However, on\nhttps://github.com/google/benchmark/blob/master/src/complexity.cc#L215 we\nmultiple the mean back up and end up reporting the right number.\nSo it looks like this is done to ensure that the standard deviation is\ncalculated based on the number of iterations, rather than the number of\nruns. This could be wrong.\nCan you show the output with the std dev for the upstream version and your\nversion so we can compare and see what is \"right\"?\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Thu, Apr 20, 2017 at 7:19 AM, Yixuan Qiu notifications@github.com\nwrote:\n\nHi @dominichamon https://github.com/dominichamon, when I was creating\nthe patch I noticed one line in the code: https://github.com/google/\nbenchmark/blob/master/src/complexity.cc#L197. Basically it adds a data\npoint to the sample with weight run.iterations. May I ask why this weight\nis set to run.iterations instead of 1? I'm asking this since it may cause\nproblems in the calculation of sample variance.\nConsider the hypothetical example below. Assume that we have two runs,\neach with 1000 iterations. The first run takes 2 seconds and the second\ntakes 3 seconds. The first run has the effect of\nreal_accumulated_time_stat += Stat1_d(0.002, 1000)\nso that inside real_accumulated_time_stat, the member variable numsamples_\n= 1000. Similarly, after adding the second run, the number of samples\nbecomes 2000.\nHowever this is not the sample size used in the calculation of sample\nstandard deviation. What we should have is numsamples_ = 2. A quick fix\nof this is to add the data without a weight (which means weight = 1), so\nthe code looks like this one: https://github.com/yixuan/\nbenchmark/blob/master/src/complexity.cc#L196-L201.\nI want to know if the weights have any other use in the code that I have\nmissed. If not, I think I can go and submit the PR. Thanks!\n(For the confidence interval, currently there is some difficulty since we\nneed the quantile function of t-distribution, which is not provided by\nstandard C++ libraries.)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/371#issuecomment-295754664,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMm1cscahmp98CMbZjjR6KFELpr3aks5rx2lXgaJpZM4M_MyE\n.\n. I notice in your patches you're also changing the sample size for the items\nper second and bytes per second. Does that also lead to correct values?\n\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Tue, May 2, 2017 at 1:36 PM, Yixuan Qiu notifications@github.com wrote:\n\nThis is the result of a simple test based on the current google/benchmark\nhttps://github.com/google/benchmark:\n\nBenchmark                      Time           CPU Iterations\nBM_SORT                     7792 ns       7782 ns      72674\nBM_SORT                     7775 ns       7766 ns      72674\nBM_SORT                     7753 ns       7745 ns      72674\nBM_SORT_mean                7774 ns       7764 ns      72674\nBM_SORT_stddev                16 ns         15 ns          0\nBM_STABLE_SORT              8179 ns       8169 ns      84570\nBM_STABLE_SORT              8249 ns       8238 ns      84570\nBM_STABLE_SORT              8252 ns       8242 ns      84570\nBM_STABLE_SORT_mean         8226 ns       8217 ns      84570\nBM_STABLE_SORT_stddev         34 ns         34 ns          0\nWe can manually verify that sqrt(((7792 - 7774)^2 + (7775 - 7774)^2 +\n(7753 - 7774)^2) / 3) = 15.98 ~= 16.\nBelow is my patched version (yixuan@9657621\nhttps://github.com/yixuan/benchmark/commit/9657621b748001386db2c8338fc3975ea9dc5748,\nyixuan@9f6434d\nhttps://github.com/yixuan/benchmark/commit/9f6434da82defb0bdebd3aeb8512258646e861a5\n):\n\nBenchmark                      Time           CPU Iterations\nBM_SORT                     7807 ns       7799 ns      83800\nBM_SORT                     7782 ns       7775 ns      83800\nBM_SORT                     7770 ns       7762 ns      83800\nBM_SORT_mean                7786 ns       7779 ns      83800\nBM_SORT_stddev                19 ns         19 ns          0\nBM_STABLE_SORT              8354 ns       8345 ns      79040\nBM_STABLE_SORT              8255 ns       8246 ns      79040\nBM_STABLE_SORT              8288 ns       8279 ns      79040\nBM_STABLE_SORT_mean         8299 ns       8290 ns      79040\nBM_STABLE_SORT_stddev         50 ns         50 ns          0\nTake the first stddev value as example, it can be verified that sqrt(((7807\n- 7786)^2 + (7782 - 7786)^2 + (7770 - 7786)^2) / 2) = 18.88 ~= 19\nThe difference is that one uses 3 as denominator and the other uses 2.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/371#issuecomment-298753307,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMrKq_1w5t2jfsZd1SxlC9tDMruESks5r15O3gaJpZM4M_MyE\n.\n. the standard deviation for items/bytes looks suspicious in both cases, but\nthe mean looks right.\n\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Tue, May 2, 2017 at 8:32 PM, Yixuan Qiu notifications@github.com wrote:\n\nI think so. I used an example derived from the test suite:\ninclude \ninclude \"benchmark/benchmark_api.h\"\nstd::set ConstructRandomSet(int size) {\n  std::set s;\n  for (int i = 0; i < size; ++i) s.insert(i);\n  return s;\n}\nstatic void BM_SetInsert(benchmark::State& state) {\n  while (state.KeepRunning()) {\n    state.PauseTiming();\n    std::set data = ConstructRandomSet(state.range(0));\n    state.ResumeTiming();\n    for (int j = 0; j < state.range(1); ++j) data.insert(rand());\n  }\n  state.SetItemsProcessed(state.iterations() * state.range(1));\n  state.SetBytesProcessed(state.iterations() * state.range(1) * sizeof(int));\n}BENCHMARK(BM_SetInsert)->Args({1024, 1});\nBENCHMARK_MAIN();\nOfficial version:\n\nBenchmark                           Time           CPU Iterations\nBM_SetInsert/1024/1             48752 ns      48713 ns      14356   80.1897kB/s   20.0474k items/s\nBM_SetInsert/1024/1             48759 ns      48722 ns      14356   80.1743kB/s   20.0436k items/s\nBM_SetInsert/1024/1             48773 ns      48733 ns      14356   80.1566kB/s   20.0392k items/s\nBM_SetInsert/1024/1_mean        48761 ns      48722 ns      14356   80.1735kB/s   20.0434k items/s\nBM_SetInsert/1024/1_stddev          9 ns          8 ns          0    13.8566B/s    3.46415 items/s\nPatched version:\n\nBenchmark                           Time           CPU Iterations\nBM_SetInsert/1024/1             48622 ns      48581 ns      14407   80.4065kB/s   20.1016k items/s\nBM_SetInsert/1024/1             48764 ns      48722 ns      14407   80.1736kB/s   20.0434k items/s\nBM_SetInsert/1024/1             48850 ns      48810 ns      14407   80.0297kB/s   20.0074k items/s\nBM_SetInsert/1024/1_mean        48745 ns      48705 ns      14407   80.2033kB/s   20.0508k items/s\nBM_SetInsert/1024/1_stddev        116 ns        115 ns          0    194.749B/s    48.6872 items/s\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/371#issuecomment-298816177,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMmnGcGBQDFKXvcoeNWAvFJQNH9Heks5r1_VigaJpZM4M_MyE\n.\n. i didn't! UI failure. I should fix that.\n\nOk, this looks good.\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Wed, May 3, 2017 at 9:52 AM, Yixuan Qiu notifications@github.com wrote:\n\nDid you notice the different units?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/371#issuecomment-298970025,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMqZ-203f6IQXrcFQzFGc0PyNoKcfks5r2LC4gaJpZM4M_MyE\n.\n. Agree on rounding errors. I still think it's worth getting the sample version in first before working on more accuracy.\n\nWould you mind putting a PR together?. LGTM. @EricWF i'll let you have the final say/merge.. I look forward to fielding the support requests from users being confused about this feature.\n;). replying privately.. Please sign the CLA so we can take a look at this change. Thanks!. @biojppm @vladoovtcharov I think you'll need to post something like \"I signed!\" for the cla bot to recheck.. Thank you so much!. it wasn't clear to me if the appveyor failures are real.. i believe they are though so i'm waiting for @yixuan to address them.. running at https://ci.appveyor.com/project/google/benchmark/build/644. nice. thanks!\n. Great! Pull Requests always welcome.. thank you!. closing due to lack of action. it can be resurrected if necessary.. using benchmark::StatisticsStdDev should work to print the std deviation, and is added by default[1] when you have multiple repetitions[2].\n\nhttps://github.com/google/benchmark/blob/a271c36af93c7a3b19dfeb2aefa9ca77a58e52e4/src/benchmark_register.cc#L243\nhttps://github.com/google/benchmark/blob/25acf220a44ccc41104a690731fcf646cc3e8192/test/reporter_output_test.cc#L314. There isn't.\n\nSorry for the delay, i've been trying to find some time to work out an\nanswer. Spoiler: I didn't.\nI think some combination of spawning threads and pausing timing while\nwaiting on mutexes to synchronize might work.\nI would expect declaring stuff as 'volatile' might help, but that's more a\nguideline than a guarantee. maybe you can use some std::atomic and atomic\nfences to synchronize writes and reads?\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Tue, May 23, 2017 at 6:15 AM, schoetbi notifications@github.com wrote:\n\nI have a question: When I want the speed of a function that processes an\ninput byte array according to different rules. How do I simulate that the\ninput data got changed from another thread? I assume that when running\nmultiple times over the same data (which is in L1 cache) the speed is quite\nhigh but when running in production (with shared memory and writes from\nother processes) the speed might differ.\nIs there something like \"ReloadFromMainMemory()\" in every loop?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/392, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMpPDUIIdQyvlAUqrsGpouVNVaQNDks5r8tvjgaJpZM4Njq3C\n.\n. thank you!. there's no good argument for it. it was developed within google as an alternative API to the existing benchmark solution (that couldn't be open sourced due to tangled dependencies) and this was marginally cleaner to implement than passing pointers around.\n\nI'd be comfortable changing to benchmark::State* to be compliant but that is a breaking API change that needs to be handled carefully.. you'll need to link pthread.\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Thu, May 25, 2017 at 10:47 AM, yangguangxu notifications@github.com\nwrote:\n\ndemo code:\ninclude \"benchmark/benchmark.h\"\nstatic void BM_StringCreation(benchmark::State& state) {\nwhile (state.KeepRunning())\nstd::string empty_string;\n}\n// Register the function as a benchmark\nBENCHMARK(BM_StringCreation);\n// Define another benchmark\nstatic void BM_StringCopy(benchmark::State& state) {\nstd::string x = \"hello\";\nwhile (state.KeepRunning())\nstd::string copy(x);\n}\nBENCHMARK(BM_StringCopy);\nBENCHMARK_MAIN(); compile command: g++ -o bm test.cc -lbenchmark running\ncommand: ./bm\nerror message:\nterminate called after throwing an instance of 'std::system_error'\nwhat(): Unknown error -1\nAborted (core dumped)\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/395, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMjE2o_I4LlCApFs2TAHtfXOj-227ks5r9b6-gaJpZM4Nmrc3\n.\n. closing due to lack of action.\n\neither it'll be handled by #599 or in the JSON work in v2.. 1. no it won't. and if you want to do some reinitialization in the loop you\ncan use PauseTiming and ResumeTiming.\n2. because slowFuncReturn is slower?\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Wed, Jun 7, 2017 at 6:09 AM, zack-snyder notifications@github.com\nwrote:\n\n1.\nI do some initialization before the loop.\nWill this also be included in the measurement?\nSomething like this:\nstatic void BM_somefunc(benchmark::State& state)\n{\n    auto foo = do_some_init();\n    while (state.KeepRunning())\n    {\n        do_some_calculation(foo);\n    }\n}\n1.\nI get sometimes following results\n\nBenchmark                  Time           CPU Iterations\nBM_fastFuncReturn      16327 ns      16392 ns      44800\nBM_slowFuncReturn      17499 ns      16881 ns      40727\nYou see that the CPU time is in one case higher. How can this be?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/397#issuecomment-306789673,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMmr0LoZi4vo2Bw93pmRdoXesTgiIks5sBqEjgaJpZM4NuTrF\n.\n. ice, ice, baby. https://travis-ci.org/google/benchmark/jobs/238921070. you did pretty well to get all but one... 32-bit, old g++, optimized.. thaaanks!. The library hasn't been designed with that in mind at all. The problem is\ngoing to be on the multiple initialization and running part, which are only\nexpected to be called once in a program's execution.\n\nIt could be changed to work, but I'm not sure that it's worth it because I\ndon't really understand what you're getting out of doing it this way. Can\nyou explain what problem you're trying to solve?\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Tue, Jun 13, 2017 at 11:50 AM, NickG notifications@github.com wrote:\n\nmaybe something as simple as:\nvoid BenchmarkFamilies::ClearBenchmark() {\n  MutexLock l(mutex_);\n  families_.clear();\n}\nTho not sure about the potential complications this might introduce in\nother parts of the code...\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/400#issuecomment-308212752,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMlZX1W7_-0efA4c1kioV7M8-f8ygks5sDtn4gaJpZM4N46Yg\n.\n. the author and commit appear to be jernkuanl@nvidia.com. Is that the email under which you signed the CLA?. Can you provide your compilation steps, the platform you're on...\n\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Fri, Jun 16, 2017 at 11:30 AM, Jeromy notifications@github.com wrote:\n\nbenchmark/test/benchmark_test.cc: In function 'void\nBM_ManualTiming(benchmark::State&)':\nbenchmark/test/benchmark_test.cc:185:5: error: 'sleep_for' is not a\nmember of 'std::this_thread'\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/405, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMlo7kIHUWSK5x5ss7HuFa3Fg-Pwwks5sEsm3gaJpZM4N8yZe\n.\n. IIRC, gcc 4.7 has some quirks when it comes to the c++11 libs. you might\nwant to force c++0x (or use devtoolset-2 which has g++ 4.8).\n\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Fri, Jun 16, 2017 at 11:53 AM, Jeromy notifications@github.com wrote:\n\nThe docker env:\nFROM centos:6\nMAINTAINER \"Jeromy Fu\" jianfu@cisco.com\nRUN yum -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm\nRUN curl https://people.centos.org/tru/devtools-1.1/devtools-1.1.repo -o /etc/yum.repos.d/devtools-1.1.repo\nRUN yum -y install devtoolset-1.1\nRUN yum -y install cmake\nThe compile command and output:\n[root@d9306eee282a build]# scl enable devtoolset-1.1 \"cmake ../\"\n-- The C compiler identification is GNU 4.7.2\n-- The CXX compiler identification is GNU 4.7.2\n-- Check for working C compiler: /opt/centos/devtoolset-1.1/root/usr/bin/cc\n-- Check for working C compiler: /opt/centos/devtoolset-1.1/root/usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working CXX compiler: /opt/centos/devtoolset-1.1/root/usr/bin/c++\n-- Check for working CXX compiler: /opt/centos/devtoolset-1.1/root/usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Could NOT find Git (missing:  GIT_EXECUTABLE)\n-- git Version: v0.0.0\n-- Version: 0.0.0\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11 - Success\n-- Performing Test HAVE_CXX_FLAG_WALL\n-- Performing Test HAVE_CXX_FLAG_WALL - Success\n-- Performing Test HAVE_CXX_FLAG_WEXTRA\n-- Performing Test HAVE_CXX_FLAG_WEXTRA - Success\n-- Performing Test HAVE_CXX_FLAG_WSHADOW\n-- Performing Test HAVE_CXX_FLAG_WSHADOW - Success\n-- Performing Test HAVE_CXX_FLAG_WERROR\n-- Performing Test HAVE_CXX_FLAG_WERROR - Success\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC - Success\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS - Success\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32 - Failed\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL - Success\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING - Success\n-- Performing Test HAVE_CXX_FLAG_WZERO_AS_NULL_POINTER_CONSTANT\n-- Performing Test HAVE_CXX_FLAG_WZERO_AS_NULL_POINTER_CONSTANT - Success\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING - Success\n-- Performing Test HAVE_CXX_FLAG_WD654\n-- Performing Test HAVE_CXX_FLAG_WD654 - Failed\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY - Failed\n-- Performing Test HAVE_CXX_FLAG_COVERAGE\n-- Performing Test HAVE_CXX_FLAG_COVERAGE - Success\n-- Performing Test HAVE_STD_REGEX\n-- Performing Test HAVE_STD_REGEX -- compiled but failed to run\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile\n-- Performing Test HAVE_POSIX_REGEX\n-- Performing Test HAVE_POSIX_REGEX -- success\n-- Performing Test HAVE_STEADY_CLOCK\n-- Performing Test HAVE_STEADY_CLOCK -- success\n-- Looking for include file pthread.h\n-- Looking for include file pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Performing Test BENCHMARK_HAS_CXX03_FLAG\n-- Performing Test BENCHMARK_HAS_CXX03_FLAG - Success\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /adaptation_resilience/code/external/benchmark-master/build\n[root@d9306eee282a build]# scl enable devtoolset-1.1 \"make\"\nScanning dependencies of target benchmark\n[  3%] Building CXX object src/CMakeFiles/benchmark.dir/benchmark.cc.o\n[  6%] Building CXX object src/CMakeFiles/benchmark.dir/benchmark_register.cc.o\n[  9%] Building CXX object src/CMakeFiles/benchmark.dir/colorprint.cc.o\n[ 12%] Building CXX object src/CMakeFiles/benchmark.dir/commandlineflags.cc.o\n[ 16%] Building CXX object src/CMakeFiles/benchmark.dir/complexity.cc.o\n[ 19%] Building CXX object src/CMakeFiles/benchmark.dir/console_reporter.cc.o\n[ 22%] Building CXX object src/CMakeFiles/benchmark.dir/counter.cc.o\n[ 25%] Building CXX object src/CMakeFiles/benchmark.dir/csv_reporter.cc.o\n[ 29%] Building CXX object src/CMakeFiles/benchmark.dir/json_reporter.cc.o\n[ 32%] Building CXX object src/CMakeFiles/benchmark.dir/reporter.cc.o\n[ 35%] Building CXX object src/CMakeFiles/benchmark.dir/sleep.cc.o\n[ 38%] Building CXX object src/CMakeFiles/benchmark.dir/string_util.cc.o\n[ 41%] Building CXX object src/CMakeFiles/benchmark.dir/sysinfo.cc.o\n[ 45%] Building CXX object src/CMakeFiles/benchmark.dir/timers.cc.o\nLinking CXX static library libbenchmark.a\n[ 45%] Built target benchmark\nScanning dependencies of target basic_test\n[ 48%] Building CXX object test/CMakeFiles/basic_test.dir/basic_test.cc.o\nLinking CXX executable basic_test\n[ 48%] Built target basic_test\nScanning dependencies of target benchmark_test\n[ 51%] Building CXX object test/CMakeFiles/benchmark_test.dir/benchmark_test.cc.o\n/adaptation_resilience/code/external/benchmark-master/test/benchmark_test.cc: In function \u2018void BM_ManualTiming(benchmark::State&)\u2019:\n/adaptation_resilience/code/external/benchmark-master/test/benchmark_test.cc:185:5: error: \u2018sleep_for\u2019 is not a member of \u2018std::this_thread\u2019\nmake[2]:  [test/CMakeFiles/benchmark_test.dir/benchmark_test.cc.o] Error 1\nmake[1]:  [test/CMakeFiles/benchmark_test.dir/all] Error 2\nmake: *** [all] Error 2\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/405#issuecomment-309106685,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMpnoTgBBswoMhjI6-aAnAUfBRQd_ks5sEs8UgaJpZM4N8yZe\n.\n. clang release builds aren't happy:\n\ngoogle/benchmark/test/batch_test.cc:20:55: error: implicit\n      conversion loses integer precision: 'size_t' (aka 'unsigned long') to\n      'value_type' (aka 'int') [-Werror,-Wshorten-64-to-32]\n  for (size_t i = 0; i < param; ++i) values.push_back(i);. @astrelni should be able to provide some benchmark numbers (ha) to support the claims. This is something that has been tested internally.. there's no cons to commits that remove parts, and then adding a second PR for the other part.. once this lands, we should do a release. then we can do a later release with the deprecated headers removed.. @EricWF  i think it's a good thing to do (it used to be one header) at least for the api/benchmark split. reporters are something that only some users would need, i think, so it may be adding more to the compile overhead.... I appreciate the adherence to correctness but it's not going to be something i'm hung up on.. true. \nmerge 'em.. Each run with the different range should be reported as a different line in the output, if i'm reading this correctly.\nExample output from your example above:\n```\nBM_memcpy/10                                            8 ns          8 ns   85693300   1.89975GB/s\nBM_memcpy/11                                            8 ns          8 ns   89337173   1.83642GB/s\nBM_memcpy/12                                            8 ns          8 ns   85161295   1.89563GB/s\nBM_memcpy/13                                            8 ns          8 ns   87540678   1.83645GB/s\nBM_memcpy/14                                            8 ns          8 ns   86102916   1.90956GB/s\nBM_memcpy/15                                            8 ns          8 ns   89331988   1.83621GB/s\nBM_memcpy/16                                            8 ns          8 ns   85581982   1.90453GB/s\nBM_memcpy/17                                            8 ns          8 ns   88325367    1.7868GB/s\nBM_memcpy/18                                            8 ns          8 ns   85018141   1.88911GB/s\nBM_memcpy/19                                            8 ns          8 ns   89423704   1.84494GB/s\nBM_memcpy/20                                            8 ns          8 ns   86438968   1.90984GB/s\n```. The short version:\n\nCreate an annotated tag (i do it on command line.. github should also\nwork).\nget the diff between this and the last tag and add any highlights to the\ntag description.\n... done!\n\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Tue, Jul 4, 2017 at 3:01 PM, Eric notifications@github.com wrote:\n\n@dominichamon https://github.com/dominichamon Can we add some docs\nabout the steps required to create a new version of Google Benchmark? I\ndidn't do the release last time so perhaps you can document what we need to\ndo?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/409#issuecomment-312960402,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMrV52J4vHmaaNbf5jTU_RLiTgpnYks5sKrZKgaJpZM4OEkj2\n.\n. I get:\n\n../src/complexity.cc:270: ComputeBigO: Check `(run.complexity_n) > (0)'\nfailed. Did you forget to call SetComplexityN?\nwhen i try your snippit.\ni believe it's because you're using SetComplexityN with an argument 0.\nSee the documentation on SetComplexityN:\n```\n404   // If this routine is called with complexity_n > 0 and complexity\nreport is\n405   // requested for the\n406   // family benchmark, then current benchmark will be part of the\ncomputation\n407   // and complexity_n will\n408   // represent the length of N.\n```\nI'm not convincing myself that setting it to 0 is wrong, but it is\nunexpected by the library, at least. Perhaps we shouldn't use '0' as a\nsignal that SetComplexityN wasn't called.\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Fri, Jun 30, 2017 at 9:46 AM, Niklas Augustsson <notifications@github.com\n\nwrote:\nWhen calculating code complexity and the range includes 0 it crashes upon\nevaluation.\nvoid BM_JumpTable(benchmark::State& state)\n{\n  while (state.KeepRunning())\n  {\n    int i = state.range(0);\n  }\n  state.SetComplexityN(state.range(0));\n}\nBENCHMARK(BM_JumpTable)->RangeMultiplier(2)->Range(0, 1)->Complexity(benchmark::o1);\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/410, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMoUVoNMp_MJfmKKRnexUrAxtinLnks5sJKfVgaJpZM4OKQUj\n.\n. i think we should differentiate between SetComplexityN wasn't called and SetComplexityN was called with 0.. are you sure you're on the latest? have you pulled tags?\n\n$ git show-ref -d --tags\na5c8da0c9a598cdfb3fc58aa15e88bd966b671eb refs/tags/v0.0.9\n8b0b73f06c615f2712e69e0d4ea1a356b8b7a445 refs/tags/v0.0.9^{}\n73b734eb1fe857296e49528cd2413640aab7c40e refs/tags/v0.1.0\n006d23ccca1375a973b7fae0cc351cedb41b812a refs/tags/v0.1.0^{}\ncd525ae85d4a46ecb2e3bdbdd1df101e48c5195e refs/tags/v1.0.0\nd76e2e66e0c3d46817227291dd7513c5db8f8b87 refs/tags/v1.1.0\n4f8bfeae470950ef005327973f15b0044eceaceb refs/tags/v1.1.0^{}\n$ git describe\nv1.1.0-112-g9d4b719\n$ git describe --tags\nv1.1.0-112-g9d4b719\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Sun, Jul 2, 2017 at 1:41 PM, verbit notifications@github.com wrote:\n\nMy Git client does not recognize the v1.1.0 tag as annotated.\n$ git --version\ngit version 2.11.0 (Apple Git-81)\n$ git clone https://github.com/google/benchmark && cd benchmark\n$ git checkout tags/v1.1.0\n$ git describe\nv0.1.0-461-g710c2b8\n$ git describe --tags\nv1.1.0\nThe following also suggests that v1.1.0 is not tagged.\n$ git show-ref -d --tags\na5c8da0c9a598cdfb3fc58aa15e88bd966b671eb refs/tags/v0.0.9\n8b0b73f06c615f2712e69e0d4ea1a356b8b7a445 refs/tags/v0.0.9^{}\n73b734eb1fe857296e49528cd2413640aab7c40e refs/tags/v0.1.0\n006d23ccca1375a973b7fae0cc351cedb41b812a refs/tags/v0.1.0^{}\ncd525ae85d4a46ecb2e3bdbdd1df101e48c5195e refs/tags/v1.0.0\n4f8bfeae470950ef005327973f15b0044eceaceb refs/tags/v1.1.0\nAs a result, CMake build reports the wrong version (since GetGitVersion\nonly considers annotated tags). Can someone check if he can reproduce it or\ntell me if I am doing something wrong?\nP.S.\nRegarding the advent of benchmarkConfig.cmake it might be reasonable to\ndrop the v version prefix in GetGitVersion result (especially, so that COMPATIBILITY\nSameMajorVersion\nhttps://github.com/google/benchmark/blob/master/src/CMakeLists.txt#L52\nworks)? But this is probably another issue.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/411, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMkOWrcokUrZM____zMqdrX2mUyPJks5sKABigaJpZM4OLr3D\n.\n.  $ git cat-file -t v1.1.0\ntag\n\nit's definitely annotated (it would show lightweight otherwise).\nCan you 'git pull --tags' and see if it helps?\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Wed, Jul 5, 2017 at 12:25 PM, verbit notifications@github.com wrote:\n\nare you sure you're on the latest?\n$ git log\ncommit 9d4b719daeda35acf3a3d81b9ac1f38fc13333d1\nhave you pulled tags?\nDoes a fresh git clone pull the tags? I also tried with git fetch --tags\nbut still get the same output:\n$ git describe\nv0.1.0-462-g9d4b719\n$ git describe --tags\nv1.1.0-112-g9d4b719\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/411#issuecomment-313201944,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMquh9WZCajdmvwjxuzQhW4TkuEj7ks5sK-M0gaJpZM4OLr3D\n.\n. I thought of that.. but git push and git push --tags show that everything\nis up to date.\n\nand yet...\n$ git clone git@github.com:google/benchmark freshbm\nCloning into 'freshbm'...\nremote: Counting objects: 3708, done.\nremote: Total 3708 (delta 0), reused 1 (delta 0), pack-reused 3707\nReceiving objects: 100% (3708/3708), 976.16 KiB | 0 bytes/s, done.\nResolving deltas: 100% (2441/2441), done.\n$ cd freshbm/\n$ git tag\nv0.0.9\nv0.1.0\nv1.0.0\nv1.1.0\n$ git cat-file -t v1.0.0\ncommit\n$ git cat-file -t v1.1.0\ncommit\no_O\nso back to the original checkout and...\n$ git push origin v1.0.0\nEverything up-to-date\nxoanon:~/git/benchmark [master] $ git push origin v1.1.0\nTo github.com:/google/benchmark\n ! [rejected]        v1.1.0 -> v1.1.0 (already exists)\nerror: failed to push some refs to 'git@github.com:/google/benchmark'\nhint: Updates were rejected because the tag already exists in the remote.\n....\n$ git pull --tags\nFrom github.com:/google/benchmark\n t [tag update]      v1.1.0     -> v1.1.0\nAlready up-to-date.\n$ git cat-file -t v1.1.0\ncommit\n$ git tag -a -f v1.1.0 v1.1.0\nUpdated tag 'v1.1.0' (was 4f8bfea)\n$ git push --force origin v1.1.0\nCounting objects: 1, done.\nWriting objects: 100% (1/1), 162 bytes | 0 bytes/s, done.\nTotal 1 (delta 0), reused 0 (delta 0)\nTo github.com:/google/benchmark\n + 4f8bfea...7ba0caf v1.1.0 -> v1.1.0 (forced update)\n$ git cat-file -t v1.1.0\ntag\nand back to the fresh checkout:\n$ git pull --tags\nremote: Counting objects: 1, done.\nremote: Total 1 (delta 0), reused 1 (delta 0), pack-reused 0\nUnpacking objects: 100% (1/1), done.\nFrom github.com:google/benchmark\n t [tag update]      v1.1.0     -> v1.1.0\nAlready up-to-date.\n$ git cat-file -t v1.1.0\ntag\n$ git cat-file -t v1.0.0\ncommit\nso i'll keep working on v1.0.0, but v1.1.0 should now be annotated.\nthanks!\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Thu, Jul 6, 2017 at 9:04 AM, verbit notifications@github.com wrote:\n\ngit pull --tags does not help and git cat-file -t v1.1.0 shows commit.\nAre you testing this on a fresh git clone? Maybe the tag is only\nannotated in your local repository and not pushed to github?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/411#issuecomment-313441483,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMrCi5Q2uaVlscSuHJ7mTds7zVddbks5sLQV7gaJpZM4OLr3D\n.\n. v1.0.0 should be annotated correctly too now.\n\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Thu, Jul 6, 2017 at 9:12 AM, Dominic Hamon dominic@google.com wrote:\n\nI thought of that.. but git push and git push --tags show that everything\nis up to date.\nand yet...\n$ git clone git@github.com:google/benchmark freshbm\nCloning into 'freshbm'...\nremote: Counting objects: 3708, done.\nremote: Total 3708 (delta 0), reused 1 (delta 0), pack-reused 3707\nReceiving objects: 100% (3708/3708), 976.16 KiB | 0 bytes/s, done.\nResolving deltas: 100% (2441/2441), done.\n$ cd freshbm/\n$ git tag\nv0.0.9\nv0.1.0\nv1.0.0\nv1.1.0\n$ git cat-file -t v1.0.0\ncommit\n$ git cat-file -t v1.1.0\ncommit\no_O\nso back to the original checkout and...\n$ git push origin v1.0.0\nEverything up-to-date\nxoanon:~/git/benchmark [master] $ git push origin v1.1.0\nTo github.com:/google/benchmark\n ! [rejected]        v1.1.0 -> v1.1.0 (already exists)\nerror: failed to push some refs to 'git@github.com:/google/benchmark'\nhint: Updates were rejected because the tag already exists in the remote.\n....\n$ git pull --tags\nFrom github.com:/google/benchmark\n t [tag update]      v1.1.0     -> v1.1.0\nAlready up-to-date.\n$ git cat-file -t v1.1.0\ncommit\n$ git tag -a -f v1.1.0 v1.1.0\nUpdated tag 'v1.1.0' (was 4f8bfea)\n$ git push --force origin v1.1.0\nCounting objects: 1, done.\nWriting objects: 100% (1/1), 162 bytes | 0 bytes/s, done.\nTotal 1 (delta 0), reused 0 (delta 0)\nTo github.com:/google/benchmark\n + 4f8bfea...7ba0caf v1.1.0 -> v1.1.0 (forced update)\n$ git cat-file -t v1.1.0\ntag\nand back to the fresh checkout:\n$ git pull --tags\nremote: Counting objects: 1, done.\nremote: Total 1 (delta 0), reused 1 (delta 0), pack-reused 0\nUnpacking objects: 100% (1/1), done.\nFrom github.com:google/benchmark\n t [tag update]      v1.1.0     -> v1.1.0\nAlready up-to-date.\n$ git cat-file -t v1.1.0\ntag\n$ git cat-file -t v1.0.0\ncommit\nso i'll keep working on v1.0.0, but v1.1.0 should now be annotated.\nthanks!\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Thu, Jul 6, 2017 at 9:04 AM, verbit notifications@github.com wrote:\n\ngit pull --tags does not help and git cat-file -t v1.1.0 shows commit.\nAre you testing this on a fresh git clone? Maybe the tag is only\nannotated in your local repository and not pushed to github?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/411#issuecomment-313441483,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMrCi5Q2uaVlscSuHJ7mTds7zVddbks5sLQV7gaJpZM4OLr3D\n.\n\n\n. appveyor looks like a false positive. We shouldn't be adding unsupported command line options. Can you share your\nlogs?\n\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Tue, Jul 11, 2017 at 9:48 AM, atrah22 notifications@github.com wrote:\n\nHello,\nMaybe this is a novice question. I need intructions on how to compile\nlibbenchmark.a for aarch64 android.\nI tried,\nmkdir build\ncd build\ncmake -DCMAKE_BUILD_TYPE=Release\n-DCMAKE_CXX_COMPILER=aarch64-linux-android-g++\n-DBENCHMARK_ENABLE_LTO=false -DBENCHMARK_ENABLE_TESTING=false ..\nI get error. When I read the \"/benchmark/build/CMakeFiles/CMakeError.log\",\nI see the error is cause due to \"aarch64-linux-android-g++: error:\nunrecognized command line option '-Wshorten-64-to-32'\".\nHence next, I tried with clang++\ncmake -DCMAKE_BUILD_TYPE=Release\n-DCMAKE_CXX_COMPILER=aarch64-linux-android-clang++\n-DBENCHMARK_ENABLE_LTO=false -DBENCHMARK_ENABLE_TESTING=false ..\nmake\nmake install\nThis creates libbenchmar.a but not for aarch64 android.\nMy questions are:\n\nIs there option not to use g++ instead of clang++?\nHow can i create libbenchmark.a for android aarch64?\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/415, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMlxGUPRR7bB4MtP0Hkgzh8RLPx1zks5sMyjWgaJpZM4OT2kM\n.\n. I'll create one today.. release v1.2.0 complete: \nhttps://github.com/google/benchmark/releases/tag/v1.2.0. i just ran a check under the AddressSanitizer (which can detect these) and\nnothing got flagged. I'll keep looking.\n\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Thu, Jul 20, 2017 at 10:40 AM, Brian Adams notifications@github.com\nwrote:\n\nBuilding master with gcc 5.4, I'm getting ODR violations. I've traced the\nissue to a9a66c8\nhttps://github.com/google/benchmark/commit/a9a66c85bbfda1d744c267c5e5aa073ef3d1c1d5\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\n[51%] Linking CXX executable cxx03_test\n/usr/include/c++/5/bits/stl_tree.h:134:12: error: type \u2018struct _Rb_tree_node\u2019 violates one definition rule [-Werror=odr]\n     struct _Rb_tree_node : public _Rb_tree_node_base\n            ^\n/usr/include/c++/5/bits/stl_tree.h:134:12: note: a different type is defined in another translation unit\n     struct _Rb_tree_node : public _Rb_tree_node_base\n            ^\n/usr/include/c++/5/bits/stl_tree.h:139:12: note: the first difference of corresponding definitions is field \u2018_M_value_field\u2019\n       _Val _M_value_field;\n            ^\n/usr/include/c++/5/bits/stl_tree.h:149:41: note: a field with different name is defined in another translation unit\n       __gnu_cxx::__aligned_membuf<_Val> _M_storage;\n                                     ^\n\n[... continues ...]\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/420, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMjvHOPIS8np1wJWaXceQXwvGnPsdks5sP5EdgaJpZM4OecLX\n.\n. Maybe we should change\nhttps://github.com/google/benchmark/blob/master/test/CMakeLists.txt#L108 so\nthat we only build the cxx03 tests for cxx03 builds, and then add to travis\na cxx03 build and test.\n\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Thu, Jul 20, 2017 at 3:43 PM, Brian Adams notifications@github.com\nwrote:\n\nThe error appears to be valid ORD violation - the test cxx03_test (built\nwith std=c++03) is linking libbenchmark.a, built with C++11. At least for\nthis version of gcc, the tree nodes in a std::map appear to have a\ndifferent layout based on the value of __cplusplus.\nDisabling LTO works around the problem.\ntemplate\n    struct _Rb_tree_node : public _Rb_tree_node_base\n    {\n      typedef _Rb_tree_node<_Val>* _Link_type;\nif __cplusplus < 201103L\n  _Val _M_value_field;\n\n  _Val*\n  _M_valptr()\n  { return std::__addressof(_M_value_field); }\n\n  const _Val*\n  _M_valptr() const\n  { return std::__addressof(_M_value_field); }\n\nelse\n  __gnu_cxx::__aligned_membuf<_Val> _M_storage;\n\n  _Val*\n  _M_valptr()\n  { return _M_storage._M_ptr(); }\n\n  const _Val*\n  _M_valptr() const\n  { return _M_storage._M_ptr(); }\n\nendif\n};\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/420#issuecomment-316850998,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMurp8-li904M3MUfaDy8KN0DqCBeks5sP9gXgaJpZM4OecLX\n.\n. @LepelTsmok Can you confirm you are ok with this PR (as you originally created the first commit).. still working on this?. Given how far behind this is from head, i'm going to close it. i'm still open to the idea of having an HTML reporter, but i do think it should be something that is generated from the JSON output instead of being embedded.. I'm wary of users accidentally breaking user counters by (ab)using the map\ninterface. This is also inspired in part by #420.\n\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Sun, Jul 23, 2017 at 1:56 PM, Eric notifications@github.com wrote:\n\nI'm not sure I think this is an improvement. People know and understand\nthe STL interface and I think it's a benefit to offer it directly.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/pull/422#issuecomment-317281516, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMsF9lTAgKQkfVjqsGMFZp14vBsIpks5sQ7N5gaJpZM4Oex_k\n.\n. fair enough :). @EricWF I agree that disabling is a short term fix. Is it ever correct though to link a binary with the benchmarks compiled in cxx03 and the library in cxx11 mode?. thank you!. (i was away for a couple of weeks. i'm still catching up but i'll get to this review in the next couple of days). I'm so sorry. Totally my fault. I'll see if I can take a look tonight.\n\nOn Aug 22, 2017 2:31 PM, \"Roman Lebedev\" notifications@github.com wrote:\n\nMaybe the last ping, pretty please? ^^\nI would gladly change the code, i just need to know what is the\nvision/what to change...\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/pull/428#issuecomment-324157700, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMtxLhWH65xglTsprh0IevRzrc917ks5sa0jFgaJpZM4OkBsb\n.\n. just a few nits. it looks really good, thank you.. thank you!. Example output comparing runs for thread range with and without realtime:\n\n```\nBM_CalculatePi/threads:1                 12406 ns      11982 ns      58755\nBM_CalculatePi/threads:2                  9306 ns      11808 ns      58672\nBM_CalculatePi/threads:4                  6132 ns      11482 ns      60272\nBM_CalculatePi/threads:8                  6010 ns      11553 ns      59600\nBM_CalculatePi/threads:16                 5508 ns      11668 ns      59728\nBM_CalculatePi/threads:32                 4139 ns      11567 ns      59648\nBM_CalculatePi/real_time/threads:1       11205 ns      11181 ns      61231\nBM_CalculatePi/real_time/threads:2        5761 ns      11214 ns     112944\nBM_CalculatePi/real_time/threads:4        4082 ns      11548 ns     164852\nBM_CalculatePi/real_time/threads:8        4014 ns      11636 ns     173552\nBM_CalculatePi/real_time/threads:16       3732 ns      11613 ns     160000\nBM_CalculatePi/real_time/threads:32       3758 ns      11612 ns     213088\n```\nWe do this to make a reasonable comparison between the cpu time, real time,\nand, if used, manual time. Using the max (or min, i suppose) would also\nwork. Ideally we'd show the full distribution through percentiles over\nthreads, as we could do over repetitions, but the reporting becomes complex.\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Thu, Jul 27, 2017 at 5:23 AM, David Bimmler notifications@github.com\nwrote:\n\nFrom https://github.com/google/benchmark/blob/master/src/benchmark.cc#L325\nI understand that the real time reported from threads is being averaged.\nI find this behaviour unexpected, I would have intuitively expected the\nreal time in a threaded benchmark to be the max real time a single thread\nused.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/429, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMtwvDtJSdgJRvuKYdBTjIEYekVyhks5sSIFbgaJpZM4OlNSK\n.\n. closing this in favour of the new JSON work in v2.. I think that would be fine. The danger will be that when cross-compiling users will need to explicitly set up features anyway, much like you have had to, but it should at least work more transparently, maybe?. Sounds like a great plan. Patches welcome.. Thank you!. appveyor failure due to negative cpu time reported by benchmark run (!). $ cmake ..\n-- Found Git: /usr/bin/git (found version \"2.14.1.342.g6490525c54-goog\") \n-- git Version: v1.2.0-28c0e9f9\nCMake Error at CMakeLists.txt:28 (string):\n  string sub-command REGEX, mode MATCH needs at least 5 arguments total to\n  command.\n\nfollowed by many more error statements. @EricWF this is a workaround to balance the external project and subdirectory approaches, and it does work well.\ngiven this doesn't break anything and improves the experience, and aligns the project with googletest, i'm happy.. there's a small old part of me that wants to check if the output wraps at 80 columns and rail against it if it does.. sounds reasonable.\nmore and more i'm thinking we should move all non-json reporting to\nseparate tooling.\nOn Fri, Sep 1, 2017 at 10:55 AM, Juha Lepola notifications@github.com\nwrote:\n\nMarkdown support helps to streamline the publishing of the results in\nGitHub. I.e. remove the need to reformat the console output to Markdown\ntable format when publishing the results in GitHub.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/441, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMo8pVOZi21Dbgxm85xYzcwJLnAGWks5seEUfgaJpZM4PKdDq\n.\n. yes. something that can take the json output and render it appropriately would be best, i think.. the benchmark_repetitions arg will run the entire benchmark multiple times.\nyou're getting an error from here\nhttps://github.com/google/benchmark/blob/a271c36af93c7a3b19dfeb2aefa9ca77a58e52e4/src/benchmark_register.cc#L139\nwhich\nhappens when the number of threads x number of potential arguments is too\nlarge (>100 currently).\n\ncan you share your benchmark definition and registration?\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Sat, Sep 2, 2017 at 3:06 AM, Bob Fang notifications@github.com wrote:\n\nHi I am running some very simple benchmark (similar to the first example\ngiven in the README). And I have encountered these errors:\nThe number of inputs is very large. BM_UFTree will be repeated at least 18446744072586632936 times.\nbenchmark(15824,0x7fffaed953c0) malloc:  mach_vm_map(size=18446743903025913856) failed (error code=3)\n error: can't allocate region\n*** set a breakpoint in malloc_error_break to debug\nlibc++abi.dylib: terminating with uncaught exception of type std::bad_alloc: std::bad_alloc\n[1]    15824 abort      ./benchmark --benchmark_repetitions=100\nAs you can see the command line argument does not seems to have any effect\nhere. Am I missing something? Thanks!\nI am testing with clang++ on Mac.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/442, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMkujY69MnMeFGDCIhZFrXr56VE72ks5seSifgaJpZM4PK297\n.\n. sounds pretty damning.\n\n440 was tooling only, #439 was cmake only (though might change the\nfeatures built somehow?) so i'd guess #428. However, that should only be\nchanging the reporting of stats after the run.\nI can't repro this on linux, which makes me think it is #439.\nCan you share your cmake/build logs from the working and non-working runs?\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Wed, Sep 6, 2017 at 3:14 AM, Bob Fang notifications@github.com wrote:\n\nHi @dominichamon https://github.com/dominichamon the code is located\nhere\nhttps://github.com/dorafmon/cf/blob/1b7c711d1235dee795f5a32ffb9086fc3b580476/benchmark.cpp#L4.\nI don't think there is anything unusual with it. I tried to check out\ncommit d704179\nhttps://github.com/google/benchmark/commit/d70417994a3c845c49c4443e92b26a52b320a759\nof benchmark and then build the benchmark it is fine but if I check out the\nlatest commit (of benchmark) and test it I got the above error, so I am\npretty sure it is the last 3 commits that caused the issue here.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/442#issuecomment-327439930,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMsIk6sntAajx-Hr6JJ9q-zgPvfYlks5sfnB4gaJpZM4PK297\n.\n. @LebedevRI might have some insights then.. no, the API/ABI version is either the tagged version or the tagged version\n+ commit if we're between versions.\n\ni can only imagine it's some quirk of osx cmake that is causing a feature\nto be mislabeled, or maybe something specific about Bob Fang's\nconfiguration?\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Wed, Sep 6, 2017 at 2:37 PM, Roman Lebedev notifications@github.com\nwrote:\n\n@dominichamon https://github.com/dominichamon did i forgot to bump some\nAPI/ABI version that would have prevented this?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/442#issuecomment-327604915,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMlzRFI6y8yyJ38q0_93Q94TwcLoKks5sfwG2gaJpZM4PK297\n.\n. @ldionne I expect that any chart generation (or other custom output) will work best from JSON, for starters, so making that the default makes sense. Once we have this, adding a python tool to generate charts from a series of JSON output is trivial (rather than adding it to the library).\n\nGetting the tabular output is still simple:\n$ tools/tabular_report.py <benchmark binary>\nbut with this change the library itself is simpler and this opens the door to much more functionality on the reporting side.. One thing it brings is a smaller library (370kb instead of 440kb for release builds on linux), but that's a minor benefit. The broader benefit is the requests we keep getting for custom reporting methods. Instead of having two classes of output, those in the library and those supported by external tools, everything is simpler both in terms of implementation and conceptually, if reporting in the library is done one way, and external tooling provides the more sophisticated support.\nHowever, this is a user-facing change, so I'm definitely interested in your (@ldionne) and others' opinions.\nOne option is to keep the tabular and json reporters in the library, remove the CSV reporter, and then have tooling to support the csv, markdown, HTML use-cases. This would be less of a user-facing change, but would at least start us down the right path.. > On the other hand, not having the tabular format be the default is a huge usability regression for users IMO, and that should weight more than internal code clarity. \nNot having it at all would be a usability regression. I'm not removing it, just removing it from the library.\n\nMy use case is registering benchmarks as CMake targets and running them as part of my unit tests. This includes on Travis CI. Having the tabular output is very convenient because I can see the result of my benchmarks without any post-processing, wherever I run the benchmarks.\n\nI think this is still possible, but you may need an add_custom_command or execute_process to run the wrapper tool. It's not as clean as running the binary directly, but I do think the reduced complexity of the native code might be worth it.\nI'm looking into the compromise of having the tabular and JSON reporters, but not the totally custom reporters, instead supporting that through python tooling.. i'm going to close this RFC in favour of @EricWF's plan.\nFTR, i'm happy to lose CSV and use something as an intermediate format. JSON is more open sourcey, protobuf more googley. i'm not sure there's much between them for this use-case.. travis failure: https://travis-ci.org/google/benchmark/jobs/275236985. sgtm. PR welcome :). haha alphabets are hard. I'd like a patch, but are you running at head? the line numbers don't line\nup (it's 91 at head) and it was replaced with int64_t at some point.\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Wed, Sep 27, 2017 at 3:33 PM, jayfurmanek notifications@github.com\nwrote:\n\nHi,\nI've tried building this on a ppc64le machine (power8) and it fails in\nsrc/cycleclock.h:\nIn file included from /tmp/build/protobuf/third_\nparty/benchmark/src/sysinfo.cc:44:0:\n/tmp/build/protobuf/third_party/benchmark/src/cycleclock.h: In function\n'int64_t benchmark::cycleclock::Now()':\n/tmp/build/protobuf/third_party/benchmark/src/cycleclock.h:82:23: error:\n'int64' does not name a type\ntbl &= -static_cast(tbu0 == tbu1);\nIt looks like there is no 64bit ppc timebase instructions. The needed\ninstruction is 'mfspr'. It returns a 64bit unsigned int and no need for\nlow/high bit wrangling.\nI'd submit a patch, but I haven't signed the CLA yet. Please let me know\nif you need a patch and I can get that started.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/449, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMjmMiTIyyT4pUgsPeoS8QPwOE5q7ks5sms0-gaJpZM4PmfNP\n.\n. Can you share your code?\n\nDoing the naive thing (building and installing benchmark, copying the sample to a main.cc, compiling) runs fine on my machine, though it is x86_64.\nAlso, unless i'm missing something, travis doesn't support arm64 so we may be missing something there.. Can you add your example, or something representative, to one of the test files? Also, please update the documentation with a section on using these macros.. Awesome! thank you :). It may be unrelated to your change. one second while i check.. Yes. this is happening already. i'll merge this now (and fix the appveyor test i guess).. thank you!. That's fair, and it is. I thought at first it was an artifact of the\ninteger printing rules that were changed recently such that negative zero\nis no longer printed as zero[1], but we print these as floats[2] so i'm\nextra confused.\nI think a PR with signbit checks around and logging that will show up in\nappveyor is a great idea. I'll put something together to see if i can\nrepro. Unfortunately i have no windows machine to test it directly.\n1.\nhttps://social.msdn.microsoft.com/Forums/vstudio/en-US/a3b18c01-00d7-44ea-b4ef-61e44234ee94/negative-zero-valid?forum=vcgeneral\n2.\nhttps://github.com/google/benchmark/blob/master/src/console_reporter.cc#L142\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Tue, Oct 10, 2017 at 1:29 AM, Pascal Leroy notifications@github.com\nwrote:\n\nIt seems to me that this PR is papering over a problem that we don't\nreally understand.\nNegative zeroes don't show up for random reasons. In particular, they\nnever arise out of subtractions. The only ways that you can construct a\nnegative zero are by (1) multiplying a positive zero by a negative number\nor (2) multiplying two numbers of opposite signs in a way that results in\nan underflow.\nLooking at the code it's unclear to me how/where this could happen. I\nwould be more comfortable if we were to pepper the code with checks or\ntraces to understand exactly where the negative zero comes from (this is\neasy to do with std::signbit).\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/pull/453#issuecomment-335400109, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMlfMRrs6LYJ877O4-sOu1ahvYNxbks5sqysFgaJpZM4PzKgt\n.\n. looks like @EricWF has an alternative approach in #475 . I'm not sure what this site is doing exactly, but showing the NOOP bar\nsuggests the reason: CPU time for the first case is the same as NOOP time.\nIe, it does nothing.\n\nRunning the same benchmark locally:\n\"\n\nBenchmark            Time           CPU Iterations\nDoNothing            4 ns          4 ns  162935671\nDoSomething         60 ns         60 ns   11839155\n\"\nwhich is what i'd expect.\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Tue, Oct 10, 2017 at 11:44 AM, Fred Tingaud notifications@github.com\nwrote:\n\nHi,\nWhen running the following benchmark: http://quick-bench.com/\nNBZW9B9t5Xqr2FRIlk7Y8Wzlhj4 with clang3.8 -O3, the first benchmark, that\ncontains no code in the while(state.KeepRunning) body returns a higher time\nthan the next one, that creates a string. (generated assembly for both\nbenchmarks is visible in the bottom right tabs).\nThe same result happens often enough that I think I can rule out the\nhypothesis of it being due to noise.\nAm I doing something wrong here, or is the compiler actually generating\nbetter code for the non-empty benchmark?\nIn the second case, do you think there is something that could be done to\navoid this problem?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/456, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMslJBrnNsQ83kxcaCbOse71YFjHdks5sq7sGgaJpZM4P0ZRQ\n.\n. (the site is great! i've been dying for a travis-like/godbolt like\nbenchmarky thing :))\n\nclang++-3.5 and 'Release' build is this:\n```\nBenchmark            Time           CPU Iterations\nDoNothing            2 ns          2 ns  341417994\nDoSomething         46 ns         46 ns   15023047\n```\nHow do you define 'NOOP time'? Maybe it would be better to just report the\nCPU time raw?\n. Got it. Well, if the run is for all the benchmarks and you report the\ncontext alongside the relative timings of each benchmark, i think you're\ncovering that hardware issue. Ie, a rerun might give you different absolute\nnumbers, but that doesn't matter.\nSome quick tests:\noptim = None, Something == 4xNothing\noptim = O1, Something == 8xNothing\noptim = O2, Something == Nothing (but both == 0.66)\noptim = O3, Something == 0.6xNothing\nthis is not what i'd expect :)\ncan you show the raw output somewhere? maybe as a json blob on the dev\nconsole in the browser.\n. that's very odd. i'm not at all sure what might be going on.\nthe compiled assembly looks appropriate at each optimisation level.\nAs a further test I replaced the original version with the one using the new ranged-for loop construct and got this:\n```\nBenchmark            Time           CPU Iterations\nDoNothing            0 ns          0 ns 1000000000\nDoSomething         44 ns         44 ns   15281068\nwhich is exactly what i'd expect from a noop benchmark.. thanks!. Looks like the commit was made with a gotsai.com email. Perhaps if you amend the commit author to the one you signed it will work?. when tests pass, go ahead and merge it please :).\ndefine BENCHMARK(n)                                     \\\nBENCHMARK_PRIVATE_DECLARE(n) =                         \\\n      (::benchmark::internal::RegisterBenchmarkInternal( \\\n          new ::benchmark::internal::FunctionBenchmark(#n, n)))\n```\n```\ndefine BENCHMARK_TEMPLATE1(n, a)                        \\\nBENCHMARK_PRIVATE_DECLARE(n) =                         \\\n      (::benchmark::internal::RegisterBenchmarkInternal( \\\n          new ::benchmark::internal::FunctionBenchmark(#n \"<\" #a \">\", n)))\n```\nI don't think that replacing one with the other would make any difference.\ncan you put the files into a gist so it's easier to take a look at them?. $ clang++ ./set_union_bench.cc -o set_union_bench -I ~/git/benchmark/include/ -L ~/git/benchmark/_build/src/ -lbenchmark -pthread --std=c++11\nIn file included from ./set_union_bench.cc:7:\n./lib.h:209:10: error: chosen constructor is explicit in copy-initialization\n  return {f1, f2, o};\nam i missing some requirement?. You're using libc++ as well right?\nI'm still trying to get your example to compile :). I think I may have found the reason, but I haven't had a chance to test it yet. I figured instead of waiting longer to get the chance to test it, I could post my thoughts and maybe someone else can. \nhttps://dendibakh.github.io/blog/2018/01/18/Code_alignment_issues\nreminded me about issues I'd seen before with aligning functions on  i-cache line boundaries. I wonder if this is what's happening. Further, I wonder if we should recommend somewhere forcing function level i-cache line alignment when compiling benchmarks.. You can't do it in quickbench, but adding -mllvm -align-all-blocks=5 should force everything to be i-cache aligned (as per the blog post) and might eliminate the difference.. I've gone over the code for complexity and i don't see anything obvious that would artificially restrict the precision of the factor.\nhttps://github.com/google/benchmark/blob/d70417994a3c845c49c4443e92b26a52b320a759/src/console_reporter.cc#L135 might be the issue, so you could try changing that if you're building benchmark from source.. you can already install it.\nhttps://github.com/google/benchmark/blob/master/src/CMakeLists.txt#L58. Gah, sorry. moving too fast.\nCan you explain a bit more why this would be necessary? Does this cause an outer project's install step to also install benchmark?. yes.. our mingw failure is flaky. we're working on it.. thanks!\n. the runtime of test/benchmark_test on my machine:\nbefore: 2m36.588s\nafter: 2m12.475s\nI don't see this slowdown you've suggested so this looks good to merge. any last thoughts?\n. I'd like to keep everything consistent between the README and the test code. Having the examples at least be 'best practice' is a good idea.. thanks!. did you try what the warning says?\n\"CMake Warning at cmake/CXXFeatureCheck.cmake:37 (message):\nIf you see build failures due to cross compilation, try setting\n  HAVE_STD_REGEX to 0\n\"\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Thu, Nov 2, 2017 at 4:51 AM, apmanol notifications@github.com wrote:\n\nHello, this issue is probable related to #436\nhttps://github.com/google/benchmark/pull/436 , during cross compiling I\nhave the following problem:\ncmake -DCMAKE_TOOLCHAIN_FILE=/home/manap/Projects/xpu/nmsngv3/tools/share/cmake/toolchain-mips-wibasip.cmake -DBENCHMARK_ENABLE_TESTING=OFF ..\n-- The C compiler identification is GNU 4.8.1\n-- The CXX compiler identification is GNU 4.8.1\n-- Check for working C compiler: /opt/cross-mips-wibasip/cross.mips/bin/mips-wibasip-linux-gnu-gcc\n-- Check for working C compiler: /opt/cross-mips-wibasip/cross.mips/bin/mips-wibasip-linux-gnu-gcc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /opt/cross-mips-wibasip/cross.mips/bin/mips-wibasip-linux-gnu-g++\n-- Check for working CXX compiler: /opt/cross-mips-wibasip/cross.mips/bin/mips-wibasip-linux-gnu-g++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\ncrosssss compiling\n-- Found Git: /usr/bin/git (found version \"2.9.5\")\n-- git Version: v1.2.0-491360b8-dirty\n-- Version: 1.2.0\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11 - Success\n-- Performing Test HAVE_CXX_FLAG_WALL\n-- Performing Test HAVE_CXX_FLAG_WALL - Success\n-- Performing Test HAVE_CXX_FLAG_WEXTRA\n-- Performing Test HAVE_CXX_FLAG_WEXTRA - Success\n-- Performing Test HAVE_CXX_FLAG_WSHADOW\n-- Performing Test HAVE_CXX_FLAG_WSHADOW - Success\n-- Performing Test HAVE_CXX_FLAG_WERROR\n-- Performing Test HAVE_CXX_FLAG_WERROR - Success\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC - Success\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS - Success\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32 - Failed\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL - Success\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING - Success\n-- Performing Test HAVE_CXX_FLAG_WZERO_AS_NULL_POINTER_CONSTANT\n-- Performing Test HAVE_CXX_FLAG_WZERO_AS_NULL_POINTER_CONSTANT - Success\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING - Success\n-- Performing Test HAVE_CXX_FLAG_WD654\n-- Performing Test HAVE_CXX_FLAG_WD654 - Failed\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY - Failed\n-- Performing Test HAVE_CXX_FLAG_COVERAGE\n-- Performing Test HAVE_CXX_FLAG_COVERAGE - Success\n-- Performing Test HAVE_STD_REGEX\nCMake Warning at cmake/CXXFeatureCheck.cmake:37 (message):\n  If you see build failures due to cross compilation, try setting\n  HAVE_STD_REGEX to 0\nCall Stack (most recent call first):\n  CMakeLists.txt:180 (cxx_feature_check)\n-- Performing Test HAVE_STD_REGEX -- success\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile\n-- Performing Test HAVE_POSIX_REGEX\nCMake Warning at cmake/CXXFeatureCheck.cmake:37 (message):\n  If you see build failures due to cross compilation, try setting\n  HAVE_POSIX_REGEX to 0\nCall Stack (most recent call first):\n  CMakeLists.txt:182 (cxx_feature_check)\n-- Performing Test HAVE_POSIX_REGEX -- success\n-- Performing Test HAVE_STEADY_CLOCK\nCMake Warning at cmake/CXXFeatureCheck.cmake:37 (message):\n  If you see build failures due to cross compilation, try setting\n  HAVE_STEADY_CLOCK to 0\nCall Stack (most recent call first):\n  CMakeLists.txt:190 (cxx_feature_check)\n-- Performing Test HAVE_STEADY_CLOCK -- success\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /projects/Thirdparty/benchmark/builds_mips2\n/benchmark/builds_mips2 (master *%)$ make VERBOSE=1\n/usr/bin/cmake -H/projects/Thirdparty/benchmark -B/projects/Thirdparty/benchmark/builds_mips2 --check-build-system CMakeFiles/Makefile.cmake 0\n/usr/bin/cmake -E cmake_progress_start /projects/Thirdparty/benchmark/builds_mips2/CMakeFiles /projects/Thirdparty/benchmark/builds_mips2/CMakeFiles/progress.marks\nmake -f CMakeFiles/Makefile2 all\nmake[1]: Entering directory '/projects/Thirdparty/benchmark/builds_mips2'\nmake -f src/CMakeFiles/benchmark.dir/build.make src/CMakeFiles/benchmark.dir/depend\nmake[2]: Entering directory '/projects/Thirdparty/benchmark/builds_mips2'\ncd /projects/Thirdparty/benchmark/builds_mips2 && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /projects/Thirdparty/benchmark /projects/Thirdparty/benchmark/src /projects/Thirdparty/benchmark/builds_mips2 /projects/Thirdparty/benchmark/builds_mips2/src /projects/Thirdparty/benchmark/builds_mips2/src/CMakeFiles/benchmark.dir/DependInfo.cmake --color=\nmake[2]: Leaving directory '/projects/Thirdparty/benchmark/builds_mips2'\nmake -f src/CMakeFiles/benchmark.dir/build.make src/CMakeFiles/benchmark.dir/build\nmake[2]: Entering directory '/projects/Thirdparty/benchmark/builds_mips2'\n[  6%] Building CXX object src/CMakeFiles/benchmark.dir/benchmark.cc.o\ncd /projects/Thirdparty/benchmark/builds_mips2/src && /opt/cross-mips-wibasip/cross.mips/bin/mips-wibasip-linux-gnu-g++  -DHAVE_POSIX_REGEX -DHAVE_STD_REGEX -DHAVE_STEADY_CLOCK -I/projects/Thirdparty/benchmark/include -I/projects/Thirdparty/benchmark/src -I/projects/Thirdparty/benchmark/src/../include  -std=c++11  -Wall  -Wextra  -Wshadow  -pedantic  -pedantic-errors  -Wfloat-equal  -fstrict-aliasing  -Wzero-as-null-pointer-constant  -Wstrict-aliasing   -o CMakeFiles/benchmark.dir/benchmark.cc.o -c /projects/Thirdparty/benchmark/src/benchmark.cc\n^Csrc/CMakeFiles/benchmark.dir/build.make:62: recipe for target 'src/CMakeFiles/benchmark.dir/benchmark.cc.o' failed\nAnd the question is should flags -DHAVE_POSIX_REGEX -DHAVE_STD_REGEX\nshould be enabled since it failed? And the second one, is how can choose\nwhich one from cmake option. The attempts with -DRUN_HAVE_POSIX_REGEX=0\nfailed.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/468, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMvuVmTg0Kl2CETnoUUr2O3gYVHwFks5syazTgaJpZM4QPkNa\n.\n. I think you mean != 0. Fix in #477.. i wonder if this is because you're overriding CXX directly. \n\ni just tried using the method from https://stackoverflow.com/questions/30549502/installed-clang3-6-on-ubuntu-cant-select-as-alternative to set the default compiler to clang/clang++ and everything built and worked as expected.. (i realise this isn't a great solution for switching between compilers on a platform, but i'm curious if it works). fixed? @gladk . thanks!. you know what would be great? tests would be great. maybe it's time to integrate with gtest and write some actual tests. maybe not for this PR i guess.. enable LTO with gcc:\n```\n$ c++ --version\ng++ (Debian 6.3.0-18) 6.3.0 20170516\nCopyright (C) 2016 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n$ cmake .. -GNinja -DBENCHMARK_ENABLE_LTO=on\n-- git Version: v1.3.0-70cf5390\n-- Version: 1.3.0\n-- Performing Test HAVE_CXX_FLAG_FLTO\n-- Performing Test HAVE_CXX_FLAG_FLTO - Success\n-- Performing Test HAVE_STD_REGEX\n-- Performing Test HAVE_STD_REGEX\n-- Performing Test HAVE_STD_REGEX -- success\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile\n-- Performing Test HAVE_POSIX_REGEX\n-- Performing Test HAVE_POSIX_REGEX\n-- Performing Test HAVE_POSIX_REGEX -- success\n-- Performing Test HAVE_STEADY_CLOCK\n-- Performing Test HAVE_STEADY_CLOCK\n-- Performing Test HAVE_STEADY_CLOCK -- success\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /usr/local/google/home/dominic/git/benchmark/_build\n$ ninja\nninja: warning: deps log version change; rebuilding\n[44/52] Linking CXX executable test/cxx03_test\n/usr/include/c++/6/bits/stl_tree.h:594:16: note: type name \u2018std::_Rb_tree, std::allocator >, std::pair, std::allocator > const, benchmark::Counter>, std::_Select1st, std::allocator > const, benchmark::Counter> >, std::less, std::allocator > >, std::allocator, std::allocator > const, benchmark::Counter> > >::_Rb_tree_impl, std::allocator > >, false>\u2019 should match type name \u2018std::_Rb_tree, std::allocator >, std::pair, std::allocator > const, benchmark::Counter>, std::_Select1st, std::allocator > const, benchmark::Counter> >, std::less, std::allocator > >, std::allocator, std::allocator > const, benchmark::Counter> > >::_Rb_tree_impl, std::allocator > >, true>\u2019\n         struct _Rb_tree_impl : public _Node_allocator\n                ^\n/usr/include/c++/6/bits/stl_map.h:135:41: note: type \u2018struct _Rep_type\u2019 should match type \u2018struct _Rep_type\u2019\n          key_compare, _Pair_alloc_type> _Rep_type;\n                                         ^\n../include/benchmark/benchmark.h:367:40: note: type \u2018struct UserCounters\u2019 should match type \u2018struct UserCounters\u2019\n typedef std::map UserCounters;\n                                        ^\n/usr/include/c++/6/bits/stl_tree.h:361:11: note: type \u2018struct _Rb_tree\u2019 itself violate the C++ One Definition Rule\n     class _Rb_tree\n           ^\n../include/benchmark/benchmark.h:655:14: note: type \u2018void Function (struct State &)\u2019 should match type \u2018void Function (struct State &)\u2019\n typedef void(Function)(State&);\n              ^\n[52/52] Linking CXX executable test/user_counters_tabular_test\n```\nand with clang:\n```\n $ c++ --version\nclang version 3.9.1-9 (tags/RELEASE_391/rc2)\nTarget: x86_64-pc-linux-gnu\nThread model: posix\nInstalledDir: /usr/bin\n$ cmake .. -GNinja -DBENCHMARK_ENABLE_LTO=on\n-- The C compiler identification is Clang 3.9.1\n-- The CXX compiler identification is Clang 3.9.1\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found Git: /usr/bin/git (found version \"2.15.0.417.g466bffb3ac-goog\") \n-- git Version: v1.3.0-70cf5390\n-- Version: 1.3.0\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11 - Success\n-- Performing Test HAVE_CXX_FLAG_WALL\n-- Performing Test HAVE_CXX_FLAG_WALL - Success\n-- Performing Test HAVE_CXX_FLAG_WEXTRA\n-- Performing Test HAVE_CXX_FLAG_WEXTRA - Success\n-- Performing Test HAVE_CXX_FLAG_WSHADOW\n-- Performing Test HAVE_CXX_FLAG_WSHADOW - Success\n-- Performing Test HAVE_CXX_FLAG_WERROR\n-- Performing Test HAVE_CXX_FLAG_WERROR - Success\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC - Success\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS - Success\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32 - Success\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL - Success\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING - Success\n-- Performing Test HAVE_CXX_FLAG_WZERO_AS_NULL_POINTER_CONSTANT\n-- Performing Test HAVE_CXX_FLAG_WZERO_AS_NULL_POINTER_CONSTANT - Failed\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING - Success\n-- Performing Test HAVE_CXX_FLAG_WD654\n-- Performing Test HAVE_CXX_FLAG_WD654 - Failed\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY - Success\n-- Performing Test HAVE_THREAD_SAFETY_ATTRIBUTES\n-- Performing Test HAVE_THREAD_SAFETY_ATTRIBUTES\n-- Performing Test HAVE_THREAD_SAFETY_ATTRIBUTES -- failed to compile\n-- Performing Test HAVE_CXX_FLAG_FLTO\n-- Performing Test HAVE_CXX_FLAG_FLTO - Success\nCMake Error at /usr/share/cmake-3.9/Modules/FindPackageHandleStandardArgs.cmake:137 (message):\n  Could NOT find LLVMAr (missing: LLVMAR_EXECUTABLE)\nCall Stack (most recent call first):\n  /usr/share/cmake-3.9/Modules/FindPackageHandleStandardArgs.cmake:377 (_FPHSA_FAILURE_MESSAGE)\n  cmake/Modules/FindLLVMAr.cmake:9 (find_package_handle_standard_args)\n  cmake/llvm-toolchain.cmake:1 (find_package)\n  CMakeLists.txt:137 (include)\n```\nfor the latter, it probably means ensuring it's clear that something needs to be installed (which wasn't necessary before).. heh, and on my distribution, clang is 3.9 but llvm is 3.8, which causes more issues during linking after i install llvm.. thanks!. Oops, forgot to take this :). if it's simple to, can you add a travis build with gcc on osx?. (actually, maybe that can wait for another PR, but it would be great to have). thank you!. Given all checks are passing, I don't see any concerns with merging. @EricWF do you still have concerns?. waiting for @EricWF because he's much smarter than i am.. (i just pushed https://github.com/google/benchmark/commit/7f2d2cd5b9d8f892862e9c7974ede413c76f9bff which should help the travis xcode builds). Can you also add a travis entry to test the config? Example here: https://github.com/korfuri/bazel-travis/blob/master/.travis.yml . Made obsolete by #533 . the json header you're using is licensed under MIT. This means we'd have to update our license to indicate that the software is now multiple-licensed.\nCan we instead depend on this JSON library in some other way (submodules, findpackage (it uses cmake)) which would also give us the opportunity to get updates?. LGTM. handle conflicts and merge?. @KindDragon We're removing support from the core library, but retaining JSON output. It is relatively to write an external tool to read the JSON and generate the CSV should it be necessary, but the library is getting very complicated with the various non-JSON reporters.. see https://github.com/google/benchmark/pull/496\nhow is this different?. can you add a travis bazel build?. you can also change the build directory to be _build if that helps.. Just checking in.. did you have a chance to look again?. Made obsolete by #533 . for one thing, these should not be explicitly unsigned unless we're representing bitfields or closed-group arithmetic. We're not, so we should be using signed integers if we're going to move away from the standard size_t type (which is unsigned by the standard, but that was an unfortunate decision).\nwhile signed integer overflow is undefined behaviour, this is detectable with ubsan and is safer (in a way) than the wrapping that unsigned integers would give us.\nie, i'm comfortable with the bytes/items processed being intxx_t instead of size_t, but not uintxx_t.. I think this has been obsoleted by other changes.. I approve the changes that I made to this PR in the branch.. it would be nice to keep the PR focused. so one PR to handle the issue here, and another to clean up the header would be most appreciated.. Are you still looking in to this, @lijinpei ?. I guess bazel/vs2017 set those macros. we should guard against them in src/internal_macros.h. I thought about that, but i didn't want to risk confusion in the case that it's set differently.\nmaybe we could set the BENCHMARK_ one to whatever is set on commandline, and if nothing is set then detect?. int64_t if you want specifically 64-bits please, and patches welcome :). Oh right: Enthusiastic reuse of code.\nTemplating AddRange on integer sgtm, given it's a generic utility function.. works for me once tests pass and @pleroy and @EricWF have weighed in.\ni think there's a broader approach here to templatize range args but i may try that after this lands to see if it works out.. I think this PR has been obsoleted by other changes.. thank you!. I don't mind the belt-and-braces approach here. Can you simplify the check as per the comment from @chfast?. @Croydon yes.. Can you post the commands you used to compile and link your benchmark?\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Mon, Feb 5, 2018 at 9:15 PM, Naveen notifications@github.com wrote:\n\nHi I am trying to run a simplest benchmark on my amd ryzen 7 machine. I am\nseeing a core dump in RunInThread function.\n2018-02-06 12:31:10\nRun on (16 X 3000 MHz CPU s)\nCPU Caches:\nL1 Data 32K (x8)\nL1 Instruction 64K (x8)\nL2 Unified 512K (x8)\nL3 Unified 8192K (x2)\nProcess finished with exit code 139 (interrupted by signal 11: SIGSEGV)\nuname -a\nLinux lt-hkg1-dws13 4.14.5-1.el7.elrepo.x86_64 #1\nhttps://github.com/google/benchmark/issues/1 SMP Sun Dec 10 09:54:56\nEST 2017 x86_64 x86_64 x86_64 GNU/Linux\ng++ --version\ng++ (GCC) 6.4.0\nCopyright (C) 2017 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions. There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\nand the backtrace of the core is\n0 0x0000000000000000 in ?? ()\n1 https://github.com/google/benchmark/issues/1 0x000000000040dc55 in\nbenchmark::internal::(anonymous namespace)::RunInThread(\nbenchmark::internal::Benchmark::Instance const, unsigned long, int,\nbenchmark::internal::ThreadManager) ()\n2 https://github.com/google/benchmark/issues/2 0x000000000040ed50 in\nbenchmark::RunSpecifiedBenchmarks(benchmark::BenchmarkReporter,\nbenchmark::BenchmarkReporter) ()\n3 https://github.com/google/benchmark/pull/3 0x0000000000405ad0 in\nmain (argc=, argv=0x7fffa795b998) at /home/nbansal/Documents/lt/\ngit/c/lt-jarvis/bench/./include/benchedsreader.h:35\nstatic void BM_test(benchmark::State& state){\n    for( auto _ : state) std::string(\"test\");\n}\nBENCHMARK(BM_test);\nBENCHMARK_MAIN();```\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/524, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMnJP9QQXR24I6_OQY1KxuSa2jDRLks5tR9_1gaJpZM4R6grK\n.\n. Can you please provide the command lines you used to build it. Was it\ncmake? Did you use make? Can you also post the output of a clean cmake run\nso I can see what was detected?\n\nOn Tue, Feb 6, 2018, 17:30 Naveen notifications@github.com wrote:\n\nI can produce it with the sample code given in this page as well.\nOne more thing I did compile the libbenchmark from the master branch and\nthe last commit is\ncommit bc83262\nhttps://github.com/google/benchmark/commit/bc83262f9d687ac7e31509b3f5177ff391a7452a\n(HEAD -> master, origin/master, origin/HEAD)\nAuthor: Tim timothy.joseph.ohearn@gmail.com\nDate: Sat Feb 3 23:04:36 2018 -0600\n.vs/ and CmakeSettings.json to gitignore (#522\nhttps://github.com/google/benchmark/pull/522)\n[nbansal@lt-hkg1-dws13 codes]$ ./a.out\n2018-02-07 09:27:47\nRun on (16 X 3000 MHz CPU s)\nCPU Caches:\n  L1 Data 32K (x8)\n  L1 Instruction 64K (x8)\n  L2 Unified 512K (x8)\n  L3 Unified 8192K (x2)\nWARNING CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead.\nSegmentation fault (core dumped)\n[nbansal@lt-hkg1-dws13 codes]$ uname -a\nLinux lt-hkg1-dws13 4.14.5-1.el7.elrepo.x86_64 #1 SMP Sun Dec 10 09:54:56 EST 2017 x86_64 x86_64 x86_64 GNU/Linux\n[nbansal@lt-hkg1-dws13 codes]$ less bench.cpp ^C\n[nbansal@lt-hkg1-dws13 codes]$ cat bench.cpp\ninclude \nstatic void BM_StringCreation(benchmark::State& state) {\n    for (auto _ : state)\n        std::string empty_string;\n}\n// Register the function as a benchmark\nBENCHMARK(BM_StringCreation);\n// Define another benchmark\nstatic void BM_StringCopy(benchmark::State& state) {\n    std::string x = \"hello\";\n    for (auto _ : state)\n        std::string copy(x);\n}\nBENCHMARK(BM_StringCopy);\nBENCHMARK_MAIN();\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/524#issuecomment-363624906,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMlmHfW0nJwTaCqVXlMxTs9QiHR06ks5tSPykgaJpZM4R6grK\n.\n. It sounds like a nice addition. maybe a method that can be called from the\nmain, or a key-value set passed in to the main macro maybe?\n\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Wed, Feb 7, 2018 at 2:40 PM, Michael McLoughlin <notifications@github.com\n\nwrote:\nI am interested in adding custom context fields to benchmarks. As far as I\ncan tell this isn't possible right now? If not would it be possible to add?\nThe use case I have in mind is adding our software build version to the\ncontext. This way output JSON/CSV data files can be passed around or stored\nand contain the information required to reproduce results.\nNot critical since this information can be stored elsewhere. But it might\nbe nice. Thoughts?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/525, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMrYmeX72v9aCwkHXD5hK3CkTNJQpks5tSiZFgaJpZM4R9gpm\n.\n. LGTM. @EricWF ?. do you have any benchmark results for this change in benchmarks? :). LGTM. Merge when ready.. Can you add a markdown documentation file about AssemblyTests or whatever name you choose, essentially with the contents of your commit message. Maybe more detail.\n\nAnd then cross-link to it from the README? . I'd prefer if the bazel TODO linked to a github issue so we can track it, but otherwise, Ship It.. Other than the minor thing above (which could be a followup) I'm comfortable with this PR. And thank you for including the tests and travis changes.\nIt seems that maintenance is relatively sane too given the use of globs.\nLet me know if you want to replace the git_repository or not and then I'll merge.. I agree, it looks like the appveyor failure is due to flakiness in the regex where the tests run slower than expected.. Leaving this a day for more comments... @pleroy @EricWF any thoughts?. thank you!\n. thanks!. thank you!. For reference, internally there is a version that supports memory tracking but it relies on linking against tcmalloc.\nThat's certainly something we can consider but it would change the allocation patterns in the cases where benchmarks are meant to be benchmarking against the system allocator.. what's the output of the cmake where it tries to download the dependencies?. Here's an example of the equivalent running on travis (g++-4.8.4 on ubuntu): https://travis-ci.org/google/benchmark/jobs/350268756\ncan you try a clean build (just blow away the build directory entirely) and post the full log of the cmake/make output?. other than those suspicious looking warnings around HandleGtest.cmake, i\ndon't see anything obvious.\nMaybe Eric (who added gtest) knows more.\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Wed, Mar 7, 2018 at 2:23 PM, rzuckerm notifications@github.com wrote:\n\n~/git/benchmark/build$ cmake .. -DCMAKE_BUILD_TYPE=RELEASE\n-DBENCHMARK_DOWNLOAD_DEPENDENCIES=ON\n-- The C compiler identification is GNU 4.8.4\n-- The CXX compiler identification is GNU 4.8.4\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Found Git: /usr/bin/git (found version \"1.9.1\")\n-- git Version: v1.2.0-61497236\n-- Version: 1.2.0\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11 - Success\n-- Performing Test HAVE_CXX_FLAG_WALL\n-- Performing Test HAVE_CXX_FLAG_WALL - Success\n-- Performing Test HAVE_CXX_FLAG_WEXTRA\n-- Performing Test HAVE_CXX_FLAG_WEXTRA - Success\n-- Performing Test HAVE_CXX_FLAG_WSHADOW\n-- Performing Test HAVE_CXX_FLAG_WSHADOW - Success\n-- Performing Test HAVE_CXX_FLAG_WERROR\n-- Performing Test HAVE_CXX_FLAG_WERROR - Success\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC - Success\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS - Success\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32 - Failed\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL - Success\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING - Success\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING - Success\n-- Performing Test HAVE_CXX_FLAG_WD654\n-- Performing Test HAVE_CXX_FLAG_WD654 - Failed\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY - Failed\n-- Performing Test HAVE_CXX_FLAG_WINVALID_OFFSETOF\n-- Performing Test HAVE_CXX_FLAG_WINVALID_OFFSETOF - Success\n-- Performing Test HAVE_CXX_FLAG_WNO_INVALID_OFFSETOF\n-- Performing Test HAVE_CXX_FLAG_WNO_INVALID_OFFSETOF - Success\n-- Performing Test HAVE_CXX_FLAG_COVERAGE\n-- Performing Test HAVE_CXX_FLAG_COVERAGE - Success\n-- Performing Test HAVE_STD_REGEX\n-- Performing Test HAVE_STD_REGEX\n-- Performing Test HAVE_STD_REGEX -- compiled but failed to run\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile\n-- Performing Test HAVE_POSIX_REGEX\n-- Performing Test HAVE_POSIX_REGEX\n-- Performing Test HAVE_POSIX_REGEX -- success\n-- Performing Test HAVE_STEADY_CLOCK\n-- Performing Test HAVE_STEADY_CLOCK\n-- Performing Test HAVE_STEADY_CLOCK -- success\n-- Looking for include file pthread.h\n-- Looking for include file pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\nCMake Warning (dev) at /usr/share/cmake-2.8/Modules/ExternalProject.cmake:242\n(message):\nvalue 'EXCLUDE_FROM_ALL' with no previous keyword in ExternalProject_Add\nCall Stack (most recent call first):\n/usr/share/cmake-2.8/Modules/ExternalProject.cmake:1762\n(_ep_parse_arguments)\ncmake/HandleGTest.cmake:34 (ExternalProject_Add)\ncmake/HandleGTest.cmake:82 (build_external_gtest)\nCMakeLists.txt:217 (include)\nThis warning is for project developers. Use -Wno-dev to suppress it.\nCMake Warning (dev) at /usr/share/cmake-2.8/Modules/ExternalProject.cmake:242\n(message):\nvalue 'ON' with no previous keyword in ExternalProject_Add\nCall Stack (most recent call first):\n/usr/share/cmake-2.8/Modules/ExternalProject.cmake:1762\n(_ep_parse_arguments)\ncmake/HandleGTest.cmake:34 (ExternalProject_Add)\ncmake/HandleGTest.cmake:82 (build_external_gtest)\nCMakeLists.txt:217 (include)\nThis warning is for project developers. Use -Wno-dev to suppress it.\n-- Performing Test BENCHMARK_HAS_O3_FLAG\n-- Performing Test BENCHMARK_HAS_O3_FLAG - Success\n-- Performing Test BENCHMARK_HAS_CXX03_FLAG\n-- Performing Test BENCHMARK_HAS_CXX03_FLAG - Success\n-- Performing Test BENCHMARK_HAS_WNO_ODR\n-- Performing Test BENCHMARK_HAS_WNO_ODR - Success\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/developer/git/benchmark/build\n~/git/benchmark/build$ make\nScanning dependencies of target googletest\n[ 2%] Creating directories for 'googletest'\n[ 4%] Performing download step (git clone) for 'googletest'\nCloning into 'googletest'...\nremote: Counting objects: 10660, done.\nremote: Compressing objects: 100% (45/45), done.\nremote: Total 10660 (delta 37), reused 58 (delta 30), pack-reused 10580\nReceiving objects: 100% (10660/10660), 3.25 MiB | 0 bytes/s, done.\nResolving deltas: 100% (7839/7839), done.\nChecking connectivity... done.\nAlready on 'master'\nYour branch is up-to-date with 'origin/master'.\n[ 7%] No patch step for 'googletest'\n[ 9%] Performing update step for 'googletest'\nAlready on 'master'\nYour branch is up-to-date with 'origin/master'.\n[ 11%] Performing configure step for 'googletest'\nloading initial cache file /home/developer/git/benchmark/\nbuild/googletest/tmp/googletest-cache.cmake\n-- The C compiler identification is GNU 4.8.4\n-- The CXX compiler identification is GNU 4.8.4\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Found PythonInterp: /usr/bin/python (found version \"2.7.6\")\n-- Looking for include file pthread.h\n-- Looking for include file pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/developer/git/benchmark/\nbuild/googletest/src/googletest-build\n[ 14%] Performing build step for 'googletest'\nScanning dependencies of target gtest\n[ 25%] Building CXX object googlemock/gtest/CMakeFiles/\ngtest.dir/src/gtest-all.cc.o\nLinking CXX static library libgtest.a\n[ 25%] Built target gtest\nScanning dependencies of target gmock\n[ 50%] Building CXX object googlemock/CMakeFiles/gmock.\ndir/src/gmock-all.cc.o\nLinking CXX static library libgmock.a\n[ 50%] Built target gmock\nScanning dependencies of target gmock_main\n[ 75%] Building CXX object googlemock/CMakeFiles/gmock_\nmain.dir/src/gmock_main.cc.o\nLinking CXX static library libgmock_main.a\n[ 75%] Built target gmock_main\nScanning dependencies of target gtest_main\n[100%] Building CXX object googlemock/gtest/CMakeFiles/\ngtest_main.dir/src/gtest_main.cc.o\nLinking CXX static library libgtest_main.a\n[100%] Built target gtest_main\n[ 16%] Performing install step for 'googletest'\n[ 25%] Built target gtest\n[ 50%] Built target gmock\n[ 75%] Built target gmock_main\n[100%] Built target gtest_main\nInstall the project...\n-- Install configuration: \"RELEASE\"\n-- Installing: /home/developer/git/benchmark/build/googletest/lib/x86_64-\nlinux-gnu/libgmock.a\n-- Installing: /home/developer/git/benchmark/build/googletest/lib/x86_64-\nlinux-gnu/libgmock_main.a\n-- Installing: /home/developer/git/benchmark/\nbuild/googletest/include/gmock\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/gmock.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/gmock-generated-function-mockers.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/gmock-generated-actions.h.pump\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/gmock-more-actions.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/gmock-generated-actions.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/gmock-spec-builders.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/gmock-more-matchers.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/gmock-generated-matchers.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/gmock-matchers.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/gmock-generated-function-mockers.h.pump\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/gmock-actions.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/gmock-cardinalities.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/gmock-generated-nice-strict.h.pump\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/gmock-generated-nice-strict.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/internal\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/internal/gmock-port.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/internal/custom\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/internal/custom/gmock-generated-actions.h.pump\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/internal/custom/gmock-generated-actions.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/internal/custom/gmock-port.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/internal/custom/gmock-matchers.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/internal/gmock-internal-utils.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/internal/gmock-generated-internal-utils.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/internal/gmock-generated-internal-utils.h.pump\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngmock/gmock-generated-matchers.h.pump\n-- Installing: /home/developer/git/benchmark/build/googletest/lib/x86_64-\nlinux-gnu/pkgconfig/gmock.pc\n-- Installing: /home/developer/git/benchmark/build/googletest/lib/x86_64-\nlinux-gnu/pkgconfig/gmock_main.pc\n-- Installing: /home/developer/git/benchmark/build/googletest/lib/x86_64-\nlinux-gnu/libgtest.a\n-- Installing: /home/developer/git/benchmark/build/googletest/lib/x86_64-\nlinux-gnu/libgtest_main.a\n-- Installing: /home/developer/git/benchmark/\nbuild/googletest/include/gtest\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/gtest_pred_impl.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/gtest-param-test.h.pump\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/gtest-death-test.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/gtest-message.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/gtest-spi.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/gtest-param-test.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/gtest_prod.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/gtest-test-part.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/internal\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/internal/gtest-filepath.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/internal/gtest-tuple.h.pump\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/internal/gtest-type-util.h.pump\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/internal/gtest-internal.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/internal/gtest-param-util.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/internal/gtest-death-test-internal.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/internal/gtest-type-util.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/internal/custom\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/internal/custom/gtest-port.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/internal/custom/gtest-printers.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/internal/custom/gtest.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/internal/gtest-string.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/internal/gtest-port.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/internal/gtest-param-util-generated.h.pump\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/internal/gtest-tuple.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/internal/gtest-linked_ptr.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/internal/gtest-port-arch.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/internal/gtest-param-util-generated.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/gtest-printers.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/gtest.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/\ngtest/gtest-typed-test.h\n-- Installing: /home/developer/git/benchmark/build/googletest/lib/x86_64-\nlinux-gnu/pkgconfig/gtest.pc\n-- Installing: /home/developer/git/benchmark/build/googletest/lib/x86_64-\nlinux-gnu/pkgconfig/gtest_main.pc\n[ 19%] Completed 'googletest'\n[ 19%] Built target googletest\nScanning dependencies of target benchmark\n[ 21%] Building CXX object src/CMakeFiles/benchmark.dir/string_util.cc.o\n[ 23%] Building CXX object src/CMakeFiles/benchmark.dir/reporter.cc.o\n[ 26%] Building CXX object src/CMakeFiles/benchmark.dir/statistics.cc.o\n[ 28%] Building CXX object src/CMakeFiles/benchmark.dir/json_reporter.cc.o\n[ 30%] Building CXX object src/CMakeFiles/benchmark.dir/colorprint.cc.o\n[ 33%] Building CXX object src/CMakeFiles/benchmark.dir/counter.cc.o\n[ 35%] Building CXX object src/CMakeFiles/benchmark.dir/\nbenchmark_register.cc.o\n[ 38%] Building CXX object src/CMakeFiles/benchmark.dir/\ncommandlineflags.cc.o\n[ 40%] Building CXX object src/CMakeFiles/benchmark.dir/csv_reporter.cc.o\n[ 42%] Building CXX object src/CMakeFiles/benchmark.dir/\nconsole_reporter.cc.o\n[ 45%] Building CXX object src/CMakeFiles/benchmark.dir/sleep.cc.o\n[ 47%] Building CXX object src/CMakeFiles/benchmark.dir/timers.cc.o\n[ 50%] Building CXX object src/CMakeFiles/benchmark.dir/complexity.cc.o\n[ 52%] Building CXX object src/CMakeFiles/benchmark.dir/benchmark.cc.o\n[ 54%] Building CXX object src/CMakeFiles/benchmark.dir/sysinfo.cc.o\nLinking CXX static library libbenchmark.a\n[ 54%] Built target benchmark\nScanning dependencies of target basic_test\n[ 57%] Building CXX object test/CMakeFiles/basic_test.dir/basic_test.cc.o\nLinking CXX executable basic_test\n[ 57%] Built target basic_test\nScanning dependencies of target benchmark_test\n[ 59%] Building CXX object test/CMakeFiles/benchmark_\ntest.dir/benchmark_test.cc.o\nLinking CXX executable benchmark_test\n[ 59%] Built target benchmark_test\nScanning dependencies of target output_test_helper\n[ 61%] Building CXX object test/CMakeFiles/output_test_\nhelper.dir/output_test_helper.cc.o\nLinking CXX static library liboutput_test_helper.a\n[ 61%] Built target output_test_helper\nScanning dependencies of target complexity_test\n[ 64%] Building CXX object test/CMakeFiles/complexity_\ntest.dir/complexity_test.cc.o\nLinking CXX executable complexity_test\n[ 64%] Built target complexity_test\nScanning dependencies of target cxx03_test\n[ 66%] Building CXX object test/CMakeFiles/cxx03_test.dir/cxx03_test.cc.o\nLinking CXX executable cxx03_test\n[ 66%] Built target cxx03_test\nScanning dependencies of target diagnostics_test\n[ 69%] Building CXX object test/CMakeFiles/diagnostics_\ntest.dir/diagnostics_test.cc.o\nLinking CXX executable diagnostics_test\n[ 69%] Built target diagnostics_test\nScanning dependencies of target donotoptimize_test\n[ 71%] Building CXX object test/CMakeFiles/donotoptimize_\ntest.dir/donotoptimize_test.cc.o\nLinking CXX executable donotoptimize_test\n[ 71%] Built target donotoptimize_test\nScanning dependencies of target filter_test\n[ 73%] Building CXX object test/CMakeFiles/filter_test.\ndir/filter_test.cc.o\nLinking CXX executable filter_test\n[ 73%] Built target filter_test\nScanning dependencies of target fixture_test\n[ 76%] Building CXX object test/CMakeFiles/fixture_test.\ndir/fixture_test.cc.o\nLinking CXX executable fixture_test\n[ 76%] Built target fixture_test\nScanning dependencies of target map_test\n[ 78%] Building CXX object test/CMakeFiles/map_test.dir/map_test.cc.o\nLinking CXX executable map_test\n[ 78%] Built target map_test\nScanning dependencies of target multiple_ranges_test\n[ 80%] Building CXX object test/CMakeFiles/multiple_\nranges_test.dir/multiple_ranges_test.cc.o\nLinking CXX executable multiple_ranges_test\n[ 80%] Built target multiple_ranges_test\nScanning dependencies of target options_test\n[ 83%] Building CXX object test/CMakeFiles/options_test.\ndir/options_test.cc.o\nLinking CXX executable options_test\n[ 83%] Built target options_test\nScanning dependencies of target register_benchmark_test\n[ 85%] Building CXX object test/CMakeFiles/register_\nbenchmark_test.dir/register_benchmark_test.cc.o\nLinking CXX executable register_benchmark_test\n[ 85%] Built target register_benchmark_test\nScanning dependencies of target reporter_output_test\n[ 88%] Building CXX object test/CMakeFiles/reporter_\noutput_test.dir/reporter_output_test.cc.o\nLinking CXX executable reporter_output_test\n[ 88%] Built target reporter_output_test\nScanning dependencies of target skip_with_error_test\n[ 90%] Building CXX object test/CMakeFiles/skip_with_\nerror_test.dir/skip_with_error_test.cc.o\nLinking CXX executable skip_with_error_test\n[ 90%] Built target skip_with_error_test\nScanning dependencies of target statistics_test\n[ 92%] Building CXX object test/CMakeFiles/statistics_\ntest.dir/statistics_test.cc.o\n/home/developer/git/benchmark/test/statistics_test.cc:6:25: fatal error:\ngtest/gtest.h: No such file or directory\ninclude \"gtest/gtest.h\"\n^\ncompilation terminated.\nmake[2]:  [test/CMakeFiles/statistics_test.dir/statistics_test.cc.o]\nError 1\nmake[1]:  [test/CMakeFiles/statistics_test.dir/all] Error 2\nmake: *** [all] Error 2\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/538#issuecomment-371153457,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMnh6Lc0kmfdrZDgAeEVukNz9u6J8ks5tb-1mgaJpZM4STwXF\n.\n. Great, thanks!\n\nThe travis failure seems unrelated but a little suspicious as it shouldn't fail: @EricWF . create a pull request and i'll take a look, thanks!. Thank you!. I agree that moving to absl is the right approach, which is dependent on the (multiple?) outstanding PRs to use bazel (or absl moving to support non-bazel build systems). However, until then, we shouldn't be blocking PRs that fix breakages.\nEither the rename or the #undef are fine with me, though I do like the consistency argument of the rename (as a strictly short-term solution). @pleroy Is this ok with you too? . SGTM, and moves us closer to absl.. fixed?. Looks like travis and appveyor are clean. @pleroy @EricWF any comments or thoughts?. Hi Wink\nWe haven't set up a regular cadence yet. What generally happens is we look\nat the list of commits since the last release and if significant things\nhave landed we tag it. Right now, the key commits since v1.3.0 are:\n\nbazel support (non-user-facing)\ns390x, fuschia, netBSD, Solaris support\ngoogletest support (non-user-facing)\nLTO for Clang\nbetter string function compatibility\nminor fixes and documentation\n\nIt may be this is enough for a minor release, but i would like to get in a\ncouple of the open PRs (#517 or #548, maybe #530).\nIs there something specific you're waiting for from this list?\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Thu, Mar 8, 2018 at 8:06 PM, Wink Saville notifications@github.com\nwrote:\n\nI recently update ponylang to an arbitrary commit on master as they were\nat pre v1.1.0. I'd like to update ponylang to a formal release if one might\nbe coming soonish, hence my question.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/549, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMnjNqSJXWdQgN9GRWLfOWTwRQt0sks5tcY8_gaJpZM4SjRKZ\n.\n. I'll see if i can get something out this week or early next.\n\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Sat, Mar 10, 2018 at 7:32 PM Wink Saville notifications@github.com\nwrote:\n\nI submitted 3 PR's in the \"minor fixes and documentation\" category from\npony and I'd like to have an official release sooner rather than later,\nwhich is why I asked. If they went in next week and then a release that\nwould be perfect.\nSo do you think a release next week is possible?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/549#issuecomment-372058665,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMocjtDcIPKDcNZ7OzXgsIzqG_53Fks5tdCpWgaJpZM4SjRKZ\n.\n. v1.4.0 is now available: https://github.com/google/benchmark/releases/tag/v1.4.0. What version are you running?. Running the test under bazel works but using cmake/make and running it, i can reproduce the failure.\n\nLooking.... Ah, the issue is that we explicitly set the \"--benchmark_counters_tabular\" flag in cmake and bazel, so when you run the binary you need to do the same.\nAlternatively, we could change the 'main' for that test to explicitly set the flag, which would be more hermetic.. the latter actually would be a little tricky as we'd have to change the command line parsing to respect values that are set explicitly but not in argv. So run the test through make or bazel and it should pass just fine.. The latter (inline) warning we're aware of and there was some PR to fix it\nat some point.\nThe others are new. We have an appveyor build for MSVC that seems to be\ngreen (mostly): https://ci.appveyor.com/project/google/benchmark. This\nincludes a Vis Studio 2017 configuration, so i'm curious what's different\nabout your setup.\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Sat, Mar 10, 2018 at 6:47 AM, Andrej Andrejev notifications@github.com\nwrote:\n\nWe are trying to use Google Benchmark with our Visual Studio 2017 project.\nWhen compiling, the following errors:\n... \\test\\output_test_helper.cc(206): error C2039: 'streampos': is not a member of 'std::basic_ios>'\nalso\n... \\test\\output_test_helper.cc(210): error C2784: 'bool std::operator >(const std::pair<_Ty1,_Ty2> &,const std::pair<_Ty1,_Ty2> &)': could not deduce template argument for 'const std::pair<_Ty1,_Ty2> &' from 'std::fpos<_Mbstatet>'\nAlso the warnings:\n... \\include\\benchmark/benchmark.h(645): warning C4141: 'inline': used more than once\nSame for line 648, as the macro BENCHMARK_ALWAYS_INLINE expands to\n__forceinline.\nChecked with STL versions 14.11.25503, 14.10.25008, as shipped with MS\nVisual Studio. It compiles fine with gcc on Linux though.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/551, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMi4p5pziEKldJOR5DKiP5s4oJ9yYks5tc3cBgaJpZM4SlKxs\n.\n. should we close this issue?. @EricWF who added the check.\n\nSimply, the static_assert exists to ensure that we keep the performance-necessary struct layout.. I noticed that in the bazel builds. We currently disable that warning for our gcc builds: https://github.com/google/benchmark/blob/master/CMakeLists.txt#L123. It's disabled in the cmake, so doing that in bazel sgtm. but only if it's gcc (ie, if the warning is supported).. Is this still something you're looking into, @EricWF ?. Is this still something you want to pursue @EricWF ?. ping @EricWF . ping :). PR in progress.. I think this might be an issue with how the protobuf project have integrated this project, as we don't support autoconf directly. the error also seems to be referencing third_party/googletest which is not part of this project. I'm happy to help, but i don't know what the m4 directory would be or why we'd need it.. Looks like v1.3.0 wasn't annotated: \n$ git show-ref -d --tags | cut -b 42-|sort |sed 's/\\^{}//' | uniq -c | sed 's/2\\ refs\\/tags\\// a /' | sed 's/1\\ refs\\/tags\\//lw /'\n       a v0.0.9\n       a v0.1.0\n       a v1.0.0\n       a v1.1.0\n       a v1.2.0\n      lw v1.3.0. I think i've resolved this. I retagged with annotation and force pushed the new v1.3.0 tag.\n$ git show-ref -d --tags | cut -b 42-|sort |sed 's/\\^{}//' | uniq -c | sed 's/2\\ refs\\/tags\\// a /' | sed 's/1\\ refs\\/tags\\//lw /'\n       a v0.0.9\n       a v0.1.0\n       a v1.0.0\n       a v1.1.0\n       a v1.2.0\n       a v1.3.0. Can you try with -DBENCHMARK_DOWNLOAD_DEPENDENCIES=ON?\nThat should work for sure and would help me determine where the issue might be.. Checking a few sizes, it looks like it scales as power series in both cases (unsurprisingly) but the nopartial is indeed ~10x faster. For very small vectors (N < 100), the existing method is marginally faster.\nThe only issue might be the memory overhead which, if i'm understanding correctly, is roughly 2x for the full copy (assuming the partial sort copy is done in-place).\nThis trade-off seems reasonable to me, and the code is simpler to read.. This PR LGTM then. I'll merge it shortly.\n\nWe're on #googlebenchmark on freenode IRC, or the google group at  https://groups.google.com/d/forum/benchmark-discuss, or you can just open an issue here.\n. I like the problem statement, but why not just change the separator in the output name? We already use '/' for threads, maybe we should do '.' for stats suffices.. That's fair. I'm a little concerned that it might break existing tooling, though i hope tooling is using the JSON output rather than console.\nMaybe it's worth doing that as a first step, actually: Having a unique ID output for the JSON (why isn't it just the \"base name\" of the benchmark, by the way?) and leave the console output alone?. let me give an example:\nc_code, c_code_mean, c_code_min would all have a field base_name that would be c_code.\ni think this is a more obvious ID (and more likely to be unique) than an integer, and it would be deterministic between runs no matter what benchmark_filter you use, which isn't the case (i think) for your current solution.. @BaaMeow yes, the appveyor regex needs to be updated to handle the longer times. it hasn't become an issue important enough to fix yet, but that's in direct proportion to how straightforward the fix is. :face_with_head_bandage: . LGTM.. I'll wait for @EricWF or @pleroy to comment as they have Opinions, usually.. Let me see if i can cut through the issues here. It really comes down to what bit of the benchmark name you think identifies a unique \"benchmark\". I can confirm that the benchmark name is the registered name + thread count + number of repetitions. It is not just the registered name.\nFor tooling reasons, you really want to include these things in the \"base name\" as you want to compare like-for-like. If you drop these, you will no longer be able to correctly understand what's happening with the benchmark.\nNow, the benchmark names were originally written pre json, With json, i imagine we could have:\njson\n{\n  \"name\": \"NOP\",\n  \"threads\": 8,\n  \"repetitions\": 3,\n  \"iterations\": ...\n  ...\n},\n{\n  \"name\": \"NOP\",\n  \"threads\": 16,\n  \"iterations\": ...\n  ...\n},\n...\nie, parse out the various bits that are interesting about the benchmark for ease of parsing and tooling. The console output tries to do this by having a full name that includes the basename and parseable separators, but you've identified it's not ideal.\nWe can migrate to this by adding these extra fields, leaving the legacy name, and adding a base name that is just the name as @BaaMeow has defined it.\n@LebedevRI does that get us to where you can use the json for tooling?. Adding fields to JSON shouldn't break anything, and it should make tooling easier to have the name broken apart.\nI'd do it for JSON but not bother for CSV. You might also want to do it in the v2 branch with @EricWF's blessing, as they're working on JSON changes.. The first is a bug that i'll push a fix for now (#569).\nThe second is a known issue that was already fixed by disabling the warning in source (#552) .. Ah maybe... @EricWF who knows more about the compiler side and how we might support that. Are you using CMake or bazel to build? I think we have assumed clang, msvc, or g++ at this point.. How was this resolved?. I think this is expected if you don't have LLVM FileCheck available... @EricWF . The upshot of this is that someone changing their configuration will need to clean their cmake? works for me but @EricWF fiddles with configs more often so they should weigh in.\n. SGTM. nice catch, thanks!. If you run with --benchmark_repetitions set to >0 then you'll get the std deviation reported already. Similarly, there are minimum, maximum, mean, median stats already calculated.\nI understand this is the std dev for a given run rather than across runs, but the fundamental notion is that we need some number of iterations in a run to get a good confidence in the reported timing. As such, i'm not sure that this is really going to be useful information for a user, as any particular iteration can be affected by a number of external variables, throwing off the min and max, and possibly the stddev.\nI do like the idea of using the stddev to guide when a particular run is \"good\", though if we're going to do that we really should use standard error to derive confidence intervals for the timing data (assuming the data is normal, etc, etc).\n. I'm going to close this because I think my last comment explains where we're coming from. Feel free to open an issue or another PR if you want to discuss it further.. You can say no, but is there any way we can add a test? Now that we integrate with googletest (which was done much later than the original implementation) it should be possible to write a unit test.. great, thank you!. Thanks!. perhaps we should accelerate a dependency on absl and pull sysinfo from there instead? and merge any changes we've made independently.... and make absl work for cmake :P. it looks like absl:base is public and exposes internal/sysinfo.h in its hdrs. so i can't use it in this project if i depend on abseil? pleeease? it'd be one more thing that helps me decide to start using abseil... :D. thanks for the more focused PR. Yes, MinGW is supported. We don't maintain a list of supported platforms because we probably would miss some on which it works.. Neat, thanks! . thank you. do you know if there's any way to set up a continuous test for enscripten builds?. whoops, thanks!. There's two ways to think about the number of observations. the number of\niterations for a given run is the number of observations that lead to the\naverage times that are reported. similarly, when repeating and getting the\nsecond level of statistics, we're looking at the stability of the\nbenchmark, and in that case the number of observations is the repetition\ncount. So i think it depends on what it is you want to look at with regard\nto the statistics. Now after the fact, we've lost the information about\nindividual iterations, and we only have the statistics. While it's somewhat\npossible to reconstruct the distribution given enough statistics\n(percentiles or quartiles, for example) it's not really viable to store and\noutput every iteration.\nI'm not sure it's true (any more?) that every repetition should have the\nsame number of iterations (as per the comment on line 105). I'm also agree\nthat the number of iterations in a statistic should likely follow the\nstatistic: the mean should be the mean across repetitions, the std dev\nshould be the std dev across repetitions, etc, to give more information\nabout what's going on with the benchmark.\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Fri, May 4, 2018 at 10:14 PM Roman Lebedev notifications@github.com\nwrote:\n\nRight now each statistic contains the same iteration count as the first\nrepetition.\nhttps://github.com/google/benchmark/blob/50ffc781b1df6d582d6a453b9942cc8cb512db69/src/statistics.cc#L156\nhttps://github.com/google/benchmark/blob/50ffc781b1df6d582d6a453b9942cc8cb512db69/src/statistics.cc#L128\nhttps://github.com/google/benchmark/blob/50ffc781b1df6d582d6a453b9942cc8cb512db69/src/statistics.cc#L105\nAre we sure this is the correct value that we should be outputting?\nWe don't actually store each of the iteration times, but average them, and\noperate \"on averages\".\nAre we sure we don't want to put the repetition count there?\nI'm currently looking into finally adding t-test stuff into tools, and\nthus thinking about \"so what is the actual number of observations\"? (and i\nguess the answer is - the repetition count.)\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/586, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMlKSpXQC0FinG-ZeD0oNX6Jc4kF_ks5tvMSdgaJpZM4TzR4_\n.\n. I don't know the right answer. I think the iteration count is \"wrong\" for statistics outputs, because you expect the values in the row to reflect the stastic. ie, for the 'mean' it should be the 'mean' number of iterations across repetitions. That doesn't make sense today, because we assume that every repetition runs the same number of iterations. It likely does, but i'm not sure if it's guaranteed with the batch running.\n\nPerhaps then it makes sense to not output the iterations at all for statistics rows.. that's odd... i've been running cmake from a build directory for ever:\n$ cd _build\n$ cmake ../\nand it was working fine.\nI don't think this breaks anything, but i'm curious what other differences you might have on your platform.. oh i see.. your build directory is parallel to the source directory :). why do you think this is unnecessary? isn't the instance of std::tm being default-initialized here? i don't believe it has a user-defined constructor that would trigger value-initialization (or zero-initialization).\nif you do want to avoid the memset, i think you still need an initializer ({}) after the variable name to trigger safe initialization.. Unfortunately, the benchmark header using C++ headers like <algorithm>, and the requirement that a benchmark takes a benchmark::State, mean that compiling benchmarks with a  C compiler isn't going to be possible.\nHowever, I believe you might be able to benchmark C code by having the code in an extern \"C\" block and called from a C++ benchmark. Something like:\n```c++\ninclude \nextern \"C\" {\n    int increment(int i) {\n        return i+1;\n    }\n}\nstatic void BM_IntAddition(benchmark::State& state) {\n    int i = 0;\n    for (auto _ : state) {\n        i = increment(i);\n        benchmark::DoNotOptimize(i);\n    }\n}\nBENCHMARK(BM_IntAddition);\nBENCHMARK_MAIN();\n```\n$ clang++ benchmark.cc -o benchmark -std=c++11 -lbenchmark -stdlib=libc++ -lpthread\n$ ./benchmark \n2018-05-10 09:06:45\nRunning ./benchmark\nRun on (4 X 3400 MHz CPU s)\nCPU Caches:\n  L1 Data 32K (x2)\n  L1 Instruction 32K (x2)\n  L2 Unified 256K (x2)\n  L3 Unified 4096K (x1)\n***WARNING*** CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead.\n***WARNING*** Library was built as DEBUG. Timings may be affected.\n------------------------------------------------------\nBenchmark               Time           CPU Iterations\n------------------------------------------------------\nBM_IntAddition          6 ns          6 ns  118022178. Conceptually, yay.\nNow, the details in which the devil resides.\nWhat populations are you comparing? You can't be comparing the full set of iterations, because we don't have this. So this means you're comparing the repetitions, which are already averages of iteration times. Idle thought: should we have a mode that outputs every iteration into a JSON file? :D\nI think this is ok, as long as the documentation is clear, but I wonder if you might want to require more than two repetitions to define a distribution.\nMore specifically still, a t-test is only useful when we have normally distributed results to compare. Do you have any reason to assume a priori that the distributions of benchmark repetition results are normally distributed? It's true that means of samples from a distribution (which is what we're talking about) tend towards normal distribution (thanks, central limit theorem!), but how quickly, and how large each sample needs to be, and how many samples you need, depends on how skewed the original data is, iirc.\nA more general approach (which is just as easy to implement, and almost as efficient, and has little variation from the t-test when the data is normally distributed) is the Mann-Whitney U test. From this you can also derive a \u03c1 statistic which gives a measure of overlap between distributions and is a direct measure of how much \"better\" one distribution is than another. This might be a more interesting, if little known, result.. ok. i'll merge this and then do a followup to default to true? (so it's easier to roll back if we need to. which we likely won't).\n. thanks!. we use c++11 throughout benchmark. perhaps it's worth adding a platform check for this, if it's unique to Android?. there are some semantic differences between the old and new methods. The newer std:: versions will throw exceptions on error, while the old versions will return 0 (or 0.0) and set errno. Strictly speaking, if we're going to use those we should check errno just in case something goes wrong reading the system info.\nI think it may be worth wrapping them in a method that uses #ifdef BENCHMARK_OS_ANDROID to call the new or old versions, and check the errno correctly.  . Thanks so much for helping clean up the documentation for first time users. If you have any other specific issues, please feel free to raise an issue, create a PR, or contact us on freenode IRC at #googlebenchmark.. If you have specific questions then I think we'll be able to tackle them,\nand then we can use that to add more to the documentation. Feel free to\nstart a thread on the benchmark-discuss google group so we can capture it.\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Thu, May 24, 2018 at 11:53 PM mattreecebentley notifications@github.com\nwrote:\n\nThanks - I think it needs a lot more work, it's just that I don't actually\nunderstand what's going on enough at this point to re-write it. There's no\ndetails about what methodology the benchmarks is using, and I can only pick\nup anecdotal stuff elsewhere.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/pull/600#issuecomment-391890289, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMqUBj7uuO2StuwOTVg-ZUiQbmSXoks5t1znlgaJpZM4ULbWq\n.\n. @pleroy This would be a followup, but I'm also not sure we'd want that anyway. I like to run my tests in parallel when sanity checking library changes and a single test binary would take quite a while to run.\n\nUnless we can do both: define a library for each test with a binary target that links in benchmark_main, and then an uber_test binary on top that links all the things?. OK, sure, that works. It's been a while since I used VS but that sounds fine.\n@pleroy Are you ok if we do this in a follow up?. The google group https://groups.google.com/forum/#!forum/benchmark-discuss\ndoesn't seem to have any posts since Apr 6th, but it generally gets good\nresponses.\nthe IRC channel googlebenchmark on freenode is generally good for\nquestions, but it may be that our timezones don't overlap. Please do feel\nfree to use this channel, or groups, or IRC and someone will see it\neventually :)\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Fri, May 25, 2018 at 8:19 AM \u6a0a\u667a\u8f89 notifications@github.com wrote:\n\nHi:\nthe only document I can find is the README.md in this repo. I checked\ngoogle-group, IRC channels, looks all of them are inactive. Finally I raise my\nquestion\nhttps://stackoverflow.com/questions/50425896/google-benchmark-code-setup\non stackoverflow, but still few people answered.\ndo we have more documents for this tool or some better places to ask\nquestions?\nThanks\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/602, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMk0CskWoPldL-kXKH-_NBuR_Erf3ks5t17CVgaJpZM4UNfC1\n.\n. My undocumented and therefore completely made-up policy has been to support whatever comes \"out of the box\".\n\nThe latest LTS from Ubuntu is 18.04 with cmake 3.10.2, but the \"current oldest\" LTS is 14.04. That version is 2.8.12. In xenial (16.04), it's 3.5.1.\nDebian stable is at 3.7.2-1.\nIf we go with \"oldest_of(2 ubuntu LTS + debian stable)\" then we're looking at 3.5.1.\nI'm ok bumping to 3.5.1 and documenting the policy as the above. I think this is going to be unique to cmake though, as bazel wasn't around back then :P\n. oh well then. 3.5.1 sgtm! :). @astrelni I think this should help.. I'm ok with this. git blame can be limited to a range of commits, and git log is a better tool for tracking changes over time anyway.\nI agree that git clang-format would be ideal, but given we've had a number of PRs where the entire file ends up being changed, i'd rather get to a 'known good' state.. Thanks!. thanks for the quick fix.. rebased and repushed. . closing this and creating a new (fresh) PR.. 1. It's used for internal testing of the framework. It is not used as part of the library itself.\n2. No features will be lost by disabling the tests.. Can you share your source as well? It's hard to know what's going on just from the error messages.. The definition of BM_ParseBenchmark should only take a benchmark::State& argument. I think your benchmarks are defined to take a State and a std::string:. In the output example you have:\n\"repetitions\": -3689348814741910324,\n      \"threads\": -3689348814741910324,\nthis doesn't look right.\nAlso, if the ID only serves to group foo:repeats/3 and foo:repeats/3_mean, but you want to also know when it's a statistical output vs a BM run, you could likely add a field for whether it's statistical. maybe a \"stats\" field that would be \"mean\" or \"median\" that could be queried.\nI'm still missing why unique IDs are necessary when we have any number of names that can be generated, that at least will be consistent across runs.. I'm a fan of any extra documentation. The README is getting a bit long though, so I wonder if a 'troubleshooting' or 'faq' doc might be useful with some of the tips pulled out from the README.. That would be great. README.md should likely be minimised with links to other docs :). LGTM but I'll let someone else comment too.\nMaybe @EricWF or @pleroy have opinions.. is this still something you wanted to pursue?. Ahah, I wondered what was going on and why it hasn't failed in the past (or in my local builds). This is something I really hope clang doesn't embrace.\n@LebedevRI I think this is not googletest being compiled with warnings, it's our test code being compiled with warnings (which is a good thing) and a macro being expanded from googletest.\nLet me see if i can disable the warning (though i can't repro the issue locally to check).. Oh even better, we're enabling -Wfloat-equal.. Thoughts on API welcome. This will be used internally with our closed-source memory management and measurement handlers.. Um, I don't know. Yes, everything committed to master was done with a CLA in place. Unless maybe someone was able to remove their agreement after committing to master? I dunno... . You're going to need to get approvals for the following commits:\nhttps://github.com/google/benchmark/commit/19048b7b65875e08c1e882e644a7a5f9bcfd3f82\nhttps://github.com/google/benchmark/commit/e9a49be7f1fd6e3717687e42ad318957e8cd6c8e\nhttps://github.com/google/benchmark/commit/ff2c255af5bb2fc2e5cd3b3685f0c6283117ce73\n(and https://github.com/google/benchmark/commit/726a9bde2491ea226c1d7f84ea7cf9722edd2b94, but that's because of the others).\nthe users in question are @jwakely and @guoyr who apparently made those commits with email addresses that are no longer listed as current, or something similar.\nI think we either need to have them update their emails on record to the ones in the commits, or do some sort of rebase where we 'edit' those commits and change the authorship. . It's up to @EricWF if the v2 branch is really v2 or just JSON-wip. for now marking them as deprecated seems reasonable, but that suggests they're going to be removed Soon (but i don't think we would by 1.6).. Thank you!. Sorry for the delay on this. I think it's a useful data point to have so I'm happy to merge it.. This is already done in both the BUILD.bazel and src/CMakeLists.txt files,\nand it's documented\nhttps://github.com/google/benchmark/blob/847c0069021ade355b7678a305f3ac4e4d6f7e79/README.md#windows-with-cmake\nunder known issues.\nIs there anywhere else we should add it?\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Wed, Jul 11, 2018 at 12:29 PM degski notifications@github.com wrote:\n\nOn windows, one needs to link, in addition to benchmark.lib ,to\nShlwapi.lib. It would be good to include that in the documentation-page, as\nit would avoid that others also waste an hour or 2, figuring that out.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/634, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMqR9VvjFrKXUxvqZJA0lsENyqR6Aks5uFeGXgaJpZM4VK9yD\n.\n. If you can find a clean way to introduce that to that part of the\ndocumentation (and remove it from the known issues part) then i'd happily\ntake a look at the PR.\n\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Wed, Jul 11, 2018 at 1:59 PM degski notifications@github.com wrote:\n\n@dominichamon https://github.com/dominichamon\nOn the READ.me page it says:\nDon't forget to inform your linker to add benchmark library e.g. through\n-lbenchmark compilation flag. Alternatively, you may leave out the\nBENCHMARK_MAIN(); at the end of the source file and link against\n-lbenchmark_main to get the same default behavior.\nthere somewhere, I would say, doing that does not work (on it's own) and\nwith vcpkg, that bit is automatic anyway, the linking to that library is\nnot though.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/634#issuecomment-404160002,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMgJru1zzNLXYYpDe0U8zAa8It0uxks5uFfbNgaJpZM4VK9yD\n.\n. PRs always welcome.\n\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Fri, Jul 13, 2018 at 2:02 AM Croydon notifications@github.com wrote:\n\nLinux, macOS and Windows static builds seem to be fine, Windows shared\nfails and I don't understand why yet\nIdeas would be appreciated\nhttps://ci.appveyor.com/project/Croydon/conan-google-benchmark-83vi6/build/job/q9nm3togdwumnct8#L1609\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/635#issuecomment-404694687,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMs87Mbpv0s3mddes5Q4P4Xu__7d2ks5uF_GTgaJpZM4VL1Ud\n.\n. @aJetHorn Can you confirm that you're ok with this PR being merged? Thanks.. How does this work on installation on linux when the public header is copied to /usr and benchmark_export.h doesn't exist? or does it also get created for linux, and get copied to /usr on installation?\n\nThis seems awfully intrusive. I think short term not supporting shared libs on windows is reasonable, until we can figure out a better way.. Can this PR be closed then?. I don't see anything odd in the attached JSON. Might this be a windows python interpreter issue?. There are conflicts so syncing to master HEAD and resolving would be useful before we can continue to review the changes.. @mattreecebentley If you're going to take that attitude when presented with reasonable comments by an established contributor to the project, perhaps it's best you spend your limited time creating PRs for other projects.\nRelated, perhaps it's time i created code of conduct and respectful communications expectation documentation for the project.. Sorry, I thought you were going to accept and use committer rights ;). Those assembly tests suggest that this is creating worse assembly. Ie, more instructions being added around the main benchmark loop. going to abandon this PR.. is this ready for review, @Croydon ?. I've set up a benchmark repository on bintray, but i can't import this repository as it is under the 'google' organisation. I'll see what i can do to set this up.. There are two things causing my hesitation:\n1. I have no idea if this is correct because I don't know anything about Conan.\n2. I haven't been able to get access to the google conan project (yet) to set up a benchmark one.\nIs it safe to merge this until 2 is satisfied?. Help me out: Is anything else necessary for this to be checked in if we use dominichamon/benchmark (and maybe update it later)?\nI'm not completely content with having API keys in the travis config in github so an alternative to that would be useful.. Hi @memsharded.\nThanks so much for weighing in! If there is a way to split this i'd really appreciate it so i can understand the changes bit-by-bit.. #728 was merged. was there anything else you were waiting for?. Thanks!. @EricWF Do you think you'll have a chance to review this?. I think CLA is failing because it includes email addresses that aren't explicitly in my profile. Specifically dominichamon@users.noreply.github.com which is the github internal email address.. Double checked CLA and everything is fine. it's all me in the author list.\nReview please?. appveyor failed due to expectation of 0-6 integers for user counter test time but it's up to 7.. Thanks!. Oops, sorry. Looks like I merged it a bit too quickly. Feel free to send a PR with just the test update if you want.. LGTM. Please report this to googletest as there's nothing we do with that mutex_ in benchmark.. LGTM. I'm a little fuzzy on the usefulness, but it also doesn't affect much, so LGTM.. I don't know much about windows development, but from reading the API it seems reasonable that this would be a better source of data than the registry.\nI also don't know what the difference is between BENCHMARK_OS_WINDOWS, which checks for _WIN32, and the WINAPI_FAMILY_PARTITION macro. If the latter is available for all the windows versions we expect to support then using that to define BENCHMARK_OS_WINDOWS in src/internal_macros.h sounds fine.. @pleroy Correct, it's only for information. I'm assuming that the registry might be less correct than the windows API call.. I have nothing further. I've been unable to come up with a satisfactory testing approach without radical replumbing of the file reporter.. ```c++\ninclude \ninclude \nstd::filesystem::read_symlink(\n    std::filesystem::path(\"/proc/self/fd\") / std::to_string(fileno(tmpfile()))\n);\n```\nwould at least work on linux :P\n. The test bots do a pretty good job of covering 32-bit and such, but unfortunately didn't catch this one. We must be missing a configuration.. Nice!\n. checking with our CLA experts to see what's possible.. Resolved in bb15a4e3bf4c5941ee7124d284ed9ef96e9a1c68.\nThanks, @mattkretz!. Appveyor seems unrelated. LGTM.. works for me. i don't have windows (let alone arm) on which to test.. I'm not philosophically opposed to whatever makes most sense. As per the previous discussion, I'm fine with bumping cmake and i'd forgotten about #613. If someone wants to do that instead and close my last attempt go for it.. This seems to work really well.  I wonder if it's worth extracting the class into a benchmark_runner.cc implementation file to help reduce the overwhelming benchmark.cc file size.. Thank you!. Thanks!. i don't think cmake is required for tests, it would be a googletest unit test of the function, but you may have to use gmock to mock out IsColorTerminal... I'm ok with that waiting for another time.. What's wrong with always printing 3 decimal places instead of 3 significant digits, and not having the check for < 100?. oh you updated that from .666 so i wouldn't complain about rounding :P\nI think we're hitting newer use cases as this project gets broader adoption. In general, the library was expected to be used to run benchmarks with similar timescales, so you'd get ns or ms all the way down. If we're not seeing that then we need to think a bit about how to handle it.\nOne option is to pick a base time scale for the entire run and use that everywhere. I'm not sure that's a great idea, but it would at least make eyeballing the runs easy. When you mix timescales you need to take care that, say, 10ms and 100ns are not being mixed up by the reader.\nHigh-level answer: I'm not a fan of the extra whitespace between the numbers and the units.. I think it would be. You'd have\n100 ns\n100 us\nor worse\n100 ns\n 10 us\nand would have to take care if you're comparing by eye. Perhaps we should have something like: one time scale per benchmark family?. If it doesn't change time unit i think that's ok. . LGTM. I ran it and master and took a diff and it looks much nicer with the extra precision.. @LebedevRI is there a concern you have with removing the check for manual time? This might lead to results being reported when not enough iterations have been run, but then users can increase the minimum time using the MinTime function. However, it will result in manually timed benchmarks completing in a more reasonable time without waiting for all the iterations to run. That seems like a win to me.... I believe that change was trying to tackle the problem from the other side, and it's proven by this change, sort of. The heuristic isn't helpful when using manual timing, except that it is when the benchmark ends up running for way too long.\nPerhaps there's a middle ground:\nc++\n(b.use_manual_time && (results.real_time_used >= 20 * min_time)) ||\n(!b.use_manual_time && (results.real_time_used >= 5 * min_time))\n. ping @EricWF . Can you provide a screenshot/link to the resulting output before and after so it's capture in the PR?. I'm missing the subtlety here. why is it ok to remove the results count check?. I can't parse this change at all, sorry. I need much more information on why this is necessary and what you're going to do with it.\nHowever this is much more complex code and I don't understand why it needs to be.. I finally had the time to code up my version of this.\nI think it's possible to do with less magic: no templating, no *this assignment, by removing the iters and seconds fields from the IterationResults. Instead, these become methods that are overridden. Then, to avoid the memory overhead, if we're going to be averaging we can essentially throw away the individual results.\nYou end up with methods for \"has_error\" \"iterations\" \"seconds\" and \"real_time_used\".\nmakes sense?. Sounds about right. PR welcome :)\nOn Mon, Oct 22, 2018 at 6:47 PM Miro Knejp notifications@github.com wrote:\n\nThe arguments passed to benchmarks are of type int64_t however when they\nget reported they get truncated to 32 bits. The following is output where\nthe argument keeps increasing beyond int precision and is printed\nincorrectly (but the benchmark does the right thing).\nbaseline_push_back/std::vector/67108864/manual_time              33079613 ns   32812500 ns         20\nbaseline_push_back/std::vector/134217728/manual_time             71565844 ns   71428571 ns          7\nbaseline_push_back/std::vector/536870912/manual_time            539197109 ns  531250000 ns          1\nbaseline_push_back/std::vector/1073741824/manual_time           914751775 ns  890625000 ns          1\nbaseline_push_back/std::vector/-2147483648/manual_time         1806131008 ns 1796875000 ns          1\nbaseline_push_back/std::vector/0/manual_time                   3629563138 ns 3625000000 ns          1\nI think the problem is that\nhttps://github.com/google/benchmark/blob/edc77a3669026eddc380721d5a3cdccd752b76cb/src/benchmark_register.cc#L185\nshould use the %lld format specifier instead.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/714, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMhrlXVyZHWJxNYhSkRupkqKEo0Rrks5ungSdgaJpZM4Xz17s\n.\n. Thanks!. do {std::abort(); } while(0) should be a good replacement for unreachable code in any compiler, i think.. travis is still unhappy (librt mismatch) and i unfortunately don't have time to pursue this. closing this PR but hoping that someone has the time to bump cmake.. thank you!. That sounds completely reasonable. Patches welcome :). LGTM. Thank you all!. It's using the webhook, which isn't deprecated. However, i'll look into the\napp version.\n\nOn Mon, Nov 26, 2018 at 2:54 PM Croydon notifications@github.com wrote:\n\nAppVeyor is enabled and builds commits and pull requests:\nhttps://ci.appveyor.com/project/google/benchmark/history\nHowever, it never reports back the status to the GitHub status API and\ntherefore GitHub never displays if Windows builds succeeded or did fail.\nNot for commits and not for pull requests.\nThis should be fixed. Seems like the Github repo <-> AppVeyor integration\nonly works somewhat.\nMaybe it uses some old integration and not the AppVeyor GitHub app?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/729, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMgH0oRIR1eynrCg7mpBkdORbsARAks5uzACkgaJpZM4YzMCO\n.\n. From what i can tell this is an issue on the google github -> appveyor authorization side. i'll have to contact the owners of the appveyor google profile for help with it.. It looks like android is being reported as BENCHMARK_OS_LINUX, but it\ndoesn't have getloadavg.\n\nThe right fix is likely to define a BENCHMARK_OS_ANDROID and then make sure\neverything that currently is compiled in is also compiled when that is\ndefined.\nOn Mon, Nov 26, 2018 at 6:50 PM Marat Dukhan notifications@github.com\nwrote:\n\nmaster version (I tested revision c9311a4\nhttps://github.com/google/benchmark/commit/c9311a44e1280853632fe2472345dd04514a2f74)\nfails to build on Android, with the error:\n/Users/marat/benchmark/src/sysinfo.cc:586:21: error: use of undeclared identifier 'getloadavg'\n  const int nelem = getloadavg(res.data(), kMaxSamples);\n                    ^\n1 error generated.\nRepro:\ngit clone https://github.com/google/benchmark.gitcd benchmark\nmkdir build && cd build\ncmake -DCMAKE_TOOLCHAIN_FILE=/path/to/android/ndk/build/cmake/android.toolchain.cmake -DBENCHMARK_ENABLE_TESTING=OFF ..\nmake\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/731, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMlIQxqbi9f7pKq3cQXelyOCoz8M6ks5uzDfxgaJpZM4Yznil\n.\n. Thanks! I'll keep an eye on the travis build.. You're right that this is a mistake in the documentation.\n\nRunning all of them is the expected design as that is the default case. See also gtest_filter in google test which works the same way.. sorry, yes. i just reparsed that as i was updating the documentation :)\nI think that is a good idea.. Hang on, sorry, let me think about this again.\nWhy would you want to pass \"\"? If you're running the code but not running any of the benchmarks, what are you doing?. good to merge?. Thanks!. We used to have a way to control the minimum iterations directly, but it\nturns out that it is easy to misconfigure. We found that every use-case\ncould be covered by minimum time or direct iteration settings and having\nboth caused quite a bit of extra complexity in the framework. So we removed\nit.\nI'm not entirely convinced that we need to put it back, as you note that\nboth the minimum time and Iterations() calls can be used.\nCan you give specific examples where the stability of a benchmark is\npredictable a priori and Iterations() doesn't do what you need?\nOn Mon, Dec 17, 2018 at 5:44 PM Adam Harries notifications@github.com\nwrote:\n\nAt present, it is not possible to specify a minimum number of iterations\nfor a benchmark. This is an issue when the framework decides that a given\nconfiguration for a benchmark should only be run a small number of times,\nfewer than we would like for the benchmark to be stable. The only way to\nmodify the number of iterations is by explicitly specifying\nIterations(...) for a given benchmark, or by passing\n--benchmark_min_time=... to run the benchmarks for a specific length of\ntime.\nOne possible solution would be a corresponding\n--benchmark_min_iterations=... flag that could be passed at runtime,\nwhich would prevent the framework from running the benchmark for any fewer\nthan N iterations, where N is some number by which we know that the\nresult should be stable.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/746, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMmJ9YzkKP8__aowLYP_8swzZEF4xks5u59gUgaJpZM4ZW1PW\n.\n. One of the likely misunderstandings is your use of \"stable\". We don't do\nanything to determine statistical stability in tracking iteration counts.\nWe merely run the number of iterations required for the benchmark to run\nfor a minimum time. This is why the only things that (currently) affect it\nare changing the minimum time or setting the number of iterations.\n\nIf you want to increase the minimum number of iterations, increase the\nminimum time. It will mean the smaller runs will run for more iterations,\nbut that would be the case if you increased minimum iterations too :)\nYou can see the check here\nhttps://github.com/google/benchmark/blob/master/src/benchmark_runner.cc#L266\nand the iteration increase logic here\nhttps://github.com/google/benchmark/blob/master/src/benchmark_runner.cc#L243\n.\nAn aside, if you're timing GPUs, you might already be using manual timing.\nIf you are, then we will run until the manual time you report is greater\nthan the minimum time.\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Tue, Dec 18, 2018 at 2:05 PM Adam Harries notifications@github.com\nwrote:\n\nAs I (think) I explained above, Benchmark::MinTime doesn't suit my\nrequirements, as it affects all configurations, not just long running ones.\nI've not come across State::KeepRunningBatch(), I'll take a look.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/746#issuecomment-448231578,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMrzdaOeYvlkPQMGjoHUBmm3kyVkoks5u6PY4gaJpZM4ZW1PW\n.\n. On Tue, Dec 18, 2018 at 4:34 PM Adam Harries notifications@github.com\nwrote:\nGood to know about the statistical stability of the benchmarks - and what\ncontrols the number of iterations. I wonder if an alternative solution\nmight be to implement alternative strategies for limiting the iterations?\ne.g. keep running the benchmark until the standard deviation is 5% of the\nmean?\nIt's possible, and I thought i'd opened an issue about this at some point\nin the past but i can't find it. A similar heuristic would be related to\nthe confidence interval becoming 'stable', ie the rate of change of the\nconfidence intervals is below some rate. They're roughly the same idea\nthough.\n\nThe reason why I haven't dug in to work on this as it becomes harder to\ntune as the timing and iteration counts become even more implicit.\n\nIt will mean the smaller runs will run for more iterations, but that would\nbe the case if you increased minimum iterations too :)\nThat wouldn't be the case - my faster configurations run for (say) 20,000\niterations, while my slower ones run for (say) 5 iterations. Let's say both\ntake about 10 seconds total. If I could set a minimum number of iterations\n(say, 30), the former set would still run for 20,000 iterations, and the\nsecond set would run for 30.\nIf I can only set the minimum time, however, then I need to set it to\nthe equivalent of 30 iterations for the slower configuration (60s). In this\ncase both the faster and slower configurations will run for that long,\nmeaning that the whole set takes 2 minutes, rather than 1m10s, and the\nfaster benchmarks will run for 120,000 iterations which slows down the\nwhole suite of benchmarks.\nI hope this makes my use case a bit clearer?\nUnderstood.\n\nAs a workaround you could split the ranges into multiple registration\ncalls with MinTime called on each individually. Ie:\n```\nstatic void BM_MatrixThing(benchmark::State& state) {\n  auto M = createMatrix(state.range(0), state.range(1));\n  for (auto _ : state) M.process();\n}\nBENCHMARK(BM_MatrixThing)->Ranges({{32, 128}, {32, 128}})->MinTime(5);\nBENCHMARK(BM_MatrixThing)->Ranges({{128, 512}, {128, 512}})->MinTime(10);\nBENCHMARK(BM_MatrixThing)->Ranges({{512, 2018}, {512, 2048}})->MinTime(50);\n```\ndoes that help?\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/746#issuecomment-448283808,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMm4Uuu9V8kNqiso3SjID8q9iynX3ks5u6RktgaJpZM4ZW1PW\n.\n. On Tue, Dec 18, 2018 at 4:49 PM Roman Lebedev notifications@github.com\nwrote:\nI wonder if an alternative solution might be to implement alternative\nstrategies for limiting the iterations? e.g. keep running the benchmark\nuntil the standard deviation is 5% of the mean?\nIt's possible, and I thought i'd opened an issue about this at some point\nin the past but i can't find it.\nSeparate iterations is a step in that direction :)\nIt shouldn't be necessary; there are iterative calculations for mean and\nstandard deviation that aren't terrible to implement.\nAs a workaround you could split the ranges into multiple registration\ncalls with MinTime called on each individually.\nThat is actually what i meant, i should have spelled it out explicitly :/\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/746#issuecomment-448289001,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMioTEGt_ZsHupe5T9eSvbtXOoiwTks5u6RyMgaJpZM4ZW1PW\n.\n. This does seem uncontroversial, yes. Can you rebase?. I wonder if your benchmark is doing something to the memory layout for the process by creating so very many allocations and you're essentially running out of memory? That is a lot of allocations.... Probably because the header is a mess. I'd love to simplify it down to just the public stuff with forward declarations for the private bits, but i'm not sure how possible that is.. It's not clear to me what this changes. Does it mean you don't need to remember to add -lpthread yourself when linking with the library?. it seems like an innocuous change, there's precedent, and it solves a problem. LGTM.. i'm not familiar with cmake-gui, but my guess is that this is related to the benchmark macros that can request cmake to download the dependency.\n\ngiven cmake-generated projects work from cli (Makefile and ninja, at least... those are what i use) i don't think we need to add the link libraries more generally.. thanks!. Hi\nI'm not sure anyone has tried that before, but maybe someone will pipe up.\nIn the mean time, your best bet is likely a custom reporter that uses Log,\nor (if you think it would be useful) changing the console reporter in the\nlibrary to use a Log class on android. The former is likely going to be\neasier.\nDominic Hamon | Google\nThere are no bad ideas; only good ideas that go horribly wrong.\nOn Fri, Feb 1, 2019 at 8:07 AM Nikola Mrzljak notifications@github.com\nwrote:\n\nHi!\nI'm trying to integrate the Google benchmark library into an Android NDK\nproject in order to test my company's product. The problem I'm encountering\nis that I can't display the output of the library in the logcat.\nJust to mention, since Android doesn't have a main function but provides\nonCreate and other lifecycle methods I have called the benchmark inside the\nonCreate method which invokes NDK method. The function that is invoked\nlooks like:\nvoid performTests() {\nint argc = 1;\nchar* argv[1] = {\"test\"};\nbenchmark::ConsoleReporter reporter;\nbenchmark::Initialize(&argc, argv);\nif (benchmark::ReportUnrecognizedArguments(argc, argv)) return;\nbenchmark::RunSpecifiedBenchmarks(&reporter, nullptr);\nstd::stringstream strOutStream;\nstrOutStream << reporter.GetOutputStream().rdbuf();\nstd::string strOutput = strOutStream.str();\nLOGD(\"%s\", strOutput.c_str());\n}\nAll I get is an empty string inside the strOutput.\nIs there a way to achieve the display in logcat or writing the output to\njson from ndk?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/759, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAfIMo40Hqd9_ldEyNuZFqT7lwCQTs1pks5vJGZUgaJpZM4aemlw\n.\n. LGTM. I'll leave @LebedevRI to do the merge honours if they're happy.. I'd expect that too, but i think that negative ranges haven't come up often. range is an int64_t so it should work in that respect, but my guess is the interpolation code doesn't actually support it.. let's see!\n\nhttps://github.com/google/benchmark/blob/d8584bda67320937334b038e736f5bffcd438875/src/benchmark_register.h#L10\nwe have a CHECK that lo is >= 0 which isn't encouraging, and then we insert lo followed by a loop starting at 1. So we don't actually support negative ranges.\nI'm really surprised this hasn't come up before.... \nThe patch seems reasonable to me. I ran it side-by-side with the master version and didn't see much difference in the basic tests (which use threads) so i might be missing where the benefit is. Could you demonstrate the problem with a code snippet or some output?\ntest/user_counters_test.cc:241: CheckAvgThreadsRate: Check `std::fabs((e.GetCounterAs< double >(\"foo\")) - ((1. / e.DurationCPUTime()))) < ((0.001) * (1. / e.DurationCPUTime()))    ' failed.\u00b7                                                                                                                                                                      \n  1 test/user_counters_test.cc:241: BM_Counters_AvgThreadsRate/threads:2:\n  2 test/user_counters_test.cc:241: expected (double)foo=88.0367 to be EQ to 133.142\n  3 test/user_counters_test.cc:241: with tolerance of 0.133142 (0.1%), but delta was -45.1053 (-33.8776%)\nLooks like some tests need updating :)\n. Thanks for the motivating example, that helps.\nThe current code supports the following concept:\n user code can be run on a single thread, or multiple threads in paralllel by the benchmark framework\n the benefit (and scaling limitations) of threading is then observed by the thread-0 (main thread) processing being ~constant until we hit CPU pressure, but the items per second processing increasing.\nYour motivation, and the more general user thread issue, isn't something we've considered. We already have a 'manual timer' that tells the library to get out of the way, which sets a precedent for having a 'this is user threaded' setting, but that's clunky.\nI think @LebedevRI is on to something already with #671 and other discussions about sorting out our timing so we are explicit about per-(benchmark)-thread time vs total process time.. It's up to me to check the commits are all by one of the signed folks (CLA bot doesn't like PRs from multiple people).\nIt looks like this satisfies the original request, but at the increase in complexity of the API. Is there a way we can change the default behaviour to be something sensible and expected without adding another boolean option and benchmark method? (the answer might be no, but i should ask).. I didn't see any concerns. The one thing, then, is to make sure we call out how the timing works and what's being measured in every case. Perhaps it's worth splitting this documentation into a separate 'timing.md' document that can be referenced from the README.md?. looks like a bad bazel installation on one of the build bots. Nothing related to your change.\ncla-bot doesn't seem to agree you signed it. can you make sure that the commits are under the same email address as the one you used to sign the CLA? thanks.. As per IRC: The divisor there has been moved around a fair bit, and manual timing was introduced later, so it's entirely possible that we've gotten away from where we need to be here.\nI would appreciate a write-up of what should happen in every case that we can review, and then implement (from scratch) and document.. Thanks!. travis error is unrelated (unsurprisingly... no QNX bots).. thanks!. This is great. Restrictions and overhead aside, having a binding is better than not having one (and as you point out we can improve it over time if necessary).. It's entirely possible (unlikely i think) that a user has a CPUInfo in some statically allocated memory in their binary. If we were to run the destructor for the CPUInfo and their destructor accessed that CPUInfo then we'd have a crash on shutdown (due to the destructor order issue).\nInitialization is thread-safe (in C++11, which we require in the library), and we avoid any destruction issues by not running the destructor.\nThis is the standard approach to safely managing static objects.. appveyor looks like a false negative; all tests passed then it failed on result upload.. use nullptr instead of 0 please\n. there are constants defined in sleep.h that will make this clearer.\n. uint32_t please\n. why 100ms? we're only sleeping for 1ms in this thread above so this seems a little long to wait. Did you come to this through trial and error or reason? or is it just a round number?\n. i believe 'done' can be a bool now instead of an int.\n. you can remove the note. all the pthread stuff should be platform specific but this at least works for pthread/posix platforms.\n. how do you feel about installing from include/ to include/ so they'll end up in /usr/local/include/benchmark/... ?\n. you should be able to combine this into one check:\nif (__ARM_ARCH >= 6) will not be true if __ARM_ARCH is not defined.\n. this can also be:\ndouble Benchmark::MeasurePeakHeapMemory(const Instance&) {\n. why not use git submodules?\n. ah yes .. i've run into that before.\n. rebase to #27 so this isn't necessary please\n. would it be easier to not do the multiply or the divide if iterations == 0?\n. this should be the only one we care about, I believe.\n. Once using cmake, you can add vcproj/sln to this list. If groups of these are created in subfolders (?) then please just ignore at the folder level.\n. the windows port of pthread could be brought in as a third party dependency. ExternalProject (cmake thing) might then be useful for bringing it in as we do with gtest.\n. can this be defined to __forceinline, or is the order different between this and gcc attributes?\n. we should be able to remove a bunch of these given that we don't compile nearly the same set of things that Craig Silverstein had in mind when he wrote this.\n. might we? :)\n. i'm a little uncomfortable making this part of the public API. Is there any way to avoid it?\n. this is a nice bugfix - can you pull it out into a separate PR please?\n. there might be better advice here .. set a minimum time using --benchmark_min_time ?\n. maybe we should have a src/pthread.h that does the same as lines 4 - 12 here.\n. please pull this out. i actually had some issues when i tried using std::regex for this project as the matching wasn't quite the same. I don't remember the details, i'm afraid, but i'd want much more testing before making this change.\n. TODO? log a warning that it's not supported?\n. 2 spaces too much?\n. add some indents here please. two spaces for the code should be enough to make it readable.\n. one thing we could do instead of this is separate the implementation into two files: re_std.cc and re_posix.cc then conditionally include in the build at the cmake level. what do you think?\n. we won't just use regex directly. we need to support older compilers that don't have it.\n. this should be brought in as a separate library in cmake rather than being copied into the project wholesale. We already do this for googletest using ExternalProject and that should work just fine here too.\n. Yes, as long as it's supported back to around gcc 4.6. I think it is, though you still need to -lpthread for the older compilers, iirc. It also should be in MSVC 11?\n. should this now be in an 'if !c++11 ' block?\n. check for regex_search?\n. please add a test for wildcard using * just so that is documented somewhere for users. \n. this won't work if i pass a filter in to benchmark_test, will it?\n. ah right - because they use pthreads under the hood for implementation. This may not be necessary for all compiler/OS versions, but it probably is ok to leave in.\n. nit: you don't need the else.\n```\nif (count == expected) {\n  return 0;\n}\nstd::cerr ....\nreturn -1;\n``\n. i'm surprised this doesn't match any whenCalculate*` does. A concern for another day ;)\n. i believe to do what you want, this would need to be initialized as a pointer to ensure it isn't freed at all.\n. there's no need for this syntactic sugar. just name the method benchmark_mutex() and change references to calls.\n. if you make the above a pointer, you don't need to do this.\n. returning a non-const reference goes against the style guide (google c++ style guide).\nplease return a raw pointer that isn't freed. that or make it a member of the benchmark families class, if the lifetime should be tied to that.\nthis also avoids the hack on line 339.\n. huh. that's frustrating given the google style guide around include ordering. Maybe you can also add a comment to explain that this ordering is important (and why) so good samaritans don't put it back :)\n. why change this?\n. this should come in too please\n. 2015 :) (if we need a copyright notice at all, which i believe we no longer do.)\n. either follow the directory naming or use #pragma once.\n. can we document ownership transfer by passing a std::unique_ptr instead?\n. as per the current version, i'm not sure that exposing this is the best API. take a look at how main looks with the current version to see what i think a good API is.\n. make sure you test this with the 64 to 32 shortening warning.\n. none of this needs to be in the public API, as you can see from the current version. please don't undo all this work :)\nto be clear, these are in the public API but part of the reporter interface.\n. I don't think so, given the requirements for registering a benchmark. but maybe then the comment is misleading :)\n. we (in master) use ifdef DEBUG quite a bit. are you sure you want to remove this?\n. does this need to be here?\n. attribute((unused)) ?\n. it might be best (as per the main.test branch) to have this as a separate library like googletest does it.\n. do you need the min and max?\n. should this be just wrapped in to how we report times? ie, report times with units.\n. lambdas in C++03?\n. benchmark_filter\n. maybe these header changes can be made on master in a separate patch.\n. there is a SetLabel that takes a std::string.\n. any reason why you moved this up?\n. nit - command should come after color .. alphabet and all :)\n. this is unfortunate - should we add the src/ folder to the test/CMakeLists.txt include_directories instead?\n. SGTM\n. s/benchmarks/benchmark_filter/\nalso - is this not used?\n. oh i get that. the re stuff has been a bit of a weird extra thing that we test -- i wonder if there's a better way of managing it.\nMaybe we should add tests to the benchmark suite to explicitly test the 'find matching benchmarks' functionality instead, which is part of the public API.\n. agreed - please add a TODO (or create an issue - or both!) to remove the re_tests and add the regex matching as a benchmark test against the public API.\n. strings instead?\n. should we just depend on glog? maybe create an issue to optionally use glog if it is available.\n. use an enum please\n. newline between the C and C++ includes?\n. no point in 'inline' here.\n. maybe kill the internal namespace and continue the anonymous one instead?\n. & with type please\n. const method?\n. good point. it's a knee-jerk reaction to seeing const char*.\n. ack.\n. i did some work to get glog to use enums but it never landed. it's safer in that it's easy to check for expected levels. i suppose we don't really need that here as we're only emulating VLOG.\n. which of these actually need to be in the header? i'm guessing not all.\nwould you mind checking before you merge?\n. colorprint before commandlineflags\nminimal_benchmark after log\n. 'all' is not a valid regex\n. why not just one number for iterations?\n. why not use the existing anonymous namespace?\n. no constructor?\n. the BenchmarkFamilies class above is a helpful abstraction. I'm not sure why you aren't using it.\n. does this build on our target platforms?\n. unique_ptr?\n. change in behaviour. why do you want an empty spec to do nothing?\n. why an explicit exit here? just let the function exit so people calling this aren't surprised.\n. put this back under internal please\n. any chance you can move this change to another patch? it's making it harder to review the deltas between the benchmark API.\n. that's something we should change... it's a hangover from the first version that had 'all'.\n. i think the single number is a cleaner API that captures the same benefit. How the number of iterations and time interact is 'tricky' and i think deterministic behaviour is better.\n. in this case you're removing one. I'd rather the callsite was cleaner and didn't have to set things (if we refactor things later).\n. i think there's more churn here because you're removing it.\n. can you please try this same change but with the implementation kept private? i'll be amazed if the overhead of the function call matches the inlined version, and the benefit to moving it is a simpler API with fewer internals exposed.\n. this was changed to fix a 64-32 shortening issue. be careful here :)\n. does this do something now? if not, an empty implementation in the header should be fine.\n. you don't think this comment is useful?\n. as per the previous, this friend declaration should be private\n. g++-4.6 should allow for >> so avoid the spaces.\n. int -> size_t for 64-32 shortening.\n. we should be in the benchmark namespace here... no?\n. calling exit from within a library is a bad idea.\n. did you mean to leave iterations and min_iters and max_iters? it's not clear how these interact.\n. do we need to check if this is even possible on the platform before allowing it?\n. can this be determined automatically? ie, if the time is < than 1e-9 then switch to pico.\n. spec can't be empty here\n. why the rename of the method?\n. i don't see that it's an issue, but i'm not that particular about it.\n. does 0 still mean they're time based? also, period at the end of the sentence please :)\n. still necessary?\n. std::exit in a library is unusual. we should really return a boolean (or raise an exception ;))\n. why not set num_threads to NumCPUs directly and do away with kNumCpuMarker? (see old implementation <---)\n. std::string is not a POD. This shouldn't be global or shouldn't be a std::string. a std::string* would work.\n. aside: this should probably be printed in the header info, not per run. maybe an issue is needed - what do you think?\n. can you leave this where it was (in the anonymous namespace at top of file)?\n. all these on the left were changed to cope with 64-32 shortening. please don't change them.\n. might be worth keeping the same help string as before then.\n. because the timings would be massively different, i suppose? hence having a warning in the header instead.\n. added issue\n. when the runs are time based, this isn't necessarily correct. This was a fix made in the current version to report the mean iteration count correctly. i don't think you should change this.\n. see the current implementation for an important bugfix when tracking iterations as a stat.\n. this could be in the anonymous namespace, right? as could TimerManager?\n. nullptr should be available in g++-4.6 - is there a reason to not use it?\n. it is.\nmin_time = 0 -> multiplier = 0.0 and is_significant = true (because XX/0 is > 0.1)\n. the current implementation has bugfixes for printing that would be nice to not lose. it's not pretty, but it's the only way to get tabular data in all cases.\ndon't break this please :)\n. s/bench_instance/benchmark/\n. we should create an issue to change spec.empty() to mean none.\n. weird .. you're getting the filter test stuff in here but it's not showing up on the other side. maybe a rebase and push?\n. if you're not iteration bound then multiple runs (repetitions) may have different iteration counts. when it comes to printing the mean column, it's weird to print the total iterations in that column. the fixed version currently implemented tracks it as a stat so it is correctly reported.\n. sure.. it means \"i don't care how quickly this runs\" :)\n. why not? if min_time is set then different runs might take different iterations to get there (assuming some variance in the benchmark time) which will lead to mean_iterations != total_iterations. unless i'm missing something?\n. oh.. that sounds like a bug. if i want the min_time and i don't care about iterations, i want each repetition to run to min_time.\n. i thought you were going to remove this?\n. I don't know. I do know that it's weird to report the total when the row is for the mean. That's the main thing that I fixed by making it a statistic.\nie, line 163 below outputs a total for a mean.\n. while you're here, we should do something better ... maybe use setlabel instead of cout?\n. BENCHMARK_MAIN?\n. can you leave the nullptr stuff alone? or does it conflict with standard library benchmarking?\n. please remove these in favour of setting it on 'state' for consistency with everything else.\n. probably. it's much more consistent.\n. std::numeric_limits::epsilon() ?\n. i wonder if there's a better name here... benchmark_macros? benchmark_impl? benchmark_api?\n. this doesn't need to be internal.\n. that sounds like a test header :)\nnaming is hard... i think benchmark_api is the closest ...\n. this could be inside the if statement, right? and there's no point in it being a unique_ptr in this case as ownership is not passed.\nif RunMatchingBenchmarks took a unique_ptr then this might be useful...\n. does this make the above check redundant (in GetDefaultReporter)?\n. consider a jsonreporter.cc file to minimize lines per file.\n. const char build_type[]\n. ah true.. i was thinking that we might be spawning threads in RunMatchingBenchmarks still which would cause an issue with the lifetime of reporter.\nYou're absolutely right of course.\n. static protected method on the base?\nbut yeah. .. not obvious.\n. can you add documentation to the comment above (and/or the README) regarding the use of this?\n. these don't need to be defined for C++11 right? can they be in the #else below?\n. BENCHMARK_PRIVATE_NAME2 could still be called BENCHMARK_CONCAT (or BENCHMARK_PRIVATE_CONCAT) which is a more useful name.\n. It takes 3...\nOn Mar 18, 2015 4:17 PM, \"Eric\" notifications@github.com wrote:\n\nIn include/benchmark/benchmark_api.h\nhttps://github.com/google/benchmark/pull/102#discussion_r26717368:\n\n// Helpers for generating unique variable names\n-#define BENCHMARK_CONCAT(a, b, c) BENCHMARK_CONCAT2(a, b, c)\n-#define BENCHMARK_CONCAT2(a, b, c) a##b##c\n+#define BENCHMARK_PRIVATE_NAME(n) \\\n-    BENCHMARK_PRIVATE_NAME2(benchmark, BENCHMARK_PRIVATE_UNIQUE_ID, n)\n\nOk, but I think it is weird to call a function CONCAT when it only takes\none input.\n\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/pull/102/files#r26717368.\n. fabs? or is it a better thing to catch this at flag parse time?\n. newline please\n. note this is just for real time measurements, right? CPU time shouldn't actually be affected.\n. no need to check for high_resolution_clock?\n. does this need to be exposed in the header?\n. does it need the default argument?\n. hide this in the cc file and remove the default = false please.\n. you'll need to take this into account in the width of the first column in tabular format.\n. no need for this .. just add this macro to the BM_Factorial benchmark above to show how it works.\n. i think you can use this in the other benchmarks too with the CalculatePi benchmark. \n. nit: update the comment please\n. consider flipping this so the positive is first:\n\n```\nifdef OS_WINDOWS\ninclude \ninclude \nelse\n....\nendif  // OS_WINDOWS\n```\n. it seems like changing MyCPUUsageRUsage to be this implementation for Windows is more accurate.\n. don't put a name on here at all please .. just TODO:\n. i didn't want to mess with it too much because it was doing voodoo.\n. do we need BENCHMARK_UNUSED to be public any more?\n. given you always pass this, you can kill the EmptyLogger above.\nhonestly, you can probably make the logger global and not even pass it through everywhere :)\n. can you add some TODO (maybe) to limit the revision to versions we know we support?\n. fair enough. i tend to err on the side of less code, minimal functionality, to avoid any subtle issues.\nhowever, this isn't on the critical path so it seems reasonable.\n. it's not required, but for consistency these should have {} even though they're single-line.\n. {} please\n. looks like travis is stuck at 2.8.7. We're using that as a proxy for minspec as it is Ubuntu 12.04 LTS which seems like a good base system.\nhaving said that, we could add something like https://gist.github.com/winterz/10cc2741d466cbd8ff21 to the travis build script to get the Trusty cmake (which is 2.8.12 iiuc).\n. that'll mean we'll only be linking them in the shared version again, right?\ni think that's ok for now. having the find_package(Threads..) is an improvement even in the static case where we don't use them as not linking against it in your final executable causes runtime errors.\n. i'm pretty sure this will statically link pthread into the static library, no? i'm not completely familiar with how target_link_libraries works under the different variants in cmake.\n. this doesn't seem to show up in the travis UI the same as the old version did.\n. not a typo. iff means if-and-only-if.\n. this change is unrelated.\n. no need to rename this.\n. bikeshed: benchmark_min_max\n. is this min/max real time or cpu time? how can we make the distinction?\n. explicit on one-arg constructors please.\n. we probably don't need this documentation twice. adding it to the README and then having some short note here is enough.\n. explicit on one-arg constructors\n. to document the ownership passing, this could take a std::unique_ptr<Benchmark> directly.\n. it's a shame we can't rely on the constructor/destructor pair to be setup/teardown. however, i agree that that won't work well here.\n. s/_Test/_Benchmark/ :)\n. it looks like BaseClass only has to be a Benchmark, rather than a Fixture.\nShould the RegisterBenchmarkInternal method be overloaded on Fixture and FunctionBenchmark to ensure users don't create their own Benchmark specialization that defines Run and then something odd breaks? \n. nit: be consistent. FixtureBenchmark to match FunctionBenchmark\nor flip them both: BenchmarkFixture and BenchmarkFunction. \n. thinking about it more, if someone wants to hack around and find a way to create a benchmark specialization, more power to them. i'd love to hear their usecase.\n. is this longer than 80 cols?\n. s/Test/Benchmark/ throughout.\n. Can Benchmark hold a unique_ptr and then use move instead of copy?\n. can you add SetUp and TearDown that get called so we see the usage?\n. this depends on the source being built without coverage, right? where is that dependency captured?\n. i'm not sure you want to restrict this at all.. is there no way to get the 'make' command from cmake?\n. it's not the cmake compiler that's the problem .. it's the generator and/or make. can you make this error clearer?\n. could you make this change please?\ni'm not sure it's a great idea to have different compiler flags across debug/release boundaries because it's too easy to forget to test release before pushing. travis helps a little, but it's still a potential point of failure. however, i can see the benefit in flexibility.\n. why?\n. i may be reading this wrong, but doesn't this always add the --coverage flag?\n. you could check which of gcov, lcov, genhtml, ctest, or the coverage flag are unset :)\n. AUTHORS are people who own the copyright, ie, if a person signs the CLA on behalf of a company, that company should be in here.\nCONTRIBUTORS are people who contribute but do not have copyright necessarily.\nEveryone who contributes should be in CONTRIBUTORS. Everyone who claims copyright should be in AUTHORS.\n. the usage says 'true' and 'false' while the help says 'yes' and 'no'. \n. a boolean might be better with default 'true' being overridden if the TERM doesn't support it.\n. !HasBenchmarkFlagPrefix?\nalso, redundant given how we parse the strings above.\n. this should be taking a string.\n. neither of these functions are useful.\n. $ test/benchmark_test --benchmark_format=foo\nbenchmark [--benchmark_list_tests={true|false}]\n          [--benchmark_filter=]\n          [--benchmark_min_time=]\n          [--benchmark_repetitions=]\n          [--benchmark_format=]\n          [--color_print={true|false}]\n          [--v=]\nthis is working as intended. the checks for specific args come next.\n. we don't need HasBenchmarkFlagPrefix (see earlier comment) and SkipPrefix is only used by it.\nFor colored output, this is not necessary.\n. I think they should all be taking std::string. @EricWF might have a preference though as some headers should be C-ish only.\n. #ifndef OS_WINDOWS is consistent with the rest (though i do prefer the long form).\n. or * 1.0e-7 to be consistent with the above.\n. can you keep these alphabetical please?\n. prefer scientific notation for readability please (1e7 or * 1e-7)\n. is the indent off here or is github playing tricks on me?\n. i wonder if this would be clearer if the OS_WINDOWS check was outside the local check.\n. or if the time setup was in one check, and the strftime call in the other....\n. if you push to the branch from which this PR was created, the code should update.\n. sure :)\n. this gives a warning in msvc? oy vey.\n. i agree, however this is test code so we might be a little less strict if it allows them to run on a whole new platform.\ni don't see any other workaround either for android. sadly their std implementation is broken in a few ways. perhaps we could define BENCHMARK_OS_ANDROID and have custom char -> unsigned long parsing just for them.\nalternatively we could bring in stringstream here to parse the number instead, though that seems like overkill.\n. this is a better option i think. unless @EricWF has any reason to object?\n. no reason for a newline here\n. nit: please use yyyy/mm/dd for clarity (iso 8601)\n. cstdio\n. good point. sorry for the noise.\n. I just noticed this version bump, but it was released in 2013. Commenting for posterity that it was noticed and approved.\n. possibly more readable\nif (NOT DEFINED CMAKE_INSTALL_LIBDIR)\n  set(CMAKE_INSTALL_LIBDIR \"lib\")\n  if (\"${CMAKE_SIZEOF_VOID_P}\" EQUAL \"8\")\n    set(CMAKE_INSTALL_LIBDIR \"lib64\")\n  endif()\nendif()\n. i don't know if i agree with my original assessment. it's not that important - they both work.\n. please leave this in\n. this shouldn't be necessary. the rest of the cmake config needs to work without this set, so there's no need to set it.\n. you should be able to combine these if/elses into a single block.\n. it might even be worth having an msvc.cmake and gcc_clang.cmake that can be conditionally included to make this cleaner.\n. why did these become so much more complicated? (i don't know anything about appveyor so i'm genuinely curious)\n. ah, that's a shame. does setting the policy to 'new' mean we have different behaviour and we have bugs in the existing cmake config, i wonder.\n. can we start with making the compiler ID check the outer conditional and bringing all these sections together:\nif (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\")\n  add_cxx_compiler_flag(-W4)\n  add_definitions(-D_CRT_SECURE_NO_WARNINGS)\n  if (BENCHMARK_ENABLE_LTO)\n    set(CMAKE_CXX_FLAGS_RELEASE \"/GL\")\n    ....\n  endif()\nelse()\n  add_cxx_compiler_flag(-Wall)\n  add_cxx_compiler_flag(-Wextra)\n  add_cxx_compiler_flag(-Wshadow)\n  ...\n  if (BENCHMARK_ENABLE_LTO)\n    add_cxx_compiler_flag(-flto)\n    ...\n  endif()\nendif()\n. oh, neat! thank you!\n. we take pains to make other values human readable. could we do the same here?\nie, instead of a flag, can we add the unit suffix to the output and automatically detect if the value should be ns, us, ms, etc?\n. right. that's essentially what we do with the values that go into the benchmark name.\nit might make at-a-glance comparison a little more awkward if a single benchmark jumps between units. i'm not sure what's best.\ni do know that flags are clumsy :)\n. i don't think anyone is parsing the console output so just changing it should be fine.\nwe have csv and json that is more readily parseable, and shouldn't be affected by this change.\n. If i've asked a benchmark to report in ms, it should be reported in ms no matter which reporter I use.\nas such, this can be moved to BenchmarkReporter and called from each of csv, json, and console reporters.\n. consistent naming style please: GetTimeUnitAndMultiplier.\nFurther, it doesn't need to be a member of any class as it doesn't use state from the class. It doesn't even need to be in include/benchmark as it'll only be called from implementations of reporters.\n. right, but they should now scale appropriately given the user has requested a particular time scale.\nthey should maybe have a field for the time unit added too...\n. you shouldn't need this. the only extra thing here is the time unit which is already in include/benchmark.\n. ah right. it's a shame to break the abstraction here and have tests that seem to rely on internals. could you add a comment to the CMakeLists.txt explaining why it's necessary?\n. whatever you want to do is fine. sleeping is ok, as long as we're clear in the cmake config why we're relying on src. i'd hate for users to read that as an example and think they need internals for some reason.\n. this isn't the approach i would take.\ncan the generated HTML reference the CDN from which these can be sourced? I understand there are benefits to having them locally, but this will do odd things to the installed package, which may be unexpected.\n. capital initial letters please.\n. don't mix underscores and case changes please.\nhttps://google.github.io/styleguide/cppguide.html\n. capital initial please\n. consistency with the rest of the code means keeping the & with the type, not the name.\n. why return input and replace inline? either take input as const & and take a copy, or just do the inline replacement. however, if you do the inline replacement, input should be a *.\nhttps://google.github.io/styleguide/cppguide.html#Reference_Arguments\n. maybe these should be in the HTML too... \n. do these structs need to be public?\n. any reason why file isn't a std::string?\n. this isn't really a string util, it should be local to the benchmark code.\nit could even be an instance method, i think...\n. can you find somewhere to declare this that isn't in the public api header?\n. consider using an unnamed inner namespace here to avoid these methods/structs being used by other translation units.\n. update these methods to have initial capital letters please.\n. can you see if these can be made private? i think they can as they're not used externally to the class.\n. method still not named to style requirements.\n. method still not named to style requirements.\n. method still not named to style requirements.\n. method still not named to style requirements.\n. method still not named to style requirements.\n. method still not named to style requirements.\n. indent 2 please\n. sorry, this doesn't fit here either. a new header would be reasonable in this case, i think. a private benchmark_util.h maybe?\n. nice catch. a shame we can't support more than 2 billion threads though :P\n. do you think it's worth separating out manual accumulated time from real accumulated time? both could be set on a benchmark, and we already have both CPU and real accumulated time.\n. see https://github.com/google/benchmark/pull/199\nthe benchmark test there uses std::this_thread::sleep_for which doesn't require pulling from src.\n. I like to minimise surprises, and i think that if someone calls ->UseRealTime()->UseManualTime() on a benchmark it should do the right thing (show real time and manual time).\nyes, it might be cluttered in console mode but it's already fairly heavy-weight and the user has requested the extra data.\n. this would be an error now.\n. SetLabel might be a better method to use to report this. or just skip it .. it doesn't add anything to the example that i can see.\n. ah, i see. it's fine :)\n. update this comment, and i think we're good to go.\n. i have no idea.. i haven't built with MSVC in a looong time.\nmaybe @anton-dirac knows\n. it might make sense to add this check to AddRange instead where we already check the limits.\n. as per google c-style, please don't use non-const reference arguments. https://google.github.io/styleguide/cppguide.html\n. there's a possible bug here: you'll set this to true, but then it can be filtered out by the regex match below (l444). you only want to set it if the args is the last one and if it's going to be added to the benchmarks set.\n. why %s for a %? just use %% in the format string.\n. please use the same style guide in regards to naming and indent as the rest of the code.\n. switch?\n. src/check.h has some useful macros for logging these errors.\n. are you using arg1 and arg2 anywhere in the reporters? i can't see them at first glance.\n. the code /might/ be simpler if the definition of the BigO enum, this method, and FittingCurve, are moved into their own files. That keeps all of the complexity methods together and allows this file to focus on things specific to the reporter, which this isn't.\n. consider different colors for the complexity report. they get lost in the output otherwise.\nalso, i don't think there's any need to output for both time and cpu, but you may think different.\nyou can suppress the iteration count too.\n. consider suppressing 'iterations' for this (and time unit).\n. same as elsewhere: iterations are unimporntant, and time unit should be suppressed.\n. ah, so you're assuming that arg1 and arg2 are used as loop indices, and only used in a single loop.\nwe have a couple of tests in basic_test that use the range loop twice (so it's already 2N) and examples in benchmark_test (BM_SetInsert) that use range_x to construct test data, then range_y to do the work. anything there based on assumptions of N and M will fail.\nit might be worth making this more like the state methods SetItemsProcessed and SetBytesProcessed. SetComplexityN and SetComplexityM? \n. is this check necessary any more?\n. please add a comment that this shouldn't be copied by users of the library or something :)\n. stick to google-style naming please. GetAbortHandler, etc.\n. !data.good?\n. so just one error per benchmark. if every iteration errors, we'll only report the.. first?\n. 'on iteration x' might be nice, if it's not too hard.\n. nit: extra indent here\n. you don't need the 'else' here\n. guard this with a __cplusplus >= 201103L check please.\n. that reminds me .. RegisterBenchmarkInternal should take a std::unique_ptr.\n. google style: no non-const references. please take a pointer directly (and CHECK for nullptr. sorry).\n. rename out to err?\n. not sure this is useful given you have the above...\n. what if Out is std::cerr?\n. doesn't seem to, just arguments.\nftr, i don't like this rule, but there it is.\n. nit: initialize doubles with 0.0 please\n. you need complexity.h, right?\n. not for you, by the way. note for me.\n. please do. i don't want refactorings to benchmark_api to lead to missing symbols. everything should include what it uses.\n. thinking about it more, these shouldn't be in this header. we want to keep the public api (under include/benchmark) as clean as possible and users would never need these.\nas such, can you move this into src/ and add the enum definition into benchmark_api.h where the Complexity method is defined?\n. needs benchmark_api.h for BigO\n. it's been complaining all along about test coverage reduction.\n. why are these in the public API?\n. it should be in include/benchmark/reporter.h then i think.\n. in this case, it's probably worth just keeping the #if _MSC_VER <= 1800 here (with a comment explaining it). the extra macro doesn't add anything useful.\n. it should do.\n#if _MSC_VER <= 1800 alone won't as _MSC_VER will be 0 if undefined. \n. one space is correct for google style.\n. std::function<double(size_t)> doesn't compile on the target platforms?\nalso, there's no need for n to be size_t. it isn't the length of a container, so int is preferable.\n. please run this through clang-format with google style to ensure these indents are correct and not changing for the sake of it.\n. can you switch this to being the positive case instead? it's easier to read.\n. no global non-PODs please, even in tests. These should be lazily instantiated pointers.\n. no static non-PODs please.\nconst char[] instead for this.\n. as above\n. yes. clang-format with google style setting. whatever that says is what we go with :)\n. then two comments:\n1. never mind, we'll fix it afterwards.\n2. can we abstract that so we don't copy-paste?\n. this should be int. google style (https://google.github.io/styleguide/cppguide.html) prefers int.\n. there's an extra space here, and extra whitespace below.\n. that sounds great.\n. why this and not Ranges({{1,2},{3,7},{5,15}}) ? ie, a vector of pairs instead of a pair of vectors. \ni find this more intuitive, and it's slightly easier to process too.\n. why not just invoke and pipe to tee then? i'm not sure that this library should be writing files at all, given that every OS has a way of redirecting output to a file.\n. we should mark these as deprecated as they're largely useless with your change. at some point  we should remove them.\n. args_.empty() is a more accurate check.\n. you should probably test that the product is what you expect.. or even better, that each range(x) is what you expect.\n. how often do you think someone would want to push the output to tooling and have it human readable? I'd expect any tooling requirement to override humans reading the output. ie, i think if you're going to push the output to a tool that tool should take care of the human readable bit too. whether that's a summary, or reformatting the raw results.\nie, i'd expect $ benchmark_my_stuff --benchmark_format=json | analyse_my_bench\nto provide me with human readable analysis results. if i need the output for debugging, then tee or some equivalent would at least give me something to look at after the fact.\n$ benchmark_my_stuff --benchmark_output=out.json --benchmark_output_format=json --benchmark_format=csv\nit's very easy to get wrong. i mean, output is also to stdout, so output_format vs format is subtle.\nthis may just be bikeshedding the arguments, i guess. i'm wary of redundancy and confusion.\n. we do not. a comment will do for now, or you could just remove them. anyone building this from head should be open to breaking changes, and we'll make sure to add this to any release notes for future releases.\n. what's the difference between this and the one above?\n. @EricWF is this an issue for the public API and your use in libc++?\n. sorry, by \"above\" i meant within this file.\nlines 9-10 and 11-12 are identical. right? can we remove one stanza?\n. don't use default arguments please. two separate constructors would be best.\nand don't forget to mark the one-argument constructor as explicit!\n. if we could avoid std::string for the benchmark_api.h, that'd be great\n. in #259, @EricWF adds a BENCHMARK_HAS_CXX11 macro which you should consider using.\n. please don't add these spaces for alignment. go with the style throughout the rest of the code.\n. i'm not sure having this private is necessary. you already have methods above for adding/setting/getting. why not just have this public?\n. consider reducing this API surface to the essential. If we want users to define a Counter type, then don't supply overrides to do it for them. it just adds complexity that's unnecessary.\n. i need more details why these are necessary, sorry. if the above API can add counters as well as set them, why would you need to set anything up in the fixture?\n. please add this in its own counter.cc implementation file\n. std::map might be better. You can avoid the 'find' cost on every API access.\n. why aren't counters part of ThreadStats?\n. pimpl would be fine, i think. @EricWF can confirm, but anything in the _api header should avoid as much of the standard library as possible (as it's used to benchmark libc++).\n. True, the number of counters will be small. If we think that 'id' is the best way then that's what we should expose to users. ie, they can only look up counters by id and they have to store them. what do you think?\n. ok, i'll buy that. bikeshedding then:\n--benchmark_out: the file to which to write results\n--benchmark_out_format: the format in which to write the results to --benchmark_out, if set\n--benchmark_format: the format in which to write the results to stdout\n?\n. why overload instead of using default arguments?\n. one rather nice trick for avoiding boolean parameters (which can be confusing at the callsite) is to define an enum with two values.\n. why is this part of this PR?\n. CreateCounter?\n. GetCounter is going to incur overhead which, if it's in the context of a benchmark keeprunning loop is not a great idea.\nthe examples here should be best in class, ie:\nwhile (state.KeepRunning()) {\n  numLighterEvents++;\n  bigNumEvents++;\n}\nstate.GetCounter(\"LighterCounter\") += numLighterEvents;\nstate.GetCounter(handle) += bigNumEvents;\n. s/Flags_e/Flags/\n. given these always return a handle, i see no reason to allow counter lookup by name. it's just going to cause developers pain in the future when their benchmarks take longer than they expected.\nPlease simplify this such that SetCounter (actually, CreateCounter) takes a string and everything else takes a handle.\n. i'm not a fan of the two-stage initialization and shutdown just to support counters in fixtures, no. I suggest that for now we don't support fixtures and get the basic counters in. then we can figure out how to support fixtures better.\nit may be ok to pass a mutable State, but i need to think about it.\n. lots of unnecessary extra whitespace. please remove it.\n. it doesn't look like this was formatted with clang-format. due you mind doing so with Google style?\n. lines too long. please run clang-format on the file with Google style.\n. the default operator isn't good enough? if you switch to string it should be, right?\n. when true :)\n. this is very short and neat, but it requires a lot of work above. can you just be more explicit here about the state changes and lose all the operator overload cleverness please?\n. s/sue/use/\n. this wasn't needed before, why is it needed now?\n. given we know this can be an issue, and it's hard for users to know which to use (or they use the simplest, which is expensive) can we do anything to ensure they do the right thing? ie, should Insert be the only API and should we restrict it to only being called outside KeepRunning?\n. if you keep this, make sure it's explicit to avoid accidental conversion from string to Counter.\n. if @EricWF says it's ok to use <vector> and <string>, we should.\n. are you going to do this in this PR?\nif not, please watch the line length: move the todo comment to the line before.\n. i think all the supported compilers allow delegating constructors now so feel free to use them.\n. a static non-pod defined in a header? say it ain't so!\nthis should probably be\nextern const char* const dec_re;\nand then in the .cc\nconst char* const dec_re = \"...\";\n. should we also mark these as deprecated?\n. change total to family_size in the comment\n. we do a lot of work now just to list the benchmarks.. why?\n. maybe comment it out with this as documentation? unless you think there's never a time we'll be able to put it back.\n. just\nenum Flags {\n ...\n};\n. no need to restate public\n. watch your line-length please. you can always use clang-format with the Google style.\n. these formatting changes introduce noise into the PR. can you revert them?\n. it's almost worth making BenchmarkCounters a struct containing the map and these methods as members.\n. nit: extra blank line.\n. const_cast, probably.\n. gah, completely read it wrong. i thought it was the other way around (with the old ones forwarding to the new ones)\n. ping: are you going to do this TODO in this PR?\n. is this an existing bug or are you incrementing thread_id for some reason related to the PR?\nwith the explicit RunInThread 0 below, should this loop start with ti = 1?\n. nit: BENCHMARK_OS_WINDOWS\n. can you do a format/style pass at some point? GetBenchmarkMutex, etc.\n. i think that's all it is.. clang-format Google-style should take care of the rest (there's some extra indents around the place).\n. done :)\n. you popped my pimpl!\ni'm actually fine with this. originally this was here to avoid having too much detail in the public header.\n. should this say 'unresolved'?\n. nit: you don't need spaces between < > any more.\n. nit: remove this newline\n. was this a whitespace change? can you revert it (unless it comes from clang-format)?\n. nit: weird indent here\n. fixed this upstream. rebase :)\n. what happens if i call Args({1, 3})->ArgName(\"first\") or if I call ArgName directly on the benchmark without Args being set?\nWould this be a cleaner api if Args took a map<string, int>\n. should this be called after Args? If we can add a constraint on that, and then check that names.size() == args.size(), i think it would be much clearer to users when they do something unexpected. what do you think?\n. wonderful! can you move it down with the other privates please?\n. I wish they could, but we don't run googletest tests (which cc_test expects, as i understand it).. I don't really understand why this was made into a separate function in the first place as i think it was much clearer inline (albeit in a macro).\ni have no massive concerns with the current approach, though i think the \"returning a boolean\" is cleaner... but then again what if someone doesn't want to printf but wants to do something else with the extra args?\nchuck it in. we'll fix it in post.. is it correct that an iphone should have both MACOSX and IOS defined?. i think this is fine for now, but if there are any other ios oddities we should consider using this hierarchy as a reference.\nthanks for the link!. use c++-style casts please.. don't forget to update the WINDOWS build too. color_output_ is referenced in there.. include/benchmark ?. is this part of a broader set of checks that should be guarded by  #if __cplusplus >= 201103L or is it truly Android specific?. 15?. for future TODO perhaps:\nreport.counters = results.counters.Finish(seconds, b.threads);. CHECK_FLOAT_EQ or CHECK_DOUBLE_EQ are more aligned with googletest.\nalternatively, we might want to start using googletest ;) another TODO!. for consistency, why not put the \" \" before the last %s and not have the space in the UserCounters string below?. an enum might be a better fit for the which argument type.. should the 1.e-5 here be eps_factor?. nit: bring this brace up a line please.\n(use clang-format with google style to catch these for you). SGTM. c.first.c_str() i think. unused function, apparently? in travis (clang release). this dropped 'else' isn't google style. can you run this change through clang-format with Google style selected?. can you replace this method with:\nVType stddev;\nVType mean = Mean(&stddev);\nreturn stddev;\nand remove the extra checks and calculations?. why so much whitespace?. it's fine. i'll clean it up later :). any reason to set this outside the Windows platform check?. should this version be the latest version tagged in the repo?. this should probably be versioned, right?. ${GIT_VERSION} should already have this for cmake.. but that would require you to generate the nuspec from cmake instead of checking it in. i don't know how that works.. remove this log please.. i would prefer separate methods to make it clear what the string argument is:\nBenchmark* Arg(int x);\nBenchmark* NamedArg(const std::string& name, int x);. is there any downside to using this method? perhaps we can be more firm on using this as the default?. boolean parameters can cause callsite readability issues. it's usually better to define an enum instead:\n```\nenum class AtomicBatch { kNo, kYes };\nint GetBatch(int size, AtomicBatch atomic);\n...\nGetBatch(n, AtomicBatch::kYes);\n```. consider CHECK_NE(batch_size, 0) from src/check.h.\nnote, you should probably move the implementation of this method to a .cc anyway.. you should be able to use CHECK in tests. The library supports C++03, and those users will have to use the KeepRunning api. Other users should strongly prefer the range loop, given the reduced overhead and simplicity of the call (even with the slightly unusual opaque variable).\nI don't know why a user might want to call State::iterations inside the loop, but we can check that case and log a warning. we do this for other calls.. you might need src/check.h or ../src/check.h.\nSee https://github.com/google/benchmark/blob/4cfe790a253d2cb47f33b29676c0a27c462704cb/test/output_test_helper.cc#L7. do you need to only run these tests for C++11 runs?. i'm still a little uncomfortable with this as a reporter. given we output in JSON (and can add more fields there if necessary) an HTML view could be a server that runs as a standalone binary pointed at a JSON result. That could then be generalised into a service to which people might upload their results and see them visualised over time.. examples like this are why i think having this be a separate service is the right approach. any changes to how the data should be rendered now require updates to the benchmark library. that's the wrong abstraction layer.. just - 1 on the next line. this variable doesn't add anything to the code.. We've had attempts in the past to clean up this run_iterations thing as it does seem like it isn't necessary. However, and I wish i could remember or dig up the reasoning, it ends up coming in to play for some combinations of runs/threads/repetitions.\nHowever, if your change shows expected results when we run the tests we have, i'm comfortable with the notion of the change.. auto return type should probably be \"double\". And a const return type doesn't make a whole lot of sense given how it can be copied into a mutable value at the callsite.. i wonder if we should (and this is total idle thought) make the standard stats methods wrappers public so a user can just use:\nBENCHMARK(BM_foo)\n  ->ReportMean()\n  ->ReportMedian()\n  ->ReportPercentile(90)\n  ->Repetitions(100);. no _ on the args please. any reason these are public?. i think this can be a fwd decl here with the implementation in the benchmark.cc file. this helps discourage library users from trying to instantiate one of these, which given the API below they shouldn't need to.. i think this belongs in the benchmark.cc file.. i'm not sure how this method ended up here, but i'm wondering if it belongs in here, or in a new statistics.cc implementation file.. why are these lambdas? it seems they could just as easily be concrete functions declared here (or in a src/statistics.h) and defined in an implementation file.. nit: maybe a warning?\n\"If you see build failures due to cross compilation, try setting HAVE_${VAR} to 0\" ?. everything else here takes a copy rather than a reference (pointer). is it possible the lifetime of family is shorter than that of instance? should you take a copy (even though it'll be a memory/cpu hit)?. further down you return explicit 0.0 instead. i think i prefer that for readability, but either way be consistent please.. you could simplify this to\nif(v.size() % 2 == 1)\n  return partial.back();\nreturn partial....;. this should be size_t as that's the type as stored.. let's try to avoid shared_ptr.\nas you've pointed out, it should be fine and i'd rather avoid the copy if we can as you've done.. https://github.com/google/benchmark/blob/e8fc2a2b8ccd33fefdd7f95e1a2aabee0788c4d6/src/benchmark.cc#L235\ni missed this cast.. you probably don't need the comment now :). should all the tests (but this one) switch to the range loop to show it's the default?. I think so, yes.\nActually this clamping doesn't work because (-0 == 0) == true, which suggests i can do something like:\nc++\nauto clamp = [](double v) {\n  if (std::fabs(v) < std::numeric_limits<double>::epsilon()) return 0.0;\n  return v;\n};\ninstead.. ha!\nyes, that's fine.. Can you rename the bm to \"fast\" too while you're here please?. should this be a separate PR to keep the functional changes separate from the refactor?. s/INSTALL_BENCHMARK/BENCHMARK_ENABLE_INSTALL/\nfor consistency and so it's clear that it's not doing the actual installation.. this changes things a bit, as it means the data will be the same for every iteration. this may drop variance within a run but increase it across runs.. oh. ConstructRandomSet is a lie. never mind then.. maybe name this 'baseline_family' or something?. i think this is where we're ending up with /8 etc. i wonder if it would look better as BM_copy|BM_memcpy/8 or something similar. this would also help if the report is output somewhere later.. it would be good to document what the '+xxx' means. ie, that it's (new-old)/old (roughly). return in.good() && ReadFromFileImp(in, args...) ?. i assume this is the aliasing issue below in the union? but it's worth a comment for disabling this warning.. are you assuming something about bit widths of long long here? should it be a ssize_t instead?. size_t?. does this need to be defined for OSX? Maybe it should be\n```c++\nif defined BENCHMARK_OS_MACOSX\n... GetCacheSizesMacOSX() {\n...\n}\nelse\n... GetCacheSizesFromKVFS() {\n...\n}\nendif\n.c++\nstatic const CPUInfo info = new CPUInfo();\nreturn info;\n```\nthis is a more google-ish pattern for static initialization of non-POD things, and is simpler to reason about with a well-defined CPUInfo constructor. . https://google.github.io/styleguide/cppguide.html#Integer_Types. remove this ifdef so we compile the function even if it isn't called.. i didn't spot these naming issues before:\nas_string or AsString (or GetAsString).\nsame below with integer accessors.. why unsigned? this shouldn't be representing a bit pattern, and i don't think you need defined overflow modulo 2^64.. we do. that PR might be wrong. or this might be a special case.. why not git submodules?. this should be the default, no? ie, why do we have it required to have it in-tree instead of just letting cmake do the right thing?. make sure it's clear this bit is for ubuntu/debian.. why do you need the second cmake?. replace all this with\nmake install. i think you can remove the test instructions as that's covered by the rest of the docs already.. note, this won't work for everyone. this is one of the reasons i held off; no way to do this kind of dynamic configuration.. this should not be necessary. We don't want the tests to have blanket access to the private parts of src.\nfor the same reason, you should be using visibility = [\"//visibility:private\"] on most of the cc_library targets above.. the travis and appveyor builds are probably the closest we have for covering the various platforms/compilers. there are other platforms that people run on, but we don't have a way to cover every combination. . ah, you're right. i wasn't sure how they'd set the default.\nthey include some things from ../src, but don't have full access. i don't know enough about bazel, but would it be better to add a BUILD in src, a BUILD in test, and manage the visibility of certain src/ libraries to test/ that way?. these should also depend on :benchmark, right?. i don't think any of these need gtest. doesn't use gtest, afaik.. i don't know enough bazel: is this defining the googletest repo dependency for use in the BUILD? or is it supposed to define the benchmark package and this is a bad copy-paste?. we generally don't do _ in type names: https://google.github.io/styleguide/cppguide.html#Type_Names. I'm a little surprised this is public. does it need to be?. i think this is the first time we've had an RTTI requirement in the library.\nhttps://google.github.io/styleguide/cppguide.html#Run-Time_Type_Information__RTTI_\nideally we'd resolve this through virtuals (though i understand this is hard given you don't want to define TryCreateFixture on the base class.\nis it completely onerous to have a separate list of fixture benchmarks on which this can be called while also having them added to the full set of benchmarks?. if you need a p_ to indicate it's a pointer, maybe use auto* instead. ie, prefer types to naming to indicate, well, type.. if (functions_to_apply_.empty()) {\n  return;\n}\n?. alternatively, perhaps ApplyFunctions can also create the fixture if necessary. ie, use a virtual ApplyFunctions method (maybe renamed to SetUp ;)) that can do more in the case of a fixture benchmark.. auto* benchmark = .... this documentation doesn't add anything.. also a pointless comment.. i buy your 'int' argument and if we need a guarantee, which we do, then this should be ?int32_t. The Google C++ style guide, interestingly, states that we can assume an int is at least 32 bits, which is a bit surprising.\nHowever, in the same section it also specifically recommends avoiding unsigned unless we need overflow modulo 2^N or representing bit patterns. Given we're doing neither, this should be signed.\nSo: int32_t please. And then promoting once to int64_t in 'iterations' seems reasonable, though i'd promote the individual parts of the calculation. ie, max_iterations - total_iterations_ could be INT_MAX already if total is 0. the +1 will cause an overflow before you get the chance to promote to int64.. why is this now under HAS_CXX11? I don't think it was before.. this should be in benchmark.cc, right?. i'd be quite happy if these could be move out to their own header/source files (in src/).. line length should be <= 80. line length. maybe you can use googletest for this instead of asserts.. huh. yeah, probably move benchmark member functions to benchmark.cc and the stuff there to benchmark_main.cc or something.. can this be put inside one of the existing internal namespaces instead of adding another block? at some point i tried to clean these blocks up.. it'd be nice to not make it messier.. is this file necessary any more?\ngtest is a new addition so we're just starting to move testing over to it. you're ahead of the curve :). i'm not sure we need more than 2^32 threads, do we?. repetitions certainly shouldn't head north of 2^32.. also here: if someone is passing this many args around we're in trouble.. this was int64_t before and probably still should be.. same here.. keep as int64_t. if this is for outputting the args (which i don't think we do other than as part of the name) then it should be int64_t value, right?. int64_t. definitely doesn't need to be updated :). this should likely be an int. int64_t if we expect very large batches.. BENCHMARK_ALWAYS_INLINE ?. still should be an int or int64_t. So should the iterations tracking really. I'd say leave this, but i'd like the public API not to change very often, even in the signedness of a parameter.. how do we get here? on line 642 if total_iterations_ >= n, we return. if we're here then total_iterations_ must be < n.\ni think you can avoid this check and improve the code generation.. I think you can replace this with something like:\n```c++\nbool State::KeepRunningInternal(int batch) {\n  if (BENCHMARK_BUILTIN_EXPECT(total_iterations_ >= n, true)) {\n    total_iterations -= n;\n    return true;\n  }\n  if (!started_) {\n    StartKeepRunning();\n    if (!error_occurred_ && total_iterations_ >= n) {\n      total_iterations_ -= n;\n      return true;\n    }\n  }\n  if (total_iterations_ > 0) {\n    batch_leftover_ = n - total_iterations_;\n    total_iterations_ = 0;\n    return true;\n  }\n  FinishKeepRunning();\n  return false;\n}\nbool State::KeepRunning() { return KeepRunningInternal(1); }\nbool State::KeepRunningBatch(int n) { return KeepRunningInternal(n); }\n```. how about not doing this yet, but adding a \"TODO: make these bitfields if required\".\nreasoning: on some platforms (at least this used to be true), bitfield accesses required quite a bit more ASM than bool accesses. Ie, the bitfield work might negate the work to get these in the cache line in the first place.. can you define a BENCHMARK_OS_FUSCHIA alongside the BENCHMARK_OS_XXX macros elsewhere? that'll help be consistent and track any other per-OS changes.. 'whose'. this should be ### i think, as you're only at ## above.. this should be StrPrintF, right?. as per bazel, it may be worth changing this to http_archive: https://docs.bazel.build/versions/master/be/workspace.html. this change looks odd. clang-format misbehaving?. do you still need this?. just base_name is fine.. still necessary?. this was fixed in another commit so you shouldn't need it any more (complexity_n is 64-bit).. can you put this back how it was? the || being lined up helps readability a lot.. you shouldn't need this if you're not printing the base name in the console reporter. it's an abstraction leakage but the field width shouldn't be used by other reporters aiui.. aah, i see. adding the option for the base name. i don't know if it's necessary to do this in this PR, is it?. We should either make it fully functional with an option, or remove it. We can always create an issue as a \"good first issue\" to add this support and option as a followup. \nthat'll make it easier to understand what was added in this PR.. is this used?. given you have to take a copy, it might be better to just pass spec in by value instead of const reference. . should this be a specific commit so we don't end up with version mismatch issues as googletest releases?. I'm more worried about tests starting to fail because of some change in the head version of googletest that is later reverted. It's unlikely, but i'd rather depend on a specific version.\nsomething like https://github.com/google/googletest/archive/3f0cf6b62ad1eb50d8736538363d3580dd640c3e.zip in the URL should work.. can you define a BENCHMARK_OS_ANDROID where we define the other OS constants (src/internal_macros.h)?\ni'm not sure we currently differentiate on libcxx but maybe we should add something there too.. why inline and why in the header?. can you add tests for these please? \nyou'll have to create a test/string_util_gtest.cc file, and add it to test/CMakeLists.txt. bazel will pick it up automatically.. s/OKBLUE/OKGREEN/. this should be under the docstring. also, if you care about the type being boolean specifically, maybe add type annotations :)\notherwise, it's up to the caller to be truthy or not. this is Python, after all.. nit: UTest. maybe the option to slightly improve codegen by assuming branch prediction?. i'm not sure why this entire file shows as modified, but you might want to look into that.. could you send a separate PR with the formatting changes? it'll help keep this focused on the actual functional change (and i'm happy to land pure format changes quickly).. watch the naming (Base10DigitCount). Also, given this isn't on the critical path, just using std::log10(n) + 1 should be good for all cases instead of the unrolled version.. please split this out to a separate PR. I'm not sure we will need it, and i'd like to keep this PR focused on the important bits.. there's no need to introduce this FLAG(..) macro. It doesn't add anything useful.. Why is the base name and overall benchmark name not enough for this?. split the CSV changes out to another PR. I don't know that we need it, and it's helpful to keep this PR more focused.. we should clang-format off this. when you get around to the format PR please make sure this isn't changed.. why are these defaulted? they're always passed in, and the default is defined in the calling binary.. can you at least add a kLog2E constant?. weird indent here.. is this correct?. a comment on output: should it have the bm name in the 'u-test' bit so people running the output through grep/ag/whatever will catch the u-test for the bm they want?. or just BM_Two_p_value ?. as long as it does the right thing :)\n. ah, you expect we might have more than one test. that's fair. BM_Two_utest_pvalue then?. can you maybe add some realistic examples of these? i'm having a hard time thinking what an iteration invariant counter would be measuring that's useful.. This should only be necessary if a package is not available. Ie, it might be worth having a section on getting the library first, with installation from source being one of the options. . cmake or bazel (bazel is preferred)\nmake or ninja or visual studio for windows\ng++ or clang++ can be replaced by \"A C++ compiler with C++11 support\". I think it's better to avoid this specificity and just have the users install the packages however they know how. there are too many platforms and variants to go into detail.. Add a doc on building and (optionally) installing and link to it, then remove the same instructions from the base README file. . this should not be necessary unless the user overrides the install location, in which case they know what they're doing.  :). maybe leave out the specifics of naming if you can. there are many standards around and this is irrelevant for library usage.. again, we work against many platforms. I think it's better to have more generic instructions about building your executable and linking against the library, then have a section below on requirements like pthread and benchmark. Users on different platforms will have different ways of linking against those libraries.. oops, thanks. and json of course lets this past :). I imagine we might have a native implementation of these things, but we might have someone with a custom allocator, or an integration with tcmalloc. I wanted to get the interface in so we can do something, and then worry about a native implementation (and all the cross-platform shenanigans that entails).. With the way this works, I'm not sure when they'd do that. I agree it would be useful, but it would have to happen between the registration (which should be before the call to RunSpecifiedBenchmarks) and the benchmark being run.\nHaving a const accessor might be useful if a benchmark instance wants to mutate the memory manager in some way.\nIf the memory manager object was part of the benchmark instance state, then it could be set on a per benchmark basis, which might be more flexible. WDYT?\nActually, thinking about it, I wonder if users could just use UserCounters already for this. If at the start of the instance they start their allocator measuring, and then stop it after the KeepRunning loop and add the allocation counters to UserCounters. The only issue with that might be the profiling overhead, if there is any. . Done. Done. Done. Done. I think you can just try adding the flag. AddCXXCompilerFlag.cmake will check if the flag exists before adding it.\nOr, as per the c++11:\nadd_cxx_compiler_flag(-Wno-deprecated-declarations)\nif (NOT HAVE_CXX_FLAG_WNO_DEPRECATED_DECLARATIONS)\n  add_cxx_compiler_flag(-wd1786)\nendif()\nmaybe. it depends if the intel compiler supports the flag but ignores it, or returns an error if you try to set it.. oh. well. let's just go with the obvious and explicit then :)\nI like your suggestion here to add the GCC flag then also add the Intel flag conditionally. It simplifies the logic a bit.. drop this line about compare_bench?. Right now line 134 is a long line. Please wrap at the same width as the rest to make it more readable.. Not everyone reads it as HTML.. This is very unfortunate. Is there no way to generate this file from cmake passing in the version as a parameter?. solaris? you'll need kstat.. @LebedevRI that's a good point. maybe we can do something like have the default be empty with a check on package creation that it's been set? and then a test that we should run on releases to ensure it's set?. given the rest of this header (above) works on clang and GNUC directly, perhaps you don't need to define the COMPILER_XXX macros?\nIf you do want to then maybe define them higher up (line 188?) and use them throughout.\nBut i think it's fine to remove lines 244 to 256 and not define new macros just for this check.. is this only ever going to be a problem for executable name or should we do this for all FormatKV where value is string?. why is this specified as a pair instead of just a single number (1000 vs 1024)? or even an enum!. Ah, I think that is suggesting either having an enum with values 1000 and 1024, or to extend Counter to have MegaCounter and MebiCounter variants, which implicitly manage the one_k definition without it being specified.. It's just.. weird. Like, why would you define one_k as 2000/2 or 500/PI?. I think it's an orthogonal feature so i'd add a OneK enum with two values.. why not give the enums the values '1000' and '1024' and pass them through (with any necessary cast) to the HumanReadableNumber call?. nit: https://google.github.io/styleguide/cppguide.html#Enumerator_Names\nOneK is a better name.. does the PR get even simpler if these are renamed to:\nARM_FileReportAggregatesOnly,\nARM_DisplayReportAggregatesOnly,\nand then the existing flag sets them both to true, and the new flag sets only the Display one to true?. is it worth returning a custom struct to avoid having the comments tracking which member of the pair is which?. would this also be simpler if the return type was 'non-aggregate', 'aggregate' instead of having the first be everything?. c++\nif (repeats != 1) {\n  run_results.display_report_aggregates_only = (\n      FLAGS_benchmark_report_aggregates_only || FLAGS_benchmark_display_aggregates_only);\n  run_results.file_report_aggregates_only = FLAGS_benchmark_report_aggregates_only;\n  if (b.aggregation_report_mode != internal::ARM_Unspecified) {\n    run_results.display_report_aggregates_only = (\n      b.aggregation_report_mode & internal::ARM_DisplayReportAggregatesOnly);\n    run_results.file_report_aggregates_only = (\n      b.aggregation_report_mode & internal::ARM_FileReportAggregatesOnly);\n  }\n}\na bit clearer than nested ternary operators, i think.... why do you need to return this? you should be able to (as it was) have a local constant and decide to add to the vector or not based on the value.. Ah right, sorry.. const ref is probably the right move for name.. We could return a copy of the state from the stack. Or we could have a State* out param that the caller can populate with a stack version, but that moves the instantiation to elsewhere and breaks some of the encapsulation of the Run method.\n. wrap at 80 please. there's extra parentheses here that i don't think are necessary.. Is it worth putting some of this logic in the OutputTestHelper or a new helper that can be shared between the two tests?. gah, vexing parse.\nnit: s/JsonOutput/json_output/. why isn't this a static method that returns the string?. personally, i'd declare it here and implement it in a source file. but that's not essential.. if this is always true, do we need to store it? or can we calculate it in a method?\nc++\nstd::string Run::benchmark_name() const {\n  std::string name = run_name;\n  if (!aggregate_name.empty()) {\n    name += \"_\" + aggregate_name;\n  }\n  return name;\n}. this should now be run_name right? at least, you assign it to run_name below.. nit: please put the space before 'actual' on the previous line.. i don't see one, so maybe add a test just for this?. python\nreturn [x for x in list1 if value in list2]\ni think will do the same. \n```\n\n\n\nl = [3,1,2,4]\nm = [3,2,4,5]\n[x for x in l if x in m]\n[3, 2, 4]\n[x for x in m if x in l]\n[3, 2, 4]\n. maybe call this 'intersect' if that's what it's doing..python\nreturn BC_FAIL if pval >= utest_alpha else BC_OKGREEN\n``. Well that merge was brutal. It now doesn't have the allocation, but it copies. Given this happens once per instance run (ie, not per iteration) and isn't measured as part of the benchmark, i think this is fine. And simpler.. inverse booleans confuse me as i'm quite slow. can this bebool first_repetitioninstead (and obviously invert the logic everywhere)?. I'm torn between this, where you have a simple accessor but the constructor is complex and actually doing the work, and an alternative where the constructor merely initialises the state, and a methodRunthat runs the benchmark and returnsRunResults.. nit: can you put the public first and have an explicit private: section please?. why here and not benchmark_runner.h?. google-style:get_results()`\n\n\n\nalso, mark the method as const?\n. DoNIterations. PredictNumItersNeeded. ShouldReportIterationResults. DoOneRepetition. nice catch :) that's much better.. i wonder if this is useful given the user can't really control the number of iterations (at least the max). Ie, this could output thousands++ of reports if the benchmark suddenly speeds up.. nit, {} around the else too please.. // The benchmark may have requested manual or real time be used in decision making. Respect that.. nit: NewResult. s/You/Implementations/. i know this is almost certainly safe but... ew.\ni think i'd prefer something a bit more explicit. like the struct being a handler with the overridden method, and a field that stores the results with an accessor.. this is a sign of fragility. is there an alternative design that doesn't have this limitation?. Even I'm getting confused between all these options.\nCan you document/explain how these all interact in a simple way?. --benchmark_report_aggregates_only --benchmark_report_separate_iterations does the same as --benchmark_report_aggregates_only ?. maybe just --benchmark_report_every_iteration or --benchmark_average_over_iterations (with the default being true) or --benchmark_one_iteration_per_run or maybe just --benchmark_max_iterations=1 ?. ugh. really?\ni think the issue i have is that i don't really understand the problem that you're trying to solve (still). which is completely my fault for not being able to focus on this with you.\nI think you're trying to get to the point where every iteration is used when we have repetitions to avoid having statistics at the repetition level that apply to averages across iterations. Is that correct?\nIt also seems like you're trying to do this in a way that is optional. Is that correct? . I don't like either of the options here. A comment warning about virtuals from constructors, and 'roll-your-own' virtual by downcasting this are both problematic.\nI feel like we should be able to get somewhere simpler.\nAbstracting BenchmarkRunner to do the iteration management and running the benchmark SGTM. Now when it comes to storing and reporting, either a simple conditional on iteration mode or (if you must) a virtual method with concrete implementations for the two modes should be doable without calling that method from the constructor. Even if that means abstracting out the construction from the running of the benchmark. What am I missing?. nit: for consistency with CPUInfo, maybe SystemInfo.. return empty string and don't print in that case?. Yes please.. 2. No. We should check and only output if it's available.. threads should be initialized to 1, i imagine, in the constructor. wdyt?. should this include some sort of repetition index? so we can output\n\"repetition_index\": 1,\n  \"repetitions\": 3,\nin the json?. repetitions is the right name but it shouldn't be reported at the per-run level. i think a repetition index might be the correct way to go without repetitions as you can, in tooling, determine what the max repetition index is for a given run and group them together easily.. does this need to be part of the public API?. it has state, a private section, a user-defined constructor: it's a class, not a struct.. this should be part of the public API, but i don't think the rest of the class needs to be.. if this is public, no _ at the end please.. s/unit/cpu_unit/. incomplete comment. const std::string& please. is there ever a time to call UseManualTime(\"myunits\")? Ie, not use the second and third parameters? Consider either removing the default parameters from here or having all the parameters default to sensible ... defaults.. we're defaulting to \"s\" but with nanosecond multipliers? that might warrant a comment to explain it.. it's marginally more readable to not call CpuUnit here but instead call time_.SetCpuUnitString(...). It saves a bit of cognitive load.. you can always set this to cost_function. If it's left unset, it'll be defaulted to SecondsCost. If the user passed nullptr then they likely made a mistake and you should CHECK that.. this line makes me think that b.time should instead be b.manual_time to make it clear it's set from 'manual time'.. It is to me. I want the public API to be very clear, especially when we have to define structures by necessity in the header.. Thanks, but no. Let's keep this focused. I'll take care of Statistics.. I don't know what Dsc stands for intuitively. I don't think Info is right either and I was fine with BenchmarkTime... maybe BenchmarkTimeConverter or BenchmarkTimeOutput if you really want to pick nits.\nWeak opinion, weakly held, so don't treat this as blocking.. Ah fair.. BenchmarkTimeDescription or maybe BenchmarkTimeDesc then?. nit: this isn't what the function does. It drops the last field after 'sep'.. This is perfectly reasonable, especially the notion of dropping the const from value types in API declarations. It's a little odd to see it as an extern declaration but it works perfectly.. please stick to the surrounding style. it's google-style per clang-format if you could please format it.. i think this is defined in limits.h for qnx (http://www.qnx.co.uk/developers/docs/6.5.0/index.jsp?topic=%2Fcom.qnx.doc.neutrino_prog%2Fposix_conformance.html). thanks for checking!. nit: kName :). we shouldn't need str below if we have this, as str() == get(Field::kAll), right?. just int is fine here.. this seems fragile. it might be worth allowing the enums to take their natural values and using kAll (which would have value 7) as the enumerator count. Then you'd just need to shift wherever you access the field in the implementation.. thinking more, and looking at the access pattern, i think this is overkill. The sections of the benchmark name are always appended in the same order. Similarly, you only care about dropping 'args' from the complexity bit (as far as i can see).\nSo why not just have a struct with all the strings stored as individual fields with an accessor name and another NameWithoutArgs.\nIe:\n```c++\nstruct BenchmarkName {\n  std::string name;\n  std::string args;\n  std::string min_time;\n  std::string iterations;\n  ...\nstd::string name() { return absl::StrJoin({name, args, min_time, iterations...}, \"/\"); }  // or equivalent non-absl version\n  std::string nameNoArgs() { return absl::StrJoin({name, min_time, iterations...}, \"/\"); }  // also non-absl.\n};\n```\nthen the benchmark name is stored (as you have) as a BenchmarkName and we have two simple accessors for getting the formatted version.. can you add a comment:\n// TODO: use absl::StrJoin. ",
    "tfarina": "Thanks! Fixed.\n. Fixed by https://github.com/google/benchmark/commit/9934396e1ff0cc64943a3af94ef35e01f27d25da.\nThanks! Closing this.\n. ",
    "dcoeurjo": "thanks @dominichamon,\nI'm still having bunch of  issues (and 4e21f5e fixes the first one I reported): https://gist.github.com/dcoeurjo/8443935\n. Few warnings that are reported as \"errors\" (thanks to -Werror option):\n/Users/davidcoeurjolly/local/src/benchmark/src/sysinfo.cc:74:8: error: unused variable 'saw_mhz' [-Werror,-  Wunused-variable]\nbool saw_mhz = false;\nand\n14%] Building CXX object CMakeFiles/benchmark.dir/src/sysinfo.cc.o\n /Users/davidcoeurjolly/local/src/benchmark/src/sysinfo.cc:47:9: error: unused function 'EstimateCyclesPerSecond' [-Werror,-Wunused-function] \n int64_t EstimateCyclesPerSecond() {\n           ^\n /Users/davidcoeurjolly/local/src/benchmark/src/sysinfo.cc:55:6: error: unused function 'ReadIntFromFile' [-Werror,-Wunused-function]\n bool ReadIntFromFile(const char* file, int* value) {\n       ^ \n 2 errors generated.\nBeside this warnings, there is no more build errors ;)\n. BTW, it'd be better to have the binary and libs compiled in the current cmake build directory rather than in the source tree (bin/, libs/).\nI'm running the benchmark_test\n. I've run the benchmark test and it's a bit weird: the process uses only 5% of my CPU and does nothing to the std:err.\nin this example, I've let it run 16min (wall clock) and then kill it with CTRL-C\ntime benchmark_test\nReading /proc/self/cputime_ns failed. Using getrusage().\n^C\nbenchmark_test  11,04s user 40,18s system 5% cpu 16:21,84 total\n. I'll prepare a pull-request for the cmake stuff. \n(And no idea for the OS issue ;)) \n. I've just updated my working copy and the current release works perfectly on MacOS ! ;)\nI'm closing this issue and I've also push a pull-request with some cmake edits (make install).\n. Done ;)\n. of course.. let me fix this..\n. ok done, sorry.\n. A  quick and dirty fix solves the problem:\ntypedef std::map<char,int> T1;\n  BENCHMARK_TEMPLATE(BM_Constructor, T1);\nis ok.. \nIt's definitely related to preprocessing macro parsing but I don't know how to protect the \",\" in the macro parameters.\n. I did try the '\"(...)\" trick but the build failed too with my compiler:\nmytest: error: \n  expected expression\n    BENCHMARK_TEMPLATE(BM_Constructor, (std::map<int,char>) ....\n   ^\n    /usr/local/include/benchmark/benchmark.h:531:63: note: expanded from macro  'BENCHMARK_TEMPLATE'\n           (new ::benchmark::internal::Benchmark(#n \"<\" #a \">\", n<a>))\n. Oh you're right... no more cmake error when I specify a build type.\nBTW, it is a bit odd to get an error when using default cmake settings. \n. sure..\n. solved in #125 if you want to have look. \n. signed.\n. Actually, everything ends up in include/benchmark/\n%%%%> make install\n[ 85%] Built target benchmark\n[100%] Built target benchmark_test\nInstall the project...\n -- Install configuration: \"\"\n -- Installing: /usr/local/lib/libbenchmark.a\n -- Installing: /usr/local/include/benchmark\n -- Up-to-date: /usr/local/include/benchmark/benchmark.h\n -- Up-to-date: /usr/local/include/benchmark/macros.h\n. ",
    "pphaneuf": "It appears @sochka has not signed the CLA (contributor license agreement)? I'm a bit new to this here, so I might be wrong, but you'd be looking at one of the following two, as appropriate:\nhttps://developers.google.com/open-source/cla/individual\nhttps://developers.google.com/open-source/cla/corporate\n. To be clear, let us know as soon as you signed it, we'll check it's good on our side and re-integrate your commit. This is just a minor administrative setback, sorry about that.\n. Oh, you just need to pick the appropriate one! If you're a student, I think you should use the individual version.\nThis is unfortunately needed to ensure license compliance, for both you and us. As mentioned, I'm a bit new to this myself, and I'm trying to find out if there can be an exception for a small typo fix, but it is simpler/safer if you simply sign the CLA (it also applies to most other Google open source project). :-)\n. No problem, thank you! I'm putting back your contribution immediately.\n. Sorry, we had to revert this merge, because it appears @Gluttton has not signed the CLA (contributor license agreement)? I'm a bit new to this here, so I might be wrong, but you'd be looking at one of the following two, as appropriate:\nhttps://developers.google.com/open-source/cla/individual\nhttps://developers.google.com/open-source/cla/corporate\n. To be clear, let us know as soon as you signed it, we'll check it's good on our side and re-integrate your commit. This is just a minor administrative setback, sorry about that.\n. And you only need to sign one, whichever is more appropriate for your situation.\n. Could you take a look at the instructions in the CONTRIBUTING.md file regarding the contributor license agreement? I couldn't find you in either the CONTRIBUTORS, AUTHORS, or our internal list of agreement signatories.\nThanks!\n. @showlabor Thanks, I just confirmed that your electronic signature is registered correctly!\nI'm just around for this kind of verification, I'll let @dominichamon finish the code review and merge in the pull request as appropriate.\n. I see the signature, good to go for CLA. Thanks!\n. Sorry, yes, that's what I mean by \"I see the signature\", there's no email notification sent for this, we have to verify it manually (which I did), so it's all good.\nBTW, this CLA applies to all Google projects, so if you try to contribute to another project that sends you to the same CLA page, you can proceed like you did here (a pull request with one commit for your fix, and a separate commit to add yourself to the AUTHORS and CONTRIBUTORS files). The project maintainers will find you in the list of CLA signers, they'll just need to look.\nThanks for your contribution!\n. I just checked, @chenshuo has indeed signed the contributor agreement. Feel free to add yourself to the AUTHORS/CONTRIBUTORS files, as per the instructions, thank you!\n. You should add a commit to add yourself to the CONTRIBUTORS file, as well.\n. Make sure to comment on the \"files changed\" tab of the pull request rather than on the commits.\nWhen you comment on the commits, if there's any rebasing, it gets lost (or, at least, becomes hard to find). Also, depending on how you configured your GitHub notifications, those emails might go to personal rather than work email... :smiley: \n. If you provide projects for specific environment, I would suggest putting them in a contrib directory, say, to make it clear that they are only an unsupported best-effort thing?\n. BTW, I added you to the AUTHORS and CONTRIBUTORS files in c7eb316e7ef74744e35694a2675e5a0955567333, as per the guidelines.\n. I confirm he did. Thanks!\n. I see that you have already signed the CLA, but could you also add a commit to put yourself in the AUTHORS/CONTRIBUTORS files, please? Thanks!\n. Thanks! I'll let others review the code proper, and merge it as appropriate... Good luck!\n. FYI: for automatic closing of PRs, it has to be a straight fast-forward merge of what's in the branch.\n. Seems to be a small hiccup with Google interns not being included as Google employees, somehow. You should feel free to ignore this, as long as you're a Google intern. :wink: \n(but I'll try to get it fixed)\n. Heh, rather hilariously, @dominichamon's CLA doesn't appear to have been migrated to the new system. :smiley: \nI know you signed it in the old system, could you re-sign it in the new one? Silly, I know, sorry...\nhttps://cla.developers.google.com/\n. Okay, now that is properly hilarious!\nWelcome back, I guess... :smiley: \n. Don't add typos, please. :smiley:\n. Some people do build in-tree, though, even if it's not the best idea...\nMe, I'd make a different comment: this should be a *.exe entry, the same way object files are handled.\n. Specifically adding the built executables makes sense for Unix platforms that don't have an executable extensions, but no need to duplicate this if there's an extension, just add a catchall.\n. ",
    "sochka": "Well, looks like there is quite a bit of bureaucracy in Google too if there is a need in signing two docs for fixing just a single typo)\nOk, what do I put at https://developers.google.com/open-source/cla/corporate  as my corporation name if I'm a student? Just leave it blank? \n. Oh, sorry. I've already signed the first one.\n. ",
    "ezhuk": "Great, thanks. Seemed to work for me on Ubuntu 13.10 3.11.0-12 x86_64.\n. PTAL final changes in 772ff6f.\n. Sure, will send a patch in a bit. Thanks.\n. Also added kNumNanosPerSecond.\n. Agree. Changed as well.\n. Mostly to be on the safe side and allow enough time for the condition variable to catch a signal. However, it looks like a better way would be to simply wait on the condition variable for 1ms (no need for SleepForMicroseconds in this case) and set the done flag before sending a signal in destructor. Made this change in ceea4cb. PTAL\n. Sure, will do in a bit.\n. Just figured it has been removed. Thanks.\n. ",
    "Gluttton": "In any case, in future it can be source of new questions and problems (like issue #5 and issue #6). In my opinion extract examples into separate source file better rather than fix comments time to time.\n. English isn't my mother tongue, so I don't clearly understand meaning of the licence.  As result, I wouldn't like to sign it (and any other document meaning of which I don't understand). But it doesn't mean that I have any claims!\nIn any way my changes couldn't be called as something significant and revert looks like the most simple way.  I'm realy sorry that I've made your trouble. I admire the Google and your projects!\n. > I think this means that our test for regex is failing with zero-as-a-null-pointer-constant.\nYes, you are right. It' caused because before launch test for regex CMAKE_CXX_FLAGS from the top project and from the benchmark project had had merged, as result test launched with combinations of flags -Wzero-as-a-null-pointer-constant -Werror. This error can be avoided by just changing sequence of directives in the top CMakeLists.txt, but this isn't obvious solution for users.\n```\ncmake_minimum_required  (VERSION 2.8)\nproject                 (MasterProject)\nadd_subdirectory (externals/benchmark)\nset (CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wzero-as-null-pointer-constant\")\nadd_subdirectory (src)\nadd_subdirectory (externals/gmock)\n```\nAnother question is this expected behaviour or some kind of error. As far as intention of test for regex is realizing of support of regex but not compile piece of code, in my opinion this is some kind of error. I believe that it would be better if in the benchmark project -Werror flag added after processing tests.\n\nCould you attach the CMakeOutput.log and CMakeError.log files please?\n\nCMakeError.log:\n```\nDetermining if the pthread_create exist failed with the following output:\nChange Dir: /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp\nRun Build Command:/usr/bin/make \"cmTryCompileExec3953001650/fast\"\n/usr/bin/make -f CMakeFiles/cmTryCompileExec3953001650.dir/build.make CMakeFiles/cmTryCompileExec3953001650.dir/build\nmake[1]: Entering directory /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\n/usr/bin/cmake -E cmake_progress_report /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/CMakeFiles 1\nBuilding C object CMakeFiles/cmTryCompileExec3953001650.dir/CheckSymbolExists.c.o\n/usr/bin/cc    -o CMakeFiles/cmTryCompileExec3953001650.dir/CheckSymbolExists.c.o   -c /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/CheckSymbolExists.c\nLinking C executable cmTryCompileExec3953001650\n/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTryCompileExec3953001650.dir/link.txt --verbose=1\n/usr/bin/cc       CMakeFiles/cmTryCompileExec3953001650.dir/CheckSymbolExists.c.o  -o cmTryCompileExec3953001650 -rdynamic \nCMakeFiles/cmTryCompileExec3953001650.dir/CheckSymbolExists.c.o: In functionmain':\nCheckSymbolExists.c:(.text+0x16): undefined reference to pthread_create'\ncollect2: error: ld returned 1 exit status\nmake[1]: *** [cmTryCompileExec3953001650] Error 1\nmake[1]: Leaving directory/home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\nmake: *** [cmTryCompileExec3953001650/fast] Error 2\nFile /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/CheckSymbolExists.c:\n/ /\ninclude \nint main(int argc, char** argv)\n{\n  (void)argv;\nifndef pthread_create\nreturn ((int*)(&pthread_create))[argc];\nelse\n(void)argc;\n  return 0;\nendif\n}\nDetermining if the function pthread_create exists in the pthreads failed with the following output:\nChange Dir: /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp\nRun Build Command:/usr/bin/make \"cmTryCompileExec2673471904/fast\"\n/usr/bin/make -f CMakeFiles/cmTryCompileExec2673471904.dir/build.make CMakeFiles/cmTryCompileExec2673471904.dir/build\nmake[1]: Entering directory /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\n/usr/bin/cmake -E cmake_progress_report /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/CMakeFiles 1\nBuilding C object CMakeFiles/cmTryCompileExec2673471904.dir/CheckFunctionExists.c.o\n/usr/bin/cc   -DCHECK_FUNCTION_EXISTS=pthread_create   -o CMakeFiles/cmTryCompileExec2673471904.dir/CheckFunctionExists.c.o   -c /usr/share/cmake-2.8/Modules/CheckFunctionExists.c\nLinking C executable cmTryCompileExec2673471904\n/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTryCompileExec2673471904.dir/link.txt --verbose=1\n/usr/bin/cc   -DCHECK_FUNCTION_EXISTS=pthread_create    CMakeFiles/cmTryCompileExec2673471904.dir/CheckFunctionExists.c.o  -o cmTryCompileExec2673471904 -rdynamic -lpthreads \n/usr/bin/ld: cannot find -lpthreads\ncollect2: error: ld returned 1 exit status\nmake[1]: *** [cmTryCompileExec2673471904] Error 1\nmake[1]: Leaving directory/home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\nmake: *** [cmTryCompileExec2673471904/fast] Error 2\nPerforming C++ SOURCE FILE Test HAVE_FLAG_CXX_14 failed with the following output:\nChange Dir: /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp\nRun Build Command:/usr/bin/make \"cmTryCompileExec2307068490/fast\"\n/usr/bin/make -f CMakeFiles/cmTryCompileExec2307068490.dir/build.make CMakeFiles/cmTryCompileExec2307068490.dir/build\nmake[1]: Entering directory /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\n/usr/bin/cmake -E cmake_progress_report /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/CMakeFiles 1\nBuilding CXX object CMakeFiles/cmTryCompileExec2307068490.dir/src.cxx.o\n/usr/bin/c++    -Wzero-as-null-pointer-constant -DHAVE_FLAG_CXX_14   --std=c++14 -o CMakeFiles/cmTryCompileExec2307068490.dir/src.cxx.o -c /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/src.cxx\nc++: error: unrecognized command line option '--std=c++14'\nmake[1]: *** [CMakeFiles/cmTryCompileExec2307068490.dir/src.cxx.o] Error 1\nmake[1]: Leaving directory/home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\nmake: *** [cmTryCompileExec2307068490/fast] Error 2\nSource file was:\nint main() { return 0;}\n```\nCMakeOutput.log:\n```\nThe system is: Linux - 3.13.0-36-generic - x86_64\nCompiling the C compiler identification source file \"CMakeCCompilerId.c\" succeeded.\nCompiler: /usr/bin/cc \nBuild flags: \nId flags: \nThe output was:\n0\nCompilation of the C compiler identification source \"CMakeCCompilerId.c\" produced \"a.out\"\nThe C compiler identification is GNU, found in \"/home/gluttton/benchmark/build/CMakeFiles/2.8.12.2/CompilerIdC/a.out\"\nCompiling the CXX compiler identification source file \"CMakeCXXCompilerId.cpp\" succeeded.\nCompiler: /usr/bin/c++ \nBuild flags: \nId flags: \nThe output was:\n0\nCompilation of the CXX compiler identification source \"CMakeCXXCompilerId.cpp\" produced \"a.out\"\nThe CXX compiler identification is GNU, found in \"/home/gluttton/benchmark/build/CMakeFiles/2.8.12.2/CompilerIdCXX/a.out\"\nDetermining if the C compiler works passed with the following output:\nChange Dir: /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp\nRun Build Command:/usr/bin/make \"cmTryCompileExec23215403/fast\"\n/usr/bin/make -f CMakeFiles/cmTryCompileExec23215403.dir/build.make CMakeFiles/cmTryCompileExec23215403.dir/build\nmake[1]: Entering directory /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\n/usr/bin/cmake -E cmake_progress_report /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/CMakeFiles 1\nBuilding C object CMakeFiles/cmTryCompileExec23215403.dir/testCCompiler.c.o\n/usr/bin/cc    -o CMakeFiles/cmTryCompileExec23215403.dir/testCCompiler.c.o   -c /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/testCCompiler.c\nLinking C executable cmTryCompileExec23215403\n/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTryCompileExec23215403.dir/link.txt --verbose=1\n/usr/bin/cc       CMakeFiles/cmTryCompileExec23215403.dir/testCCompiler.c.o  -o cmTryCompileExec23215403 -rdynamic \nmake[1]: Leaving directory/home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\nDetecting C compiler ABI info compiled with the following output:\nChange Dir: /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp\nRun Build Command:/usr/bin/make \"cmTryCompileExec4208331672/fast\"\n/usr/bin/make -f CMakeFiles/cmTryCompileExec4208331672.dir/build.make CMakeFiles/cmTryCompileExec4208331672.dir/build\nmake[1]: Entering directory /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\n/usr/bin/cmake -E cmake_progress_report /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/CMakeFiles 1\nBuilding C object CMakeFiles/cmTryCompileExec4208331672.dir/CMakeCCompilerABI.c.o\n/usr/bin/cc    -o CMakeFiles/cmTryCompileExec4208331672.dir/CMakeCCompilerABI.c.o   -c /usr/share/cmake-2.8/Modules/CMakeCCompilerABI.c\nLinking C executable cmTryCompileExec4208331672\n/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTryCompileExec4208331672.dir/link.txt --verbose=1\n/usr/bin/cc     -v CMakeFiles/cmTryCompileExec4208331672.dir/CMakeCCompilerABI.c.o  -o cmTryCompileExec4208331672 -rdynamic  \nUsing built-in specs.\nCOLLECT_GCC=/usr/bin/cc\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/4.8/lto-wrapper\nTarget: x86_64-linux-gnu\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu 4.8.2-19ubuntu1' --with-bugurl=file:///usr/share/doc/gcc-4.8/README.Bugs --enable-languages=c,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-4.8 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --with-gxx-include-dir=/usr/include/c++/4.8 --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --enable-gnu-unique-object --disable-libmudflap --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\nThread model: posix\ngcc version 4.8.2 (Ubuntu 4.8.2-19ubuntu1) \nCOMPILER_PATH=/usr/lib/gcc/x86_64-linux-gnu/4.8/:/usr/lib/gcc/x86_64-linux-gnu/4.8/:/usr/lib/gcc/x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/4.8/:/usr/lib/gcc/x86_64-linux-gnu/\nLIBRARY_PATH=/usr/lib/gcc/x86_64-linux-gnu/4.8/:/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/:/lib/x86_64-linux-gnu/:/lib/../lib/:/usr/lib/x86_64-linux-gnu/:/usr/lib/../lib/:/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../:/lib/:/usr/lib/\nCOLLECT_GCC_OPTIONS='-v' '-o' 'cmTryCompileExec4208331672' '-rdynamic' '-mtune=generic' '-march=x86-64'\n /usr/lib/gcc/x86_64-linux-gnu/4.8/collect2 --sysroot=/ --build-id --eh-frame-hdr -m elf_x86_64 --hash-style=gnu --as-needed -export-dynamic -dynamic-linker /lib64/ld-linux-x86-64.so.2 -z relro -o cmTryCompileExec4208331672 /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crt1.o /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crti.o /usr/lib/gcc/x86_64-linux-gnu/4.8/crtbegin.o -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -L/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu -L/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib -L/lib/x86_64-linux-gnu -L/lib/../lib -L/usr/lib/x86_64-linux-gnu -L/usr/lib/../lib -L/usr/lib/gcc/x86_64-linux-gnu/4.8/../../.. CMakeFiles/cmTryCompileExec4208331672.dir/CMakeCCompilerABI.c.o -lgcc --as-needed -lgcc_s --no-as-needed -lc -lgcc --as-needed -lgcc_s --no-as-needed /usr/lib/gcc/x86_64-linux-gnu/4.8/crtend.o /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crtn.o\nmake[1]: Leaving directory/home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\nParsed C implicit link information from above output:\n  link line regex: [^( |.[/])(ld|([^/]+-)?ld|collect2)[^/]*( |$)]\n  ignore line: [Change Dir: /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp]\n  ignore line: []\n  ignore line: [Run Build Command:/usr/bin/make \"cmTryCompileExec4208331672/fast\"]\n  ignore line: [/usr/bin/make -f CMakeFiles/cmTryCompileExec4208331672.dir/build.make CMakeFiles/cmTryCompileExec4208331672.dir/build]\n  ignore line: [make[1]: Entering directory `/home/gluttton/benchmark/build/CMakeFiles/CMakeTmp']\n  ignore line: [/usr/bin/cmake -E cmake_progress_report /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/CMakeFiles 1]\n  ignore line: [Building C object CMakeFiles/cmTryCompileExec4208331672.dir/CMakeCCompilerABI.c.o]\n  ignore line: [/usr/bin/cc    -o CMakeFiles/cmTryCompileExec4208331672.dir/CMakeCCompilerABI.c.o   -c /usr/share/cmake-2.8/Modules/CMakeCCompilerABI.c]\n  ignore line: [Linking C executable cmTryCompileExec4208331672]\n  ignore line: [/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTryCompileExec4208331672.dir/link.txt --verbose=1]\n  ignore line: [/usr/bin/cc     -v CMakeFiles/cmTryCompileExec4208331672.dir/CMakeCCompilerABI.c.o  -o cmTryCompileExec4208331672 -rdynamic  ]\n  ignore line: [Using built-in specs.]\n  ignore line: [COLLECT_GCC=/usr/bin/cc]\n  ignore line: [COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/4.8/lto-wrapper]\n  ignore line: [Target: x86_64-linux-gnu]\n  ignore line: [Configured with: ../src/configure -v --with-pkgversion='Ubuntu 4.8.2-19ubuntu1' --with-bugurl=file:///usr/share/doc/gcc-4.8/README.Bugs --enable-languages=c,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-4.8 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --with-gxx-include-dir=/usr/include/c++/4.8 --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --enable-gnu-unique-object --disable-libmudflap --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu]\n  ignore line: [Thread model: posix]\n  ignore line: [gcc version 4.8.2 (Ubuntu 4.8.2-19ubuntu1) ]\n  ignore line: [COMPILER_PATH=/usr/lib/gcc/x86_64-linux-gnu/4.8/:/usr/lib/gcc/x86_64-linux-gnu/4.8/:/usr/lib/gcc/x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/4.8/:/usr/lib/gcc/x86_64-linux-gnu/]\n  ignore line: [LIBRARY_PATH=/usr/lib/gcc/x86_64-linux-gnu/4.8/:/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/:/lib/x86_64-linux-gnu/:/lib/../lib/:/usr/lib/x86_64-linux-gnu/:/usr/lib/../lib/:/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../:/lib/:/usr/lib/]\n  ignore line: [COLLECT_GCC_OPTIONS='-v' '-o' 'cmTryCompileExec4208331672' '-rdynamic' '-mtune=generic' '-march=x86-64']\n  link line: [ /usr/lib/gcc/x86_64-linux-gnu/4.8/collect2 --sysroot=/ --build-id --eh-frame-hdr -m elf_x86_64 --hash-style=gnu --as-needed -export-dynamic -dynamic-linker /lib64/ld-linux-x86-64.so.2 -z relro -o cmTryCompileExec4208331672 /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crt1.o /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crti.o /usr/lib/gcc/x86_64-linux-gnu/4.8/crtbegin.o -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -L/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu -L/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib -L/lib/x86_64-linux-gnu -L/lib/../lib -L/usr/lib/x86_64-linux-gnu -L/usr/lib/../lib -L/usr/lib/gcc/x86_64-linux-gnu/4.8/../../.. CMakeFiles/cmTryCompileExec4208331672.dir/CMakeCCompilerABI.c.o -lgcc --as-needed -lgcc_s --no-as-needed -lc -lgcc --as-needed -lgcc_s --no-as-needed /usr/lib/gcc/x86_64-linux-gnu/4.8/crtend.o /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crtn.o]\n    arg [/usr/lib/gcc/x86_64-linux-gnu/4.8/collect2] ==> ignore\n    arg [--sysroot=/] ==> ignore\n    arg [--build-id] ==> ignore\n    arg [--eh-frame-hdr] ==> ignore\n    arg [-m] ==> ignore\n    arg [elf_x86_64] ==> ignore\n    arg [--hash-style=gnu] ==> ignore\n    arg [--as-needed] ==> ignore\n    arg [-export-dynamic] ==> ignore\n    arg [-dynamic-linker] ==> ignore\n    arg [/lib64/ld-linux-x86-64.so.2] ==> ignore\n    arg [-zrelro] ==> ignore\n    arg [-o] ==> ignore\n    arg [cmTryCompileExec4208331672] ==> ignore\n    arg [/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crt1.o] ==> ignore\n    arg [/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crti.o] ==> ignore\n    arg [/usr/lib/gcc/x86_64-linux-gnu/4.8/crtbegin.o] ==> ignore\n    arg [-L/usr/lib/gcc/x86_64-linux-gnu/4.8] ==> dir [/usr/lib/gcc/x86_64-linux-gnu/4.8]\n    arg [-L/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu] ==> dir [/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu]\n    arg [-L/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib] ==> dir [/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib]\n    arg [-L/lib/x86_64-linux-gnu] ==> dir [/lib/x86_64-linux-gnu]\n    arg [-L/lib/../lib] ==> dir [/lib/../lib]\n    arg [-L/usr/lib/x86_64-linux-gnu] ==> dir [/usr/lib/x86_64-linux-gnu]\n    arg [-L/usr/lib/../lib] ==> dir [/usr/lib/../lib]\n    arg [-L/usr/lib/gcc/x86_64-linux-gnu/4.8/../../..] ==> dir [/usr/lib/gcc/x86_64-linux-gnu/4.8/../../..]\n    arg [CMakeFiles/cmTryCompileExec4208331672.dir/CMakeCCompilerABI.c.o] ==> ignore\n    arg [-lgcc] ==> lib [gcc]\n    arg [--as-needed] ==> ignore\n    arg [-lgcc_s] ==> lib [gcc_s]\n    arg [--no-as-needed] ==> ignore\n    arg [-lc] ==> lib [c]\n    arg [-lgcc] ==> lib [gcc]\n    arg [--as-needed] ==> ignore\n    arg [-lgcc_s] ==> lib [gcc_s]\n    arg [--no-as-needed] ==> ignore\n    arg [/usr/lib/gcc/x86_64-linux-gnu/4.8/crtend.o] ==> ignore\n    arg [/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crtn.o] ==> ignore\n  remove lib [gcc]\n  remove lib [gcc_s]\n  remove lib [gcc]\n  remove lib [gcc_s]\n  collapse library dir [/usr/lib/gcc/x86_64-linux-gnu/4.8] ==> [/usr/lib/gcc/x86_64-linux-gnu/4.8]\n  collapse library dir [/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu] ==> [/usr/lib/x86_64-linux-gnu]\n  collapse library dir [/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib] ==> [/usr/lib]\n  collapse library dir [/lib/x86_64-linux-gnu] ==> [/lib/x86_64-linux-gnu]\n  collapse library dir [/lib/../lib] ==> [/lib]\n  collapse library dir [/usr/lib/x86_64-linux-gnu] ==> [/usr/lib/x86_64-linux-gnu]\n  collapse library dir [/usr/lib/../lib] ==> [/usr/lib]\n  collapse library dir [/usr/lib/gcc/x86_64-linux-gnu/4.8/../../..] ==> [/usr/lib]\n  implicit libs: [c]\n  implicit dirs: [/usr/lib/gcc/x86_64-linux-gnu/4.8;/usr/lib/x86_64-linux-gnu;/usr/lib;/lib/x86_64-linux-gnu;/lib]\n  implicit fwks: []\nDetermining if the CXX compiler works passed with the following output:\nChange Dir: /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp\nRun Build Command:/usr/bin/make \"cmTryCompileExec3828177552/fast\"\n/usr/bin/make -f CMakeFiles/cmTryCompileExec3828177552.dir/build.make CMakeFiles/cmTryCompileExec3828177552.dir/build\nmake[1]: Entering directory /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\n/usr/bin/cmake -E cmake_progress_report /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/CMakeFiles 1\nBuilding CXX object CMakeFiles/cmTryCompileExec3828177552.dir/testCXXCompiler.cxx.o\n/usr/bin/c++     -o CMakeFiles/cmTryCompileExec3828177552.dir/testCXXCompiler.cxx.o -c /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/testCXXCompiler.cxx\nLinking CXX executable cmTryCompileExec3828177552\n/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTryCompileExec3828177552.dir/link.txt --verbose=1\n/usr/bin/c++        CMakeFiles/cmTryCompileExec3828177552.dir/testCXXCompiler.cxx.o  -o cmTryCompileExec3828177552 -rdynamic \nmake[1]: Leaving directory/home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\nDetecting CXX compiler ABI info compiled with the following output:\nChange Dir: /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp\nRun Build Command:/usr/bin/make \"cmTryCompileExec1785567019/fast\"\n/usr/bin/make -f CMakeFiles/cmTryCompileExec1785567019.dir/build.make CMakeFiles/cmTryCompileExec1785567019.dir/build\nmake[1]: Entering directory /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\n/usr/bin/cmake -E cmake_progress_report /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/CMakeFiles 1\nBuilding CXX object CMakeFiles/cmTryCompileExec1785567019.dir/CMakeCXXCompilerABI.cpp.o\n/usr/bin/c++     -o CMakeFiles/cmTryCompileExec1785567019.dir/CMakeCXXCompilerABI.cpp.o -c /usr/share/cmake-2.8/Modules/CMakeCXXCompilerABI.cpp\nLinking CXX executable cmTryCompileExec1785567019\n/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTryCompileExec1785567019.dir/link.txt --verbose=1\n/usr/bin/c++      -v CMakeFiles/cmTryCompileExec1785567019.dir/CMakeCXXCompilerABI.cpp.o  -o cmTryCompileExec1785567019 -rdynamic  \nUsing built-in specs.\nCOLLECT_GCC=/usr/bin/c++\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/4.8/lto-wrapper\nTarget: x86_64-linux-gnu\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu 4.8.2-19ubuntu1' --with-bugurl=file:///usr/share/doc/gcc-4.8/README.Bugs --enable-languages=c,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-4.8 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --with-gxx-include-dir=/usr/include/c++/4.8 --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --enable-gnu-unique-object --disable-libmudflap --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\nThread model: posix\ngcc version 4.8.2 (Ubuntu 4.8.2-19ubuntu1) \nCOMPILER_PATH=/usr/lib/gcc/x86_64-linux-gnu/4.8/:/usr/lib/gcc/x86_64-linux-gnu/4.8/:/usr/lib/gcc/x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/4.8/:/usr/lib/gcc/x86_64-linux-gnu/\nLIBRARY_PATH=/usr/lib/gcc/x86_64-linux-gnu/4.8/:/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/:/lib/x86_64-linux-gnu/:/lib/../lib/:/usr/lib/x86_64-linux-gnu/:/usr/lib/../lib/:/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../:/lib/:/usr/lib/\nCOLLECT_GCC_OPTIONS='-v' '-o' 'cmTryCompileExec1785567019' '-rdynamic' '-shared-libgcc' '-mtune=generic' '-march=x86-64'\n /usr/lib/gcc/x86_64-linux-gnu/4.8/collect2 --sysroot=/ --build-id --eh-frame-hdr -m elf_x86_64 --hash-style=gnu --as-needed -export-dynamic -dynamic-linker /lib64/ld-linux-x86-64.so.2 -z relro -o cmTryCompileExec1785567019 /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crt1.o /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crti.o /usr/lib/gcc/x86_64-linux-gnu/4.8/crtbegin.o -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -L/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu -L/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib -L/lib/x86_64-linux-gnu -L/lib/../lib -L/usr/lib/x86_64-linux-gnu -L/usr/lib/../lib -L/usr/lib/gcc/x86_64-linux-gnu/4.8/../../.. CMakeFiles/cmTryCompileExec1785567019.dir/CMakeCXXCompilerABI.cpp.o -lstdc++ -lm -lgcc_s -lgcc -lc -lgcc_s -lgcc /usr/lib/gcc/x86_64-linux-gnu/4.8/crtend.o /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crtn.o\nmake[1]: Leaving directory/home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\nParsed CXX implicit link information from above output:\n  link line regex: [^( |.[/])(ld|([^/]+-)?ld|collect2)[^/]*( |$)]\n  ignore line: [Change Dir: /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp]\n  ignore line: []\n  ignore line: [Run Build Command:/usr/bin/make \"cmTryCompileExec1785567019/fast\"]\n  ignore line: [/usr/bin/make -f CMakeFiles/cmTryCompileExec1785567019.dir/build.make CMakeFiles/cmTryCompileExec1785567019.dir/build]\n  ignore line: [make[1]: Entering directory `/home/gluttton/benchmark/build/CMakeFiles/CMakeTmp']\n  ignore line: [/usr/bin/cmake -E cmake_progress_report /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/CMakeFiles 1]\n  ignore line: [Building CXX object CMakeFiles/cmTryCompileExec1785567019.dir/CMakeCXXCompilerABI.cpp.o]\n  ignore line: [/usr/bin/c++     -o CMakeFiles/cmTryCompileExec1785567019.dir/CMakeCXXCompilerABI.cpp.o -c /usr/share/cmake-2.8/Modules/CMakeCXXCompilerABI.cpp]\n  ignore line: [Linking CXX executable cmTryCompileExec1785567019]\n  ignore line: [/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTryCompileExec1785567019.dir/link.txt --verbose=1]\n  ignore line: [/usr/bin/c++      -v CMakeFiles/cmTryCompileExec1785567019.dir/CMakeCXXCompilerABI.cpp.o  -o cmTryCompileExec1785567019 -rdynamic  ]\n  ignore line: [Using built-in specs.]\n  ignore line: [COLLECT_GCC=/usr/bin/c++]\n  ignore line: [COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/4.8/lto-wrapper]\n  ignore line: [Target: x86_64-linux-gnu]\n  ignore line: [Configured with: ../src/configure -v --with-pkgversion='Ubuntu 4.8.2-19ubuntu1' --with-bugurl=file:///usr/share/doc/gcc-4.8/README.Bugs --enable-languages=c,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-4.8 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --with-gxx-include-dir=/usr/include/c++/4.8 --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --enable-gnu-unique-object --disable-libmudflap --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu]\n  ignore line: [Thread model: posix]\n  ignore line: [gcc version 4.8.2 (Ubuntu 4.8.2-19ubuntu1) ]\n  ignore line: [COMPILER_PATH=/usr/lib/gcc/x86_64-linux-gnu/4.8/:/usr/lib/gcc/x86_64-linux-gnu/4.8/:/usr/lib/gcc/x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/4.8/:/usr/lib/gcc/x86_64-linux-gnu/]\n  ignore line: [LIBRARY_PATH=/usr/lib/gcc/x86_64-linux-gnu/4.8/:/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib/:/lib/x86_64-linux-gnu/:/lib/../lib/:/usr/lib/x86_64-linux-gnu/:/usr/lib/../lib/:/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../:/lib/:/usr/lib/]\n  ignore line: [COLLECT_GCC_OPTIONS='-v' '-o' 'cmTryCompileExec1785567019' '-rdynamic' '-shared-libgcc' '-mtune=generic' '-march=x86-64']\n  link line: [ /usr/lib/gcc/x86_64-linux-gnu/4.8/collect2 --sysroot=/ --build-id --eh-frame-hdr -m elf_x86_64 --hash-style=gnu --as-needed -export-dynamic -dynamic-linker /lib64/ld-linux-x86-64.so.2 -z relro -o cmTryCompileExec1785567019 /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crt1.o /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crti.o /usr/lib/gcc/x86_64-linux-gnu/4.8/crtbegin.o -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -L/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu -L/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib -L/lib/x86_64-linux-gnu -L/lib/../lib -L/usr/lib/x86_64-linux-gnu -L/usr/lib/../lib -L/usr/lib/gcc/x86_64-linux-gnu/4.8/../../.. CMakeFiles/cmTryCompileExec1785567019.dir/CMakeCXXCompilerABI.cpp.o -lstdc++ -lm -lgcc_s -lgcc -lc -lgcc_s -lgcc /usr/lib/gcc/x86_64-linux-gnu/4.8/crtend.o /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crtn.o]\n    arg [/usr/lib/gcc/x86_64-linux-gnu/4.8/collect2] ==> ignore\n    arg [--sysroot=/] ==> ignore\n    arg [--build-id] ==> ignore\n    arg [--eh-frame-hdr] ==> ignore\n    arg [-m] ==> ignore\n    arg [elf_x86_64] ==> ignore\n    arg [--hash-style=gnu] ==> ignore\n    arg [--as-needed] ==> ignore\n    arg [-export-dynamic] ==> ignore\n    arg [-dynamic-linker] ==> ignore\n    arg [/lib64/ld-linux-x86-64.so.2] ==> ignore\n    arg [-zrelro] ==> ignore\n    arg [-o] ==> ignore\n    arg [cmTryCompileExec1785567019] ==> ignore\n    arg [/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crt1.o] ==> ignore\n    arg [/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crti.o] ==> ignore\n    arg [/usr/lib/gcc/x86_64-linux-gnu/4.8/crtbegin.o] ==> ignore\n    arg [-L/usr/lib/gcc/x86_64-linux-gnu/4.8] ==> dir [/usr/lib/gcc/x86_64-linux-gnu/4.8]\n    arg [-L/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu] ==> dir [/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu]\n    arg [-L/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib] ==> dir [/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib]\n    arg [-L/lib/x86_64-linux-gnu] ==> dir [/lib/x86_64-linux-gnu]\n    arg [-L/lib/../lib] ==> dir [/lib/../lib]\n    arg [-L/usr/lib/x86_64-linux-gnu] ==> dir [/usr/lib/x86_64-linux-gnu]\n    arg [-L/usr/lib/../lib] ==> dir [/usr/lib/../lib]\n    arg [-L/usr/lib/gcc/x86_64-linux-gnu/4.8/../../..] ==> dir [/usr/lib/gcc/x86_64-linux-gnu/4.8/../../..]\n    arg [CMakeFiles/cmTryCompileExec1785567019.dir/CMakeCXXCompilerABI.cpp.o] ==> ignore\n    arg [-lstdc++] ==> lib [stdc++]\n    arg [-lm] ==> lib [m]\n    arg [-lgcc_s] ==> lib [gcc_s]\n    arg [-lgcc] ==> lib [gcc]\n    arg [-lc] ==> lib [c]\n    arg [-lgcc_s] ==> lib [gcc_s]\n    arg [-lgcc] ==> lib [gcc]\n    arg [/usr/lib/gcc/x86_64-linux-gnu/4.8/crtend.o] ==> ignore\n    arg [/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crtn.o] ==> ignore\n  remove lib [gcc_s]\n  remove lib [gcc]\n  remove lib [gcc_s]\n  remove lib [gcc]\n  collapse library dir [/usr/lib/gcc/x86_64-linux-gnu/4.8] ==> [/usr/lib/gcc/x86_64-linux-gnu/4.8]\n  collapse library dir [/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu] ==> [/usr/lib/x86_64-linux-gnu]\n  collapse library dir [/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../../lib] ==> [/usr/lib]\n  collapse library dir [/lib/x86_64-linux-gnu] ==> [/lib/x86_64-linux-gnu]\n  collapse library dir [/lib/../lib] ==> [/lib]\n  collapse library dir [/usr/lib/x86_64-linux-gnu] ==> [/usr/lib/x86_64-linux-gnu]\n  collapse library dir [/usr/lib/../lib] ==> [/usr/lib]\n  collapse library dir [/usr/lib/gcc/x86_64-linux-gnu/4.8/../../..] ==> [/usr/lib]\n  implicit libs: [stdc++;m;c]\n  implicit dirs: [/usr/lib/gcc/x86_64-linux-gnu/4.8;/usr/lib/x86_64-linux-gnu;/usr/lib;/lib/x86_64-linux-gnu;/lib]\n  implicit fwks: []\nDetermining if files pthread.h exist passed with the following output:\nChange Dir: /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp\nRun Build Command:/usr/bin/make \"cmTryCompileExec2131177943/fast\"\n/usr/bin/make -f CMakeFiles/cmTryCompileExec2131177943.dir/build.make CMakeFiles/cmTryCompileExec2131177943.dir/build\nmake[1]: Entering directory /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\n/usr/bin/cmake -E cmake_progress_report /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/CMakeFiles 1\nBuilding C object CMakeFiles/cmTryCompileExec2131177943.dir/CheckIncludeFiles.c.o\n/usr/bin/cc    -o CMakeFiles/cmTryCompileExec2131177943.dir/CheckIncludeFiles.c.o   -c /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/CheckIncludeFiles.c\nLinking C executable cmTryCompileExec2131177943\n/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTryCompileExec2131177943.dir/link.txt --verbose=1\n/usr/bin/cc       CMakeFiles/cmTryCompileExec2131177943.dir/CheckIncludeFiles.c.o  -o cmTryCompileExec2131177943 -rdynamic \nmake[1]: Leaving directory/home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\nDetermining if the function pthread_create exists in the pthread passed with the following output:\nChange Dir: /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp\nRun Build Command:/usr/bin/make \"cmTryCompileExec3263492280/fast\"\n/usr/bin/make -f CMakeFiles/cmTryCompileExec3263492280.dir/build.make CMakeFiles/cmTryCompileExec3263492280.dir/build\nmake[1]: Entering directory /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\n/usr/bin/cmake -E cmake_progress_report /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/CMakeFiles 1\nBuilding C object CMakeFiles/cmTryCompileExec3263492280.dir/CheckFunctionExists.c.o\n/usr/bin/cc   -DCHECK_FUNCTION_EXISTS=pthread_create   -o CMakeFiles/cmTryCompileExec3263492280.dir/CheckFunctionExists.c.o   -c /usr/share/cmake-2.8/Modules/CheckFunctionExists.c\nLinking C executable cmTryCompileExec3263492280\n/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTryCompileExec3263492280.dir/link.txt --verbose=1\n/usr/bin/cc   -DCHECK_FUNCTION_EXISTS=pthread_create    CMakeFiles/cmTryCompileExec3263492280.dir/CheckFunctionExists.c.o  -o cmTryCompileExec3263492280 -rdynamic -lpthread \nmake[1]: Leaving directory/home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\nPerforming C++ SOURCE FILE Test HAVE_FLAG_CXX_11 succeded with the following output:\nChange Dir: /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp\nRun Build Command:/usr/bin/make \"cmTryCompileExec60651109/fast\"\n/usr/bin/make -f CMakeFiles/cmTryCompileExec60651109.dir/build.make CMakeFiles/cmTryCompileExec60651109.dir/build\nmake[1]: Entering directory /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\n/usr/bin/cmake -E cmake_progress_report /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/CMakeFiles 1\nBuilding CXX object CMakeFiles/cmTryCompileExec60651109.dir/src.cxx.o\n/usr/bin/c++    -Wzero-as-null-pointer-constant -DHAVE_FLAG_CXX_11   --std=c++11 -o CMakeFiles/cmTryCompileExec60651109.dir/src.cxx.o -c /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/src.cxx\nLinking CXX executable cmTryCompileExec60651109\n/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTryCompileExec60651109.dir/link.txt --verbose=1\n/usr/bin/c++    -Wzero-as-null-pointer-constant -DHAVE_FLAG_CXX_11    CMakeFiles/cmTryCompileExec60651109.dir/src.cxx.o  -o cmTryCompileExec60651109 -rdynamic \nmake[1]: Leaving directory/home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\nSource file was:\nint main() { return 0;}\nPerforming C++ SOURCE FILE Test HAVE_FLAG_CXX_0X succeded with the following output:\nChange Dir: /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp\nRun Build Command:/usr/bin/make \"cmTryCompileExec1987886490/fast\"\n/usr/bin/make -f CMakeFiles/cmTryCompileExec1987886490.dir/build.make CMakeFiles/cmTryCompileExec1987886490.dir/build\nmake[1]: Entering directory /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\n/usr/bin/cmake -E cmake_progress_report /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/CMakeFiles 1\nBuilding CXX object CMakeFiles/cmTryCompileExec1987886490.dir/src.cxx.o\n/usr/bin/c++    -Wzero-as-null-pointer-constant -DHAVE_FLAG_CXX_0X   --std=c++0x -o CMakeFiles/cmTryCompileExec1987886490.dir/src.cxx.o -c /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/src.cxx\nLinking CXX executable cmTryCompileExec1987886490\n/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTryCompileExec1987886490.dir/link.txt --verbose=1\n/usr/bin/c++    -Wzero-as-null-pointer-constant -DHAVE_FLAG_CXX_0X    CMakeFiles/cmTryCompileExec1987886490.dir/src.cxx.o  -o cmTryCompileExec1987886490 -rdynamic \nmake[1]: Leaving directory/home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\nSource file was:\nint main() { return 0;}\nPerforming C++ SOURCE FILE Test HAVE_WALL succeded with the following output:\nChange Dir: /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp\nRun Build Command:/usr/bin/make \"cmTryCompileExec1686999449/fast\"\n/usr/bin/make -f CMakeFiles/cmTryCompileExec1686999449.dir/build.make CMakeFiles/cmTryCompileExec1686999449.dir/build\nmake[1]: Entering directory /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\n/usr/bin/cmake -E cmake_progress_report /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/CMakeFiles 1\nBuilding CXX object CMakeFiles/cmTryCompileExec1686999449.dir/src.cxx.o\n/usr/bin/c++    -Wzero-as-null-pointer-constant --std=c++11 -DHAVE_WALL   -Wall -o CMakeFiles/cmTryCompileExec1686999449.dir/src.cxx.o -c /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/src.cxx\nLinking CXX executable cmTryCompileExec1686999449\n/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTryCompileExec1686999449.dir/link.txt --verbose=1\n/usr/bin/c++    -Wzero-as-null-pointer-constant --std=c++11 -DHAVE_WALL    CMakeFiles/cmTryCompileExec1686999449.dir/src.cxx.o  -o cmTryCompileExec1686999449 -rdynamic \nmake[1]: Leaving directory/home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\nSource file was:\nint main() { return 0;}\nPerforming C++ SOURCE FILE Test HAVE_WSHADOW succeded with the following output:\nChange Dir: /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp\nRun Build Command:/usr/bin/make \"cmTryCompileExec636583574/fast\"\n/usr/bin/make -f CMakeFiles/cmTryCompileExec636583574.dir/build.make CMakeFiles/cmTryCompileExec636583574.dir/build\nmake[1]: Entering directory /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\n/usr/bin/cmake -E cmake_progress_report /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/CMakeFiles 1\nBuilding CXX object CMakeFiles/cmTryCompileExec636583574.dir/src.cxx.o\n/usr/bin/c++    -Wzero-as-null-pointer-constant --std=c++11 -Wall -DHAVE_WSHADOW   -Wshadow -o CMakeFiles/cmTryCompileExec636583574.dir/src.cxx.o -c /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/src.cxx\nLinking CXX executable cmTryCompileExec636583574\n/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTryCompileExec636583574.dir/link.txt --verbose=1\n/usr/bin/c++    -Wzero-as-null-pointer-constant --std=c++11 -Wall -DHAVE_WSHADOW    CMakeFiles/cmTryCompileExec636583574.dir/src.cxx.o  -o cmTryCompileExec636583574 -rdynamic \nmake[1]: Leaving directory/home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\nSource file was:\nint main() { return 0;}\nPerforming C++ SOURCE FILE Test HAVE_WERROR succeded with the following output:\nChange Dir: /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp\nRun Build Command:/usr/bin/make \"cmTryCompileExec1996091765/fast\"\n/usr/bin/make -f CMakeFiles/cmTryCompileExec1996091765.dir/build.make CMakeFiles/cmTryCompileExec1996091765.dir/build\nmake[1]: Entering directory /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\n/usr/bin/cmake -E cmake_progress_report /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/CMakeFiles 1\nBuilding CXX object CMakeFiles/cmTryCompileExec1996091765.dir/src.cxx.o\n/usr/bin/c++    -Wzero-as-null-pointer-constant --std=c++11 -Wall -Wshadow -DHAVE_WERROR   -Werror -o CMakeFiles/cmTryCompileExec1996091765.dir/src.cxx.o -c /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/src.cxx\nLinking CXX executable cmTryCompileExec1996091765\n/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTryCompileExec1996091765.dir/link.txt --verbose=1\n/usr/bin/c++    -Wzero-as-null-pointer-constant --std=c++11 -Wall -Wshadow -DHAVE_WERROR    CMakeFiles/cmTryCompileExec1996091765.dir/src.cxx.o  -o cmTryCompileExec1996091765 -rdynamic \nmake[1]: Leaving directory/home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\nSource file was:\nint main() { return 0;}\nPerforming C++ SOURCE FILE Test HAVE_PEDANTIC_ERRORS succeded with the following output:\nChange Dir: /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp\nRun Build Command:/usr/bin/make \"cmTryCompileExec3438811830/fast\"\n/usr/bin/make -f CMakeFiles/cmTryCompileExec3438811830.dir/build.make CMakeFiles/cmTryCompileExec3438811830.dir/build\nmake[1]: Entering directory /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\n/usr/bin/cmake -E cmake_progress_report /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/CMakeFiles 1\nBuilding CXX object CMakeFiles/cmTryCompileExec3438811830.dir/src.cxx.o\n/usr/bin/c++    -Wzero-as-null-pointer-constant --std=c++11 -Wall -Wshadow -Werror -DHAVE_PEDANTIC_ERRORS   -pedantic-errors -o CMakeFiles/cmTryCompileExec3438811830.dir/src.cxx.o -c /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/src.cxx\nLinking CXX executable cmTryCompileExec3438811830\n/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTryCompileExec3438811830.dir/link.txt --verbose=1\n/usr/bin/c++    -Wzero-as-null-pointer-constant --std=c++11 -Wall -Wshadow -Werror -DHAVE_PEDANTIC_ERRORS    CMakeFiles/cmTryCompileExec3438811830.dir/src.cxx.o  -o cmTryCompileExec3438811830 -rdynamic \nmake[1]: Leaving directory/home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\nSource file was:\nint main() { return 0;}\nPerforming C++ SOURCE FILE Test HAVE_FNO_STRICT_ALIASING succeded with the following output:\nChange Dir: /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp\nRun Build Command:/usr/bin/make \"cmTryCompileExec2091003888/fast\"\n/usr/bin/make -f CMakeFiles/cmTryCompileExec2091003888.dir/build.make CMakeFiles/cmTryCompileExec2091003888.dir/build\nmake[1]: Entering directory /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\n/usr/bin/cmake -E cmake_progress_report /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/CMakeFiles 1\nBuilding CXX object CMakeFiles/cmTryCompileExec2091003888.dir/src.cxx.o\n/usr/bin/c++    -Wzero-as-null-pointer-constant --std=c++11 -Wall -Wshadow -Werror -pedantic-errors -DHAVE_FNO_STRICT_ALIASING   -fno-strict-aliasing -o CMakeFiles/cmTryCompileExec2091003888.dir/src.cxx.o -c /home/gluttton/benchmark/build/CMakeFiles/CMakeTmp/src.cxx\nLinking CXX executable cmTryCompileExec2091003888\n/usr/bin/cmake -E cmake_link_script CMakeFiles/cmTryCompileExec2091003888.dir/link.txt --verbose=1\n/usr/bin/c++    -Wzero-as-null-pointer-constant --std=c++11 -Wall -Wshadow -Werror -pedantic-errors -DHAVE_FNO_STRICT_ALIASING    CMakeFiles/cmTryCompileExec2091003888.dir/src.cxx.o  -o cmTryCompileExec2091003888 -rdynamic \nmake[1]: Leaving directory/home/gluttton/benchmark/build/CMakeFiles/CMakeTmp'\nSource file was:\nint main() { return 0;}\n```\n. > We have to compile the code to test for support.\nYou try to realise this with answer the question:\nCan be some piece of code compiled without warnings?\nIn my opinion this is wrong, and the right way to realise this with answer the question:\nCan be some piece of code compiled?\n\nI just tested this and the tests pass with that warning flag enabled, so something else is going on.\n\nI don't have big expirience with CMake but in my opinion all what you need is just put: add_cxx_compiler_flag(-Werror) in the: CMakeLists.txt after: C++ feature checks block.\n. ",
    "pleroy": "FWIW I have a port for Windows based on the pthread library at http://locklessinc.com/downloads/ (BSD 3-clause).  Not quite ready for a pull request, but getting there.\n. So I am running a test containing a single BENCHMARK with --benchmark_repetitions=2.\nSay that each repetition runs 1 iteration (the times don't matter here).  The stddev of the number of iterations at line 281 in the original code yields 0 and the final division by result.iteration in PrintRunData ends up being a zero divide.\nNow another scenario: say that the first repetition runs 1 iteration that takes 5 seconds and the second repetition runs 2 iterations that take a total of 12 seconds (so 6 seconds each).  For the second repetition the Stat1_d object at lines 251 of the original code is created with (12, 2) which ends up adding 24 seconds to the sum in the accumulated Stat1_d object.  So the mean shows up as (1 * 5 + 2 * 12)/3 = 29/3, much longer than the slowest test.\nI am not sure what you mean by \"This misreports the number of iterations for the mean and stddev lines in the report as the total iterations\".  What number of iterations do you think should be shown on the mean/stddev lines?\n. Upon reflection I would argue that the mean and stddev of the number of iterations are mostly irrelevant.  If the output tells me that my test took 5 seconds +/- 0.5, I want to know how many iterations have been executed in total, as it gives some indication of how reliable the statistics are.  Knowing, for instance, that the stddev on the number of iterations was 0 is really useless.\n. Ping?  I'd like to make progress on this issue...\n. Thanks for the feedback, I'll try to find a way to do what you suggest.  (In the meantime I may circulate other pull requests as I have accumulated quite a bit of stuff in my fork.)\n. Ok, it's a bit of a hack, but I've got code that does what you wish and fixes the bugs.  Here is an example output:\n```\nDEBUG: Benchmark                 Time(ns)    CPU(ns) Iterations\n\nDEBUG: BM_LongTest/64k             718159     740787        358\nDEBUG: BM_LongTest/64k             821440     821058        323\nDEBUG: BM_LongTest/64k_mean        767145     778860        341\nDEBUG: BM_LongTest/64k_stddev       51572      40082         17\nDEBUG: BM_LongTest/256k           3315655    3356984         79\nDEBUG: BM_LongTest/256k           3314799    3315021         80\nDEBUG: BM_LongTest/256k_mean      3315225    3335870         79\nDEBUG: BM_LongTest/256k_stddev        428      20981          0\nDEBUG: BM_LongTest/2M            29124379   29466856          9\nDEBUG: BM_LongTest/2M            29116299   29466856          9\nDEBUG: BM_LongTest/2M_mean       29120339   29466856          9\nDEBUG: BM_LongTest/2M_stddev         4040          0          0\nDEBUG: BM_LongTest/16M          421215415  421202700          1\nDEBUG: BM_LongTest/16M          419668794  421202700          1\nDEBUG: BM_LongTest/16M_mean     420442104  421202700          1\nDEBUG: BM_LongTest/16M_stddev      773311          0          0\nDEBUG: BM_LongTest/128M        1707170844 1700410900          1\nDEBUG: BM_LongTest/128M        2712272048 2714417400          1\nDEBUG: BM_LongTest/128M_mean   2209721446 2207414150          1\nDEBUG: BM_LongTest/128M_stddev  502550602  507003250          0\nDEBUG: BM_LongTest/256M        6074606776 6084039000          1\nDEBUG: BM_LongTest/256M        5447518945 5444434900          1\nDEBUG: BM_LongTest/256M_mean   5761062860 5764236950          1\nDEBUG: BM_LongTest/256M_stddev  313543916  319802050          0\n```\n. Ok, I have squashed all the commits into a big one (e5efd2b), keeping the changes that I did to address some of your comments separate.  It seems that unfortunately we have now lost quite a bit of the discussion (which doesn't seem unexpected since the history was rewritten and your comments were on individual commits).\n. @dominichamon: Sorry, I don't know cmake and I am not interested in learning it, so I'm not going there.  I need a benchmark library on Windows, I'm happy to contribute the port, I realize it may not be perfect, I'm happy to address minor comments, but that's as much time as I am willing to invest in this.\nAlso, I don't think that it's very user-friendly to use cmake on Windows.  As a Windows user I would expect to be able to use Visual Studio out of the box, without having to install prerequisites which try to make my system looks like *nix.\n. So @dominichamon, @ckennelly, where should we go with this pull request?  I addressed quite a bit of the comments made in the original review and then the thread died out.\n. @dominichamon: I understand the desire to use a unified build system, and I'd be fine with removing the MSVC files from this pull request.  However, I still think that the MSVC files generated by cmake should ultimately be in the repository, because otherwise you are asking every Windows user to install cmake.  That's AFAICT the approach adopted by gtest and glog.\n. @mattyclarkson: \"Simple things should be simple, complex things should be possible.\"  Provide the necessary files to run with the latest VS (2013 at this point) so that people with the most basic requirements can get started in 30 minutes.  Those who want to use, say, VS 2008 with clang will either use cmake or create their project setup themselves.  If you are making things complicated for the simplest use case you'll lose 80% of the users.\n. @EricWF, yeah, it seems that if PRs are left unanswered for 6 months they become impossible to merge.  I'll close this and keep using my own fork at pleroy/benchmark.  I am personally not interested in MinGW.\n. @mattyclarkson: I am using VS 2013.  It's pretty much the only version that makes sense anyway for this code because previous versions didn't have C++11 support.\n. On #29: unfortunately there was a first round of comments which I addressed/answered and then the review thread died out.  And I went on vacation and stuff and didn't ping the thread.  I guess it would be good to revive it.\nOn Regex: I agree that if we use the C++11 <regex> we can just drop the entire class and use <regex> directly in the flag parsing.  As a matter of fact I don't quite understand @dominichamon's objection to using the C++11 library since it's hard to believe that there's a lot of code (scripts?) out there depending on the exact syntax of regexs in flags.\n. @mattyclarkson: For testing, I'd suggest to make sure that you can select various groups of tests in benchmark_test.cc, e.g., the ones that have to do with Pi or exclude the LongTest or some such.  I have not used particularly complicated regex myself but it you're interested you can find some of the command lines that I am using in the comments at the beginning of e.g. https://github.com/mockingbirdnest/Principia/blob/master/benchmarks/n_body_system.cpp (and other files in that directory).\n. @EricWF, A more interesting question is: why is the iterations field a size_t but the iters variable an int?  There is a lot of confusion between signed and unsigned for the number of iterations (I know, the Google style guide recommendations don't really help).  This has caused trouble for me on 32-bit machines (yes, that's still a thing).\n. I believe that the member iterations in reporter.h should be fixed too (I seem to remember that it's much more pervasive).\n. FWIW I'd vote for size_t because it's going to be less surprising for clients.  For better or for worse int64_t is not used very often in APIs.  And anyway I don't imagine a practical need to do more than 1B iterations.\nI remember running into problems on 32-bit Windows with this line in console_reporter.cc:\nColorPrintf(COLOR_CYAN, \"%10lld\", result.iterations);\nIt would just print out garbage.  Changing to %10u fixed the problem for me.\n. Nope, some of these should be size_t.  Also, int is typically 32 bits, so you must make sure that you are not introducing overflows.\nThis is a tricky change, it's certainly not a global substitute.\n. I don't think that we should go out of our way to support ancient compilers (3.4.0 dates back to December 2013 if I am not mistaken).  That way lies madness.  Every change, however innocuous, would need to be tested with all ten versions of Clang between 3.4.0 and 3.7.0.  And the same would have to be done with gcc, Visual Studio, your-favorite-compiler-here.\nI don't buy the \"adoption\" argument, it's a straw man.  If someone is serious about benchmarking, they can easily apt-get a recent Clang, that will take all of 10 minutes.\n. I'm afraid you didn't do your math correctly, @dominichamon.  32 bits in nanoseconds is about 4 seconds.\n. @dominichamon: 64-bit doubles only have 53 bits of mantissa, so integers between 2^53 and 2^63 (roughly) can be represented as int64 but not as double.  Hence the warning.\n. I seem to remember that state.PauseTiming() and state.ResumeTiming() are expensive because they call CHECK and CHECK is expensive.  It does:\n```\ndefine CHECK(b) ::benchmark::internal::GetNullLogInstance()\n```\nBut it should really do:\n```\ndefine CHECK(b) while (false) ::benchmark::internal::GetNullLogInstance()\n```\n. FWIW this greatly annoys me.  It means that I cannot pull from head without doing major changes to my benchmarks.  As a matter of fact, I don't see that the fixtures cover all the possible use cases.  This seems like a drastic change for something which is only mildly broken.  These functions work just fine if what you are benchmarking takes long enough.\n. Two examples:\n- https://github.com/mockingbirdnest/Principia/blob/7cf932755c9c0546be3b5f16159a43b702ef4bd4/benchmarks/%D1%87%D0%B5%D0%B1%D1%8B%D1%88%D1%91%D0%B2_series.cpp has a benchmark BM_NewhallApproximation which generates a new set of (random) input data each time through the loop.  This makes sure that the benchmark samples a broad set of input values (the performance of the algorithm is quite sensitive to its input).\n- https://github.com/mockingbirdnest/Principia/blob/807fd9b78eb586138e5dafcd43a9676f23b82b50/benchmarks/embedded_explicit_runge_kutta_nystr%C3%B6m_integrator.cpp has benchmarks that do some numerical processing (e.g. SolveHarmonicOscillatorAndComputeError3D) and compute the resulting error.  When trying to optimize the code under test it is useful to evaluate the impact on the accuracy (there are many performance/accuracy tradeoffs here).\nIt seems to me that these use cases would be awkward/impossible to cover with fixtures.\n. Exactly, these benchmarks take minutes to run.  In some cases I ran into some slowness due to CHECK but after fixing that it worked for me.  ([boatcat]: I should send a PR.)\n. Would it make sense to specify the unit in the benchmark registration itself?  Something like:\nBENCHMARK(BM_NewhallApproximation)->Arg(4)->Arg(8)->Arg(16)->Unit(kMillisecond);\nI'd say that when you write a benchmark you have a reasonable idea of the time it will take.  The notion of having the reporting magically decide what order of magnitude works best worries me: in the above example it would be very inconvenient if the run for Arg(4) used microseconds but the run for Arg(16) used milliseconds.\n. This looks like a terrible idea to me.  In real life the setup can be fairly complicated and costly.  Running it in the loop would defeat the purpose of the loop, which is to get stable numbers by adjusting the number of iterations.\n. +1 to @EricWF's comment.  Each time we add something to the API we are making it a little bit harder to use.  Now you have two ways to create a main and the naive user has no way to decide which one they should pick.\nThe analogy with gtest is flawed: AFAICT gtest doesn't provide a \"main\" macro, it has RUN_ALL_TESTS but that's a different thing entirely.  If we were starting from scratch it might be nicer to have a \"main\" library than a \"main\" macro, but we cannot break compatibility so that's not an option.\nI'd say that the whole thing is insufficiently broken and that fixing it is not worth the added complexity.\n. FWIW I like this API much better.  No time to look at #262 tonight though.\n. The library being benchmarked might very well have its own flags and want to parse argc/argv.. I don't care that much about BENCHMARK_MAIN, but I don't think we should deviate too much from the Google flags library.  Otherwise it makes it very hard to use different Google libraries together (protocol buffers, gtest, gmock come to mind) just because the handling of flags was \"improved\" in slightly different manners in different libraries.. I think it would be nicer.  The sad thing though is that gflags is based on the Google flag library as it existed in 2008, so it lags a bit.. > But still, I would say that the current algorithm to calculate standard deviation has large rounding errors, since the numbers passed to the stats class are usually very large (they are in the units of nanoseconds or bytes), and moreover the stats class computes their sum of squares that makes the total even larger.\nI believe that this reasoning is faulty.  Whether the numbers are large or small has no bearing on the accuracy of the result as long as there are no overflows/underflows.  To see why, just imagine multiplying/dividing all the numbers by a power of 2 before the computations: you could make them as large or as small as you want but that wouldn't affect the accuracy of the result.\nThere are two things that affect the accuracy of the result:\n1. The order in which numbers are added.  Ideally you'd want to add them in increasing order of magnitude, but in practice that's expensive so it would be sufficient to use compensated summation.\n1. The final subtraction avg_squares - Sqr(mean).  While it might be exact because the two numbers are probably within a factor of 2 or each other, you just end up with a significant cancellation, especially if the results have little dispersion.  Again, compensated summation would help preserve accuracy in the final result.\nHaving ranted about \"numerical analysis is hard\" I must add that this is spectacularly unimportant: the deviation is typically computed on a dozen samples or so, so it's not significant beyond the first or second digit.  Nobody cares about getting these last bits right.. @yixuan: Ah, I didn't realize that you wanted to use an offset.  I agree that it's a good solution that's well-conditioned and easy to implement.. What makes you think that the two benchmarks are going to loop the same number of times?  If BM_deleteInt went into more iterations than BM_newInt you'd index into the boonies.. I am going to side with @ldionne and claim that this PR is a horrible idea.  Depending on external tooling is pushing the burden of conversion on the user, and is making the library much less useful.  Now I need to install Python on all the machines that run benchmarks, find a random external tool that does the conversion I need, install that, and keep all this mess running.  Oh, and it gets worse when someone decides to write a conversion tool in Perl or Ruby or whatever.  Also, I frankly don't quite understand why an external Python tool is going to be easier to maintain than a simple C++ reporter.\nIf this PR went in I would probably be forced to stop upgrading to newer versions of this library, which would annoy me greatly.. Ah, I misunderstood what you were doing.\nI agree that CSV is not a great format, and probably not suitable for complex datasets.  I could imagine using CSV to provide essentially the same data as the console/tabular reporting, but it's hard to justify the complexity, with all the quoting and text massaging that needs to happen.\nI am a wee bit annoyed by the notion of using JSON as the \"pivot\" format: I'd have preferred to use protobuf (the alpha and omega of all things Google) for that purpose, as it avoids messy parsing/unparsing.  If for instance someone wants to produce XML (an eminently reasonable thing to do on Windows) it seems rather awkward to first produce JSON and then reparse it to produce XML.  My sense of good taste is a bit offended ;-). It seems to me that this PR is papering over a problem that we don't really understand.\nNegative zeroes don't show up for random reasons.  In particular, they never arise out of subtractions.  The only ways that you can construct a negative zero are by (1) multiplying a positive zero by a negative number or (2) multiplying two numbers of opposite signs in a way that results in an underflow.\nLooking at the code it's unclear to me how/where this could happen.  I would be more comfortable if we were to pepper the code with checks or traces to understand exactly where the negative zero comes from (this is easy to do with std::signbit).. I have the feeling, like @EricWF, that this issue is somehow related to MinGW.  I am using VS2015 and I don't remember ever getting negative numbers, even when benchmarking computations in the 5-10 ns range.. @EricWF: You mention \"the GNU printf behavior we expect (ie rounding to positive zero)\".  Well, your expectations are bad and you should feel bad.  Note 276 in section 7.21.6.1 of the latest C standard, ISO/IEC 9899:2011, says \"The results of all floating conversions of a negative zero, and of negative values that round to zero, include a minus sign\".\nIt's also amusing that the MinGW page that you link mentions \"The vc6.0 msvcrt.dll that MinGW-w64 targets\".  VC++ 6.0 was released in 1989 (not a typo); it's hard to believe that it would be compliant with modern C/C++ standards in all cases.. This is moving in the wrong direction.  The API may be broken and require rework, and the reporters may need to be rewritten from scratch, but this deprecation a feature that people use is a serious usability regression.\nSee also the discussion on #443: it's hard to believe that there is consensus for this change.. @EricWF: Sure, go ahead.  I need more time to digest your plans regarding JSON (will look at that carefully over the week-end) but in the meantime CSV can die.. This PR looks odd.\nI am using VS2015 and I don't see any warning about double inline.  Also, as @EricWF indicated, inline and __forceinline are completely different things.  Removing inline, a linkage directive, looks like it could cause ODR nightmares.\nThe line numbers in the Appveyor logs point at lines 644 and 647, but the declarations have not been at these lines since commit fa341e51cb7f6bce69a7577f4000381a03f61c70.  Are you sure that the problems still exists in the latest master commit?  (There was a problem with these declaration that got fixed in #469.)\n  . (Unrelated comment: my preference would be to remove inline from all the templates because it has really no effect whatsoever there.). (A) I was not trying to fix this issue (which I think is a red herring addressed by #469), just stating a preference (B) do we know that there are really compilers like that around? in this day and age, I think it's best to assume that inline doesn't have anything to do with performance.. TIL about LLVM FileCheck.  Pretty cool!. Looks good.  Thanks for doing this.. I find it hard to be excited about this:\n\nForking a process would make the benchmarks considerably slower, so it would have to be optional.\nForking a process in a way that works on all the OSes where the library runs is no easy feat.  See for instance what googletest does for death tests.  There are also nasty interactions with threads (again, see googletest).\nFor benchmarks to be heavily affected by the state of the allocator, they'd have to spend most of their time in the allocator, in which case you are really benchmarking the allocator, not your code.\n\nOverall, it seems like a lot of complexity for little benefits.. (FWIW I am using this library on Windows.)\nThis PR is moving in the wrong direction in my opinion as it would complicate the transition to Abseil, which I believe should be our (distant) goal as it would make it much easier to use Google libraries together.\nThe Windows headers are well known for polluting the macro space (it's the 90s, go for it!).  If that gets in the way, just #undef the conflicting symbols (I'd vote for doing that just after the Windows includes for clarity).  After all, this is sysinfo.cc, it already contains a lot of platform-specific crud.. I'd rather undef, and rename String to Str if we care about consistency, as that seems to be what absl has chosen.. Do you see the inlining warnings at the latest commit?  If so what compiler are you using and what /std flag?  For some reason this issue keeps popping up over and over again but AFAICT it was fixed a while ago.. Actually, the warning reproduces fine, it just depends in the order of the two directives, see https://godbolt.org/g/jWEmpV.  (Godbolt has an old compiler but I am running 15.7 preview 3 which was released this week, and I see the same thing.)  This was fixed in #469 a long time ago but somehow people keep reporting it.. This PR looks similar to #493.  The bottom line is that you cannot just remove inline, it's going to cause ODR violations.  I can't seem to reproduce a problem on godbolt (see https://godbolt.org/g/WS4We6), can you detail what issue you are running into, with which version of MSVC, etc.?. @BaaMeow said: \"because names can conceptually be anything (because of the more generic RegisterBenchmark function), I believe it would only be a temporary fix and tooling would run into issues later if they accidentally chose what we use as the \"reserved\" delimiter (not that it's actually reserved)\".\nI don't get this.  The benchmark name better be a C++ identifier, so a separator that cannot show up in an identifier is fine (Dominic proposed '.' which would work).\nI for one have a fair amount of tooling that parses the console output (was written long before JSON/CSV output existed).  I do not relish the perspective of updating it for what is in fact a very minor improvement.. Could we use this (and remove BENCHMARK_MAIN) in the tests of this project?  The current situation of having one main per test makes it hard to link all the tests in a single binary.  (Why you'd want to do that: to have a single test project in Visual Studio.). Exactly what @astrelni said.  A follow-up is fine by me.. Dealing the low-hanging fruit of replacing the macro with the benchmark_main target would already be an improvement IMO.. @dominichamon @LebedevRI: kIsIterationInvariant seems eminently reasonable to me.  There are many benchmarks where each iteration does a constant number of things.  You can also count them yourself, but having that as a flag seems useful.. This PR makes me cry tears of blood.\nWhen parsing any of the values in this test, which are all exactly representable in IEEE binary64, I surely hope that I get the exact value, not something fudged to within 4 ulps.\nNow it appears that some genius at the FSF put in GCC a warning that says \"don't compare floats for equality\".  Could we please disable this warning?  There are circumstances where comparing for equality is fine, and this is one of them.  And if you don't know what you are doing, compiler warnings are not helping anyway.\nFor your enjoyment, here are some words of wisdom from @stephentyrone, who understands numerics much better than I ever will. . I blamed this line and I am looking sternly at @EricWF.... See #634.. 1. The proposed fix would work, but it dodges the issue a bit.  If you are targeting UWP, then you ought to have code for the other versions of UWP (app store, xbox, etc.).  Replacing _WIN32 by WINAPI_FAMILY_PARTITION(WINAPI_PARTITION_DESKTOP) is fine for the Windows desktop but it is not going to work the next time a poor sap wants to target, say, the Metro API.\n\nDo we have any reason to believe that QueryPerformanceFreq() is related to the CPU frequency in any way?  It's not obvious from the documentation.  As far as I remember, the CPU frequency is only used in an informational message somewhere; it's not clear that the performance counter frequency would be more useful. . I don't understand why the registry would be less correct.  It's actually likely to be more correct if what you are after the CPU frequency: according to MSDN:\n\n\nThese APIs may make use of RDTSC, but might instead make use of a timing devices on the motherboard or some other system services that provide high-quality high-resolution timing information.\n\nSo QueryPerformanceFrequency may be the CPU frequency but then again it may be the frequency of some completely unrelated timing chip on your motherboard.. I'm not sure we want to put more information in the console reporter.  It seems to me that the kind of analysis that you suggest would be better done by tooling on top of the JSON output.\nAlso, what you propose seems awfully specific to one particular use case.  I have for instance a benchmark that compares C++ and FORTRAN implementations thus:\nBENCHMARK(BM_ComputeGeopotentialCpp)->Arg(2)->Arg(3)->Arg(5)->Arg(10);\nBENCHMARK(BM_ComputeGeopotentialF90)->Arg(2)->Arg(3)->Arg(5)->Arg(10);\nWhat's relevant in my case is to compare degree 2 with degree 2, degree 3 with degree 3, etc.  Your proposed output not work well there.. I have run into this with DenseRange, fixed it in my fork, never upstreamed it (shame).. I have wanted this feature for years.  I have code where the main thread does nothing but forking a bunch of threads that do CPU-intensive work and synchronize at the end.  At the moment the output looks like this (the second parameter of the benchmark is the number of threads used):\n```\n\nBenchmark                                       Time         CPU Iterations\nBM_EphemerisMultithreadingBenchmark/3/1 102495353 ns        0 ns        100 +9.98765338665172644e+06 m +1.99941207398748249e+07 m +2.99993670217528194e+07 m\nBM_EphemerisMultithreadingBenchmark/3/2  77841192 ns   156001 ns        100 +9.98765338665172644e+06 m +1.99941207398748249e+07 m +2.99993670217528194e+07 m\nBM_EphemerisMultithreadingBenchmark/3/3  49749029 ns        0 ns        100 +9.98765338665172644e+06 m +1.99941207398748249e+07 m +2.99993670217528194e+07 m\nBM_EphemerisMultithreadingBenchmark/3/4  46776591 ns        0 ns        100 +9.98765338665172644e+06 m +1.99941207398748249e+07 m +2.99993670217528194e+07 m\nBM_EphemerisMultithreadingBenchmark/3/5  46512723 ns   156001 ns        100 +9.98765338665172644e+06 m +1.99941207398748249e+07 m +2.99993670217528194e+07 m\n``\nThe CPU time reported here is completely useless.  I would really like to have a proper display of the total CPU time consumed by all threads, as that would give me a sense of the overhead of the multithreading.. Yes, it's a bit more code but it feels less ugly.\n. Done (merged, actually; I always confuse myself with rebase).\n. Fixed in bdb0ba1.\n. Done in 2579604.\n. Changed to a minimal set of files/directories in 575a2cb.\n. The syntax is different:__forceinlineis _instead_ ofinlinewhile the gcc attribute is _in addition_ toinline.  This could probably be handled with more complicated macros, but it's not worth the bother, I have never seen__forceinlineproduce useful results.\n. MingW64 is for gcc, I have no interest in using gcc on Windows.\n. I have trimmed downport.{h,cc}to the bare minimum in 5454072.\n. Unfortunately, this doesn't help, even with a very long --benchmark_min_time.  I'll need to rethink the way overhead is measured on Windows because the real-time numbers look semi-random at the moment (but the CPU times are good, which is what I am after at this point).\n. Yes, this can be done with a bit of forward declaration.  The slightly annoying thing is that now the benchmark test needs to depend on thesrc/directory to get a mutex, but I guess it's no big deal.  See c6ce76b.\n. Done in 9e0db00.\n. Done in 46f0680.\n. @dominichamon: I don't understand why you want it pulled out.  There is just no alternative on Windows.  We can argue whether the C++11 library is the right thing to use in general (that's the topic of #30) but there is no other way to do it on Windows.  Even if the regex matching is not quite the same, it's probably no big deal, it's just for a flag anyway.\n@ckennelly: Sorry, I'm not doing that.  #30 can wait.  Having to resolve all the conflicts that would result from doing #30 before this PR is not my idea of fun (and is a great way to introduce bugs).\n. @dominichamon: done in #31.\n@ckennelly: what data structure do you have in mind?  I don't know of an STL container that you can modify while iterating over it.  Remember that the modification is done miles away from the loop, so it's not like we can tweak the iterator as we loop.  What I did is just to reuse the null entries of the vector.\n. That's the point: I don't think that the changes will be restricted to these two files.  There is no reason to havere.{h,cc}anymore if we use the C++11regex.  We might as well includeregexinbenchmark.ccand use it directly inFindBenchmarks(after all, we don't wrapvectorormapin an auxiliary class).  Thenregex_test.ccgoes away too (we don't test standard libraries).  This significantly simplifies the code, but the restructuring is sizeable.\n. @ckennelly: I don't understand what you mean.  Iteration is necessary in the destructor of BenchmarkFamilies, which indirectly wants to modify the container being iterated upon.  This is a problem even if the index values are not compact.\n. Agreed, this discussion is superseded by #41 anyway.\n. I don't quite get this since I do know cmake, but from looking at what it does for googletest, I don't think it's applicable here, because I had to modify rather heavily the implementation that I found on the Interwebz.  I guess there might be a better library around, but I would prefer to check in this version to get something that works sooner rather than later.\n. Mixing signed and unsigned is going to cause a mess on Windows.  I don't think we should do this at a time when other PRs are trying to fix signed/unsigned confusion.\n. Ifstringstreamworks on Android, I vote for using that.  It's a bit more code, but at least it will be clear what happens and we won't have a signed->unsigned cast hidden in the middle of a complicated expression.\n. This include is makingbenchmark_mpi.hpart of the benchmark API.  That's a bad idea, it's leaking implementation details.  You should callinitialize_mpiat the beginning ofInitializeandfinalize_mpiat the end ofRunSpecifiedBenchmarks.\n. This file in general and this line in particular are making it impossible to compile the project without using cmake.  That's not acceptable: the code should be self-standing and compilable with any modern C++ compiler, at least in the case where mpi is not being used.\n. This PR has many style issues, I'll let @dominichamon comment on them, but this statement is missing a space before(and the code in this project uses!notnot.\n. It's not so similar because the two take different kinds of regexp.  Please document this.\n. It would be nice if the style was consistent, eg with the way that*and&are glued to the type or the variable (compare the two previous declarations).  I would suggest to run clang-format with style Google.\n. Please don't leave commented out code around.\n. Please use//` comments whenever possible.\n. I have seen the benchmarks, but I remain unconvinced by the wisdom of having a half-baked map-like class that's not quite like a map.  In particular (1) the iterator exposes internal details (such as the fact that the element is not a pair) and (2) the method names are misleading wrt their complexity (Find is O(N)).\nI don't quite buy the argument that this class has to be super-efficient and, like @EricWF, I would prefer to use STL containers whenever possible.  Your benchmark is a bit of a red-herring: anyone wanting to update the counters in the tight loop would be well-advised to grab an iterator to the counter before the loop and use it in the loop.  A std::map would work just fine for that purpose.\nAs it stands this PR increases considerably the surface of the API.  While I think that user-defined counters are a great idea, there has to be a way to add them with a much more compact API, and without an implementation that does its own management of storage and containers.\n. Please be consistent wrt the placement of const.  This library generally puts it before the type name.  (I am in the \"after the type name\" camp, but consistency trumps.) \n. This should go to stderr IMO because, guess what, it's an error.. I think that this should still CHECK when the return value is 0.. Not your problem, but it is utterly confusing that CHECK actually means DCHECK.  If we ever want to layer the benchmark library on top of glog (I am still hoping that one day all the bits of code that Google open-sources will work together) this will get in the way.. My point is that it would be nice if the code continued to CHECK-fail (i.e., crash the binary) in case of error in debug mode.  That would make it obvious if something goes wrong when reading the file.  (And I agree that a < 0 return value should also CHECK-fail.). This comment makes me sad.  I think it should say something like \n\nMonotonic clocks are hard, MinGW doesn't manage to get them right, so sometimes the subtraction will yield a negative value.  Guard against that.. @EricWF: This makes total sense.  It's a constant pain with compilers using x87 arithmetic to implement double, as they spill to memory in random places.\n\nI believe that the fix is to force flushing to double at the end of MakeTime using a volatile variable.  (Oh, and remove the max with 0 ;-). Fine, but that's going to be a bigger change, with all the platform-specific code and such.. This change seems unnecessary.  On a 32-bit system it has no effect and on a 64-bit system there is no harm in using size_t.  Same comment for all the places that touch iterations counters.. And I claim that it's a bad property.\nI happen to only ever run benchmarks on 64-bit systems.  Why would you force me to have to deal with 32-bit overflows just for a misconceived notion of uniformity?. I'm sorry, I don't buy the int vs. int32_t argument.\nAccording to cppreference, int can only be a 16-bit type on LP32 systems, e.g., Win16 or, maybe, some microcontrollers.  We have no evidence that the library was ever ported to such a system.  If software hasn't been ported, it's not portable, period.  The code is choke-full of usages of int so there is no point in an illusory attempt at improving it by using int32_t in a handful of places.\nIf someone wanted to port the library to Win16 that might be an interesting exercise, but that's not what this PR does.. This will be called StrFormat in absl, apparently.. Two spaces before //. The derivation of the formula is quite straightforward: take the sum of squares S in the Wikipedia article, derive it with respect to k and equate the derivative with 0, you get the estimated k.\nUsing a similar analysis it seems that the quantity that you are minimizing is the sum of terms like c * t_i - c^2 * gn_i / 2.  Look at this expression carefully: it can be negative for outliers: outliers will actually reduce the error.  Put it another way: what you are minimizing is not even mathematically a norm.  Therefore your claim that your method is \"equivalent\" to least square has no mathematical basis.  (It is if you ignore outliers, but that's not an acceptable thing to do: outliers will occur and they should not break the statistical analysis.)\nI don't think that this PR should go in.  I would welcome a reference to the Wikipedia article, though.. ",
    "mattyclarkson": "@dominichamon, I tried having a go at getting VS2013 up and going but there are a few things missing with the C++11 support. I'll just brain dump here in case anyone can pick it up:\n- gettimeofday needs to be polyfilled in walltime.cc\n- (gm|local)time_r needs to be changed for (gm|local)time_s. arguments to swap over.\n- constexpr support is sketchy so things don't compile\n- noexcept wasn't supported\n- CMakeList.txt needs if (MSVC) to avoid all the GCC/Clang flag checks (not an issue as they fail gracefully, just looks weird)\n- By default cmake adds /Wall which turns on warnings that even the STL that is shipped with Visual Studio doesn't compile with. I'd suggest starting with /W0 and slowly cranking that up to /W4 putting the necessary /wdXXXX in the CMakeList.txt or the pragma warning() around the offending areas. It would be great to have a clean MSVC build at high warning levels as it might catch things that GCC/Clang might miss\nIt's almost there, just needs a bit of love. cmake -G \"Visual Studio 12 2013\" gets things going.\nOnce the Visual Studio build is going the appveyor support to continuously check for MSVC support can be added by implementing msbuild like cppformat does here. Will just have to add the necessary lines to appveyor.yml when if \"%compiler_name%\"==\"msvc\".\n. @pleroy , the problem with adding MSVS files to the project is it supports just a small subset of users - what version of MSVS files do we commit? VS2013? VS2012 with clang? VS2010 with clang? What do we do when VS2014 comes around? I actually have gcc integrated into MSVS - the solution files won't help me with that.\nIt is the same requirement on any OS, I have to install cmake on my Arch, Ubuntu and Fedora boxes to build this library with Eclipse or make. On MacOSX people may want to build with XCode or make, but they have to have cmake to do it. It would be the same to add XCode or Eclipse IDE configuration files to the project. That would allow IDE users on MacOSX and Linux to not have to install cmake but then you end up with combinatorial explosion of IDE solutions.\nI would happily write you a Windows batch script that downloaded the latest cmake binaries from their website (no install) and generated a MSVS solution file by double clicking on the .bat if you feel that is needed for Windows users. However, aside from the gtest and glog examples you provide above I haven't personally come across any CMake projects that include the solution files and find it a clean solution to building projects.\nThis is obviously my two cents, I'm not bashing on your opinion and respect that you would like to have MSVS solution for convenience. Just trying to provide another view :smiley: \n. @izaid, I've done some porting to MinGW over at #64. That branch just needs a little bit more to get working on VS. Haven't had any time to do it recently though.\n. @EricWF, I'm keen to get the benchmarking library to work on Windows. I wrote a thinner MinGW port branch #64 but have been swamped with other stuff. I can try to have a look soon. I'll have to get reacquainted with the recent changes that have gone in though. Probably best to close both and create an issue for Windows support and wait for someone to pick it up?\n. @pleroy, what version of Visual Studio are you using? I can have a look at adding support now that you can get VS for free.\n. @pleroy, I'll have a look when I do the MinGW port - hopefully there shouldn't be much that VS needs to get going.\n. @pleroy, I had a look at getting MSVC up and running on the project. There's not much that needs to be done with regards to polyfilling the unix functions. Only gettimeofday really needs to be implemented and the correct headers. I've given up as I can't get on with the MSVC compilers implementation of the C++ standard and the warnings that it throws up. I'll have another look when VS 2015 is out and the compiler has moved along a bit. TBH, by then clang will be probably be a better alternative anyway.\n. I don't have a MacOSX machine so there may need to be extra extensions added for that? Does it use .dylib?\n. @pphaneuf, done.\n. @pphaneuf, cheers. Thanks for the quick response :smile:\n. Just added the shared library versions.\n. Hold on a second, I'm gonna make the library versions come from the git tags.\n. Actually, the git tag thing is much more work than I thought - will submit that as a later PR. You're welcome to merge this after review.\n. @dominichamon, added them in 982f346\n. OK, added .dylib in .gitignore and squashed the commits.\n. @dominichamon, there is a guide to pull the version number from the git tags. Then you can just stamp the repo at v1.0.1 and CMake would sort itself out?\n. By v1.0.1 I mean a semantic version - not saying that the project should be major version :wink:\n. @dominichamon, I forgot to set up my git configuration correctly when I cloned the repository. The commits are stamped with my work email matt.clarkson@vcatechnology.com. Depending on your views of rewriting history, you could rebase the commits to have my public email address - mattyclarkson@gmail.com. I don't mind having my work email address - but that'll die one day but my public address will only die with GMail. I'd hope that GMail will be around a little longer :wink:. Completely your shout.\n. No problems at all :smile:\n. Can we close this now #42 has been merged?\n. @ckennelly, that's OK. I just don't want to break the project if I start adding in changes :fearful:\n. @pleroy, that's alright, sometimes people get busy! Hoping to revive the Windows port.\nDropping the Regex class would nuke support for gcc 4.7 and 4.8 which have entirely functional C++11 libraries but don't have <regex> implemented. Having it fall back to POSIX regex is a win in those situations. If the collaborators are happy to require full C++11 support then we can move forward with just using <regex>.\nRegarding @dominichamon objection I think he saw some issues with the matching of the regular expressions. This could have been an issue with the std::regex_constants::syntax_option_type not being set correctly. There are various flavours of regex syntax available. That is just a wild guess, I can't talk for him :wink:\nFor now I'm just trying to get issue #30 solved so I can move on to the Windows building. Just need a shout out on what I should do. From @ckennelly comments above:\n1. Detect the correct regex header using CMake\n2. Implement the correct Regex back end depending on what was detected\n3. Determine some strings that should be matched that can test the back end correctly\nI'll try to do 1 and 2 today but may slip until after the weekend. As the regex is basically the --benchmark_filter option used as a matching I guess I could just create some arbitrary string tests - do you guys have any --benchmark_filter options that you use daily that you could contribute to the tests?\nI'm actually quite keen to keep the Regex class in there as it does provide a simple abstraction of the regular expression engines out there, someone might want to pull in PCRE in a fork or something.\n. @pleroy, thanks, will look into it!\n. The current commits do the following:\n- Tidy up CMakeLists.txt\n- Snippets to detect the correct regular expression engine\n  - These need to be snippets to make sure the regex engine works (gcc < 4.9)\n- Implements the C++11 backend\n- Adds a test case for some more complicated regex test case patterns\nRFC\n. I think this is ready for merge, if everyone is happy with it.\n. I'll rebase this in the morning off master to catch the other merge PRs\n. This is rebased on master now.\n. Hold on before merging - the regular expression is matching correctly but the benchmark_test is filtering everything out.\n. @dominichamon, have submitted a fix for that. That does mean that you now have to write a filter that is an exact regex match. Do you want me to make the regexec POSIX function do an exact match also?\n. I guess the regexec is actually doing regex_search not regex_match. Would you like me to change the C++11 engine to regex_search? Not sure how to change the regexec without going for re_match GNU regular expression function.\n. OK. Changed to regex_search and dropped patch for default . to .* as it is not needed now. Tested with both the C++11 backend and POSIX backend and they both pass the same tests.\n. Rebased on master to resolve merge conflicts.\n. @dominichamon, *Calculate* isn't a regular expression, I thought that --filter was just a regular expression? Let me fix this up.\n. I've pushed a set of tests to the benchmark_test branch that'll allow a set of tests for --benchmark_filter flag that I can then make sure pass at all times. If we could work on merging #47 I will have a test set to fix this bug?\nIt seems that --benchmark_filter=*Calculate doesn't let any tests run on master. I've added a test for this.\n. Rebased on master. I ran make test and re-ran your examples above on Arch Linux (gcc 4.9.1):\nstd::regex, 100% tests passed, 0 tests failed out of 7:\n$ test/benchmark_test --benchmark_filter=\"Calculate*\"\n$ # Runs BM_CalculatePi tests\n$ test/benchmark_test --benchmark_filter=\"*Calculate*\"\n$ # Doesn't run any tests (expected, invalid regex)\n$ test/benchmark_test --benchmark_filter=\"Calculate.*\"\n$ # Runs BM_CalculatePi tests\n$ test/benchmark_test --benchmark_filter=\".*Calculate.*\"\n$ # Runs BM_CalculatePi tests\n$ test/benchmark_test --benchmark_filter=\".*Calculate\"\n$ # Runs BM_CalculatePi tests\n$ test/benchmark_test --benchmark_filter=\".*Calculate.*\"\n$ # Runs BM_CalculatePi tests\nPOSIX regex, 100% tests passed, 0 tests failed out of 7:\n$ test/benchmark_test --benchmark_filter=\"Calculate*\"\n$ # Runs BM_CalculatePi tests\n$ test/benchmark_test --benchmark_filter=\"*Calculate*\"\n$ # Doesn't run any tests (expected, invalid regex)\n$ test/benchmark_test --benchmark_filter=\"Calculate.*\"\n$ # Runs BM_CalculatePi tests\n$ test/benchmark_test --benchmark_filter=\".*Calculate.*\"\n$ # Runs BM_CalculatePi tests\n$ test/benchmark_test --benchmark_filter=\".*Calculate\"\n$ # Runs BM_CalculatePi tests\n$ test/benchmark_test --benchmark_filter=\".*Calculate.*\"\n$ # Runs BM_CalculatePi tests\nCould you re-run on your box, to see if the tests pass and try some filters?\n. OK, I haven't tested with clang yet. I've got Ubuntu, Debian, Cent OS, Fedora VMs so will do a round of testing tomorrow with clang and gcc\n. Ah, the reason is that the check_cxx_source_compiles doesn't run the code. gcc passes this but the actual regular expression engine is unimplemented. I'll change it to try_run or something like that.\n. New commits fix this. They use try_run to make sure the regular expression engine actually works. Tested on Ubuntu 14.04.\nI've fixed up the compiler flag tests as well as they weren't actually working.\n. I checked try_run support and it's in the docs back to 2.8.0 so should be OK on drone.io\n. I haven't actually used clang for a while but I was under the impression\nthey were fully c++11 and c++14 compliant? Guess it depends on version. At\nleast it will fallback to one of the other engines!\nOn 22 Aug 2014 19:34, \"Dominic Hamon\" notifications@github.com wrote:\n\nlibstdc++ vs libc++, apparently.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/pull/41#issuecomment-53103162.\n. Ah, understood. Problem for another day... On to the Windows porting!\nOn 23 Aug 2014 15:31, \"Dominic Hamon\" notifications@github.com wrote:\nIt is, but libstdc++ isn't. Or something.\nWe could use libc++ if it is available but I'm not sure it is worth it.\nOn 23 Aug 2014 05:52, \"Matt Clarkson\" notifications@github.com wrote:\n\nI haven't actually used clang for a while but I was under the\nimpression\nthey were fully c++11 and c++14 compliant? Guess it depends on version.\nAt\nleast it will fallback to one of the other engines!\nOn 22 Aug 2014 19:34, \"Dominic Hamon\" notifications@github.com wrote:\n\nlibstdc++ vs libc++, apparently.\n\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/pull/41#issuecomment-53103162.\n\n\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/pull/41#issuecomment-53152034.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/pull/41#issuecomment-53154453.\n. @ckennelly, it's just the output of git describe, I'm not doing anything special there. Happy to modify it however you like.\n. @dominichamon, I added the line:\n\nstring(REGEX REPLACE \"-[0-9]+-g\" \"-\" GIT_VERSION ${GIT_VERSION})\nWhich changes v1.4.0-1-geed82ee3-dirty to v1.4.0-eed82ee3-dirty. The g before the SHA1 stands for git but you know that already because you're using git.\n. If this is merged along with #42 and #43 the CMakeLists.txt will be getting quite large. Once (and if) they are all merged I'll do a reduction pass to make the files smaller and neater. For example putting the detection of compiler flags into a one liner CMake function, etc.\n. I'd suggest link time optimisation as well if detected -flto?\n. LTO will probably bring minor benefits in such a small library, it allows optimisations to occur across internal compilation units inside a library. I've found that combined with -fvisibility=hidden it can help out quite nicely.\nI like my tools to try to help out as much as they can so would happily add as many flags that can help me not write bugs. It is completely up to you and the other collaborators!\n. @dominichamon, I added a CMake function to make it simpler in the CMakeList.txt to add flags.\n. Sorry, the two merge request were competing. Would have rebased in between but you're too efficient :smile:\n. Oops, sorry guys. Will check drone.io in future!\nOn 7 Aug 2014 18:30, \"Dominic Hamon\" notifications@github.com wrote:\n\npulled change out - also added a second build to drone.io that uses gcc4.8\nso we can enable c++11 features in that build.\nOn Thu, Aug 7, 2014 at 10:27 AM, Dominic Hamon dma@stripysock.com wrote:\n\nnope. damn.\nOn Thu, Aug 7, 2014 at 10:26 AM, Dominic Hamon dma@stripysock.com\nwrote:\n\nchecking if drone.io offers an upgraded cmake.\nOn Thu, Aug 7, 2014 at 10:23 AM, Chris Kennelly <\nnotifications@github.com\n\nwrote:\nURL_HASH isn't supported by all versions of CMake 2.8.x, namely the one\nused by drone.io for automated builds:\nhttps://drone.io/github.com/google/benchmark/latest\nI added this in 6087edd\n<\nhttps://github.com/google/benchmark/commit/6087edda9dbf6fcd91e319d3167a0f7a3f96dcc8\nbut then removed it in 92cd2e8\n<\nhttps://github.com/google/benchmark/commit/92cd2e82af8563b828f331a1b1af7b2eab901de3\nbecause of this issue.\n\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/pull/46#issuecomment-51503457.\n\n\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/pull/46#issuecomment-51504336.\n. Looks like the one on drone.io uses URL_MD5 - that works in CMake 3.0 and backwards.\n. Created #48 for a patch that will work on drone.io\n. Output of make test:\n\n```\nRunning tests...\nTest project /home/matt/git/benchmark\n    Start 1: benchmark\n1/7 Test #1: benchmark .........................   Passed   57.46 sec\n    Start 2: benchmark_filter_simple\n2/7 Test #2: benchmark_filter_simple ...........   Passed    5.97 sec\n    Start 3: benchmark_filter_prefix\n3/7 Test #3: benchmark_filter_prefix ...........   Passed    0.50 sec\n    Start 4: benchmark_filter_suffix\n4/7 Test #4: benchmark_filter_suffix ...........   Passed    5.92 sec\n    Start 5: benchmark_filter_both\n5/7 Test #5: benchmark_filter_both .............   Passed    0.50 sec\n    Start 6: benchmark_filter_regex_wildcard\n6/7 Test #6: benchmark_filter_regex_wildcard ...   Passed    5.94 sec\n    Start 7: regex\n7/7 Test #7: regex .............................   Passed    0.00 sec\n100% tests passed, 0 tests failed out of 7\nTotal Test time (real) =  76.28 sec\n``\n. I added a commit to solve theCONCATissue.  Just useset()instead. Does the same thing.\n. @dominichamon, #66 will be great to get in! I'll wait for that to touchdown before continuing with this. I've tested the patches on Arch, Windows. unfortunately I don't have access to MacOS so can't test that. Having travis do that would be great. Does travis support Windows?\n. That branch builds on Windows 8.1 and passes all tests with [x86_64-4.9.2-release-posix-seh-rt_v4-rev2`](http://sourceforge.net/projects/mingw-w64/files/Toolchains%20targetting%20Win64/Personal%20Builds/mingw-builds/4.9.2/threads-posix/seh/) using the following commands:\ncmake -G \"MinGW Makefiles\" -DCMAKE_BUILD_TYPE=DEBUG\nmingw32-make\nIt would be cool to set up appveyor to build this on Windows continuously. Any thoughts on that?\n. Just having a go at setting up appveyor on my personal repo. Will create a PR on top of this once it is merged.\n. @dominichamon, it's a cloud based windows build service (it's free for open source projects). I'm most of the way with the integration - should be able to finish it up on Monday. Would be really good to get that in!\n. The appveyor stuff has taken much longer than I thought it would - I needed to write a script that could download a specific version of MinGW depending on the version, threading model and exception model. The appveyor YAML file took a bit of cajoling but got there in the end. I'll push up the final changes today to the nt branch - I'm currently just working on the appveyor branch.\n. Got the build working for mingw-4.9.2-posix-seh, Now just building up the matrix to cover some other common build situations.\n. I edited the history to resolve your comments @dominichamon.\nI've pushed up the appveyor.yml configuration file on top of the MinGW port commits. It currently is conservative with what it compiles for as the build times are lengthy due to downloading a specific version of MinGW. You can see a completed build here. I've told appveyor to cache the download of the MinGW compiler so that should decrease build times. I cannot set up the appveyor for google/benchmark as I don't have the necessary admin rights to give to appveyor to set up the web hooks. Would it be possible for one of the benchmark team to set this up?\n. Cool - if the appveyor backend can get set up I can tune the configuration file to give speedy builds for MinGW.\n. :+1: \n. @tamarlev you can use the downloading script to get the correct MinGW version in this repository.. I don't really have a use case :blush: it's just if you look at autotools you can do --enabled-shared --enable-static which is what I assumed BENCHMARK_ENABLE_SHARED was implementing.\nIf we are just going with the BUILD_SHARED_LIBS so that each build does only one linkage type we need to modify @EricWF  5b41e128 changes that overrided the changes I did in #39.\nThat commit added BENCHMARK_ENABLE_SHARED and the if/else. Need @EricWF, comments here though as he probably changed that for a reason.\nI would expect if we only want the build to produce either a static or shared library then BECNHMARK_ENABLE_SHARED should set BUILD_SHARED_LIBS to YES when enabled and cmake will just do the right thing. The if/else block could just be:\n```\nDon't need to specify static or shared here as cmake respects the BUILD_SHARED_LIBS\nadd_library(benchmark ${SOURCE_FILES} ${RE_FILES})\nNot sure why we only need to link the threads when shared, maybe Eric has more insight.\nif (BUILD_SHARED_LIBS)\n  find_package(Threads REQUIRED)\n  target_link_libraries(benchmark ${CMAKE_THREAD_LIBS_INIT})\nendif()\n``\n. @EricWF, they would be output aslibbenchmark.aandlibbenchmark.so` so you could have a build that does both if you wanted.\nOK I'll get a patch together. @EricWF do you know why you move the linking of threading into the shared only branch here\n. @EricWF, nice! FYI you can turn issues into PRs with the github API.\n. @EricWF, FYI if you add resolves #114 in either the PR description or the commit message it will close the associated issue when you merge the PR.\n. @geoffromer, might want to have a look at #115 as that's changing the cmake code slightly.\n. @geoffromer, consider running git rebase master on your branch to remove the unnecessary Merge branch 'master' into cmake-patch commit.\n. @EricWF, no worries fella. Have got thick skin :wink:  Plus you own the repo so your decisions FTW.\n. @dominichamon, good point about test being included in the coverage. That's not very useful. Will ignore that directory.\n@EricWF yeah I was doing an in place build. I can patch it up to support out of source builds. It's interesting the project allows in source builds - usually CMake projects disable that.\n. Have pushed up a rebased branch addressing the comments. The error message for when coverage isn't supported is still a bit crappy:\nmessage(WARNING \"Coverage not available. Currently only available when the compiler supports coverage\")\nAny suggestions on the best thing to put there? I've never actually used coverage on clang it seems that you can use llvm-cov to generate the gcov compatible data that lcov picks up. I don't currently have my Mac plugged in so can't test today. Might be able to look at that in another patch.\n. @dominichamon, @EricWF addressed your comments and pushed up rebased commits.\n. @EricWF, @dominichamon want me to set up coveralls support? It's easy to do and uploads all the gcov data to there.\n. @EricWF, I just pushed up the change to see if the coveralls works. Once the build finishes the results should show up here. I can drop the patch if @dominichamon  doesn't want to use coveralls.\n. The coveralls support works Will just need to make it enabled for the repository after the PR is merged.\n. \n. @dominichamon, looks like coveralls is all set up, even commented on the PR! Ready for merge I guess.\n. It's still working on my account\n. @dominichamon, it's all ready to go - just have to enable it in appveyor and it'll read the appveyor.yml and start building.\n. Once it's going you could add the building badge to the README.md like the travis one?\n. Looks like it - https://ci.appveyor.com/project/google/benchmark\nNice work, thanks for that!\n. :+1:\n. :+1:\n. Done, this is actually much nicer because it fails faster - if no regular expression engine is found it fails at CMake configuration time rather than at build time :smile:\n. @dominichamon, there is pthread win32 - we can have a look at working out the correct ExternalProject_Add options to build that correctly on Windows. The README is pretty thorough for that project.\n. Having a look at the usage of pthread in the library, would a PR to port it to std::thread be acceptable?\n. I'll have a look at gcc support. I'll write it in a way that the benchmark::thread is selected from std::thread, boost::thread and then just::thread as they all provide the same semantics (for our purposes). I'll do it using the same detection that the regex back end enjoys.\nApparently there are some bugs in the MSVC11 implementation.\n. Creating threads in a constructor means that this is not fully constructed. In this case there are no virtual functions, nothing is happening in the body of the constructor and the order of the class members has the background thread last - so everything should be OK. :worried: \n. Well std::thread still requires you to like -pthread (well at least it did on gcc 4.7). I'll have a look into it.\n. :+1:\n. Added a test for ^a*$. Is that what you meant? I can add more tests for .* and other if needed?\n. Yeah - if you don't have the find_package(Theads REQUIRED) you get linker errors due to pthread not being found. When I get around to doing some Windows porting I'll look into what this means on that platform. Looking at the FindThread.cmake it looks like it doesn't really do much for Windows, but that'll be another problem for another day I guess!\n. well the test will run, it'll just return -1. make test runs the benchmark_test without a filter so there should be 50 tests ran. When I was doing the regex work this dropped to zero, but the test still passed.\ndepends how you use benchmark_test? I've been using it through make test to make sure that any changes I do to the project still run all the tests.\n. I've modified this so that the number of expected tests is passed on the command line, the program will work exactly the same as before if no integer is passed.\n. I'll leave those tests in, useful to test all cases. I wouldn't expect ill formed filters to match anything, but maybe we should be reporting an error for badly formed regular expressions when the argument is parsed.\n. :+1: \n. It automatically linked the shwapi.lib statically. I moved it into the CMakeList.txt\n. I think we shouldn't build in tree. I tried to follow the current .gitignore style that was specifically adding the built executables. I think a catchall *.exe would be much more useful.\n. Looking at the master I don't think so. This branch has bit rotted for about 3 months. I'll pull it's socks up this week.\n. Done. Thanks for the advice.\n. The reasons it's there is so someone can use that python module as a library and turn logging on (or leave it off) when using the methods individually. For example I can then do:\npython\nimport mingw\nversions = mingw.repository()\nroot = mingw.root(version = (4, 9, 2))\nAnd that will give me all the mingw operations without logging anything or I could pass my own logger in.\nThat being said - completely happy to remove all that stuff if you don't want it in there! I tried to make the script as reusable as possible, if people want to use it in other projects for appveyor support or integrate it into python based tool chains.\n. Well we should always just use the latest revision, which is what the script does. The revision of the mingw-builds is just when niXman rebuilds the compiler because there was a bug in the build. It's actually really hard to limit the revision as each compiler in the matrix has a different number of revisions.\nHowever, I may have misunderstood your question. The appveyor.yml limits the compilers we test against.\n. This comment would make more sense on line 19. A more apt comment would be # Link threading if building shared library\n. @geoffromer, can you detect what version of CMAKE is being used and put the INTERFACE in there if it is available?\nSomething like:\ntarget_link_libraries(benchmark ${CMAKE_TARGET_LINK_INTERFACE} ${CMAKE_THREAD_LIBS_INIT})\nCMAKE_TARGET_LINK_INTERFACE would be empty if cmake is too old. Just thinking out loud - then it would do the right thing if the user has a newer version of cmake but gracefully fallback if not.\n. I wasn't actually trying to fix anything. Was allowing CMake to pick the latest standard but that's not really necessary. I can suck this commit out if you like?\n. ack, sorry.\n. OK. Sorry about that! Will rebase now.\n. This adds -fsanitize=address to the link line as well which results in linking the sanitization library correctly. I can add a check for -static-asan to link the address sanitizer statically if available.\nHappy to remove though. Pretty handy in debug mode to get pointer bounds checking.\n. Ah, on board now. Will back out that change.\n. Just so you can implement features in debug mode quickly without having to worry about the high warning levels. For example, I'm hacking on a new feature and I want to just block out some code with #if 0 then I get unused variable errors but I'm just hacking so it's not an issue. However, once the feature stabilized in debug mode and the warnings have reduced (or gone) I can move to release mode and finalize the feature.\nAs always, if you don't agree happy to back it out.\n. It depends on the source being built with coverage. In the CMakeList.txt file in the route it sets up the COVERAGE build variant. It adds the --coverage flag.\n. I can look into that.\n. Well it depends on three things:\n- coverage support in the compiler used (--coverage on gcc)\n- lcov available on the command line\n- Knowing the generator to run the unit tests\nNot sure what the correct message is here.\n. That's fine by me. Pedantic errors are usually checking for standards conformance.\n. Yeah, you're correct.  Without these wrappers the LTO won't work due to AR not loading the correct plugin. The gcc-ar translates the -flto into the correct plugin flag. Without the wrappers static linking of the library fails spectacularly on 4.9.2\n. I can take that out. It doesnt do any harm being in there but is redundant\n. Ack. ctest would make perfect sense. Thanks.\n. I haven't changed that - it was that way in the project on master. Look at line 44. I just moved it so that it was grouped with the release flags. Want me to make it into the following?\nadd_cxx_compiler_flag(-Wstrict-aliasing)\nadd_cxx_compiler_flag(-fstrict-aliasing\n. OK. Will do that.\n. Will do!\n. @EricWF, you're correct the --coverage flag will only be enabled if someone does CMAKE_BUILD_TYPE=Coverage\n. I'll add the warning conditionally depending on if the flag is supported.\n. Makes sense. Will push up the change.\n. Yeah. Will push up a patch.\n. ",
    "KindDragon": "Relate to PR #175\n. CMake >= 3.1 will generate warning by policy CMP0054\nWhich CMake version you use?\n. Latest results https://ci.appveyor.com/project/KindDragon/benchmark\n. Please also enable appveyor for pull requests\n. Ready to merge\n. AppVeryour  does not want to build new commits\n. Maybe try re-add webhook?\n. Can you try start build manually?\n. It's work on Windows with several compilers https://ci.appveyor.com/project/google/benchmark. https://ci.appveyor.com/project/google/benchmark/branch/master/job/jqa5k85dur2lrr71. This PR broke Windows build: https://ci.appveyor.com/project/google/benchmark/build/692. @LebedevRI I can't find broken builds from master branch before your changes. Ah, it's because of line: \nBM_CounterSet2_Tabular/threads:8          231 ns    1953125 ns          8         30         40         10\n\n1953125 ns can't match regex [ ]*%hrfloat which is [ ]*[0-9]{1,5} ns. Why you remove CSV support? CSV support really easy to open in LibreOffice Calc and build some charts. > It is relatively to write an external tool to read the JSON and generate the CSV should it be necessary,\nYes, but not as easy as it is now \ud83d\ude1e . Just for test disabled\n. New CMake produce a lot of warnings without CMP0054 \n. > it might even be worth having an msvc.cmake and gcc_clang.cmake that can be conditionally included to make this cleaner.\nI don't know how to do that\n. With this code you can see CTest results directly in AppVeyor https://ci.appveyor.com/project/KindDragon/benchmark/build/job/0xjejcyo0930a00u/tests\n. Warning appear near \"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\", so it is unlikely to lead to errors\n. Ok\n. Done\n. Yes, sorry. Please leave on Windows \"lib\" name. Why iterations %float?. ",
    "EricWF": "Closing since the Windows build seems to be working and Appveyor is testing it on every commit.\n. With the recent and large scale changes to just about all the code I don't think this patch is going to merge in any meaningful way.\nIs there anybody that is willing to continue on with this work?\n. The MinGW port in #64 looks reasonable. Lets try and get that in soon.\n. A couple of issues with this:\n- Walltime seems to be a much faster implementation than std::chrono::steady_clock (or high_resolution_clock). Ironically I haven't benchmarked this quite yet.\n- We don't know if the standard library implementation is going to have support for std::chrono yet because we support c++0x along with c++11.\nWe could use it as a fallback implementation if we can't use the non-portable walltime implementation\n. What if we don't use git for our version control system? Github supports SVN as well and CMake 2.8 only allows SVN to be used with ExternalProject. I don't see why we can't just define the version numbers in the CMakeLists.txt by hand.\nI disagree with this approach entirely. \n. Is it possible that the line number just changed? Also a better implementation of CHECK would be nice. One that contains the expression would be nice. \n. Why would we want to build in C++14 when we don't use c++14? I think we should just drop that option from CMake.\nAlso the link error you are seeing is that gtest is built against libstdc++ while benchmark is built against libc++. We need to somehow propagate -stdlib=libc++ to gtest.\n. I've encountered this error before and it only triggers when your using the C11 standard. As far as I'm aware c++11 uses C99 and C++14 uses C11. So it should only happen in C++14 and beyond.\n. I did remove c++14 from the CMakelists.txt. did I miss something?\n On Feb 21, 2015 6:02 PM, \"Dominic Hamon\" notifications@github.com wrote:\n\nWhy did you close this? The issue still happens and will until we remove\nc++14.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/issues/74#issuecomment-75398086.\n. Any reason not to put both in there?\n. LGTM. Deleting branch.\n. LGTM. Merged.\n. This LGTM and I'll try and merge it by the end of the day. However CMake is currently segfaulting while configuring benchmark on FreeBSD 10.1 so I want to look into that first.\n. I fixed my CMake segfault issues. I'm happy to adopt your patch but I want to know if you have talked to the freeBSD folks about why sysctl.h doesn't include all the headers it uses? It seems to me this is a FreeBSD bug.\n. Bug filed under: https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=198178\n\nI'll merge the request. Thanks\n. Can we blanket update our copyright on files without actually making a change to that file? \n. Address review comments.\n. Address more comments.\n. Closing this. The patch was broken up and committed in multiple parts.\n. FYI I have a pull request (#81) that adds a -DBENCHMARK_ENABLE_TESTING CMake option. \n. Tests are failing again due to an existing bug. pushing.\n. Merged.\n. LGTM.\n. Sorry, I didn't mean to close this. I just wanted to say I have a possible fix in my patch.\n. LGTM.\n. I don't think we can support per-thread timings. Currently we measure CPU time using getrusage. However getrusage seems to work on a per-process basis and not a per-thread basis.\nUntil we have a way to measure per-thread CPU usage we cannot change this.\n. It might add overhead but nothing that would show up in the benchmark timings. I don't think an extra indirection would add any major overhoad. Also the compiler is free to inline the BenchmarkImp methods into the Benchmark calls. \nUnfortunately I can't use unique_ptr for the same reason I have to pimpl this but it would be really nice if I could.\n. The problem only occurs when CPU scaling is enabled. I'm not what to do about that.\n. I had similar concerns but I noticed that the CPU time and real time usually match almost exactly without CPU scaling. \n. We could switch to a std::chrono based approach.\n. Below are the measurements I got when running some benchmarks with the old walltime implementation and the new chrono implementation.\nCurrent Walltime results\n```\nRun on (12 X 3201 MHz CPUs)\n2015-03-26 16:25:14\nBuild Type: DEBUG\nBenchmark                                              Time(ns)    CPU(ns) Iterations\n\nBM_spin_pause_during/8                                      636        639    1334833                               \nBM_spin_pause_during/8                                      636        636    1334833                               \nBM_spin_pause_during/8                                      637        617    1334833                               \nBM_spin_pause_during/8                                      636        653    1334833                               \nBM_spin_pause_during/8                                      636        614    1334833                               \nBM_spin_pause_during/8                                      636        634    1334833                               \nBM_spin_pause_during/8                                      636        639    1334833                               \nBM_spin_pause_during/8                                      636        648    1334833                               \nBM_spin_pause_during/8                                      636        652    1334833                               \nBM_spin_pause_during/8                                      636        626    1334833                               \nBM_spin_pause_during/8_mean                                 636        636    1334833                               \nBM_spin_pause_during/8_stddev                                 0         13          0                                 \nBM_spin_pause_during/512                                   1709       1649     400708                               \nBM_spin_pause_during/512                                   1709       1750     400708                               \nBM_spin_pause_during/512                                   1710       1637     400708                               \nBM_spin_pause_during/512                                   1709       1699     400708                               \nBM_spin_pause_during/512                                   1708       1749     400708                               \nBM_spin_pause_during/512                                   1709       1723     400708                               \nBM_spin_pause_during/512                                   1709       1716     400708                               \nBM_spin_pause_during/512                                   1709       1641     400708                               \nBM_spin_pause_during/512                                   1710       1678     400708                               \nBM_spin_pause_during/512                                   1709       1716     400708                               \nBM_spin_pause_during/512_mean                              1709       1696     400708                               \nBM_spin_pause_during/512_stddev                               0         40          0                                 \nBM_spin_pause_during/8k                                   17861      17977      39485                               \nBM_spin_pause_during/8k                                   17871      17839      39485                               \nBM_spin_pause_during/8k                                   17875      17696      39485                               \nBM_spin_pause_during/8k                                   17862      17617      39485                               \nBM_spin_pause_during/8k                                   17861      17720      39485                               \nBM_spin_pause_during/8k                                   17862      17854      39485                               \nBM_spin_pause_during/8k                                   17862      17925      39485                               \nBM_spin_pause_during/8k                                   17861      17748      39485                               \nBM_spin_pause_during/8k                                   17862      17169      39485                               \nBM_spin_pause_during/8k                                   17865      17858      39485                               \nBM_spin_pause_during/8k_mean                              17864      17740      39485                               \nBM_spin_pause_during/8k_stddev                                5        217          0                                 \nBM_spin_pause_during/8/threads:12                          2397       3465     213480                               \nBM_spin_pause_during/8/threads:12                          2394       3280     213480                               \nBM_spin_pause_during/8/threads:12                          2371       3285     213480                               \nBM_spin_pause_during/8/threads:12                          2370       3313     213480                               \nBM_spin_pause_during/8/threads:12                          2375       3206     213480                               \nBM_spin_pause_during/8/threads:12                          2361       3317     213480                               \nBM_spin_pause_during/8/threads:12                          2367       3399     213480                               \nBM_spin_pause_during/8/threads:12                          2387       3272     213480                               \nBM_spin_pause_during/8/threads:12                          2386       3291     213480                               \nBM_spin_pause_during/8/threads:12                          2377       3266     213480                               \nBM_spin_pause_during/8/threads:12_mean                     2379       3309     213480                               \nBM_spin_pause_during/8/threads:12_stddev                     11         69          0                                 \nBM_spin_pause_during/512/threads:12                        2546       5682     126900                               \nBM_spin_pause_during/512/threads:12                        2561       6134     126900                               \nBM_spin_pause_during/512/threads:12                        2535       5933     126900                               \nBM_spin_pause_during/512/threads:12                        2540       5811     126900                               \nBM_spin_pause_during/512/threads:12                        2546       5898     126900                               \nBM_spin_pause_during/512/threads:12                        2524       5521     126900                               \nBM_spin_pause_during/512/threads:12                        2517       5809     126900                               \nBM_spin_pause_during/512/threads:12                        2539       5634     126900                               \nBM_spin_pause_during/512/threads:12                        2536       6094     126900                               \nBM_spin_pause_during/512/threads:12                        2536       6149     126900                               \nBM_spin_pause_during/512/threads:12_mean                   2538       5866     126900                               \nBM_spin_pause_during/512/threads:12_stddev                   11        206          0                                 \nBM_spin_pause_during/8k/threads:12                         4653      23858      24516                               \nBM_spin_pause_during/8k/threads:12                         4648      26173      24516                               \nBM_spin_pause_during/8k/threads:12                         4621      27290      24516                               \nBM_spin_pause_during/8k/threads:12                         4655      25130      24516                               \nBM_spin_pause_during/8k/threads:12                         4631      29774      24516                               \nBM_spin_pause_during/8k/threads:12                         4674      25796      24516                               \nBM_spin_pause_during/8k/threads:12                         4643      23914      24516                               \nBM_spin_pause_during/8k/threads:12                         4644      24025      24516                               \nBM_spin_pause_during/8k/threads:12                         4649      27797      24516                               \nBM_spin_pause_during/8k/threads:12                         4678      25874      24516                               \nBM_spin_pause_during/8k/threads:12_mean                    4650      25963      24516                               \nBM_spin_pause_during/8k/threads:12_stddev                    16       1811          0                                 \nBM_pause_during                                             617        638    1311205                               \nBM_pause_during                                             617        629    1311205                               \nBM_pause_during                                             616        611    1311205                               \nBM_pause_during                                             616        666    1311205                               \nBM_pause_during                                             616        620    1311205                               \nBM_pause_during                                             617        647    1311205                               \nBM_pause_during                                             617        655    1311205                               \nBM_pause_during                                             616        611    1311205                               \nBM_pause_during                                             616        634    1311205                               \nBM_pause_during                                             616        637    1311205                               \nBM_pause_during_mean                                        616        635    1311205                               \nBM_pause_during_stddev                                        0         17          0                                 \nBM_pause_during/threads:12                                 2368       3450     167652                               \nBM_pause_during/threads:12                                 2333       3344     167652                               \nBM_pause_during/threads:12                                 2373       3465     167652                               \nBM_pause_during/threads:12                                 2349       3416     167652                               \nBM_pause_during/threads:12                                 2388       3316     167652                               \nBM_pause_during/threads:12                                 2378       3408     167652                               \nBM_pause_during/threads:12                                 2353       3230     167652                               \nBM_pause_during/threads:12                                 2377       3433     167652                               \nBM_pause_during/threads:12                                 2354       3114     167652                               \nBM_pause_during/threads:12                                 2371       3255     167652                               \nBM_pause_during/threads:12_mean                            2365       3343     167652                               \nBM_pause_during/threads:12_stddev                            16        108          0                                 \nBM_pause_during_realtime                                    616        598    1136351                               \nBM_pause_during_realtime                                    616        608    1136351                               \nBM_pause_during_realtime                                    615        622    1136351                               \nBM_pause_during_realtime                                    616        608    1136351                               \nBM_pause_during_realtime                                    616        639    1136351                               \nBM_pause_during_realtime                                    616        651    1136351                               \nBM_pause_during_realtime                                    616        628    1136351                               \nBM_pause_during_realtime                                    616        610    1136351                               \nBM_pause_during_realtime                                    615        674    1136351                               \nBM_pause_during_realtime                                    615        604    1136351                               \nBM_pause_during_realtime_mean                               616        624    1136351                               \nBM_pause_during_realtime_stddev                               0         23          0                                 \nBM_pause_during_realtime/threads:12                        2362       3339     295476                               \nBM_pause_during_realtime/threads:12                        2417       3328     295476                               \nBM_pause_during_realtime/threads:12                        2372       3299     295476                               \nBM_pause_during_realtime/threads:12                        2411       3394     295476                               \nBM_pause_during_realtime/threads:12                        2398       3397     295476                               \nBM_pause_during_realtime/threads:12                        2402       3529     295476                               \nBM_pause_during_realtime/threads:12                        2426       3448     295476                               \nBM_pause_during_realtime/threads:12                        2430       3474     295476                               \nBM_pause_during_realtime/threads:12                        2400       3363     295476                               \nBM_pause_during_realtime/threads:12                        2362       3211     295476                               \nBM_pause_during_realtime/threads:12_mean                   2398       3378     295476                               \nBM_pause_during_realtime/threads:12_stddev                   24         87          0                               \n```\nstd::chrono Walltime results\n```\nRun on (12 X 3201 MHz CPUs)\n2015-03-26 16:28:29\nBuild Type: DEBUG\nBenchmark                                              Time(ns)    CPU(ns) Iterations\n\nBM_spin_pause_during/8                                      664        680     972993                               \nBM_spin_pause_during/8                                      664        689     972993                               \nBM_spin_pause_during/8                                      665        648     972993                               \nBM_spin_pause_during/8                                      665        671     972993                               \nBM_spin_pause_during/8                                      665        640     972993                               \nBM_spin_pause_during/8                                      664        667     972993                               \nBM_spin_pause_during/8                                      664        686     972993                               \nBM_spin_pause_during/8                                      664        681     972993                               \nBM_spin_pause_during/8                                      664        703     972993                               \nBM_spin_pause_during/8                                      664        673     972993                               \nBM_spin_pause_during/8_mean                                 664        674     972993                               \nBM_spin_pause_during/8_stddev                                 0         18          0                                 \nBM_spin_pause_during/512                                   1735       1768     387380                               \nBM_spin_pause_during/512                                   1734       1775     387380                               \nBM_spin_pause_during/512                                   1738       1774     387380                               \nBM_spin_pause_during/512                                   1738       1724     387380                               \nBM_spin_pause_during/512                                   1737       1772     387380                               \nBM_spin_pause_during/512                                   1736       1762     387380                               \nBM_spin_pause_during/512                                   1736       1738     387380                               \nBM_spin_pause_during/512                                   1737       1736     387380                               \nBM_spin_pause_during/512                                   1737       1724     387380                               \nBM_spin_pause_during/512                                   1734       1712     387380                               \nBM_spin_pause_during/512_mean                              1736       1749     387380                               \nBM_spin_pause_during/512_stddev                               1         23          0                                 \nBM_spin_pause_during/8k                                   17811      17324      38532                               \nBM_spin_pause_during/8k                                   17809      17841      38532                               \nBM_spin_pause_during/8k                                   17811      18589      38532                               \nBM_spin_pause_during/8k                                   17830      18105      38532                               \nBM_spin_pause_during/8k                                   17820      18415      38532                               \nBM_spin_pause_during/8k                                   17816      17564      38532                               \nBM_spin_pause_during/8k                                   17815      17885      38532                               \nBM_spin_pause_during/8k                                   17812      18040      38532                               \nBM_spin_pause_during/8k                                   17817      17670      38532                               \nBM_spin_pause_during/8k                                   17813      17974      38532                               \nBM_spin_pause_during/8k_mean                              17815      17941      38532                               \nBM_spin_pause_during/8k_stddev                                6        359          0                                 \nBM_spin_pause_during/8/threads:12                          2367       3323     207684                               \nBM_spin_pause_during/8/threads:12                          2400       3432     207684                               \nBM_spin_pause_during/8/threads:12                          2387       3398     207684                               \nBM_spin_pause_during/8/threads:12                          2372       3309     207684                               \nBM_spin_pause_during/8/threads:12                          2376       3333     207684                               \nBM_spin_pause_during/8/threads:12                          2377       3227     207684                               \nBM_spin_pause_during/8/threads:12                          2402       3272     207684                               \nBM_spin_pause_during/8/threads:12                          2362       3404     207684                               \nBM_spin_pause_during/8/threads:12                          2388       3269     207684                               \nBM_spin_pause_during/8/threads:12                          2412       3305     207684                               \nBM_spin_pause_during/8/threads:12_mean                     2384       3327     207684                               \nBM_spin_pause_during/8/threads:12_stddev                     15         63          0                                 \nBM_spin_pause_during/512/threads:12                        2527       5659     128580                               \nBM_spin_pause_during/512/threads:12                        2550       5735     128580                               \nBM_spin_pause_during/512/threads:12                        2556       5880     128580                               \nBM_spin_pause_during/512/threads:12                        2545       5581     128580                               \nBM_spin_pause_during/512/threads:12                        2576       6269     128580                               \nBM_spin_pause_during/512/threads:12                        2570       5692     128580                               \nBM_spin_pause_during/512/threads:12                        2563       5862     128580                               \nBM_spin_pause_during/512/threads:12                        2590       6325     128580                               \nBM_spin_pause_during/512/threads:12                        2548       5611     128580                               \nBM_spin_pause_during/512/threads:12                        2571       6204     128580                               \nBM_spin_pause_during/512/threads:12_mean                   2560       5882     128580                               \nBM_spin_pause_during/512/threads:12_stddev                   17        269          0                                 \nBM_spin_pause_during/8k/threads:12                         4669      23068      27660                               \nBM_spin_pause_during/8k/threads:12                         4664      23333      27660                               \nBM_spin_pause_during/8k/threads:12                         4641      29158      27660                               \nBM_spin_pause_during/8k/threads:12                         4665      25472      27660                               \nBM_spin_pause_during/8k/threads:12                         4642      25603      27660                               \nBM_spin_pause_during/8k/threads:12                         4640      25169      27660                               \nBM_spin_pause_during/8k/threads:12                         4640      23439      27660                               \nBM_spin_pause_during/8k/threads:12                         4635      26222      27660                               \nBM_spin_pause_during/8k/threads:12                         4646      25363      27660                               \nBM_spin_pause_during/8k/threads:12                         4632      24937      27660                               \nBM_spin_pause_during/8k/threads:12_mean                    4647      25176      27660                               \nBM_spin_pause_during/8k/threads:12_stddev                    13       1678          0                                 \nBM_pause_during                                             639        641    1199554                               \nBM_pause_during                                             639        627    1199554                               \nBM_pause_during                                             638        646    1199554                               \nBM_pause_during                                             639        663    1199554                               \nBM_pause_during                                             639        615    1199554                               \nBM_pause_during                                             639        652    1199554                               \nBM_pause_during                                             639        640    1199554                               \nBM_pause_during                                             639        648    1199554                               \nBM_pause_during                                             639        641    1199554                               \nBM_pause_during                                             640        626    1199554                               \nBM_pause_during_mean                                        639        640    1199554                               \nBM_pause_during_stddev                                        1         13          0                                 \nBM_pause_during/threads:12                                 2397       3347     224160                               \nBM_pause_during/threads:12                                 2423       3311     224160                               \nBM_pause_during/threads:12                                 2379       3514     224160                               \nBM_pause_during/threads:12                                 2398       3285     224160                               \nBM_pause_during/threads:12                                 2410       3425     224160                               \nBM_pause_during/threads:12                                 2397       3169     224160                               \nBM_pause_during/threads:12                                 2395       3336     224160                               \nBM_pause_during/threads:12                                 2366       3295     224160                               \nBM_pause_during/threads:12                                 2383       3353     224160                               \nBM_pause_during/threads:12                                 2404       3436     224160                               \nBM_pause_during/threads:12_mean                            2395       3347     224160                               \nBM_pause_during/threads:12_stddev                            15         90          0                                 \nBM_pause_during_realtime                                    639        658    1096722                               \nBM_pause_during_realtime                                    639        640    1096722                               \nBM_pause_during_realtime                                    639        655    1096722                               \nBM_pause_during_realtime                                    639        608    1096722                               \nBM_pause_during_realtime                                    640        655    1096722                               \nBM_pause_during_realtime                                    640        656    1096722                               \nBM_pause_during_realtime                                    640        650    1096722                               \nBM_pause_during_realtime                                    639        676    1096722                               \nBM_pause_during_realtime                                    639        653    1096722                               \nBM_pause_during_realtime                                    639        635    1096722                               \nBM_pause_during_realtime_mean                               639        649    1096722                               \nBM_pause_during_realtime_stddev                               0         17          0                                 \nBM_pause_during_realtime/threads:12                        2371       3316     294084                               \nBM_pause_during_realtime/threads:12                        2437       3401     294084                               \nBM_pause_during_realtime/threads:12                        2394       3312     294084                               \nBM_pause_during_realtime/threads:12                        2410       3380     294084                               \nBM_pause_during_realtime/threads:12                        2401       3027     294084                               \nBM_pause_during_realtime/threads:12                        2423       3343     294084                               \nBM_pause_during_realtime/threads:12                        2406       3461     294084                               \nBM_pause_during_realtime/threads:12                        2395       3442     294084                               \nBM_pause_during_realtime/threads:12                        2392       3343     294084                               \nBM_pause_during_realtime/threads:12                        2396       3380     294084                               \nBM_pause_during_realtime/threads:12_mean                   2402       3340     294084                               \nBM_pause_during_realtime/threads:12_stddev                   17        115          0                               \n``\n. I benchmarked both implementations of thewalltime::Now()` method and these are the results w/ g++-4.9.\n```\neric@ds2:~/workspace/build-benchmark$ ./test/walltime_test --benchmark_repetitions=3\nRun on (12 X 3201 MHz CPUs)\n2015-03-26 18:36:40\nBuild Type: DEBUG\nBenchmark                 Time(ns)    CPU(ns) Iterations\n\nBM_CPUTimeNow                  125        125    5606234                               \nBM_CPUTimeNow                  125        125    5606234                               \nBM_CPUTimeNow                  125        125    5606234                               \nBM_CPUTimeNow_mean             125        125    5606234                               \nBM_CPUTimeNow_stddev             0          0          0                                 \nBM_ChronoTimeNow               258        258    2718352                               \nBM_ChronoTimeNow               257        257    2718352                               \nBM_ChronoTimeNow               257        257    2718352                               \nBM_ChronoTimeNow_mean          257        257    2718352                               \nBM_ChronoTimeNow_stddev          0          0          0\n```\n. @rryan  I think this is probably a separate bug and I honestly have no idea what's causing it. My best guess is undefined behavior somewhere in the library, and that's a terrible guess. I'll see if I can reproduce this today.\n. I can reproduce on my Mac. Bisecting the issue as we speak. \n. Silly Googlebot. Anyway what is the rational for not making this another class? If we add more formats (ie JSON) it seems like ConsoleReporter could become needlessly complicated to support them all.\n. I don't think we need to make that jump now but I wanted to ask.\n. I don't know a lot about CSV, but I think templated benchmarks might cause problems with the output because the benchmark name may contain commas. Ex BM_myTest.\nCan you comment on this?\n. Travis has passed on the last commit. Merging.\n. This doesn't work as well as I wanted. Closing.\n. I buggered the revision history on this branch. Commited via patch.\n. Both the names and the labels have to be quoted because they can contain commas. \nWe also need to replace every occurrence of \" in the label with \"\".\n. The names can have commas in them if the benchmark function is a template.\nc++\ntemplate <class T, class U>\nvoid BM_foo(benchmark::State& state);\n// name: BM_foo<T, U>\n. It is possible for the name field to include nested quotes as well (in very rare cases). Example:\nc++\ntemplate <char MyChar>\nvoid BM_foo(benchmark::State& state);\nBENCHMARK_TEMPLATE(BM_foo, '\"');\n. LGTM\n. That makes sense to me for the most part. I agree that we shouldn't need to break the current API. \nHowever I don't know if we should use the constructor/destructer for initialization/destruction. Since the fixture classes will likely have a static lifetime. I also don't know how your given example of BM_Fixture would have _count in scope.\n. Hmm, Perhaps I'm not understanding what your fully suggesting. Could you explain further how you see these fixture classes working?\n. @dominichamon I really don't think we should depreciate the \"KeepRunning()\" loop.\nRemembering that the timer only runs over the the contents of the loop, and not anything that comes before or after it. So Fixtures don't really buy us anything in terms of setup/teardown.\nDon't you think that setting up a fixture has more boilerplate than simply using the \"state\" object?\n. I need to update the doc to make that point explicit. However I don't think we should deprecate the KeepRunning() loop. I actually prefer that interface to the fixture one and I think others do as well.\nAlso one nice thing about using \"KeepRunning()\" inside the benchmark instead of repeatedly calling the benchmark function as suggested is that we can force the compiler to inline \"KeepRunning()\" because we own the function. However we can't force the compiler to inline repeated calls into a users benchmark function. (Am I making sense?)\n. For sure. Should we close this issue though or do you still want to consider deprecating that interface.\n. LGTM.\n. I think some of my changes were inadvertent. I was unaware of the CMake option BUILD_SHARED_LIBS.\nI think I have a use-case for building both shared and static version of benchmark in the same build, but I wouldn't want the shared and static libraries being output with the same name. For that reason I don't know how much sense BENCHMARK_ENABLE_STATIC makes.\nFor now I think it makes the most sense to simply remove BENCHMARK_ENABLE_SHARED and respect BUILD_SHARED_LIBS instead.\n. I've already got a patch in the works.\n. So I think we should find a way to upgrade CMake to 2.8.12. I checked the LLVM CMake and mailing lists because this is what they use. Here is the rational they have for choosing that version:\nhttp://lists.cs.uiuc.edu/pipermail/llvmdev/2015-February/081970.html\nhttp://lists.cs.uiuc.edu/pipermail/llvmdev/2015-March/083672.html\nAfter doing this we should be able to use the target_link_directories command as well as specifying INTERFACE libraries.\n. You'll actually need to change CMake to 2.8.11. That was the closest version I could find for travis. Sorry.\n. Oh, I forgot to say... In order for the CMake upgrade to work with travis we will need to merge #117 \n. Pushed without commit history.\n. LGTM, feel free to merge.\n. Closing. This PR seems dead. @michaelbacci feel free to reopen if you want to continue.\n. Fixtures allow you to share some data between different tests.\nValue benchmarks allow you to share different data with common tests.\nSay I want to benchmark std::sort. I need to test std::sort against different inputs, different iterator types and different value types but I don't want to re-write the sort benchmark function for each input.\nWith value benchmarks I can re-use the same benchmark body for each of these tests and just pass in different inputs.\nWith fixtures I have to define a new fixture every time I want different input and I need to re-define the benchmark body for each test.\nDoes that make more sense?\n. I'll come up with better and clearer motivating cases and post them here. I think there is great utility in supporting an interface similar to this that I have yet to demonstrate. Until I manage that I won't push to move this forward.\n\nThe only reason i'm pushing back is to try to keep the API simple and to avoid adding things that are\nC++11 only.\n\nCan you try and expand on what you think is not clean about this approach? I don't disagree but I want to focus in on what can be improved. Also I can implement this in C++03 for a limited function arity with a lot more code but we should have a discussion on if that is worth it. I think it might just be best to restrict these features to C++11.\n. Thank you very much for the code coverage patch. It seems very cool. Sorry for being so negative off the bat. \n. I'm having no luck using your patch to generate coverage. The problem I ran into is that the .gcno and .gcda files are created in /src/CMakeFiles/benchmark.dir/ and /test/CMakeFiles/.dir/ but those specific directories are not passed to lcov. This results in an empty info.lcov file.\nWhat platform are you testing this on?\n. I figured out the problem. Are you building inside the top level of the repository? I build in a directory outside of the repo. For this reason --no-external excludes all coverage data because the actual sources files are not in CMAKE_BINARY_DIR. Adding CMAKE_SOURCE_DIR as a second directory fixed this problem.\n. @mattyclarkson I've created a patch against your coverage branch that gets this working in out of tree builds and fixes some bugs.\nhttps://gist.github.com/EricWF/eaf2dc4ce888cb659275\n. @dominichamon Do you have anywhere you think we should host these results? I'm happy to put them somewhere one one of my domains (efcs.ca)\n. @mattyclarkson Oh, Cool resource. I'll have to let @dominichamon decide if that's what he wants. I don't work at Google anymore so I wouldn't feel comfortable saying yes to using that.\n(however I'm going to try and use coveralls on other projects I work on. Thanks).\n. Other than the requested changes this LGTM. Thanks for all the work! I'll let @dominichamon merge it once he figures out coverall.\n. Seems like a reasonable patch. Is there a reason to perfer using C strings in the function interfaces as opposed to std::string? If there isn't then I would prefer to use std::string.\n. @nickhutchinson Are you interested in continuing with this? \n. Thanks again for this patch!\nPS. I really liked your command line argument checking. If you want to submit that as a separate patch it would be  very welcome.\n. LGTM\n. Why upcast it to int64_t before down casting it to size_t (on 32 bit systems)? Your not fixing the lossy conversion, simply silencing the warning.\nWouldn't it make more sense to just remove the int64_t cast all together?\n. @pleroy The answer isn't as interesting as the question. I think these oddities are leftovers from a big overhaul I made a couple of months ago. I'll try and cleanup the types used to make them consistent.\nI think the number of iterations should be represented using an unsigned type because it's important to support a very large number of iterations and there is no need for that number to be negative.\n. Urg, std::is_trivially_destructible is the cause of a lot of pain. I think we can safely just remove the static_assert in walltime.cc though. I don't think that Walltime::now() is used at all during program termination. \nI'll submit a patch that removes the static_assert.\n. This has already been fixed. Please reopen if you disagree.\n. That's not a typo. iff is commonly used to mean if and only if. If you want to change it to if and only if that's fine but changing it to 'if' is wrong.\n. Sorry I've just been insanely busy with school, work and moving I haven't had a chance to work on this. I'll commit a fix tonight. \n. So I'll commit the change using a dynamic allocation because it clearly expresses intent. However because the type is trivially destructible I don't think it actually makes a semantic difference. Originally I thought avoiding dynamic allocation would prevent WallTimeImp from being wrapped in a static guard variable. However because WallTimeImp's constructor is not constexpr it will always have a static guard around it.\nOn another note I'm not convinced that the termination race condition is a real issue. You would have to call StartTimer from the destructor of a static object O, and where the construction of O was not sequenced before the first call to WallTimeImp. If your writing this kind of code your already screwed.\nTo be even more pedantic our fix still has undefined behavior in the case described above from the point of view of the standard. AFAIK accessing even a trivially destructible type after its destructors ran is UB. \n. Closing this bug.\n1. It's a year old.\n2. It's a duplicate of #147 (albeit older).\n3. int64_t still exists but not within iteration counts.\n. I don't understand why we do this at all. What do we gain over simply writing the version number in the CMake file?\n. This seems like maintenance to me. I would prefer to do the simple thing and just embed the version numbers and tag the release. Then, however they checkout the source code, they will get the right version. \n. I think returning the number of benchmarks run is the best solution. Since @dominichamon doesn't object I'm going to go ahead and implement it.\n. Yeah I see the issues. The simple fix is to make kMaxIterationsCount unsigned. However a max iteration count of 1 billion is plenty most of the time. With the default settings the benchmark would be allowed to run for a total of 0.5 seconds. In order to do 1 billion iterations in 0.5 seconds each iteration would take 1ns.  In the default configuration we wouldn't benefit from allowing more iterations.\nI'm hesitant to start using int64_t in the interface again. API usage starts to get cluttered up with\ncasts when users don't use the same integer types (and they probably don't). These conversions have already caused us tons of pain. I obviously don't want to pass that on to users.\nAnyway, I'm going to defer to @dominichamon. He is much wiser than I am.\n. I was concerned about the performance of 64 bit arithmetic on a 32 bit system. Obviously I decided to benchmark it. I posted the test here https://gist.github.com/EricWF/29868ce86eaa4a03132d.\nIt seems like using a 64 bit iteration count on a 32 bit system can dramatically affect the speed of a benchmark. For this reason I think we shouldn't use uint64_t to represent iterations.\nI think it makes sense to support uint64_t for items and bytes though.\n. I don't think we should put those changes in this PR. This change is still needed to fix some warnings\n. Honestly last time I checked the status of this bug I used grep -R int64_t.\n. Exactly what @pleroy said. The remainder of the changes are tricky, most long hanging fruit is gone.\n. The change to arraysize.h is not OK. The header uses size_t but never includes a header that provides it. The code may compile now but it's a lot more brittle. It's also not reasonable to restrict the use of \"\" within a project to work around a compiler bug.\n@izaid  This bug is caused because clang 3.4.0 does not provide which shipped a version of stddef.h that does not provide the required max_align_t typedef. I think this is fixed in the 3.4.2 release. The correct and only fix to your problem is to upgrade your compiler or at very least the stddef.h header. \n@dominic I think libc++ should work. We explicitly handle this problem in ToT. However it's wrong to force a non-default standard library on users and it might cause issues. If a user wants to compile with libc++ all they have to do is configure with -DCMAKE_CXX_FLAGS=\"-stdlib=libc++\".\n. In this case we can fix arraysize.h by including stddef.h instead of cstddef. I'm fine with trying to fix it here so long as it doesn't harm this code. However your not going to get very far trying to use a broken compiler.\n. I can't reproduce with Clang 3.4.2 nor clang 3.5 nor clang 3.6. Only clang 3.4.2. I did this on Ubuntu. I don't think your correct. I work on LLVM and I use clang on Ubuntu every day. It's super easy to get a working copy.\n. If we choose to perform the cast we should assert that the value is representative is the new type. We don't want to silently truncate.\n@dominichamon Should we change the interface of these functions to take size_t for extra precision and to prevent overflows? \n. This patch isn't needed. You're right that the static cast is incorrect, but it's also entirely unneeded. kNumMicrosPerMilli is also an int so the result type of the multiplication also has the type int.\nThe correct thing is to remove the cast entirely, which has already been done.. This has already been fixed.\n. LGTM. Thanks.\n. I think #259 would allow you to easily right your own  template range registration function.\nFor example:\n``` c++\ntemplate  void BM_template(benchmark::State&) { / ... / }\nvoid swallow(...);\ntemplate \nvoid BenchmarkTemplateRange() {\n  swallow(benchmark::RegisterBenchmark(NAME(Types), &BM_template)...);\n}\n``\n. @disconnect3d I'm closing this because I think the solution I proposed allows you to do this already. Please reopen if you disagree.\n. @jll63 Do you still want to continue with this? If the CLA is still a problem I would be happy to take over this change myself.\n. I'm not sure I agree with this change at all. For example the default CMake installation root is/usr/local. On my Linux machines/usr/local/lib64is empty,/usr/local/libis used for 64 bit libraries and/usr/local/lib32` is used for 32 bit libraries.. @jll63 You need to fix the CLA issue before we can move forward with this. Maybe try opening a new PR to see if that helps?. Closing this PR due to unfixed CLA issues. \n@jll63 Please re-open a new PR once you can work out the CLA issues, If you're having problems please let myself or @dominichamon know, and we'll be happy to help you through it.\nHowever for now I'm going to close this PR, since it hasn't been making progress.. I haven't looked into this in depth, but you can do the initialization you need before entering the \"keepRunning\" loop.\nIMO If you need to pause a benchmark in the middle you should try to redesign your benchmark.\n. To clarify the timer isn't actually running until the first iteration of the \"keepRunning()\" loop. So, like fixtures, your free to initialize whatever you need before starting the loop.\n. Precisely. In that case I would initialize a vector of values using rand() % size outside of loop. Unfortunately It's not always optimal or possible to do this.\n. Glad to hear :-) \nI think the startRunning()/stopRunning() discussion is misguided. We could try to make them faster, but they are really meant as a last resort. The quality of a micro benchmark will always be greatly affected by executing any non-benchmarked code while timing.\nUnfortunately startRunning() and stopRunning() are particularly expensive operations. I don't think we can do anything about that.\n. Could you explain how to do that?\n. Patches welcome :-). LGTM. Libc++ FTW :-P\n. This should work, as of April 3rd, using find_package(benchmark) after adding the installation prefix to CMAKE_PREFIX_PATH (if it's not something like /usr/local).\nPlease re-open this if the problem still persists.. No? I nanosecond is a pretty tiny amount of time. If you have a function that benchmarks at 2 nanoseconds it's likely the compiler optimized away your benchmark all together. \n2 nanoseconds is like enough time to run 6 CPU instructions at most if I'm not mistaken.\n. I did some tests and I'm 100% sure your benchmark is getting optimized away.\nI tested an empty benchmark and a benchmark that only contained volatile int x = 42. The benchmarks ran in 1ns and 3ns respectively. \n. @izaid I'm going to go ahead and close this issue based on what I think is happening. Do not hesitate to re-open it if you disagree. Sometimes I'm spectacularly wrong :-)\n. Any particular reason for bazel? Alternatively we have a pretty simple build process and we could simply provide a buildit script or makefile.\n. Closing this issue for lack of activity. If the documentation is unclear or could be improved please reopen.\n. @WilliamTambellini: @jknuuttila's changes to add UseManualTime have landed. It should help you address your problem.\nPlease re-open if it does not.. @BillyONeal Should the CMake be linking shlwapi.lib instead?\n. Thanks Billy. I don't work on Windows so I'm trying to figure out if this is our bug, or just a general quirk of Windows compilation (like -pthread or -lm on Unix). If we ever get a CMake configuration module we should ensure shlwapi.lib is listed as an interface link library.\nI think the best course of action is to prominently document the dependency as a known issue and close this bug.\n. @BillyONeal Patches correctly implementing #pragma comment(lib, \"shlwapi.lib\") are welcome. I would do it myself but I'm unfamiliar with Windows.\n. Any preference on how we should enforce these? My suggestion would be printf and abort.\n. Right, it's been a long time since I worked on the code :-(. I'll put a patch together.\n. I don't think AbortBenchmark(...) could actually \"abort\" cause the benchmark to leave scope (without throwing an exception). The user would still have to manually return. I imagine an interface like:\n``` c++\nvoid BM_foo(benchmark::State& state) {\n  while (state.KeepRunning()) {\n     if (error_condition) {\n        state.SetError(\"foo error\");\n       return; // This is required\n    } else {\n       return; // Return without SetError would be an error, and would have to be diagnosed.\n    }\n  }\n}\nSetError(...) could also ensure that KeepRunning() returns false for the next iteration, but as mentioned that's not the same as directly leaving scope.\nI'm not sure how to handle multi-threaded benchmarks though :-(. If one thread reports an error the rest of the threads would still continue as if nothing happened. Is this behavior acceptable?\n``\n. SoIgnoreIteration()would not run any iterations? I assumed it wanted to throw away a single iteration of theKeepRunning()` loop, which is simply not possible to implement.\n. I submitted pull request #217 which adds a \"State::SkipWithError(const char*)\" which will skip the benchmark and report it as an error.\n. pull request #217 has been merged. Closing this issue.\n. Urg. so the \"rm\" used to mean put this value in a register if possible, otherwise put it in memory. I think there's another way to say this that will avoid the conflicting constraints error.\n. I cannot reproduce the error your seeing with GCC. (See https://goo.gl/krlgJM).\nCould you please try changing \"+rm\" to \"+g\" and tell me if that works?\n. OK so I managed to reproduce here: https://goo.gl/CJqGjJ. Unfortunately the change to \"+g\" doesn't work. Versions of GCC prior to v5 ICE on this code. Changing \"+rm\" to \"+g\" still emits the same error. I'll file a GCC bug since \"+g\" shouldn't have any conflicting constraints since its obviously a single constraint.\n. GCC Bug Filed: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=71246\n. I think this has been fixed in master. Please reopen if I'm incorrect.\n. Just using \"+m\" massively changes the codegen, which is not what we want for micro benchmarks.\nI outlined the differences is codegen here: https://gist.github.com/EricWF/90e14dacab3b28d1b811#donotoptimize\nAlso template <class Tp*> should be template <class Tp>. \n. @zabereer So I've looked into this more. I think we should go ahead with your change, or a change similar to it. I'm happy to do it myself but I wanted to ask you first.\n. Yeah. Also you don't need to provide the additional overload for pointers. When you make the change could you please merge the clang and GCC implementations since this will make them the same.\n. > is this ready to merge or were you going to add the 'on iteration x' to the error output?\nI'm going to look into improving the diagnostic but that could be done as a followup. Other than this is ready to land. Note that pull request #216 should land before this.\n. I seem to have buggered up the console reporter as well. Working to fix it.\n. Is this OK to land?\n. Huh, no Idea how I managed to break the json reporter.\n. OK, so that specific test provides a custom reporter which the --benchark_format option does not override. It is not the only test to do this. I think it behaves as intended.\n. Super! Thanks. LGTM. Do you need me to commit this?\n. Now that I think about it... In order to prevent regression would you be able to write a new tests that checks 'DoNotOptimize(...)'. Said test doesn't even have to contain any benchmarks. A main function in a new test file that simply calls 'DoNotOptimize(...)' with a variety of types would be great. Let me know if you need any help with the CMake magic.\n. LGTM. Fix the merge conflicts and I'll merge the branch! Thanks again.\n. This has been committed from another PR.\n. One example of where this functionality is useful is testing container templates. For example say I want to test the performance of emplace(...Args) on the Unordered containers in the STL. Writing a good benchmark is hard and I only want to do it once. Therefore I need to write this benchmark in such a way that it works with:\n1. 4 different container types. unordered_map, unordered_set, unordered_multimap andunordered_multiset. \n2. 2 different value_type's between the 4 containers. std::pair<Key, Value> for maps and Value for sets.\n3. Multiple key types with different hashing behaviors and costs.\n4. Pathological sets of keys in an attempt to trigger hash collisions.\n5. Multiple different types of arguments to emplace. In particular ones that don't match the containers value_type. For example std::pair<const Key, Value> and std::pair<Key, Value>.\nThe biggest issue is that inputs to the benchmark need to be constructed within the benchmark itself. This essentially requires writing a different benchmark whenever you want to test with sufficiently different inputs (be it types or values).\nFixtures don't solve this problem. They allow different benchmarks to share the same the same SetUp() code but they don't allow the same benchmark to use different SetUp functions.\nThe solution to these problems is allowing the inputs to the benchmark to be passed in as a function argument. The benchmarks I'm hoping to write look like this:\nc++\ntemplate <class Container, class Inputs>\nvoid BM_emplace_test(State& state, Container C, Inputs const& inputs) {\n  auto It = Inputs.begin();\n  const auto ItEnd = Inputs.end();\n  while (state.KeepRunning()) {\n      DoNotOptimize(C.emplace(*It++));\n     if (It == ItEnd) It = Inputs.begin();\n  } \n}\nBENCHMARK_CAPTURE(BM_emplace_test, unordered_set_of_strings, \n    std::unordered_set<std::string>{}, std::vector<std::string>{/* inputs */});\nBENCHMARK_CAPTURE(BM_emplace_test, unordered_set_of_strings_with_collisions, \n    std::unordered_set<std::string>{}, std::vector<std::string>{/* inputs */});\nBENCHMARK_CAPTURE(BM_emplace_test, unordered_map_of_ints, \n   std::unordered_map<int, int>{}, std::vector<std::pair<int, int>>{/* inputs */});\nGoing one step further this change makes it possible to write incredibly generic benchmarks by passing in a lambda to perform the operation under test. For example:\nc++\ntemplate <class Container, class Inputs>\nvoid BM_container_op(State& state, Container C, Inputs const& inputs, Functor F) {\n  auto It = Inputs.begin();\n  const auto ItEnd = Inputs.end();\n  while (state.KeepRunning()) {\n     F(C, *It++);\n     if (It == ItEnd) It = Inputs.begin();\n  } \n}\nBENCHMARK_CAPTURE(BM_container_op, insert_a_set_of_strings, \n    std::unordered_set<std::string>{}, std::vector<std::string>{/* inputs */},\n   [](auto& C, auto& Input) { DoNotOptimize(C.insert(Input)) });\nBENCHMARK_CAPTURE(BM_container_op, emplace_a_set_of_strings, \n    std::unordered_set<std::string>{}, std::vector<std::string>{/* inputs */},\n   [](auto& C, auto& Input) { DoNotOptimize(C.emplace(Input)) });\nUnlike calls through opaque function pointers the lambdas call operator will always be inlinable. within the benchmark. This would allow benchmarks to be written with almost no boilerplate. We have previously discussed deprecating the KeepRunning() loop and this seems like one possible way to do it.\n. PS. I'll want to squash the commit history on this patch before merging. It's way too noisy.\n. It has now been rebased and I don't see any error reporting changes.\n. @ismaelJimenez I would also like to consider removing the \"ReportComplexty\" interface all together. Instead the complexity reports would be precomputed and passed to \"ReportRuns\" as appropriate. Do you think this would work for your case?\n. Sounds good! I think your initial implementation made the most sense given the current interface.\nPull Request #226 is the changes I'm suggesting.\n. This has been fixed.\n. @ismaelJimenez I moved ComputeStats to complexity.cc as suggested. It seems like a better place than inside benchmark.cc.\n. Sorry I just saw your comments today. I'm OK with all of this.\n. @googlebot I'm Okay with these commits. How do I convince you?\n. @googlebot I signed it.\n. I'm sorry I should have read that more carefully. It should be fixed now. Let me know if you have any other problems.\n. This has been addressed.\n. > we have a few global non-PODs in tests. these should be removed.\nI believe your referencing the class-type globals in benchmark.cc like timer_manager but that's the least of our worries. There is a far worse static initialization order fiasco (SIOF) in our public API.\nThe real issue here is\"dynamic initialization\" as opposed to \"static initialization\", which is determined more by the initializer than the type it's initializing. Static initialization is ironically ordered and entirely safe, the opposite of dynamic initialization.\nFor example:\nc++\nstruct PODType {\n  int value;\n};\nint foo();\nPODType p1 = 42; // OK. Has a constant initializer.\nPODType p2 = foo(); // Unsafe. The initializer is dynamic\nint bar() {\n  extern int i;\n  static int local = i;\n  return local;\n}\nint y = bar(); // Unsafe. 'local' is initialized but its initializer, 'i', may not have been..\nUnfortunately the entire library API is built around dynamic initialization. The BENCHMARK registration macros work by executing arbitrary code in the  initializers of trivially constructible global objects. Actually addressing this issue means deprecating the BENCHMARK interface in favor of a new runtime interface like the RegisterBenchmark function.\nHowever I'm not convinced this is worth while in our case. The global registration facilities are super convenient and easy to use. Although such techniques would be ill-advised in large production codebases that's not what we provide.. SIOF issues between the user and the library can be avoided with a couple of cleverly placed initializers Unfortunately the API can still cause the SIOF outside of the library because things like fixture constructors execute arbitrary user code during program startup (as @rtzcoder mentioned).\nIn conclusion I think we should begin to implement and promote the new safe API which is not designed for use during program startup.  The intent is not to replace the old registration macros but to provide an alternative interface without builtin SIOF bugs.\n. I'll take a look at this tomorrow.\n. Closing this since another approach has already been committed.\n. @dominichamon I rewrote the Appveyor config and this warning is now gone. I'm closing this pull request for that reason. Sorry to step on your toes.\n. Why don't you just clear the map yourself in the loop?\n. @tahsinH Would you be willing to add such documentation, since you've experienced the issue personally.\n. Closing this issue since there is a PR tracking it.\n. DoNotOptimize(...) can only help prevent the optimization of the result, and not the intermediate expressions.\n. I'll look into this more over the weekend.\n. Essentially yes that is correct, except the optimizations can happen even without constexpr.\nDoNotOptimize(<expr>) works by forcing the result of <expr> to be stored to memory, which in turn forces the compiler to actually evaluate <expr>. It does not prevent the compiler from optimizing the evaluation of <expr> but it does prevent the expression from being discarded completely.\nAs you noted in your example the compiler optimized <expr> so that it only had to be evaluated once and therefore could reuse the result each loop iteration. Unfortunately you just have to be aware of these Gotcha's when writing benchmarks.\nIn my experience it's important to give the benchmark different inputs on every iteration to prevent this kind of optimization from taking place.\n. I checked in a slightly improved version of DoNotOptimize(...) and additional docs that try and clarify how to use it. Including a description of the problem your running into. I'm going to close this because I don't think I can do much better than that.\nThanks for the report. It's greatly appreciated.\n. Closing since this has been fixed already.\n. LGTM. Please resolve merge conflicts.\n. Closing due to inactivity. I committed similar documentation in 9c261681266d051c0b04ddbe2064b569c8b19528.\n. Here's a very simple script that runs and then compares two different benchmarks. This is what I would like to work towards.\n``` python\n!/usr/bin/env python\nimport os\nimport subprocess\nimport sys\nimport tempfile\nimport json\nclass bcolors:\n    MAGENTA = '\\033[95m'\n    CYAN = '\\033[96m'\n    OKBLUE = '\\033[94m'\n    HEADER = '\\033[92m'\n    WARNING = '\\033[93m'\n    WHITE = '\\033[97m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\ndef calculate_change(old_val, new_val):\n    return float(new_val - old_val) / abs(old_val)\ndef run_bench(fname, flags):\n    thandle, tname = tempfile.mkstemp()\n    os.close(thandle)\n    cmd = [fname] + flags\n    print(\"RUNNING: %s\" % ' '.join(cmd))\n    exitCode = subprocess.call(cmd + ['--benchmark_output=%s' % tname])\n    if exitCode != 0:\n        print('TEST FAILED...')\n        sys.exit(exitCode)\n    with open(tname, 'r') as f:\n        return json.load(f)\ndef report_results(json1, json2):\n    longest_name = 1\n    for bc in json1['benchmarks']:\n        if len(bc['name']) > longest_name:\n            longest_name = len(bc['name'])\ndef find_test(name):\n    for b in json2['benchmarks']:\n        if b['name'] == name:\n            return b\n    return None\nfor bn in json1['benchmarks']:\n    other_bench = find_test(bn['name'])\n    if not other_bench:\n        continue\n\n    def get_color(res):\n        if res > 0.05:\n            return bcolors.FAIL\n        elif res > -0.07:\n            return bcolors.WHITE\n        else:\n            return bcolors.CYAN\n\n    fmt_str = \"{}{:<{}s}{endc}    {}{:+.2f}{endc}     {}{:+.2f}{endc}\"\n    tres = calculate_change(bn['real_time'], other_bench['real_time'])\n    cpures = calculate_change(bn['cpu_time'], other_bench['cpu_time'])\n    print fmt_str.format(\n        bcolors.HEADER, bn['name'], longest_name + 3,\n        get_color(tres), tres, get_color(cpures), cpures,\n        endc=bcolors.ENDC)\n\ndef main():\n    # Parse the command line flags\n    def usage():\n        print('compare.py   [benchmark options]...')\n        exit(1)\n    if '--help' in sys.argv or len(sys.argv) < 3:\n        usage()\n    tests = sys.argv[1:3]\n    bench_opts = sys.argv[3:]\n    for t in tests:\n        if not os.path.isfile(t):\n            print(\"invalid benchmare file: '%s'\" % t)\n            sys.exit(1)\n    bench_opts = list(bench_opts)\n    color_print = True\n    for opt in bench_opts:\n        if opt.startswith('--color_print=0') or \\\n                opt.startswith('--color_print=f') or \\\n                opt.startswith('--color_print=F'):\n            color_print = False\n    # Run the benchmarks and report the results\n    json1 = run_bench(tests[0], bench_opts)\n    json2 = run_bench(tests[1], bench_opts)\n    report_results(json1, json2)\nif name == 'main':\n    main()\n```\n. I opened PR #266 with a similar script.\n. I honestly have no clue what's causing AppVeyorBot to fail :-S\n. @dominichamon Do you have an issue with the coverage drop here? I don't want to write tests that create output files since it's messy to clean them up.\n. @googlebot confirmed.\n. Sounds good. I'm going to commit directly to involve getting the history.\n. This patch expects fully conformant C++11 compilers, which excludes most of our bots unfortunately. However it's been half a decade since C++11 and I think we should start using it.\n. > no matter how long it has been, we have users who do not have C++11 support. dropping them would be unfortunate.\nI agree. I use the public API in C++03 within libc++. I don't want to drop support for that. Note that we already require C++11 to build/test the library, so we've already \"dropped\" C++03 support in that regard. Users who do not have \"C++11 support\" when benchmarking their project often do not have it because their project doesn't support C++11.  The same users can often find a pre-compiled version of the library or can use a newer compiler to build the library.\n\nhaving said that, if we state that as of version X we are only supporting C++11, that just leaves them on an old (working) version. What do you think?\n\nI don't think we should kill C++03 compatibility in the headers and instead we should maintain the core API we have now so that it stays C++03 compatible. Therefore all old C++03 tests will keep working and they can continue upgrading to newer versions of the library. However I also want to allow new API additions to use C++11.  So I think the rule should be \"Our core API is C++03 compatible, but additional features may use C++11 where appropriate so long as they don't conflict with the core API.\"\nHowever I think we should be more aggressive in the compilers we require to build/test the library.\nIf we want to allow C++11 features we need to test against quality C++11 compilers, otherwise the tests will fail. Currently Jenkins uses GCC 4.6, and this is NOT a C++11 compiler. (GCC 4.7 isn't much better)\nMy suggestion is that we upgrade Jenkins to GCC 4.9. This way we can test new C++11 features without running into compiler bugs. However it would be nice if the library continued to build with older GCC versions even if they can't run all of the tests.\n. I just pushed a merge commit. Hopefully that will get things going.\n. Ah nevermind...\n. Urg... The patch looks like it works but I think \"complexity_test.cc\" is flaky. \n. > The solutions could be:\n\n\nRemove const from the State argument to the SetUp() and TearDown() of Fixture. I chose not to do this because it was more disruptive to how the public API was. But if this was my own project, I would have gone for this.\n\n\nLet's do this,  but we don't have to break the API right away. Instead modify Fixture as follows:\nc++\nclass Fixture : public internal::Benchmark {\n/* [...] */\npublic:\n  // Already exist. We should deprecate them soon.\n  virtual void SetUp(const State&) {}\n  virtual void TearDown(const State&) {}\n  // Add these!\n  virtual void SetUp(State& st) { SetUp(static_cast<State const&>(st)); }\n  virtual void TearDown(State& st) { TearDown(static_cast<State const&>(st)); }\n};\nThe library will call the new non-const functions which can be overridden by the user, otherwise the default implementation will forward to the old const API.\n. I think this patch would be a lot smaller if we used the standard library more freely in the public API, especially std::string.I would like to see this patch update so that it uses the C++03 STL as needed within the public API.\nBenchmarkCounters should be replaced with std::map<std::string, Counter> or even std::map<std::string, double> if we can get rid of the Counter class all together. The current BenchmarkCounters implementation is doing all kinds of sketchy memory management. This would get rid of that entirely, and it would provided an standard container interface which seems to be what we want.\nAdditionally the State class should just expose the counters_ (renamed counters) object directly as a public member instead of the SetCounter/GetCounter/Counters interface. \nInline comments regarding Counter coming soon...\n. Addendum (On general policy, not necessarily this patch):\nAlthough STL usage should be encouraged were appropriate within the public API,\nit must not be used to implement features called within the KeepRunning() loop since things\nlike STL containers/std::string are going to introduce jitter and differ between platforms.\n. FYI I plan to look at this tonight.\n. > Your benchmark is a bit of a red-herring: anyone wanting to update the counters in the tight loop would be well-advised to grab an iterator to the counter before the loop and use it in the loop. A std::map would work just fine for that purpose.\nExactly right. Instead of size_t ID = counters.Insert(\"foo\", value) you would just write iterator IT = counters.emplace(\"foo\", value).first.Admittedly it would be faster to use unordered_map but that requires C++11 and benchmark::State is limited to C++03 only.\n\nI don't quite buy the argument that this class has to be super-efficient \n\nI violently agree. The counters container shouldn't be used in the KeepRunning() loop, since that loop should contain only the code under test. Trying to optimize the performance of the container is akin to optimizing for poorly written benchmarks because a benchmark that modifies counters within the loop is a bad benchmark.\n\nAs it stands this PR increases considerably the surface of the API. While I think that user-defined counters are a great idea, there has to be a way to add them with a much more compact API, and without an implementation that does its own management of storage and containers.\n\nAgain I violently agree! I'm opposed to accepting this patch with BenchmarkCounters and anything that resembles hand-rolled containers or manual memory management. \n. @biojppm @pleroy I implemented a different version of this as PR #276 . Please take a look at the updated interface and tell me what you think.\nI don't mean to hijack @biojppm pull request, I'm simply created PR #276  in order to show an alternative interface.\n. Alright this is looking a lot better. Thanks for all the hard work. I have a couple of comments on the new ouput format though.\nI would prefer that additional counters simply get printed at the end of the test line, instead of printing a new header for every test. Parsing \"= \" should be as easy as parsing \"\\n----------------------\\n`. \n. Also see closed issue #240.\n. > Do you mean you want the counter names out of the header? Ie, printed like bytes processed?\nYes. I was thinking consistency with \"bytes processed\" would be preferred. In particular because \"bytes processed\" should really just become a \"custom counter\" in the future. \n\nI did do this in the first implementation, but the counters become unaligned when their set changes and the output is confusing to read (eg, where do the names go? printed on every counter value? at the top they can't go because the set may be different).\n\nI was thinking the name, unit and value for each counter would be printed for every counter value. The output would look something link:\n```\ncounters[\"foo\"] = 42;\ncounters[\"bar\"] = {100, benchmark::Counter::kIsRate};\nBM_Foo 4 4 100 foo=42 bar=100/ns\n```\nArguably your format is nicer when you have a number of tests all with the same custom counters, since the header will only be printed once.\nHowever when you have (A) one or two counters or (B) different counters for every test then it starts to get noisy. In the case of (B) each benchmark will be 4 lines of output instead of one.\nI'm not opposed to your output plan either. I think we should consider both.\n@dominichamon @pleroy Do you have any preference between the two output formats? (see below)\n```\nRun on (4 X 4227.95 MHz CPU s)\n2016-09-01 18:53:44\n\nBenchmark                    Time           CPU Iterations           Bar           Bat           Baz           Foo\nBM_UserCounter               2 ns          2 ns  307017544             2             5             3         1024m\nBenchmark                    Time           CPU Iterations          ABar          AFoo           Bat\nBM_UserCounter2/0/1          2 ns          2 ns  296610169             2         1024m             5\nBenchmark                    Time           CPU Iterations\nBM_NoCounters                2 ns          2 ns  291666667\nBenchmark                    Time           CPU Iterations          ABar          AFoo           Bat           Baz\nBM_UserCounter2/1/2          2 ns          2 ns  307017544             2         1024m             5             3\n```\n```\nRun on (4 X 4227.95 MHz CPU s)\n2016-09-01 18:53:44\nBenchmark                    Time           CPU Iterations          Additional Counters\n\nBM_UserCounter               2 ns          2 ns  307017544 Bar=2 Bat=5 Baz=3 Foo=1024m\nBM_UserCounter2/0/1          2 ns          2 ns  296610169 ABar=2 AFoo=1024m Bat=5\nBM_NoCounters                2 ns          2 ns  291666667\nBM_UserCounter2/1/2          2 ns          2 ns  307017544 ABar=2 AFoo=1024m Bat=5 Baz=3\n``\n. @biojppm OK I think it's been decided that the single-line output format is prefered. I'm just waiting on those changes before finishing this review.\n. @biojppm I just created some merge conflicts for you (sorry). They are pretty tame but let me know if you need help.\n. @biojppm I'm happy to merge this once all of the tests pass in debug mode, even if theCSV` stuff isn't fully correct yet.\n@biojppm Other than that this LGTM. Please ping me once you've updated this patch so we can merge it.. @biojppm I can't reproduce the failures I was seeing either. So I must have done something wrong on my end. \n@biojppm This LGTM. Thanks for all your time working on this!\n@dominichamon Any last words or can I merge this?. In the trunk sysinfo.cc:256 is does not look like that. Are you sure your building the most recent version of the library?\n. Woops, sorry it was me who is behind trunk.\n. It should be fixed now. Please confirm and close.\n. So you register the benchmark twice:\n``` c++\nBENCHMARK(BM_memcpy)->Arg(8)->Arg(64)->Arg(512)->Arg(1<<10)->Arg(8<<10);\n// Register the function as a benchmark\nBENCHMARK(BM_memcpy);\n```\nThe second time you register the benchmark you do so without an state.x_range() arg, so it asserts when you call the function.\nI would close this issue as NAD.\n. Closing as NAD. Please feel free to ask for any further clarification.\n. Correct, This patch upgrades travis to use Ubuntu Trusty, and Clang 3.5 is preinstalled on those machines.\n. > i'm not sure about a script deciding to run executables to get the output. it seems to me that one could do that directly from a shell and this script is much simpler when it can ignore being passed executable files.\nIt's absolutely possible, but it's a lot more typing for the user. For example a common invocation I use is:\n./compare_bench.py a.out b.out --benchmark_repetitions=5 --benchmark_min_time=2.0\nThe equivalent invocation without supporting executables is:\n./a.out --benchmark_repetitions=5 --benchmark_min_time=2.0 --benchmark_out=/tmp/a.json && ./b.out --benchmark_repetitions=5 --benchmark_min_time=2.0 --benchmark_out=/tmp/b.json && ./compare_bench.py /tmp/a.json /tmp/b.json && rm /tmp/a.json /tmp/b.json\nWhich is a bunch longer and makes manually invoking the script a bit of a pain. The unix-y thing to do would be to pipe the results into the script, but since we have two sets of results and only one stdin we can't do this.\n. >  this script is much simpler when it can ignore being passed executable files.\nYes, most of the code in this PR is silly utility code to determine the input type, but I don't intend for this to be the only script. My basic design idea was to create a python module containing everything you need to easily write new/custom tooling. For this reason I think providing utility functions to determine/diagnose/load input files is helpful.\n. > Every cell in my unix body is screaming but it is more useful in its current form.\nBetter command line invocations are welcome, but this is a starting point :-)\n. Yes it can be \"--benchmark_aggregates_only\". Additionally I'll change ReportRepetitions to ReportAggregates\n. The new names are:\n- --benchmark_report_aggregates_only\n- ->ReportAggregatesOnly(bool = true)\n. > appveyor now failing because http://stackoverflow.com/questions/14191566/c-mutex-in-namespace-std-does-not-name-a-type\nApparently only MinGW-64 provides POSIX threading. I'm juts going to remove the MinGW32 builder. I think one bot is enough coverage.\n. Would modifying the SetLabel(...) feature so that it has a global default label work for you?\nIt doesn't change the name but it could act as a unique identifier to distinguish between runs.\n. Cool. I'm going to close this issue then. If you create some cool tools upstream patches are welcome :-D\n. I would love to know what convenience this offers over BENCHMARK_MAIN()\nbecause it's not immediately apparent to me.\n. Abandoning since this was only ever meant as an example.\n. > thank you so much for bothering with this, even though it's a pain.\nNo worries. I kinda need it :-P\n\nthere is another alternative: should we replace cmake with bazel or something else?\n\nI don't think this issues is motivation enough to switch from CMake. Hopefully the situation for using libc++ will improve over time.\n. The easiest way would be to implement a custom reporter (see reporters.h). The custom reporter would receive all of the run data and it could use that to check the expected complexity.\n``` c++\nclass TestReporter : public benchmark::ConsoleReporter {\npublic:\n using Base = benchmark::ConsoleReporter;\n using Base::Base;\nvoid ReportRuns(const std::vector& report) {\n    for (auto& R : report) assert(CheckRun(R));\n    Base::ReportRuns(report);\n }\nprivate:\n   bool CheckRun(Run const& R) const; // implement checking here.\n};\n// Example main with a custom reporter\nint main(int argc, char** argv) {\n  benchmark::Initialize(&argc, argv);\n  TestReporter rep;\n  benchmark::RunSpecifiedBenchmarks(&rep);\n}\n```\n. > Should I create a pull request?\nIf you have an idea and a patch you should always make a PR. Issues are for people without patches :-P\n\nfor my work I want to use a range of threads\n\nYou must have a lot of threads... I'm initially skeptical of the need for this feature. Typically you don't want to exceed the number of threads supported by the hardware (at least by much). Please try and add a motivating example when you open the PR.\nI'm closing this issue to make way for the PR.\n. > I will probably need a few more things in the future like thread pinning and more accurate version of PauseTiming so I can make sure they leave at an exact time.\nFYI PR #286 makes PauseTiming() per thread instead of per-process so it doesn't perform synchronization and block the threads. \n. Alright so there are a couple things left to do.\n1. The DenseThreadRange(...) declaration in benchmark_api.h needs to be documented like the other functions around it. Take a look at DenseRange for an example.\n2. This needs a test. Somethink like this should work:\n``` c++\nstatic void BM_ThreadRanges(benchmark::State& st) {\n  switch (st.range(0)) {\n  case 1:\n    assert(st.threads == 1 || st.threads == 2 || st.threads == 3);\n    break;\n  case 2:\n    assert(st.threads == 1 || st.threads == 3);\n    break;\n  case 3:\n    assert(st.threads == 5 || st.threads == 8 || st.threads == 11 || st.threads == 14);\n    break;\n  default:\n    assert(false && \"Invalid test case number\");\n  }\n  while (st.KeepRunning()) {}\n}\nBENCHMARK(BM_ThreadRanges)->Arg(1)->DenseThreadRange(1, 3);\nBENCHMARK(BM_ThreadRanges)->Arg(2)->DenseThreadRange(1, 3, 2);\nBENCHMARK(BM_ThreadRanges)->Arg(3)->DenseThreadRange(5, 14, 3);\n```\n. There is a config file now. It was just checked in today.\nTo format patches use clang-format-diff\n. Stick the test wherever makes sense and use --style=Google if the config doesn't work. \n. Looks good to me! Thanks for the patch.\n. To discuss some of your other comments:\n\nmore accurate version of PauseTiming so I can make sure they leave at an exact time.\n\nPauseTiming now act's a lot differently. In particular it's thread-local and doesn't act as a barrier so hopefully this solves your problem. Also SetIterationTime is also per-thread now. The final value is the average of each of the threads values. \nI'm looking into \"thread pinning\" as a feature, but I'm not sure what the correct way to pin the threads is.\nCould you clarify what you need here? \n. > Last Saturday I had big troubles getting my code to work in the benchmark harness, again.\n\nI liked having the barrier there.\n\nSorry but the barrier in PauseTiming() essentially made multithreaded benchmarks synchronous , which is obviously not what we want. Removing the barrier drastically improved the timing accuracy of multithreaded benchmarks.\nI plan to add barrier functionality to State so that users can block when need be, but I don't think PauseTiming() should be it.\n\nI am not sure what was going on.\nIt would help, if I could acquire shared resources before spawning any threads and pass them.\nIf all threads start the benchmark at the same time, I have to make sure everyone has the correct data.\nStarting the benchmarks via my own main seems to not help with that, since I can only start all benchmarks at the same time, and not individually (I can only acquire all resources before running any benchmark, and release them afterwards).\n\nThat shouldn't be a problem with the existing API. See the example below.\n\nCan threads overtake each other in KeepRunning() or is there a barrier between iterations?\n\nKeepRunning() only acts as a barrier on the first and last iteration. This ensures that the initialization code has been run before any thread enters the loop and that the teardown code only runs after all threads are done. You should be able to setup/teardown shared state between multiple threads like this:\nc++\nvoid SetupGlobalState();\nvoid TeardownGlobalState();\nstd::vector<int> global_state;\nstatic BM_Foo(benchmark::State& st) {\n  if (st.thread_index == 0) { // A single thread executes this setup.\n    SetupGlobalState();\n  }\n  PinCurrentThread(); // All thread execute this\n  while (st.KeepRunning()) { // No thread starts the loop until all threads are here.\n     /* ... */\n  } // No thread passes here until all threads are complete.\n  if (st.thread_index == 0) {\n    TeardownGlobalState();\n  }\n}\n\nAs for SetIterationTime, I would also like to have the maximum of all threads and the time from the earliest before to the latest after (kind of time maximum time collectively spent in the code).\n\nYeah I think SetIterationTime() could offer a bunch more functionality. I'm looking into better support for custom timers.\n\nSince I benchmark synchronization primitives I want to make sure (1) threads are not migrated during benchmark execution, (2) threads are placed in a sensible way (not 1, 3 on 2 cpus with 2 threads each, e.g.), perhaps I want to even place them individually myself because distances between CPUs affect performance.\n\nI think users will generally want to place the threads themselves. That's why IDK if this can be a library feature. Also it's super easy for users to pin threads today (see above).\n\nLast time I used pthread_getaffinity_np, pthread_setaffinity_np. Haven't looked into it yet. Maybe there is a newer, nicer way. Also sched_setaffinity exists on linux.\n\nNo that's the way to do it. But as mentioned above it's not clear what to set the affinity to. \n. Unfortunately this patch is starting to get big. I had to fix a bunch of TSAN race conditions, including existing ones in CHECK and VLOG.\nHowever I have managed to remove all (?) of the global state used to run the benchmarks.\n. > this lgtm. have you run a comparison for all the benchmark tests to see how the timing changes?\nI've done some. The results for basic_test.cc are at the top of this PR. I just need to test this a little more on OS X which uses a really weird implementation.\n. >  have you run a comparison for all the benchmark tests to see how the timing changes?\nI have now. The results are very close to the same except for on threaded benchmarks. Surprisingly threaded benchmarks that don't use PauseTiming still benefit, sometimes greatly.\nPreviously all threads would have to complete the KeepRunning loop before the timer could be paused, this essentially made each benchmark as slow as its slowest thread. This added as much as 30% to the real time measurements.  Now that has been fixed.\n. A couple of other changes in this patch:\n- Change CMake to glob for source/header files so we don't have to enumerate them.\n- Merge re_std.cc and re_posix.cc into re.h.\n. I was originally waiting on PR #262 to land before committing this (in order to avoid merge conflicts). But it's getting to be a big patch so I'm committing it before it gets any bigger. \n. I think it would be better to flush the streams in RunMatchingBenchmarks rather than in each reporter.\n. Thanks!\n. Urg this is why the -pthread flag exists. I wonder if the FindThreads CMake module isn't attempting to use that flag.\nEither way do we need to reverse the older in ToT to fix this? It seems to me that having pthread last is the correct order. Although technically it needs to go after libc.\n. That's weird... IDK why it would define that macro if it doesn't provide clock_gettime.\nWhat version of OS X do you have? I can't reproduce on 10.11.6.\n. OK, so this is interesting. My \"Command line\" headers don't provide the CLOCK_PROCESS_CPUTIME_ID but the XCode SDK ones do. In order to use those headers I have to define MACOSX_DEPLOYMENT_TARGET=10.12, which is the only SDK provided by XCode. \nI'm guessing that /usr/lib/libSystem.B.dylib will contain the required clock_gettime on OS X 10.12, but not 10.11 which is what we are both using.\nI would prefer to use clock_gettime once OS X actually provides it, but I'm OK with this fix for now. Please add a // FIXME note above the change so we know to remove it in the future.\n. LGTM. Thanks for the fix.\n. @dominichamon I debugged this with @xaxxon offline. The bug here is that CPU scaling is enabled and giving us negative times, but we fail to detect this.\nIMHO the fix to this bug is to properly diagnose CPU scaling.\n. Huh it's very interesting that GCC on CYGWIN doesn't define -D_GNU_SOURCE. @EricBackus Can you provide the output of c++.exe -dM -E -xc++ <empty-file>?\n. > I understand this isn't the regex that was causing the error above, but it should probably be changed to not require a decimal point anyway.\nI agree. Fixed in 2555035.\nClosing this issue because I feel it has been addressed.\n. @dominichamon This LGTM. I imagine the new formulation shouldn't run into any versioning issues.\n. Could you please provide the benchmark your trying to run?\n. I can't reproduce with GCC 4.8.5 on Ubuntu 16.04.\n. And your 100% sure it exits with a 0 exit code and doesn't produce any core dumps?\n. I also can't reproduce with GCC 4.8.0 nor 4.8.2 on Ubuntu 16.04.\n. What's the output of cat /opt/rh/devtoolset-2/enable?\n. OK, I think I've figured this out. Did you compile libbenchmark.a using GCC 4.8.2, or more likely something like 5.2? Versions of the library built with the cxx11 ABI cannot be used with GCC < 5, and libraries built with the old ABI are not compatible with GCC > 5. I suspect if you build the library and test with the same compiler they will work.\nHowever it's very weird that it's not causing a segfault. \n. @dominichamon Awesome thanks! \n. Fixed by #317.. The argument handling done by compare_bench.py is very sloppy. It secretly inserts another --benchmark_out=temp_file after the user provided arguments, which caused your flag to be ignored.\nI'll be checking in a rewrite of the argument parsing shortly, and that should make your case work. However passing --benchmark_out to both benchmarks will cause the first result to be overwritten, but at least it will sort of work.\n. Closing this issue because I believe it has been addressed.\n. I'll look into releasing a newly tagged release in the next couple days.. This seems to be a bug/legacy behavior for CMake 2.8.11 which is changed/fixed in 2.8.12. This commit removes the behavior from CMake.\nI think we should bump the CMake minimum version to 2.8.12.\n. Closing since we bumped the minimum CMake version to 2.8.12, which no longer exhibits this problem.. Yeah, you really have to be using Clang for that flag to work.. Merging w/o review. post-commit comments welcome.. @NiklasRosenstein Why not just use the CMake build system?. We could probably be safe diagnosing unrecognized flags starting with --benchmark_*, but like others have said some tests might have to parse their own flags. (For example people frequently use gtest and gbenchmark in the same executable).. @NiklasRosenstein I would create a PR for your changes. However I would move the actual reporting logic out of the macro and into a function in the library.. PR #332 has been merged which should address this issue.. Please run the tests using python report.py. They are currently failing for me and are outputting very poorly aligned output.\nI suspect that fixing the alignment is going to take a little more work. I really phoned in the original code.. LGTM. Thank you very much!. I agree we should try and emit some sort of warning; We used to but I removed it while re-writing the timing internals.\n\nwhen something else than 'performance' cpu frequency governor is used or not all CPUs are set to the same frequency,\n\nUnfortunately checking for the 'performance' CPU governor isn't a great way to detect CPU scaling. Depending on the loaded CPU  module/driver the scaling govenor may be ignored (e.g. Intel P-state).\nHowever checking that all of the CPU's share the same frequency should be a fairly good test to detect scaling.\nHowever I don't think the library should mess with the system's CPU governor. It requires root privileges and we shouldn't encourage running benchmarks as root. Also I don't think we shouldn't be messing with system wide settings that affect other programs. Additionally we can't know the correct max frequency to set the CPU to, since the max frequency reported by the CPU may actually be it's max turbo speed and not max clock speed w/o turbo.\nPatches welcome :-) I would suggest changing the Initialize function in benchmark.cc to detect and report CPU scaling.\n. Printing the min and max seems like a good idea to me. Patches welcome :-). @dominichamon What's the plan with this?. I think you're misunderstanding. The library runs each benchmark multiple times with increasing number of iterations in order to determine how many repetitions are needed to get a large enough sample. What you're observing is those runs. One thread is still being used (Add a printf(\"finished\\n); to observe this).. Right, so the library is going to attempt to run each benchmark for 0.5 seconds by default. Repeating until it gets above that, which is probably requires a lot more than 5000 iterations.\nThe minimum time can be changed using --benchmark_min_time=0.1 (or w/e value you wish).\nAlso see:\n * https://github.com/google/benchmark#controlling-number-of-iterations\n * --benchmark_min_time=N.N\n* Benchmark::MinTime(N.N). >  Shouldn't increasing the mintime increase the run time roughly linearly?\nThey should roughly increase together but the actual relationship may not be linear.\n\n, I found that 0.025 runs in an instance (producing around 5800 iterations), but 0.03 never seems to stop.\n\nThat sounds like a bug. Could you provide a full reproducer?\n. A minimal reproducer would be greatly preferred and appreciated.. I can't build your project on Linux due to this error:\n\n/home/eric/other-workspace/rpc-bench/include/thrift/ThriftServiceBenchmark.h:10:39: fatal error: thrift/TDispatchProcessor.h: No such file or directory\n\nI'm on the gbenchmark branch and have installed all the requested projects using conan.\n. Looks like conan isn't adding the proper include path for thrift.\nI see -I/home/eric/.conan/data/thrift/0.9.3/sztomi/testing/package/776f8cd6d5903762adae77c3f10ba7618c95e25c/include in the compile command but the include path seems to be /home/eric/.conan/data/thrift/0.9.3/sztomi/testing/build/776f8cd6d5903762adae77c3f10ba7618c95e25c/thrift/lib/cpp/src/thrift/TDispatchProcessor.h. > but the include path seems to be /home/eric/.conan/data/thrift/0.9.3/sztomi/testing/build/776f8cd6d5903762adae77c3f10ba7618c95e25c/thrift/lib/cpp/src/thrift/TDispatchProcessor.h\nNevermind that's the build patch. It seems that your conan package file for thrift doesn't correctly install the headers. The include path used by my build doesn't exist.. Actually it seems like all of the generated conan include paths are wrong. I'm giving up trying to reproduce this myself. . >  I can get back to you in a few days with a more minimal repro.\nAlternatively you can just let me know when your original build instructions work.. Closing since it's no longer reproducible.. LGTM. I'll merge this tomorrow night unless @dominichamon has any comments.. @zhangzhimin I'm going to close this report for lack of activity. As @dominichamon pointed out the library\nalready has this behavior. If you figure out what your problem was please re-open this bug.. Well if there is truly no way to detect the CPU clock frequency on such devices then printing \"CPU frequency cannot be detected\" is probably an OK thing to do.. Looks like you still need to link to a C++11 standard library, so yes that appears to be no longer true. You just don't need to enable C++11 still, only link to a compatible stdlib. . compare_benchs.py is a pretty small script. I would add another different script. But try to use and expand the existing python library.. LGTM except for the question left in the inline comment.. > x86_64-unknown-nacl is (non-Portable) Native Client target, it compiles to regular x86-64 instructions (with some limitations due to sandboxing), and supports inline assembly. You need PNaCl toolchain from Native Client SDK or emcc toolchain from Emscripten SDK to try it.\nI can successfully compile code using clang -cc1 -triple le32-unknown-nacl, but as you mentioned the error happens at link time. Honestly I'm really surprised that this doesn't result in an error when compiling or optimizing object files, because it must emit the inline assembly at that point.\nAnyway... Thanks for the patch. And thank you for working on WebAssembly! . DoNotOptimize() should do that already.. DoNotOptimize() may do other things, but it will also do this.\nconst uint32_t divisor x = 3;\nDoNotOptimize(&x);. *Actually don't declare the variable as const. Woops.\nIf you want to provide MakeUnpredictable as a wrapper for DoNotOptimize with a different interface that  would be OK too.. No, Clang and GCC both generate div instructions.\nhttps://godbolt.org/g/76Rz4K. No, because you don't always want to escape the stack address of a variable. Sometimes you just want to escape/consume its value. \nDoNotOptimize is tricky, but so is the optimizer.. I think the trickiness of the API is a good reason to provide a MakeUnpredictable so we can handle the pitfalls correctly for users.. It's been a year, but I finally have a patch ready:\nhttps://github.com/efcs/benchmark/tree/make-unpredictable\nJust waiting on PR #530 to land first.. I can't reproduce on godbolt.org, including with GCC 4.8 on PowerPC.  You need to provide a full standalone reproducer including how you're using DoNotOptimize and all of the compile flags needed to reproduce or the target system.\n. @Maratyszcza Can you please provide a standalone reproducer? Preferably one that works on godbolt.\nI'm afraid there's not much I can do if I can't reproduce.. @meastman Awesome, Thank you for the reproducers! I've put together a patch that I believe should fix the issues.. @GregBowyer Are you going to be able to accept the CLA?. @GregBowyer Ping... This patch cannot be merged until the CLA is accepted.. @GregBowyer Closing this because you haven't signed the CLA. Please re-open if/when you're reading to do that.. Closing since the custom counters patch has landed.\nPlease re-open if that patch doesn't address this issue.. state.KeepRunning() does not synchronize the threads.. > The way I understood it, there is an implicit barrier before and also after the KeepRunning loop.\nSorry I should have been more clear. The KeepRunning() loop does act as a barrier, but only at the start of the first iteration and end of the last one. It does not perform synchronization every iteration.\nPerhaps we should provide a barrier class as part of the API?. A single iterations is the wrong granularity to block on. It introduces too much noise into the timings. A single iteration should spend as little time it benchmark:: code as possible.\n\nOtherwise the throughput is much higher than it should be since some threads can progress without waiting for others.\n\nCan you elaborate on this point? Ideally give an example?\nThe library design works hard to report the best metrics available. The thread timings are taken per-thread. So if one finishes early, the time spent waiting isn't present in the reported results.\n. Can you make sure to squash your commits before sending a PR? This PR currently has 80 different commits as part of it and that seems entirely unnecessary. . When you pull from upstream it doesn't overwrite your local commit history, it simply merges or re-bases your local commits on top of the upstream commits.\nPersonally I would do something like this:\n(Disclaimer: I'm not responsible for anything bad git does)\n```\nReset your commit history to match upstream/master. All changes not committed to upstream will\nwill now be in staging.\ngit reset --soft upstream/master\nRe-commit all of the staged changes as a single commit.\ngit commit -am \"my commit message\"\nPush the commits to github\ngit push --force\n``` . I don't recall what I wanted to do. Feel free to proceed without me.. You should still add tests for user defined counters though.. The approach looks good to me. I'll need a couple days before I can look it over.. @zlqhem Can you report back if this works for you?. All the bots are passing so I'm going to go ahead and merge this.. Thanks!. Could you put the body of this issue into the description of the pull-request? That way it's present in the git commit logs.\nAdditionally I'm closing this issue since there is a PR tracking it. Thanks for the report and for the fix.. This LGTM other than the one requested change.. Thanks for the report and pull request!. Merging since all of the bots have since passed.. The appveyor failure seems unrelated, and instead caused by flaxey tests. . On Wed, Apr 12, 2017 at 6:40 AM, Sergey Lyubimov notifications@github.com\nwrote:\n\nhello.\ni build benchmarklib as cmake subproject\nif(DARIADB_ENABLE_BENCHMARKS)\n   set(BENCHMARK_ENABLE_TESTING OFF)\n   add_subdirectory(extern/benchmark)\n   include_directories(extern/benchmark/include)\n   set_target_properties(benchmark PROPERTIES FOLDER Extern)endif(DARIADB_ENABLE_BENCHMARKS)\nwhere 'extern/benchmark' is a git submodule. But when i build in msvc\n2015, google.benchmark always build as shared dll, not as static. this is a\nbug? i see you build scripts, and you not set library type of benchmark lib.\nGoogle Benchmark builds a static/shared library depending on the value of\nBUILD_SHARED_LIBS. On UNIX platforms this defaults to OFF so a static\nlibrary is built. Are you setting this value to ON?\n\nAlso you should build Google Benchmark as an External Project\nhttps://cmake.org/cmake/help/v3.0/module/ExternalProject.html, not using\nadd_subdirectory.\n\nadd_library(benchmark ${SOURCE_FILES})\ni think, that should see be like this\nadd_library(benchmark STATIC ${SOURCE_FILES})\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/368#issuecomment-293563399,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABHza87ued0sPigseRz4G2R6Q0_-Ky96ks5rvMYigaJpZM4M497J\n.\n. Yes this library works under Windows, and we have multiple Windows builders that test each commit.\n\n@navia1991 This issue is likely on your end, likely the weirdness you're doing to setup/configure the CMake project.\nClosing because the library works on Windows. @navia1991 Please open a new, more specific issue if you find the cause of your problem and it's Google Benchmark's fault.\n. >> Are you setting this value to ON?\n\nno, i'm no set this variable on build script or manually.\n\n@lysevi Well somebody is setting it to ON. I just checked the Windows builders and they generate a static library by default. Here is the doc for BUILD_SHARED_LIBS. I agree with @dominichamon that explicit iteration count control is almost always unneeded, and that users are likely to misuse it when using MinTime would be appropriate/correct. \n\nOne is that it's some combination of CPU time/ wall time, whereas it would be nice to have a manual timer,\n\nIt's not \"some combination\", it's CPU time by default unless the user calls UseRealTime or UseManualTime.\nAlso the library already supports using user provided/manual timers. Take a look at UseManualTime and SetIterationTime in benchmark_api.h.\n. That being said I do think there is good rational for adding explicit iteration count control; Specifically to avoid the hidden \"timing runs\" of the benchmark which are used to determine the number of iterations.\nI can imagine writing a benchmark that accesses some global/external state that should only be accessed once, meaning the benchmark may only be run once. Currently this isn't possible since the library will almost always run the benchmark more than once. The only way to avoid this is to allow the user to explicitly provide an iteration count.\nFor this reason I went ahead and implemented benchmark::Iterations in PR #370. > Edit: Never mind. The manual timer is used. It's just that in my case, the manual time is much smaller than real time, and the iteration count is always determined by (results.real_time_used >= 5 * min_time).\nHmm. That's meant to be a safe guard against malicious benchmarks using little/no CPU time; Preventing them from taking way longer than expected/allowed. I think we should probably disable this condition when a manual timer is used. I'll commit that change tonight.. > I am using those functions for manual timers, but --benchmark_min_time doesn't tie into the manual timer, right ?\n--benchmark_min_time does tie into the manual timers. It should respect the timer used by the benchmark. . > The manual timer is used. It's just that in my case, the manual time is much smaller than real time, and the iteration count is always determined by (results.real_time_used >= 5 * min_time).\nFixed in master. If the manual timing feature had worked all along would have you needed the explicit iteration count change at all?. >  it's difficult to pick a good value for min_time, and I would like to ensure that every benchmark is run for at least an acceptable number of iterations. \nYou can set different values of min_time for different benchmarks usingBenchmark::MinTime(...)\n\nI am also interested in seeing any cache/tlb/other microarchitectural warm-up artifacts. So ideally, I would like to see how the the iteration time varies with number of iterations. Is it possible to get individual iteration's times or aggregate stats ?\n\nIf you're trying to expose warm-up artifacts then you likely want to use --benchmark_repetitions=N or Benchmark::Repetitions(...) instead of specifying a manual iteration count. Each repetitions will run the same amount of iterations, so you can compare the different repetitions by seeing how long they each took.\nI'm starting to think you didn't need the ability to explicitly set the iteration count after all, and that @dominichamon was right suggesting that the feature would be a red herring that confuses users out of doing the right thing.\n . > I know that the documentation mentions --benchmark_repetitions, but that's for inter-benchmark stats and not intra-benchmark stats, right ?\nI'm not sure what you mean here, can you clarify?\n--benchmark_repetitions and Benchmark::Repetitions repeat the same benchmark test multiple times with the same number of iterations.. Now that you have the feature do you think you can provide an example that demonstrates the behavior you described?. This LGTM minus my last inline comment. Thanks for the quick turnaround time.. Why do you think this is a bug with Google Benchmark and not with your CMake. You explicitly add the problematic pthread library here:\n\ntarget_link_libraries(benchmarks tgv pthread libgbench)\n\nThe library currently tests against 10 different Windows configurations, including two which use MinGW, GCC, and pthreads.\nHere is the script that the Windows bots use to configure CMake \nI'm not sure I can help beyond that. I'll leave this bug open for a couple days incase anybody can help further, but after that I plan to close this as WORKSFORME.. Here is another CMake example setup, hopefully it will help: https://gist.github.com/BillyONeal/9da4780049f3fd04765888e7e11564e4\n . Closing this bug since it's not related to Google Benchmark but instead to your configuration.. @dominichamon Can you make the final call on this?. It looks like you're not linking libbenchmark.a on this line:\ntarget_link_libraries(chess_ann_benchmarks -lpthread)\nShouldn't that read target_link_libraries(chess_ann_benchmarks benchmark -lpthread) ?\n. @sciencefyll Also if you were a little more patient or IRC I would have responded. . Also building Google Benchmark as a subdirectory of an existing project is unsupported. You should use CMake's ExternalProject feature.. That sounds like the correct behavior. It's an error to use BENCHMARK_MAIN() in a file with no benchmarks.\nMore specifically it's an error to call RunSpecifiedBenchmarks() when the regex passed to --benchmark_filter=<regex> matches none of the tests. By default the regex . is used.. Please feel free to re-open if you run into other problems concerning the library.. Would something like this work for you? https://github.com/google/benchmark#passing-arbitrary-arguments-to-a-benchmark. There is no support for changing the type accepted by Benchmark::Arg and returned by state.range(...).\nThere is also no easy way to make the library accept arbitrary argument types (It would require a lot of template magic).. @schoetbi Just waiting on you to fix the build before I can review this any further.. I'm not sure there is a good enough reason to break the existing API and make the change.\nIIRC the main rational against passing input parameters as non-const references is that it's confusing to the callers of that function; That is it's unclear at the callsite if the argument will be mutated.\nHowever the only call sites for registered benchmarks are inside the library itself, and internally there is certainly no confusion about what happens to the State argument.\nAdditionally I don't see any API issues or confusion arising for the callee.\nSince the current API, although inconsistent with the style guide, isn't confusing or detrimental to users, and since it would be a pain to change, I don't see why we would.\nNote: It would be possible, with a little work, to support both API's to avoid breaking backwards compat. However I don't think it's worth the trouble.\n. @yangguangxu Have you managed to solve this problem?. @GeorgeARM Is there a reason you can't use State::Label to give the benchmark a name when it's run?. > @EricWF can I filter tests based on their Label?\n@GeorgeARM No you cannot, because the Label is provided at the end of running the benchmark.\nAnother way to achieve your goal would be to use RegisterBenchmark, which allows you to register benchmarks with arbitrary names.. I believe this PR/Issue would be fixed if we adopted the JSON interface I'm proposing. See this test case for an example.. > It printed till 111 and said iterations: 100. Why?\nBecause the benchmark is tested a couple of times with different iteration counts to determine the correct count to use. After it finds an iteration count that causes the benchmark to run for a significant enough period of time it reports only that run, and not the test runs before it.\nResetting result to 0 at the start of the benchmark will show you this.\n\nIs there an explanation of the standard benchmark table results?\n\nTime: The average wall time per iteration.\nCPU:  The average CPU time per iteration. By default this clock is used when determining the amount of iterations to run. When you sleep your process it is no longer accumulating CPU time. I'm assuming some_func() is almost trivial, and that the compiler optimized almost all of it away. (DoNotOptimize is not magical and needs to be used very carefully. See https://github.com/google/benchmark#preventing-optimisation) \nIterations: The number of iterations the benchmark ran. (See https://github.com/google/benchmark#controlling-number-of-iterations)\nPerhaps somebody should add basic documentation about the output information.\n. Ouch, That's probably my fault. Writing a \"portable\" version of DoNotOptimize is awfully hard.. @Timmmm Sorry my mistake. I merged this and then had to revert it due to your missing CLA.\nPlease re-open a new PR and ensure the CLA is signed and I'll re-merge these changes right away.. I've been meaning to allow registration from within a function, (with direct calls to some register function). I'll look more into this on Friday.. @elderRex Woops, I misunderstood your original post.\n\nOne of the problem is that invoking the function twice results in broken pipe\n\nRegistered benchmarks are always going to be invoked multiple times to determine the iteration count to use in the final (and actually reported) run. You need to design your benchmarks to allow for this.\nIs there a reason you can't do this? (Also do I understand your problem now?)\n. @elderRex Let me see if I understand. You want to be able to invoke RunSpecifiedBenchmarks multiple times, but removing certain benchmarks each time?\n. Hmm, OK. ClearBenchmarks might be possible, but individually removing them likely would not be.\nThe problem is that a single registered benchmark actually represents a \"benchmark family\", that itself can generate more than one actual benchmark test. Trying to remove a family or single benchmark test seems confusing.\nHowever removing all of them seems entirely possible. :-). @elderRex See https://github.com/google/benchmark/pull/402. @elderRex Thanks for the report.. @jernkuan Please sign the CLA. We can't move forward until that is done.. @jernkuan I'm not sure what you mean \"not passed to Makefiles\". Can you clarify? Maybe provide an example?. @jernkuan  that would likely work to resolve the issue. feel free to open a new PR.. I'm really skeptical that the claim that the range based for loop is quicker, and about these changes in general. > The motivation is that running state.KeepRunning() every loop creates a limit on how fast very fast benchmarks can run. Approximately 2 - 3 ns / iteration. The cause is unoptimizable memory load / store operations\nHmm, it does indeed. I guess that's because the State object is passed in by reference, and compilers might assume that the state is changed elsewhere in the program between iterations. The ranged-for solution works because it creates a temporary that the compiler can now prove isn't accessed elsewhere.\n@astrelni Could you please separate the ranged-for addition from the main part of this patch, which is the getBatch change? I think they are separate concerns and I would like to review the separately.. @astrelni Do you think you'll have time to revive this soon?. @astrelni Do you mind if I go ahead with my own version of the ranged-based-for loop patch? Since I think that has the performance benefits you suggested even without batch iteration (which can come later)?. @astrelni No need to apologize. Thanks for letting me hijack your idea. I hope you don't mind :-). @dominichamon I'm not sure this is the right thing to do, and I would love your input.. > @EricWF i think it's a good thing to do (it used to be one header) at least for the api/benchmark split. \nAgreed. at minimum that case is better. However we should also consider that changing benchmark.h to no longer include reporter.h would be a breaking change. \n\nreporters are something that only some users would need, i think, so it may be adding more to the compile overhead...\n\nThe actual reporter class declarations shouldn't add a significant amount of overhead, they're pretty small. However it does require including <set>, but we already include <map> in benchmark_api.h, so we're already pulling in most of <set> anyway (because STL's implement them both using the same internals).\n@dominichamon I'll do some tests to see if compile times have slowed noticeably. . > I appreciate the adherence to correctness but it's not going to be something i'm hung up on.\nNormally I can't even consider such a breaking change (in libc++). I'm perfectly happy to keep the reporters header separate. I'll revert that part of the change.\n. Although it does kind of suck that a change that didn't fold in reporter.h would actually cause the library to be, at least technically, less of a single header library than it started. Because previously benchmark.h actually acted like a single header, without actually being one.. @dominichamon Thanks! Merging with reporter.h being merged into benchmark.h.. @dominichamon Can we add some docs about the steps required to create a new version of Google Benchmark? I didn't do the release last time so perhaps you can document what we need to do?. Closing this bug since it's no longer valid. There is another issue open regarding releasing a new version.. @atrah22 Have you read the CMake Cross-Compiling docs? In particular you should look at CMAKE_SYSTEM_NAME.. LGTM. Feel free to merge once the bots sign off.. I'm not sure I think this is an improvement. People know and understand the STL interface and I think it's a benefit to offer it directly.. > I'm wary of users accidentally breaking user counters by (ab)using the map\ninterface. \nThat feels like a \"Doctor, it hurts when I do this\" situation, where users are free to make their lives difficult for themselves.\nYour suggested interface was a part of the original review, but was removed for a number of reasons. One in particular being when you want to be able to change the value of an existing counter without having to always lookup up the key in the map. Ex:\nc++\nstatic void BM_Foo(benchmark::State &S) {\n  auto &CounterRef = (S.counters[\"foo\"] = 42);\n  while (S.KeepRunning()) {\n    int Res = Foo();\n    CounterRef += Res;\n  } \n}\n\nThis is also inspired in part by #420.\n\nI don't think this will have an affect on #420, since the std::map definitions will still be in libbenchmark.a and will be visible during link time.\n. > GCC 5.x does not keep the same definitions for std::map between\nC++11 and C++03, leading to ODR violations for anything using\nstd::map with link type optimizations.\nI'm not sure disabling the test really does anything of value. Either way the definitions that libstdc++ provides, although different, should be compatible if you can somehow suppress the diagnostic.. @gnarlie Disabling LTO for that test sounds reasonable.\n\nto be valid, the C++03 binary would need to be compiled with the same ABI settings introduced in GCC 5 to support C++11. It can be done, but I can't think of a use case.\n\nI'm almost certain this isn't actually an ABI issue; both versions should compile to ABI compatible code. Typically standard libraries don't break ABI compatibility between dialects.\nHowever this is technically a ODR violation as the diagnostic correctly points out, but the violation should be benign.\n. @gnarlie I went ahead and fixed this in abafced9909c7e5e8f6b8236eecd953caa4f8e6f, so your patch is no longer needed.\nThanks for the report and PR. My apologies for hijacking the patch.. I agree with @dominichamon here. Averaging the real-time results provides a more reasonable comparison with CPU time.. I would like to see a lot more rational for this change. Plenty of users have proposed adding various meta-data to the interface, but almost never in the same way, and never in a way that's as helpful to other users as intended.\nThe first thing this patch needs is documentation; both examples and function specifications. I'm not even really sure what you're trying to do here that SetLabel or RegisterBenchmarkName couldn't possibly do.\nMy ideal implementation for storing meta-data would be a std::any like type, but since we can't depend on C++17 std::any is out of the question. However a lesser implementation in the library might be acceptable.\n. I believe this PR/Issue would be fixed if we adopted the JSON interface I'm proposing. See this test case for an example.. LGTM. Thanks for the patch.. > CMake provides a way to build projects (ideally, which have cmake build system themselves) as part of some bigger master build.\nYes, it does. It's called ExternalProject. You should be using that, not configuring it directly as a subdirectory.. @pleroy I hear and understand your concerns. But the approach I'm exploring is different than that proposed in this PR.\nInstead of moving toward external tooling, I propose moving away from it. Step one will be to introduce a fully featured JSON library, which will be used to communicate arbitrary data structures between the user and the library. \nThe library will continue to provide console/tabular reporting as well.\nHowever, W.R.T. #488: The CSV reporter is just fundamentally incompatible with the structure of the benchmarking data. Which is too complex to easily emit and as a useful list of comma separated values.\nIt simply can't provide a quality tool, and so I think that it, in specific, still needs to be removed.. I am a wee bit annoyed by the notion of using JSON as the \"pivot\" format: I'd have preferred to use protobuf (the alpha and omega of all things Google) for that purpose, as it avoids messy parsing/unparsing.\nThree points. First, I'm proposing introducing a existing JSON library to handle all the parsing/unparsing of JSON; so we don't need to maintain that code ourselves. (In particular: https://github.com/nlohmann/json)\nSecond, by using the JSON library, we can make the JSON representation less of a pivot format, but instead as the primary format by which we encode all results and inputs. Both internally within the library, and via the external API's. User's wont be given JSON strings, but instead a JSON object representing the structured data.\nThird, the real issues I'm trying to address is the limitations of strongly-typed input/output data types such as BenchmarkReporter::Run (which is also why I didn't consider protobuf). The problem with the Run type is that it's limited to reporting only what can be stored it its data members, which prevents users from supplying arbitrary output. Even internally our usage of Run is a mess. We often use the\nsame members to hold entirely different types of data depending on the report type we're generating (ie. Regular vs BigO). This makes the meaning of the data members very confusing, and prevents users\nfrom meaningfully using them.\nTherefore, I believe the general solution is to make the types passed to the reporters (and more generally, passed as input to the benchmarks), should allow supplying of \"weakly typed\" data. JSON is one such possibility. Other possibilities include using std::any or std::variant to help manage different or arbitrary report types, but this creates the problem of figuring out how to report them; especially in the case of entirely custom user data passed via std::any. For this reason I went down the path of using JSON to represent benchmark reports, since it essentially type-erases the report to a simple JSON object, but also provides a perfect object-to-text conversion that Google Benchmark doesn't have to supply.\n\nIf for instance someone wants to produce XML (an eminently reasonable thing to do on Windows) it seems rather awkward to first produce JSON and then reparse it to produce XML. My sense of good taste is a bit offended ;-)\n\nThe use case I'm proposing wouldn't be as awkward as this suggests. You wouldn't have to first produce JSON, since that's that's the primary format, there's nothing to produce it from. Second, you wouldn't really have to re-parse anything. A rough implementation of the case your suggesting would look like this:\nc++\nvoid foo() {\n  JSON result = RunBenchmark(...);\n  MyXMLBuilder builder;\n  for (JSON::iterator It = result.begin(); it != result.end(): ++it) {\n    // Assuming all fields are primitive.\n    builder.addField(it.key(), it.value().dump());\n  }\n}. Urg. Of course... @NeumannIMT could you test the suggested patch? Our Appveyor Windows builders seem to be broken (builds have been queued for months).. LGTM. Thanks.. The Travis and Appveyor failures are unrelated to this change. Indeed it looks good to go. Committing.. This LGTM, but could you push a dummy commit to re-trigger the Appveyor bot? I think the failure was spurious. . I did a bunch of investigation, and I found one important bit of information: We're not actually dealing with negative zero. The values that are getting through are negative numbers larger than epsilon() but smaller than round_error().\nI'm not sure exactly what's causing them yet, but I suspect the measured CPU times on MinGW are close to zero they sometimes result in negative numbers, but honestly I have no clue.\nHowever we should be able to target a workaround to MinGW only, where we check std::signbit and return 0.0 whenever we accidentally get a negative time.. @dominichamon To make this easier on you, it might make sense to move the MinGW builders to the top of the appveyor.yml file so they run first.. My educated guess is that MinGW's g++ uses a different floating point model than MSVC does normally, \nand because we're using the MSVC runtimes (in particular to provide stdio). Likely related to this bug:\nhttps://sourceforge.net/p/mingw-w64/bugs/520/\nIt seems that the GNU printf behavior we expect (ie rounding to positive zero) doesn't occur with the MSVC stdio implementation.\n. Woops, didn't mean to close this. Sorry.. Failures due to unrelated -0 ns output failure on MinGW. This patch should be good to go.. OK, Perhaps this is a bug in MinGW?. Merging pre-test completion. Appveyor only failed on the non-functioning MinGW tests, and Travis is hung waiting on OS X test environments to become available, but I tested this patch on OS X personally. . @dominichamon Merging w/o review due to deadline. :-S sorry.. I can't really reproduce with the quick-bench link provided.. Are you setting -DCMAKE_SYSTEM_NAME=foo?. The Appveyor failure is separate from, and unrelated to, this change! Hurray!\nIt looks like the new failure is caused because the regex used only accepts times of 5 digits or less, but for some reason the last run of BM_Counter2_Tabular with 16 threads produces a 6 digit time in nanoseconds.\nThat should probably be investigated and dealt with in a separate commit.. > you know what would be great? tests would be great. maybe it's time to integrate with gtest and write some actual tests. maybe not for this PR i guess.\nAgreed. Adding GTest so it's there to use would be great.\nI was looking for opportunities to better test this patch. But it seems unfortunately hard to actually write portable unit tests where the expected output is system specific.\n. Bots are passing. This change LGTM. Committing.. @pleroy Mind if I proceed with this now that some misunderstandings have been cleared up?\nI plan to remove CSVReporter entirely from the v2 branch today.. Thanks. No rush on the JSON stuff. we've created a v2 branch to stage the changes on. Honestly it might be better to look later rather than sooner, once there is a more clear vision of the full feature.. I'm not sure this is correct. inline is a linkage specifier, __force_inline\nis not\nOn Dec 2, 2017 11:30 AM, \"Elias Kosunen\" notifications@github.com wrote:\n\nWhen _MSC_VER is defined, BENCHMARK_ALWAYS_INLINE is defined as\n__forceinline. Some functions were defined as inline\nBENCHMARK_ALWAYS_INLINE, which after preprocessing became inline\n__forceinline. This PR fixes the warnings caused by that by removing the\nextra inline.\n\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/google/benchmark/pull/493\nCommit Summary\n\nFix warnings on MSVC about double inline\n\nFile Changes\n\nM AUTHORS\n   https://github.com/google/benchmark/pull/493/files#diff-0 (1)\nM CONTRIBUTORS\n   https://github.com/google/benchmark/pull/493/files#diff-1 (1)\nM include/benchmark/benchmark.h\n   https://github.com/google/benchmark/pull/493/files#diff-2 (14)\n\nPatch Links:\n\nhttps://github.com/google/benchmark/pull/493.patch\nhttps://github.com/google/benchmark/pull/493.diff\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/pull/493, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABHza1SKEMYckOwwCXsLLfkeWcBS9moQks5s8ZdSgaJpZM4QzaQA\n.\n. @eliaskosunen I can't duplicate the warning on Godbolt. Could you clarify what version of MSVC you're using?\n. @eliaskosunen Nevermind. I found your Appveyor links. You're using a newer compiler that Godbolt. So this hasn't been fixed in newer versions, like I initially though; instead it's only been added in newer versions.\n\nI still don't like this PR, since I think MSVC is being dumb here. I'm going to open a dialog with the MSVC devs to see what they think.. @pleroy I also considered removing inline on templates. But (A) that doesn't really fix this issue for non-templates, and (B) what about compilers that actually take inline as an optimizer hint and don't support either __forceinline or __attribute__((always_inline))?\n. Just spoke to an MSVC compiler dev. They couldn't reproduce with the latest version of MSVC either (19.13.xxx).\nI think we should go ahead and close this PR. The warning appears to be a briefly shipped bug in MSVC.. Sounds reasonable. Thanks @ldionne. Feel free to submit a PR, if not I'll get to it over christmas.. It's not related. Some tests are still flaky.\nOn Sun, Dec 3, 2017 at 6:05 PM, Louis Dionne notifications@github.com\nwrote:\n\nLooking at the appveyor log, it's not clear the failure is related to this\nPR.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/pull/495#issuecomment-348836008, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABHza6OtfiLmRU_kpKtrJ2dSp0xljf3-ks5s80VegaJpZM4Qz9ei\n.\n. I'm not really sure what to do with this. \n\nObviously, you've correctly identified this as an classic example of the static initialization order fiasco.\n\nIs it possible to register benchmarks in main function?\n\nYes. https://github.com/google/benchmark#using-registerbenchmarkname-fn-args\n. Yeah, the real bug here is that the library API just begs new users to write this kind of bug; and that's on us.\nHowever I'm not exactly sure how to fix it. The current interface to register benchmarks at startup is both widely used and very useful (except when it leads to bugs like this). I don't think we can simply remove it, and there is no simple way to change it so bugs like yours don't occur.\n\nWouldn't it be better if benchmarks are registered in main function by default (i.e. in BENCHMARK_MAIN)?\n\nIf by \"registered by default\" you mean like they currently are now, using the BENCHMARK macros, then there is no way to change that interface so that it registers the benchmarks in \"main\" and not during static initialization.\nThe real fix is to provide an API which doesn't require static initialization, which is the intent of RegisterBenchmark. However, as you point out there is no easy way to use this with fixtures. I'll look into how that can be improved over the semester break. However, with a little creativity it should be simple to transform your fixtures into C++11 lamba's using RegisterBenchmark. A somewhat literal translation might look like this (but there are certainly different, if not better ways to de-duplicated the setup/teardown code using lambdas and RegisterBenchmark\n```c++\n// C++11\nusing benchmark::State;\nstruct MyFixture {\n    void Setup();\n    void TearDown();\n    // ... Start Test Cases ... //\n    void BenchOne(State& st) const;\n    void BenchTwo(State& st) const;\n    // ... More Benchmarks ... ///\nusing BenchmarkCaseT = void(MyFixture::*)(State&) const;\nstatic void RunCase(State& st, BenchmarkCaseT Case) {\n  MyFixture Fixture;\n  Fixture.Setup();\n  (Fixture.*Case)(st); // Weird C++ member function pointer invocation syntax.\n  // In C++17 use std::invoke(Case, Fixture, st);\n  Fixture.TearDown();\n}\nstatic void CreateBenchmarks() {\n  using benchmark::RegisterBenchmark;\n  auto RunFn = &MyFixture::RunCase;\n  RegisterBenchmark(\"bench1\", RunFn, &MyFixture::BenchOne);\n  RegisterBenchmark(\"bench2\", RunFn, &MyFixture::BenchTwo);\n}\n\n};\nint main(int argc, char** argv) {\n  benchmark::Initialize(&argc, argv);\n  MyFixture::CreateBenchmarks();\n  benchmark::RunSpecifiedBenchmarks();\n}\n```\nAs for this particular bug, I'm going to close it and replace it with a meta-bug about how our API causes users to write static initialization order bugs.\nFeel free to follow up with any additional questions.. See issue #498.. @pleroy Here is the first set of changes I'm proposing in order to move toward  using JSON as the primary format.. The coverage decrease is very likely caused by the introduction of the new header, which we aren't testing.. @biojppm I was planning on converting the tests myself in a later commit, since it's a good opportunity for me to demonstrate how I believe the API should be used and tested.. Urg. This Google bot thing is going to be a problem using the v2 branch.. We support GCC 7. If it isn't working we should fix it (assuming it's our bug), not suggest Clang.\nWhat was the error message you were getting with GCC 7?\nAlso, I don't think it's the libraries place to suggest what compiler you use. Additionally, I don't see why Clang should be prefered (Just because you ran into problems with GCC?)\n. @mstankus Using libc++ with GCC isn't a common use case, including on OS X. (I'm the libc++ maintainer as well, so I can speak to that).\nHowever, since I've needed to do similar things, Google Benchmark has a CMake option to make this use case easier. The option -DBENCHMARK_USE_LIBCXX=ON can be used to correctly add the required libc++ flags automatically.\nHowever I don't think we should add the documentation you suggest. First, it's a pretty rare use case, and documenting it may confuse users looking for solutions to other problems. Second, it's still not clear to me why we would suggest using libc++ in general. Using GCC on OS X isn't really\na supported configuration for any project. I'm sure you have your reasons, but it's not clear we should document it as a part of this project.\nClosing this bug for the above reasons. Please re-open if you disagree or would like to further the discussion.\n. @winstondu Can you sign the CLA so I can merge this?\nOr, could you give me permission to close this PR and make my own version?. Ha, that triggered the bot! Thanks.. I'm not really concerned with \"fixing\" the Apply, since it should be immediately apparent to callers that it is executed during static initialization. This is unlike Fixtures.\nI would prefer to see a minimal version of this patch that just handles fixtures and leaves Apply untouched.. You likely don't have a new enough version of the library. I've seen that exact error when mistakenly using old versions.. Closing as FIXED, since I'm assuming my comment is correct.\nPlease re-open if I was mistaken.. @Maratyszcza Thanks for the patch. Don't worry about ABI breaks. The project is free to break the ABI.\nFirst, I agree with @dominichamon about using signed integer types. And I agree about making this change for things we explicitly want to be 64 bits.\nHowever, I don't know if using explicit 32 bit types makes sense. In these cases, it seems to make the most sense to simply use int, which I think is what we should try to do universally except where 64 bits are needed.  What do you think?\n. I don't think we actually need to go through this process, because every contributor to the master branch already signed the CLA. This seems like a bug in the bot. Maybe we could report it to somebody (since we're going to be doing these kind of merges a lot).. Also could you add this test to the bottom of fixture_test.cpp:\nhttps://gist.github.com/EricWF/e033d66c024fe65f9e15705b653bd3f7. Also this change should be against the main branch, not v2.. > I will add the new test https://gist.github.com/EricWF/e033d66c024fe65f9e15705b653bd3f7 as fixture_init_test.cc, is this OK?\nYes. . @lijinpei When you want to update a PR, please use the same branch and PR on github -- don't create new ones every time.\nYou can either squash the old commits and force-push yourself, or you can simply push the new commits. When we merge the PR we'll squash the commits into a single one anyway.\n. Should we change the compiler macros to use the BENCHMARK_ prefix to avoid conflicts?  . I should have clarified that I think that the preventing re-definition part of this patch is correct. Feel free to land this as-is and we can discuss adding a prefix later.\nI'm just a bit uncomfortable that certain build systems are defining these macros magically while others are not. So we have slightly different build behavior for each.. No. That lamba is declared as having no captures. All referenced identifiers must be available at global scope. They are not captured either by-value or by-reference -- they are not captured at all.\nthe BENCHMARK_CAPTURE macro is only intended to be used at global scope.\nClosing as NAD. Re-open if you disagree.. Ide like to take a look at this, but I'm busy until Friday.. So, this patch actually results in worse codegen for the KeepRunning loop as a result of adding the extra completed_iterations_ variable. Every loop now includes an extra load from memory.\nSample Code:\n```c++\ninclude \nusing namespace benchmark;\nextern \"C\" void foo();\nvoid BM_TestKeepRunning(State &S) {\n  while (S.KeepRunning())\n    foo();\n}\n```\nASM Before Change:\nasm\nBM_TestKeepRunning(benchmark::State&): # @BM_TestKeepRunning(benchmark::State&)\n  pushq %rbx\n  movq %rdi, %rbx\n  cmpb $1, (%rbx)\n  je .LBB0_3\n  jmp .LBB0_2\n.LBB0_4: # %while.body\n  callq foo\n  cmpb $1, (%rbx)\n  jne .LBB0_2\n.LBB0_3: # %if.end.i\n  addq $-1, 8(%rbx)\n  jne .LBB0_4\n  jmp .LBB0_5\n.LBB0_2: # %if.then.i\n  movq %rbx, %rdi\n  callq benchmark::State::StartKeepRunning()\n  addq $-1, 8(%rbx)\n  jne .LBB0_4\n.LBB0_5: # %while.end\n  movq %rbx, %rdi\n  popq %rbx\n  jmp benchmark::State::FinishKeepRunning() # TAILCALL\nASM After Change:\nasm\nBM_TestKeepRunning(benchmark::State&): # @BM_TestKeepRunning(benchmark::State&)\n  pushq %rbx\n  movq %rdi, %rbx\n  cmpb $1, (%rbx)\n  je .LBB0_3\n  jmp .LBB0_2\n.LBB0_4: # %while.body\n  callq foo\n  cmpb $1, (%rbx)\n  jne .LBB0_2\n.LBB0_3: # %if.end.i\n  movq 8(%rbx), %rax # Worse codegen starts here.\n  leaq -1(%rax), %rcx\n  movq %rcx, 8(%rbx)\n  testq %rax, %rax\n  jne .LBB0_4\n  jmp .LBB0_5\n.LBB0_2: # %if.then.i\n  movq %rbx, %rdi\n  callq benchmark::State::StartKeepRunning()\n  jmp .LBB0_3\n.LBB0_5: # %while.end\n  xorl %esi, %esi\n  movq %rbx, %rdi\n  popq %rbx\n  jmp benchmark::State::FinishKeepRunning(unsigned long) # TAILCALL\nThe  simple benchmark doesn't exactly show a concerning performance difference in terms of \"reported iteration time\", but I would still like to keep this loop as tight as possible (and an empty benchmark runs fewer iterations than it previously did).\nMore investigation/suggestions coming later.\n. I'm wondering if it might be better to formulate this as a for loop with special iterators. By using iterators to count we can avoid the problem of non optimizable memory accesses in State.\nFor example:\nc++\nfor (auto It=State.begin_batch(1000); It != State.end_batch(); ++It) {\n  /* Do stuff */\n}\nOr\nc++\nfor (auto It=State.begin_batch(); It != State.end_batch(); It += 1000) {\n /* Do stuff */\n}. This LGTM. The codegen looks amazing! Thanks.. @dominichamon Woops, sorry I missed your last comment. Could we address it in a follow up commit since I just merged this change :-S\n@sam-panzer ^. This seems like an odd approach to me. Why are people specifying this option for compilers that don't support it?\nLooking at how LLVM handles this, I think their approach is more correct. For example, IDK if your patch correctly handles clang-cl on Windows. Perhaps the check should be:\nif( CMAKE_SIZEOF_VOID_P EQUAL 8 AND NOT WIN32 ). I think this is a bug we should try to fix on the v2 branch using JSON.\nI think the best way would be to make the BenchmarkFamilies class part of the public interface. I'll take a stab at something.. @sam-panzer This again generates worse assembly for the KeepRunning case, although the assembly is never taken (so naively it should have no impact on performance). \nAs @dominichamon discussed, plenty of people read the assembly the library generates, so it would be nice to keep it as small and simple as possible.\nHowever, there is a way to fix this. Having both KeepRunning and KeepRunningBatch call an internal function KeepRunningImpl(size_t n, bool is_batch) seems to allow the compiler to eliminate the bits only needed for batch the batch function. I tested this and it does solve the problem.\nCould we go ahead and implement that?. @sam-panzer Can you add a dummy commit to force the bots to re-build? I should have fixed the problems making them spuriously fail.. > do you have any benchmark results for this change in benchmarks? :)\nHa. Ironically I don't -- at least not a portable one. But when I get around to adding FlushCache(ptr) it should be easy enough to write. Until then, this ia32 specific benchmark shows a difference of about 2x. From 140 ns to 70 ns.\n``c++\nstatic void BM_Flush(State &S) {\n  while (S.KeepRunning()) {\n    __builtin_ia32_clflush(&S.threads); // the object previously beforemax_iterations`\n    DoNotOptimize(S.iterations());\n  }\n}\nBENCHMARK(BM_Flush)->Repetitions(3);\n```\n. The travis bot seems to be good. Re-running to see if I can trigger a failure again.\nIf not I'll merge this tonight or tomorrow.. @dominichamon Yep. Can do.\n@pleroy:\n\nTIL about LLVM FileCheck. Pretty cool!\n\nIndeed. It's an amazing tool to have when you need to test string output.\n. @dominichamon I added documentation as you requested. Please take a look at your leisure.. @dominichamon Any final words?. This LGTM. Just waiting on the bots.. static libraries need to go on the right side of the source file.. I'm in the middle of exams. I'll need a couple days before I can look into this.. @rzuckerm What version of CMake are you using?. LGTM. I'll leave this for 24 hours to see if anybody else has any comments, but after that I think it's good to merge. . LGTM.. I spoke to the MSVC team about this they couldn't reproduce the warning with MSVC Microsoft (R) C/C++ Optimizing Compiler Version 19.13.26020. I suspect they realized the warning was bogus, and removed it.\nI'm not sure we should do anything about this.. @pleroy Thanks for the correction. I'll pass it on to the MSVC team.. yeah, my bad. I figured this might come up. However all compilers seem to support the behavior as an extension even though they generate a warning about it.. Are we just complaining the the compiler issues a warning? If so we should just disable the warning. As long as the static assert passes there is no undefined behaviour; because there can't be undiagnosed undefined behaviour at compile time.. Yeah, either that or we could wrap the static assert in a #pragma diagnostic disable \"-Winvalid-offsetof\". Either option seems good to me.. I decided to disable it in the source instead, just to ensure we don't have invalid applications ofoffsetof` in other places we don't intend. . IDK how any of this works, but I don't see any glaring problems. . Urg. This is harder than I thought. I'll fix this up in the next couple days.. Yeah, I plan on working on this when I find time.\nThe problem seems to be coming up with the right set of inline assembly to use as to not produce too much assembly but which also ensures the desired effects.. Just  creating the PR to cycle the bots. . What version of Cmake?. @rzuckerm Nevermind, I see you provided the CMake version already.\nI can reproduce. The reason I wasn't seeing it is that I have an installation of gmock that was being found.. Ironically, I feel I should ask if there is anyway you can benchmark the perf improvements for this change?. Urg. I hate this warning.\nUB is not allowed in constant expressions, so if the static assert passes, the code must be well formed. It's dumb to emit a warning for it.\nI'll look into this more tomorrow.. There is no dependency fetcher for LLVM Filecheck. The bots do some magic, but it's not a part of the supported build.\nEven if you did get the dependencies, the tests still wouldn't compile or pass, since the logic that builds them expects a certain assembly syntax and compiler options.\nSimply put these tests aren't meant to be built on Windows, at least for now.. I would much rather bump the required CMake version. CMake is trivial to install on all platforms. There is no reason for people to be using CMake versions that old.. So we should at least bump it to that, but again, it's trivial for users to install newer versions.. LLVM is currently requiring 3.4.3, with the other information provided above it seems like we can agree to bump it to 3.5.1?. I would also settle for 3.4.3 if needed, because if LLVM is getting away with it, so can we.\nI'm also a large fan of the potentially controversial policy of commit now, worry later. We have enough users that if bumping to 3.5.1 is problematic, we'll hear about it in short order, and we can walk it back.\nIn my experience maintaining large projects, I've found this methodology to be quite useful; Allowing forward momentum beyond what you might expect to be possible.. I'm not thrilled to change the blame on almost every line.\nPeople shouldnt be blindly formatting entire files when they submit pull requests. That's what git clang-format is for. then I withdraw my objection.. Hmm. Not sure how to get around the CLA problems here. Since the commits are already in master (or were in master), it should be OK to merge?. Tangentially, that comment doesn't say anything close to what they want it to say.\nDefault initialization leaves uninitialized fields uninitialized. What I think they're trying to say is they're counting on the zero-initialization which takes place during static initialization.\nI'll take my pedant hat off now.. This thread disappoints me. It's unfortunate that it seems we need a code of conduct. Personal attacks are never acceptable during code review.. Yeah, I should find time today\nOn Sep. 10, 2018 8:20 a.m., \"Dominic Hamon\" notifications@github.com\nwrote:\n@EricWF https://github.com/EricWF Do you think you'll have a chance to\nreview this?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/pull/650#issuecomment-419892991, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABHza71yfGO82XElK_fVVvTqCyvakRH2ks5uZlj8gaJpZM4V6r4S\n.\n. I haven't looked at this, but my preference would be to encode this info in the type system in some way. Sort of like std::ratio.\nSo maybe make the parameter something similar instead of just taking an integer?. LGTM. Though I think we should get @rnk's sign off explicitly if we're literally just going to steal the commit. . > How important is it to do that alignment of . separator? Can it be avoided?\nI think it's pretty important. It's allows you to understand the magnitude of a benchmark by glancing. \nWhat's you're objection exactly? To readability? Do you think the space between the value and ns is ugly?. > > > How important is it to do that alignment of . separator? Can it be avoided?\n\n\nI think it's pretty important.\nIt's allows you to understand the magnitude of a benchmark by glancing.\n\nTrue.\n\nWhat's you're objection exactly? To readability? Do you think the space between the value and ns is ugly?\n\nReadability is good; but yes, i don't like that wasted space, and the yet-increased width of the user-counter-less line.\n\nArguabbly the space isn't \"wasted\", since I think we agree it has value in representing magnitude. Otherwise we would just remove it. But it's not ideal either.\nI can get rid of a little width (4 characters) if we don't care about making BigO calculations aligned with everything else, but that's ugly and doesn't help much.\nDo you think this issue should block the revision?. A couple of alternative solutions:\n\n\nAdd a flag to change the alignment to always be right aligned to the unit, which should waste less space. But IMO getting the old behavior back isn't worth the cost of another flag.\n\n\nUse the \"wasted space\" explicitly so it seems \"less wasted\". That is, fill the wasted space with three trailing decimal digits for all outputs. I don't like this either because those digits aren't significant and make it harder to see the information that is.\n\n\nJust right align everything all the time. This means it's a lot harder for the user to determine the magnitude of a benchmark, and ever harder yet to compare two benchmarks next to each other.. > I think we're hitting newer use cases as this project gets broader adoption. In general, the library was expected to be used to run benchmarks with similar timescales, so you'd get ns or ms all the way down. If we're not seeing that then we need to think a bit about how to handle it.\n\nOne option is to pick a base time scale for the entire run and use that everywhere. I'm not sure that's a great idea, but it would at least make eyeballing the runs easy. When you mix timescales you need to take care that, say, 10ms and 100ns are not being mixed up by the reader.\n\n\n\nUnless the user explicitly changes the time unit for a benchmark, this is the behavior we have today.\nSo in almost all cases I suspect we won't be mixing up the reader.\n\n\nThat is not what i meant. I was literally talking about the current code, but without that padding with spaces.\nI.e.\n...\n100 ms\n0.1 ms\n  1 ms\n...\n\nIf it doesn't change time unit i think that's ok.\n\nI personally find the other format easier to read at a glance. Especially as numbers are flying by which is often the case.. Test... I'm having trouble posting.. I've updated the patch to no longer align the decimal point as requested.. This patch is correct.\nCMAKE_SOURCE_DIR seems to always refer to the top level project directory,\nwhile CMAKE_CURRENT_SOURCE_DIR refers to the most recent CMakeLists.txt being processed via add_subdirectory.  Consider the following directory layout:\nmy_project/\n  CMakeLists.txt\n  benchmark/\n    googletest/\nAssuming my_project uses add_subdirectory(benchmark), then, when evaluating HandleGTest.cmake, the value of CMAKE_SOURCE_DIR will be my_project/ and CMAKE_CURRENT_SOURCE_DIR will be my_project/benchmark. \n. > I think i'm failing to convey my thoughts here.\nAh, OK. Yes we could do a lot better in supporting other configurations for googletest. But that's a separate discussion, and we should file a bug to track it.\nThis patch is \"correct\" because it brings the implementation inline with the currently documented behavior, which says:\n\nThis dependency can be provided two ways:\n Checkout the Google Test sources into benchmark/googletest as above.\n Otherwise, if -DBENCHMARK_DOWNLOAD_DEPENDENCIES=ON is specified during configuration, > the library will automatically download and build any required dependencies.\n\nI'm merging this patch to make the behavior inline with the documentation. Lets continue the broader discussion elsewhere. \n. @olzhabay Could you re-open this with your latest changeset?\n. @olzhabay OK, so lets land your original fix first. And then we can explore improvements as a follow up PR.\n. @olzhabay Sorry about the mixup. Thanks for the patch!\n. LGTM, but I'll wait for the bots to cycle before merging  this.. LGTM. Thanks.. @LebedevRI Thanks for the quick review. Just waiting on the bots to cycle.. Agreed.\nunused warnings on internal linkage symbols often pain me. I wish the warning would suppress itself when the symbol's name appears in undefined conditional compilation blocks.. The comment states that there is a problem with CMake 2.8 and older. I think that includes 2.8.12.\nI think this workaround is still needed, but I would strongly support bumping our min CMake version.. Do you have the gtest/gmock headers along your system include paths? . I reproduced and confirmed #564 originally. I remember it being specific to 2.8. \nLet's remove the workaround, but lets also bump the CMake required version. Abseil requires 3.1, LLVM requires 3.4.3.\nWe should  require the most modern version we can.   Installing a modern CMake is trivial on most platforms, so the version in the package manager isn't too important.. The leak is intentional and  it sucks your tooling is reporting it :-( \nAre you using valgrind? Perhaps we can suppress it somehow?\nDuring program termination users could still be touching the CPUInfo, so we need to be sure its destructor hasn't run.  Using new is a common pattern to achieve this.\n. You're right. The object is never freed. Because we can never be sure it is safe to do so. For example\nLet's figure out how to suppress the diagnostic false positive.. It was unintentional. I meant to fix it. \n. Noted.\n. Noted.\n. Noted.\n. The pointer should be a static function pointer so we don't own it. Are there any use cases where somebody could (or would want to) pass a dynamically allocated function pointer?\n. I have.\n. It is in the public API to allow custom reporters. Should I remove it (I'm happy to).\n. It's only used a couple of times in master and I don't see a reason why we can't use NDEBUG everywhere.\n. These macros live in minimal_benchmark.h now. \n. Done.\n. If the end goal is to merge this then does that make much sense?\n. Getting that to work between clang and GCC and in debug mode vs release mode was a royal pain. This always works.\n. A thought we didn't support building in C++03? This works with GCC 4.6 in c++0X.\n. How do you mean? We do change the reported units if that is what your asking.\n. Yeah, I think they are useful to have. Why do you disagree?\n. This works just fine for my cases. However I would like to build benchmark as a shared library and I don't think you can have a benchmark_main as a shared library.\nWe could provide both?\n. I moved it into the internal namespace because that is how main does it. However I'm happy moving this out of the headers entirely. \n. Didn't notice that. Removed.\n. I felt like it didn't make sense that it lived between the add_cxx_compiler_flag(...) and cxx_feature_check(...) blocks. Those seem to be logically related to each other and setting the SO version there seemed out of place.\n. Noted.\n. I made this change as a precaution that out tests didn't use any non-public headers accidentally. Instead forcing their inclusion to be somewhat more explicit.\nI'll leave it up to you which you would rather have.\n. Noted.\nYeah, it is currently unused. I'll remove it. I had a use for it but I didn't merge those tests yet.\n. I agree we should add tests for that, but I would like to get through more of this patch before I add even more new code. \n. I'll open an issues. Also benchmark_test.cc already sort of tests the regex matching by checking how many benchmarks were run given a specific regex. \n. What is the rational for switching to std::string if the constructor is only called from the macro?\n. I don't think so. Then we introduce another dependency which some users don't want. If we do it optionally there is another configuration to support and provides little extra value.\nBut I'll defer to your judgement on this if you really want it.\n. Sure.\n. Noted.\n. I'll make the change. Any input for what to call the levels?\nFYI the reason I didn't use an enum is because the log level is set via the -v=<int> flag. So I figured there was no point obfuscating that input behind an enum.\n. Ok.\n. Ok.\n. Ok.\n. Actually are you sure you want an enum here? Glog doesn't use an enum for it's verbose log either.\n. Done.\n. Done.\n. Sure, but the documentation says \"all\" == \".\" in both the new and old versions.\n. Different semantics. benchmark_iterations runs the test with exactly N iterations. benchmark_min_iters and benchmark_max_iters allow the benchmark to be run with any number of iterations in that range.\n. I think this has to do with the thread annotation attributes not working with variables in anonymous namespaces.\n. I could add one, but the Instance's are only constructed in one spot and it would be more unclear to use a constructor than it would be to assign values to each field.\n. Because adding it would have resulted in even more churn. It seems like a good abstraction though. Do you want it in this patch?\n. It builds with GCC 4.6 in -std=c++0x mode. So yes?\n. Reverted that change.\n. Sure. \n. My starting point was the internal version of this file, not the current one. So on my end it was more churn to add it.\n. ack.\n. Won't this cause the v-table to be emitted in every translation unit?\n. I think the headers should be usable in C++03. If this were in the library source I would make the change.\n. Woopies. I do think it is useful but it somehow got lost. \n. ack. Any particular reason besides style?\n. ack.\n. ack.\n. Yep.\n. I'm not sure I understand what you mean. Could you expand on this?\n. It is indeed. Sorry about that.\n. Reverting to old behavior where an empty spec matched all benchmarks.\n. I wanted to put iterations back so that I didn't remove functionality. I think I will rename benchmark_min_iters and benchmark_max_iters so that their purpose is more clear.\nAny suggestion for the name?\n. AFAIK this is just a formatting thing. The same unit is used internally but is converted to either nano seconds or pico seconds before outputting.\nI'm going to just go ahead and remove the option.\n. I've done the legwork. Here are some of the things I noticed.\n1. There was no difference when building in debug mode.\n2. There was a difference when building in release. I saw a 5% change on some benchmarks in basic_test. This may just be noise.\nThe one thing I like about moving KeepRunning() out of the header is that when the user compiles their tests with two different optimization levels the amount of overhead KeepRunning() introduces doesn't change.\nHowever I also worry about introducing any amount of noise into micro benchmarks. I like the idea of keeping it in the header and letting the compiler decide what is best.\nI think we can still expose fewer internals by moving some of the State methods out of the header. I'll make that change and not move KeepRunning() for now. I'll continue to look into this though.\n. Yes. We now just use an iteration count to try and get the benchmarks to run for benchmark_min_time.\n. Doc bug.\n. kMinIterations isn't really needed.\n. ack.\n. ack.\n. ack.\n. I agree.\n. Actually I'm not too sure it should be printed at all. Why are we telling them the library was built in debug mode?\n. Sure. I think it makes more sense to put it at the bottom with the reporter stuff but I'll move this and some other stuff that used to be at the top of the file.\n. arraysize returns a size_t. These need to be 64 bits to prevent those exact warnings. Without these changes -Wshorten-64-to-32 fires.\n. ack.\n. Could you explain the issues further? I don't understand.\n. In this version iterations cannot differ between repetitions so I don't think you run into this bug because stddev_iterations = total_iterations.\n. Yep. Done.\n. I don't think so.\n. Should min_time=0 be allowed?\n. ack.\n. Ok.\n. ack.\n. I can push, but I can't rebase because it won't rebase against master.\n. In this version repetitions always have the same iteration count. That iteration count is determined based on time but it isn't changed between runs.\n. To duplicate my above comment:\nThe amount of iterations to run a benchmark for are computed so that the benchmark runs for ~min_time. This is done for the first run only. Once the amount of iterations every repetition of the benchmark uses the same amount of iterations.\n. The iteration count is computed so that the benchmark runs in about 1.4x min_time. Computing the iteration count is a rather expensive operation because it requires repeatedly re-running the benchmark on sample iteration counts until you find one large enough. I don't think we want to repeat this operation for every repetition.\nHow many benchmarks differ by at least 40% between repetitions?\n. Pushing the commit helps...\n. Woo! I would love to. The only reason I left these in was for backwards compat.\n. I tried to leave it alone inside the library source. However I would like to keep the headers compiling in C++03 so I can benchmark stuff in C++03.\n. Ack.\n. Should we also do this for UseRealTime()?\n. ack.\n. Didn't know that existed. Thanks.\n. - benchmark_macros.h: It contains more than just macros.\n- benchmark_impl.h: That seems to imply it is private, but users should be able to us it.\n- benchmark_api.h: I'm ok with this but I don't feel it convase the actual intention of the header.\nHow about benchmark_test.h because It gives you everything to create and run the tests?\n. ack.\n. Or perhaps just test.h.\n. It should.\n. When a reporter is not provided GetDefaultReporter() dynamically allocates one and returns it via a unique_ptr, so there is ownership transfer in this case.\nThe unique_ptr instance needs to be declared outside of the if statement so that the unique_ptr is not destroyed until the end of the function (and not the end of the if statement).\n. ack.\n. Gladly. \n. Actually I'm going to hold off for now until I find a place to put ComputeStats\n. Why do we need these? I don't think we should be building in-tree like this.\nEither way the stuff already seems to be there for Unix so I guess this is fine.\n. Do we need these changes at all anymore?\n. Make sure to include the new file \"internal_macros.h\" to get COMPILER_MSVC.\n. Make sure to include the new file \"internal_macros.h\" to get OS_WINDOWS.\n. Make sure to include the new file \"internal_macros.h\" to get OS_WINDOWS.\n. Make sure to include the new file \"internal_macros.h\" to get OS_WINDOWS.\n. Same as above.\n. Probably unneeded now?\n. What did this do?\n. Probably unneeded now?\n. Make sure to include the new file \"internal_macros.h\" to get COMPILER_MSVC.\n. Done.\n. They don't need to be but I see value in keeping them defined. If I have a BM_foo<T, U> in foo.cpp and I don't know if foo.cpp will be compiled in C++11 then I would want to use BENCHMARK_TEMPLATE2 so that it always works.\n. Ok, but I think it is weird to call a function CONCAT when it only takes one input.\n. Oh, I see what you mean.\n. Doesn't seem like it. GCC 4.6 implements system_clock and high_resolution_clock (likely as a typedef for system_clock). \n. ack.\n. Noted.\n. At this point? No. If a reporter wants to report a GMT time (as opposed to local) then it or something similar must be exposed.\n. The way we calculate the width seems weird. Shouldn't it just be the size of the longest name plus room for _stddev?\n. ack.\n. I'll fix the way the column width is calculated.\n. I think so. BENCHMARK_UNUSED is used in the BENCHMARK() macros.\n. ack.\n. ack.\n. ack.\n. ack.\n. Yes it will. The correct thing to do would be to use the INTERFACE specifier but since that isn't available I don't think we should statically link pthread.\n. Shouldn't this be PUBLIC when pthread is a shared library and PRIVATE when pthread is static? Also wouldn't we want to avoid linking pthread as a static library? \n. If libbenchmark.so links to pthread.so then anything that links to benchmark must also link pthread in order to get the pthread symbols, otherwise it will fail to compile.\nIf libbenchmark.so links to libpthread.a then all of the symbols needed for benchmark are already provided and other targets do not need to link to pthread.\n-\n. It controls whether benchmark is built as a shared library. I don't think it controls how pthread is linked. \n. When we build benchmark as a static library we will copy some of the symbols from libpthread.a into libbenchmark.a. Now whenever libbenchmark.a is used in an executable that also links pthread we get two definitions of some symbols.\n. CMake is going to pass -lpthread to the compiler, so it is up to the compiler on how to link it. AFAIK the compiler will prefer to just about always link pthread dynamically. I think the only case where it won't do so is if your linking pthread into a static library but I'm not sure.\n. > the cmake documentation seems to be saying that PUBLIC/PRIVATE/INTERFACE is only relevant for shared libraries.\nAh my mistake. CMake just ignores the pthread input when building a static library. It should still be PUBLIC when we are building benchmark as a shared library though.\n. It didn't show up before. Here is the last build of master (with the old config) https://travis-ci.org/google/benchmark.\nI was hoping a different syntax would make it show up, but it doesn't. \n. ack.\n. ack.\n. ack. This can't take unique_ptr directly to satisfy my need that no c++ library types cross the library boundary. \n. ack.\n. I don't really want to name Fixture anything more complex than Fixture. FunctionBenchmark is an internal type so the name should matter a little less for now.\n. It actually does have to be Fixture because the Fixture class declares virtual void TestCase(State&) = 0; that the resulting test case needs to override.\nI want the RegisterBenchmarkInternal be the universal way to register benchmarks. I don't really understand why we would try and protect against silly users like that. Are user supposed to be able to manually write out benchmark specializations? \n. ack.\n. Not anymore.\n. ack.\n. Unfortunately that would require\n1. Passing std::unique_ptr across the library boundary.\n2. C++11 in the headers.\n. I don't think the code compiles as C++98.\n. What is the point of enabling C++14 for anything other than testing the code in the headers? I don't see what is limiting about only building the library in C++11. What problem are you trying to fix?\nWhere does \"HAVE_CXX_FLAG_STD_CXX14\" get set?\n. If you are using clang as your c++ compiler it will require a sanitized standard library. Not sure about GCC. Can you pull these changes out for now? They don't seem to fit.\n. Yeah I think that makes sense. C++14 enables C11 which removes gets(...). There are certain compiler/stdlib/libc combinations on linux where gets(...) is used but is not available in the headers. \ncheck_cxx_compiler_flag(-std=c++14) doesn't see this issue because the compile test does not include any system headers that use gets(...).\nSince we don't need any C++14 features it is easier to just limit the library to C++11 for now.\n. Your right, it is super useful and it is a feature I want to have. I don't think the way you handle it is portable though. I'm sure it works with GCC and libstdc++, however I don't think this will work on any platform that ships Clang and libc++ as their C++ stack (OS X, FreeBSD).\nTo be clear, I would really really really really like to enable asan and ubsan for testing by default. This change just needs a little more work.\n. Why only use pedantic errors in release mode?\n. Can we just use ctest instead?\n. Seems reasonable. What about turning only turning on -Werror in RELEASE but always keeping -pedantic-errors on. I don't think that -pedantic-errors turns unused variables and other small warnings into errors.\n. I'm not sure we should need to sanitize VARIANT like this. The way it is used implies that it should always be a valid CMake identifier. I realize this is an existing problem but since your in the area :)\n. I'm assuming these binaries are needed for GCC to do LTO? What happens if we add the -flto flag but we don't find the required ar/ranlib?\n. I like the idea of using LTO. In particular because every optimization helps us remove  timing overhead from the benchmark measurements. However setting everything up to build with LTO seems fragile and non-portable. \nFor that reason I don't think we should enable LTO by default just yet (or enable it without a way to turn it off). Could you guard all of this stuff within a cmake option (ex BENCHMARK_ENABLE_LTO)? \n. the presence of the second COVERAGE arg means that the flag gets added to CMAKE_CXX_FLAGS_COVERAGE and not directly to CMAKE_CXX_FLAGS.\n. Agreed. I think we should always use -fstrict-aliasing -Wstrict-aliasing.\n. Sorry, I realize this already existed but since it ended up in the diff it got my attention. I discussed this @dominichamon last night and we agreed that there is no reason why benchmark should/would violate the strict aliasing rules. Please make the change.\n. Small nit. Could you reverse the order of these flags? -Wstrict-aliasing has no effect unless -fstrict-aliasing is given. For that reason I think the reverse order is better (although it probably has no effect). \n. If a user asks for LTO I think we should give it to them regardless of if it is a DEBUG or RELEASE build. Any thoughts?\n. I would really like COVERAGE to work as well. Could you convert ${CMAKE_BUILD_TYPE} to upper-case (or lower-case) before comparing?\n. I know the top level namespace posix is reserved for the implementation in C++. I would prefer not using a nested one either. Could you come up with another name?\n. This doesn't really matter, but what is the difference between \"authors\" and \"contributors\"?\n. I think the value of this switch should be verified below.\n. It seems like it would be a lot easier to compare a std::string than a C string.\nc++\nstd::string const& color_print = FLAG(color_print);\nconst bool term_supports_color =\n   term == \"xterm\" ||\n   term == \"foo\" ||\n  ...;\n. Not opposed towards dropping the namespace.\n. As long as the std::string doesn't cross the DSO boundary. Internally I agree we should use std::string.\n. Leave a comment here with the date and why it can't be qualified with \"std::\". This way we prevent people from breaking it.\n. Lets keep the sorted order please.\n. sorted order.\n. No reason. I agree that seems like a nice solution. \n. Sort of. 'running_benchmark' checks that the benchmark was actually started by the library. So that check would still fire if a user constructed their own 'State' object and tried to use it.\n. I was imagining the \"std::basic_istream\" semantics where you read from the stream and then ask the stream, not the data, if it's still valid.\nIf you want this change made I'm happy to make it.\n. I think we differ on the definition of \"iteration\".  Once a thread reports an error it performs no future iterations. I'll add code to enforce that each thread only calls \"SkipWithError(...)\" once.\nThis code selects the first error reported by a multi-threaded benchmark. Different threads may report an error later but IMO the first error message is likely the most relevant.\n. Ack.\n. Ack.\n. Ack. It seems like I can kill the whole loop.\n. Ack. Does this apply to the getters as well?\n. Yeah. I'll remove this.\n. Note to self: remove this comment since it's been implemented.\n. It could likely be made to work but I have no way of testing on Windows so I avoided changing it.\n. C++11 language and library features are used elsewhere in the tests.  Only cxx03_test.cc compiles in C++03.\nIs there a reason this particular feature needs guarding?\n. Nevermind this change is needed. GCC 4.6 is a C++0x compiler and so BENCHMARK_CAPTURE isn't defined for it. \n. To support custom reporters.\n. Done in a follow up commit.\n. There's a big difference between output for humans and output for tooling. Simply put I can't grok JSON and tooling can't grok colored console output, so I don't want to choose between the two. I want the both.\nI agree the library shouldn't write to files in general, but -o output is a pretty universal feature.\n. This line was lying. \"COMPILER=4.8\" is ignored entirely. If you look at the output you'll see this was invoking 4.6\n. The COMPILER bit is only helpful in naming the compiler in diagnostic output. Otherwise it has no effect.\n. Ah good catch. I changed the top line to read \"BUILD_TYPE=COVERAGE\" since that's what it should have been.\n. > how often do you think someone would want to push the output to tooling and have it human readable? \nI do it all the time. I want to see the benchmark results as the come in, and I also want to do something with the results later.\n\n, i think if you're going to push the output to a tool that tool should take care of the human readable bit too. \n\nI think that tool should take care of the human readable input for the information it presents, I don't expect the tool to repeat the benchmark output. \n\nie, i'd expect $ benchmark_my_stuff --benchmark_format=json | analyse_my_bench\nto provide me with human readable analysis results. if i need the output for debugging, then > tee or some equivalent would at least give me something to look at after the fact.\n\nI wouldn't want to require \"analyse_my_bench\" to know how to pretty print basic benchmark output better than the library can already do.\n\n$ benchmark_my_stuff --benchmark_output=out.json --benchmark_output_format=json --benchmark_format=csv\nit's very easy to get wrong. i mean, output is also to stdout, so output_format vs format is subtle.\ni'm wary of redundancy and confusion.\n\nI'm sympathetic to that. A better way would be nice, or better names. However I don't think offering two output methods is too confusing. This behavior mirrors how LLVM's test runner LIT works. For example $ lit -o out.json test.cpp will give human readable output to stdout, and will print a copy of the JSON results to out.json.\nIn summary I agree with the general concern over confusion, but I think the feature is still useful to have.\n. Good question: It's to avoid the \"zero as null pointer constant\" warning GCC emits in C++98 mode. \nOptimally we would use \"= nullptr\" as the default argument, but that's C++11 only, and using either \"NULL\" or \"0\" as the default value will trigger the warning.\nI would prefer to use default arguments though.\n. Sounds good to me. Changes incoming.\n. Ack. Changed in the newest revision.\n. It's just git messing up history. This change is already in master, but I merged and caught the commit. I'll commit this patch without history.\n. Unnecessary white space change.\n. I'm not a fan of this API. We should absolutely diagnose this in debug mode, but probably in release mode as well.\nBikeshedding: If SetCounter only creates/add a new counter, and never sets an existing one, then maybe it should be called AddCounter or similar?\n. What's wrong with\nc++\nenum Flags_e {\n  kDefaults = 0,\n  kIsRate = 1,\n  kisRate = 2\n};\n. std::string is fine now. Please use that instead.\n. _SetName is a reserved identifier, please choose a different name.\nFor reference the following names are reserved and cannot be used:\n- identifiers with a double underscore anywhere are reserved;\n- identifiers that begin with an underscore followed by an uppercase letter are reserved;\n- identifiers that begin with an underscore are reserved in the global namespace.\n. No issue. So long as STL stuff doesn't affect the timing results then I think we should encourage it's use. \n. Fixed.\n. Done. The operator overloads are removed and  the report options are no longer treated as a bit mask.\n. The non-pod part is definitely an issue, but static initialization is safer if the initializer is in the header so the compiler can see it.\n. These docs are out of date.\n. Still needs to be changed.\n. This can be a single constructor Counter(double v = 0, Flags flags = kDefaults)\n. And let's make this a non-bitmask enum so we can store Flags instead of uint32_t.  I would add a kAvgThreadRate to handle that case.\n. This class should have an assignment operator Counter& operator=(double) to support things like c = 42.0;\n. This change should be made as a separate PR, since it's unrelated to this patch AFAIK.\n. This change seems unneeded since the macro is already defined.\n. Please submit this change as a separate PR.\n. If the functions are internal then they shouldn't be declared in the public headers. Please move the declarations elsewhere.\n. I removed this warning because GCC generates it when compiling w/ libc++ even though the actual warning originates from a system header.\n. I don't think so. Using the replacements requires C++11. So if you want C++03 benchmarks you still need this interface.\n. I was to replace usage of std::cout/std::cerr with the reporters output/error streams, even when just listing the tests.\n. I'll just disable it when BENCHMARK_USE_LIBCXX is ON.\nAs I side note I dislike this warning in general. The warning seems to take into account constant propagation when it should really only warn on the literal token '0'.\n. Nah this should be static_cast. const_cast is for removing const.\n. I changed it so the main thread always runs as thread 0 for multi-threaded benchmarks. Previously we just slept the main thread.\n. I'll do it right now if you clarify what the style should be (Other than FunctionNameAllUpper).\n. Can we check in a .clang-format file to make formatting easier?\n. I would suggest DenseThreadRange(int min_threads, int max_threads, int stride=1) since it matches the DenseRange function we already have.\n. Just implement AddRange2 directly in this function body. No need to dispatch.\n. 'emplace()' doesn't have initializer list overloads.\n. An empty benchmark body is fine, no need to waste cycles.\n. The comment is incorrect.\n. Let's make this idiomatic C++ and change the return type to bool\n. I'm concerned about naming a header posix.h since it's a very generic name. Maybe posix_support.h or benchmark_posix_support.h\n. This now returns true for the empty string. Simply calling IsTruthyFlagValue should give you the right answer.\n. Add a TODO or FIXME tag to this comment so we can grep for it.\n. We only really support color output to stdout, not arbitrary files. Therefore I think this should be StdoutIsColorTerminal() or similar.\n. Actually since IsATTY and FileNo seem like unneeded abstractions since they only have one caller, IsColorTerminal. I would remove these and just call isatty and fileno directly from there.\n. Sorry! I figured since we already use STL types in State there was no need to keep PIMPL'ing benchmark. Mostly it was just the cost of the double dispatch that I wanted to remove.\n. Shouldn't we be adding -std=c++11 somewhere?. This test needs an explicit -std=c++03.. This assertion fires when running benchmark_test. You should mention that the counters map is per-thread and describe how the final value is calculated with multiple threads.. This still seems all wrong.  I think we need to make the CSV reporter store every reported run and only report them inside Finalize. That would mean the CSV reporter wouldn't be usable as a console reporter, instead it would be limited to outputting to files. @dominichamon Does this seem reasonable to you? If so I'm willing to fix this post-commit.. Why have this check at all? ReportUnrecognizedArguments handles that case already, doesn't it?. You could have ReportUnrecognizedArguments(...) return true if there are unrecognized arguments and false otherwise.  Then change this to if (ReportUnrecognizedArguments(...)) exit(1);. EMSCRIPTEN defines CLOCK_THREAD_CPUTIME_ID but doesn't provide clock_gettime? That seems weird... Apple only does that is to support targeting older platforms with -mmacosx-version-min=<ver> .. This case feels like the #else case., but it definitely shouldn't be the primary #if. Please restructure these ifdefs so that the most specialized implementations are at the top, and the generic one is the #else.. You don't need to guard includes when they're standard C++ headers.. Please restructure these ifdefs so that the most specialized implementations are at the top, and the generic one is the #else.. Please restructure these ifdefs so that the most specialized implementations are at the top, and the generic one is the #else. If you have to add restrictions to the primary #if then it's probably better suited as an #elif or #else.. This change seems wrong. #if defined(__GNUC__) guards against compilers that don't support inline assembly. NaCl and EMSCRIPTEN are both platforms, not compilers.\nWhat's the rational for this change?. Does this actually fail to compile? If so that's a Clang bug. The inline assembly doesn't actually generate instructions, so I would hope it would still work regardless of Clang's target.\nIf not can you provide a full error log so I can used it to file a Clang bug?. OK I probably overstated \"not supporting inline assembly\" as a bug. But if Clang parses \"asm volatile(...)\" it should probably also support generating code, at least for empty instructions.\nI just tried compiling an file which used inline assembly with clang++ -target x86_64-unknown-nacl and it accepted it. I can't test linking and running it for obvious reasons, but I'm still skeptical of this change. . The PNaCl docs call out asm(\"\" ::: \"memory\") as being explicitly supported, which is what ClobberMemory uses, so I guess their's hope that these will be supported eventually.\nHowever for now it seems OK to disable them. However in the long run it's important that we find a way to provide a low/no overhead way of preventing optimizations; Calling internal::UseChar isn't sufficient. I'll reach out to friends working on NaCL to request a fix. \n. I don't think Compiler vendors will even give us builtins with better behavior than the inline assembly.\nThe results of testing the fallback DoNotOptimize aren't going to be outrageous, but they're going to be orders of magnitude more expensive than the nop assembly instructions.. What's the rational for this change, and the others like it?. I don't see a point in this change either, can you further explain?. Is there a better, more GCC-like, name for this warning?. Please declare read_err on the same line you initialize it, not before.\nAlso read_err is only used in debug mode, so in release mode we'll get a unused variable error about it. To work around this please add ((void)read_err); // prevent unused warning. Ah sorry I should have figured that out from #354. Go ahead and remove this change. I'm going to commit a different fix upstream. \nI think the actual bug here is that kNumMicrosPerMilli is specified as int64_t. I think it would be more correct to do st.range(0) / static_cast<int>(sizeof(int)). @dominichamon is probably right. It's easier to fix than it is to suppress. However maybe a name like safe_dec_re is more appropriate given the above comment?\nHowever I must add that I think ICC is INSANE emitting a -Wshadow diagnostic between a global symbol and a function-local variable. God forbid you ever write #include <math.h> void foo() { int abs = bar(); }.. See PR #359 for the aforementioned fix.. Yeah, I would rather see the __GLIBCXX__ check in there as well. As you mentioned this works fine with libc++ :-D. Let's just use vsnprintf unconditionally. I see no reason to prefer the version in std:: .. Same comment as above.. __GLIBCPP__ was removed in favor of __GLIBCXX__ in GCC 3.4. I can't imagine anybody is still using that old of a GCC version. Are you sure it's needed?. How about instance.name += FormatString(\"%d\", arg) instead?\nI just want to avoid as much conditional compilation as possible. The more of it the library accumulates the harder it gets to maintain.. We should get the default value of CMAKE_INSTALL_LIBDIR by including the GNUInstallDirs.cmake module, not by setting it ourselves.\nAfter fixing that this patch LGTM.. This needs to be off by default, with a CMake option to turn it on. Ex:\noption(BENCHMARK_BUILD_NUGET_PACKAGE \"Build a nuget package for benchmark\" OFF)\n[...]\nif (BENCHMARK_BUILD_NUGET_PACKAGE)\n[...]\nendif()\nAnd I think that all of the output should be put under a nuget subdirectory, instead of polluting the top-level build directory with everything.. Missing newline.\n. Missing newline.\n. Please don't make this a warning. There are plenty of cases the project has to build w/o git. For example libc++ has a copy of Google Benchmark, but libc++ uses SVN. . Should we check if PROJECT_SOURCE_DIR/.git is a directory as well? . Agreed, we can provide multiple interfaces, but we should strongly prefer one. . @dominichamon I think it's very important to keep this method in the header. We really want it to get inline into the caller where possible.. This seems like a bad way to suppress unused value warnings. Having a non-trivial destructor could, when it's not optimized away, potentially undermine the entire point of this; which is writing the most efficient benchmark possible.\nUsing struct BENCHMARK_UNUSED Value suppresses the warning for GCC, we should use that instead.\n. Should be use BENCHMARK_ALWAYS_INLINE here?. This should very likely have BENCHMARK_ALWAYS_INLINE too.. BENCHMARK_ALWAYS_INLINE. BENCHMARK_ALWAYS_INLINE . BENCHMARK_ALWAYS_INLINE. This seems like the wrong type, since we usually use size_t when dealing with iterations. It appears that the use of int64_t below is probably wrong too, but no need to address that in this patch.. The cast should be unnecessary here one results.iterations is made to have the type size_t.. Cast should be unnecessary here as well.. DISREGARD ME. See comment below.\nSorry I know @dominichamon just suggested the opposite, but I think using <cassert> is this test is preferable, just so we can avoid any potential ODR issues for non-debug builds of libbenchmark.a.\nIdeally the other tests shouldn't use check.h either, but the added output turns out to be infinitely helpful in those cases. In this case <cassert> should do fine. I'll look into fixing the other tests, or fixing check.h to avoid ODR violations in the near future.\n. This seems to explicitly allow the batch size to change within the same run of a benchmark, ie the argument passed to the first call differs from the argument passed to the second (within the same run). Is this behavior intentional? Is it needed?\nIt appears to me that if the batch size is fixed we can choose a max_iteration count that is exactly divisible by kBatchSize. Not sure how much utility that will have, but I just wanted to inquire.. Actually disregard me. There are no ODR violations when using check.h so this is fine. My mistake.. We've been trying to get away from using explicitly sized types (See issue #147).\nAll other variables referring to iteration counts are size_t and so I don't see why this one should be any different.\nAlthough your point about overflow technically stands, but  on further inspection I'm not sure why you chose to make the change to store the sum of all the iterations performed by all threads. Each thread is required to perform the exact same number of iterations. Summing the iterations to get a total just to later divide it by the thread count seems pointless. Am I missing the reason for this change?. > As much as I'd like it not to be the case, I think the range-based for loop API will depend on this behavior somewhat.\nCould you submit a separate PR (likely derived from this branch) with the range-based for loop implementation so it's visible and therefore easier to discuss?\n\nThe range-based for uses 1000 as the batch size,  but the API could easily be used by a benchmark which is slow enough to only run in the single digits to tens of iterations in a reasonable amount of time.\n\nShouldn't the range-based for loop simply use the total iteration count as it's batch size? I'm assuming the range-based for loop can only be used to execute all of the iterations. Having the range-based for loop execute larger batch sizes then there are max iterations doesn't make sense and conflicts with the fundamental design of the library. Perhaps I'm not understanding something?\n\nSo we want to easily be able to cut the batch size short when that feature comes.\n\nIDK what \"cutting the batch size short\" really means. Can you give a motivating example?\n\nI guess thinking more, we could give a bool flag for State for whether KeepRunningBatch was called or not and use that when calculating the number of iterations. (Also would add the batch size as a State attribute and update the contract that the batch size must remain the same)\n\nHmm, that sounds reasonable to me but it does complicate things, because before the first attempted run of the benchmark the library doesn't know the batch size that will be used (if any). I guess the solution would be to have a special first run of the benchmark where KeepRunning() and KeepRunningBatch(n) only return true once, just so we can determine if KeepRunningBatch is used and if so then the potential batch size. The library would then calculate the actual number of iterations to use as a multiple of the batch size. In order for this to work it's critical the batch size always equal the batch size specified during the first iteration of the first run.\nUsing this approach ensures that the library still has full control over the number of iterations run, and that the final batch of iterations doesn't \"overflow\" the max iteration count selected by the library. Ex. The library selects 15 iterations, but the batch size used by the user is 10, meaning either 10 or 20 iterations will be performed but not the 15 selected by the library.\nI'm also concerned that allowing variable batch counts will allow users to write benchmarks that have misleading or inaccurate results, because the size of the batch reported to the library doesn't actually reflect the amount of work done by the actual loop iteration.\n. Indeed.. Something like this seems cleaner:\nc++\nauto clamp = [](double V) {\n  if (V < 0 && std::fabs(V) < std::numeric_limits<double>::epsilon())\n    return 0.0;\n  return V;\n};\nconst double real_time = clamp(result.GetAdjustedRealTime());\nconst double cpu_time = clamp(result.GetAdjustedCPUTime());. *Actually, since this should probably apply to all reporters, maybe the clamping should be done in GetAdjustedCPUTime() et al.\nWhat do you think?. Could I do that as a separate refactoring, and use it in my assignment? :-D. Yeah, that seems to work.. Yep. I'll commit separately. . @pleroy I don't think that's whats happening here.\nFirst, the ThreadCPUUsage is using GetProcessTimes from the Windows Kernel library; which is not MinGW rolling it's own clock AFAIK.\nSecond, the negative numbers don't come from the clock itself. When the problem occurs when two consecutive calls to GetProcessTimes return the exact same value. (See here)\nTherefore the problem is caused by the FP operations we perform. Whether GCC's FP model in MinGW is partially to blame is up for debate. But it's certainly not the fault of the clock.\n. Thank you for the in-depth and excellent analysis.\n\nWhat is instead happening is that you are seeing x87's 80-bit extended precision format hiding beneath those doubles.\n\nAgreed.\n\nIt should be possible to verify that (isolating the issue from the hard-to-trace x87 floating-point) by looking at the actual integers in MakeTime.\n\nThe trace previously mentioned does print out the integer values as suggested. It verifies the opposite. \nKernel: 0\nUser: 156250\nKernel: 0\nUser: 156250\nd1: 00111111 10010000 00000000 00000000 00000000 00000000 00000000 00000000 \nd2: 00111111 10010000 00000000 00000000 00000000 00000000 00000000 00000000 \nd3: 10111100 00101010 00011000 00000000 00000000 00000000 00000000 00000000\nMy less technical guess as to what's happening is this: One of the two operands to the subtraction has been written to memory truncating it, while the other has not, and therefore still has extended precision. \nWithin the library, and not my example, this makes a lot more sense. Since start_cpu_time_ is an lvalue that would have to be loaded from memory. Whereas the return value of ThreadCPUUsage(), calling conventions permitting, could still have extended precision. \nI'm nowhere close to as knowledgeable about FP conventions. Does this sound reasonable, or am I out to lunch?. Also, I should mention that I couldn't reproduce the negative result when I wrote both d1 and d2 through volatile, forcing truncation. . I think the real fix here is to store the integer representation instead of converting to double, at least until as late as possible. . Yes. And for now this change is correct.. It doesn't need to be defined, but I find code that's always exposed to the compiler is easier to maintain. Especially when you're not actually developing on that platform.\n. Ack.. Ack.. Roughly I'm assuming that long long can't get any longer, but apart from that the specific bit width isn't too important.\nOn 32 bit systems size_t/ssize_t can be 32 bits where as long long/unsigned long long are 64 bits. Since sysctl can return 64 bit integers in 32 bit builds, we need to ensure the return type is large enough to handle them.\nI choose not to use int64_t or, better yet, intmax_t, because using the fundamental types seems cleaner and equally as correct. . See comment above.. I know I'm being inconsistent, but this function calls GetSysctl which is not available on all platforms (because sysctlbyname isn't available). Ack. I'll move to using int64_t.\nAlthough I thought we had a PR to purge all the explicitly sized integer types :-P. Some sysctl fields are explicitly specified as being unsigned integers, and this attempts to allow the user to read them as such.\nIt also seems entirely reasonable that a field could represent a bit pattern as opposed to an integer value.. Ack.. Ack.. This line still needs to stay for when out-of-tree GTest installations are used. However the SYSTEM can be removed, since it seems to be implicitly specified after adding the INTERFACE_SYSTEM_INCLUDE_DIRECTORIES line.\n. Nevermind... You're right.. SGTM. Thanks for the feedback. . First, I don't really care to allow users to specify random/arbirtrary out-of-tree GTest source directories. We already have a mechanism to allow users to specify arbitrary GTest installation using CMAKE_MODULE_PATH to influence find_package(GTest).\nI've changed the build, as you've suggested, to only download dependencies if -DBENCHMARK_DOWNLOAD_DEPENDENCIES=ON is specified. So GTest installations among /usr/ and /usr/local/ should work by default.. I've looked into 2 step approaches, but I think that they're less idiomatic CMake.\nIf CMake meant for you to download and load subdirectories during configuration time, they would have provided an actually supported way to do so.. ++pleroy. @Maratyszcza I maintain an STL, and I honestly I don't think we even worry about 16 bit integers. Let's go with the Google style guide and assume that int is at least 32 bits.. This should be upper case as with the rest of the methods. I would vote for the name Init(). I'm not sure I agree in making this function part of the public API. Nobody should be calling this directly, right? . These Create methods can be replaced with a single template.\nc++\ntemplate <class FixtureT>\nFixture *CreateFixture() { return new FixtureT(); }. You might as well define the functions inline.. Nice job moving these classes out of the header.\nThey can probably put in an unnamed namespace now, since we don't really want to leak their symbols.. Lose virtual add override to be consistent with below?. What I meant by \"define inline\" here is that you could put the function body inside the class. I don't see much value in re-declaring it only a few lines later.. Please keep the lambda and function one as part of the public API.\nThe fixture overload works entirely differently that the existing ones, since you have to pass it a special function pointer which isn't the benchmark, but a function which creates it. The fact that this is how we handle registering fixtures feels like an internal implementation detail; not a part of the public API.. > But class Benchmark itself is in the internal namespace,\nYeah, that's kind of dumb. I think the initial intent was that users were never supposed to name or create the type themselves; so the name was effectively \"internal\".. I would just put the unnamed namespace inside benchmark::internal instead of adding using statements. . The test LGTM. Thanks.. What's this change?. We're never going to have more than 4 billion threads. . All these changes for thread counts seem unneeded.. Why are you doing the reverse here, and changing an int64_t into an int?. We should note the requirement that n != 0 since we'll enter an infinite loop if a user passes 0. And users are silly, they might attempt to do that.. Sounds reasonable to me.\nPerhaps we could still attack the low hanging fruit and enable it on x86?. Isn't this identical to the #else case? Is that intended?. If it's no matter, I actually prefer having the smaller section headers for these bits. IMHO it's looks nicer and flows better.. I think -pthread is preferable. It's like -lpthread but the compiler puts it in the right place. So you can even put the flag before your object files, and they'll still link.. > will lead to runtime exceptions, not linker errors. See [issue #67]\nHey now! Libc++ has the decency to fail to link!\n. Why?. Ack. Fixed.. Could you clarify a bit?\nThe reason for the current implementation of FormatString is to ensure columns with the same place value align. For example.\n```\nBenchmark                                                Time                CPU         Iterations\n\nBM_empty                                                0.572 ns            0.572 ns     1000000000\nBM_empty/threads:72                                     0.021 ns            1.50  ns      958819032\nBM_spin_empty/8                                         7.47  ns            7.47  ns      180868971\nBM_spin_empty/512                                     954     ns          953     ns        1466617\nBM_spin_empty/8192                                  15449     ns        15447     ns          90954\nBM_spin_empty/8/threads:72                              0.258 ns           18.2   ns       72560520\nBM_spin_empty/512/threads:72                           22.2   ns         1564     ns         872568\nBM_spin_empty/8192/threads:72                         382     ns        26233     ns          38664\n```\nI'm not too familiar with printf specifiers, but if there is a tool I can use to do that, please let me know.. To align it with the Iterations header.. Yeah, I don't love the extra width either. But being able to see sub-nanosecond results is worth the hit I think.. default cases are bad for code maintainability, because I want the compiler to warn me that the switch isn't fully covered when I add an enumerator. \nMaybe we should just disable the \"no default\" warning in the source code?. Does std::abort work? Or any other noreturn function?. > Disabling that warning on the whole code can be harmful.\nNot on the whole of the code base, just for that function. But I'm strongly opposed to adding a default case.\nWe've discussed this before, and we've decided on this direction.. Yes. There was a bug originally where we misaligned the last couple fields of the header when complexity was involved.\n. I think this example is an antipattern. It should use KeepRunningBatch, no?\nStarting and stopping the timer every iteration certainly is. We don't recommend writing benchmarks like this, we certainly shouldn't provide them as examples.\nThe library goes out of it's way to give you a better way to do this.. Time is a well-defined unit of measure. It is not an arbitrary measurement unit for the benchmark. We shouldn't design an API around confusing the two.. I can see why that documentation might be confusing.\nThe KeepRunningBatch documentation is found in the header, here: https://github.com/google/benchmark/blob/master/include/benchmark/benchmark.h#L481\nThe for (auto) formulation is preferred as default. It ekes out every bit of optimization possible. When you need to do more complicated things, like batching, you need to use other interfaces.\nEverything around the main benchmark loop should be as optimized as possible. Otherwise it introduces noise into the benchmark results.. I don't think we should allow it to represent an arbitrary value to begin with.\nIf we do allow custom timers, I would like the timer to be provided to the library, and have the library invoke it using the same semantics it does with the other clocks. \nI would also like to move towards using strong types for times. e.g. std::chrono::time_point and std::chrono::duration. Then we don't need BenchmarkTimeInfo.. The benchmark loop has been optimized within an inch of its life. It's as low-cost as possible; this is super important. Otherwise running the loop itself appears in or influences the timing data. This is BAD. It's bad bad not good. \nMy main objection to this patch is that it makes you write benchmarks that make the main loop costly. This leads to bad benchmarks and bad benchmarking data.\nI'm not saying your PR isn't well motivated: returning arbitrary data or measurements is a reasonable goal that we should support.\nMy objection is how it does this.\nFor example, the library automatically determines how many iterations to run the benchmark for by guessing at different iteration counts, and then seeing if the timing results meet a minimum threshold (Ex. if I run the benchmark 100 iterations, does it take 1 second).\nIn your manual clock example, do you want your \"manual time\" to be used to determine the number of iterations? Ie. If I say --benchmark_min_time=2.0, then benchmarks using your manual clock will run until your manual timer reports 2.0 units of w/e your manual clock represents?\nAt this point, I think I would be OK with an approach that allowed users to specify a manual timer that acts like the other timers. That is, it the timer should only only stopped/started when the rest of the timers are, and not every iteration.. Why is it imperative that rdtsc is called every iteration, and not just at the start and end of the benchmark?. What should happen when you've explicitly checked out googletest into benchmark/, but also have already defined the targets? It's not clear what version we should use.\nAlso, what happens the second time this CMake file is processed? AKA, the first pass we create the targets ourselves. The second pass we detect that the targets are present?. ",
    "syoyo": "Super cool! Thank you!\n. ",
    "ckennelly": "Hi, have you signed a Google Contributor License Agreement?  If not, you'll need to sign the appropriate agreement:\nhttps://developers.google.com/open-source/cla/individual\nhttps://developers.google.com/open-source/cla/corporate\n. One approach that can be used in similar situations is to add parenthesis around the macro argument (see http://gcc.gnu.org/onlinedocs/cpp/Macro-Arguments.html ):\nc++\nBENCHMARK_TEMPLATE(BM_Construct, (std::map<char,int>));\nThis isn't a complete fix though, as there are some downstream consequences for n<a> in the macro expansion in benchmark.h, so the code fails to compile (albeit for a different reason).\n. @showlabor Add a commit to this pull request adding yourself to the AUTHORS and CONTRIBUTORS file (see #10 for an example) and one of us can go ahead and merge this.\n. Thanks for pointing this out.\nFor reference, I ran benchmark_test under Valgrind 3.9.0 and found (with --leak-check=full):\n```\n36 bytes in 1 blocks are possibly lost in loss record 25 of 52\n   at 0x4C2B050: operator new(unsigned long) (vg_replace_malloc.c:319)\n   by 0x5109FC8: std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator const&) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x510B710: char std::string::_S_construct(char const, char const, std::allocator const&, std::forward_iterator_tag) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x510BB37: std::basic_string, std::allocator >::basic_string(char const, std::allocator const&) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x4209D9: benchmark::internal::Benchmark::Benchmark(char const*, std::function) (benchmark.cc:600)\n   by 0x41C6E0: __static_initialization_and_destruction_0(int, int) (benchmark_test.cc:144)\n   by 0x41C90F: _GLOBAL__sub_I_main (benchmark_test.cc:153)\n   by 0x42BE4C: __libc_csu_init (in /home/ckennelly/projects/benchmark/benchmark_test)\n   by 0x58839F4: (below main) (in /lib64/libc-2.19.so)\n37 bytes in 1 blocks are possibly lost in loss record 26 of 52\n   at 0x4C2B050: operator new(unsigned long) (vg_replace_malloc.c:319)\n   by 0x5109FC8: std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator const&) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x510B710: char std::string::_S_construct(char const, char const, std::allocator const&, std::forward_iterator_tag) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x510BB37: std::basic_string, std::allocator >::basic_string(char const, std::allocator const&) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x4209D9: benchmark::internal::Benchmark::Benchmark(char const*, std::function) (benchmark.cc:600)\n   by 0x41C358: __static_initialization_and_destruction_0(int, int) (benchmark_test.cc:50)\n   by 0x41C90F: _GLOBAL__sub_I_main (benchmark_test.cc:153)\n   by 0x42BE4C: __libc_csu_init (in /home/ckennelly/projects/benchmark/benchmark_test)\n   by 0x58839F4: (below main) (in /lib64/libc-2.19.so)\n37 bytes in 1 blocks are possibly lost in loss record 27 of 52\n   at 0x4C2B050: operator new(unsigned long) (vg_replace_malloc.c:319)\n   by 0x5109FC8: std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator const&) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x510B710: char std::string::_S_construct(char const, char const, std::allocator const&, std::forward_iterator_tag) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x510BB37: std::basic_string, std::allocator >::basic_string(char const, std::allocator const&) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x4209D9: benchmark::internal::Benchmark::Benchmark(char const*, std::function) (benchmark.cc:600)\n   by 0x41C517: __static_initialization_and_destruction_0(int, int) (benchmark_test.cc:85)\n   by 0x41C90F: _GLOBAL__sub_I_main (benchmark_test.cc:153)\n   by 0x42BE4C: __libc_csu_init (in /home/ckennelly/projects/benchmark/benchmark_test)\n   by 0x58839F4: (below main) (in /lib64/libc-2.19.so)\n39 bytes in 1 blocks are possibly lost in loss record 28 of 52\n   at 0x4C2B050: operator new(unsigned long) (vg_replace_malloc.c:319)\n   by 0x5109FC8: std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator const&) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x510B710: char std::string::_S_construct(char const, char const, std::allocator const&, std::forward_iterator_tag) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x510BB37: std::basic_string, std::allocator >::basic_string(char const, std::allocator const&) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x4209D9: benchmark::internal::Benchmark::Benchmark(char const*, std::function) (benchmark.cc:600)\n   by 0x41C406: __static_initialization_and_destruction_0(int, int) (benchmark_test.cc:70)\n   by 0x41C90F: _GLOBAL__sub_I_main (benchmark_test.cc:153)\n   by 0x42BE4C: __libc_csu_init (in /home/ckennelly/projects/benchmark/benchmark_test)\n   by 0x58839F4: (below main) (in /lib64/libc-2.19.so)\n39 bytes in 1 blocks are possibly lost in loss record 29 of 52\n   at 0x4C2B050: operator new(unsigned long) (vg_replace_malloc.c:319)\n   by 0x5109FC8: std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator const&) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x510B710: char std::string::_S_construct(char const, char const, std::allocator const&, std::forward_iterator_tag) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x510BB37: std::basic_string, std::allocator >::basic_string(char const, std::allocator const&) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x4209D9: benchmark::internal::Benchmark::Benchmark(char const*, std::function) (benchmark.cc:600)\n   by 0x41C461: __static_initialization_and_destruction_0(int, int) (benchmark_test.cc:71)\n   by 0x41C90F: _GLOBAL__sub_I_main (benchmark_test.cc:153)\n   by 0x42BE4C: __libc_csu_init (in /home/ckennelly/projects/benchmark/benchmark_test)\n   by 0x58839F4: (below main) (in /lib64/libc-2.19.so)\n39 bytes in 1 blocks are possibly lost in loss record 30 of 52\n   at 0x4C2B050: operator new(unsigned long) (vg_replace_malloc.c:319)\n   by 0x5109FC8: std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator const&) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x510B710: char std::string::_S_construct(char const, char const, std::allocator const&, std::forward_iterator_tag) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x510BB37: std::basic_string, std::allocator >::basic_string(char const, std::allocator const&) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x4209D9: benchmark::internal::Benchmark::Benchmark(char const*, std::function) (benchmark.cc:600)\n   by 0x41C4C1: __static_initialization_and_destruction_0(int, int) (benchmark_test.cc:72)\n   by 0x41C90F: _GLOBAL__sub_I_main (benchmark_test.cc:153)\n   by 0x42BE4C: __libc_csu_init (in /home/ckennelly/projects/benchmark/benchmark_test)\n   by 0x58839F4: (below main) (in /lib64/libc-2.19.so)\n41 bytes in 1 blocks are possibly lost in loss record 31 of 52\n   at 0x4C2B050: operator new(unsigned long) (vg_replace_malloc.c:319)\n   by 0x5109FC8: std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator const&) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x510B710: char std::string::_S_construct(char const, char const, std::allocator const&, std::forward_iterator_tag) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x510BB37: std::basic_string, std::allocator >::basic_string(char const, std::allocator const&) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x4209D9: benchmark::internal::Benchmark::Benchmark(char const*, std::function) (benchmark.cc:600)\n   by 0x41C63C: __static_initialization_and_destruction_0(int, int) (benchmark_test.cc:112)\n   by 0x41C90F: _GLOBAL__sub_I_main (benchmark_test.cc:153)\n   by 0x42BE4C: __libc_csu_init (in /home/ckennelly/projects/benchmark/benchmark_test)\n   by 0x58839F4: (below main) (in /lib64/libc-2.19.so)\n41 bytes in 1 blocks are possibly lost in loss record 32 of 52\n   at 0x4C2B050: operator new(unsigned long) (vg_replace_malloc.c:319)\n   by 0x5109FC8: std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator const&) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x510B710: char std::string::_S_construct(char const, char const, std::allocator const&, std::forward_iterator_tag) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x510BB37: std::basic_string, std::allocator >::basic_string(char const, std::allocator const&) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x4209D9: benchmark::internal::Benchmark::Benchmark(char const*, std::function) (benchmark.cc:600)\n   by 0x41C693: __static_initialization_and_destruction_0(int, int) (benchmark_test.cc:135)\n   by 0x41C90F: _GLOBAL__sub_I_main (benchmark_test.cc:153)\n   by 0x42BE4C: __libc_csu_init (in /home/ckennelly/projects/benchmark/benchmark_test)\n   by 0x58839F4: (below main) (in /lib64/libc-2.19.so)\n44 bytes in 1 blocks are possibly lost in loss record 33 of 52\n   at 0x4C2B050: operator new(unsigned long) (vg_replace_malloc.c:319)\n   by 0x5109FC8: std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator const&) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x510B710: char std::string::_S_construct(char const, char const, std::allocator const&, std::forward_iterator_tag) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x510BB37: std::basic_string, std::allocator >::basic_string(char const, std::allocator const&) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x4209D9: benchmark::internal::Benchmark::Benchmark(char const*, std::function) (benchmark.cc:600)\n   by 0x41C3A6: __static_initialization_and_destruction_0(int, int) (benchmark_test.cc:61)\n   by 0x41C90F: _GLOBAL__sub_I_main (benchmark_test.cc:153)\n   by 0x42BE4C: __libc_csu_init (in /home/ckennelly/projects/benchmark/benchmark_test)\n   by 0x58839F4: (below main) (in /lib64/libc-2.19.so)\n54 bytes in 1 blocks are possibly lost in loss record 34 of 52\n   at 0x4C2B050: operator new(unsigned long) (vg_replace_malloc.c:319)\n   by 0x5109FC8: std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator const&) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x510B710: char std::string::_S_construct(char const, char const, std::allocator const&, std::forward_iterator_tag) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x510BB37: std::basic_string, std::allocator >::basic_string(char const, std::allocator const&) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x4209D9: benchmark::internal::Benchmark::Benchmark(char const*, std::function) (benchmark.cc:600)\n   by 0x41C5E2: __static_initialization_and_destruction_0(int, int) (benchmark_test.cc:101)\n   by 0x41C90F: _GLOBAL__sub_I_main (benchmark_test.cc:153)\n   by 0x42BE4C: __libc_csu_init (in /home/ckennelly/projects/benchmark/benchmark_test)\n   by 0x58839F4: (below main) (in /lib64/libc-2.19.so)\n56 bytes in 1 blocks are possibly lost in loss record 35 of 52\n   at 0x4C2B050: operator new(unsigned long) (vg_replace_malloc.c:319)\n   by 0x5109FC8: std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator const&) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x510B710: char std::string::_S_construct(char const, char const, std::allocator const&, std::forward_iterator_tag) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x510BB37: std::basic_string, std::allocator >::basic_string(char const, std::allocator const&) (in /usr/lib64/gcc/x86_64-pc-linux-gnu/4.8.2/libstdc++.so.6.0.18)\n   by 0x4209D9: benchmark::internal::Benchmark::Benchmark(char const*, std::function) (benchmark.cc:600)\n   by 0x41C582: __static_initialization_and_destruction_0(int, int) (benchmark_test.cc:100)\n   by 0x41C90F: _GLOBAL__sub_I_main (benchmark_test.cc:153)\n   by 0x42BE4C: __libc_csu_init (in /home/ckennelly/projects/benchmark/benchmark_test)\n   by 0x58839F4: (below main) (in /lib64/libc-2.19.so)\n256 bytes in 1 blocks are definitely lost in loss record 50 of 52\n   at 0x4C2B600: malloc (vg_replace_malloc.c:291)\n   by 0x5930157: regcomp (in /lib64/libc-2.19.so)\n   by 0x42170E: benchmark::internal::Benchmark::FindBenchmarks(std::string const&, std::vector >) (benchmark.cc:749)\n   by 0x422F1C: benchmark::internal::RunMatchingBenchmarks(std::string const&, benchmark::BenchmarkReporter const) (benchmark.cc:1138)\n   by 0x423309: benchmark::RunSpecifiedBenchmarks(benchmark::BenchmarkReporter const*) (benchmark.cc:1190)\n   by 0x41C0F0: main (benchmark_test.cc:152)\n2,640 (224 direct, 2,416 indirect) bytes in 1 blocks are definitely lost in loss record 52 of 52\n   at 0x4C2B600: malloc (vg_replace_malloc.c:291)\n   by 0x4C2B89F: realloc (vg_replace_malloc.c:687)\n   by 0x592F92E: re_compile_internal (in /lib64/libc-2.19.so)\n   by 0x59301BF: regcomp (in /lib64/libc-2.19.so)\n   by 0x42170E: benchmark::internal::Benchmark::FindBenchmarks(std::string const&, std::vector >) (benchmark.cc:749)\n   by 0x422F1C: benchmark::internal::RunMatchingBenchmarks(std::string const&, benchmark::BenchmarkReporter const) (benchmark.cc:1138)\n   by 0x423309: benchmark::RunSpecifiedBenchmarks(benchmark::BenchmarkReporter const*) (benchmark.cc:1190)\n   by 0x41C0F0: main (benchmark_test.cc:152)\nLEAK SUMMARY:\n    definitely lost: 480 bytes in 2 blocks\n    indirectly lost: 2,416 bytes in 16 blocks\n      possibly lost: 463 bytes in 11 blocks\n    still reachable: 1,700 bytes in 24 blocks\n         suppressed: 0 bytes in 0 blocks\n```\nValgrind also found a race condition in our multi-threaded example during cleanup.  I found this when I was running Valgrind with no arguments (it doesn't show up when I used --leak-check=full, but such is the nature of race conditions):\nInvalid read of size 8\n    at 0x41CCD2: std::vector<int, std::allocator<int> >::push_back(int const&) (stl_vector.h:903)\n    by 0x41BF3E: BM_SetupTeardown(benchmark::State&) (benchmark_test.cc:124)\n    by 0x41D06A: std::_Function_handler<void (benchmark::State&), void (*)(benchmark::State&)>::_M_invoke(std::_Any_data const&, benchmark::State&) (functional:2071)\n    by 0x425334: std::function<void (benchmark::State&)>::operator()(benchmark::State&) const (functional:2464)\n    by 0x422D9D: benchmark::State::Run() (benchmark.cc:1110)\n    by 0x422EC1: benchmark::State::RunWrapper(void*) (benchmark.cc:1127)\n    by 0x4E3B072: start_thread (in /lib64/libpthread-2.19.so)\n    by 0x594A44C: clone (in /lib64/libc-2.19.so)\n  Address 0x783ce18 is 8 bytes inside a block of size 24 free'd\n    at 0x4C29F1C: operator delete(void*) (vg_replace_malloc.c:502)\n    by 0x41BF98: BM_SetupTeardown(benchmark::State&) (benchmark_test.cc:131)\n    by 0x41D06A: std::_Function_handler<void (benchmark::State&), void (*)(benchmark::State&)>::_M_invoke(std::_Any_data const&, benchmark::State&) (functional:2071)\n    by 0x425334: std::function<void (benchmark::State&)>::operator()(benchmark::State&) const (functional:2464)\n    by 0x422D9D: benchmark::State::Run() (benchmark.cc:1110)\n    by 0x422EC1: benchmark::State::RunWrapper(void*) (benchmark.cc:1127)\n    by 0x4E3B072: start_thread (in /lib64/libpthread-2.19.so)\n    by 0x594A44C: clone (in /lib64/libc-2.19.so)\nI'll look into writing a fix for this.\n. I originally tested this with CMake 2.8.12 (http://www.cmake.org/cmake/help/v2.8.12/cmake.html#module:ExternalProject).\nI checked the documentation for CMake 2.8.7 (http://www.cmake.org/cmake/help/v2.8.7/cmake.html#module:ExternalProject) and noticed that the generic URL_HASH specification is not supported.  The Google Test project specifies a SHA1, so it seemed cleaner to use theirs rather than generate my own MD5.\n. Perhaps we should run make test for the debug build as well?\n. Could you move/copy your comment in the pull request into the commit message so everything is in one place (the git history)?\n. Can you rebase to get rid of the merge commits and the minor \"cleanup\" commits?\n824324d seems to be doing multiple things other than porting the code to Windows.\n. Before this pull request is merged, can it be rebased against master?  e5efd2b (add MSVC files) and 7b145d7 (remove MSVC files) are redundant (and don't need to become part of the project's permanent history).\n. Your use case isn't entirely clear to me.  Do you have an example that might illustrate it?\nI'm thinking of a case where I'm trying to run a micro benchmark on a piece of code that has a nondeterministic branch that affects its behavior/performance.  If I'm interested in one half of the branch, I would probably either modify the branch condition (to force it to take a particular path) or benchmark a smaller piece of code.\n. Okay, that's more clear.  I thought you wanted to be able to suppress individual iterations.  (That said, if your code under benchmark is nondeterministic, you'd want to check every output value, not just the last.)\nIn general though, this use seems like it overlaps with what a testing framework (such as the framework this project uses) is for.  I think you should be able to write a test case (using gtest) and a microbechmark in the same file (if you desire), first running gtest and, if it returns 0 (where normal uses of gtest typically return from main), you would run the microbenchmark.\n. I must admit that I didn't intend for those regular expressions tests to be comprehensive.  (Given I was fixing a memory leak by adopting RAII semantics, I mostly wanted the test to check that.)\n. CMake has a few built in modules, one of which is CHECK_INCLUDE_FILE_CXX (it's at least standard with 2.8.12) to verify <regex> is available.  Alternatively, if you're in need of something more general, CHECK_CXX_SOURCE_COMPILES also should do the trick if you want to try compiling re.cc (or something similar).\n(At the moment, I'm a bit envious of Google Test's glob/wildcard-style filtering: https://code.google.com/p/googletest/source/browse/trunk/src/gtest.cc#444)\n. If you have the SHA1 and the tag, is it essential to include the number of commits between the two?\n. URL_HASH isn't supported by all versions of CMake 2.8.x, namely the one used by drone.io for automated builds: https://drone.io/github.com/google/benchmark/54\nI added this in 6087edd but then removed it in 92cd2e82 because of this issue.\n. LGTM.\n. @dominichamon: @zjx20 has signed the CLA.\n. Does benchmark_test need the CMake dependency?  It doesn't link against/include any Google Test libraries.\n. Is it even necessary to export the CHECK_XXX macros at all in the header file?  These are being used internally rather than in benchmark.h.\n. Yes, the CLA has been signed.\n. Are you using the stock Ubuntu 14.04 libc?\n. Thanks!\n. Is there a reason to prefer _build to build, etc.?\n. https://code.google.com/p/googletest/source/checkout exposes a subversion repository, rather one with git.\nChromium closely tracks the subversion repository at https://chromium.googlesource.com/external/googletest.git, but it's currently a commit behind head.\n. This is nominally going to be addressed by #30, but I would want to merge that (and test it) before moving forward with the rest of this patch.\n. We should probably replace the families_ vector with a different data structure when we do?\n. Since the changes here are confined entirely to these two files, you would be able to revert the changes to re.cc and re.h before rebasing.  There wouldn't be any merge conflicts to resolve.\n. Why not use an unordered_map?  Since the index values chosen do not need to be compact, the only operations necessary are insert and delete, iteration isn't necessary.\n. I agree with Dominic, I'd rather this exist outside of the project as well.\nOn a related note, is it absolutely necessary that this library is built alongside this project?  Installing library prerequisites is somewhat common in the Linux world, so I'm not sure Windows should be different.\n. Calculate* is a properly formed regular expression (match calculat followed by zero or more e's).\nCalculate is not, as there's no initial prefix to repeat for the .\n. I'd rather keep things Valgrind-clean.\n. ",
    "iiSeymour": "@ckennelly yes, I have signed the appropriate agreement. \n. ",
    "showlabor": "@pphaneuf: I just signed the individual CLA electronically. I don't know how long it will take to process.\n. @ckennelly I had just been at it ;-)\n. I just briefly glimpsed in the code again: There's code specific for __ARM_ARCH >= 6 and code  for __ARM_ARCH. The latter of which includes the return statement. It's not possible to combine these into a single check.\nMaybe the first check should be altered to #if (__ARM_ARCH >= 3) to be more in line with the origianl version. But since I don't know where or when the original ARMV3 would appear I just leave it this way.\n. ",
    "r3dlex": "Oh! I just saw this default implementation on MIPS. Copied that and everything went smooth. Maybe you could downgrade this error to a warning and provide a default implementation (as in MIPS)?\nThanks!\n. ",
    "Nukesor": "Closed. This is just a workaround. I'll try to fix it inside the src.\n. Did you receive the CLA? I'm not sure if it worked, because i didn't get any email notification yet.\n. ",
    "chenshuo": "Added myself to those files.\n. CMake 2.8.11 added supports for INTERFACE_INCLUDE_DIRECTORIES.\nhttps://cmake.org/cmake/help/v2.8.11/cmake.html#prop_tgt:INTERFACE_INCLUDE_DIRECTORIES\nAnd gtest makes use of it if CMake >= 2.8.11.\nhttps://github.com/google/googletest/blob/master/googletest/CMakeLists.txt#L130\nUbuntu 14.04 has 2.8.12, I will test it and update this post.\nhttps://packages.ubuntu.com/trusty/cmake. I tested it on Ubuntu 14.04 with CMake 2.8.12.  All passed, see my log:\nbenchmark-patch-2.txt\nMaybe upstream changes in googletest fixed it?. No, I don't have gtest/gmock installed.\n```text\n[~]\nschen@cello$ sudo updatedb\n[~]\nschen@cello$ locate gtest/gtest.h\n/home/schen/.cache/bazel/_bazel_schen/install/d8f72112707d582748aa5797593ef9c7/_embedded_binaries/embedded_tools/third_party/gtest/gtest.h\n/home/schen/benchmark/googletest/googletest/include/gtest/gtest.h\n/home/schen/download/texlive-20170524-source/texk/dvisvgm/dvisvgm-src/tests/gtest/include/gtest/gtest.h\n/home/schen/git/benchmark/googletest/googletest/include/gtest/gtest.h\n/home/schen/git/grpc/third_party/googletest/include/gtest/gtest.h\n/home/schen/git/kythe/third_party/googletest/include/gtest/gtest.h\n/home/schen/git/llvm/utils/unittest/googletest/include/gtest/gtest.h\n/home/schen/git/minix/external/bsd/llvm/dist/llvm/utils/unittest/googletest/include/gtest/gtest.h\n```\nI verified that benchmark/googletest is used in compilation.\n```text\n$ make string_util_gtest VERBOSE=1\nformatted by me\n[100%] Building CXX object test/CMakeFiles/string_util_gtest.dir/string_util_gtest.cc.o\ncd /home/schen/benchmark/build/test && /usr/bin/c++   -DHAVE_POSIX_REGEX -DHAVE_STEADY_CLOCK -DTEST_BENCHMARK_LIBRARY_HAS_NO_ASSERTIONS \\\n -std=c++11  -Wall  -Wextra  -Wshadow  -pedantic  -pedantic-errors  -fstrict-aliasing  -Wno-deprecated-declarations  -Wstrict-aliasing \\\n -I/home/schen/benchmark/include -I/home/schen/benchmark/src/../include \\\n -isystem /home/schen/benchmark/googletest/googletest/include \\\n -isystem /home/schen/benchmark/googletest/googletest \\\n -isystem /home/schen/benchmark/googletest/googlemock/include \\\n -isystem /home/schen/benchmark/googletest/googlemock    \\\n  -UNDEBUG -o CMakeFiles/string_util_gtest.dir/string_util_gtest.cc.o -c /home/schen/benchmark/test/string_util_gtest.cc\n``. If gtest/gmock targets are already defined, we should not usebenchmark/googletest`. Otherwise CMake complains about duplicated targets, as it requires target names being globally unique [https://cmake.org/cmake/help/v3.9/policy/CMP0002.html].\ntext\nCMake Error at benchmark/googletest/googletest/cmake/internal_utils.cmake:145 (add_library):\n  add_library cannot create target \"gtest\" because another target with the\n  same name already exists.  The existing target is a static library created\n  in source directory\n  \"/home/schen/git/recipes/benchmark/googletest/googletest\".\n  See documentation for policy CMP0002 for more details.\nCall Stack (most recent call first):\n  benchmark/googletest/googletest/cmake/internal_utils.cmake:197 (cxx_library_with_type)\n  benchmark/googletest/googletest/CMakeLists.txt:125 (cxx_library)\nI don't quite understand how this file could be processed twice.  It's only included by root/CMakeLists.txt. If root/CMakeLists.txt is added twice, I am pretty sure we will have the same duplicate targets error as above.. ",
    "d235j": "cmake simply generates an MSVC project files based on the CMakeLists.txt, and is very much unlike the usual *nix build tools like automake and Make. CMake on Windows installs in C:\\Program Files and has a rather nice GUI.\nThat said, are you doing anything special in the MSVC project? I made a minor change to the CMakeLists.txt (wrapping the three set(CMAKE_CXX_FLAGS) in if(NOT ${CMAKE_SYSTEM_NAME} MATCHES \"Windows\") ... endif() as the MSVC compiler doesn't understand those flags) and it happily generated an MSVC project file that compiled fine.\nOf course, I don't know if the output is correct \u2014 it seems to build a .lib.\n. ",
    "izaid": "Can I ask what the status is on this? What needs to be done to make it work?\n. This is useful -- is there a possibility to get it merged?\nOn a related issue, we've started using benchmark in our library and it works great. However, we already have Google Test included as a thirdparty library. Is it possible to make benchmark compile using that rather then download its own version of Google Test?\n. I signed it!\n. Thanks @dominichamon! As to your question, I have two answers:\n1) As far as I can tell (and I tried), linking to libc++ did not help here.\n2) I think most users of clang on Linux will ultimately link to libstd anyway, as that is what is available. So we probably should make it work for that.\n. @EricWF Okay, so we should try and resolve this. If the change to arraysize.h is not okay, how can we make Google Benchmark work around this bug?\nYes, it is a compiler issue, but unfortunately it is a compiler issue that is widespread due to it being packaged this way in Ubuntu. I'd rather we fix it so that people can use Google Benchmark on Ubuntu with Clang and C++14. If we don't fix it here, it simply makes adoption of Google Benchmark harder.\n. Actually, it's not that easy, because the bug affects all versions of Clang on Ubuntu -- 3.5 and 3.6 as well. It is literally any Clang packaged in the default Ubuntu repository.\n. @EricWF What I said is what I recall not working.\nIn any case, we are talking about removing one include <cstddef>. When I made this PR, Google Benchmark worked and it was great. If you feel very strongly about that, then put it back and I will test it again and I will tell you if it is broken.\nI did not find it \"super easy\" to make things work otherwise I would have not made this PR. A Google Benchmark that works with more compilers is generally better than a Google Benchmark that doesn't, especially if the changes are very minimal (like here).\n. Yeah, okay, this worked. My bad!\nSorry, closing the issue now.\n. Thanks for the suggestion! I switched to fixtures here.\nI guess I need to also modify the normal state.KeepRunning() loop to terminate when I run out of keys. I can add a simple check to see if there are no more keys, but what's the right way to signal to state to stop the benchmark?\n. I was doing one lookup per 'keep running' iteration. I really just want to benchmark the lookup cost, and this seems like the right way to do it. But, if it's not, let me know.\nYour example with the SetUp seems reasonable to me. That's what I'm attempting to do now -- e.g., N lookups per 'keep running' iteration. I've got this working as per the set example. I guess it's slightly annoying that I have to divide the benchmark time by the number of lookups to get the actual cost per lookup, but I can live with that. Is it possible to get access to state::range_x() in the benchmark fixture? (e.g., how should I tell my fixture the size of the random set it should generate?)\n. Even if I comment out all the CHECK(...) I can find, it still seems PauseTiming() and ResumeTiming() takes in the 100s of nanoseconds.\nIn my particular situation, using the fixture (or otherwise precomputing the keys) avoids that problem. Whether this is a problem in general, I don't know.\n. @dominichamon Thanks, your map test is exactly what I have in mind. Would love to see that merged.\nAs you also found out, PauseTiming and ResumeTiming basically drown out the map lookup. The fixture is a totally acceptable solution here, so I'd agree we should probably deprecate PauseTiming and ResumeTiming -- I can't imagine they will ever be fast enough to be negligible.\nHere's another idea that may be worth considering: What about if we allow the user to specify the number of iterations in state. That way, in something like BM_MapLookup, we will have N iterations that correspond to N lookups from the random key set prepared in the fixture. The advantage of this is that the time reported will be exactly the map lookup time, not the total time for N map lookups.\n. @EricWF Thanks, all of the suggestions here were good -- we can simply run N times the benchmark. I appreciate the feedback, you're right that I wasn't quite looking for the right thing.\n. ",
    "predmond": "could use this:\n+double WalltimeNow() {\n+  auto now = std::chrono::steady_clock::now();\n+  return static_cast<double>(\n+             std::chrono::duration_cast<std::chrono::microseconds>(\n+                 now.time_since_epoch()).count()) /\n+         static_cast<double>(kNumMicrosPerSecond);\n+}\n+\nand this:\n-  int remainder_ms;\n-  std::cout << walltime::Print(walltime::Now(), \"%Y/%m/%d-%H:%M:%S\",\n-                               true,  // use local timezone\n-                               &remainder_ms) << \"\\n\";\n+  auto now = std::chrono::system_clock::now();\n+  std::time_t now_time = std::chrono::system_clock::to_time_t(now);\n+  std::cout << std::ctime(&now_time) << \"\\n\";\n. Take the pi benchmark from benchmark_test.cc:\nstatic void BM_CalculatePi(benchmark::State& state) {\n  static const int depth = 1024;\n  double pi ATTRIBUTE_UNUSED = 0.0;\n  while (state.KeepRunning()) {\n    pi = CalculatePi(depth);\n  }\n}\nI want to assert that CalculatePi actually computes pi. If someone make a change to CalculatePi and breaks it and it's suddenly twice as fast I don't want to get excited.\nstatic void BM_CalculatePi(benchmark::State& state) {\n  static const int depth = 1024;\n  double pi ATTRIBUTE_UNUSED = 0.0;\n  while (state.KeepRunning()) {\n    pi = CalculatePi(depth);\n  }\n  if (isNotPi(pi))\n    state.SetInvalid();\n}\nIf a benchmark is invalid then it should appear red in the output, for example.\n. I didn't see the other related pull request. Ignore this if you thing the existing pull request is a better solution.\n. hmm, I expected that users can create their own reporters and register them. The user can still pick which one to use with an option.\n. I was thinking that for normal use you want it to print to stdout however for things like CI it may be easier to deal with data in CSV. It seems like a pretty common use case to me. CSV is easier to convert to spreadsheets etc. so providing one out-of-the-box could be convenient to users.\n. That sounds pretty good to me.\n. ",
    "eddyxu": "@dominichamon I add another pull request for AUTHORS/CONTRIBUTORS.\nI've already signed CLA :smile: \n. @dominichamon I've already signed CLA.\n. ",
    "zjx20": "Update: the issue is link-order related\n``` sh\ncrash\ng++ -o test_bench test_bench.cpp -Wall -Wextra --std=c++11 -lbenchmark\n```\n``` sh\nok\ng++ -lbenchmark -o test_bench test_bench.cpp -Wall -Wextra --std=c++11\n``\n. Alright, in fact, I don't like the hack I made too, I just want to resolve the issue with least changes. But I also don't like a never freed pointer. So next time, I will try the other way.\n. @dominichamon It would be much simpler if use pointer instead of static variable, but that will also result in a memory leak. (it's actually not a big deal, justvalgrind` prints junk) Do you have any idea?\n. ",
    "core-process": "probably not :-).. in that case \"add_dependencies(re_test googletest)\" is sufficient...\n. because it does not compile otherwise\n. ",
    "tamarlev": "Im currently trying to compile google benchmark for MingW and failing for c++11 features as mutex and threads, do you know how to resolve that?. ",
    "3Hren": "Done with readme.\nTesting both on gcc and clang on linux is problematically, because you need to track manually for ABI compatibility of every third-party library using in your project. Instead of this here will be OSX build, which uses clang.\nI've just read http://docs.travis-ci.com/user/multi-os/, that they aren't currently accepting multi-os requests :(\n. Done! Also I've signed the CLA.\n. ",
    "DaimonPl": "Problem is still happening. I'm also using gtest library in exactly same setup and works without any problems. If you need more information please let me know.\nCompiled with CLANG 3.5 in C++11 mode, 64bit, ubuntu 14.04\nStack trace:\nThread [1] 32222 [core: 1] (Suspended : Signal : SIGABRT:Aborted)   \n    __GI_raise() at raise.c:56 0x7ffff722abb9   \n    __GI_abort() at abort.c:89 0x7ffff722dfc8   \n    __gnu_cxx::__verbose_terminate_handler() at 0x7ffff7b366b5  \n    0x7ffff7b34836  \n    std::terminate() at 0x7ffff7b34863  \n    __cxa_throw() at 0x7ffff7b34aa2 \n    std::__throw_system_error() at 0x7ffff7b86990   \n    void std::call_once<void (&)()> at 0x4137e4 \n    benchmark::CyclesPerSecond() at 0x41367c    \n    benchmark::walltime::Initialize() at 0x413b22   \n    benchmark::Initialize() at 0x408569 \n    main() at main.cc:26 0x4031e6\nBTW: libbenchmark was compiled using cmake, so most likely G++ was used for that part\n. Let me know if it will be tracked here or  new issue is required\n. As I checked code it looks to be some problem in sysinfo.cc or related calls - so might be related to my library building and/or environment.\nHere's output from cmake:\n-- The C compiler identification is GNU 4.8.2\n-- The CXX compiler identification is GNU 4.8.2\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Performing Test HAVE_FLAG_CXX_14\n-- Performing Test HAVE_FLAG_CXX_14 - Failed\n-- Performing Test HAVE_FLAG_CXX_11\n-- Performing Test HAVE_FLAG_CXX_11 - Success\n-- Performing Test HAVE_FLAG_CXX_0X\n-- Performing Test HAVE_FLAG_CXX_0X - Success\n-- Performing Test HAVE_WALL\n-- Performing Test HAVE_WALL - Success\n-- Performing Test HAVE_WSHADOW\n-- Performing Test HAVE_WSHADOW - Success\n-- Performing Test HAVE_WERROR\n-- Performing Test HAVE_WERROR - Success\n-- Performing Test HAVE_PEDANTIC_ERRORS\n-- Performing Test HAVE_PEDANTIC_ERRORS - Success\n-- Performing Test HAVE_FNO_STRICT_ALIASING\n-- Performing Test HAVE_FNO_STRICT_ALIASING - Success\n-- git Version: v0.0.0\n-- Version: 0.0.0\n-- Performing Test HAVE_STD_REGEX\n-- Performing Test HAVE_STD_REGEX -- compiled but failed to run\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile\n-- Performing Test HAVE_POSIX_REGEX\n-- Performing Test HAVE_POSIX_REGEX -- success\n-- Looking for include file pthread.h\n-- Looking for include file pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE  \n-- Configuring done\n-- Generating done\nHere's output from make:\nScanning dependencies of target googletest\n[  5%] Creating directories for 'googletest'\n[ 11%] Performing download step (download, verify and extract) for 'googletest'\n-- downloading...\n     src='https://googletest.googlecode.com/files/gtest-1.7.0.zip'\n     dst='/home/damian/benchmark/third_party/src/gtest-1.7.0.zip'\n     timeout='none'\n-- downloading... done\n-- verifying file...\n     file='/home/damian/benchmark/third_party/src/gtest-1.7.0.zip'\n-- verifying file... done\n-- extracting...\n     src='/home/damian/benchmark/third_party/src/gtest-1.7.0.zip'\n     dst='/home/damian/benchmark/third_party/gtest'\n-- extracting... [tar xfz]\n-- extracting... [analysis]\n-- extracting... [rename]\n-- extracting... [clean up]\n-- extracting... done\n[ 16%] No patch step for 'googletest'\n[ 22%] No update step for 'googletest'\n[ 27%] Performing configure step for 'googletest'\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/damian/benchmark/third_party/src/googletest-build\n[ 33%] Performing build step for 'googletest'\n[ 50%] Built target gtest\n[100%] Built target gtest_main\n[ 38%] No install step for 'googletest'\n[ 44%] Completed 'googletest'\n[ 44%] Built target googletest\nScanning dependencies of target benchmark\n[ 50%] Building CXX object src/CMakeFiles/benchmark.dir/benchmark.cc.o\n[ 55%] Building CXX object src/CMakeFiles/benchmark.dir/colorprint.cc.o\n[ 61%] Building CXX object src/CMakeFiles/benchmark.dir/commandlineflags.cc.o\n[ 66%] Building CXX object src/CMakeFiles/benchmark.dir/sleep.cc.o\n[ 72%] Building CXX object src/CMakeFiles/benchmark.dir/sysinfo.cc.o\n[ 77%] Building CXX object src/CMakeFiles/benchmark.dir/walltime.cc.o\n[ 83%] Building CXX object src/CMakeFiles/benchmark.dir/re_posix.cc.o\nLinking CXX static library libbenchmark.a\n[ 83%] Built target benchmark\nScanning dependencies of target benchmark_re\n[ 88%] Building CXX object src/CMakeFiles/benchmark_re.dir/re_posix.cc.o\nLinking CXX static library libbenchmark_re.a\n[ 88%] Built target benchmark_re\n[ 94%] Building CXX object test/CMakeFiles/benchmark_test.dir/benchmark_test.cc.o\nLinking CXX executable benchmark_test\n[ 94%] Built target benchmark_test\n[100%] Building CXX object test/CMakeFiles/re_test.dir/re_test.cc.o\nLinking CXX executable re_test\n[100%] Built target re_test\nIf you have some idea about info you need, let me know. Reverting back to previous question - now it's confirmed that example was compiled with clang, but library itself with gnu\n. I also tried compiling library with export CXX=/usr/bin/clang++; export CC=/usr/bin/clang but result is exactly the same (i mean error during runtime)\n. ldd --version\nldd (Ubuntu EGLIBC 2.19-0ubuntu6.3) 2.19\nIf this is what you are asking for :) I did not do any standard library variations, except of installing libc++ but AFAIK it's not being used by default\n. That's exactly what I did when I mentioned exporting CXX with clang value (make clean, removed cmake files, run cmake, make) \nUnfortunately result is the same. Have no idea what can be the problem, gtest works just fine so it looks to be specific to some library  which benchmark depends on\n. ",
    "gbaier": "I had the same problem, running Ubuntu 14.04 and using g++ 4.8.2:\nWhat did the trick, at least for me, was linking against lpthread:\ng++ -o google_benchmark google_benchmark.cpp -Wall -Wextra --std=c++11 -lbenchmark -lpthread\n. ",
    "odeits": "g++ -o google_benchmark google_benchmark.cpp -Wall -Wextra --std=c++11 -lbenchmark -lpthread worked but\ng++ -o google_benchmark google_benchmark.cpp -Wall -Wextra --std=c++11  -lpthread -lbenchmark did not... apparently if you link pthreads BEFORE benchmark bad things happen.\n. https://github.com/odeits/test_benchmarks/\n. ```\ncmake version 3.5.2\nCMake suite maintained and supported by Kitware (kitware.com/cmake).\n.\nodeits@hpdt100251:~/test_benchmarks$ cmake -DCMAKE_VERBOSE_MAKEFILE=ON .\n-- The CXX compiler identification is GNU 4.8.4\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found benchmark: /usr/local/lib/libbenchmark.a\n-- Looking for C++ include pthread.h\n-- Looking for C++ include pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/odeits/test_benchmarks\nodeits@hpdt100251:~/test_benchmarks$ make\n/usr/local/bin/cmake -H/home/odeits/test_benchmarks -B/home/odeits/test_benchmarks --check-build-system CMakeFiles/Makefile.cmake 0\n/usr/local/bin/cmake -E cmake_progress_start /home/odeits/test_benchmarks/CMakeFiles /home/odeits/test_benchmarks/CMakeFiles/progress.marks\nmake -f CMakeFiles/Makefile2 all\nmake[1]: Entering directory /home/odeits/test_benchmarks'\nmake -f CMakeFiles/bench.dir/build.make CMakeFiles/bench.dir/depend\nmake[2]: Entering directory/home/odeits/test_benchmarks'\ncd /home/odeits/test_benchmarks && /usr/local/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/odeits/test_benchmarks /home/odeits/test_benchmarks /home/odeits/test_benchmarks /home/odeits/test_benchmarks /home/odeits/test_benchmarks/CMakeFiles/bench.dir/DependInfo.cmake --color=\nScanning dependencies of target bench\nmake[2]: Leaving directory /home/odeits/test_benchmarks'\nmake -f CMakeFiles/bench.dir/build.make CMakeFiles/bench.dir/build\nmake[2]: Entering directory/home/odeits/test_benchmarks'\n[ 50%] Building CXX object CMakeFiles/bench.dir/main.cpp.o\n/usr/bin/c++    -I/usr/local/include  -std=gnu++11 -o CMakeFiles/bench.dir/main.cpp.o -c /home/odeits/test_benchmarks/main.cpp\n[100%] Linking CXX executable bench\n/usr/local/bin/cmake -E cmake_link_script CMakeFiles/bench.dir/link.txt --verbose=1\n/usr/bin/c++      CMakeFiles/bench.dir/main.cpp.o  -o bench -lpthread /usr/local/lib/libbenchmark.a\nmake[2]: Leaving directory /home/odeits/test_benchmarks'\n[100%] Built target bench\nmake[1]: Leaving directory/home/odeits/test_benchmarks'\n/usr/local/bin/cmake -E cmake_progress_start /home/odeits/test_benchmarks/CMakeFiles 0\n```\n. Reversing the order fixed it. Thank you. You can mark this as closed.\n. ",
    "jwakely": "@odeits you should use GCC's -pthread option instead of linking with -lpthread (GCC will ensure it's at the right place in the link command).. This seems to be fixed:\nhttps://github.com/google/benchmark/blob/a96ff121b34532bb007c51ffd8e626e38decd732/src/benchmark_register.cc#L178. Not fixed: \nJianXiong < Joao (in AUTHORS too)\nKai < Kaito\nTobias < Tom\n. :-)  Thanks - I will try to keep them sorted if/when I add myself!. I don't understand why a Travis check failed, but it doesn't seem to be caused by my change (which only touches the README).. IMHO referring to issue #67 here isn't actually very helpful, since that issue contains lots of back and forth trying to identify the problem, when the solution is simply \"you need to use -pthread\".. I don't understand the problem with my email address, it's the same as it was when it got committed to master. My github account has several email addresses linked to it, including the ones used for these commits. My understanding is that I'm covered by Red Hat's corporate CLA via my membership of a particular group, I don't think I've signed an individual CLA with any email address.. ",
    "romange": "Sorry, it happens on the other test\nwhile (state.KeepRunning()) {\n    sink_result(time(NULL));\n  }\nwhich probably happens due to almost no-op code in the loop. Still framework code should be protected against this\n. ",
    "mrdomino": "It turns out to be sufficient to add -Wshorten-64-to-32 to the compilation flags, at least on my system:\n```\ndiff --git a/CMakeLists.txt b/CMakeLists.txt\nindex 480d8a5..d46c0dd 100644\n--- a/CMakeLists.txt\n+++ b/CMakeLists.txt\n@@ -33,6 +33,7 @@ endif()\n# Turn compiler warnings up to 11\n include(AddCXXCompilerFlag)\n+add_cxx_compiler_flag(-Wshorten-64-to-32)\n add_cxx_compiler_flag(-Wall)\n add_cxx_compiler_flag(-Wshadow)\n add_cxx_compiler_flag(-Werror)\n```\nThe resulting warnings (treated as errors):\n[ 50%] Building CXX object src/CMakeFiles/benchmark.dir/benchmark.cc.o\nIn file included from /usr/local/src/benchmark/src/benchmark.cc:15:\n/usr/local/src/benchmark/include/benchmark/benchmark.h:230:35: error: implicit conversion loses integer precision: 'const int64_t' (aka 'const long long') to 'int' [-Werror,-Wshorten-64-to-32]\n  int iterations() const { return total_iterations_; }\n                           ~~~~~~ ^~~~~~~~~~~~~~~~~\n/usr/local/src/benchmark/src/benchmark.cc:119:23: error: implicit conversion loses integer precision: 'unsigned long' to 'int' [-Werror,-Wshorten-64-to-32]\n        *exponent = i + 1;\n                  ~ ~~^~~\n/usr/local/src/benchmark/src/benchmark.cc:133:24: error: implicit conversion loses integer precision: 'unsigned long' to 'int' [-Werror,-Wshorten-64-to-32]\n        *exponent = -i - 1;\n                  ~ ~~~^~~\n/usr/local/src/benchmark/src/benchmark.cc:351:14: error: implicit conversion loses integer precision: 'size_t' (aka 'unsigned long') to 'int' [-Werror,-Wshorten-64-to-32]\n      return index;\n      ~~~~~~ ^~~~~\n/usr/local/src/benchmark/src/benchmark.cc:354:15: error: implicit conversion loses integer precision: 'size_type' (aka 'unsigned long') to 'int' [-Werror,-Wshorten-64-to-32]\n  int index = families_.size();\n      ~~~~~   ^~~~~~~~~~~~~~~~\n/usr/local/src/benchmark/src/benchmark.cc:397:54: error: implicit conversion loses integer precision: 'size_t' (aka 'unsigned long') to 'int' [-Werror,-Wshorten-64-to-32]\n        instances = family->CreateBenchmarkInstances(x, Benchmark::kNoRange);\n                    ~~~~~~                           ^\n/usr/local/src/benchmark/src/benchmark.cc:404:56: error: implicit conversion loses integer precision: 'size_t' (aka 'unsigned long') to 'int' [-Werror,-Wshorten-64-to-32]\n          instances = family->CreateBenchmarkInstances(x, y);\n                      ~~~~~~                           ^\n/usr/local/src/benchmark/src/benchmark.cc:404:59: error: implicit conversion loses integer precision: 'size_t' (aka 'unsigned long') to 'int' [-Werror,-Wshorten-64-to-32]\n          instances = family->CreateBenchmarkInstances(x, y);\n                      ~~~~~~                              ^\n/usr/local/src/benchmark/src/benchmark.cc:1239:65: error: implicit conversion loses integer precision: 'unsigned long' to 'const int' [-Werror,-Wshorten-64-to-32]\n          std::max<int>(name_field_width, benchmark.name.size() + 17);\n          ~~~                             ~~~~~~~~~~~~~~~~~~~~~~^~~~\n/usr/local/src/benchmark/src/benchmark.cc:1242:65: error: implicit conversion loses integer precision: 'unsigned long' to 'const int' [-Werror,-Wshorten-64-to-32]\n          std::max<int>(name_field_width, benchmark.name.size() + 10);\n          ~~~                             ~~~~~~~~~~~~~~~~~~~~~~^~~~\n/usr/local/src/benchmark/src/benchmark.cc:1245:65: error: implicit conversion loses integer precision: 'unsigned long' to 'const int' [-Werror,-Wshorten-64-to-32]\n          std::max<int>(name_field_width, benchmark.name.size() + 7);\n          ~~~                             ~~~~~~~~~~~~~~~~~~~~~~^~~\n/usr/local/src/benchmark/src/benchmark.cc:1247:58: error: implicit conversion loses integer precision: 'size_type' (aka 'unsigned long') to 'const int' [-Werror,-Wshorten-64-to-32]\n      name_field_width = std::max<int>(name_field_width, benchmark.name.size());\n                         ~~~                             ^~~~~~~~~~~~~~~~~~~~~\n12 errors generated.\nmake[2]: *** [src/CMakeFiles/benchmark.dir/benchmark.cc.o] Error 1\n. Thanks!\n. Actually, there's one more:\n../../3rdparty/benchmark/src/sleep.cc:30:36: error: implicit conversion loses integer precision: 'long long' to '__darwin_time_t' (aka 'long') [-Werror,-Wshorten-64-to-32]\n  sleep_time.tv_sec = microseconds / kNumMicrosPerSecond;\n                    ~ ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~\n. ",
    "pdavydov108": "I sent a pull request, maybe I did something wrong, cause it didn't appear here..\n. No, I haven't. It's written in man 3 sysctl that this headers should be included in this order, so I thought freebsd developers did it intentionally for some reason.\n. ",
    "loverszhaokai": "Maybe it's ok\n. ",
    "budziq": "Thanks for the heads up :) I just stumbled on this repository while searching for a C++ benchmark and always on Google Test download in CMake was seriously problematic for my deployment. \nI admit I haven't read through the PRs as none of the titles seamed relevant. I'm pulling the PR then.\n. ",
    "googlebot": "Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. t looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n need_author_consent \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n need_author_consent \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n need_author_consent \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n need_author_consent \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n need_author_consent \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\nGooglers can find more info about SignCLA and this PR by following this link.\n need_author_consent \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\nGooglers can find more info about SignCLA and this PR by following this link.\n need_author_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the cla label to yes (if enabled on your project), and then merge this pull request when appropriate.\nGooglers can find more info about SignCLA and this PR by following this link.\n need_author_consent \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\nGooglers can find more info about SignCLA and this PR by following this link.\n need_sender_cla \n. CLAs look good, thanks!\nGooglers can find more info about SignCLA and this PR by following this link.\n ok \n. \nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\nThe email used to register you as an authorized contributor must also be attached to your GitHub account.\n\n\u2139\ufe0f Googlers: Go here for more info.\n need_sender_cla \n. CLAs look good, thanks!\n\u2139\ufe0f Googlers: Go here for more info.\n ok \n. ",
    "rryan": "Greetings! I'm hitting this at HEAD (0a1f4849a6a6d704036f1189f62ffd7c8f061b06) on OS X 10.10.5 built with Xcode 7.  Was the switch to std::chrono supposed to fix this?\nTo build:\nbash\n$ cmake .\n$ make\n$ test/benchmark_test\nRun on (8 X 1000 MHz CPU s)\n2015-12-11 10:35:06\n***WARNING*** Library was built as DEBUG. Timings may be affected.\nBenchmark                                  Time(ns)    CPU(ns) Iterations\n-------------------------------------------------------------------------\nBM_Factorial                                     68         31   21298801                                 40320\nBM_Factorial/real_time                           30         30  108857330                                 40320\nBM_CalculatePiRange/1                            10         15   44374291                                 0\nBM_CalculatePiRange/8                           167         76    9087133                                 3.28374\nBM_CalculatePiRange/64                         1555        703     955462                                 3.15746\nBM_CalculatePiRange/512                      -10203       5727     119834                                 3.14355\nBM_CalculatePiRange/4k                       111288      50000      15716                                 3.14184\nBM_CalculatePiRange/32k                      899764     406529       1918                                 3.14162\nBM_CalculatePiRange/256k                   -5828697    3219161        205                                 3.1416\nBM_CalculatePiRange/1024k                  27948368   12598333         60                                 3.14159\nBM_CalculatePi/threads:8                       4502      13980      50184\nBM_CalculatePi/threads:1                     -12347      12085      56291\nBM_CalculatePi/threads:2                      12794      11551      59820\nBM_CalculatePi/threads:4                       7432      12525      54728\nBM_CalculatePi/threads:8                       4722      13901      50080\nBM_CalculatePi/threads:16                      4102      14109      50032\nBM_CalculatePi/threads:32                      4303      14111      49632\nBM_CalculatePi/threads:8                       4451      13941      50448\nBM_SetInsert/1024/1                          193560      87401       8503  44.6936kB/s   11.1734k items/s\nBM_SetInsert/1024/8                         -112382      90893       7409   343.81kB/s   85.9525k items/s\nBM_SetInsert/1024/10                         219552      98186       7918  397.842kB/s   99.4606k items/s\nBM_SetInsert/4k/1                            738581     334901       1992  11.6639kB/s   2.91598k items/s\nBM_SetInsert/4k/8                            796059     354221       1827  88.2217kB/s   22.0554k items/s\nBM_SetInsert/4k/10                           821261     368719       1994  105.941kB/s   26.4853k items/s\nBM_SetInsert/8k/1                           1748361     788675        984  4.95293kB/s   1.23823k items/s\nBM_SetInsert/8k/8                           1677774     759626        807  41.1387kB/s   10.2847k items/s\nBM_SetInsert/8k/10                          1595644     722654        913  54.0542kB/s   13.5136k items/s\nBM_Sequential<std::vector<int>,int>/1           -42         20   34636319  188.145MB/s   47.0363M items/s\nBM_Sequential<std::vector<int>,int>/8          2870       1302     548809  23.4332MB/s   5.85831M items/s\nBM_Sequential<std::vector<int>,int>/64        -5070       4289     169837  56.9228MB/s   14.2307M items/s\nBM_Sequential<std::vector<int>,int>/512       43827      19887      31327  98.2134MB/s   24.5533M items/s\nBM_Sequential<std::vector<int>,int>/1024      84883      38187      17398  102.292MB/s   25.5731M items/s\nBM_Sequential<std::list<int>>/1                  70         32   21494877  120.194MB/s   30.0486M items/s\nBM_Sequential<std::list<int>>/8               -2584       1283     576697  23.7895MB/s   5.94737M items/s\nBM_Sequential<std::list<int>>/64              24375      11101      60897   21.993MB/s   5.49826M items/s\nBM_Sequential<std::list<int>>/512            -62294      86000       7337  22.7108MB/s    5.6777M items/s\nBM_Sequential<std::list<int>>/1024           482911     217501       3501  17.9597MB/s   4.48992M items/s\nBM_Sequential<std::vector<int>, int>/512     -22571      21844      32839  89.4131MB/s   22.3533M items/s\nBM_StringCompare/1                              106         48   12083132\nBM_StringCompare/8                              -41         47   15526056\nBM_StringCompare/64                              96         44   17566401\nBM_StringCompare/512                            -72         61   11471273\nBM_StringCompare/4k                             556        251    3057543\nBM_StringCompare/32k                          -1490       1402     490729\nBM_StringCompare/256k                         30787      14018      55813\nBM_StringCompare/1024k                       120152      54691      12047\nBM_SetupTeardown/threads:8                     7545       3595     188656\nBM_LongTest/64k                              519825     235864       3422\nBM_LongTest/256k                           -1899211     967960        743\nBM_LongTest/2M                             17610425    7934686         86\nBM_LongTest/16M                           -24508635   58078500         12\nBM_LongTest/128M                          978582740  444864000          2\nBM_LongTest/256M                         -567115784  994492000          1\n. @EricWF @dominichamon should I open a new bug or do you think this is related?\n. Thanks @EricWF / @dominichamon  -- I'll split this into a separate issue. Happy to help debug!\n. BTW, this is using mach_absolute_time, not std::chrono.\n-- LOG(1): Reading /proc/self/cputime_ns failed. Using getrusage().\n-- LOG(1): Using the CPU cycle clock to provide walltime::Now().\n. Looks like mach_absolute_time may use rdtsc in some cases:\nhttp://www.opensource.apple.com/source/Libc/Libc-320.1.3/i386/mach/mach_absolute_time.c\n. Can't reproduce anymore with f662e8b -- so looks like that did it @EricWF. Thanks to both of you!\n. ",
    "geoffromer": "@mattyclarkson Thanks, I've re-synced to pick up that change.\n. Squashed all my changes into a single commit. Sorry if that messed anything up; I'm new at Git collaboration.\n. OK, I've updated this to require cmake 2.8.12.\n. @EricWF the description of #117 indicates that you're upgrading to CMake 3.2. Is that right?\n. It's also possible to leave the minimum at 2.8; it just means we'd have to drop INTERFACE from the target_link_libraries line. Would you prefer that?\n. OK, dropped the INTERFACE.\n@mattyclarkson I don't know cmake well enough to confidently pull that off, and it seems like unnecessary complexity.\n. Why not? IIUC, static linking only links the pieces that are needed, so if benchmark doesn't use pthread, this will have no effect.\n. Oh, OK, I thought linking pthread statically was the intent of this line. If the goal is just to add pthread to the link interface, why are the static and shared cases treated differently?\n. So wait, does BUILD_SHARED_LIBS control whether pthread is shared, or whether benchmark is shared, or both? I'd been assuming it at least controlled benchmark, but your answer makes it sound like that's not the case.\n. OK, I remain confused, but I'm out of my depth here, so I'm happy to do what you tell me. But how can I \"avoid linking pthread as a static library\"?\n. Actually, the cmake documentation seems to be saying that PUBLIC/PRIVATE/INTERFACE is only relevant for shared libraries (and dependencies are always propagated when linking statically), so I think this can be simplified a lot. What do you think of this?\n. Judging from the gcc documentation, gcc will link against shared libs when available, unless you pass -static. CMake doesn't seem to pass that flag, so I think this will not statically link pthread even when building static libs, and so it should work fine even without INTERFACE.\n. ",
    "michaelbacci": "Hello Dominic,\nI was creating a new branch with the minimal modification that you suggested, but I realized that there are some problems.\n1)BenchmarkReporter::ComputeStats() run only for more than two reports.\n2)for every line of single report I need to compute the min/max (if command line flag enabled), and is not possible just using BenchmarkReporter::ComputeStats() (look the (1) problem).\nI've pushed my code to the repository to help other people like me that need to known the real min/max performance, and not the mean of tot_consumed_time/tot_iteration.\nWe can try to calculate the real min/max only calculating for every iteration of \"while(State::KeepRunning()) { my_code(); }\" the real consumed time.\nSo, all the changes I've done, are necessary from my point of view. Maybe I can change programmation style, name of function/variables etc...but the main problem I tryed to solve is calculate the real min/max and it's possible only looking at each iteration of State::KeepRunning() and not using the information collected after the benchmark.\nAbout ComputeStats() I followed your correct suggestion, and now the code print for the ${benchmark}_mean, the min/max related to the statistics and not the absolute min/max: that's more correct.\nBest Regards and thanks for feedbacks\n. It depend if the user choose the UseRealTIme() or not.\nFor every State::KeepRunning() I calculate the min&max values (vector for threads), that it will be used in the RunBenchmark() and stored following the logic of the flag use_real_time.\n. correct!\n. good to know.\n. that's right\n. done\n. ",
    "coveralls": "\nChanges Unknown when pulling ef479921251d6104ca590ddf0d85c331fdbf803a on mattyclarkson:coverage into * on google:master*.\n. \nCoverage remained the same at 67.44% when pulling 68e69213e76ab1b3acf65db00253face9312e235 on dcoeurjo:cmakeEmptybuildtype into d18ebcb99a8ac64a9af280e65b33a539e23de4d7 on google:master.\n. \nCoverage decreased (-0.83%) to 66.61% when pulling 49112c6b8e91b922e524292bbcd35f29f81b639c on nickhutchinson:auto-color-output into f6c2ea7fef6ba01f405e2c078cba9f58edfca966 on google:master.\n. \nCoverage decreased (-0.35%) to 67.09% when pulling 523fdf1fd4dc7db30c30dea7216082b628743e28 on nickhutchinson:auto-color-output into f6c2ea7fef6ba01f405e2c078cba9f58edfca966 on google:master.\n. \nCoverage decreased (-0.3%) to 86.948% when pulling 6d69db7d7a174bd96e7e36fd0fb156476de89c18 on nickhutchinson:auto-color-output into 72be9523bb88d5b96e3891776fad18b790bfd2d2 on google:master.\n. \nCoverage decreased (-0.1%) to 86.817% when pulling 1be8b7324d033b95043188ffb44edf4c2524b5e1 on nickhutchinson:auto-color-output into c6f3f0eb9cd68150371c0c45b84aeb0dc72114c9 on google:master.\n. \nCoverage remained the same at 67.44% when pulling 9afa6c986c978c8f649073e914bc8f212645d6bf on myd7349:master into f6c2ea7fef6ba01f405e2c078cba9f58edfca966 on google:master.\n. \nCoverage increased (+0.2%) to 68.525% when pulling cded70a1660e81f854b5d41795a514ec0825c32a on NewProggie:feature/add-ms-time-report into b2e734087532897b7bb4c51a6b4f503060c9a20f on google:master.\n. \nCoverage increased (+0.2%) to 68.525% when pulling 3a02c462c795fd60e2620fc78c6f8c5b63aa1764 on NewProggie:feature/add-ms-time-report into b2e734087532897b7bb4c51a6b4f503060c9a20f on google:master.\n. \nCoverage increased (+0.9%) to 69.231% when pulling 7c69b36078b5773fbd6b09b539a30400138607a7 on NewProggie:feature/add-ms-time-report into b2e734087532897b7bb4c51a6b4f503060c9a20f on google:master.\n. \nCoverage increased (+0.9%) to 69.231% when pulling 0b4111c3b31db8806e0c3960c7f1f541b20cdb8b on NewProggie:feature/add-ms-time-report into b2e734087532897b7bb4c51a6b4f503060c9a20f on google:master.\n. \nCoverage increased (+0.7%) to 69.005% when pulling f352c30f1cc4312760c5d704433945a492276947 on NewProggie:feature/add-ms-time-report into 0500ec0ab877f4eebaefd4ec89a391ed22c95ef2 on google:master.\n. \nCoverage increased (+0.1%) to 68.464% when pulling fb733897c5eea5b9a04ebed5cfae37d608262129 on NewProggie:feature/add-ms-time-report into 0500ec0ab877f4eebaefd4ec89a391ed22c95ef2 on google:master.\n. \nCoverage decreased (-9.9%) to 58.426% when pulling 3bfbc9419648b77c93092c5267bf4914f11387ce on LepelTsmok:master into b2e734087532897b7bb4c51a6b4f503060c9a20f on google:master.\n. \nCoverage increased (+0.001%) to 68.352% when pulling 5b29510a6ff22f40af818cc5f9055cef44349ec0 on LepelTsmok:master into b2e734087532897b7bb4c51a6b4f503060c9a20f on google:master.\n. \nCoverage increased (+0.001%) to 68.352% when pulling 4c406921b6211401aac97d0a67bdb997c1de9807 on LepelTsmok:master into b2e734087532897b7bb4c51a6b4f503060c9a20f on google:master.\n. \nCoverage increased (+0.001%) to 68.352% when pulling 4ed6e85d12d0f10c6ae2b6f16d5b817bbce2cb01 on LepelTsmok:master into b2e734087532897b7bb4c51a6b4f503060c9a20f on google:master.\n. \nCoverage increased (+0.05%) to 68.404% when pulling 927e787dd42b152d22a5c099f58bd505519019c2 on LepelTsmok:master into b2e734087532897b7bb4c51a6b4f503060c9a20f on google:master.\n. \nCoverage increased (+0.06%) to 68.408% when pulling 493cfcbe981b303334efa99c0b276ea95ac90c03 on LepelTsmok:master into b2e734087532897b7bb4c51a6b4f503060c9a20f on google:master.\n. \nCoverage remained the same at 68.351% when pulling 838719dc02d1104153fb4193acea32a64d5559e7 on dvyukov:patch-1 into b2e734087532897b7bb4c51a6b4f503060c9a20f on google:master.\n. \nCoverage remained the same at 68.351% when pulling d6f96ed6399bd943be7592e1114bf0ddf64d68b0 on iterationdoc into 075a8a6fbf90edeb653865657b8caa1854e4e411 on master.\n. \nCoverage increased (+0.8%) to 69.169% when pulling d4d4c9d0da56be4b5f6c8e9c394c39cbd40a597f on jknuuttila:master into 0500ec0ab877f4eebaefd4ec89a391ed22c95ef2 on google:master.\n. \nCoverage decreased (-0.3%) to 68.045% when pulling 083316d142f94fba8c4218c64ce575571ea7e749 on jknuuttila:master into 0500ec0ab877f4eebaefd4ec89a391ed22c95ef2 on google:master.\n. \nCoverage increased (+0.8%) to 69.462% when pulling dcf832fc0d63707b55476e1f1f84b89da34dd30d on jknuuttila:master into 277e7aafe693e3617c0911bc5defbc0804326cb4 on google:master.\n. \nCoverage increased (+0.8%) to 69.462% when pulling e253a284029c34f764fc26bb7859298d4ec0551b on jknuuttila:master into 277e7aafe693e3617c0911bc5defbc0804326cb4 on google:master.\n. \nCoverage remained the same at 69.462% when pulling 09edc486b851ab948cc859e2c4a85a189378b284 on BillyONeal:master into 354b14d1a0770da28f1725fa8409af3264de3a79 on google:master.\n. \nCoverage remained the same at 69.462% when pulling 24f710de937482c7f5ddc6023d0ed08a2d050534 on BillyONeal:master into 354b14d1a0770da28f1725fa8409af3264de3a79 on google:master.\n. \nCoverage remained the same at 69.462% when pulling f03ce669d1c8c23b0ee6bd8764419034a7bbad0d on BillyONeal:master into 354b14d1a0770da28f1725fa8409af3264de3a79 on google:master.\n. \nCoverage remained the same at 69.462% when pulling 6f8a974c22dd0df80dca75193f05447ef45a66ee on BillyONeal:master into 354b14d1a0770da28f1725fa8409af3264de3a79 on google:master.\n. \nCoverage remained the same at 69.462% when pulling 09edc486b851ab948cc859e2c4a85a189378b284 on BillyONeal:msvc-cli-warnings into 354b14d1a0770da28f1725fa8409af3264de3a79 on google:master.\n. \nCoverage remained the same at 69.462% when pulling df9ab80113a890c38ff93ef37699078eeceb29fc on BillyONeal:fix-appveyor into 354b14d1a0770da28f1725fa8409af3264de3a79 on google:master.\n. \nCoverage remained the same at 69.462% when pulling fa0e7ef8c66f85007170e69f28afe9e1b86a957f on BillyONeal:travis-init into 336fd111f56b60ff5b622041bc1ba60856dae787 on google:master.\n. \nCoverage increased (+0.2%) to 69.654% when pulling c60eefdbb78b29522568b1055941f3c20c843e78 on ismaelJimenez:master into 360e66c1c4777c99402cf8cd535aa510fee16573 on google:master.\n. \nCoverage increased (+0.2%) to 69.654% when pulling d2103de3d390715845f0cb42d3f070a039814580 on ismaelJimenez:master into 360e66c1c4777c99402cf8cd535aa510fee16573 on google:master.\n. \nCoverage remained the same at 69.654% when pulling 0e4ac320dd71d689cb9cd40033fb3b987fb923cc on zabereer:issue213 into 0a60062b3e8ae0376e6fcd41c5da50714a9ef47b on google:master.\n. \nCoverage remained the same at 69.654% when pulling 0e4ac320dd71d689cb9cd40033fb3b987fb923cc on zabereer:issue213 into 0a60062b3e8ae0376e6fcd41c5da50714a9ef47b on google:master.\n. \nCoverage increased (+1.2%) to 70.883% when pulling 8afbf0ed3801ad12c4066d10e9d25764181321f4 on ismaelJimenez:complexity into 0a60062b3e8ae0376e6fcd41c5da50714a9ef47b on google:master.\n. \nCoverage increased (+1.2%) to 70.883% when pulling 8afbf0ed3801ad12c4066d10e9d25764181321f4 on ismaelJimenez:complexity into 0a60062b3e8ae0376e6fcd41c5da50714a9ef47b on google:master.\n. \nCoverage increased (+1.2%) to 70.851% when pulling ac05c045335d3e32ec75e3aae930ecc1c6533212 on ismaelJimenez:complexity into 0a60062b3e8ae0376e6fcd41c5da50714a9ef47b on google:master.\n. \nCoverage increased (+1.2%) to 70.851% when pulling 266addc3f51f07ec182ed34af06e6d75b0f6d09f on ismaelJimenez:complexity into 0a60062b3e8ae0376e6fcd41c5da50714a9ef47b on google:master.\n. \nCoverage increased (+1.2%) to 70.851% when pulling ea69a8479046413d96b0eb826f1d982985281a67 on ismaelJimenez:complexity into 0a60062b3e8ae0376e6fcd41c5da50714a9ef47b on google:master.\n. \nCoverage increased (+1.2%) to 70.851% when pulling ea69a8479046413d96b0eb826f1d982985281a67 on ismaelJimenez:complexity into 0a60062b3e8ae0376e6fcd41c5da50714a9ef47b on google:master.\n. \nCoverage increased (+1.2%) to 70.851% when pulling d577987fd76595cb52602bd75b2866886e95b0f2 on ismaelJimenez:complexity into 0a60062b3e8ae0376e6fcd41c5da50714a9ef47b on google:master.\n. \nCoverage increased (+1.2%) to 70.851% when pulling 43ef17441cc8767f5523031878a2f43ab1d7790b on ismaelJimenez:complexity into 0a60062b3e8ae0376e6fcd41c5da50714a9ef47b on google:master.\n. \nCoverage increased (+1.04%) to 70.692% when pulling 266b3bd635a37b28d6e92125c615d3e17f5022ea on ismaelJimenez:complexity into 0a60062b3e8ae0376e6fcd41c5da50714a9ef47b on google:master.\n. \nCoverage increased (+1.04%) to 70.692% when pulling 0c23d2852f58dd8e264f67045fd6e454cf481c5c on ismaelJimenez:complexity into 0a60062b3e8ae0376e6fcd41c5da50714a9ef47b on google:master.\n. \nCoverage increased (+1.04%) to 70.692% when pulling 855786acf518db0162779f8196d930820e4f6b8c on ismaelJimenez:complexity into 0a60062b3e8ae0376e6fcd41c5da50714a9ef47b on google:master.\n. \nCoverage increased (+1.6%) to 71.246% when pulling 36a9ae197f220df65ef7ed1a21015a8fb4ef1ece on ismaelJimenez:complexity into 0a60062b3e8ae0376e6fcd41c5da50714a9ef47b on google:master.\n. \nCoverage increased (+1.6%) to 71.246% when pulling e5cf020d9730d24391e1cd1dfb37aef2c163d82c on ismaelJimenez:complexity into 0a60062b3e8ae0376e6fcd41c5da50714a9ef47b on google:master.\n. \nCoverage increased (+1.6%) to 71.246% when pulling 5e10e120db2ddf36d75e910ec2c77adebbf7543f on ismaelJimenez:complexity into 0a60062b3e8ae0376e6fcd41c5da50714a9ef47b on google:master.\n. \nCoverage increased (+1.6%) to 71.246% when pulling a24ef95e1199c463bb088d321caa1b0591817813 on ismaelJimenez:complexity into 0a60062b3e8ae0376e6fcd41c5da50714a9ef47b on google:master.\n. \nCoverage increased (+1.02%) to 70.671% when pulling 029f37446daeef98db8dc3b26165405a1df2cc80 on efcs:diagnostic-checks into f434ce3fb650d40db186780ea3506269d5035ffd on google:master.\n. \nCoverage increased (+1.2%) to 70.899% when pulling 43017f8b1510a50855e6b0ea145a534d3d754068 on efcs:skip_with_error into f434ce3fb650d40db186780ea3506269d5035ffd on google:master.\n. \nCoverage increased (+1.2%) to 70.899% when pulling 90c9ab1d8e0a44d229dae6b4f5f6355161de761a on efcs:skip_with_error into f434ce3fb650d40db186780ea3506269d5035ffd on google:master.\n. \nCoverage increased (+0.04%) to 72.207% when pulling bdeb38718e23468613c000463e7e42e3b5516b23 on efcs:skip_with_error into 2440b752fd335d00349b6dd77d67e5a6401565fb on google:master.\n. \nCoverage increased (+0.04%) to 72.207% when pulling 924b8cee7af0a302ecb07b3429760c4a906460c7 on efcs:skip_with_error into 2440b752fd335d00349b6dd77d67e5a6401565fb on google:master.\n. \nCoverage decreased (-0.3%) to 71.864% when pulling b195f82554819d7e152e2b32b39220b85af18cf7 on efcs:skip_with_error into 1080b17bf5d0a093e5aa6fb2a474df443e05bb82 on google:master.\n. \nCoverage decreased (-0.3%) to 71.856% when pulling ee8e37c67db9a717059f9f7bdfb83549208d65e9 on efcs:skip_with_error into 1080b17bf5d0a093e5aa6fb2a474df443e05bb82 on google:master.\n. \nCoverage decreased (-0.2%) to 72.003% when pulling 6f84ffcd8b53b035fe4430070dbca19641892def on efcs:skip_with_error into 1080b17bf5d0a093e5aa6fb2a474df443e05bb82 on google:master.\n. \nCoverage decreased (-0.5%) to 71.699% when pulling 1003a70e5fe80967fe465da54c2f924944ad9fa6 on efcs:skip_with_error into 1080b17bf5d0a093e5aa6fb2a474df443e05bb82 on google:master.\n. \nCoverage remained the same at 70.198% when pulling 206d84e389dadbbfa9559c79d1b5e6edca8baf4a on zabereer:donotoptimize into 9341d705a14afed7c4e0f8a22b1e6e8a6259fcc4 on google:master.\n. \nCoverage increased (+11.5%) to 87.784% when pulling 013b755d74b1cbe166968ca07d572f459c474e45 on efcs:fix-reporters into a38f022b5a2b21c272c672f2d7651e1bee1c18bf on google:master.\n. \nCoverage increased (+11.5%) to 87.784% when pulling 013b755d74b1cbe166968ca07d572f459c474e45 on efcs:fix-reporters into a38f022b5a2b21c272c672f2d7651e1bee1c18bf on google:master.\n. \nCoverage increased (+11.5%) to 87.8% when pulling 535c63fb1e16f8c428dd3d19631761f543713750 on efcs:fix-reporters into a38f022b5a2b21c272c672f2d7651e1bee1c18bf on google:master.\n. \nCoverage increased (+11.5%) to 87.808% when pulling 87812992867f225071f836529f670406dcec54d2 on efcs:fix-reporters into 3f7a9c76fbbd0a4b8f54f866ccbcc4d6871c7cd1 on google:master.\n. \nCoverage remained the same at 76.891% when pulling fc1f897011250587cb146cfc081df7fbb5ec904b on zabereer:donotoptimize_with_test into 84bc4d703b6f27a0bdcb48443c9bca3f60e1818c on google:master.\n. \nCoverage decreased (-0.6%) to 76.321% when pulling b217741210ff27cf48d18d6b5348c8929f251617 on efcs:lambda-benchmarks into 84bc4d703b6f27a0bdcb48443c9bca3f60e1818c on google:master.\n. \nCoverage remained the same at 76.337% when pulling f26d36beb4df6db810cf51a24fd6c3fe2fcd6ba1 on efcs:lambda-benchmarks into 3f7a9c76fbbd0a4b8f54f866ccbcc4d6871c7cd1 on google:master.\n. \nCoverage increased (+0.02%) to 76.907% when pulling 171588561112744263caa5847847e76e9bbde562 on ismaelJimenez:update_complexity into 84bc4d703b6f27a0bdcb48443c9bca3f60e1818c on google:master.\n. \nCoverage decreased (-0.6%) to 76.337% when pulling 37ab858e4b245a49805b01358655fab069474a7c on ismaelJimenez:update_complexity into 84bc4d703b6f27a0bdcb48443c9bca3f60e1818c on google:master.\n. \nCoverage decreased (-0.6%) to 76.337% when pulling d82f0c313133c60e3a5db5be6f7d2299cd5ffdd8 on ismaelJimenez:update_complexity into 84bc4d703b6f27a0bdcb48443c9bca3f60e1818c on google:master.\n. \nCoverage decreased (-0.6%) to 76.337% when pulling 805e8baee9da3744428e3f646f321c29283c4072 on ismaelJimenez:update_complexity into 84bc4d703b6f27a0bdcb48443c9bca3f60e1818c on google:master.\n. \nCoverage decreased (-0.6%) to 76.337% when pulling 805e8baee9da3744428e3f646f321c29283c4072 on ismaelJimenez:update_complexity into 84bc4d703b6f27a0bdcb48443c9bca3f60e1818c on google:master.\n. \nCoverage decreased (-0.6%) to 76.337% when pulling c1c7d33279b463088550986fe6f311a3ad2faa2e on ismaelJimenez:update_complexity into 84bc4d703b6f27a0bdcb48443c9bca3f60e1818c on google:master.\n. \nCoverage remained the same at 76.321% when pulling 74e82e822f55871a969b1642019d57639d9a4eb4 on zabereer:donotoptimize_with_test_merged into a38f022b5a2b21c272c672f2d7651e1bee1c18bf on google:master.\n. \nCoverage increased (+0.5%) to 88.294% when pulling 01fab9836c85f8501f0bd4ae88f40fd03e4ed48a on efcs:cleanup-reporters into 238e558fdb6f00a2e3eb75d6c353030f8a510f8c on google:master.\n. \nCoverage increased (+0.5%) to 88.294% when pulling c547840e67a3b1dbd81410f5a2213e6481aaf994 on efcs:cleanup-reporters into 238e558fdb6f00a2e3eb75d6c353030f8a510f8c on google:master.\n. \nCoverage remained the same at 87.808% when pulling e515a66276b8cad7754e2c00292011a901462e85 on fixvscheck into 238e558fdb6f00a2e3eb75d6c353030f8a510f8c on master.\n. \nCoverage increased (+0.5%) to 88.294% when pulling f2a1e448e86891fe7138a32612434b58da7c4bc3 on fixvscheck into 238e558fdb6f00a2e3eb75d6c353030f8a510f8c on master.\n. \nCoverage increased (+0.5%) to 88.294% when pulling 0948b3895014760219a86b940b77f35141238fd4 on fixvscheck into 238e558fdb6f00a2e3eb75d6c353030f8a510f8c on master.\n. \nCoverage increased (+0.5%) to 88.294% when pulling 0948b3895014760219a86b940b77f35141238fd4 on fixvscheck into 238e558fdb6f00a2e3eb75d6c353030f8a510f8c on master.\n. \nCoverage increased (+0.5%) to 88.294% when pulling f572d6b1909ba2e1a7e5aa8e3935e9254ddb00a9 on fixvscheck into 238e558fdb6f00a2e3eb75d6c353030f8a510f8c on master.\n. \nCoverage increased (+0.5%) to 88.294% when pulling 7d694d2d7512013a2bfddf53325abee49ddb1246 on fixvscheck into 238e558fdb6f00a2e3eb75d6c353030f8a510f8c on master.\n. \nCoverage increased (+0.5%) to 88.294% when pulling 5b14d00b5dda8e5f1937adfacc6edbb756943ac8 on fixvscheck into 238e558fdb6f00a2e3eb75d6c353030f8a510f8c on master.\n. \nCoverage increased (+0.5%) to 88.294% when pulling f55649fa086b2050e03225a16a62623e4f59ec7d on fixvscheck into 238e558fdb6f00a2e3eb75d6c353030f8a510f8c on master.\n. \nCoverage increased (+0.5%) to 88.294% when pulling 1e2fca5ba7c013620f7ad3652339ab53b86fb33d on fixvscheck into 238e558fdb6f00a2e3eb75d6c353030f8a510f8c on master.\n. \nCoverage remained the same at 88.294% when pulling dd54c50fb30bc43f8eda9878814d7a4d4167d7be on BillyONeal:fixvs2013 into 74a278e206f9387dec72ec000436a18bfcb3070e on google:master.\n. \nCoverage remained the same at 88.294% when pulling 55d33b362025c2f875dfdd00b40a56fb14affe9f on BillyONeal:fixvs2013 into 3685cad68c5180e0da2afb7c05bd03422c0fe454 on google:master.\n. \nCoverage remained the same at 88.294% when pulling 0c37860fa955f982b24b7cfd4be5a32ee2b74014 on BillyONeal:fixvs2013 into 3685cad68c5180e0da2afb7c05bd03422c0fe454 on google:master.\n. \nCoverage increased (+0.05%) to 88.343% when pulling 3ef63399716c7ac213d4016ab9454422f4f9d6d1 on ismaelJimenez:added_lambdas into 84cd50b85e2c7ff384a362933446691d3c3be357 on google:master.\n. \nCoverage decreased (-0.08%) to 88.212% when pulling f964480080c3e2b3f6f437b76ccd07bbfdb92ad1 on ismaelJimenez:added_lambdas into 84cd50b85e2c7ff384a362933446691d3c3be357 on google:master.\n. \nCoverage decreased (-0.08%) to 88.212% when pulling 8ba94b4c1842d9424c592258a6dfc9beea4912c8 on ismaelJimenez:added_lambdas into 84cd50b85e2c7ff384a362933446691d3c3be357 on google:master.\n. \nCoverage decreased (-0.08%) to 88.212% when pulling 2859ae93949a7a3415082e65001f25e8e5e78284 on ismaelJimenez:added_lambdas into 84cd50b85e2c7ff384a362933446691d3c3be357 on google:master.\n. \nCoverage decreased (-0.07%) to 88.147% when pulling a98585454e9c9d7d923315ed9c10d6bc06012591 on abstract into 2d088a9f2d41acb77afc99d045f669e1a21b61ef on master.\n. \nCoverage remained the same at 88.212% when pulling f35edc8688a0e6e32f0c195cd86a9b69fa9d032f on ismaelJimenez:fix_issue_235 into 2d088a9f2d41acb77afc99d045f669e1a21b61ef on google:master.\n. \nCoverage remained the same at 88.212% when pulling bf362a552831bc5f1ce5a87df33cf2ab0459d922 on dartconfiguration into 2d088a9f2d41acb77afc99d045f669e1a21b61ef on master.\n. \nCoverage remained the same at 88.212% when pulling 9de7e14ef0c7cdc7f75307117e7078e537bcf454 on dartconfiguration into 2d088a9f2d41acb77afc99d045f669e1a21b61ef on master.\n. \nCoverage increased (+0.02%) to 88.228% when pulling 5cae9cfbb59b9a13fc51fa7e7ebaf76bd87cc1ce on dartconfiguration into 2d088a9f2d41acb77afc99d045f669e1a21b61ef on master.\n. \nCoverage remained the same at 88.212% when pulling 4cde68422244d58cfd01f67a57f3882ced2a1254 on ryanvo:patch-1 into 3fdd76bd14ff122c6881d7f15ec5cb2629241e7a on google:master.\n. \nCoverage remained the same at 88.212% when pulling 8ee598aa18899fc479f15a3f1d1546c8a095dcbc on NAThompson:patch-1 into 1d53e5e0d8d0c5b69fc19d7e95dda2c8385d03f4 on google:master.\n. \nCoverage increased (+0.008%) to 88.22% when pulling a96671e4a86c97b4711aa3846ffc158f4c6eacec on ismaelJimenez:checkComplexityN into 1d53e5e0d8d0c5b69fc19d7e95dda2c8385d03f4 on google:master.\n. \nCoverage increased (+0.008%) to 88.22% when pulling 401daf5e800cc44388ac5c15d3138f40e22d4241 on ismaelJimenez:fix244 into 1d53e5e0d8d0c5b69fc19d7e95dda2c8385d03f4 on google:master.\n. \nCoverage increased (+0.008%) to 88.22% when pulling bd172600a926f97f005583ac464912bebaf59797 on ismaelJimenez:checkForComplexityN into 1d53e5e0d8d0c5b69fc19d7e95dda2c8385d03f4 on google:master.\n. \nCoverage remained the same at 88.212% when pulling 0735abc7af508b51bbef467eb18ab5a6c8f28a9e on NAThompson:patch-2 into 1d53e5e0d8d0c5b69fc19d7e95dda2c8385d03f4 on google:master.\n. \nCoverage remained the same at 88.228% when pulling 64d00372a5a75d237fb0ef6c71c842581910abeb on steve-downey:master into 885ca41cf835313eca052ad112608631685ae6f2 on google:master.\n. \nCoverage remained the same at 88.228% when pulling 02b8a6200452e28c55d27025f440b09b86ebc2af on enh:arm-fixes into 7e40ff9e35699ea14a6addd2ce20cd23be519430 on google:master.\n. \nCoverage increased (+0.2%) to 88.432% when pulling cf0a7bc7f932f0b356423004165af1c95b4c17d1 on loganek:master into ebd37b191c757cd0b20c9c8517e38d1e1ac45069 on google:master.\n. \nCoverage increased (+0.2%) to 88.432% when pulling 62fc2313236226be75ae2c021f05a59d0c28f5bb on loganek:master into ebd37b191c757cd0b20c9c8517e38d1e1ac45069 on google:master.\n. \nCoverage increased (+0.2%) to 88.432% when pulling 332786c433b23837b65fa20ed35a03e3b1702013 on loganek:master into ebd37b191c757cd0b20c9c8517e38d1e1ac45069 on google:master.\n. \nCoverage increased (+0.2%) to 88.432% when pulling f101921c9b77f438f8a5a1550a4835a13e0fbfaa on loganek:master into ebd37b191c757cd0b20c9c8517e38d1e1ac45069 on google:master.\n. \nCoverage increased (+0.2%) to 88.432% when pulling f101921c9b77f438f8a5a1550a4835a13e0fbfaa on loganek:master into ebd37b191c757cd0b20c9c8517e38d1e1ac45069 on google:master.\n. \nCoverage increased (+0.2%) to 88.424% when pulling 743061eba5b7f75ca9390531fccf4f78981a4ce5 on loganek:master into ebd37b191c757cd0b20c9c8517e38d1e1ac45069 on google:master.\n. \nCoverage increased (+0.3%) to 88.489% when pulling e2d16f194091df76a168dffe43a7bf0677d41179 on loganek:master into ebd37b191c757cd0b20c9c8517e38d1e1ac45069 on google:master.\n. \nCoverage increased (+0.08%) to 88.308% when pulling d011f4fe9aba477465bb83bc3e2915024850c5b8 on loganek:master into ebd37b191c757cd0b20c9c8517e38d1e1ac45069 on google:master.\n. \nCoverage decreased (-0.4%) to 87.844% when pulling 124b86a6e8c66d038fa599a50ad2eabf9c4e5c77 on efcs:benchmark_output into ebd37b191c757cd0b20c9c8517e38d1e1ac45069 on google:master.\n. \nCoverage decreased (-0.4%) to 87.844% when pulling 205b9cb71d40b5ca9100cb29bea3bd195b66aae3 on efcs:benchmark_output into ebd37b191c757cd0b20c9c8517e38d1e1ac45069 on google:master.\n. \nCoverage decreased (-0.4%) to 87.852% when pulling 58aa779a1f78038d2bec9e38c38a0687720ad881 on efcs:benchmark_output into ebd37b191c757cd0b20c9c8517e38d1e1ac45069 on google:master.\n. \nCoverage remained the same at 88.228% when pulling 171a9f505c7ef6b35dda4c9583f86f055449922d on drozdvadym:support_step_in_dense_range into ebd37b191c757cd0b20c9c8517e38d1e1ac45069 on google:master.\n. \nCoverage remained the same at 88.228% when pulling 68354c4a7aad7bf3388c39d2458b3fa162de69a2 on drozdvadym:support_step_in_dense_range into ebd37b191c757cd0b20c9c8517e38d1e1ac45069 on google:master.\n. \nCoverage decreased (-0.4%) to 87.816% when pulling fe2f41f0ee6a444c155e490b9eb5d0be4ea429e1 on efcs:upgrade-travis into 1bd62bd0becbb692c7088f331c876ca3aeff4c65 on google:master.\n. \nCoverage decreased (-2.3%) to 85.917% when pulling 6c935b7374075305e9b9c3d1dbde8bdc9c88954e on biojppm:master into 950c8b74403f6f0ad962cac63efe1ccb18100692 on google:master.\n. \nCoverage decreased (-1.8%) to 86.418% when pulling 4db43b93f2c45953107a1d83843662cfe9632f6f on biojppm:master into f68e64c60a6b20a1f50046df58111107a09fd6a3 on google:master.\n. \nCoverage decreased (-1.8%) to 86.425% when pulling 399e7410afd81caad38c4df83fb29995c4b73c3a on biojppm:master into f68e64c60a6b20a1f50046df58111107a09fd6a3 on google:master.\n. \nCoverage decreased (-3.05%) to 84.357% when pulling fa6453eddad3f6e7ed9d271189976c5346fc6a8c on biojppm:master into 7f1da4a68b83fa8474ff61c13d26922866871285 on google:master.\n. \nCoverage decreased (-2.6%) to 84.853% when pulling b01d6c1014370433ef2874f4f866e2f3965ae66f on biojppm:master into 7f1da4a68b83fa8474ff61c13d26922866871285 on google:master.\n. \nCoverage decreased (-5.03%) to 82.452% when pulling 7a3ad1affddfd27e7849416879ce150585fba1b1 on biojppm:master into dfe026075480f117f424d254c2f701ac97ea4cdd on google:master.\n. \nCoverage decreased (-5.03%) to 82.452% when pulling 4ae7a323c47b601ac88d6732aec3d5d70a9706a8 on biojppm:master into dfe026075480f117f424d254c2f701ac97ea4cdd on google:master.\n. \nCoverage decreased (-5.03%) to 82.452% when pulling 4ae7a323c47b601ac88d6732aec3d5d70a9706a8 on biojppm:master into dfe026075480f117f424d254c2f701ac97ea4cdd on google:master.\n. \nCoverage decreased (-5.1%) to 82.452% when pulling 7084c63d10168520cfc2a1b5fb415382c2d8027f on biojppm:master into 95dee3c699e0c972fca9ad414f80e451e0bf4d7f on google:master.\n. \nCoverage decreased (-5.1%) to 82.452% when pulling 19ba89cbeb581ebb01cf7d4ace45bab542c8c57a on biojppm:master into 95dee3c699e0c972fca9ad414f80e451e0bf4d7f on google:master.\n. \nCoverage decreased (-5.1%) to 82.452% when pulling da89cfc0dd2334b21db07a46c8797de802eabc0b on biojppm:master into 95dee3c699e0c972fca9ad414f80e451e0bf4d7f on google:master.\n. \nCoverage decreased (-1.3%) to 86.239% when pulling 5d73d0a21f66c51783629946e075e9249d233c0d on biojppm:master into 95dee3c699e0c972fca9ad414f80e451e0bf4d7f on google:master.\n. \nCoverage decreased (-0.4%) to 87.167% when pulling 904e55df61149fd67d53fce2e71fef09aaded57d on biojppm:master into 95dee3c699e0c972fca9ad414f80e451e0bf4d7f on google:master.\n. \nCoverage decreased (-0.4%) to 87.108% when pulling 7cfff2212ee6d72bdc7561157eb13569b1b508bc on biojppm:master into 95dee3c699e0c972fca9ad414f80e451e0bf4d7f on google:master.\n. \nCoverage decreased (-0.6%) to 87.165% when pulling 777ec36a4bb0f2bcf4a9dc48be2890d506dc8d39 on biojppm:master into 61f570e82a9334ef13f794b9149013feeb1a2ab1 on google:master.\n. \nCoverage decreased (-0.5%) to 87.157% when pulling 2b6e1c44770d9c7f8a72e5e696612204731a557f on biojppm:master into 83561f05803dc2001626b61652e02192fa528690 on google:master.\n. \nCoverage decreased (-0.7%) to 86.969% when pulling fedae8fbfc0dbc7b2df3e48ae2f4b1318254777d on biojppm:master into 83561f05803dc2001626b61652e02192fa528690 on google:master.\n. \nCoverage decreased (-1.3%) to 85.632% when pulling 2faf8f45ad4dc881bded1202b9811cb770818981 on biojppm:master into 917b86e615f659d9fb9819d1fa765cd459fc6861 on google:master.\n. \nCoverage decreased (-1.3%) to 85.632% when pulling e12753d6443f4d5c9fb0aa26355e8931a851f510 on biojppm:master into 917b86e615f659d9fb9819d1fa765cd459fc6861 on google:master.\n. \nCoverage decreased (-1.8%) to 85.1% when pulling b9b57f320acd161e804973335fc808c057116512 on biojppm:master into 917b86e615f659d9fb9819d1fa765cd459fc6861 on google:master.\n. \nCoverage decreased (-1.8%) to 85.1% when pulling 55b894223a7e399a2b224d4fc8247321c004ca72 on biojppm:master into 917b86e615f659d9fb9819d1fa765cd459fc6861 on google:master.\n. \nCoverage decreased (-2.04%) to 85.742% when pulling bd87835340b49d3d0a2f96c8b31904945d20ca52 on biojppm:master into 4f8bfeae470950ef005327973f15b0044eceaceb on google:master.\n. \n\nCoverage decreased (-2.05%) to 84.952% when pulling b8cd5321c90806ac60a9a3e89e0d5e9d058e0f6c on biojppm:master into 070c0ca0a9abeaa6076d6a6118e54aa39b31d0a2 on google:master.\n. \nCoverage decreased (-0.4%) to 87.816% when pulling f5e2f9e8627d1d2068d6d35bffae194da7388d3b on efcs:upgrade-travis into f68e64c60a6b20a1f50046df58111107a09fd6a3 on google:master.\n. \nCoverage remained the same at 87.406% when pulling ac2d3a9e8c8d2431acfb0f15e1dca0c43306e9cf on efcs:benchmark-scripts into 5f5ca31ce0f9e5fa33e622aa98f9feee31b0c099 on google:master.\n. \nCoverage remained the same at 87.406% when pulling ac2d3a9e8c8d2431acfb0f15e1dca0c43306e9cf on efcs:benchmark-scripts into 5f5ca31ce0f9e5fa33e622aa98f9feee31b0c099 on google:master.\n. \nCoverage remained the same at 87.406% when pulling cc0687633eef45512f92d7173c495a84d20ed1a0 on efcs:benchmark-scripts into 25eae60c4c8145553a50d536dac922a5b65bcee6 on google:master.\n. \nCoverage increased (+0.06%) to 87.469% when pulling 24ddbcc13334148ab5ccc9088d14af0ed5ee3a76 on efcs:report-repetitions into 25eae60c4c8145553a50d536dac922a5b65bcee6 on google:master.\n. \nCoverage increased (+0.06%) to 87.469% when pulling 0a0ead79ffbdf34812004dc140feea33183f4500 on efcs:report-repetitions into 7f1da4a68b83fa8474ff61c13d26922866871285 on google:master.\n. \nCoverage increased (+0.05%) to 87.539% when pulling a59180e21eb1b7e680b0c715ec0eea9c4429b74c on efcs:report-repetitions into dfe026075480f117f424d254c2f701ac97ea4cdd on google:master.\n. \nCoverage increased (+0.2%) to 87.648% when pulling 0b7394e5640991c49a55d518f644e55117122fbe on efcs:report-repetitions into 96a5965b6ed2f3723b5c71e6ef32a008b8b0c107 on google:master.\n. \nCoverage increased (+0.1%) to 87.586% when pulling 0b7394e5640991c49a55d518f644e55117122fbe on efcs:report-repetitions into 96a5965b6ed2f3723b5c71e6ef32a008b8b0c107 on google:master.\n. \nCoverage remained the same at 87.484% when pulling 3a1b1000d59f63c0eed26390a0bc839f8d8d1a57 on efcs:compiler-reqs into dfe026075480f117f424d254c2f701ac97ea4cdd on google:master.\n. \nCoverage decreased (-0.06%) to 87.484% when pulling 3a1d08844b9c7ef7f18c1f0adfca7050ee6fcd06 on efcs:travis-ubsan into c04f703ab499058c62d6e3c4e05c11d3cb1e8781 on google:master.\n. \nCoverage decreased (-0.06%) to 87.484% when pulling a707dd092a074c5673072aaa10e9370525f9b6de on efcs:travis-ubsan into c04f703ab499058c62d6e3c4e05c11d3cb1e8781 on google:master.\n. \nCoverage decreased (-0.06%) to 87.484% when pulling 5536012813b1c675b9933fdea5841f997b851d70 on efcs:travis-ubsan into c04f703ab499058c62d6e3c4e05c11d3cb1e8781 on google:master.\n. \nCoverage decreased (-0.06%) to 87.484% when pulling 178e714807834171eba34e1eba0d366ec2c01f73 on efcs:travis-ubsan into c04f703ab499058c62d6e3c4e05c11d3cb1e8781 on google:master.\n. \nCoverage decreased (-0.06%) to 87.484% when pulling 7f28e76bf234a1f31d25c75cd873c1b1d236c303 on efcs:travis-ubsan into c04f703ab499058c62d6e3c4e05c11d3cb1e8781 on google:master.\n. \nCoverage decreased (-0.06%) to 87.484% when pulling 7f28e76bf234a1f31d25c75cd873c1b1d236c303 on efcs:travis-ubsan into c04f703ab499058c62d6e3c4e05c11d3cb1e8781 on google:master.\n. \nCoverage decreased (-0.06%) to 87.484% when pulling c497bca738a9fab0eba156c2f31fb680616bb7cf on efcs:travis-ubsan into c04f703ab499058c62d6e3c4e05c11d3cb1e8781 on google:master.\n. \nCoverage decreased (-0.06%) to 87.484% when pulling 6245c4da0862822be70a6105583b8a8c5839e452 on efcs:travis-ubsan into c04f703ab499058c62d6e3c4e05c11d3cb1e8781 on google:master.\n. \nCoverage decreased (-0.06%) to 87.484% when pulling 8d8a3f8883d74320b866e79c6d041f4a357c2868 on efcs:travis-ubsan into c04f703ab499058c62d6e3c4e05c11d3cb1e8781 on google:master.\n. \nCoverage decreased (-0.06%) to 87.484% when pulling 73eb49cea33fd4ef06fe98a5324ad5531e79bde0 on efcs:travis-ubsan into c04f703ab499058c62d6e3c4e05c11d3cb1e8781 on google:master.\n. \nCoverage decreased (-0.06%) to 87.484% when pulling 43121dc5c4f20e4f9e98472e4ac7756f9e72bf6e on efcs:travis-ubsan into c04f703ab499058c62d6e3c4e05c11d3cb1e8781 on google:master.\n. \nCoverage remained the same at 87.484% when pulling 1de8391ab2817b27f4d50e4119f52d854dfa089c on efcs:libcxx into 8228b3e8a7298c4725cc079f96e5fa4bc96919b6 on google:master.\n. \nCoverage remained the same at 87.484% when pulling 289da18e95029627fab957a2c548565e3d6eec93 on efcs:libcxx into 8228b3e8a7298c4725cc079f96e5fa4bc96919b6 on google:master.\n. \nCoverage remained the same at 87.484% when pulling ef2df61fc642a791c98e4f066f0cf05104fd667e on efcs:libcxx into 8228b3e8a7298c4725cc079f96e5fa4bc96919b6 on google:master.\n. \nCoverage increased (+0.06%) to 87.547% when pulling 682ef19a5d00d18d6c6530e378e9d32c99b89904 on efcs:libcxx into 8228b3e8a7298c4725cc079f96e5fa4bc96919b6 on google:master.\n. \nCoverage remained the same at 87.484% when pulling e9f86982baff2c69c6f4e9dac02f35ddac0d38df on efcs:libcxx into 8228b3e8a7298c4725cc079f96e5fa4bc96919b6 on google:master.\n. \nCoverage increased (+0.008%) to 87.656% when pulling ae870e400fd4e36555f033ebd47530e515f746b2 on AlexanderSidorenko:feature-lib-with-main into 577baa0dbdc6ea6d916db0f12ed2dcc5cee19b99 on google:master.\n. \nCoverage decreased (-0.05%) to 87.594% when pulling 4374c4e13116c2887e591bd9f4b965d8fcd76391 on AlexanderSidorenko:feature-lib-with-main into 577baa0dbdc6ea6d916db0f12ed2dcc5cee19b99 on google:master.\n. \nCoverage decreased (-0.05%) to 87.594% when pulling e98d8fe815ed446f322fedc55fd2edfa23479175 on AlexanderSidorenko:feature-lib-with-main into 577baa0dbdc6ea6d916db0f12ed2dcc5cee19b99 on google:master.\n. \nCoverage decreased (-0.3%) to 87.329% when pulling 0f65ae4e94403e243bae40995f1617fe0f5a6899 on efcs:my-custom-counters into 577baa0dbdc6ea6d916db0f12ed2dcc5cee19b99 on google:master.\n. \nCoverage decreased (-0.3%) to 87.321% when pulling 65538a181a8255d7d00826ddca0364a82e8a0cdb on efcs:my-custom-counters into 577baa0dbdc6ea6d916db0f12ed2dcc5cee19b99 on google:master.\n. \nCoverage decreased (-0.06%) to 87.586% when pulling e044b908d8bdd279bac4a797c44f8bd732c1bdd4 on efcs:refactor-output-test into 577baa0dbdc6ea6d916db0f12ed2dcc5cee19b99 on google:master.\n. \nCoverage remained the same at 87.648% when pulling d7b9565cfec57bde138dc81f2ae8f6c27646d154 on efcs:refactor-output-test into 577baa0dbdc6ea6d916db0f12ed2dcc5cee19b99 on google:master.\n. \nCoverage decreased (-0.06%) to 87.586% when pulling 8cc226e2fcc8a2f560df0ad11ba160755054ec12 on efcs:refactor-output-test into 577baa0dbdc6ea6d916db0f12ed2dcc5cee19b99 on google:master.\n. \nCoverage increased (+0.1%) to 87.725% when pulling 01d768b064788534e260936c768c2f23f829aca5 on efcs:ranges-revert into 0ed44560973cad74be04848a74eca3dda0ed7957 on google:master.\n. \nCoverage increased (+0.1%) to 87.763% when pulling b28b33f889a4df6b017e9f445ed3910c9a46ec85 on efcs:ranges-revert into db1af86d16f6ffb59e8fc628be2bfc660ec54470 on google:master.\n. \nCoverage increased (+0.08%) to 87.77% when pulling 2378b5a33c1392c675f37bca1a30f3a71c4a356e on efcs:ranges-revert into 2e0796e1487d2f060d5fb526d6e0e387c55233aa on google:master.\n. \nCoverage decreased (-0.05%) to 87.531% when pulling 1362608e2ee2d57c29db71d4dec7f02cbc08a1c0 on efcs:warn-large-inputs into 0ed44560973cad74be04848a74eca3dda0ed7957 on google:master.\n. \nCoverage decreased (-0.05%) to 87.57% when pulling 48a583d41b6473d8561b322f3bf83fcc815214bd on efcs:warn-large-inputs into db1af86d16f6ffb59e8fc628be2bfc660ec54470 on google:master.\n. \nCoverage decreased (-0.1%) to 87.577% when pulling 39e1b6d495b819185f92a8b4c0f5d2cedbd3a0f7 on efcs:warn-large-inputs into 2e0796e1487d2f060d5fb526d6e0e387c55233aa on google:master.\n. \nCoverage decreased (-0.1%) to 87.577% when pulling e1bb108ccae74fc6ae1ad2308a62026e0a6e95dc on efcs:warn-large-inputs into 2e0796e1487d2f060d5fb526d6e0e387c55233aa on google:master.\n. \nCoverage remained the same at 87.586% when pulling ac578d8446ea941f3a8648b9946919f1c205ef6b on efcs:support-libcxx into 78e22f10de53033cbc1139df91274bc2c8fbf802 on google:master.\n. \nCoverage remained the same at 87.586% when pulling edca6f66d3d1f5195f1e1ecb785af8b950f985d2 on efcs:support-libcxx into 78e22f10de53033cbc1139df91274bc2c8fbf802 on google:master.\n. \nCoverage increased (+0.05%) to 87.632% when pulling 706a4bccc3fe198d3e36fada27245137a8427f1a on efcs:support-libcxx into 78e22f10de53033cbc1139df91274bc2c8fbf802 on google:master.\n. \nCoverage decreased (-0.8%) to 86.906% when pulling f9e71dd8f73598792f4238438e6e6f5db4420794 on hydroo:master into 61f570e82a9334ef13f794b9149013feeb1a2ab1 on google:master.\n. \nCoverage increased (+0.09%) to 87.219% when pulling 86ebccf54d87cb809f01e962fbc1d1a8f7681d07 on hydroo:master into 60e88c21e44c34a847b5c11e7b9f28107602cfc3 on google:master.\n. \nCoverage decreased (-0.06%) to 87.654% when pulling 219e04b23246afa37c0936dd23f4ae38e418fa99 on biojppm:cmkheaders into 61f570e82a9334ef13f794b9149013feeb1a2ab1 on google:master.\n. \nCoverage decreased (-0.05%) to 87.669% when pulling b7b8c9addd09826ac7726c7e519432b20e6df5f2 on biojppm:fixture into 61f570e82a9334ef13f794b9149013feeb1a2ab1 on google:master.\n. \nCoverage increased (+0.02%) to 87.73% when pulling b8d142401ad5f3647fbd697a9b6157362b8638ee on biojppm:fixture into 61f570e82a9334ef13f794b9149013feeb1a2ab1 on google:master.\n. \nCoverage decreased (-0.05%) to 87.669% when pulling 1f862417531ccd43d779b771fccbafc293f3c5a4 on biojppm:fixture into 61f570e82a9334ef13f794b9149013feeb1a2ab1 on google:master.\n. \nCoverage decreased (-0.6%) to 87.059% when pulling 724ce26bdbe7b3595cca25def66506c5d4225b69 on efcs:per-thread-timers into 6a28f1e96819be19377472fa59a04602769c9c9d on google:master.\n. \nCoverage decreased (-0.7%) to 86.931% when pulling fb608df3466b4ed36bb2cb98f17aa7b2c18bbd27 on efcs:per-thread-timers into 6a28f1e96819be19377472fa59a04602769c9c9d on google:master.\n. \nCoverage decreased (-0.7%) to 86.931% when pulling 48caee720e2bba407fd52de974767e0a78db9ad4 on efcs:per-thread-timers into 6a28f1e96819be19377472fa59a04602769c9c9d on google:master.\n. \nCoverage decreased (-0.8%) to 86.826% when pulling 448c797271e750af34750fff2293bfe11d86983d on efcs:per-thread-timers into 94c2a30a3eefa6ae91b2e44825a8ca5884b47707 on google:master.\n. \nCoverage decreased (-0.8%) to 86.826% when pulling 9130bea90aacc6d75bfa9d1125898ab092e45587 on efcs:per-thread-timers into 94c2a30a3eefa6ae91b2e44825a8ca5884b47707 on google:master.\n. \nCoverage decreased (-0.8%) to 86.826% when pulling d2cbeacbc05c901eff62a6d93f03c1d2825fe889 on efcs:per-thread-timers into 94c2a30a3eefa6ae91b2e44825a8ca5884b47707 on google:master.\n. \nCoverage decreased (-0.8%) to 86.826% when pulling ef4640b95cf981f5ed0497b672a46d43dd14b7bc on efcs:per-thread-timers into 94c2a30a3eefa6ae91b2e44825a8ca5884b47707 on google:master.\n. \nCoverage decreased (-0.8%) to 86.826% when pulling 2a6e747f3356b027d6062f5175482d25ef714c92 on efcs:per-thread-timers into 94c2a30a3eefa6ae91b2e44825a8ca5884b47707 on google:master.\n. \nCoverage decreased (-0.8%) to 86.826% when pulling 3e683c518306d9d1d31bfe18bb12487e345fad4e on efcs:per-thread-timers into 94c2a30a3eefa6ae91b2e44825a8ca5884b47707 on google:master.\n. \nCoverage decreased (-0.5%) to 87.191% when pulling e8858c55613dd28ec65987a73086fe15e09b4a42 on efcs:per-thread-timers into 94c2a30a3eefa6ae91b2e44825a8ca5884b47707 on google:master.\n. \nCoverage decreased (-0.5%) to 87.133% when pulling 25d56592c7734d27451a85c3f8691bfb91d980d4 on efcs:per-thread-timers into 94c2a30a3eefa6ae91b2e44825a8ca5884b47707 on google:master.\n. \nCoverage decreased (-0.5%) to 87.125% when pulling 7bc016b6b752271570f850069525000ccd881e35 on efcs:per-thread-timers into 94c2a30a3eefa6ae91b2e44825a8ca5884b47707 on google:master.\n. \nCoverage decreased (-0.2%) to 86.89% when pulling c21db7b0fe576643f5a4e7a2235f0ae3f35f0e36 on efcs:refactor into cba945e37dd8f336c7c8f5367f3c7d9498d5e09b on google:master.\n. \nCoverage decreased (-0.2%) to 86.89% when pulling ecc618936f53235a8621188f01bd91af50f9f4af on efcs:refactor into cba945e37dd8f336c7c8f5367f3c7d9498d5e09b on google:master.\n. \nCoverage decreased (-0.2%) to 86.89% when pulling 8f0f05b3dbb7ac6231ee82ea92a29328e87cbfb0 on efcs:refactor into 756f069918f7791110c867064e225b467f3a45bf on google:master.\n. \nCoverage decreased (-0.2%) to 86.89% when pulling 08ae2007237e3bd27cd077be48437799fafdb247 on efcs:refactor into f261c68074ef621b1ff4079a451518ff830a166a on google:master.\n. \nCoverage decreased (-0.2%) to 86.89% when pulling a9b358a63be624f8622a074cc4dc8c498e4afd2f on efcs:refactor into 07ee194092e8ad246dea873b57f00d296c16b691 on google:master.\n. \nCoverage decreased (-0.3%) to 86.963% when pulling 50d5e469a9c9bad6b5bd23ca95f27d9e4b20b064 on efcs:refactor into 72be9523bb88d5b96e3891776fad18b790bfd2d2 on google:master.\n. \nCoverage decreased (-0.3%) to 86.963% when pulling 24393628a200404283e986b3a181830e908d8f03 on efcs:refactor into 72be9523bb88d5b96e3891776fad18b790bfd2d2 on google:master.\n. \nCoverage increased (+0.04%) to 87.261% when pulling 6a47f19d061890527e1ca42076535b90fe18fa7b on nickhutchinson:flush-streams into 72be9523bb88d5b96e3891776fad18b790bfd2d2 on google:master.\n. \nCoverage increased (+0.09%) to 87.034% when pulling 8e4cfdfd49807f9f8010c9d76247b2cd0bcc3953 on nickhutchinson:flush-streams into c6f3f0eb9cd68150371c0c45b84aeb0dc72114c9 on google:master.\n. \nCoverage increased (+0.05%) to 87.014% when pulling 4836a6362047ae576947686a8c8d4b631a57453c on efcs:cleanup-run into d038472c18e7aa5b206896a9cd71b220b36b5eb8 on google:master.\n. \nCoverage decreased (-0.1%) to 86.798% when pulling 7f94142aec02717fc1ca4cdece41f542bc058d5c on biojppm:fixhuman into 917b86e615f659d9fb9819d1fa765cd459fc6861 on google:master.\n. \nCoverage decreased (-0.1%) to 86.798% when pulling 7f94142aec02717fc1ca4cdece41f542bc058d5c on biojppm:fixhuman into 917b86e615f659d9fb9819d1fa765cd459fc6861 on google:master.\n. \nCoverage decreased (-0.1%) to 86.798% when pulling e30444ab062aa3fec06df5a4046d193e894a7b3a on biojppm:fixhuman into 917b86e615f659d9fb9819d1fa765cd459fc6861 on google:master.\n. \nCoverage increased (+0.02%) to 86.924% when pulling a00219ec5c13346c13be9672779c09884ae41315 on biojppm:fixhuman into 917b86e615f659d9fb9819d1fa765cd459fc6861 on google:master.\n. \nCoverage remained the same at 86.974% when pulling 2f2a7220cf268e5a745a3519ec36df2d150ee708 on PSIAlt:fix_macos_clock_gettime into 62c68ba4f0b093f1249715d7aa276891497ec6a5 on google:master.\n. \nCoverage decreased (-0.07%) to 86.906% when pulling adde4e195f4c97ac87857ebb7690657a6a23e2ad on PSIAlt:fix_macos_clock_gettime into 62c68ba4f0b093f1249715d7aa276891497ec6a5 on google:master.\n. \nCoverage remained the same at 86.889% when pulling 5d51bf767b1a077a247b15a04f692aabbc25f5ba on ktnyt:master into 2aca242bf69870cc208cb7da3c026c1f64b12cdb on google:master.\n. \nCoverage remained the same at 86.889% when pulling a98a4fde3cfadb5467c4e4e56de55a3b2c969b35 on clangformat into d1daeee4e90c1125b52baa9bcaff46a987c2a795 on master.\n. \nCoverage decreased (-0.2%) to 86.648% when pulling c3b55b5b21c81b32521df2d33b95531ea385ea6f on clangformat_src into 1100e9190709a48b0819c84b3cebeaf2bf904d65 on master.\n. \nCoverage increased (+0.02%) to 86.76% when pulling 8aeb20f16b7d539e3d3cf857fb4c82bf56d1a605 on mkurdej:fix-time-unit into 57a22c69b382b3f010ec4042c9574ea3fd8dcbb4 on google:master.\n. \nChanges Unknown when pulling 17e1c405dd67858ca47b53b5968e564895dba965 on mkurdej:arg-names into * on google:master*.\n. \nChanges Unknown when pulling c1c01b2cd303826cab1f419f1a1582d18d5905df on mkurdej:arg-names into * on google:master*.\n. \nChanges Unknown when pulling cfee1a54e4d4dcc455ee077092aa5e2d054899c7 on mkurdej:arg-names into * on google:master*.\n. \nChanges Unknown when pulling 3f23832a097db2da0d9823987dc574be3cefa1f3 on mkurdej:arg-names into * on google:master*.\n. \nCoverage increased (+0.9%) to 87.827% when pulling 0869ae73f62c205f0bf34f3650f2a254dd08547a on mkurdej:per-second-tests into 8e08aa75d9d25ae88ca2fbb8859238aec60b6ed4 on google:master.\n. \nCoverage increased (+0.8%) to 87.758% when pulling 9e289c0012e1525f5823c7aae103b57021115c59 on mkurdej:per-second-tests into 8e08aa75d9d25ae88ca2fbb8859238aec60b6ed4 on google:master.\n. \nCoverage increased (+0.8%) to 87.758% when pulling 536d2a369ed24a94521c5e76ba030503070b2e8c on mkurdej:per-second-tests into 8e08aa75d9d25ae88ca2fbb8859238aec60b6ed4 on google:master.\n. \nCoverage increased (+0.03%) to 87.783% when pulling c9cb92532245c42d49f89fddc5019744c6b3f929 on 306 into 0064c56abdcfebc6998a66a8cb837ec88cfc9840 on master.\n. \n\nCoverage remained the same at 87.783% when pulling 3cc0905f1be50d57c5f56f9ff484751f97979991 on efcs:update-cmake-ver into a8aa40c596edf5454ccc72604037ffc8183398fe on google:master.\n. \n\nCoverage increased (+0.07%) to 87.852% when pulling 03d1ad13ebcd12a37f28289bf765a60fe74d201e on efcs:fix-exceptions into a8aa40c596edf5454ccc72604037ffc8183398fe on google:master.\n. \n\nCoverage remained the same at 87.783% when pulling 54623469cb4b3f51deb0a635e8c8ac0e02854f8a on efcs:document-compare-bench into a8aa40c596edf5454ccc72604037ffc8183398fe on google:master.\n. \n\nCoverage increased (+0.07%) to 87.852% when pulling 988a558ef4f3af3a79581efabf6a4edeabf30675 on NiklasRosenstein:fix-parsecommandlineflags-argv-access into efd4e992a6aa07ac46859dadb96246430959538d on google:master.\n. \n\nCoverage decreased (-0.07%) to 87.783% when pulling 8279dc11254faa5949ba63e8706b7d83bbe9ab15 on pcampr:master into 56336e70f151f9eb828176e795f7c5dfe6d6bb59 on google:master.\n. \n\nCoverage decreased (-0.07%) to 87.783% when pulling 380c0bdfcfae22a33d413f1ae1dc436cb714f302 on pcampr:master into 56336e70f151f9eb828176e795f7c5dfe6d6bb59 on google:master.\n. \n\nCoverage decreased (-0.6%) to 87.157% when pulling e790e143e8c722c8dc7ca4563282bc4dd2d49f92 on BRevzin:issue-327 into e381139474cfd997941f7c48fe17001bf6bfdedd on google:master.\n. \n\nCoverage remained the same at 87.157% when pulling 0e3905228aa67dca96bd6f4d4ceb1878c647838b on bazel into 4bf28e611b55de8a2d4eece3c335e014f8b0f630 on master.\n. \n\nCoverage decreased (-0.07%) to 87.088% when pulling b9296314dd5b874264ed7d67385774f38acb056d on PSIAlt:fix_librt_linking into 4bf28e611b55de8a2d4eece3c335e014f8b0f630 on google:master.\n. \n\nCoverage decreased (-0.2%) to 86.858% when pulling 40fc6eb3201e5cced66d56ef5b0245396997f483 on NiklasRosenstein:report-unrecognised-flags into b4fdf6e9df82ea5a424440436dcc4ac0474decce on google:master.\n. \n\nCoverage decreased (-0.2%) to 86.858% when pulling 736b0f35ee54818ad7a63bb19f9705fbfd1604a3 on NiklasRosenstein:report-unrecognised-flags into b4fdf6e9df82ea5a424440436dcc4ac0474decce on google:master.\n. \n\nCoverage decreased (-0.03%) to 87.004% when pulling 2df78693264569b7eac0dc95c9d92ebb00f4a309 on NiklasRosenstein:report-unrecognised-flags into b4fdf6e9df82ea5a424440436dcc4ac0474decce on google:master.\n. \n\nCoverage decreased (-0.03%) to 87.004% when pulling 2df78693264569b7eac0dc95c9d92ebb00f4a309 on NiklasRosenstein:report-unrecognised-flags into b4fdf6e9df82ea5a424440436dcc4ac0474decce on google:master.\n. \n\nCoverage remained the same at 87.004% when pulling f7d39219bd0dc6ec50fb57a7d7a0899f5bed36d7 on sieren:fix-ios-cpu-freq into 817bfee273246f2a90c08e700e67a5f32711f979 on google:master.\n. \n\nCoverage increased (+0.07%) to 87.073% when pulling c59d0423b24084da353002f1c27c3228b45a1de4 on grypp:master into 246ee86428758be3d3af1d2c502086eb6036d9fe on google:master.\n. \n\nCoverage remained the same at 87.004% when pulling b28090a5df35783c4b5dc3ce26c629aba4c5e3a9 on Maratyszcza:master into fef203bd021bad019421abf5785a4ae89bd6ea15 on google:master.\n. \n\nCoverage remained the same at 87.004% when pulling 985fcb898969badacbb780c15c87162fe5ad6986 on Maratyszcza:master into fef203bd021bad019421abf5785a4ae89bd6ea15 on google:master.\n. \n\nCoverage remained the same at 87.088% when pulling 85fe4d49016c901fcb1cfd41f4b18e6b94135b9a on GregBowyer:patch-1 into 0e3905228aa67dca96bd6f4d4ceb1878c647838b on google:bazel.\n. \n\nChanges Unknown when pulling 7afdbd00e7e8da7e70efed096331bf3c5bb77559 on biojppm:usercounters into  on google:master.\n. \n\nChanges Unknown when pulling 0de985ae9d4ea1582125a5414a99ca90c368ca10 on biojppm:compact into  on google:master.\n. \n\nChanges Unknown when pulling c846eedfebbacaca755db7dcbf45a0cb9fbdb320 on biojppm:compact into  on google:master.\n. \n\nChanges Unknown when pulling d84d911d37b27e0adeb780266c34ba97c11cb6e5 on biojppm:compact into  on google:master.\n. \n\nChanges Unknown when pulling 707dd893448cc0bcacde78a45cf4d9d7454eb2fd on biojppm:compact into  on google:master.\n. \n\nCoverage increased (+1.0%) to 86.486% when pulling 020bac985b93308750b90cca6921b7425ca57b3d on biojppm:compact into da8cd74d85fa44c8feb08e886847517caa35a0d1 on google:master.\n. \n\nCoverage increased (+1.0%) to 86.486% when pulling a31088632a54d179308d3d937f2e0f9a19df353d on biojppm:compact into da8cd74d85fa44c8feb08e886847517caa35a0d1 on google:master.\n. \n\nCoverage increased (+1.0%) to 86.486% when pulling 17354131886a9eec06e5e34e0be0583ac32728ca on biojppm:compact into da8cd74d85fa44c8feb08e886847517caa35a0d1 on google:master.\n. \n\nCoverage increased (+1.0%) to 86.486% when pulling ec6f03579eb7fe1296d161c838f960ca993fd182 on biojppm:compact into da8cd74d85fa44c8feb08e886847517caa35a0d1 on google:master.\n. \n\nCoverage decreased (-0.06%) to 84.932% when pulling 8edd0282311bc4e0410883de8fecaf1065dd8d95 on efcs:implement-donotoptimize-msvc into 8ae6448cc7ec6353e3491a2a15f972f9735f124b on google:master.\n. \n\nCoverage remained the same at 84.932% when pulling 1e023eb0590b3521f3c2dee5f25530c4aae4d0c4 on yasushi-saito:master into f682f7e9a41a88468c72ce5f7f6abe76913b82ad on google:master.\n. \n\nCoverage remained the same at 84.932% when pulling c5cc4c72e1fbfb8f97547682d78a24aa6f518d13 on MVafin:fix-355 into f5ff6d0e0d3d00cf07bb8306548b637e98b13720 on google:master.\n. \n\nCoverage decreased (-0.05%) to 84.878% when pulling 6e4a8dddcae128b7739f8bd3abd1a1c69fdc7d62 on MVafin:fix-355 into f5ff6d0e0d3d00cf07bb8306548b637e98b13720 on google:master.\n. \n\nCoverage decreased (-0.05%) to 84.878% when pulling 1e257bda1aa734ad3b9bfe513e40f1cad06a3f08 on MVafin:fix-355 into 94c512c0439aa9b625d9e179767ce870df9f76a8 on google:master.\n. \n\nCoverage remained the same at 84.932% when pulling 5c3b6cf5d58423a9bf1dd2ee8019248c19a37680 on rolandschulz:master into f5ff6d0e0d3d00cf07bb8306548b637e98b13720 on google:master.\n. \n\nCoverage remained the same at 84.932% when pulling fcf45286116a6aaaf40cd08dcfda3abbf107693f on rolandschulz:master into f5ff6d0e0d3d00cf07bb8306548b637e98b13720 on google:master.\n. \n\nCoverage remained the same at 84.932% when pulling 7e305b0c203c24becb7c17ce38197250edb514b2 on efcs:fix-int64_t-usages into f5ff6d0e0d3d00cf07bb8306548b637e98b13720 on google:master.\n. \n\nCoverage remained the same at 84.932% when pulling 45d546aaa051a21cbc7b887caa8de8bd67fd863d on efcs:add-build-32-bits-option into 94c512c0439aa9b625d9e179767ce870df9f76a8 on google:master.\n. \n\nCoverage remained the same at 84.932% when pulling df61040ff2cf919e6d99fd4b63c4da91dfec307b on efcs:add-build-32-bits-option into 94c512c0439aa9b625d9e179767ce870df9f76a8 on google:master.\n. \n\nCoverage remained the same at 84.932% when pulling bee5deffc61489047c815241a99b213c32ece744 on efcs:add-build-32-bits-option into 94c512c0439aa9b625d9e179767ce870df9f76a8 on google:master.\n. \n\nCoverage remained the same at 84.932% when pulling 765ef85276ad62e2bcb20ed49e8ca1e104b888bd on efcs:add-build-32-bits-option into 94c512c0439aa9b625d9e179767ce870df9f76a8 on google:master.\n. \n\nCoverage remained the same at 84.932% when pulling d9ee2f877a5048a39428dde6946cf4ce14d98423 on efcs:add-build-32-bits-option into 94c512c0439aa9b625d9e179767ce870df9f76a8 on google:master.\n. \n\nCoverage remained the same at 84.932% when pulling a43641a55022f624f182c7bb550c320fc5e425dc on efcs:add-build-32-bits-option into 94c512c0439aa9b625d9e179767ce870df9f76a8 on google:master.\n. \n\nCoverage remained the same at 84.932% when pulling 2d7cabb99c9f51e4e9adfebd31f016a18b8eadfa on efcs:add-build-32-bits-option into 94c512c0439aa9b625d9e179767ce870df9f76a8 on google:master.\n. \n\nCoverage remained the same at 84.878% when pulling 8a84166b9f4ab21f8c0cc45ecf491b4bdc694ac9 on rayglover-ibm:py3 into 0dbcdf56a0d0ed817a7fccf8f622259ee8dafa18 on google:master.\n. \n\nCoverage increased (+0.01%) to 84.887% when pulling ea26e62a0dcaa2df529c5b0e901745a1b914063d on biojppm:master into 17298b2dc0e6dc9f78b149ab9256064d0ac96520 on google:master.\n. \n\nCoverage remained the same at 84.878% when pulling 824bbb818e2d34e546c77d719687ab732264603c on piribes:1.1 into 17298b2dc0e6dc9f78b149ab9256064d0ac96520 on google:master.\n. \n\nCoverage remained the same at 84.878% when pulling 128fe25025dcf75baa2e1c533b1aaf90a5f1c7fc on piribes:1.1 into 17298b2dc0e6dc9f78b149ab9256064d0ac96520 on google:master.\n. \n\nCoverage remained the same at 84.887% when pulling 9db12eb10c9baab4810061635b1adc9e1129c6b6 on kunitoki:master into 9a5072d1bf9187b32ce9a88842dffa31ef416442 on google:master.\n. \n\nCoverage remained the same at 84.887% when pulling a125f4bdee224591637745acb4bd848bb9adb6b4 on hunter-packages:hunter into 9a5072d1bf9187b32ce9a88842dffa31ef416442 on google:master.\n. \n\nCoverage remained the same at 84.887% when pulling 06087c30c2510c95e11ea31126a0dfe3f167fa54 on horttanainen:master into 9a5072d1bf9187b32ce9a88842dffa31ef416442 on google:master.\n. \n\nCoverage remained the same at 84.887% when pulling 323a5d1aa0dde42a8fb0f423a6fa8fad6013f77f on vargad:master into 9a5072d1bf9187b32ce9a88842dffa31ef416442 on google:master.\n. \n\nCoverage remained the same at 84.887% when pulling 99b41f532ea52374f4a3423b901ca1117423ce55 on merlin-ext:android-fix into 312d9d0ac5c280e81aee0689045a10ae441b4db1 on google:master.\n. \n\nCoverage remained the same at 84.887% when pulling 62dbc85575d560ccbefe19c43147b6e3f8df1ba1 on merlin-ext:android-fix into 312d9d0ac5c280e81aee0689045a10ae441b4db1 on google:master.\n. \n\nCoverage remained the same at 84.887% when pulling 62dbc85575d560ccbefe19c43147b6e3f8df1ba1 on merlin-ext:android-fix into 312d9d0ac5c280e81aee0689045a10ae441b4db1 on google:master.\n. \n\nCoverage remained the same at 84.887% when pulling 234a75444ad1f7f00417c185a764981a8bda2a43 on merlin-ext:android-fix into 312d9d0ac5c280e81aee0689045a10ae441b4db1 on google:master.\n. \n\nCoverage remained the same at 84.887% when pulling 234a75444ad1f7f00417c185a764981a8bda2a43 on merlin-ext:android-fix into 312d9d0ac5c280e81aee0689045a10ae441b4db1 on google:master.\n. \n\nCoverage decreased (-0.9%) to 84.026% when pulling 791a840293d872b54094b9066eecea06b3a8b367 on merlin-ext:android-fix into 312d9d0ac5c280e81aee0689045a10ae441b4db1 on google:master.\n. \n\nCoverage increased (+0.1%) to 83.995% when pulling eb40aa97ddd56a8c69ca760b42aff45891fc8c43 on efcs:add-iteration-count into 7f87c98d36279b1819e9c8ee0dc93c6a8ea64aee on google:master.\n. \n\nCoverage increased (+0.2%) to 84.059% when pulling 46fd7f016db037639d9e2d4d2a4fe726a471feda on efcs:add-iteration-count into 7f87c98d36279b1819e9c8ee0dc93c6a8ea64aee on google:master.\n. \n\nCoverage remained the same at 84.026% when pulling 7dc96fd9eeef78657f9d2bb1640d5bb1b6176f5f on merlin-ext:android-r10e-fix into 09b93ccc6a9aed84c269b6f5b8130c878e518ebb on google:master.\n. \n\nCoverage remained the same at 84.026% when pulling 307ece09428c59441effed5bfbd4c42f30674855 on KindDragon:vc2017 into 7a74b74856bae690a0998c967c7807dd2272af82 on google:master.\n. \n\nCoverage remained the same at 84.026% when pulling 307ece09428c59441effed5bfbd4c42f30674855 on KindDragon:vc2017 into 7a74b74856bae690a0998c967c7807dd2272af82 on google:master.\n. \n\nCoverage remained the same at 84.026% when pulling 087c0f9a0419708eca36dd24e90db5ac0a5de34f on KindDragon:vc2017 into 7a74b74856bae690a0998c967c7807dd2272af82 on google:master.\n. \n\nCoverage remained the same at 84.026% when pulling b5d87af36982eddb2c082325c5b7e4bc6afc7627 on KindDragon:vc2017 into 7a74b74856bae690a0998c967c7807dd2272af82 on google:master.\n. \n\nCoverage remained the same at 84.026% when pulling b5d87af36982eddb2c082325c5b7e4bc6afc7627 on KindDragon:vc2017 into 7a74b74856bae690a0998c967c7807dd2272af82 on google:master.\n. \n\nCoverage remained the same at 84.026% when pulling dcf3e16f212872ba5a0fec102c03fb2d7f2cc685 on KindDragon:vc2017 into 7a74b74856bae690a0998c967c7807dd2272af82 on google:master.\n. \n\nCoverage remained the same at 84.026% when pulling dcf3e16f212872ba5a0fec102c03fb2d7f2cc685 on KindDragon:vc2017 into 7a74b74856bae690a0998c967c7807dd2272af82 on google:master.\n. \n\nCoverage remained the same at 84.026% when pulling dcf3e16f212872ba5a0fec102c03fb2d7f2cc685 on KindDragon:vc2017 into 7a74b74856bae690a0998c967c7807dd2272af82 on google:master.\n. \n\nCoverage remained the same at 84.026% when pulling 4a732ec0deeadc22a2c3d23326f1a971b72ac9be on felixduvallet:report_complexity into 3336ea00d892fc312715c3a00d33a9568261e86a on google:master.\n. \n\nCoverage remained the same at 84.026% when pulling 0a4ba33bf7ca6adf41119a82609ece76fb171fae on felixduvallet:report_complexity into 3336ea00d892fc312715c3a00d33a9568261e86a on google:master.\n. \n\nCoverage increased (+1.5%) to 85.523% when pulling cdbcaaf2b64fbe113c9fe3b3526f22d2be282c15 on biojppm:test_usercounters into 3336ea00d892fc312715c3a00d33a9568261e86a on google:master.\n. \n\nCoverage increased (+1.5%) to 85.523% when pulling 2a2eb44b3004ec67f694c26dacf488e57c63e568 on biojppm:test_usercounters into 3336ea00d892fc312715c3a00d33a9568261e86a on google:master.\n. \n\nCoverage increased (+1.5%) to 85.523% when pulling c3424c64ac0d7b2e586fa2c4ec9a115b2f9a93d8 on biojppm:test_usercounters into 3336ea00d892fc312715c3a00d33a9568261e86a on google:master.\n. \n\nCoverage increased (+1.5%) to 85.523% when pulling 77b9362b06d7fdef3937a28d56eb6ea24b3d9696 on biojppm:test_usercounters into 3336ea00d892fc312715c3a00d33a9568261e86a on google:master.\n. \n\nCoverage increased (+1.5%) to 85.514% when pulling eb2bf345244eff2ceaa129e32ad112254e7603e2 on biojppm:test_usercounters into 3336ea00d892fc312715c3a00d33a9568261e86a on google:master.\n. \n\nCoverage remained the same at 86.486% when pulling 1f07f9576868286368a862611df92aacb2b1af9f on yixuan:master into 4cfe790a253d2cb47f33b29676c0a27c462704cb on google:master.\n. \n\nCoverage decreased (-0.003%) to 86.483% when pulling a7946d834d7870d2e36012ddd1c8d4cafe49f219 on yixuan:master into 4cfe790a253d2cb47f33b29676c0a27c462704cb on google:master.\n. \n\nCoverage increased (+0.005%) to 86.492% when pulling 775f69f3844998146cc9caab7dd17a64c5580ff8 on yixuan:master into 4cfe790a253d2cb47f33b29676c0a27c462704cb on google:master.\n. \n\nCoverage remained the same at 86.486% when pulling 81781a846fb62a69bcd6bd05c9948eeee0aed6b2 on tusharpm:travis into cb8a0cc10f8b634fd554251ae086da522b58f50e on google:master.\n. \n\nCoverage remained the same at 86.486% when pulling 077d428307d0533a970563a8be468fd7ced0bfc2 on schoetbi:build_nuget into febd0d7a7a22dc98db3303eebfaecac9c7007af0 on google:master.\n. \n\nCoverage remained the same at 86.486% when pulling 47d22e02a92a60b24b60d61f37f33c9cd4478dad on schoetbi:build_nuget into febd0d7a7a22dc98db3303eebfaecac9c7007af0 on google:master.\n. \n\nCoverage remained the same at 86.486% when pulling 47d22e02a92a60b24b60d61f37f33c9cd4478dad on schoetbi:build_nuget into febd0d7a7a22dc98db3303eebfaecac9c7007af0 on google:master.\n. \n\nCoverage remained the same at 86.486% when pulling e78fdb1f635b73a779f812d80dbfbcc9d009b5da on dkruger:master into febd0d7a7a22dc98db3303eebfaecac9c7007af0 on google:master.\n. \n\nCoverage decreased (-0.05%) to 86.441% when pulling 740ac051b8b4d354b658fadac63b00842b534698 on GeorgeARM:add-run-name into 15e9ebaf8363e723478cba23d0651274201c7750 on google:master.\n. \n\nCoverage increased (+0.02%) to 86.503% when pulling e5392de45444f89532c922ce3d788644ccfec881 on GeorgeARM:add-run-name into 15e9ebaf8363e723478cba23d0651274201c7750 on google:master.\n. \n\nCoverage remained the same at 86.486% when pulling b846368400f6c68eb80453d28bfd1dfee1d0a979 on efcs:fix-do-not-optimize into 15e9ebaf8363e723478cba23d0651274201c7750 on google:master.\n. \n\nCoverage remained the same at 86.486% when pulling f5eaf8c74b406598b3cd1ea7d556722f971afb7b on efcs:fix-do-not-optimize into 15e9ebaf8363e723478cba23d0651274201c7750 on google:master.\n. \n\nChanges Unknown when pulling 72d81e81672f5063780095df6737ff71b1e08ad0 on Timmmm:patch-1 into  on google:master.\n. \n\nCoverage increased (+0.3%) to 86.745% when pulling e082232c6683c18ba06664788ace0f6dc7ec38d0 on efcs:add-clear-benchmarks into d6aacaf48f18d076feb2ab8c0655efc74e82dfbd on google:master.\n. \n\nCoverage increased (+0.3%) to 86.745% when pulling e082232c6683c18ba06664788ace0f6dc7ec38d0 on efcs:add-clear-benchmarks into d6aacaf48f18d076feb2ab8c0655efc74e82dfbd on google:master.\n. \n\nCoverage remained the same at 86.745% when pulling 6d481fe2c1ae3defde091792cc4e5fb943382d9c on jernkuan:dev/CXXFeatureCheck into b8a2206fb20107c8a60a543928106a0ed71e89b5 on google:master.\n. \n\nCoverage remained the same at 86.745% when pulling b8b9a5c64b3f95534681c00295012688c50ae65f on jernkuan:dev/CXXFeatureCheck into b8a2206fb20107c8a60a543928106a0ed71e89b5 on google:master.\n. \n\nCoverage remained the same at 86.745% when pulling c49ccde365d98d1ade673bd0e473d05351c82632 on jernkuan:dev/CXXFeatureCheck into b8a2206fb20107c8a60a543928106a0ed71e89b5 on google:master.\n. \n\nCoverage increased (+11.5%) to 87.134% when pulling 82d6ade9a4f12331023094702f19616ea63e4fec on astrelni:batch_and_range into 710c2b89d8c6e839e51ac148b4e840ce2c009dbb on google:master.\n. \n\nCoverage increased (+11.4%) to 87.073% when pulling 82d6ade9a4f12331023094702f19616ea63e4fec on astrelni:batch_and_range into 710c2b89d8c6e839e51ac148b4e840ce2c009dbb on google:master.\n. \n\nCoverage increased (+11.4%) to 87.018% when pulling 2dc3b5173a8dcb0372402ac88361fe71746ee28f on astrelni:batch_and_range into 710c2b89d8c6e839e51ac148b4e840ce2c009dbb on google:master.\n. \n\nCoverage increased (+11.2%) to 86.881% when pulling 43a75002a1723fad55d1df8854d90ca8f8eb2a8f on astrelni:batch_and_range into 710c2b89d8c6e839e51ac148b4e840ce2c009dbb on google:master.\n. \n\nCoverage increased (+11.1%) to 86.762% when pulling 77a7f566cc4f66a160639cab51ad0412fe6151d1 on efcs:refactor-headers into 710c2b89d8c6e839e51ac148b4e840ce2c009dbb on google:master.\n. \n\nCoverage remained the same at 86.762% when pulling 95bb543583ccd4d8c0f5f8a647149c90b04dc5a8 on tommadams:rtems_thread_cputime into 9d4b719daeda35acf3a3d81b9ac1f38fc13333d1 on google:master.\n. \n\nCoverage remained the same at 86.762% when pulling 5026cecf147ccbc8ce52a46cd8efa2d4c473f444 on tommadams:rtems_thread_cputime into 9d4b719daeda35acf3a3d81b9ac1f38fc13333d1 on google:master.\n. \n\nCoverage remained the same at 86.762% when pulling 431045094c4c1105a7ae138a4d1db01eeba354dd on clangtidy into ee3cfca651deae5c8af3e873f5d085074cde9fad on master.\n. \n\nCoverage remained the same at 86.762% when pulling 661a048848d84ba3ec73f7b98ca3018327695222 on moreclangtidy into e8fc2a2b8ccd33fefdd7f95e1a2aabee0788c4d6 on master.\n. \n\nCoverage decreased (-8.5%) to 78.306% when pulling 329e0d7fac98008438ca17684cafe2d1661484c1 on vgvassilev:FixesHTMLReport into 5b7683f49e1e9223cf9927b24f6fd3d6bd82e3f8 on google:master.\n. \n\nCoverage increased (+0.4%) to 87.167% when pulling a4d3e345284f563e541e6fb0506b07cef05a9a19 on vgvassilev:AddHTMLReport2 into 5b7683f49e1e9223cf9927b24f6fd3d6bd82e3f8 on google:master.\n. \n\nCoverage increased (+0.1%) to 86.881% when pulling b4b592facd7bb6d6f0c44543b3234c6b191a0ae6 on usercounters into 5b7683f49e1e9223cf9927b24f6fd3d6bd82e3f8 on master.\n. \n\nCoverage increased (+0.1%) to 86.881% when pulling b4b592facd7bb6d6f0c44543b3234c6b191a0ae6 on usercounters into 5b7683f49e1e9223cf9927b24f6fd3d6bd82e3f8 on master.\n. \n\nCoverage increased (+0.06%) to 86.824% when pulling 19dc3959e308a93bbc510b55ffb9162cae43208c on gnarlie:gcc5-compatibility into 5b7683f49e1e9223cf9927b24f6fd3d6bd82e3f8 on google:master.\n. \n\nCoverage remained the same at 86.762% when pulling e2ce2930c55ee542b49d5b37874525af657f7761 on gnarlie:gcc5-compatibility into 5b7683f49e1e9223cf9927b24f6fd3d6bd82e3f8 on google:master.\n. \n\nCoverage increased (+0.05%) to 86.811% when pulling 516d60c3d4d1d7c28a01dfed0ef0e14ca82b556b on LebedevRI:json-fp-precision into 5b7683f49e1e9223cf9927b24f6fd3d6bd82e3f8 on google:master.\n. \n\nCoverage increased (+0.04%) to 86.803% when pulling c9ac77c971060949d09133a2828ef22b8c9a77b3 on LebedevRI:json-fp-precision into 5b7683f49e1e9223cf9927b24f6fd3d6bd82e3f8 on google:master.\n. \n\nCoverage increased (+0.1%) to 86.865% when pulling 957e4dc13409f3dd6a19820ef0c7c3a6780f69c5 on LebedevRI:json-fp-precision into 5b7683f49e1e9223cf9927b24f6fd3d6bd82e3f8 on google:master.\n. \n\nCoverage remained the same at 86.803% when pulling fc580ce5ebf6ac6c3e4b927de3192a330f2e8e46 on LebedevRI:tooling into b9be142d1e2dbc06cf2efd44919f6c422c43cddb on google:master.\n. \n\nCoverage increased (+0.2%) to 86.973% when pulling 1f4c2ed5c0b58ff48ef44580d8ea6ce8774742a4 on LebedevRI:median into d474450b8931c2c5c53e5d950341cb65f39c0f40 on google:master.\n. \n\nCoverage increased (+0.2%) to 86.973% when pulling 52bc7d36621d6a162b67ad16e3089d89bbb4969a on LebedevRI:median into d474450b8931c2c5c53e5d950341cb65f39c0f40 on google:master.\n. \n\nCoverage increased (+0.2%) to 87.035% when pulling c3c579f513d41852e0fcbf07c364acafcc0f8260 on LebedevRI:median into d474450b8931c2c5c53e5d950341cb65f39c0f40 on google:master.\n. \n\nCoverage increased (+0.2%) to 87.005% when pulling 29975f89bfbe6c05f279e307995d9d7af3cd0481 on LebedevRI:median into d474450b8931c2c5c53e5d950341cb65f39c0f40 on google:master.\n. \n\nCoverage increased (+0.2%) to 86.989% when pulling c0aca322126a038627f4589b2b590b04e24ad8d5 on LebedevRI:median into d474450b8931c2c5c53e5d950341cb65f39c0f40 on google:master.\n. \n\nCoverage increased (+0.2%) to 86.981% when pulling ba2736dd0d20b8a9e4845a6c17391300577a30ee on LebedevRI:median into d474450b8931c2c5c53e5d950341cb65f39c0f40 on google:master.\n. \n\nCoverage decreased (-0.8%) to 86.002% when pulling 6279d237182a2671cd8bf7761585da0591d16997 on DerThorsten:metadata into abafced9909c7e5e8f6b8236eecd953caa4f8e6f on google:master.\n. \n\nCoverage remained the same at 86.803% when pulling 65a8995083409ad43a0af3cceb73c6f5af8aed64 on LebedevRI:fixup-test into abafced9909c7e5e8f6b8236eecd953caa4f8e6f on google:master.\n. \n\nCoverage remained the same at 86.803% when pulling 9a27d1f051c2bc37e87bc2a1710f8e478a5073b1 on disconnect3d:master into 3347a20e0e7aff733371cdc2ec371ea73bfdcecb on google:master.\n. \n\nCoverage remained the same at 86.803% when pulling f5a063c4b3c47edd9e0ba2b7b84a936c86f93897 on LebedevRI:compare-bench-args into 3347a20e0e7aff733371cdc2ec371ea73bfdcecb on google:master.\n. \n\nCoverage remained the same at 86.803% when pulling df58b547cae6c20820e82f477816d937482d5280 on pwnall:fix_cross into 3347a20e0e7aff733371cdc2ec371ea73bfdcecb on google:master.\n. \n\nCoverage remained the same at 86.803% when pulling ce7202b0e2dd645f3ccca39c8ec6dec6f145ae2d on pwnall:fix_cross into 3347a20e0e7aff733371cdc2ec371ea73bfdcecb on google:master.\n. \n\nCoverage remained the same at 86.865% when pulling e77ea47ba63c1493ffec41cc7b3aa297fc81d1d9 on fix_counter_rounding into 902936033d90dc42ddaa5ca5c120722d32c90602 on master.\n. \n\nCoverage remained the same at 86.981% when pulling 95f8cfa8f1cbdccd23b00d5be59591300fe9a911 on LebedevRI:cmake-PARENT_SCOPE into a271c36af93c7a3b19dfeb2aefa9ca77a58e52e4 on google:master.\n. \n\nCoverage remained the same at 86.981% when pulling d3e4ada4201eb8bd457803298ee3185e1bf19a28 on LebedevRI:tool-compare-bench-decimals into a271c36af93c7a3b19dfeb2aefa9ca77a58e52e4 on google:master.\n. \n\nCoverage remained the same at 86.981% when pulling fad0639b2a0d3eea870b4a2c3f12969969c94a6e on LebedevRI:tool-compare-bench-decimals into a271c36af93c7a3b19dfeb2aefa9ca77a58e52e4 on google:master.\n. \n\nCoverage decreased (-5.3%) to 81.765% when pulling e323aede2ca44b5dbba434d7123b4b1dfe045589 on reportercleanup into 886585a3b79710d67eb3e7368f0cb8d748eb1d5f on master.\n. \n\nCoverage decreased (-5.3%) to 81.765% when pulling b7f0a931b7346fe61b47ad8b2449760bb9dac837 on reportercleanup into 886585a3b79710d67eb3e7368f0cb8d748eb1d5f on master.\n. \n\nCoverage decreased (-5.3%) to 81.765% when pulling 1846e27ba9e89ae0172830be05da2c775b1fd0ff on reportercleanup into 886585a3b79710d67eb3e7368f0cb8d748eb1d5f on master.\n. \n\nCoverage remained the same at 86.981% when pulling 536b0b82b8ec12fc7e17e6d243633618f294a739 on efcs:fix-cxx11-check into 24b804273382d258b92b3c4beceed3a2ab8ddc65 on google:master.\n. \n\nCoverage remained the same at 86.981% when pulling 4ed90c9ada78d3ab465215ab2eee24f6551097c3 on onto:master into a96ff121b34532bb007c51ffd8e626e38decd732 on google:master.\n. \n\nCoverage increased (+0.06%) to 87.043% when pulling 4ed90c9ada78d3ab465215ab2eee24f6551097c3 on onto:master into a96ff121b34532bb007c51ffd8e626e38decd732 on google:master.\n. \n\nCoverage increased (+0.06%) to 87.043% when pulling f22ab111fa5a91ca512c970acdc04f73c094bbe1 on onto:master into a96ff121b34532bb007c51ffd8e626e38decd732 on google:master.\n. \n\nCoverage remained the same at 87.043% when pulling e82bbdad0b245d74c9951631b92c85ce689770f7 on mwinterb:clangcl_warning_fix into 819adb4cd111df5bbe89f8a3ecccc716b50832ca on google:master.\n. \n\nCoverage remained the same at 87.043% when pulling 6fd9bb20386731a0cb41f7304f2d28cc261cf6d3 on mwinterb:clangcl_warning_fix into 819adb4cd111df5bbe89f8a3ecccc716b50832ca on google:master.\n. \n\nCoverage decreased (-0.2%) to 86.844% when pulling e84a26dbdea98fafe8955035d520907f8d5e877c on fixappveyor into f3cd636f18aff1dc47837b37ccee5331c6427f64 on master.\n. \n\nCoverage decreased (-0.1%) to 86.933% when pulling 6741abbfd6b9f2ded5d1e016623a63eb6af933b2 on fixappveyor into f3cd636f18aff1dc47837b37ccee5331c6427f64 on master.\n. \n\nCoverage decreased (-0.05%) to 86.994% when pulling 098914f7c4bef267500deb4c2dc7c891344b7505 on fixappveyor into f3cd636f18aff1dc47837b37ccee5331c6427f64 on master.\n. \n\nCoverage decreased (-0.05%) to 86.994% when pulling bc3493ed35d4549bb1b6cb6741d5f20fd9d9def9 on fixappveyor into f3cd636f18aff1dc47837b37ccee5331c6427f64 on master.\n. \n\nCoverage decreased (-0.1%) to 86.933% when pulling bc3493ed35d4549bb1b6cb6741d5f20fd9d9def9 on fixappveyor into f3cd636f18aff1dc47837b37ccee5331c6427f64 on master.\n. \n\nCoverage decreased (-0.1%) to 86.933% when pulling 392b33c5716c4b39d31a3cc61925638cd8d5f509 on fixappveyor into f3cd636f18aff1dc47837b37ccee5331c6427f64 on master.\n. \n\nCoverage decreased (-0.1%) to 86.933% when pulling fc696b0a1b512b28673f0fc6ea9d15c6aecb55ef on fixappveyor into f3cd636f18aff1dc47837b37ccee5331c6427f64 on master.\n. \n\nCoverage decreased (-0.1%) to 86.933% when pulling 2988764ff3b8fff842d6df89c556a75e8e0b86da on fixappveyor into f3cd636f18aff1dc47837b37ccee5331c6427f64 on master.\n. \n\nCoverage increased (+0.1%) to 87.192% when pulling 29306b71791b85d6392afdb09ccde1cbe483cfb9 on efcs:add-range-based-for-loop into f3cd636f18aff1dc47837b37ccee5331c6427f64 on google:master.\n. \n\nCoverage increased (+0.09%) to 87.131% when pulling 104251dd69517ecc11611969dc35ed50f4e439c4 on efcs:add-range-based-for-loop into f3cd636f18aff1dc47837b37ccee5331c6427f64 on google:master.\n. \n\nCoverage increased (+0.04%) to 87.083% when pulling b4b5695f6e5a9058ef0b27c264eabb9b5d0ee6cc on efcs:fixappveyor-ericwfattempt into f3cd636f18aff1dc47837b37ccee5331c6427f64 on google:master.\n. \n\nCoverage increased (+0.1%) to 87.145% when pulling 424e0ac232894eb6974d3c837c6f9d3f0de8016c on efcs:fixappveyor-ericwfattempt into f3cd636f18aff1dc47837b37ccee5331c6427f64 on google:master.\n. \n\nCoverage increased (+0.03%) to 87.075% when pulling fbaa583a4942fea3b88fe10f4fe5c2bcab7c55ad on efcs:fixappveyor-ericwfattempt into f3cd636f18aff1dc47837b37ccee5331c6427f64 on google:master.\n. \n\nCoverage increased (+0.06%) to 87.099% when pulling bc45a2459d0c46ef4a4c9e5240a46a7eef9c7dc5 on efcs:fixappveyor-ericwfattempt into f3cd636f18aff1dc47837b37ccee5331c6427f64 on google:master.\n. \n\nCoverage increased (+0.06%) to 87.099% when pulling 7d86c8815ca8e88054e590871a904d5f763d55c1 on efcs:fixappveyor-ericwfattempt into f3cd636f18aff1dc47837b37ccee5331c6427f64 on google:master.\n. \n\nCoverage increased (+0.04%) to 87.083% when pulling c961e7af5980636217e57361f132342ee7c45ad9 on efcs:fixappveyor-ericwfattempt into f3cd636f18aff1dc47837b37ccee5331c6427f64 on google:master.\n. \n\nCoverage decreased (-0.03%) to 87.013% when pulling 86d1d325e4d04354823c356f022f3f02ebccb5be on efcs:fixappveyor-ericwfattempt into f3cd636f18aff1dc47837b37ccee5331c6427f64 on google:master.\n. \n\nCoverage decreased (-0.03%) to 87.013% when pulling 95f66289cdc1e17f0a2528a8f24b0d7b8fc9184a on efcs:fixappveyor-ericwfattempt into f3cd636f18aff1dc47837b37ccee5331c6427f64 on google:master.\n. \n\nCoverage increased (+0.03%) to 87.075% when pulling 2d1759df947df5e6ad996f4af2f9a5fe5f67939a on efcs:fixappveyor-ericwfattempt into f3cd636f18aff1dc47837b37ccee5331c6427f64 on google:master.\n. \n\nCoverage increased (+0.1%) to 87.145% when pulling 45fdee54f3071f554085030c919b51d34ead80c9 on efcs:fixappveyor-ericwfattempt into f3cd636f18aff1dc47837b37ccee5331c6427f64 on google:master.\n. \n\nCoverage remained the same at 87.131% when pulling e5b1c94b2e9e7299661bc115f307312d4ce5c2f2 on Algunenano:conversion into 05267559447a610476515d1d089d142e5941f61b on google:master.\n. \n\nCoverage increased (+0.06%) to 87.192% when pulling 4225a8cb0f3702652bc6d7087b4dcc88dbd8369e on FredTingaud:master into cacd3218080fa648efdfe0b09227ab7f96fb6bb2 on google:master.\n. \n\nCoverage remained the same at 87.131% when pulling 0f446e3b095a35630c631aeb3732b711322da4a7 on FredTingaud:master into cacd3218080fa648efdfe0b09227ab7f96fb6bb2 on google:master.\n. \n\nCoverage remained the same at 87.131% when pulling 7871aab74da6e5ecc0c114e602c3a75cadc0cc2d on FredTingaud:master into cacd3218080fa648efdfe0b09227ab7f96fb6bb2 on google:master.\n. \n\nCoverage decreased (-0.02%) to 87.115% when pulling 67bfa3bb04d9b73f9f3c57c5b0148243a02ee809 on efcs:refactor-keep-running-tests into 2fc2ea0e45578704378ae452b6745d0c54b528fb on google:master.\n. \n\nCoverage decreased (-0.02%) to 87.115% when pulling 8c03afebd28d1c3c381861bc16ce6d84bd9bd336 on efcs:refactor-keep-running-tests into 2fc2ea0e45578704378ae452b6745d0c54b528fb on google:master.\n. \n\nCoverage decreased (-0.02%) to 87.115% when pulling dd71cc8ffbfe5633b5e6300de22f7f17e2a833b3 on efcs:refactor-keep-running-tests into 2fc2ea0e45578704378ae452b6745d0c54b528fb on google:master.\n. \n\nCoverage decreased (-0.02%) to 87.131% when pulling 57bed27615692f0ac3c059e67f855fdd850a4113 on efcs:refactor-keep-running-tests into 22fd1a556eadce4f314a9819ed29cc886e9ba552 on google:master.\n. \n\nCoverage increased (+0.02%) to 87.146% when pulling ccf3345fc4e456a44b5a3c054f833ae7928b42a5 on efcs:count-down-loop into 2fc2ea0e45578704378ae452b6745d0c54b528fb on google:master.\n. \n\nCoverage increased (+0.06%) to 87.192% when pulling b11977606f08066c351de22cf8e2478b64b38231 on Yangqing:master into 25acf220a44ccc41104a690731fcf646cc3e8192 on google:master.\n. \n\nCoverage remained the same at 87.131% when pulling ed6ad3e981c00b4ea2fb96de5214192ab245e3ca on Yangqing:master into 25acf220a44ccc41104a690731fcf646cc3e8192 on google:master.\n. \n\nCoverage remained the same at 87.131% when pulling 500545672cf7b08e741fd9c57694aeb01f55d1d8 on leokoppel:fix_example into 491360b833aaab96818dce256a8409f6296dd995 on google:master.\n. \n\nCoverage remained the same at 87.131% when pulling 6afb95f844830cc279fef16cff4124e91a0735d4 on leokoppel:fix_example into 491360b833aaab96818dce256a8409f6296dd995 on google:master.\n. \n\nCoverage increased (+0.08%) to 87.208% when pulling 0fa51f735b4f6961d775fce88ca075d4a71b494a on ensonic:master into fa341e51cb7f6bce69a7577f4000381a03f61c70 on google:master.\n. \n\nCoverage remained the same at 87.131% when pulling 4fc94fb3ee5dbb747a46e159ee59a02dcac057c0 on fix467 into fa341e51cb7f6bce69a7577f4000381a03f61c70 on master.\n. \n\nCoverage remained the same at 87.131% when pulling bb85ec510ee8b1dc6b386af44f6f754ab4b561e3 on LebedevRI:cla into 4463a60efe6d70ae9af2d58261f30d2fff06920f on google:master.\n. \n\nCoverage increased (+0.06%) to 87.192% when pulling 818785a301be66334e2a91245ac198596af70bb8 on rmheaders into 1e52560157d73d7e180e6e2c8d928be5a8e99795 on master.\n. \n\nCoverage remained the same at 87.131% when pulling 237023c7752f0ce08c0e28c0c71055ce5ad885ee on LebedevRI:tools into f65c6d9a2cfb01d268d0f0aa7aafa2c2ff9f1b8f on google:master.\n. \n\nCoverage increased (+0.06%) to 87.192% when pulling e3bc69a5c54180b803cc8f8007eabc12ec61c725 on LebedevRI:tools into 90aa8665b5080f7a931567fbc92cda883bc870b4 on google:master.\n. \n\nCoverage remained the same at 87.131% when pulling 88bf0f21e531bc5c4a41db0cdde35b42044366b9 on LebedevRI:tools into 90aa8665b5080f7a931567fbc92cda883bc870b4 on google:master.\n. \n\nCoverage remained the same at 87.131% when pulling e64b077c985076c0bcc5512750ad6be4fd09dc6f on LebedevRI:tools into 90aa8665b5080f7a931567fbc92cda883bc870b4 on google:master.\n. \n\nCoverage remained the same at 87.131% when pulling ba13257075f16bdd332128a05faf471962341896 on efcs:attempt-to-fix-mingw into f65c6d9a2cfb01d268d0f0aa7aafa2c2ff9f1b8f on google:master.\n. \n\nCoverage remained the same at 87.131% when pulling 9b6d6c29bd9f72fd13619f8d13675202a80a06c2 on fix476 into f65c6d9a2cfb01d268d0f0aa7aafa2c2ff9f1b8f on master.\n. \n\nCoverage remained the same at 87.131% when pulling 74965c67210e19ee35d3e7a1db8722f8b2bc4c6b on sesse:master into ed5764ea2815aa7cfbdf5ad2693a99fab10e0451 on google:master.\n. \n\nCoverage remained the same at 87.131% when pulling 312b11c5be8db38b08b6d71b9254eb73c8a0bcab on sesse:master into ed5764ea2815aa7cfbdf5ad2693a99fab10e0451 on google:master.\n. \n\nCoverage remained the same at 87.131% when pulling af3703a16ccadcb172871e0e584bef3e499261a6 on krytarowski:netbsd-1 into 0c3ec998c4cc6b7bba7226a3b35a7917613b3802 on google:master.\n. \n\nCoverage decreased (-0.2%) to 87.019% when pulling 56f4f393151eeaed353ed54a76ed50e71bc9d3ea on efcs:refactor-system-info into aad6a5fa767529d3353bd3beb89e126c7b0868ca on google:master.\n. \n\nCoverage increased (+0.4%) to 87.568% when pulling 29a469ce90307d56a93208e11aba53e84cb61833 on efcs:refactor-system-info into aad6a5fa767529d3353bd3beb89e126c7b0868ca on google:master.\n. \n\nCoverage increased (+0.4%) to 87.568% when pulling 8d1b874f9ef1040b0e1dd414b8b5706933c5f1ef on efcs:refactor-system-info into aad6a5fa767529d3353bd3beb89e126c7b0868ca on google:master.\n. \n\nCoverage increased (+0.3%) to 87.538% when pulling f7d4316e27a40508be80f649de26ca8c901bd532 on efcs:refactor-system-info into aad6a5fa767529d3353bd3beb89e126c7b0868ca on google:master.\n. \n\nCoverage decreased (-0.3%) to 86.915% when pulling 6821ab12941ea7d4387443bceeb4263117d552e0 on efcs:refactor-system-info into aad6a5fa767529d3353bd3beb89e126c7b0868ca on google:master.\n. \n\nCoverage decreased (-0.3%) to 86.863% when pulling a2f27f05f5dc50f7293d59ea75f3f2e802336ef4 on efcs:refactor-system-info into aad6a5fa767529d3353bd3beb89e126c7b0868ca on google:master.\n. \n\nCoverage decreased (-0.3%) to 86.847% when pulling 17b91d517fa284d81ec7fa123b2ae220e4119ed6 on efcs:refactor-system-info into aad6a5fa767529d3353bd3beb89e126c7b0868ca on google:master.\n. \n\nCoverage decreased (-0.4%) to 86.791% when pulling 85a832668887b0df8f9873452c8152b67a5f1730 on efcs:refactor-system-info into aad6a5fa767529d3353bd3beb89e126c7b0868ca on google:master.\n. \n\nCoverage decreased (-0.4%) to 86.799% when pulling e82af5b39d9ca784f6b00e5602ba38134a1d046f on efcs:refactor-system-info into aad6a5fa767529d3353bd3beb89e126c7b0868ca on google:master.\n. \n\nCoverage decreased (-0.4%) to 86.799% when pulling a272fcbcd8cd24dd1b86db52ccc111268ddad221 on efcs:refactor-system-info into aad6a5fa767529d3353bd3beb89e126c7b0868ca on google:master.\n. \n\nCoverage decreased (-0.04%) to 86.823% when pulling 79a2610069610f682ef5da6e9a2d849d46c89e8f on LebedevRI:usercounters-consolereporter into 27e0b439cf0f9be329f617056795b4e5c3612f34 on google:master.\n. \n\nCoverage decreased (-0.06%) to 86.799% when pulling f50c91963c0e87f2a8d729dfa33508dcf00cbc2d on efcs:add-googletest into 27e0b439cf0f9be329f617056795b4e5c3612f34 on google:master.\n. \n\nCoverage decreased (-0.06%) to 86.799% when pulling 0d971fd59f97dc8fe4768edce2a29b9ede69e337 on efcs:add-googletest into 27e0b439cf0f9be329f617056795b4e5c3612f34 on google:master.\n. \n\nCoverage decreased (-0.06%) to 86.799% when pulling 1cafd5817206121a4228f31f408364563483b6ac on efcs:add-googletest into 27e0b439cf0f9be329f617056795b4e5c3612f34 on google:master.\n. \n\nCoverage decreased (-0.06%) to 86.799% when pulling 1cafd5817206121a4228f31f408364563483b6ac on efcs:add-googletest into 27e0b439cf0f9be329f617056795b4e5c3612f34 on google:master.\n. \n\nCoverage decreased (-0.06%) to 86.799% when pulling cc5ce7e8223175aebb0f0e6b7b140da74f1c5422 on efcs:add-googletest into 27e0b439cf0f9be329f617056795b4e5c3612f34 on google:master.\n. \n\nCoverage decreased (-0.06%) to 86.799% when pulling 18bf4e12b718b069f69423324f097b1c8705d8d1 on efcs:add-googletest into 27e0b439cf0f9be329f617056795b4e5c3612f34 on google:master.\n. \n\nCoverage decreased (-0.06%) to 86.799% when pulling d4ba15d6bba1422144a05ccc7d48cdf9ac95c47d on efcs:add-googletest into 27e0b439cf0f9be329f617056795b4e5c3612f34 on google:master.\n. \n\nCoverage decreased (-0.04%) to 86.815% when pulling 6a209b9113ff0bd039de0ca0b4c50d2fca8bc499 on efcs:add-googletest into 27e0b439cf0f9be329f617056795b4e5c3612f34 on google:master.\n. \n\nCoverage decreased (-0.06%) to 86.799% when pulling 969426fef867b44f3a04ad63fe248a3484622532 on efcs:add-googletest into 27e0b439cf0f9be329f617056795b4e5c3612f34 on google:master.\n. \n\nCoverage remained the same at 86.926% when pulling fb279bea7f286af8ddf2a9404c09eafee778a67e on efcs:add-googletest into de725e5a7c788311e802f5ed0c3763331a9db60b on google:master.\n. \n\nCoverage increased (+0.06%) to 86.918% when pulling 812050b612f43c26aefeaa3d01f1185c3dc46fa3 on efcs:add-cache-info-windows into 27e0b439cf0f9be329f617056795b4e5c3612f34 on google:master.\n. \n\nCoverage increased (+0.06%) to 86.918% when pulling 70ded702afa62ed109d0340d1076daf7aed82d53 on efcs:add-cache-info-windows into 27e0b439cf0f9be329f617056795b4e5c3612f34 on google:master.\n. \n\nCoverage increased (+0.06%) to 86.918% when pulling da60aaa88f4250c5f0736b82f2d83f420f02d435 on efcs:add-cache-info-windows into 27e0b439cf0f9be329f617056795b4e5c3612f34 on google:master.\n. \n\nCoverage increased (+0.06%) to 86.918% when pulling 748d794e8aac38d6dd66db9184abfd0f7a25d235 on efcs:add-cache-info-windows into 27e0b439cf0f9be329f617056795b4e5c3612f34 on google:master.\n. \n\nCoverage decreased (-0.06%) to 86.799% when pulling 70cf5390eede5173e3f7b937fa02b332789d2272 on LebedevRI:clang-lto into 27e0b439cf0f9be329f617056795b4e5c3612f34 on google:master.\n. \n\nCoverage increased (+0.07%) to 86.926% when pulling 573f91e42d4f8944e46959a05ce7b7dfc34554bc on LebedevRI:clang-lto into 27e0b439cf0f9be329f617056795b4e5c3612f34 on google:master.\n. \n\nCoverage remained the same at 86.918% when pulling 747292b2a7a50e5438f2b8fbc451f18e55278ce5 on efcs:deprecate-csv-reporter into 11dc36822b2aaccb51bb1c75dbd43d2026b23cf6 on google:master.\n. \n\nCoverage remained the same at 86.926% when pulling 83c0b3fc55cad605880dc8fb04583131e0aa135e on Kumar-Kishan:master into ec5684ed759e01b62a82a21cf1a268da3b8ecc5d on google:master.\n. \n\nCoverage remained the same at 86.926% when pulling ccc0edb3f24a222efc4bc766c8ff3053f6a94b3b on Kumar-Kishan:master into ec5684ed759e01b62a82a21cf1a268da3b8ecc5d on google:master.\n. \n\nCoverage remained the same at 86.926% when pulling c76579096a8af5ef5afd65352e07ed499c3d5327 on pwnall:fix-osx-gcc into c45f01866bdfe7c62a83b51dfb974acb7fa5cab6 on google:master.\n. \n\nCoverage remained the same at 86.926% when pulling a29858a61fac022657db7460903fc1c20edde35c on pwnall:travis-gcc into 95a1435b8171f7e781fe0465bff8146609a488c1 on google:master.\n. \n\nCoverage remained the same at 86.926% when pulling 278921bb7da56d014c69c299ecdc2b5fb0a94e80 on eliaskosunen:eliaskosunen-patch-1 into 0bbaeeaf7a557469c1f672ee3b7ca926e1a13e88 on google:master.\n. \n\nCoverage remained the same at 86.926% when pulling b9b56b63d70ea6a7f6dac5c544285de707a3efe7 on ldionne:master into 0bbaeeaf7a557469c1f672ee3b7ca926e1a13e88 on google:master.\n. \n\nCoverage remained the same at 86.926% when pulling 1dad394244066346bc5e4643c37e14543941a076 on monkeynova:master into 0bbaeeaf7a557469c1f672ee3b7ca926e1a13e88 on google:master.\n. \n\nCoverage decreased (-34.2%) to 52.753% when pulling a440db540eba65b9f9034df76237458ff7c86098 on efcs:add-json-header into 3d8dbc69ba6b3cde0b10a44f8d80d6bb195fbe6a on google:v2.\n. \n\nCoverage decreased (-34.2%) to 52.776% when pulling cea3a6f83e05156ce0a2e7aa3ad88204afb6d17e on efcs:add-json-header into 3d8dbc69ba6b3cde0b10a44f8d80d6bb195fbe6a on google:v2.\n. \n\nCoverage decreased (-34.2%) to 52.753% when pulling 71aa25c59cc8b18d0a3d3391145fde6515bbb4db on efcs:add-json-header into 3d8dbc69ba6b3cde0b10a44f8d80d6bb195fbe6a on google:v2.\n. \n\nCoverage decreased (-34.2%) to 52.753% when pulling eefcc6f18fadb5767d086a3de15cefd604a802b0 on efcs:add-json-header into 3d8dbc69ba6b3cde0b10a44f8d80d6bb195fbe6a on google:v2.\n. \n\nCoverage decreased (-34.2%) to 52.753% when pulling 315026a56ee5954fa26efc754f0ea1e76b90159e on efcs:add-json-header into 3d8dbc69ba6b3cde0b10a44f8d80d6bb195fbe6a on google:v2.\n. \n\nCoverage increased (+0.5%) to 87.464% when pulling 3aa27367b18b7befed595d07800e6bea5e18dd56 on efcs:add-json-header into 3d8dbc69ba6b3cde0b10a44f8d80d6bb195fbe6a on google:v2.\n. \n\nCoverage increased (+0.5%) to 87.464% when pulling 78860209a3bad713d066f49ba78343a504287c85 on efcs:add-json-header into 5471e3d7b286d6024f11ac6b58acaf9d636f4ba6 on google:v2.\n. \n\nCoverage decreased (-0.6%) to 86.285% when pulling b01b0356cb6fb52d170630783b6ca2eab8b12305 on efcs:v2-remove-csv into 3d8dbc69ba6b3cde0b10a44f8d80d6bb195fbe6a on google:v2.\n. \n\nCoverage decreased (-0.6%) to 86.285% when pulling 19c41a35f4c47276a467bbd7551b9d281f9b0c1e on efcs:v2-remove-csv into 3d8dbc69ba6b3cde0b10a44f8d80d6bb195fbe6a on google:v2.\n. \n\nCoverage remained the same at 86.926% when pulling 39ffb950775db644a04c9bdacee47fdb8cc8d8fc on qzmfranklin:bazel into e4ccad7c4a36f2a6dfaa2d391c00c5ec4ea63d7c on google:master.\n. \n\nCoverage remained the same at 86.926% when pulling a1f5d4362f6359b13822c443de4152a2ce2805dd on qzmfranklin:bazel into e4ccad7c4a36f2a6dfaa2d391c00c5ec4ea63d7c on google:master.\n. \n\nCoverage remained the same at 86.926% when pulling 1fce7a8357c51d7fb6fb527418bec42540e40b8b on qzmfranklin:bazel into e4ccad7c4a36f2a6dfaa2d391c00c5ec4ea63d7c on google:master.\n. \n\nCoverage remained the same at 86.926% when pulling a08d3f4df1312d0ff1d110983bc47d3b23ec9d74 on winstondu:patch-1 into e4ccad7c4a36f2a6dfaa2d391c00c5ec4ea63d7c on google:master.\n. \n\nCoverage increased (+0.06%) to 86.989% when pulling 0d91e100ab7b8b2675e4f35e60d1646b82b4e9f7 on lijinpei:v2 into 3d8dbc69ba6b3cde0b10a44f8d80d6bb195fbe6a on google:v2.\n. \n\nCoverage increased (+0.01%) to 86.939% when pulling 4832b6b785c83cafe1af0dbb16c92d6e9abfdfec on lijinpei:v2 into 3d8dbc69ba6b3cde0b10a44f8d80d6bb195fbe6a on google:v2.\n. \n\nCoverage increased (+0.1%) to 87.042% when pulling 975dcc51086e32c5fca15f50ccc05c6ad9d739ee on lijinpei:v2 into 3d8dbc69ba6b3cde0b10a44f8d80d6bb195fbe6a on google:v2.\n. \n\nCoverage increased (+0.06%) to 86.984% when pulling 975dcc51086e32c5fca15f50ccc05c6ad9d739ee on lijinpei:v2 into 3d8dbc69ba6b3cde0b10a44f8d80d6bb195fbe6a on google:v2.\n. \n\nCoverage remained the same at 86.926% when pulling 7def1192c3eb7d1773b49a4b34646bc855e9e291 on Maratyszcza:master into e4ccad7c4a36f2a6dfaa2d391c00c5ec4ea63d7c on google:master.\n. \n\nCoverage remained the same at 86.926% when pulling 7def1192c3eb7d1773b49a4b34646bc855e9e291 on Maratyszcza:master into e4ccad7c4a36f2a6dfaa2d391c00c5ec4ea63d7c on google:master.\n. \n\nCoverage remained the same at 86.926% when pulling b5ba7a91470ebe7dd03b1121373c4f18bdbafbbd on Maratyszcza:master into e4ccad7c4a36f2a6dfaa2d391c00c5ec4ea63d7c on google:master.\n. \n\nCoverage remained the same at 86.926% when pulling be535f8326ede3c171f93168609ed1fad54951cc on Maratyszcza:master into e4ccad7c4a36f2a6dfaa2d391c00c5ec4ea63d7c on google:master.\n. \n\nCoverage remained the same at 86.926% when pulling be535f8326ede3c171f93168609ed1fad54951cc on Maratyszcza:master into e4ccad7c4a36f2a6dfaa2d391c00c5ec4ea63d7c on google:master.\n. \n\nCoverage decreased (-0.06%) to 86.926% when pulling 33a3e037f78c8dae2ba84089a38bf889fa2e8ed7 on Maratyszcza:master into 052421c82337774e769051581a00e66dca58c122 on google:master.\n. \n\nCoverage decreased (-0.06%) to 86.926% when pulling 33a3e037f78c8dae2ba84089a38bf889fa2e8ed7 on Maratyszcza:master into 052421c82337774e769051581a00e66dca58c122 on google:master.\n. \n\nCoverage decreased (-0.06%) to 86.926% when pulling a01cb7257eae1f3612c1132430148db9d2ff7d8f on Maratyszcza:master into 052421c82337774e769051581a00e66dca58c122 on google:master.\n. \n\nCoverage remained the same at 86.926% when pulling 052421c82337774e769051581a00e66dca58c122 on master into 3d8dbc69ba6b3cde0b10a44f8d80d6bb195fbe6a on v2.\n. \n\nCoverage decreased (-0.06%) to 86.926% when pulling 778b85a7a938edccdb77b6c4fc0610999131bc1a on efcs:fix-gtest-install into 052421c82337774e769051581a00e66dca58c122 on google:master.\n. \n\nCoverage increased (+0.6%) to 87.478% when pulling 78860209a3bad713d066f49ba78343a504287c85 on efcs:add-json-header-squash into 5471e3d7b286d6024f11ac6b58acaf9d636f4ba6 on google:v2.\n. \n\nCoverage decreased (-0.002%) to 86.924% when pulling 5d27675909b9baf0c5ff38b2cf3b1f79251097a6 on lijinpei:v2-fixture into 5471e3d7b286d6024f11ac6b58acaf9d636f4ba6 on google:v2.\n. \n\nCoverage increased (+0.01%) to 86.939% when pulling 0800b71ec225659d3698def20bbb409318de95d6 on lijinpei:v2-fixture into e1c3a83b8197cf02e794f61228461c27d4e78cfb on google:master.\n. \n\nCoverage increased (+0.01%) to 86.939% when pulling c02a75a2af34dab953c50ad01083e5af73615422 on lijinpei:v2-fixture into e1c3a83b8197cf02e794f61228461c27d4e78cfb on google:master.\n. \n\nCoverage decreased (-0.01%) to 86.916% when pulling 84e24f4843b95dcaf8f1a05d5411e2c9d25176d0 on lijinpei:v2-fixture into e1c3a83b8197cf02e794f61228461c27d4e78cfb on google:master.\n. \n\nCoverage decreased (-0.01%) to 86.916% when pulling 8bc7f4a365c47e23a57bdf5e3785fda6ea24d6f7 on lijinpei:v2-fixture into e1c3a83b8197cf02e794f61228461c27d4e78cfb on google:master.\n. \n\nCoverage decreased (-0.2%) to 86.682% when pulling f0008ff61b82b53facda7c6291bd342e197e8727 on lijinpei:v2-fixture into e1c3a83b8197cf02e794f61228461c27d4e78cfb on google:master.\n. \n\nCoverage decreased (-0.2%) to 86.682% when pulling df045ffa96bb5b30035701de7b6be5979d705f66 on lijinpei:v2-fixture into 9f5694ceb6ed34c1eecb1be17415f3fb2d19828f on google:master.\n. \n\nCoverage decreased (-0.2%) to 86.741% when pulling f8ed087957c6e0ef2ee26ed69fb3a1dff9a37dd4 on lijinpei:v2-fixture into 9f5694ceb6ed34c1eecb1be17415f3fb2d19828f on google:master.\n. \n\nCoverage decreased (-0.01%) to 86.916% when pulling c88f257bb08023d434d5d293851e397a40c6547e on lijinpei:v2-fixture into 4fe0206b658c6aa1d74a2278c26b50f0938a4e64 on google:master.\n. \n\nCoverage remained the same at 86.926% when pulling 58ad5f619f61f621b8d3e367ae3d6c46d86b0c3b on fix513 into e1c3a83b8197cf02e794f61228461c27d4e78cfb on master.\n. \n\nCoverage increased (+0.06%) to 86.985% when pulling 8d180bba4a26cb7f5a58792e9df2486346030cfc on NAThompson:master into 9f5694ceb6ed34c1eecb1be17415f3fb2d19828f on google:master.\n. \n\nCoverage remained the same at 86.926% when pulling d5f07bf949d714b27969d7eefab33b8e5320c64f on NAThompson:master into 9f5694ceb6ed34c1eecb1be17415f3fb2d19828f on google:master.\n. \n\nCoverage remained the same at 86.926% when pulling 9dcdd393e5b4f273b25c162bacf4cdc332cc05e6 on NAThompson:master into 9f5694ceb6ed34c1eecb1be17415f3fb2d19828f on google:master.\n. \n\nCoverage remained the same at 86.926% when pulling 55c89ef2e4391720013af4772bd15c1e57f699ee on oskidan:fix-sysinfo-implicit-conversion-error into 9f5694ceb6ed34c1eecb1be17415f3fb2d19828f on google:master.\n. \n\nCoverage increased (+0.07%) to 87.0% when pulling 0505aebb2947881a749d8eab7a4725daecd157d6 on man-contributions:issue-516 into 4fe0206b658c6aa1d74a2278c26b50f0938a4e64 on google:master.\n. \n\nCoverage remained the same at 86.926% when pulling c836e97de7da9ba0a85ff45fe15db5779bee1afb on clangtidy into 4fe0206b658c6aa1d74a2278c26b50f0938a4e64 on master.\n. \n\nCoverage increased (+0.1%) to 87.048% when pulling 17180dd26f0cdfb13ada7d930c1a8174ea21fcdd on sam-panzer:master into df415adb2abac7fc275c2cd25b0ef6e117688e1e on google:master.\n. \n\nCoverage increased (+0.06%) to 86.985% when pulling 38f847533a41dad8f7ab2afeeea8300554df050d on aJetHorn:visualstudio-gitignore into df415adb2abac7fc275c2cd25b0ef6e117688e1e on google:master.\n. \n\nCoverage remained the same at 86.926% when pulling 0a0748248b249257483194c40ec1fc936b16da02 on aJetHorn:exclude-build-32b-for-msvc into df415adb2abac7fc275c2cd25b0ef6e117688e1e on google:master.\n. \n\nCoverage increased (+0.01%) to 87.061% when pulling c4e1d41998f62c65d83af66d1475ff237c9a166e on sam-panzer:master into 296ec5693e7dae47c50244a70297172fab69ccae on google:master.\n. \n\nCoverage increased (+0.008%) to 87.055% when pulling 9a6ab3578dbdaf6b5e1da755f88b8f2e43baaf47 on efcs:improve-state-packing into dd8dcc8da1fb03cced6129041e3f06a607889346 on google:master.\n. \n\nCoverage remained the same at 87.048% when pulling a2584a5f6dc3fe17ed435ef23c4209e182cd1eef on efcs:fix-travis-timeout into dd8dcc8da1fb03cced6129041e3f06a607889346 on google:master.\n. \n\nCoverage remained the same at 87.048% when pulling 498633cb4f2d5614990e1d8fc755c5325942f91a on efcs:fix-gtest-warning into 296ec5693e7dae47c50244a70297172fab69ccae on google:master.\n. \n\nCoverage increased (+0.008%) to 87.01% when pulling 1c3a90cf4c917ef74bc3e2cebb3ff75f41ec9396 on ianloic:fuchsia into 3924ee7b8a6c6427083662c861c0f51be2a38bd9 on google:master.\n. \n\nCoverage increased (+0.04%) to 87.048% when pulling fd49f8b1a8032924ed2052221a19a9ac8f67edb4 on jmillikin-stripe:bazel-build into 6ecf8a8e80b92a7292558ae32466b4df331c851d on google:master.\n. \n\nCoverage increased (+0.04%) to 87.048% when pulling 7c6cd9613c4834cf1a912d8e8c56115b50edf41f on efcs:print-exe-name into 6ecf8a8e80b92a7292558ae32466b4df331c851d on google:master.\n. \n\nCoverage remained the same at 87.01% when pulling 83492643cd73724ed51473d285eb46371a4ccc66 on jwakely:master into 858688b845b86dc43a7e23f8ec0f94a8e63bfe20 on google:master.\n. \n\nCoverage remained the same at 87.01% when pulling 8f42cc04d83dd32848a495b3fee969507141bee4 on jwakely:pthreads-note into 858688b845b86dc43a7e23f8ec0f94a8e63bfe20 on google:master.\n. \n\nCoverage decreased (-0.02%) to 87.048% when pulling 54b54a25931b96185620e4e36a93db463b240b32 on alekseyshl:solaris-support into 56f52ee228783547f544d9ac4a533574b9010e3f on google:master.\n. \n\nCoverage remained the same at 87.063% when pulling 94e51b0280b48823e9f4ae31b6e4df413b179d44 on guoyr:s390x-support into 56f52ee228783547f544d9ac4a533574b9010e3f on google:master.\n. \n\nCoverage increased (+0.07%) to 87.121% when pulling dfc159d9522382e644b7b4a0031c7bf9bc559f7e on winksaville:Spelling-fixes into 47df49e5731fd795043bd40c0285ba44ab3e2dbb on google:master.\n. \n\nCoverage remained the same at 87.048% when pulling d51efb1ef0f08728565b75c701d92e35d9195b8b on winksaville:Rename-StrCat-to-StringCat into 47df49e5731fd795043bd40c0285ba44ab3e2dbb on google:master.\n. \n\nCoverage increased (+0.06%) to 87.114% when pulling 250d0a335062296434590e1166047459c5705011 on int64 into e7eb54b5f8d29f3d91f5fbda0909c65c2ae70ba9 on master.\n. \n\nCoverage decreased (-0.02%) to 87.048% when pulling fa67a076e40476b08bcd79683673cae25b07509b on thread_classes into a9beffda0b89a6995372100456a4ad894d29b93b on master.\n. \n\nCoverage remained the same at 87.048% when pulling 23035e049907b36e52771967edcb002f4b28286e on regex into e668e2a1baaa969a8e5a9220e3949e3202dea9e8 on master.\n. \n\nCoverage increased (+0.03%) to 87.101% when pulling 5070150ea9d107df09c20bc1219932d7903cecd3 on efcs:make-unpredictable into 7b03df7ff76844a39359e9233f31ca8cdb073313 on google:master.\n. \n\nCoverage decreased (-0.02%) to 87.055% when pulling 72cfb607ca853ceaf7f4196eeea5a08bb225b6e8 on efcs:cleanup-cmake-for-tests into 7b03df7ff76844a39359e9233f31ca8cdb073313 on google:master.\n. \n\nCoverage decreased (-0.02%) to 87.055% when pulling 6ef6e900541feafe949f1098e09e4e10c2a4c240 on initialize into 7b03df7ff76844a39359e9233f31ca8cdb073313 on master.\n. \n\nCoverage remained the same at 87.07% when pulling 57714288a338f86447c56ce18065f3062fed37c9 on chfast:fix-double-inline into 9913418d323e64a0111ca0da81388260c2bbe1e9 on google:master.\n. \n\nCoverage increased (+0.02%) to 87.07% when pulling 9a34a2451f9903d1709ab95e4de6cfc083509f78 on FredTingaud:master into 2844167ff99edb63f629c9768ea1f3f94ebfb8fe on google:master.\n. \n\nCoverage increased (+0.07%) to 87.181% when pulling 9e60580390b0343132c21e2ab986603f6b9cc561 on BaaMeow:master into 8986839e4ac67facb52abc7fff3d5082e854aab5 on google:master.\n. \n\nCoverage remained the same at 87.07% when pulling 0f95e44df7b6e5eb0c3f36bc84d7071b9a630c07 on int64_result into 50ffc781b1df6d582d6a453b9942cc8cb512db69 on master.\n. \n\nCoverage increased (+0.008%) to 87.078% when pulling 3cd6380be7e725d699e71e8c6dac0d1d059a0281 on iter_report into 64e5a13fa05693dc0243817d6081d7c0e5081d6f on master.\n. \n\nCoverage remained the same at 87.078% when pulling 7434a6ee2e9a979e98c96e0d91a0492ee3e2752a on Yangqing:master into c4858d8012acd54d8ef89053c74e9cdcf0fa6649 on google:master.\n. \n\nCoverage increased (+0.02%) to 87.093% when pulling 38f7becc70fd4dba467f429ec7224543e2184359 on pwnall:msvc-warn into c4858d8012acd54d8ef89053c74e9cdcf0fa6649 on google:master.\n. \n\nCoverage increased (+0.8%) to 87.869% when pulling 7e8578f4fdadc7aa57b910abf70a590f23746148 on FredTingaud:wip/standard-dev into c4858d8012acd54d8ef89053c74e9cdcf0fa6649 on google:master.\n. \n\nCoverage increased (+0.04%) to 87.115% when pulling 9ae109dc4ba4dd442d08b774feb7ad3157dd48be on Tim020:master into 105ac14b2f671f7d448f621290404098189f424d on google:master.\n. \n\nCoverage remained the same at 87.115% when pulling bfc1c0dec88c43feeece62863f0806624b146e6d on astrelni:bazel_pthread into ed1bac8434b3f970a5b7de3fea8f09dc4b1773ee on google:master.\n. \n\nCoverage remained the same at 87.188% when pulling e4d22a19e77a80fd4b3657cde89be616561ef231 on astrelni:msvc_link_pragma into b678a2026df75bdcaebbda40440b8495babf078e on google:master.\n. \n\nCoverage decreased (-0.01%) to 87.174% when pulling afe692d5140edf4f21df3f723f6d4a3538814078 on astrelni:msvc_link_bazel into b678a2026df75bdcaebbda40440b8495babf078e on google:master.\n. \n\nCoverage decreased (-0.07%) to 87.115% when pulling 0fceac33252bc3545583aa462d27b063a095e895 on NanXiao:openbsd_porting into b678a2026df75bdcaebbda40440b8495babf078e on google:master.\n. \n\nCoverage remained the same at 87.115% when pulling 750159a060f95798e0de7a960501c7b99c48a80c on sbc100:emscripten into ea5551e7b3129d8ee468b15e2a4242edd610ee02 on google:master.\n. \n\nCoverage increased (+0.01%) to 87.13% when pulling b278622cb3f921f0aac11ec0af1d0621e748afc4 on LebedevRI:cmake into 8986839e4ac67facb52abc7fff3d5082e854aab5 on google:master.\n. \n\nCoverage remained the same at 87.115% when pulling b291847cd7220bc301f27ff7ac87ab2c5d89a84e on LebedevRI:python3 into 8986839e4ac67facb52abc7fff3d5082e854aab5 on google:master.\n. \n\nCoverage increased (+0.03%) to 87.145% when pulling 03c02a1a4c86ff56c5a3e6b4966aa0fd09bac550 on sam-panzer:master into 8986839e4ac67facb52abc7fff3d5082e854aab5 on google:master.\n. \n\nCoverage remained the same at 87.115% when pulling d7c954ac2188c5f40fa06830badc763a7260b8b6 on php1ic:fix-GetGitVersion into 8986839e4ac67facb52abc7fff3d5082e854aab5 on google:master.\n. \n\nCoverage increased (+0.04%) to 87.159% when pulling 2fc31848d3d25ae3f12bdf77a42fccc422d70bc8 on NanXiao:remove-unnessary_memset into 8986839e4ac67facb52abc7fff3d5082e854aab5 on google:master.\n. \n\nCoverage remained the same at 87.159% when pulling c7080bd6a4ff28f01863f95f6564e7a9f3066623 on LebedevRI:tools-ttest into 6d74c0625b8e88c1afce72b4f383c91b9a99dbe6 on google:master.\n. \n\nCoverage remained the same at 87.159% when pulling 9e29f61d91612ed012bb064358498d5ce7681444 on denizevrenci:split_list into e90801ae475f23877319d67b96c07bf0c52405f3 on google:master.\n. \n\nCoverage increased (+0.01%) to 87.172% when pulling b9b7b872fb05a67ec770ed1be757539d00f6ed1d on Maratyszcza:fix-android-gnustl into 4c2af0788977d3bd900585528c2d08b875b2cd39 on google:master.\n. \n\nCoverage increased (+0.01%) to 87.174% when pulling 83f0d94a584a04122e310487c37e6301df46210e on sam-panzer:master into 6d74c0625b8e88c1afce72b4f383c91b9a99dbe6 on google:master.\n. \n\nCoverage decreased (-0.3%) to 86.814% when pulling 305671d63a3a97b343748b24d730efedfe8dd210 on BaaMeow:feature/names_id_tooling into 6d74c0625b8e88c1afce72b4f383c91b9a99dbe6 on google:master.\n. \n\nCoverage remained the same at 87.159% when pulling 75053ebe2a5fba24c4d28094fc41d285977c0b5f on mattreecebentley:master into 6d74c0625b8e88c1afce72b4f383c91b9a99dbe6 on google:master.\n. \n\nCoverage decreased (-0.007%) to 87.239% when pulling d1f921e855cbd6ee2d63ba7df89aa4e6887344f2 on astrelni:benchmark_main into d7aed73677888834d4e8af2b300d01bfb724b70f on google:master.\n. \n\nCoverage remained the same at 87.181% when pulling 2b93eea77b25e6fbeca5a370eb722c564d1720fd on stats_test into ec0f69c28e412ec1fb1e8d170ada4faeebdc8293 on master.\n. \n\nCoverage remained the same at 87.181% when pulling afec4afc23994820d80b57708b4b29acd8dc65b7 on gcc@7-timeouts into a6a1b0d765b116bb9c777d45a299ae84a2760981 on master.\n. \n\nCoverage remained the same at 87.181% when pulling 5644d06f87c3aa65cda3110d8b1a5e04f6482985 on clang-format into 16703ff83c1ae6d53e5155df3bb3ab0bc96083be on master.\n. \n\nCoverage decreased (-0.06%) to 87.181% when pulling 1cfad9f7167c441d44a8452014be6782e4eac15d on iOS-argv into d07372e64ba16fe3d81bfffbdad7635d19a29198 on master.\n. \n\nCoverage remained the same at 87.181% when pulling 3cdd5abe1724c00d77957241167560bd471afb77 on deprecation-error into 4fbfa2f3368cb8d8a0cba48edda584c7dd9f0a14 on master.\n. \n\nCoverage decreased (-0.02%) to 87.157% when pulling 8d20368cbdb781946fbd133ce79e4469a8c7e7d4 on BaaMeow:editorial/formatting into 4fbfa2f3368cb8d8a0cba48edda584c7dd9f0a14 on google:master.\n. \n\nCoverage remained the same at 87.157% when pulling b98d4e2ea5817774a63e1bd3b9afa5669f40ce84 on sergiud:cmake-config-versioning into 7fb3c564e51ce3aa6100484a0b25c603ea5fd123 on google:master.\n. \n\nCoverage remained the same at 87.157% when pulling 89f67d517288e72c20d670c9695c291453112863 on Maratyszcza:fix-cmake-3.5-build into 1301f53e3173e5e8c1583b9b57a34bbbd5970366 on google:master.\n. \n\nCoverage increased (+2.1%) to 89.288% when pulling 581b0bb5ed50dd44f14ab77e76cd43f808b0a06d on document_deps into 505be96ab23056580a3a2315abba048f4428b04e on master.\n. \n\nCoverage increased (+0.3%) to 87.192% when pulling 69a9e90b327db58adc21e332466814c5a5f05c7c on BaaMeow:feature/names_id_json_tooling into af441fc1143e33e539ceec4df67c2d95ac2bf5f8 on google:master.\n. \n\nCoverage decreased (-0.01%) to 87.157% when pulling 23df0578b473bef9bbdfea7475dd5bb82bf53d18 on LebedevRI:tools into 151ead6242b2075b5a3d55905440a3aab245a800 on google:master.\n. \n\nCoverage increased (+0.5%) to 87.674% when pulling 285b233f11d55c4db5601151cf5cfee24bc3ed5d on LebedevRI:counter-flags into 7d03f2df490c89b2a2055e9be4e2c36db5aedd80 on google:master.\n. \n\nCoverage remained the same at 87.157% when pulling 6cc299a9292d647ffb4132444dcccbbac544b2c4 on kanak:run_first_benchmark_doc into 7d03f2df490c89b2a2055e9be4e2c36db5aedd80 on google:master.\n. \n\nCoverage remained the same at 87.157% when pulling cc616c9bf3f6cc43f87f90ef9bf74bb752f4223f on fix623 into 7d03f2df490c89b2a2055e9be4e2c36db5aedd80 on master.\n. \n\nCoverage decreased (-0.7%) to 86.962% when pulling 6469a714d2184aa64ff597535e319d8f560b8d8e on memory into 847c0069021ade355b7678a305f3ac4e4d6f7e79 on master.\n. \n\nCoverage remained the same at 87.674% when pulling f7d2c8661e53c624fe163168624a5f37e3f133c9 on mumumu:patch-1 into b123abdcf4a004ffadca7834d614bdeb36c843c0 on google:master.\n. \n\nCoverage remained the same at 87.674% when pulling bfebedcb92ae561c32afaec3c261ac3a85f153af on nazavode:fix/intel-1875 into 847c0069021ade355b7678a305f3ac4e4d6f7e79 on google:master.\n. \n\nCoverage decreased (-0.01%) to 87.674% when pulling 1f7e72e25464f6a00dba92a8a1c7c67b1527e8a2 on nazavode:fix/intel into 5946795e82f778fff891c37f56c0d7a76a118bf9 on google:master.\n. \n\nCoverage remained the same at 87.674% when pulling e08d744a783e71161b85e6838e399815dd1b4cec on nazavode:fix/contributors into 0c21bc369aa157c10e63f4e392fd19768ac7e05b on google:master.\n. \n\nCoverage increased (+0.08%) to 87.759% when pulling e1150acab90b39bd7da79622c0fa340d78f09f41 on atdt:report_loadavg into 1f35fa4aa71bffb5e5672f7ca876561d6adef4fd on google:master.\n. \n\nCoverage remained the same at 87.674% when pulling dfe9fde95e6f102dfc2fcb71700252f424315619 on Croydon:croydon/fix_32_msvc into 1f35fa4aa71bffb5e5672f7ca876561d6adef4fd on google:master.\n. \n\nCoverage remained the same at 87.674% when pulling 1d25a147e21660534f2900d1a87c9529df9bf85a on LebedevRI:deduplication into 63e183b38945a0d67671f94bd3f74d7ffb6f5a82 on google:master.\n. \n\nCoverage remained the same at 86.962% when pulling a88803354cb53362c71295d8571d5531c820b8dc on int64_t into f965eab5083010ea7c3a8298176a837e5bff3016 on master.\n. \n\nCoverage remained the same at 89.41% when pulling 866b12145cc9f9e9f2ba3732e7d18d2d7f72786c on Croydon:conan into eec9a8e4976a397988c15e5a4b71a542375a2240 on google:master.\n. \n\nCoverage remained the same at 86.962% when pulling 701fe252face7066f3f40d04275af074f1210ef6 on readme into f965eab5083010ea7c3a8298176a837e5bff3016 on master.\n. \n\nCoverage decreased (-0.1%) to 86.862% when pulling 514fbfe942d7292e32040129cacad4f0bab00b2a on kbobyrev:default-compile-error into d939634b8ce7e0741a79c1c1f22205fae54b375d on google:master.\n. \n\nCoverage increased (+0.02%) to 89.047% when pulling deae1d940c46c1d0caca2a4b397c5cebb14cb84d on private_ctor into eb8cbec0776455463274ea9947ab0ecfe0f768fe on master.\n. \n\nCoverage remained the same at 86.862% when pulling 1cfad9f7167c441d44a8452014be6782e4eac15d on iOS-argv into 94c4d6d5c65b8f0a5f9963db7d93e9568f8aa3db on master.\n. \n\nCoverage increased (+0.02%) to 86.877% when pulling 04f9b2b9ec53257650ca60f2ad663407981f1c16 on BaaMeow:fix/json_path_names into 94c4d6d5c65b8f0a5f9963db7d93e9568f8aa3db on google:master.\n. \n\nCoverage remained the same at 86.877% when pulling 7c917ebc2c7e9405abb470ee7acdbb8b1835faab on bmwiedemann:j1 into af441fc1143e33e539ceec4df67c2d95ac2bf5f8 on google:master.\n. \n\nCoverage remained the same at 87.635% when pulling d6fee5fb404307d27491ce3fe9fec9ecfeeafd68 on LebedevRI:deprecation into caa2fcb19c83e233bdcaf56d299b9951b8414066 on google:master.\n. \n\nCoverage remained the same at 86.877% when pulling 4ac14170a60bad0569ee7f1482dddf80407fdb0e on LebedevRI:rename-ReportMode into af441fc1143e33e539ceec4df67c2d95ac2bf5f8 on google:master.\n. \n\nCoverage increased (+0.7%) to 87.635% when pulling 2706e3112a89639ab7912f03582a18fe5bdb9613 on LebedevRI:counter-thousand into d9cab612e40017af10bddaa5b60c7067032a9e1c on google:master.\n. \n\nCoverage increased (+0.01%) to 86.887% when pulling 8c0cdebcbb498c1750e0608f9d25deda3766b6ec on LebedevRI:benchmark-type into af441fc1143e33e539ceec4df67c2d95ac2bf5f8 on google:master.\n. \n\nCoverage decreased (-0.008%) to 86.869% when pulling 18577561eb5d5ddccfa8117f05fe8dae9d1bcb6d on GevArakelyan:master into af441fc1143e33e539ceec4df67c2d95ac2bf5f8 on google:master.\n. \n\nCoverage remained the same at 86.887% when pulling f6db093979c17d40f89733f0b169ab75545ef5cd on LebedevRI:but-its-not-really-a-console-reporter into 8688c5c4cfa1527ceca2136b2a738d9712a01890 on google:master.\n. \n\nCoverage increased (+0.7%) to 88.351% when pulling 4f05864e0fd91d5d92df0a0f6f28b288e709dc1a on LebedevRI:display-aggregates-only into f0901417c89d123474e6b91365029cfe32cf89dc on google:master.\n. \n\nCoverage remained the same at 87.635% when pulling 0cbaf603e3626fee5ed76f91105ceadc75d5d88f on pseyfert:typo into 5159967520f45d2bb31b226b810f4f8dc03df1c4 on google:master.\n. \n\nCoverage remained the same at 87.635% when pulling cc9cd9668cb9665767bf695ca998d9922112f92a on LebedevRI:llvm-osx into fbfc495d7f810dbab8d75449acd4fa1ced17eb0b on google:master.\n. \n\nCoverage remained the same at 87.635% when pulling 5c16677f7e3c82675d4d597031528ee3b17e3f7b on snnn:master into fbfc495d7f810dbab8d75449acd4fa1ced17eb0b on google:master.\n. \n\nCoverage remained the same at 87.635% when pulling 5542a4f20efac43895e075918ea4b3c5788dd655 on LebedevRI:travis-osx-32bit into 305ba313be0973e83efd70ccc365d284256f6c50 on google:master.\n. \n\nCoverage remained the same at 87.635% when pulling 9962437490bb939838a8520fc8bb4b2f635dc122 on mattkretz:fix_cmake_errors into f0901417c89d123474e6b91365029cfe32cf89dc on google:master.\n. \n\nCoverage remained the same at 87.635% when pulling bbf887d3604c54d42cfcc88d3bf9d08ffefb4e31 on LebedevRI:llvm into f0901417c89d123474e6b91365029cfe32cf89dc on google:master.\n. \n\nCoverage remained the same at 88.351% when pulling ded5ecb81b5d7f7c3ccb87c935ec827fb7e34af1 on LebedevRI:tooling-display-aggregates-only into c614dfc0d4eadcd19b188ff9c7e226c138f894a1 on google:master.\n. \n\nCoverage increased (+0.9%) to 89.274% when pulling ca09fac925996b25fdfab7b39fb2e5a472b42845 on LebedevRI:track-more-names into c614dfc0d4eadcd19b188ff9c7e226c138f894a1 on google:master.\n. \n\nCoverage decreased (-0.3%) to 89.022% when pulling bc446a768f59fcf69346c4ec30434d17399c0791 on LebedevRI:sugar-is-bad-for-health into 58588476ce52c0af4cb54f9c596c4579f78bc953 on google:master.\n. \n\nCoverage remained the same at 89.022% when pulling d2e9a41f52a34b6476b4d4cc523733d66fcb1c0c on LebedevRI:tooling-unbreak-repetitions into a5e9c061d9363403c93d16cf79d16f30f94063f6 on google:master.\n. \n\nCoverage remained the same at 89.022% when pulling 610f3f39c27b293313db2e0f5144c19132eb2925 on mstorsjo:lowercase-windows into a5e9c061d9363403c93d16cf79d16f30f94063f6 on google:master.\n. \n\nCoverage remained the same at 89.022% when pulling 6cbe2407a827c50590cbacc6601626029ea97085 on mstorsjo:windows-arm into 52613079824ac58d06c070aa9fbbb186a5859e2c on google:master.\n. \n\nCoverage remained the same at 89.022% when pulling d1e5a1d579d7469a1519abbda9a95a5bf98796d5 on janisozaur:patch-1 into aad33aab3c1a8db06053665af39e4679bc774b74 on google:master.\n. \n\nCoverage remained the same at 89.022% when pulling a4cfaad3c0217293a924b4045ac5233186f731e8 on litmusautomation:configurable-deployment-destinations into aad33aab3c1a8db06053665af39e4679bc774b74 on google:master.\n. \n\nCoverage increased (+0.5%) to 87.663% when pulling d4e15a700a22b6ae50d3efd9cc1fb6ca4825cb00 on private_ctor into 1cfad9f7167c441d44a8452014be6782e4eac15d on iOS-argv.\n. \n\nCoverage increased (+0.2%) to 89.256% when pulling b3ec1c8d5409394faabb9796cf99cf0d4320ba9b on LebedevRI:refactor-RunBenchmark into edc77a3669026eddc380721d5a3cdccd752b76cb on google:master.\n. \n\nCoverage remained the same at 89.022% when pulling 14d686d00805dc7ae7b0cf67d31d0797df47f645 on LebedevRI:appveyor-msvs-googletest into aad33aab3c1a8db06053665af39e4679bc774b74 on google:master.\n. \n\nCoverage remained the same at 89.047% when pulling 4805a86f59ee7243deee107c483cb58d0e5d2d60 on jbeich:gtest into edc77a3669026eddc380721d5a3cdccd752b76cb on google:master.\n. \n\nCoverage increased (+0.006%) to 89.053% when pulling 73ca0c63624063feaa37d8c9a83fe3d8a9f77400 on pwnall:msvc-warn into edc77a3669026eddc380721d5a3cdccd752b76cb on google:master.\n. \n\nCoverage remained the same at 89.256% when pulling 6b764fec259aafa99be114f22288c08d9182aba7 on Lord-Kamina:master into a8082de5dfdd2132e8bda432314efbce3f280f99 on google:master.\n. \n\nCoverage decreased (-0.03%) to 89.224% when pulling f5009d7e660d6053a0b2353c167ed79920f97af6 on iillyyaa:fix-559-benchmark-color-auto into 9ffb8df6c5d62a677ee27d19cee93a6f3197fd19 on google:master.\n. \n\nCoverage remained the same at 89.224% when pulling 93da8010eecf57108f387d4f9894c67b08c4882d on peterjc123:clang_fix into 8503dfe537a1fc60a88116ee024f57e9fb5131a1 on google:master.\n. \n\nCoverage increased (+0.06%) to 89.347% when pulling 9ba6c8e548b34297d84df71d06eba9743c25bc3e on efcs:print-more-precision into b1717916d1acafbd0753c145f30f76b06f2933e5 on google:master.\n. \n\nCoverage remained the same at 89.224% when pulling a36617fcfa7a2bd48e3f08f1b75abc8e7c4398a1 on timshen91:dev into 8503dfe537a1fc60a88116ee024f57e9fb5131a1 on google:master.\n. \n\nCoverage remained the same at 89.224% when pulling 62809ea058f01320d2691dde77c3dea3b34333a1 on olzhabay:master into 8503dfe537a1fc60a88116ee024f57e9fb5131a1 on google:master.\n. \n\nCoverage remained the same at 89.224% when pulling 769491bd6a3a40f2b76ed4b7f26cd1e293585605 on LebedevRI:aggregates-iteration-count into d731697a5da9cec25034683ed9807304595ea72c on google:master.\n. \n\nCoverage remained the same at 89.224% when pulling a71a0f0b23b9b61cc0a6a318369c1a54d526d473 on LebedevRI:nfc-has-aggregates into d731697a5da9cec25034683ed9807304595ea72c on google:master.\n. \n\nCoverage increased (+0.06%) to 89.347% when pulling 5de56c13ec139dfef5cb763bb6e5b6081de13727 on LebedevRI:BenchmarkRunner-refactor into b1717916d1acafbd0753c145f30f76b06f2933e5 on google:master.\n. \n\nCoverage decreased (-1.08%) to 88.139% when pulling a9cbfc5de6e5cf812f1205aa2bd7e70af131c116 on LebedevRI:separate-iterations into c6193afe7eb1eb7802e34833e55e1528cb65c533 on google:master.\n. \n\nCoverage remained the same at 89.224% when pulling e095f4476813c307c57e478c86c04701d50a58e8 on gladk:fix/s390_cpuinfo_parser into 507c06e636e0d21ca900024df464e965d3e1b669 on google:master.\n. \n\nCoverage remained the same at 89.224% when pulling 8ec936f0036090796704a3da861e25e031fd648e on kbobyrev:msvc-exceptions-fix into c6193afe7eb1eb7802e34833e55e1528cb65c533 on google:master.\n. \n\nCoverage remained the same at 89.224% when pulling 5cdbab7258ebbfff6029b1482445d68208532e63 on amallia:patch-1 into a9b31c51b1ee7ec7b31438c647123c2cbac5d956 on google:master.\n. \n\nCoverage remained the same at 89.288% when pulling b8582e8a2e8264fdce60913a2b7b92cc13f652d9 on cmake-3_5_1 into b1717916d1acafbd0753c145f30f76b06f2933e5 on master.\n. \n\nCoverage remained the same at 89.288% when pulling 702c97bea2994049aa243e386474b33d62a31353 on traceon:fix-cpp17-compilation into b1717916d1acafbd0753c145f30f76b06f2933e5 on google:master.\n. \n\nCoverage remained the same at 89.288% when pulling 5d4ad0a277e1761463a227cf50d7748fcf5f1ef4 on ulvgard:refactor_complexity into 56f5cd6a729280ba7639574bd49a2bf4be7f1c4b on google:master.\n. \n\nCoverage increased (+0.006%) to 89.294% when pulling 1ad81cef6e792ba1d89bd70c120cf570e6258493 on LebedevRI:print-range into c9311a44e1280853632fe2472345dd04514a2f74 on google:master.\n. \n\nCoverage remained the same at 89.41% when pulling bf883977a60ca1b2622fc6960f60a8c8b7cd6771 on Croydon:croydon/conan_minimal into 4528c76b718acc9b57956f63069c699ae21edcab on google:master.\n. \n\nCoverage increased (+0.06%) to 89.358% when pulling f63f02ef67628e8f04ef8bec9c8b5bd77b4d59e9 on jatinx:SystemName into 19f7d5c2bcc98d34f4dadad04cc3c979589fc3ae on google:master.\n. \n\nCoverage increased (+0.06%) to 89.352% when pulling a651c6f6440395d77dcf759ef260fc5ec84be822 on jatinx:HostName into eee8b05c97d7b832bf67d6e000958d012ab30165 on google:master.\n. \n\nCoverage remained the same at 89.294% when pulling 2bd23f85f183a935752c07b876b688100dce7a1c on efcs:replace-tempnam into 19f7d5c2bcc98d34f4dadad04cc3c979589fc3ae on google:master.\n. \n\nCoverage remained the same at 89.294% when pulling 58f4d53c8708ecc3b76c2617880095a2d189eb6d on Sigill:expect_eq_ul into eee8b05c97d7b832bf67d6e000958d012ab30165 on google:master.\n. \n\nCoverage remained the same at 89.352% when pulling 507e4b15dd1e0fafe794eecddf2425366254e1b3 on dominichamon-patch-1 into 47a5f77d754892f20b8c717ead26de836e0bb552 on master.\n. \n\nCoverage increased (+0.1%) to 89.508% when pulling e792aaf623d0450f13d453f657032b03dc2ae0e1 on BaaMeow:feature/threads_repetitions_output into 4528c76b718acc9b57956f63069c699ae21edcab on google:master.\n. \n\nCoverage increased (+0.09%) to 89.493% when pulling c942bb5483c8d60c99dfc96a2d7956d2c6651cee on BaaMeow:feature/custom_manual_time into 785e2c3158589e8ef48c59ba80e48d76bdbd8902 on google:master.\n. \n\nCoverage remained the same at 89.41% when pulling 650a36f6abc186f5cf24f0ae559719569d74dc3d on rnk:cmp048 into 4528c76b718acc9b57956f63069c699ae21edcab on google:master.\n. \n\nCoverage decreased (-0.006%) to 89.405% when pulling 98b8bb2484638a0dc5ea92771f370ac1070f51f0 on berestovskyy:master into eec9a8e4976a397988c15e5a4b71a542375a2240 on google:master.\n. \n\nCoverage remained the same at 89.405% when pulling cacb02c018fa5e16c823c4cdba471874789b8a41 on internal_statistics into 4b9f43e2c4ea559c59f5d86561c02207008a15ac on master.\n. \n\nCoverage remained the same at 89.405% when pulling fbcf1f056643b3849812f5b0b3608e89f70de13d on rainergericke:master into 785e2c3158589e8ef48c59ba80e48d76bdbd8902 on google:master.\n. \n\nCoverage remained the same at 87.578% when pulling 8d4209d624f39dfbf479d43d10098d2b8a3b6c46 on orgads:fix-cross-mingw into 40fce80e0d4aac21e05333668d6ed6e498a6c2a3 on google:v2.\n. \n\nCoverage increased (+0.006%) to 89.41% when pulling d22a20a8ce1850402eff9a36de9ab0d5169c8322 on dharvey1986:capture_complexity_naming_#730 into 97393e5ef820fda0330830d56c774949b4cfbb29 on google:master.\n. \n\nCoverage increased (+0.1%) to 89.542% when pulling 08892c7ece96aad0a78fb3ea197708c2278228ef on dharvey1986:capture_complexity_naming_#730_v2 into b8ca0c42179b7b5d656494e61dda8b861057122f on google:master.\n. \n\nCoverage increased (+0.2%) to 89.631% when pulling f3977673bdfc81e74c65abe0bddd16ce4a86136c on bryan-lunt-supercomputing:openmp-compatibility into b8ca0c42179b7b5d656494e61dda8b861057122f on google:master.\n. \n\nCoverage remained the same at 89.405% when pulling 20d3993cadcf869860fb49fae29feaa7aff2cb40 on jilinzhou:master into b8ca0c42179b7b5d656494e61dda8b861057122f on google:master.\n. \n\nCoverage remained the same at 89.405% when pulling f3efbf05ecb70d01297a0d3ddca0443e836af3a3 on jilinzhou:master into b8ca0c42179b7b5d656494e61dda8b861057122f on google:master.\n. \n\nCoverage remained the same at 89.405% when pulling e7a81d6f9467a9ae0f06111543ffe8723de21218 on jilinzhou:master into b8ca0c42179b7b5d656494e61dda8b861057122f on google:master.\n. \n\nCoverage remained the same at 89.405% when pulling e60d9ad1bb4b425bb94069afb856384898ae4039 on chenshuo:patch-1 into d205ead299c7cddd5e1bc3478d57ad4320a4a53c on google:master.\n. \n\nCoverage remained the same at 89.405% when pulling c2bcbb9eabb2a06974a11f6abe06eec31c83b438 on chenshuo:chenshuo-patch-2 into d205ead299c7cddd5e1bc3478d57ad4320a4a53c on google:master.\n. \n\nCoverage remained the same at 89.542% when pulling 7032b32aff7f553e0d6fc2a3e449aee3e740d00e on LebedevRI:llvm into f6e96861a373c90ea0c727177fc68d2984e048bb on google:master.\n. \n\nCoverage decreased (-0.09%) to 89.457% when pulling 502fbcf74f699c2cdbb457aa11df9fc783b14c92 on tesch1:fix-784 into 5acb0f05ed4938ac3140918048cfe1da2b5c6951 on google:master.\n. ",
    "nickhutchinson": "Sorry for the delay, I\u2019ll try fix this up this week.\n\nOn 29 Aug 2016, at 06:21, Eric notifications@github.com wrote:\n@nickhutchinson https://github.com/nickhutchinson Are you interested in continuing with this?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub https://github.com/google/benchmark/pull/126#issuecomment-243034408, or mute the thread https://github.com/notifications/unsubscribe-auth/AAuH7KjeTf9hqmHHi02JX-Sc_iWk-dL4ks5qkmx2gaJpZM4EzKUw.\n. Finally got around to rebasing this on master -- thanks for your patience. I believe I've addressed the review comments, in that the new patch:\n- uses std::string instead of const char*\n- removes error-checking of CLI arguments -- as pointed out, this isn't relevant to colorised output.\n\nI've tested on Win10/MSVC2015 and OS X 10.11/Clang 3.8.\n. Addressed code review comments.\n- rebase on master\n- make IsTruthyFlagValue() return true if given the empty string.\n- remove posix.h header, folded the functions into IsColorTerminal() (which no longer accepts any arguments)\n- remove GetEnv() shim, update callers to use stdlib's getenv() directly.\n- add FIXME to --color_print comment.\n. Addressed review comments\n. My model here was Google Test's internal/gtest-port.h. I'll have a ponder about an alternate namespace name. Would you be open to ditching the nested namespace altogether?\n. I think the three states are necessary: it's sometimes useful to pipe to some command that does support colorized output. In this case, the isatty() test would fail, so you do need a way to forcibly enable colors.\n. How lenient should the check be? ParseBoolFlag() is quite generous with what it accepts, as is Google Test's --gtest_color option. \nI've changed it to assert that the user passes a non-empty string. Is that enough?\n. Fixed\n. Not quite -- without this check, you could pass --benchmark_format or similar with no associated value, and no error will be reported.\n. Is there a reason this one should take a std::string when the others in the header take a c string?\n. Could you elaborate?\n. ",
    "AppVeyorBot": ":x: Build benchmark 445 failed (commit https://github.com/google/benchmark/commit/a7ed56c34b by @nickhutchinson)\n. :white_check_mark: Build benchmark 452 completed (commit https://github.com/google/benchmark/commit/b6cc5dede9 by @nickhutchinson)\n. :x: Build benchmark 208 failed (commit https://github.com/google/benchmark/commit/75db3a6350 by @ismaelJimenez)\n. :x: Build benchmark 209 failed (commit https://github.com/google/benchmark/commit/673fe8218e by @ismaelJimenez)\n. :x: Build benchmark 210 failed (commit https://github.com/google/benchmark/commit/e9e5423b7f by @ismaelJimenez)\n. :x: Build benchmark 214 failed (commit https://github.com/google/benchmark/commit/4ed4dccf31 by @dominichamon)\n. :x: Build benchmark 215 failed (commit https://github.com/google/benchmark/commit/d71bb22ca8 by @ismaelJimenez)\n. :x: Build benchmark 217 failed (commit https://github.com/google/benchmark/commit/363e8cab2e by @dominichamon)\n. :x: Build benchmark 220 failed (commit https://github.com/google/benchmark/commit/09d69fa5c7 by @dominichamon)\n. :white_check_mark: Build benchmark 234 completed (commit https://github.com/google/benchmark/commit/c8e1bad79c by @dominichamon)\n. :x: Build benchmark 223 failed (commit https://github.com/google/benchmark/commit/74ee35b6cb by @NAThompson)\n. :x: Build benchmark 225 failed (commit https://github.com/google/benchmark/commit/7133ad4009 by @)\n. :x: Build benchmark 226 failed (commit https://github.com/google/benchmark/commit/777e7a6443 by @)\n. :x: Build benchmark 227 failed (commit https://github.com/google/benchmark/commit/be0804814f by @NAThompson)\n. :x: Build benchmark 231 failed (commit https://github.com/google/benchmark/commit/7b1a8a7cbe by @steve-downey)\n. :x: Build benchmark 236 failed (commit https://github.com/google/benchmark/commit/cd7c5b57a1 by @enh)\n. :x: Build benchmark 238 failed (commit https://github.com/google/benchmark/commit/bb4830f044 by @SvenJo)\n. :x: Build benchmark 240 failed (commit https://github.com/google/benchmark/commit/e6405d6e90 by @loganek)\n. :x: Build benchmark 246 failed (commit https://github.com/google/benchmark/commit/df567de3a7 by @loganek)\n. :x: Build benchmark 247 failed (commit https://github.com/google/benchmark/commit/aa66305520 by @loganek)\n. :x: Build benchmark 256 failed (commit https://github.com/google/benchmark/commit/050f35eb51 by @loganek)\n. :x: Build benchmark 270 failed (commit https://github.com/google/benchmark/commit/2e7d6bd811 by @loganek)\n. :x: Build benchmark 293 failed (commit https://github.com/google/benchmark/commit/92cae43633 by @loganek)\n. :x: Build benchmark 248 failed (commit https://github.com/google/benchmark/commit/2ac4c04622 by @EricWF)\n. :x: Build benchmark 249 failed (commit https://github.com/google/benchmark/commit/c6f1458d98 by @EricWF)\n. :x: Build benchmark 250 failed (commit https://github.com/google/benchmark/commit/ce747460a7 by @EricWF)\n. :x: Build benchmark 251 failed (commit https://github.com/google/benchmark/commit/51b6c9531c by @EricWF)\n. :x: Build benchmark 252 failed (commit https://github.com/google/benchmark/commit/74316d3662 by @EricWF)\n. :x: Build benchmark 278 failed (commit https://github.com/google/benchmark/commit/19b9da178d by @EricWF)\n. :x: Build benchmark 253 failed (commit https://github.com/google/benchmark/commit/ea06dfc4ed by @EricWF)\n. :x: Build benchmark 254 failed (commit https://github.com/google/benchmark/commit/7d59322f70 by @EricWF)\n. :x: Build benchmark 255 failed (commit https://github.com/google/benchmark/commit/cb62828c2b by @EricWF)\n. :x: Build benchmark 268 failed (commit https://github.com/google/benchmark/commit/a2aa7afac3 by @EricWF)\n. :x: Build benchmark 269 failed (commit https://github.com/google/benchmark/commit/ab6ca49203 by @EricWF)\n. :x: Build benchmark 280 failed (commit https://github.com/google/benchmark/commit/1d32619659 by @EricWF)\n. :x: Build benchmark 257 failed (commit https://github.com/google/benchmark/commit/5b2c23df65 by @)\n. :x: Build benchmark 258 failed (commit https://github.com/google/benchmark/commit/5880f77e44 by @drozdvadym)\n. :x: Build benchmark 261 failed (commit https://github.com/google/benchmark/commit/8ea563a060 by @EricWF)\n. :x: Build benchmark 263 failed (commit https://github.com/google/benchmark/commit/67e1303746 by @EricWF)\n. :x: Build benchmark 265 failed (commit https://github.com/google/benchmark/commit/3b6dc38ed5 by @EricWF)\n. :x: Build benchmark 266 failed (commit https://github.com/google/benchmark/commit/dd8415c59e by @jppm)\n. :x: Build benchmark 267 failed (commit https://github.com/google/benchmark/commit/f1c62a2248 by @jppm)\n. :x: Build benchmark 273 failed (commit https://github.com/google/benchmark/commit/5dc778aea7 by @jppm)\n. :x: Build benchmark 274 failed (commit https://github.com/google/benchmark/commit/e665b4a130 by @jppm)\n. :x: Build benchmark 275 failed (commit https://github.com/google/benchmark/commit/62d2c358c6 by @jppm)\n. :x: Build benchmark 291 failed (commit https://github.com/google/benchmark/commit/c5b32e5591 by @jppm)\n. :x: Build benchmark 292 failed (commit https://github.com/google/benchmark/commit/c73ba69d67 by @jppm)\n. :x: Build benchmark 295 failed (commit https://github.com/google/benchmark/commit/cd8c184416 by @jppm)\n. :x: Build benchmark 296 failed (commit https://github.com/google/benchmark/commit/26ce1305de by @jppm)\n. :x: Build benchmark 297 failed (commit https://github.com/google/benchmark/commit/3dd0d4eb72 by @jppm)\n. :x: Build benchmark 298 failed (commit https://github.com/google/benchmark/commit/4bc3867ec9 by @jppm)\n. :x: Build benchmark 353 failed (commit https://github.com/google/benchmark/commit/4ceef8c1ba by @jppm)\n. :x: Build benchmark 354 failed (commit https://github.com/google/benchmark/commit/948d6054a3 by @jppm)\n. :x: Build benchmark 355 failed (commit https://github.com/google/benchmark/commit/48cf50ebed by @jppm)\n. :x: Build benchmark 378 failed (commit https://github.com/google/benchmark/commit/ca927540e5 by @jppm)\n. :x: Build benchmark 379 failed (commit https://github.com/google/benchmark/commit/38245ec8b2 by @jppm)\n. :white_check_mark: Build benchmark 380 completed (commit https://github.com/google/benchmark/commit/27141ce128 by @jppm)\n. :white_check_mark: Build benchmark 404 completed (commit https://github.com/google/benchmark/commit/41f16de2a3 by @jppm)\n. :x: Build benchmark 408 failed (commit https://github.com/google/benchmark/commit/fbc9f8eda8 by @jppm)\n. :white_check_mark: Build benchmark 410 completed (commit https://github.com/google/benchmark/commit/2f7b33a05a by @jppm)\n. :white_check_mark: Build benchmark 456 completed (commit https://github.com/google/benchmark/commit/efde799dd9 by @jppm)\n. :white_check_mark: Build benchmark 457 completed (commit https://github.com/google/benchmark/commit/b869b585e4 by @jppm)\n. :white_check_mark: Build benchmark 458 completed (commit https://github.com/google/benchmark/commit/c45539ffc4 by @jppm)\n. :white_check_mark: Build benchmark 459 completed (commit https://github.com/google/benchmark/commit/a3f93e589a by @jppm)\n. :white_check_mark: Build benchmark 498 completed (commit https://github.com/google/benchmark/commit/6e4aa66fec by @jppm)\n. :x: Build benchmark 533 failed (commit https://github.com/google/benchmark/commit/12695784e1 by @jppm). :x: Build benchmark 277 failed (commit https://github.com/google/benchmark/commit/3327244193 by @EricWF)\n. :x: Build benchmark 284 failed (commit https://github.com/google/benchmark/commit/8b50d4c231 by @EricWF)\n. :x: Build benchmark 285 failed (commit https://github.com/google/benchmark/commit/7c7305fc98 by @EricWF)\n. :x: Build benchmark 287 failed (commit https://github.com/google/benchmark/commit/015127d01e by @EricWF)\n. :x: Build benchmark 288 failed (commit https://github.com/google/benchmark/commit/52c018b4b7 by @EricWF)\n. :x: Build benchmark 290 failed (commit https://github.com/google/benchmark/commit/bb239546d8 by @EricWF)\n. :x: Build benchmark 299 failed (commit https://github.com/google/benchmark/commit/b615b4f1a2 by @EricWF)\n. :x: Build benchmark 300 failed (commit https://github.com/google/benchmark/commit/da7f47d012 by @EricWF)\n. :x: Build benchmark 357 failed (commit https://github.com/google/benchmark/commit/75f589f07a by @EricWF)\n. :x: Build benchmark 358 failed (commit https://github.com/google/benchmark/commit/1f73e23c40 by @EricWF)\n. :x: Build benchmark 301 failed (commit https://github.com/google/benchmark/commit/c158009d1b by @EricWF)\n. :x: Build benchmark 307 failed (commit https://github.com/google/benchmark/commit/219290b80a by @EricWF)\n. :x: Build benchmark 308 failed (commit https://github.com/google/benchmark/commit/9b056986a5 by @EricWF)\n. :x: Build benchmark 309 failed (commit https://github.com/google/benchmark/commit/47268fbcba by @EricWF)\n. :x: Build benchmark 310 failed (commit https://github.com/google/benchmark/commit/58b302dfdc by @EricWF)\n. :x: Build benchmark 311 failed (commit https://github.com/google/benchmark/commit/1baf517a18 by @EricWF)\n. :x: Build benchmark 322 failed (commit https://github.com/google/benchmark/commit/d1e5a0ff1f by @EricWF)\n. :x: Build benchmark 323 failed (commit https://github.com/google/benchmark/commit/248f12912e by @EricWF)\n. :x: Build benchmark 324 failed (commit https://github.com/google/benchmark/commit/192d608473 by @EricWF)\n. :x: Build benchmark 325 failed (commit https://github.com/google/benchmark/commit/f867f7ebb8 by @EricWF)\n. :x: Build benchmark 326 failed (commit https://github.com/google/benchmark/commit/ca9ca7f052 by @EricWF)\n. :x: Build benchmark 327 failed (commit https://github.com/google/benchmark/commit/031e35797b by @EricWF)\n. :x: Build benchmark 328 failed (commit https://github.com/google/benchmark/commit/f48dc73383 by @EricWF)\n. :x: Build benchmark 330 failed (commit https://github.com/google/benchmark/commit/a372157cb5 by @EricWF)\n. :x: Build benchmark 331 failed (commit https://github.com/google/benchmark/commit/eae1b908a9 by @EricWF)\n. :x: Build benchmark 332 failed (commit https://github.com/google/benchmark/commit/7f574fb02b by @EricWF)\n. :x: Build benchmark 366 failed (commit https://github.com/google/benchmark/commit/24e083db08 by @AlexanderSidorenko)\n. :x: Build benchmark 367 failed (commit https://github.com/google/benchmark/commit/8c17eee038 by @AlexanderSidorenko)\n. :x: Build benchmark 368 failed (commit https://github.com/google/benchmark/commit/abab9ed038 by @AlexanderSidorenko)\n. :x: Build benchmark 369 failed (commit https://github.com/google/benchmark/commit/d76dadde74 by @EricWF)\n. :x: Build benchmark 374 failed (commit https://github.com/google/benchmark/commit/83342ddc26 by @EricWF)\n. :white_check_mark: Build benchmark 376 completed (commit https://github.com/google/benchmark/commit/a95703ca99 by @EricWF)\n. :x: Build benchmark 370 failed (commit https://github.com/google/benchmark/commit/f96d0d647b by @EricWF)\n. :x: Build benchmark 371 failed (commit https://github.com/google/benchmark/commit/eef9b59d3f by @EricWF)\n. :x: Build benchmark 372 failed (commit https://github.com/google/benchmark/commit/cb64c59b21 by @EricWF)\n. :x: Build benchmark 373 failed (commit https://github.com/google/benchmark/commit/1975afc95e by @EricWF)\n. :white_check_mark: Build benchmark 375 completed (commit https://github.com/google/benchmark/commit/47efba584b by @EricWF)\n. :white_check_mark: Build benchmark 377 completed (commit https://github.com/google/benchmark/commit/ea906a9f41 by @EricWF)\n. :x: Build benchmark 383 failed (commit https://github.com/google/benchmark/commit/c562760de4 by @EricWF)\n. :x: Build benchmark 390 failed (commit https://github.com/google/benchmark/commit/dd76817db3 by @EricWF)\n. :x: Build benchmark 394 failed (commit https://github.com/google/benchmark/commit/47edc4be5c by @EricWF)\n. :x: Build benchmark 384 failed (commit https://github.com/google/benchmark/commit/f82db3098f by @EricWF)\n. :x: Build benchmark 391 failed (commit https://github.com/google/benchmark/commit/ddbe36327c by @EricWF)\n. :white_check_mark: Build benchmark 393 completed (commit https://github.com/google/benchmark/commit/ef8e42ad30 by @EricWF)\n. :white_check_mark: Build benchmark 395 completed (commit https://github.com/google/benchmark/commit/4567373dfc by @EricWF)\n. :white_check_mark: Build benchmark 387 completed (commit https://github.com/google/benchmark/commit/82f95965a8 by @EricWF)\n. :white_check_mark: Build benchmark 388 completed (commit https://github.com/google/benchmark/commit/56d86e95a5 by @EricWF)\n. :white_check_mark: Build benchmark 396 completed (commit https://github.com/google/benchmark/commit/f8c5a34631 by @EricWF)\n. :white_check_mark: Build benchmark 401 completed (commit https://github.com/google/benchmark/commit/b457d0c0eb by @hydroo)\n. :x: Build benchmark 442 failed (commit https://github.com/google/benchmark/commit/b9ba8c6b88 by @hydroo)\n. :white_check_mark: Build benchmark 402 completed (commit https://github.com/google/benchmark/commit/c56ba0f67d by @jppm)\n. :white_check_mark: Build benchmark 403 completed (commit https://github.com/google/benchmark/commit/593e40b416 by @jppm)\n. :white_check_mark: Build benchmark 405 completed (commit https://github.com/google/benchmark/commit/99e137a332 by @jppm)\n. :white_check_mark: Build benchmark 407 completed (commit https://github.com/google/benchmark/commit/44c8fdda78 by @jppm)\n. :x: Build benchmark 411 failed (commit https://github.com/google/benchmark/commit/3668875662 by @EricWF)\n. :x: Build benchmark 412 failed (commit https://github.com/google/benchmark/commit/13b4a6c641 by @EricWF)\n. :x: Build benchmark 413 failed (commit https://github.com/google/benchmark/commit/b6f58fa5a2 by @EricWF)\n. :x: Build benchmark 414 failed (commit https://github.com/google/benchmark/commit/a6ab48a224 by @EricWF)\n. :x: Build benchmark 415 failed (commit https://github.com/google/benchmark/commit/a0eac36099 by @EricWF)\n. :x: Build benchmark 417 failed (commit https://github.com/google/benchmark/commit/6e0eb519be by @EricWF)\n. :x: Build benchmark 418 failed (commit https://github.com/google/benchmark/commit/92aefcfe62 by @EricWF)\n. :x: Build benchmark 419 failed (commit https://github.com/google/benchmark/commit/c3a8866d99 by @EricWF)\n. :x: Build benchmark 420 failed (commit https://github.com/google/benchmark/commit/7d3644d3cf by @EricWF)\n. :x: Build benchmark 421 failed (commit https://github.com/google/benchmark/commit/5867586ae3 by @EricWF)\n. :x: Build benchmark 422 failed (commit https://github.com/google/benchmark/commit/69e69bd07e by @EricWF)\n. :x: Build benchmark 423 failed (commit https://github.com/google/benchmark/commit/ebc1b81f5c by @EricWF)\n. :x: Build benchmark 424 failed (commit https://github.com/google/benchmark/commit/809a62661c by @EricWF)\n. :x: Build benchmark 425 failed (commit https://github.com/google/benchmark/commit/3b22deabfb by @EricWF)\n. :x: Build benchmark 427 failed (commit https://github.com/google/benchmark/commit/538b646417 by @EricWF)\n. :x: Build benchmark 428 failed (commit https://github.com/google/benchmark/commit/eec3951a56 by @EricWF)\n. :x: Build benchmark 429 failed (commit https://github.com/google/benchmark/commit/5bff2b5262 by @EricWF)\n. :x: Build benchmark 434 failed (commit https://github.com/google/benchmark/commit/3c35e9f281 by @EricWF)\n. :x: Build benchmark 436 failed (commit https://github.com/google/benchmark/commit/12b9f69bf3 by @EricWF)\n. :white_check_mark: Build benchmark 438 completed (commit https://github.com/google/benchmark/commit/93b7edc0c4 by @EricWF)\n. :white_check_mark: Build benchmark 444 completed (commit https://github.com/google/benchmark/commit/b4c98ebff8 by @EricWF)\n. :white_check_mark: Build benchmark 447 completed (commit https://github.com/google/benchmark/commit/f18c68a165 by @EricWF)\n. :white_check_mark: Build benchmark 446 completed (commit https://github.com/google/benchmark/commit/b9ccba9656 by @nickhutchinson)\n. :white_check_mark: Build benchmark 453 completed (commit https://github.com/google/benchmark/commit/0058ae7be2 by @nickhutchinson)\n. :white_check_mark: Build benchmark 450 completed (commit https://github.com/google/benchmark/commit/55451dda8a by @EricWF)\n. :white_check_mark: Build benchmark 460 completed (commit https://github.com/google/benchmark/commit/f2f597827f by @jppm)\n. :white_check_mark: Build benchmark 461 completed (commit https://github.com/google/benchmark/commit/f2f597827f by @jppm)\n. :white_check_mark: Build benchmark 462 completed (commit https://github.com/google/benchmark/commit/b385ee6ce2 by @jppm)\n. :white_check_mark: Build benchmark 463 completed (commit https://github.com/google/benchmark/commit/5fbe78deee by @jppm)\n. :white_check_mark: Build benchmark 465 completed (commit https://github.com/google/benchmark/commit/ba013275a6 by @PSIAlt)\n. :white_check_mark: Build benchmark 469 completed (commit https://github.com/google/benchmark/commit/00ed1144af by @ktnyt)\n. :white_check_mark: Build benchmark 484 completed (commit https://github.com/google/benchmark/commit/1a829bbd61 by @mkurdej)\n. :white_check_mark: Build benchmark 486 completed (commit https://github.com/google/benchmark/commit/04692ab0a4 by @mkurdej)\n. :white_check_mark: Build benchmark 487 completed (commit https://github.com/google/benchmark/commit/5cef4c6beb by @mkurdej)\n. :white_check_mark: Build benchmark 488 completed (commit https://github.com/google/benchmark/commit/c0d40d1987 by @mkurdej)\n. :white_check_mark: Build benchmark 489 completed (commit https://github.com/google/benchmark/commit/cc8d4c6804 by @mkurdej)\n. :x: Build benchmark 491 failed (commit https://github.com/google/benchmark/commit/30aec0605c by @mkurdej)\n. :white_check_mark: Build benchmark 492 completed (commit https://github.com/google/benchmark/commit/d7b1f60e74 by @mkurdej)\n. :white_check_mark: Build benchmark 493 completed (commit https://github.com/google/benchmark/commit/ee73d36897 by @mkurdej)\n. :white_check_mark: Build benchmark 496 completed (commit https://github.com/google/benchmark/commit/aa3922395a by @dominichamon)\n. :white_check_mark: Build benchmark 502 completed (commit https://github.com/google/benchmark/commit/516747af0a by @EricWF). :white_check_mark: Build benchmark 503 completed (commit https://github.com/google/benchmark/commit/5177824d43 by @EricWF). :white_check_mark: Build benchmark 508 completed (commit https://github.com/google/benchmark/commit/c9401fc053 by @NiklasRosenstein). :white_check_mark: Build benchmark 510 completed (commit https://github.com/google/benchmark/commit/15ea80a64b by @pcampr). :white_check_mark: Build benchmark 513 completed (commit https://github.com/google/benchmark/commit/1ef7ce0e60 by @BRevzin). :white_check_mark: Build benchmark 516 completed (commit https://github.com/google/benchmark/commit/db09e7a577 by @dominichamon). :white_check_mark: Build benchmark 517 completed (commit https://github.com/google/benchmark/commit/95e91d9a96 by @PSIAlt). :white_check_mark: Build benchmark 520 completed (commit https://github.com/google/benchmark/commit/c7b39925d6 by @NiklasRosenstein). :white_check_mark: Build benchmark 521 completed (commit https://github.com/google/benchmark/commit/003c8a4aaa by @NiklasRosenstein). :white_check_mark: Build benchmark 525 completed (commit https://github.com/google/benchmark/commit/a5fd862651 by @sieren). :white_check_mark: Build benchmark 527 completed (commit https://github.com/google/benchmark/commit/71fe7d282a by @grypp). :white_check_mark: Build benchmark 529 completed (commit https://github.com/google/benchmark/commit/b8e702262c by @Maratyszcza). :x: Build benchmark 530 failed (commit https://github.com/google/benchmark/commit/8e17cf22d9 by @Maratyszcza). :white_check_mark: Build benchmark 532 completed (commit https://github.com/google/benchmark/commit/0d9424605b by @GregBowyer). :x: Build benchmark 539 failed (commit https://github.com/google/benchmark/commit/cc719ed895 by @biojppm). :x: Build benchmark 540 failed (commit https://github.com/google/benchmark/commit/e0e3889b33 by @biojppm). :x: Build benchmark 541 failed (commit https://github.com/google/benchmark/commit/4ec49b8a30 by @biojppm). :x: Build benchmark 542 failed (commit https://github.com/google/benchmark/commit/10bc6154fe by @biojppm). :white_check_mark: Build benchmark 570 completed (commit https://github.com/google/benchmark/commit/3ce9f21d60 by @biojppm). :x: Build benchmark 616 failed (commit https://github.com/google/benchmark/commit/d5091285f8 by @biojppm). :x: Build benchmark 617 failed (commit https://github.com/google/benchmark/commit/b0458113f2 by @biojppm). :x: Build benchmark 618 failed (commit https://github.com/google/benchmark/commit/09866a07f8 by @biojppm). :white_check_mark: Build benchmark 619 completed (commit https://github.com/google/benchmark/commit/02184fedf1 by @biojppm). :white_check_mark: Build benchmark 620 completed (commit https://github.com/google/benchmark/commit/03d2027393 by @biojppm). :white_check_mark: Build benchmark 544 completed (commit https://github.com/google/benchmark/commit/330d21ef9a by @EricWF). :white_check_mark: Build benchmark 546 completed (commit https://github.com/google/benchmark/commit/deb21733e2 by @). :white_check_mark: Build benchmark 548 completed (commit https://github.com/google/benchmark/commit/f747bd6b4a by @MVafin). :white_check_mark: Build benchmark 549 completed (commit https://github.com/google/benchmark/commit/80cbbf1cfd by @MVafin). :white_check_mark: Build benchmark 564 completed (commit https://github.com/google/benchmark/commit/8b41132917 by @MVafin). :white_check_mark: Build benchmark 550 completed (commit https://github.com/google/benchmark/commit/1b021cde4b by @rolandschulz). :white_check_mark: Build benchmark 551 completed (commit https://github.com/google/benchmark/commit/62cb2f391c by @EricWF). :white_check_mark: Build benchmark 555 completed (commit https://github.com/google/benchmark/commit/e775fa3b9a by @EricWF). :x: Build benchmark 556 failed (commit https://github.com/google/benchmark/commit/4824ce5325 by @EricWF). :white_check_mark: Build benchmark 557 completed (commit https://github.com/google/benchmark/commit/5051fad7bd by @EricWF). :x: Build benchmark 558 failed (commit https://github.com/google/benchmark/commit/c3b2874b5c by @EricWF). :white_check_mark: Build benchmark 559 completed (commit https://github.com/google/benchmark/commit/a2530b0204 by @EricWF). :white_check_mark: Build benchmark 560 completed (commit https://github.com/google/benchmark/commit/030ebbf541 by @EricWF). :white_check_mark: Build benchmark 561 completed (commit https://github.com/google/benchmark/commit/98943d3877 by @EricWF). :white_check_mark: Build benchmark 562 completed (commit https://github.com/google/benchmark/commit/5166e22c24 by @EricWF). :x: Build benchmark 563 failed (commit https://github.com/google/benchmark/commit/192123df5f by @EricWF). :white_check_mark: Build benchmark 567 completed (commit https://github.com/google/benchmark/commit/95ac63d0f9 by @rayglover-ibm). :white_check_mark: Build benchmark 569 completed (commit https://github.com/google/benchmark/commit/5662dceb2f by @biojppm). :white_check_mark: Build benchmark 571 completed (commit https://github.com/google/benchmark/commit/c4902c40e0 by @piribes). :white_check_mark: Build benchmark 572 completed (commit https://github.com/google/benchmark/commit/808cbf457d by @piribes). :white_check_mark: Build benchmark 575 completed (commit https://github.com/google/benchmark/commit/83a65149ca by @kunitoki). :white_check_mark: Build benchmark 576 completed (commit https://github.com/google/benchmark/commit/e720f88c01 by @piribes). :white_check_mark: Build benchmark 578 completed (commit https://github.com/google/benchmark/commit/0672e510fa by @vargad). :white_check_mark: Build benchmark 580 completed (commit https://github.com/google/benchmark/commit/2fdbac0749 by @Slonegg). :white_check_mark: Build benchmark 581 completed (commit https://github.com/google/benchmark/commit/e744126c2a by @Slonegg). :white_check_mark: Build benchmark 582 completed (commit https://github.com/google/benchmark/commit/3b89219a13 by @Slonegg). :white_check_mark: Build benchmark 583 completed (commit https://github.com/google/benchmark/commit/b7f741b759 by @Slonegg). :white_check_mark: Build benchmark 584 completed (commit https://github.com/google/benchmark/commit/3f12ffb2d7 by @Slonegg). :white_check_mark: Build benchmark 590 completed (commit https://github.com/google/benchmark/commit/ecb3263776 by @Slonegg). :x: Build benchmark 586 failed (commit https://github.com/google/benchmark/commit/cdd135e5e2 by @EricWF). :x: Build benchmark 587 failed (commit https://github.com/google/benchmark/commit/784bdc7ae0 by @EricWF). :white_check_mark: Build benchmark 592 completed (commit https://github.com/google/benchmark/commit/8495daf78e by @Slonegg). :x: Build benchmark 594 failed (commit https://github.com/google/benchmark/commit/7d67d11894 by @KindDragon). :x: Build benchmark 595 failed (commit https://github.com/google/benchmark/commit/6b2e41649e by @KindDragon). :white_check_mark: Build benchmark 596 completed (commit https://github.com/google/benchmark/commit/cdb4d2b740 by @KindDragon). :white_check_mark: Build benchmark 601 completed (commit https://github.com/google/benchmark/commit/24ee86f4fe by @KindDragon). :white_check_mark: Build benchmark 603 completed (commit https://github.com/google/benchmark/commit/05c6300a39 by @). :white_check_mark: Build benchmark 613 completed (commit https://github.com/google/benchmark/commit/4caf7433f7 by @felixduvallet). :x: Build benchmark 604 failed (commit https://github.com/google/benchmark/commit/aa60cc863c by @biojppm). :x: Build benchmark 605 failed (commit https://github.com/google/benchmark/commit/c3f3276580 by @biojppm). :x: Build benchmark 606 failed (commit https://github.com/google/benchmark/commit/a55d0da9e7 by @biojppm). :x: Build benchmark 607 failed (commit https://github.com/google/benchmark/commit/1cc3a7abac by @biojppm). :x: Build benchmark 608 failed (commit https://github.com/google/benchmark/commit/8e22a00bec by @biojppm). :white_check_mark: Build benchmark 609 completed (commit https://github.com/google/benchmark/commit/8075d27fb9 by @biojppm). :white_check_mark: Build benchmark 610 completed (commit https://github.com/google/benchmark/commit/b3780aa1fe by @). :white_check_mark: Build benchmark 611 completed (commit https://github.com/google/benchmark/commit/ae4c4e44c5 by @biojppm). :x: Build benchmark 612 failed (commit https://github.com/google/benchmark/commit/b613c1fefd by @biojppm). :x: Build benchmark 622 failed (commit https://github.com/google/benchmark/commit/b43b518d46 by @yixuan). :x: Build benchmark 623 failed (commit https://github.com/google/benchmark/commit/49bb73274e by @yixuan). :x: Build benchmark 624 failed (commit https://github.com/google/benchmark/commit/a5c38c596d by @yixuan). :white_check_mark: Build benchmark 644 completed (commit https://github.com/google/benchmark/commit/d6c977a02a by @yixuan). :x: Build benchmark 626 failed (commit https://github.com/google/benchmark/commit/ad13a888fe by @KindDragon). :x: Build benchmark 627 failed (commit https://github.com/google/benchmark/commit/26c6b2c219 by @KindDragon). :x: Build benchmark 631 failed (commit https://github.com/google/benchmark/commit/a2efdae924 by @). :white_check_mark: Build benchmark 632 completed (commit https://github.com/google/benchmark/commit/417ebb33bd by @). :white_check_mark: Build benchmark 633 completed (commit https://github.com/google/benchmark/commit/4d56ba0a1d by @schoetbi). :white_check_mark: Build benchmark 636 completed (commit https://github.com/google/benchmark/commit/093b30c49d by @schoetbi). :x: Build benchmark 637 failed (commit https://github.com/google/benchmark/commit/55bddc10bc by @schoetbi). :white_check_mark: Build benchmark 638 completed (commit https://github.com/google/benchmark/commit/4a38b1ed95 by @schoetbi). :white_check_mark: Build benchmark 658 completed (commit https://github.com/google/benchmark/commit/8f5dd2827c by @schoetbi). :white_check_mark: Build benchmark 659 completed (commit https://github.com/google/benchmark/commit/8e201e8544 by @schoetbi). :white_check_mark: Build benchmark 634 completed (commit https://github.com/google/benchmark/commit/76f45a4fcc by @dkruger). :white_check_mark: Build benchmark 639 completed (commit https://github.com/google/benchmark/commit/a148b26a85 by @GeorgeARM). :white_check_mark: Build benchmark 640 completed (commit https://github.com/google/benchmark/commit/b7071dc285 by @gepinita). :x: Build benchmark 641 failed (commit https://github.com/google/benchmark/commit/6adac658c5 by @EricWF). :white_check_mark: Build benchmark 646 completed (commit https://github.com/google/benchmark/commit/0ea4bd5534 by @Timmmm). :x: Build benchmark 649 failed (commit https://github.com/google/benchmark/commit/b8cbdc5872 by @EricWF). :white_check_mark: Build benchmark 651 completed (commit https://github.com/google/benchmark/commit/7827ba11ea by @EricWF). :white_check_mark: Build benchmark 652 completed (commit https://github.com/google/benchmark/commit/033e103034 by @EricWF). :white_check_mark: Build benchmark 654 completed (commit https://github.com/google/benchmark/commit/deee2db522 by @). :x: Build benchmark 655 failed (commit https://github.com/google/benchmark/commit/baf6f95899 by @). :white_check_mark: Build benchmark 656 completed (commit https://github.com/google/benchmark/commit/5d53e3fca0 by @jernkuan). :x: Build benchmark 660 failed (commit https://github.com/google/benchmark/commit/196fea76d9 by @astrelni). :x: Build benchmark 661 failed (commit https://github.com/google/benchmark/commit/55b5b5934e by @astrelni). :x: Build benchmark 665 failed (commit https://github.com/google/benchmark/commit/95f035b1b6 by @astrelni). :x: Build benchmark 666 failed (commit https://github.com/google/benchmark/commit/5e587a34a6 by @astrelni). :x: Build benchmark 662 failed (commit https://github.com/google/benchmark/commit/3b5dec3c78 by @EricWF). :x: Build benchmark 663 failed (commit https://github.com/google/benchmark/commit/7fca2a7edb by @EricWF). :x: Build benchmark 664 failed (commit https://github.com/google/benchmark/commit/4480201bc2 by @EricWF). :x: Build benchmark 671 failed (commit https://github.com/google/benchmark/commit/006fcc0ac0 by @tommadams). :white_check_mark: Build benchmark 677 completed (commit https://github.com/google/benchmark/commit/f729ed0604 by @dominichamon). :x: Build benchmark 680 failed (commit https://github.com/google/benchmark/commit/536880eed5 by @). :white_check_mark: Build benchmark 683 completed (commit https://github.com/google/benchmark/commit/9b7ba1121a by @dominichamon). :x: Build benchmark 685 failed (commit https://github.com/google/benchmark/commit/9fb7924099 by @dominichamon). :x: Build benchmark 687 failed (commit https://github.com/google/benchmark/commit/51515dbd82 by @gnarlie). :white_check_mark: Build benchmark 688 completed (commit https://github.com/google/benchmark/commit/d8f3d2f89c by @gnarlie). :x: Build benchmark 689 failed (commit https://github.com/google/benchmark/commit/2304a81a21 by @LebedevRI). :x: Build benchmark 690 failed (commit https://github.com/google/benchmark/commit/3942dcc306 by @LebedevRI). :white_check_mark: Build benchmark 691 completed (commit https://github.com/google/benchmark/commit/77b2e6a994 by @LebedevRI). :white_check_mark: Build benchmark 693 completed (commit https://github.com/google/benchmark/commit/467e205181 by @LebedevRI). :x: Build benchmark 695 failed (commit https://github.com/google/benchmark/commit/957ee79a08 by @LebedevRI). :white_check_mark: Build benchmark 696 completed (commit https://github.com/google/benchmark/commit/1421976bd1 by @LebedevRI). :x: Build benchmark 697 failed (commit https://github.com/google/benchmark/commit/dbbbea194d by @LebedevRI). :x: Build benchmark 698 failed (commit https://github.com/google/benchmark/commit/9ac885c21c by @LebedevRI). :x: Build benchmark 699 failed (commit https://github.com/google/benchmark/commit/ad32f8305f by @LebedevRI). :x: Build benchmark 700 failed (commit https://github.com/google/benchmark/commit/5ea3df437e by @LebedevRI). :white_check_mark: Build benchmark 702 completed (commit https://github.com/google/benchmark/commit/e28dd51773 by @DerThorsten). :x: Build benchmark 799 failed (commit https://github.com/google/benchmark/commit/525c44a4cc by @DerThorsten). :white_check_mark: Build benchmark 703 completed (commit https://github.com/google/benchmark/commit/f4baf6774e by @LebedevRI). :white_check_mark: Build benchmark 705 completed (commit https://github.com/google/benchmark/commit/f0c6004d12 by @disconnect3d). :x: Build benchmark 706 failed (commit https://github.com/google/benchmark/commit/693680343e by @LebedevRI). :white_check_mark: Build benchmark 707 completed (commit https://github.com/google/benchmark/commit/078befd516 by @pwnall). :white_check_mark: Build benchmark 708 completed (commit https://github.com/google/benchmark/commit/33b3564b0d by @pwnall). :x: Build benchmark 711 failed (commit https://github.com/google/benchmark/commit/f925f0c89c by @dominichamon). :x: Build benchmark 734 failed (commit https://github.com/google/benchmark/commit/668a1413c1 by @EricWF). :x: Build benchmark 743 failed (commit https://github.com/google/benchmark/commit/a4e9291d4f by @onto). :x: Build benchmark 746 failed (commit https://github.com/google/benchmark/commit/72604da53f by @onto). :x: Build benchmark 750 failed (commit https://github.com/google/benchmark/commit/833a524749 by @mwinterb). :x: Build benchmark 751 failed (commit https://github.com/google/benchmark/commit/2b9cc71dd4 by @mwinterb). :x: Build benchmark 754 failed (commit https://github.com/google/benchmark/commit/6d360cb836 by @dominichamon). :x: Build benchmark 770 failed (commit https://github.com/google/benchmark/commit/d314068262 by @dominichamon). :x: Build benchmark 772 failed (commit https://github.com/google/benchmark/commit/bb79b6cba1 by @dominichamon). :x: Build benchmark 777 failed (commit https://github.com/google/benchmark/commit/90ef8b3d90 by @dominichamon). :x: Build benchmark 779 failed (commit https://github.com/google/benchmark/commit/b1302e2860 by @dominichamon). :x: Build benchmark 781 failed (commit https://github.com/google/benchmark/commit/b91fc9159e by @dominichamon). :x: Build benchmark 755 failed (commit https://github.com/google/benchmark/commit/746b30d9bf by @EricWF). :x: Build benchmark 756 failed (commit https://github.com/google/benchmark/commit/dab5a40346 by @EricWF). :x: Build benchmark 757 failed (commit https://github.com/google/benchmark/commit/81654978d8 by @EricWF). :x: Build benchmark 760 failed (commit https://github.com/google/benchmark/commit/66f01f1cc8 by @EricWF). :white_check_mark: Build benchmark 761 completed (commit https://github.com/google/benchmark/commit/8f70af5c5a by @EricWF). :x: Build benchmark 762 failed (commit https://github.com/google/benchmark/commit/213751fef1 by @EricWF). :x: Build benchmark 763 failed (commit https://github.com/google/benchmark/commit/fadd29259b by @EricWF). :white_check_mark: Build benchmark 764 completed (commit https://github.com/google/benchmark/commit/c026cb7bcc by @EricWF). :x: Build benchmark 767 failed (commit https://github.com/google/benchmark/commit/51b77052ac by @EricWF). :x: Build benchmark 782 failed (commit https://github.com/google/benchmark/commit/40bb5cc27c by @Algunenano). :x: Build benchmark 784 failed (commit https://github.com/google/benchmark/commit/95d05d82a7 by @). :x: Build benchmark 785 failed (commit https://github.com/google/benchmark/commit/b243a69289 by @FredTingaud). :x: Build benchmark 786 failed (commit https://github.com/google/benchmark/commit/70b431e719 by @FredTingaud). :x: Build benchmark 788 failed (commit https://github.com/google/benchmark/commit/fd091a430b by @FredTingaud). :x: Build benchmark 790 failed (commit https://github.com/google/benchmark/commit/1e3a751350 by @EricWF). :x: Build benchmark 793 failed (commit https://github.com/google/benchmark/commit/e999f47b8e by @EricWF). :x: Build benchmark 791 failed (commit https://github.com/google/benchmark/commit/c64c586692 by @EricWF). :x: Build benchmark 796 failed (commit https://github.com/google/benchmark/commit/9c9142e2e0 by @Yangqing). :x: Build benchmark 797 failed (commit https://github.com/google/benchmark/commit/e2fc01f4e6 by @Yangqing). :x: Build benchmark 800 failed (commit https://github.com/google/benchmark/commit/32c35f1619 by @leokoppel). :x: Build benchmark 801 failed (commit https://github.com/google/benchmark/commit/8e27a9d9f7 by @leokoppel). :x: Build benchmark 803 failed (commit https://github.com/google/benchmark/commit/70057c8d8e by @ensonic). :x: Build benchmark 804 failed (commit https://github.com/google/benchmark/commit/a2df3de679 by @dominichamon). :x: Build benchmark 812 failed (commit https://github.com/google/benchmark/commit/c4c7dcdf09 by @dominichamon). :x: Build benchmark 815 failed (commit https://github.com/google/benchmark/commit/57613440af by @dominichamon). :x: Build benchmark 817 failed (commit https://github.com/google/benchmark/commit/82bc2e8fb9 by @LebedevRI). :white_check_mark: Build benchmark 824 completed (commit https://github.com/google/benchmark/commit/806bf3fc9a by @LebedevRI). :x: Build benchmark 825 failed (commit https://github.com/google/benchmark/commit/813d38603b by @LebedevRI). :x: Build benchmark 818 failed (commit https://github.com/google/benchmark/commit/f47c8b0f32 by @EricWF). :x: Build benchmark 820 failed (commit https://github.com/google/benchmark/commit/0d58fc408b by @dominichamon). :white_check_mark: Build benchmark 829 completed (commit https://github.com/google/benchmark/commit/6cad04890e by @sesse). :white_check_mark: Build benchmark 832 completed (commit https://github.com/google/benchmark/commit/1fc8741eb0 by @krytarowski). :x: Build benchmark 834 failed (commit https://github.com/google/benchmark/commit/563bb9438b by @EricWF). :white_check_mark: Build benchmark 835 completed (commit https://github.com/google/benchmark/commit/9379087576 by @EricWF). :x: Build benchmark 842 failed (commit https://github.com/google/benchmark/commit/f467c0fb18 by @EricWF). :white_check_mark: Build benchmark 843 completed (commit https://github.com/google/benchmark/commit/2908a6186f by @EricWF). :white_check_mark: Build benchmark 844 completed (commit https://github.com/google/benchmark/commit/4650fccae7 by @EricWF). :white_check_mark: Build benchmark 845 completed (commit https://github.com/google/benchmark/commit/e0877a12e4 by @EricWF). :white_check_mark: Build benchmark 847 completed (commit https://github.com/google/benchmark/commit/54365255ce by @LebedevRI). :x: Build benchmark 848 failed (commit https://github.com/google/benchmark/commit/6cd3b8eda5 by @EricWF). :x: Build benchmark 849 failed (commit https://github.com/google/benchmark/commit/9d0bacadd0 by @EricWF). :x: Build benchmark 850 failed (commit https://github.com/google/benchmark/commit/497ff83244 by @EricWF). :x: Build benchmark 851 failed (commit https://github.com/google/benchmark/commit/dbdcb0b7c4 by @EricWF). :x: Build benchmark 852 failed (commit https://github.com/google/benchmark/commit/f8f8985666 by @EricWF). :x: Build benchmark 853 failed (commit https://github.com/google/benchmark/commit/647665d819 by @EricWF). :x: Build benchmark 854 failed (commit https://github.com/google/benchmark/commit/0c90eb3b8b by @EricWF). :x: Build benchmark 855 failed (commit https://github.com/google/benchmark/commit/7813239f2e by @EricWF). :x: Build benchmark 856 failed (commit https://github.com/google/benchmark/commit/1ac7d74268 by @EricWF). :x: Build benchmark 857 failed (commit https://github.com/google/benchmark/commit/decddf1fe5 by @EricWF). :white_check_mark: Build benchmark 858 completed (commit https://github.com/google/benchmark/commit/c32affa913 by @EricWF). :x: Build benchmark 859 failed (commit https://github.com/google/benchmark/commit/8b1aed6a5b by @EricWF). :x: Build benchmark 860 failed (commit https://github.com/google/benchmark/commit/eb4cebbe3a by @EricWF). :white_check_mark: Build benchmark 861 completed (commit https://github.com/google/benchmark/commit/670f5304d8 by @EricWF). :white_check_mark: Build benchmark 862 completed (commit https://github.com/google/benchmark/commit/ec5d73f757 by @EricWF). :white_check_mark: Build benchmark 865 completed (commit https://github.com/google/benchmark/commit/e1d1cd3986 by @EricWF). :white_check_mark: Build benchmark 864 completed (commit https://github.com/google/benchmark/commit/dc2864f980 by @EricWF). :x: Build benchmark 868 failed (commit https://github.com/google/benchmark/commit/bd63b55662 by @EricWF). :white_check_mark: Build benchmark 866 completed (commit https://github.com/google/benchmark/commit/0e7bed58df by @LebedevRI). :white_check_mark: Build benchmark 877 completed (commit https://github.com/google/benchmark/commit/e97952c4df by @LebedevRI). :white_check_mark: Build benchmark 871 completed (commit https://github.com/google/benchmark/commit/d8b4fa8eea by @EricWF). :white_check_mark: Build benchmark 874 completed (commit https://github.com/google/benchmark/commit/555affc4b8 by @Kumar-Kishan). :white_check_mark: Build benchmark 875 completed (commit https://github.com/google/benchmark/commit/118be72a22 by @Kumar-Kishan). :x: Build benchmark 879 failed (commit https://github.com/google/benchmark/commit/b026ffa417 by @pwnall). :white_check_mark: Build benchmark 881 completed (commit https://github.com/google/benchmark/commit/f8bc49521b by @pwnall). :white_check_mark: Build benchmark 883 completed (commit https://github.com/google/benchmark/commit/0cc132f403 by @eliaskosunen). :white_check_mark: Build benchmark 884 completed (commit https://github.com/google/benchmark/commit/b61a66afb3 by @eliaskosunen). :x: Build benchmark 885 failed (commit https://github.com/google/benchmark/commit/d5e5ced42a by @ldionne). :white_check_mark: Build benchmark 886 completed (commit https://github.com/google/benchmark/commit/3eceac86fe by @monkeynova). :x: Build benchmark 895 failed (commit https://github.com/google/benchmark/commit/6dfb6a4131 by @EricWF). :x: Build benchmark 896 failed (commit https://github.com/google/benchmark/commit/e56db24676 by @EricWF). :white_check_mark: Build benchmark 907 completed (commit https://github.com/google/benchmark/commit/6364bc04d2 by @EricWF). :white_check_mark: Build benchmark 938 completed (commit https://github.com/google/benchmark/commit/17f586c302 by @EricWF). :white_check_mark: Build benchmark 944 completed (commit https://github.com/google/benchmark/commit/011c050cef by @EricWF). :white_check_mark: Build benchmark 897 completed (commit https://github.com/google/benchmark/commit/9b08350231 by @EricWF). :white_check_mark: Build benchmark 905 completed (commit https://github.com/google/benchmark/commit/0bff9681ff by @EricWF). :white_check_mark: Build benchmark 909 completed (commit https://github.com/google/benchmark/commit/900c2f7d30 by @qzmfranklin). :white_check_mark: Build benchmark 910 completed (commit https://github.com/google/benchmark/commit/538a966059 by @qzmfranklin). :white_check_mark: Build benchmark 916 completed (commit https://github.com/google/benchmark/commit/c706dc2345 by @qzmfranklin). :white_check_mark: Build benchmark 921 completed (commit https://github.com/google/benchmark/commit/e602f99d9b by @qzmfranklin). :white_check_mark: Build benchmark 922 completed (commit https://github.com/google/benchmark/commit/7fc481f922 by @winstondu). :x: Build benchmark 923 failed (commit https://github.com/google/benchmark/commit/b6dbac6363 by @lijinpei). :white_check_mark: Build benchmark 926 completed (commit https://github.com/google/benchmark/commit/e08548ba89 by @lijinpei). :white_check_mark: Build benchmark 927 completed (commit https://github.com/google/benchmark/commit/da546b801d by @Maratyszcza). :white_check_mark: Build benchmark 928 completed (commit https://github.com/google/benchmark/commit/7afcca4fc8 by @Maratyszcza). :white_check_mark: Build benchmark 929 completed (commit https://github.com/google/benchmark/commit/8b4f11e1f9 by @Maratyszcza). :white_check_mark: Build benchmark 930 completed (commit https://github.com/google/benchmark/commit/08a0561a14 by @Maratyszcza). :white_check_mark: Build benchmark 931 completed (commit https://github.com/google/benchmark/commit/cdd596fd8d by @Maratyszcza). :x: Build benchmark 936 failed (commit https://github.com/google/benchmark/commit/62340aac82 by @Maratyszcza). :white_check_mark: Build benchmark 937 completed (commit https://github.com/google/benchmark/commit/fefdcd5b80 by @Maratyszcza). :x: Build benchmark 945 failed (commit https://github.com/google/benchmark/commit/f9dbe71e7f by @lijinpei). :white_check_mark: Build benchmark 947 completed (commit https://github.com/google/benchmark/commit/b122cfb5ec by @lijinpei). :x: Build benchmark 950 failed (commit https://github.com/google/benchmark/commit/ca7b0cccaf by @lijinpei). :x: Build benchmark 951 failed (commit https://github.com/google/benchmark/commit/690b2a4035 by @lijinpei). :white_check_mark: Build benchmark 952 completed (commit https://github.com/google/benchmark/commit/529fc8788c by @lijinpei). :x: Build benchmark 953 failed (commit https://github.com/google/benchmark/commit/7cd3f6b4a0 by @lijinpei). :x: Build benchmark 955 failed (commit https://github.com/google/benchmark/commit/d9940082d3 by @lijinpei). :x: Build benchmark 956 failed (commit https://github.com/google/benchmark/commit/a7d37aac99 by @lijinpei). :x: Build benchmark 962 failed (commit https://github.com/google/benchmark/commit/c7e2ac7617 by @lijinpei). :x: Build benchmark 963 failed (commit https://github.com/google/benchmark/commit/d64de82c6f by @lijinpei). :x: Build benchmark 966 failed (commit https://github.com/google/benchmark/commit/0b6085ebd0 by @lijinpei). :x: Build benchmark 967 failed (commit https://github.com/google/benchmark/commit/2b1d502e05 by @lijinpei). :x: Build benchmark 1302 failed (commit https://github.com/google/benchmark/commit/bbce1f06c3 by @lijinpei). :white_check_mark: Build benchmark 949 completed (commit https://github.com/google/benchmark/commit/2009bfc4a7 by @dominichamon). :white_check_mark: Build benchmark 957 completed (commit https://github.com/google/benchmark/commit/69b4d7718d by @NAThompson). :x: Build benchmark 958 failed (commit https://github.com/google/benchmark/commit/2a02f88a1d by @NAThompson). :x: Build benchmark 959 failed (commit https://github.com/google/benchmark/commit/ab7f5f972b by @NAThompson). :x: Build benchmark 960 failed (commit https://github.com/google/benchmark/commit/15ee0134b3 by @NAThompson). :x: Build benchmark 961 failed (commit https://github.com/google/benchmark/commit/ebba6ca394 by @NAThompson). :white_check_mark: Build benchmark 968 completed (commit https://github.com/google/benchmark/commit/56eadec07f by @manparvesh). :x: Build benchmark 973 failed (commit https://github.com/google/benchmark/commit/d090cf4b56 by @sam-panzer). :x: Build benchmark 974 failed (commit https://github.com/google/benchmark/commit/6629056c2e by @sam-panzer). :white_check_mark: Build benchmark 975 completed (commit https://github.com/google/benchmark/commit/de38336323 by @sam-panzer). :white_check_mark: Build benchmark 976 completed (commit https://github.com/google/benchmark/commit/fb400d9288 by @sam-panzer). :x: Build benchmark 980 failed (commit https://github.com/google/benchmark/commit/8065d70394 by @sam-panzer). :x: Build benchmark 981 failed (commit https://github.com/google/benchmark/commit/91e1d51239 by @sam-panzer). :x: Build benchmark 982 failed (commit https://github.com/google/benchmark/commit/a81b440a8d by @sam-panzer). :white_check_mark: Build benchmark 983 completed (commit https://github.com/google/benchmark/commit/7cc0b0a728 by @sam-panzer). :white_check_mark: Build benchmark 977 completed (commit https://github.com/google/benchmark/commit/41fb84f33b by @aJetHorn). :white_check_mark: Build benchmark 978 completed (commit https://github.com/google/benchmark/commit/d073d46b29 by @aJetHorn). :white_check_mark: Build benchmark 986 completed (commit https://github.com/google/benchmark/commit/f7548693a0 by @sam-panzer). :x: Build benchmark 989 failed (commit https://github.com/google/benchmark/commit/80f5d6068b by @sam-panzer). :white_check_mark: Build benchmark 1005 completed (commit https://github.com/google/benchmark/commit/63ce927272 by @sam-panzer). :x: Build benchmark 987 failed (commit https://github.com/google/benchmark/commit/5fa59229a5 by @EricWF). :x: Build benchmark 988 failed (commit https://github.com/google/benchmark/commit/1dcc4acb2b by @EricWF). :x: Build benchmark 998 failed (commit https://github.com/google/benchmark/commit/eed1eebbcf by @EricWF). :white_check_mark: Build benchmark 1001 completed (commit https://github.com/google/benchmark/commit/a492f9fc0d by @EricWF). :white_check_mark: Build benchmark 1004 completed (commit https://github.com/google/benchmark/commit/03c4d63e31 by @EricWF). :white_check_mark: Build benchmark 1006 completed (commit https://github.com/google/benchmark/commit/e23d97b807 by @EricWF). :x: Build benchmark 997 failed (commit https://github.com/google/benchmark/commit/0e4f6f5a4d by @EricWF). :white_check_mark: Build benchmark 1038 completed (commit https://github.com/google/benchmark/commit/ea9de3658c by @EricWF). :white_check_mark: Build benchmark 1047 completed (commit https://github.com/google/benchmark/commit/4b063052b6 by @EricWF). :white_check_mark: Build benchmark 1055 completed (commit https://github.com/google/benchmark/commit/afed53d504 by @EricWF). :x: Build benchmark 1056 failed (commit https://github.com/google/benchmark/commit/907711ef1d by @EricWF). :white_check_mark: Build benchmark 1057 completed (commit https://github.com/google/benchmark/commit/428d4ca88c by @EricWF). :white_check_mark: Build benchmark 1061 completed (commit https://github.com/google/benchmark/commit/630e0e8506 by @EricWF). :white_check_mark: Build benchmark 1063 completed (commit https://github.com/google/benchmark/commit/af7ce5fdec by @EricWF). :white_check_mark: Build benchmark 1065 completed (commit https://github.com/google/benchmark/commit/7738ee1ce5 by @EricWF). :white_check_mark: Build benchmark 1069 completed (commit https://github.com/google/benchmark/commit/fb2552a0c2 by @EricWF). :white_check_mark: Build benchmark 1070 completed (commit https://github.com/google/benchmark/commit/52fd2436f5 by @EricWF). :white_check_mark: Build benchmark 1130 completed (commit https://github.com/google/benchmark/commit/408113083e by @EricWF). :white_check_mark: Build benchmark 1132 completed (commit https://github.com/google/benchmark/commit/57aa030a13 by @EricWF). :white_check_mark: Build benchmark 1136 completed (commit https://github.com/google/benchmark/commit/ab6fe60e62 by @EricWF). :white_check_mark: Build benchmark 1064 completed (commit https://github.com/google/benchmark/commit/bc81b2c1d9 by @ianloic). :white_check_mark: Build benchmark 1075 completed (commit https://github.com/google/benchmark/commit/37eee8aac9 by @jmillikin-stripe). :white_check_mark: Build benchmark 1079 completed (commit https://github.com/google/benchmark/commit/111cf3821f by @jmillikin-stripe). :x: Build benchmark 1102 failed (commit https://github.com/google/benchmark/commit/950de57c23 by @jmillikin-stripe). :x: Build benchmark 1080 failed (commit https://github.com/google/benchmark/commit/3a61199d66 by @EricWF). :x: Build benchmark 1081 failed (commit https://github.com/google/benchmark/commit/3bd0139831 by @EricWF). :white_check_mark: Build benchmark 1082 completed (commit https://github.com/google/benchmark/commit/1f5ec176a2 by @EricWF). :white_check_mark: Build benchmark 1084 completed (commit https://github.com/google/benchmark/commit/7e5dac61f1 by @jwakely). :white_check_mark: Build benchmark 1085 completed (commit https://github.com/google/benchmark/commit/e4cdf9b186 by @jwakely). :white_check_mark: Build benchmark 1089 completed (commit https://github.com/google/benchmark/commit/6f17bd0d9d by @alekseyshl). :white_check_mark: Build benchmark 1090 completed (commit https://github.com/google/benchmark/commit/66216b754d by @alekseyshl). :x: Build benchmark 1091 failed (commit https://github.com/google/benchmark/commit/a2bfc22de7 by @guoyr). :white_check_mark: Build benchmark 1094 completed (commit https://github.com/google/benchmark/commit/518b22b74e by @winksaville). :white_check_mark: Build benchmark 1095 completed (commit https://github.com/google/benchmark/commit/1a42ef9855 by @winksaville). :white_check_mark: Build benchmark 1098 completed (commit https://github.com/google/benchmark/commit/dc8a3746ad by @winksaville). :white_check_mark: Build benchmark 1100 completed (commit https://github.com/google/benchmark/commit/a9e4c16d7c by @winksaville). :x: Build benchmark 1105 failed (commit https://github.com/google/benchmark/commit/21b66bcb2b by @dominichamon). :white_check_mark: Build benchmark 1117 completed (commit https://github.com/google/benchmark/commit/8cd0929606 by @dominichamon). :white_check_mark: Build benchmark 1118 completed (commit https://github.com/google/benchmark/commit/2323e34dc4 by @dominichamon). :white_check_mark: Build benchmark 1121 completed (commit https://github.com/google/benchmark/commit/abc62f23ed by @dominichamon). :white_check_mark: Build benchmark 1135 completed (commit https://github.com/google/benchmark/commit/3e52ca375a by @dominichamon). :x: Build benchmark 1155 failed (commit https://github.com/google/benchmark/commit/1bd6bd8118 by @dominichamon). :x: Build benchmark 1157 failed (commit https://github.com/google/benchmark/commit/509e7a8cba by @dominichamon). :white_check_mark: Build benchmark 1160 completed (commit https://github.com/google/benchmark/commit/ce3821d9f7 by @dominichamon). :white_check_mark: Build benchmark 1124 completed (commit https://github.com/google/benchmark/commit/0d3243d39f by @dominichamon). :white_check_mark: Build benchmark 1129 completed (commit https://github.com/google/benchmark/commit/30b3018766 by @dominichamon). :x: Build benchmark 1142 failed (commit https://github.com/google/benchmark/commit/ab66414f12 by @EricWF). :x: Build benchmark 1146 failed (commit https://github.com/google/benchmark/commit/553302234f by @EricWF). :x: Build benchmark 1147 failed (commit https://github.com/google/benchmark/commit/b824690938 by @EricWF). :white_check_mark: Build benchmark 1148 completed (commit https://github.com/google/benchmark/commit/7c8bbc574e by @EricWF). :white_check_mark: Build benchmark 1150 completed (commit https://github.com/google/benchmark/commit/ddf49fd9b9 by @EricWF). :x: Build benchmark 1152 failed (commit https://github.com/google/benchmark/commit/6a49811bbc by @dominichamon). :x: Build benchmark 1166 failed (commit https://github.com/google/benchmark/commit/eefd4f9782 by @FredTingaud). :white_check_mark: Build benchmark 1167 completed (commit https://github.com/google/benchmark/commit/11651a3e48 by @FredTingaud). :x: Build benchmark 1169 failed (commit https://github.com/google/benchmark/commit/b5d980f763 by @BaaMeow). :white_check_mark: Build benchmark 1170 completed (commit https://github.com/google/benchmark/commit/37f4f36c8f by @BaaMeow). :white_check_mark: Build benchmark 1174 completed (commit https://github.com/google/benchmark/commit/3c6035f16e by @BaaMeow). :white_check_mark: Build benchmark 1180 completed (commit https://github.com/google/benchmark/commit/668038b858 by @BaaMeow). :white_check_mark: Build benchmark 1181 completed (commit https://github.com/google/benchmark/commit/4d60eca0b2 by @BaaMeow). :white_check_mark: Build benchmark 1182 completed (commit https://github.com/google/benchmark/commit/a82397669e by @BaaMeow). :x: Build benchmark 1183 failed (commit https://github.com/google/benchmark/commit/d405119f2e by @BaaMeow). :white_check_mark: Build benchmark 1184 completed (commit https://github.com/google/benchmark/commit/08a18475c1 by @BaaMeow). :white_check_mark: Build benchmark 1185 completed (commit https://github.com/google/benchmark/commit/a26dd40c10 by @dominichamon). :white_check_mark: Build benchmark 1188 completed (commit https://github.com/google/benchmark/commit/c7d3a29198 by @BaaMeow). :white_check_mark: Build benchmark 1190 completed (commit https://github.com/google/benchmark/commit/c804bae4ee by @dominichamon). :x: Build benchmark 1217 failed (commit https://github.com/google/benchmark/commit/047b848130 by @BaaMeow). :white_check_mark: Build benchmark 1218 completed (commit https://github.com/google/benchmark/commit/7d092824ad by @BaaMeow). :white_check_mark: Build benchmark 1187 completed (commit https://github.com/google/benchmark/commit/f13e6fbf06 by @dominichamon). :white_check_mark: Build benchmark 1191 completed (commit https://github.com/google/benchmark/commit/6d3bb2b57b by @Yangqing). :white_check_mark: Build benchmark 1192 completed (commit https://github.com/google/benchmark/commit/d18e1ed9a4 by @pwnall). :x: Build benchmark 1193 failed (commit https://github.com/google/benchmark/commit/85ecbea23e by @FredTingaud). :white_check_mark: Build benchmark 1194 completed (commit https://github.com/google/benchmark/commit/42a9de25cb by @FredTingaud). :x: Build benchmark 1197 failed (commit https://github.com/google/benchmark/commit/5ef6e8b159 by @Tim020). :white_check_mark: Build benchmark 1199 completed (commit https://github.com/google/benchmark/commit/90e0e6e40d by @Tim020). :white_check_mark: Build benchmark 1200 completed (commit https://github.com/google/benchmark/commit/0ac2de3863 by @Tim020). :x: Build benchmark 1205 failed (commit https://github.com/google/benchmark/commit/b7d3b544c2 by @astrelni). :x: Build benchmark 1206 failed (commit https://github.com/google/benchmark/commit/aa272d2b62 by @astrelni). :white_check_mark: Build benchmark 1207 completed (commit https://github.com/google/benchmark/commit/0e74ad89f6 by @astrelni). :white_check_mark: Build benchmark 1208 completed (commit https://github.com/google/benchmark/commit/8144f205a5 by @NanXiao). :white_check_mark: Build benchmark 1211 completed (commit https://github.com/google/benchmark/commit/2110279563 by @sbc100). :white_check_mark: Build benchmark 1213 completed (commit https://github.com/google/benchmark/commit/fa96114fe5 by @LebedevRI). :white_check_mark: Build benchmark 1214 completed (commit https://github.com/google/benchmark/commit/ed98d0211e by @LebedevRI). :white_check_mark: Build benchmark 1215 completed (commit https://github.com/google/benchmark/commit/9bc009d3d5 by @sam-panzer). :white_check_mark: Build benchmark 1216 completed (commit https://github.com/google/benchmark/commit/7ed1818809 by @php1ic). :white_check_mark: Build benchmark 1219 completed (commit https://github.com/google/benchmark/commit/8d7d74a851 by @NanXiao). :white_check_mark: Build benchmark 1224 completed (commit https://github.com/google/benchmark/commit/23ebb5c95c by @LebedevRI). :white_check_mark: Build benchmark 1227 completed (commit https://github.com/google/benchmark/commit/98ff7b3563 by @LebedevRI). :white_check_mark: Build benchmark 1228 completed (commit https://github.com/google/benchmark/commit/9511a22ed7 by @LebedevRI). :white_check_mark: Build benchmark 1233 completed (commit https://github.com/google/benchmark/commit/b6550ed7cd by @LebedevRI). :white_check_mark: Build benchmark 1225 completed (commit https://github.com/google/benchmark/commit/d0ef02004a by @denizevrenci). :white_check_mark: Build benchmark 1229 completed (commit https://github.com/google/benchmark/commit/23da59f4fb by @Maratyszcza). :x: Build benchmark 1230 failed (commit https://github.com/google/benchmark/commit/c60a676d19 by @Maratyszcza). :white_check_mark: Build benchmark 1231 completed (commit https://github.com/google/benchmark/commit/22d87ec06b by @Maratyszcza). :white_check_mark: Build benchmark 1232 completed (commit https://github.com/google/benchmark/commit/2bce6f714b by @Maratyszcza). :white_check_mark: Build benchmark 1269 completed (commit https://github.com/google/benchmark/commit/9cdd297ed3 by @Maratyszcza). :x: Build benchmark 1270 failed (commit https://github.com/google/benchmark/commit/f7d2f8bcfa by @Maratyszcza). :white_check_mark: Build benchmark 1271 completed (commit https://github.com/google/benchmark/commit/7ff6cca59d by @Maratyszcza). :white_check_mark: Build benchmark 1272 completed (commit https://github.com/google/benchmark/commit/9099b20361 by @Maratyszcza). :white_check_mark: Build benchmark 1234 completed (commit https://github.com/google/benchmark/commit/ff42c86379 by @sam-panzer). :x: Build benchmark 1237 failed (commit https://github.com/google/benchmark/commit/7c9d84097a by @sam-panzer). :white_check_mark: Build benchmark 1235 completed (commit https://github.com/google/benchmark/commit/ee5dca72ad by @BaaMeow). :white_check_mark: Build benchmark 1236 completed (commit https://github.com/google/benchmark/commit/bab7e74058 by @BaaMeow). :white_check_mark: Build benchmark 1239 completed (commit https://github.com/google/benchmark/commit/31cb913529 by @BaaMeow). :white_check_mark: Build benchmark 1238 completed (commit https://github.com/google/benchmark/commit/27e5322e96 by @mattreecebentley). :white_check_mark: Build benchmark 1242 completed (commit https://github.com/google/benchmark/commit/f21d7bab1a by @astrelni). :white_check_mark: Build benchmark 1266 completed (commit https://github.com/google/benchmark/commit/14f394cd57 by @dominichamon). :white_check_mark: Build benchmark 1267 completed (commit https://github.com/google/benchmark/commit/338fc3adf7 by @BaaMeow). :white_check_mark: Build benchmark 1274 completed (commit https://github.com/google/benchmark/commit/c859bb8250 by @sergiud). :white_check_mark: Build benchmark 1276 completed (commit https://github.com/google/benchmark/commit/3aaaa4bc9a by @Maratyszcza). :white_check_mark: Build benchmark 1277 completed (commit https://github.com/google/benchmark/commit/45c457ac62 by @Maratyszcza). :white_check_mark: Build benchmark 1280 completed (commit https://github.com/google/benchmark/commit/9aaedea3c3 by @dominichamon). :x: Build benchmark 1284 failed (commit https://github.com/google/benchmark/commit/032be05e02 by @dominichamon). :white_check_mark: Build benchmark 1286 completed (commit https://github.com/google/benchmark/commit/9a8871dc30 by @BaaMeow). :white_check_mark: Build benchmark 1288 completed (commit https://github.com/google/benchmark/commit/b121917ed9 by @BaaMeow). :x: Build benchmark 1367 failed (commit https://github.com/google/benchmark/commit/4768af1dbe by @BaaMeow). :white_check_mark: Build benchmark 1368 completed (commit https://github.com/google/benchmark/commit/7d7a0aec1d by @BaaMeow). :white_check_mark: Build benchmark 1371 completed (commit https://github.com/google/benchmark/commit/e023a94ade by @BaaMeow). :white_check_mark: Build benchmark 1373 completed (commit https://github.com/google/benchmark/commit/ea8b468b39 by @BaaMeow). :white_check_mark: Build benchmark 1386 completed (commit https://github.com/google/benchmark/commit/08614d7ca6 by @BaaMeow). :white_check_mark: Build benchmark 1387 completed (commit https://github.com/google/benchmark/commit/e2f4089675 by @BaaMeow). :white_check_mark: Build benchmark 1287 completed (commit https://github.com/google/benchmark/commit/a8263d98e1 by @LebedevRI). :white_check_mark: Build benchmark 1292 completed (commit https://github.com/google/benchmark/commit/b608e8f53d by @LebedevRI). :white_check_mark: Build benchmark 1295 completed (commit https://github.com/google/benchmark/commit/b04b81eb53 by @LebedevRI). :white_check_mark: Build benchmark 1297 completed (commit https://github.com/google/benchmark/commit/6c0ae4037f by @). :white_check_mark: Build benchmark 1299 completed (commit https://github.com/google/benchmark/commit/d7fbbbbc5f by @dominichamon). :white_check_mark: Build benchmark 1301 completed (commit https://github.com/google/benchmark/commit/ad2fa1ea14 by @dominichamon). :white_check_mark: Build benchmark 1304 completed (commit https://github.com/google/benchmark/commit/38a8ade25a by @dominichamon). :x: Build benchmark 1306 failed (commit https://github.com/google/benchmark/commit/68993e6319 by @dominichamon). :white_check_mark: Build benchmark 1309 completed (commit https://github.com/google/benchmark/commit/f9e84d10e6 by @dominichamon). :white_check_mark: Build benchmark 1317 completed (commit https://github.com/google/benchmark/commit/4225cfb76b by @dominichamon). :white_check_mark: Build benchmark 1321 completed (commit https://github.com/google/benchmark/commit/b073e75738 by @dominichamon). :x: Build benchmark 1310 failed (commit https://github.com/google/benchmark/commit/88c0fcfdf5 by @EricWF). :x: Build benchmark 1312 failed (commit https://github.com/google/benchmark/commit/65a1115271 by @EricWF). :white_check_mark: Build benchmark 1313 completed (commit https://github.com/google/benchmark/commit/ccfaa705f0 by @mumumu). :white_check_mark: Build benchmark 1318 completed (commit https://github.com/google/benchmark/commit/6e07761b21 by @nazavode). :white_check_mark: Build benchmark 1322 completed (commit https://github.com/google/benchmark/commit/34a4f17560 by @nazavode). :white_check_mark: Build benchmark 1327 completed (commit https://github.com/google/benchmark/commit/ded9ab4f71 by @atdt). :white_check_mark: Build benchmark 1328 completed (commit https://github.com/google/benchmark/commit/a60a808712 by @Croydon). :white_check_mark: Build benchmark 1332 completed (commit https://github.com/google/benchmark/commit/6fe2ba9aa1 by @Croydon). :white_check_mark: Build benchmark 1333 completed (commit https://github.com/google/benchmark/commit/74c9260717 by @Croydon). :white_check_mark: Build benchmark 1329 completed (commit https://github.com/google/benchmark/commit/8b1b45f0d7 by @SSE4). :x: Build benchmark 1331 failed (commit https://github.com/google/benchmark/commit/52104d7c80 by @SSE4). :white_check_mark: Build benchmark 1335 completed (commit https://github.com/google/benchmark/commit/a144dc45c7 by @LebedevRI). :white_check_mark: Build benchmark 1336 completed (commit https://github.com/google/benchmark/commit/48859a1530 by @LebedevRI). :white_check_mark: Build benchmark 1339 completed (commit https://github.com/google/benchmark/commit/23b9e70d9f by @dominichamon). :white_check_mark: Build benchmark 1340 completed (commit https://github.com/google/benchmark/commit/480ea1fdc9 by @Croydon). :white_check_mark: Build benchmark 1342 completed (commit https://github.com/google/benchmark/commit/433dd01f47 by @Croydon). :x: Build benchmark 1350 failed (commit https://github.com/google/benchmark/commit/e06e42ccde by @Croydon). :white_check_mark: Build benchmark 1351 completed (commit https://github.com/google/benchmark/commit/725e138895 by @Croydon). :white_check_mark: Build benchmark 1352 completed (commit https://github.com/google/benchmark/commit/f8165d00d2 by @Croydon). :white_check_mark: Build benchmark 1354 completed (commit https://github.com/google/benchmark/commit/b1e036d229 by @Croydon). :x: Build benchmark 1446 failed (commit https://github.com/google/benchmark/commit/693d8b7d27 by @Croydon). :x: Build benchmark 1458 failed (commit https://github.com/google/benchmark/commit/97af35af59 by @Croydon). :x: Build benchmark 1459 failed (commit https://github.com/google/benchmark/commit/90f97b47e0 by @Croydon). :x: Build benchmark 1463 failed (commit https://github.com/google/benchmark/commit/ec6546605f by @Croydon). :x: Build benchmark 1464 failed (commit https://github.com/google/benchmark/commit/9a8147b993 by @Croydon). :x: Build benchmark 1465 failed (commit https://github.com/google/benchmark/commit/ee3bb6de2a by @Croydon). :x: Build benchmark 1466 failed (commit https://github.com/google/benchmark/commit/d42986e768 by @Croydon). :x: Build benchmark 1469 failed (commit https://github.com/google/benchmark/commit/fd96d43aff by @Croydon). :x: Build benchmark 1473 failed (commit https://github.com/google/benchmark/commit/a10edbe4e6 by @Croydon). :white_check_mark: Build benchmark 1476 completed (commit https://github.com/google/benchmark/commit/a95554063f by @Croydon). :white_check_mark: Build benchmark 1482 completed (commit https://github.com/google/benchmark/commit/700a7f8a0a by @Croydon). :white_check_mark: Build benchmark 1488 completed (commit https://github.com/google/benchmark/commit/0574b28a98 by @Croydon). :white_check_mark: Build benchmark 1489 completed (commit https://github.com/google/benchmark/commit/459e30dac6 by @Croydon). :white_check_mark: Build benchmark 1490 completed (commit https://github.com/google/benchmark/commit/230aa5afc1 by @Croydon). :white_check_mark: Build benchmark 1491 completed (commit https://github.com/google/benchmark/commit/0df3d3422a by @Croydon). :white_check_mark: Build benchmark 1493 completed (commit https://github.com/google/benchmark/commit/a77acb132d by @Croydon). :white_check_mark: Build benchmark 1496 completed (commit https://github.com/google/benchmark/commit/67c7f86071 by @Croydon). :white_check_mark: Build benchmark 1497 completed (commit https://github.com/google/benchmark/commit/c249d7e4d5 by @Croydon). :white_check_mark: Build benchmark 1498 completed (commit https://github.com/google/benchmark/commit/062dea779e by @Croydon). :white_check_mark: Build benchmark 1500 completed (commit https://github.com/google/benchmark/commit/c4a9e94935 by @Croydon). :white_check_mark: Build benchmark 1512 completed (commit https://github.com/google/benchmark/commit/cd441c7383 by @Croydon). :white_check_mark: Build benchmark 1513 completed (commit https://github.com/google/benchmark/commit/76fd033e6f by @Croydon). :white_check_mark: Build benchmark 1516 completed (commit https://github.com/google/benchmark/commit/3417eaf6f3 by @Croydon). :x: Build benchmark 1518 failed (commit https://github.com/google/benchmark/commit/cf2cbb0d7a by @Croydon). :white_check_mark: Build benchmark 1519 completed (commit https://github.com/google/benchmark/commit/3783b2d143 by @Croydon). :white_check_mark: Build benchmark 1529 completed (commit https://github.com/google/benchmark/commit/57bd2edaab by @Croydon). :white_check_mark: Build benchmark 1553 completed (commit https://github.com/google/benchmark/commit/e93d45ae0a by @Croydon). :x: Build benchmark 1355 failed (commit https://github.com/google/benchmark/commit/a5f532658c by @kbobyrev). :x: Build benchmark 1357 failed (commit https://github.com/google/benchmark/commit/6ffea96906 by @kbobyrev). :x: Build benchmark 1358 failed (commit https://github.com/google/benchmark/commit/42adc264be by @kbobyrev). :x: Build benchmark 1359 failed (commit https://github.com/google/benchmark/commit/eba1763d52 by @kbobyrev). :white_check_mark: Build benchmark 1360 completed (commit https://github.com/google/benchmark/commit/0d1906785f by @kbobyrev). :white_check_mark: Build benchmark 1365 completed (commit https://github.com/google/benchmark/commit/bc47c95194 by @dominichamon). :x: Build benchmark 1420 failed (commit https://github.com/google/benchmark/commit/aa4eb0428f by @dominichamon). :x: Build benchmark 1423 failed (commit https://github.com/google/benchmark/commit/ccfba5f3b5 by @dominichamon). :x: Build benchmark 1457 failed (commit https://github.com/google/benchmark/commit/4520662985 by @dominichamon). :x: Build benchmark 1484 failed (commit https://github.com/google/benchmark/commit/6d9e5ac45f by @dominichamon). :white_check_mark: Build benchmark 1366 completed (commit https://github.com/google/benchmark/commit/5429d77878 by @dominichamon). :white_check_mark: Build benchmark 1369 completed (commit https://github.com/google/benchmark/commit/892467dc40 by @BaaMeow). :white_check_mark: Build benchmark 1374 completed (commit https://github.com/google/benchmark/commit/162e569dae by @bmwiedemann). :white_check_mark: Build benchmark 1375 completed (commit https://github.com/google/benchmark/commit/d46012d163 by @LebedevRI). :white_check_mark: Build benchmark 1400 completed (commit https://github.com/google/benchmark/commit/2f02367416 by @LebedevRI). :white_check_mark: Build benchmark 1376 completed (commit https://github.com/google/benchmark/commit/ddbe76d867 by @LebedevRI). :x: Build benchmark 1377 failed (commit https://github.com/google/benchmark/commit/9dd521fc67 by @LebedevRI). :x: Build benchmark 1378 failed (commit https://github.com/google/benchmark/commit/cb960fbba8 by @LebedevRI). :x: Build benchmark 1379 failed (commit https://github.com/google/benchmark/commit/0d4a129ac2 by @LebedevRI). :x: Build benchmark 1380 failed (commit https://github.com/google/benchmark/commit/28d2e3e708 by @LebedevRI). :white_check_mark: Build benchmark 1381 completed (commit https://github.com/google/benchmark/commit/d7a05b3067 by @LebedevRI). :white_check_mark: Build benchmark 1382 completed (commit https://github.com/google/benchmark/commit/a40d39a42e by @LebedevRI). :white_check_mark: Build benchmark 1383 completed (commit https://github.com/google/benchmark/commit/f79a0f2a51 by @LebedevRI). :x: Build benchmark 1396 failed (commit https://github.com/google/benchmark/commit/262e146e02 by @LebedevRI). :white_check_mark: Build benchmark 1397 completed (commit https://github.com/google/benchmark/commit/506fe68a83 by @LebedevRI). :white_check_mark: Build benchmark 1398 completed (commit https://github.com/google/benchmark/commit/4a40148f12 by @LebedevRI). :white_check_mark: Build benchmark 1384 completed (commit https://github.com/google/benchmark/commit/09e1e77c8a by @LebedevRI). :white_check_mark: Build benchmark 1385 completed (commit https://github.com/google/benchmark/commit/bf84bb7676 by @LebedevRI). :x: Build benchmark 1388 failed (commit https://github.com/google/benchmark/commit/8226d5c726 by @GevArakelyan). :white_check_mark: Build benchmark 1392 completed (commit https://github.com/google/benchmark/commit/4c15d82d18 by @LebedevRI). :x: Build benchmark 1402 failed (commit https://github.com/google/benchmark/commit/82c280381b by @LebedevRI). :x: Build benchmark 1403 failed (commit https://github.com/google/benchmark/commit/658116960e by @LebedevRI). :x: Build benchmark 1413 failed (commit https://github.com/google/benchmark/commit/ebf7a6f2ad by @LebedevRI). :x: Build benchmark 1415 failed (commit https://github.com/google/benchmark/commit/3580387aea by @LebedevRI). :x: Build benchmark 1416 failed (commit https://github.com/google/benchmark/commit/b92ecfb4ff by @LebedevRI). :x: Build benchmark 1417 failed (commit https://github.com/google/benchmark/commit/45bfd09832 by @LebedevRI). :x: Build benchmark 1426 failed (commit https://github.com/google/benchmark/commit/38bc928fb7 by @LebedevRI). :x: Build benchmark 1427 failed (commit https://github.com/google/benchmark/commit/e87c71478b by @LebedevRI). :x: Build benchmark 1404 failed (commit https://github.com/google/benchmark/commit/38f6d47690 by @pseyfert). :x: Build benchmark 1406 failed (commit https://github.com/google/benchmark/commit/e65c8d5ece by @LebedevRI). :x: Build benchmark 1407 failed (commit https://github.com/google/benchmark/commit/edde8a0b4b by @snnn). :x: Build benchmark 1414 failed (commit https://github.com/google/benchmark/commit/c5ff09b6aa by @mattkretz). :x: Build benchmark 1432 failed (commit https://github.com/google/benchmark/commit/4e336bc522 by @LebedevRI). :x: Build benchmark 1433 failed (commit https://github.com/google/benchmark/commit/a6af2ed1f6 by @LebedevRI). :x: Build benchmark 1434 failed (commit https://github.com/google/benchmark/commit/583c4ef1b5 by @LebedevRI). :x: Build benchmark 1438 failed (commit https://github.com/google/benchmark/commit/2e697c5709 by @LebedevRI). :x: Build benchmark 1440 failed (commit https://github.com/google/benchmark/commit/5f31abde8c by @LebedevRI). :x: Build benchmark 1443 failed (commit https://github.com/google/benchmark/commit/24bab0e192 by @LebedevRI). :x: Build benchmark 1444 failed (commit https://github.com/google/benchmark/commit/bd6f6396d8 by @mstorsjo). :x: Build benchmark 1447 failed (commit https://github.com/google/benchmark/commit/9d5649b3be by @mstorsjo). :x: Build benchmark 1450 failed (commit https://github.com/google/benchmark/commit/cdddc06b28 by @janisozaur). :x: Build benchmark 1451 failed (commit https://github.com/google/benchmark/commit/d0a7f8ea97 by @janisozaur). :x: Build benchmark 1452 failed (commit https://github.com/google/benchmark/commit/31c76e4c00 by @). :x: Build benchmark 1470 failed (commit https://github.com/google/benchmark/commit/93720b92ec by @LebedevRI). :x: Build benchmark 1471 failed (commit https://github.com/google/benchmark/commit/b7ef961139 by @LebedevRI). :x: Build benchmark 1487 failed (commit https://github.com/google/benchmark/commit/c356a2f34d by @LebedevRI). :x: Build benchmark 1494 failed (commit https://github.com/google/benchmark/commit/c7a3c9277c by @LebedevRI). :white_check_mark: Build benchmark 1495 completed (commit https://github.com/google/benchmark/commit/fdd6be7f6c by @LebedevRI). :white_check_mark: Build benchmark 1504 completed (commit https://github.com/google/benchmark/commit/ac305f101c by @LebedevRI). :white_check_mark: Build benchmark 1472 completed (commit https://github.com/google/benchmark/commit/ffec02f9c2 by @LebedevRI). :white_check_mark: Build benchmark 1501 completed (commit https://github.com/google/benchmark/commit/9a27b19426 by @jbeich). :white_check_mark: Build benchmark 1502 completed (commit https://github.com/google/benchmark/commit/725183bc2d by @pwnall). :white_check_mark: Build benchmark 1506 completed (commit https://github.com/google/benchmark/commit/b2f8488aae by @Lord-Kamina). :white_check_mark: Build benchmark 1509 completed (commit https://github.com/google/benchmark/commit/5231f34aa8 by @iillyyaa). :white_check_mark: Build benchmark 1511 completed (commit https://github.com/google/benchmark/commit/e7180a7a07 by @peterjc123). :x: Build benchmark 1514 failed (commit https://github.com/google/benchmark/commit/e2e04fa150 by @EricWF). :white_check_mark: Build benchmark 1515 completed (commit https://github.com/google/benchmark/commit/a310dd0e40 by @EricWF). :white_check_mark: Build benchmark 1565 completed (commit https://github.com/google/benchmark/commit/0ca09d3fe9 by @EricWF). :white_check_mark: Build benchmark 1566 completed (commit https://github.com/google/benchmark/commit/5e0cb5180f by @EricWF). :white_check_mark: Build benchmark 1520 completed (commit https://github.com/google/benchmark/commit/450c07dd6c by @timshen91). :white_check_mark: Build benchmark 1521 completed (commit https://github.com/google/benchmark/commit/15ead5e9da by @olzhabay). :x: Build benchmark 1525 failed (commit https://github.com/google/benchmark/commit/040a1eec87 by @EricWF). :white_check_mark: Build benchmark 1530 completed (commit https://github.com/google/benchmark/commit/1e959e4d75 by @LebedevRI). :white_check_mark: Build benchmark 1531 completed (commit https://github.com/google/benchmark/commit/dcb7aea9f9 by @LebedevRI). :white_check_mark: Build benchmark 1532 completed (commit https://github.com/google/benchmark/commit/823fb4aa64 by @LebedevRI). :x: Build benchmark 1533 failed (commit https://github.com/google/benchmark/commit/deeaf6e032 by @LebedevRI). :white_check_mark: Build benchmark 1539 completed (commit https://github.com/google/benchmark/commit/9fc98ddc2d by @LebedevRI). :x: Build benchmark 1544 failed (commit https://github.com/google/benchmark/commit/c28ce986d2 by @LebedevRI). :x: Build benchmark 1545 failed (commit https://github.com/google/benchmark/commit/1abddb0d2b by @LebedevRI). :white_check_mark: Build benchmark 1546 completed (commit https://github.com/google/benchmark/commit/b76ecbe384 by @LebedevRI). :white_check_mark: Build benchmark 1547 completed (commit https://github.com/google/benchmark/commit/5076aad0b1 by @LebedevRI). :white_check_mark: Build benchmark 1564 completed (commit https://github.com/google/benchmark/commit/1d7d02ad14 by @LebedevRI). :white_check_mark: Build benchmark 1540 completed (commit https://github.com/google/benchmark/commit/da78736ffa by @LebedevRI). :white_check_mark: Build benchmark 1548 completed (commit https://github.com/google/benchmark/commit/32bb01e3da by @LebedevRI). :white_check_mark: Build benchmark 1541 completed (commit https://github.com/google/benchmark/commit/5d44fa5500 by @gladk). :white_check_mark: Build benchmark 1542 completed (commit https://github.com/google/benchmark/commit/1e3ccc8991 by @gladk). :white_check_mark: Build benchmark 1549 completed (commit https://github.com/google/benchmark/commit/7e7cca4e03 by @kbobyrev). :white_check_mark: Build benchmark 1551 completed (commit https://github.com/google/benchmark/commit/72f4f798a1 by @amallia). :white_check_mark: Build benchmark 1552 completed (commit https://github.com/google/benchmark/commit/5e2e0bf53c by @amallia). :white_check_mark: Build benchmark 1560 completed (commit https://github.com/google/benchmark/commit/ea7dfb8007 by @dominichamon). :white_check_mark: Build benchmark 1562 completed (commit https://github.com/google/benchmark/commit/f7b32cb7df by @dominichamon). :white_check_mark: Build benchmark 1567 completed (commit https://github.com/google/benchmark/commit/de980c4f19 by @traceon). :white_check_mark: Build benchmark 1569 completed (commit https://github.com/google/benchmark/commit/cbf1e47d9c by @). :white_check_mark: Build benchmark 1570 completed (commit https://github.com/google/benchmark/commit/ea1180937f by @). :white_check_mark: Build benchmark 1571 completed (commit https://github.com/google/benchmark/commit/7ccf229f3a by @). :x: Build benchmark 1573 failed (commit https://github.com/google/benchmark/commit/d342a80563 by @LebedevRI). :x: Build benchmark 1574 failed (commit https://github.com/google/benchmark/commit/c9cd2e6927 by @LebedevRI). :x: Build benchmark 1576 failed (commit https://github.com/google/benchmark/commit/ed94bded4a by @LebedevRI). :x: Build benchmark 1578 failed (commit https://github.com/google/benchmark/commit/b12d0c87e3 by @LebedevRI). :white_check_mark: Build benchmark 1579 completed (commit https://github.com/google/benchmark/commit/67c53f8cf4 by @LebedevRI). :white_check_mark: Build benchmark 1580 completed (commit https://github.com/google/benchmark/commit/376b12b007 by @LebedevRI). :white_check_mark: Build benchmark 1575 completed (commit https://github.com/google/benchmark/commit/29b778268a by @Croydon). :x: Build benchmark 1584 failed (commit https://github.com/google/benchmark/commit/f70f5b39af by @jatinx). :x: Build benchmark 1585 failed (commit https://github.com/google/benchmark/commit/7f7bb81561 by @jatinx). :white_check_mark: Build benchmark 1586 completed (commit https://github.com/google/benchmark/commit/7e957a81be by @jatinx). :white_check_mark: Build benchmark 1587 completed (commit https://github.com/google/benchmark/commit/ffd0fa611f by @jatinx). :white_check_mark: Build benchmark 1589 completed (commit https://github.com/google/benchmark/commit/7fda14a749 by @jatinx). :white_check_mark: Build benchmark 1598 completed (commit https://github.com/google/benchmark/commit/5c4b5786b0 by @). :white_check_mark: Build benchmark 1600 completed (commit https://github.com/google/benchmark/commit/58dde37f5d by @jatinx). :white_check_mark: Build benchmark 1603 completed (commit https://github.com/google/benchmark/commit/f61aac7286 by @jatinx). :white_check_mark: Build benchmark 1604 completed (commit https://github.com/google/benchmark/commit/e16277d78a by @jatinx). :white_check_mark: Build benchmark 1588 completed (commit https://github.com/google/benchmark/commit/39a7046160 by @EricWF). :white_check_mark: Build benchmark 1591 completed (commit https://github.com/google/benchmark/commit/572655c4a0 by @JBakamovic). :white_check_mark: Build benchmark 1592 completed (commit https://github.com/google/benchmark/commit/c900dacbcb by @JBakamovic). :white_check_mark: Build benchmark 1594 completed (commit https://github.com/google/benchmark/commit/8c8b9aba9b by @JBakamovic). :white_check_mark: Build benchmark 1596 completed (commit https://github.com/google/benchmark/commit/d333842b05 by @JBakamovic). :white_check_mark: Build benchmark 1597 completed (commit https://github.com/google/benchmark/commit/8ca8b8e703 by @Sigill). ",
    "myd7349": "OK. :)\n. @EricWF Got it. Thanks! :-)\n. ",
    "umireon": "Thank you @dominichamon , I will make a PR and CLA Agreement according to CONTRIBUTORS.md soon.\n. Here is a PR #130 , which fixes this issue.\n. Thank you for merging #130!\n. THANKS!\n. ",
    "dulacp": "Thanks @googlebot :)\n. ",
    "jll63": "Quite a few days have elapsed since then and here at Bloomberg we would like to package it for internal use. Our packaging process requires a tagged revision though, and the last tag dates back to March 2015. If not ready for 1.0, can you make an intermediate release please?\n. My company (Bloomberg LP) has signed the corporate contribution agreement.\n. What do we do with this?\n. I removed the overload and Travis is all green now.\nAs for the CCLA I was told on 8/12 that we were \"still waiting for Google to get their CCLA process fixed\" then on 8/28 that \"[my] request has been approved\". Let me know if I need to do something on my side.\n. > Your email must be a member of bloomberg-contributors@googlegroups.com\nWhat is that, a group on groups.google.com? But I can't find it...\n\n, and the commit in the PR must be associated with that exact email.\nI was told by the open source manager that I could use my personal repo,\nwhich I did. However the author field in the commits do contain my\nBloomberg address.\n. Just did it. And yay, it says \"You\u2019ve been added to the bloomberg organization!\".\n. What's up with the cla check? We solved this last time - I added my email at bloomberg.net to my github account and it solved the problem. I checked and it is still there...\n. I signed it!\n\nMore precisely, my company (Bloomberg) signed the Company CLA.\nOn Mon, Jan 25, 2016 at 11:49 AM googlebot notifications@github.com wrote:\n\nThanks for your pull request. It looks like this may be your first\ncontribution to a Google open source project. Before we can look at your\npull request, you'll need to sign a Contributor License Agreement (CLA).\n[image: :memo:] Please visit https://cla.developers.google.com/\nhttps://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll\nverify. Thanks.\n\nIf you've already signed a CLA, it's possible we don't have your\n  GitHub username or you're using a different email address. Check your\n  existing CLA data https://cla.developers.google.com/clas and verify\n  that your email is set on your git commits\n  https://help.github.com/articles/setting-your-email-in-git/.\nIf you signed the CLA as a corporation, please let us know the\n  company's name.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/pull/172#issuecomment-174578078.\n. Yes my BB address is still there. Tickling the bot...\n. uuuh I think so...I didn't change a thing to my GH account since PR 137. How do you check that gropup thing again?\n. I vaguely remember that last time we came to the conclusion that\nbloomberg-contributors@googlegroups.com has nothing to do with Google\nGroups, in spite of appearance. I searched my mailbox and I did not find\nany trace of what that is. My PR was eventually merged nonetheless..\n\nI am a member of the Bloomberg organisation on GitHub though. FWIW...\nOn Fri, Feb 26, 2016 at 12:39 PM Dominic Hamon notifications@github.com\nwrote:\n\ni can't check... i think you go to groups.google.com and look for it. if\nyou're in it, you can see it.. maybe? i really don't know.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/google/benchmark/pull/172#issuecomment-189385963.\n. `@EricWF Yes I would to see this merged. You can take over. As explained in the very first comment,  the main point here is to honor CMAKE_INSTALL_LIBDIR. I think this is uncontroversial. Secondarily,what happens if CMAKE_INSTALL_LIBDIR is not set? Here we have a choice: default to /usr/local/lib, or lib64. I don't have a strong preference on this, so what about merging only the first commit?. Probably me.\n\nWhat should I do? Amend my commits according to your remarks, force push to\nmy repo and submit a new PR? Or create one (or several) corrective commits?\n. or * 1e-7 to be really consistent (above says 1e-6).\n. 10000000L (and the entire expression) is integer, 1e7 is a double. What about 10L * 1000 * 1000 ?\n. Since you don't express a firm opinion and mine is that swapping the conditionals will make the code exactly as unclear ;) I will leave it as it is and in your hands ;)\n. Well I think that it's a choice between imperative and imperative imperative ;) Me prefer as least imperative as possible but you're the owner ;)\n. ",
    "mathieuk": "So, what would be the best way to move this forward?\n. ",
    "theopolis": "Hi @EricWF, is there a problem with the CLA signing? It would be great to have this merged into benchmark's master ASAP!\n. ",
    "eliben": "@googlebot - this is certainly strange as I'm part of the https://github.com/google organization (Googler!) and contribute to other Google projects on Github (for example https://github.com/google/yapf)\n. @dominichamon ah, this is what happens when I edit files from inside Github. This looks like something @goolebot should handle, but oh well.\nSeems like I'd have to rewrite all history to change author - so I'll just create a new PR - there's another typo needs fixing there anyways.\n. See #139 \n. .... aaand @googlebot doesn't like my primary email account either :-/ I tried just submitting the CLA anew. Hope this helps.\n. Added.\nNot sure about the Clang build failure on Travis. It complains about a libcstd++ header and was reported on the previous version of the PR which only had a comment change, so it looks suspicious.\n. ",
    "ulvgard": "Using find_package which sets GIT_EXECUTABLE. If Git is found then proceed to check the tags, otherwise define version to v0.0.0 as the scripts do today. I tried this today and made it work.\nhttps://cmake.org/cmake/help/v2.8.12/cmake.html#command:find_package\nhttps://cmake.org/cmake/help/v2.8.12/cmake.html#module:FindGit\n. I've initiated the process of signing the CLA irrespective of whether we ditch the Git-version check feature or not. \n. Dominic said:\n\n\"... we have max_iters in State as a size_t, but maybe it should be uint64_t to support high iteration counts on 32-bit machines. consistency is great, and i usually prefer to use size_t for sizes because, y'know, it's in the name.\"\n\nI agree, but most of all I wish this could be implemented for items_processed, and bytes_processed as these are used generally. I do realize that this is outside this discussion but I wanted to bring attention to this issue also as size_t is frequently insufficient for me.\n. Separate fix for bytes and items then?\n. I signed it! \n(Corporation DiracResearch)\n. The author email is an address associated with our googlegroup which I believe should be sufficient?\nMaybee, the CLA must be approved by Google first.\n. The CLA was just signed and filed by Google\n. I would also like to remove the CMake message that prints GIT_VERSION?\n. I signed it!\n(Corporation DiracResearch)\n. Yep, somehow the email was not related to the googlegroup. Even though I remember registering it. Anyway... Thanks Dominic\n. googlebot please try again\n. Are any actions required by me to move forward?. > Looks like CLA bot is still unhappy?\nIs it because the initial commit was made with another Author and Email? I squshed that and rewrote the auther to my CLA signed user.. > https://github.com/google/benchmark/commit/5d4ad0a277e1761463a227cf50d7748fcf5f1ef4.patch\n\nFrom: Tobias Ulvgard <tobias@ulvgard.se>\n^ i guess it is checking this email. Is that the one with signed CLA?\n\nIt is yes.. Is is possible to merge this? From what I understand googlebot is confused because the initial commit was made by me but with another author signature. . ping @LebedevRI ?. @LebedevRI thanks for the feedback, I appreciate someone taking a second look at this since I arrived at a different final expression.\nIndeed they are not equivalent. And in my comment I should not have stated that the factors can be \"cancelled\". The contribution suggests to compute the coefficient using the approach detailed in the comments. I cant find a motivation why the sum of the square of the execution time and model, with sigma_time_gn and  sigma_gn_squared is used for computing the coefficient of growth?\nThe minimum least-square part happens in the computation of the RMS as I interpret this function?. Strange. I re-performed the calculation again an landed in the same result that the coefficient is estimated by the proportion between sum of measurement and model. Good to see a source stating the result with squares. It is unfortunate to see that they omit the full derivation.\nHaving thought about it some more, I would actually claim that these methods are equivalent. The coefficient in the current case is computed as the proportion between the sum of the area of rectangles (time[i] * gn_i) and the sum of the area of squares (gn_i * gn_i). This is the same as computing the proportion between the sum of  lines (time[i] or length of one side of the rectangle) and the sum of other lines (gn_i or the length of the side of the square). This is not generally true of course, but for real positive values it holds. It is also possible to arrive at this conclusion by formulating the least-squre equation and solve for the coefficient. \nIn any case, I am happy if we pursue this PR or update the reference to the calculation with a link to your Wikipedia-article or even use the link to the old author's new repository? Also a test-case for the computation of the coefficient could be nice if it is interesting?. Thanks, your comments help me revisit the calculation and get it right. Also, I agree it is should have been obvious that the difference is not a norm. Updated the PR with the Wikipedia article.. ",
    "divayprakash": "@EricWF I'd like to take this up. Could you give a little more info on how to go about it?\n. So just swap out int for all occurrences of int64_t? \n. ",
    "anton-dirac": "I signed it! \n(Corporation DiracResearch)\n. I signed it! \n(Corporation DiracResearch)\n. Yes, with W4 it does... (warning C4127: conditional expression is constant)\nAn alternative would be to disable this warning in the CMake file.\n. <stringstream>  works, for example:\n``` cpp\nint main(int argc, char* argv[]) {\n  benchmark::Initialize(&argc, argv);\nTestReporter test_reporter;\n  benchmark::RunSpecifiedBenchmarks(&test_reporter);\nif (argc == 2) {\n      // Make sure we ran all of the tests\n      std::stringstream ss(argv[1]);\n      size_t expected;\n      ss >> expected;\n  const size_t count = test_reporter.GetCount();\n  if (count != expected) {\n      std::cerr << \"ERROR: Expected \" << expected << \" tests to be ran but only \"\n          << count << \" completed\" << std::endl;\n      return -1;\n  }\n\n}\n  return 0;\n}\n```\n. OK\n. Sure thing.\nhttps://xkcd.com/1179/\n. I added stdio.h because it's vsnprintf that is used, not std::vsnprintf.\nShould it still be cstdio?\n. ",
    "guanqun": "The current code doesn't check the overflow sadly... \nhttps://github.com/google/benchmark/blob/master/src/sleep.cc#L47\n. @dominichamon give me two days and I'll try to fix this. Otherwise it's all yours. :D\n. Actually it's an error:\n/sandbox/benchmark/src/sysinfo.cc(128): error #2259: non-pointer conversion from \"int64_t={long}\" to \"double\" may lose significant bits\n        cpuinfo_cycles_per_second = EstimateCyclesPerSecond();\n                                  ^\n. milliseconds is already an integer, it doesn't need to be casted to integer again. Instead, kNumMicrosPerMilli is a long, so after multiplication, it needs to be casted into int.\n. ",
    "philippefoubert": "Same problem with the -Wthread-safety option which is not recognized by g++.\nA way to avoid this kind of problem could be to modify the add_cxx_compiler_flag function to test whether a compiler option is supported before adding it (a bit like it is done in cxx_feature_check).\nThis could be done the same way it is done in  OpenCV with the macro ocv_check_compiler_flag (more details in https://github.com/Itseez/opencv/blob/master/cmake/OpenCVUtils.cmake).\n. ",
    "facontidavide": "my fault. -lpthread fixed the problem.\n. ",
    "ryobg": "With pleasure, but I do not have any experience with such activity. Should I use GIT to make a patch file and upload it here?\n. Was not easy, but I think I have made it. Thank you for the reference.\n.  I signed it!\n. ",
    "dimztimz": "When we measure memory bandwidth, we should generally measure how much data flows through the wires so we can compare to the maximum theoretical bandwidth of the hardware.\nThe way I see it, if one uses asymptotic function (big O) for the work done, one should use SetItemsProcessed. But for memory, we should be precise and count both the loads and the stores.\n. ",
    "alycm": "I signed it!\n. I signed it!\nHopefully it works this time (looks like my previous comment was before the CLA actually registered).\n. ",
    "didenko": "(reposting under the  proper account, sorry for the confusion)\nGiven that the #183 PR was dropped, any ideas on how to deal with the Pause/Resume issues? I think there are couple more illustrations of the issue at the discussion group: https://groups.google.com/forum/#!topic/benchmark-discuss/o5BvqFZIs3Y\n. @EricWF so the idea is to that after the first call to state.KeepRunning() the timer is ON and there is no per-iteration grunt work advised, because it will be included in the time measurement? E.g. in this:\nhttps://github.com/google/benchmark/commit/53068f9#diff-bbda13de265011c415b5604c6db76408R50\nThe rand() % size calc cannot be moved to the top of the iteration outside of the measurement?\n. @EricWF Ok, we are on the same page, including the \"not always...\" part.\nThank you!\n. ",
    "buchgr": "@dominichamon PTAL\n. ",
    "lkleen": "Set the CPU affinity of the main thread to a single core.. ",
    "amin-jabri": "I signed it!\n. Hi! You could  use your own cmake find module. A basic starter:\n``` cmake\nFindbenchmark.cmake\n- Try to find benchmark\n\nThe following variables are optionally searched for defaults\nbenchmark_ROOT_DIR:  Base directory where all benchmark components are found\n\nOnce done this will define\nbenchmark_FOUND - System has benchmark\nbenchmark_INCLUDE_DIRS - The benchmark include directories\nbenchmark_LIBRARIES - The libraries needed to use benchmark\nset(benchmark_ROOT_DIR \"\" CACHE PATH \"Folder containing benchmark\")\nfind_path(benchmark_INCLUDE_DIR \"benchmark/benchmark.h\"\n  PATHS ${benchmark_ROOT_DIR}\n  PATH_SUFFIXES include\n  NO_DEFAULT_PATH)\nfind_path(benchmark_INCLUDE_DIR \"benchmark/benchmark.h\")\nfind_library(benchmark_LIBRARY NAMES \"benchmark\"\n  PATHS ${benchmark_ROOT_DIR}\n  PATH_SUFFIXES lib lib64\n  NO_DEFAULT_PATH)\nfind_library(benchmark_LIBRARY NAMES \"benchmark\")\ninclude(FindPackageHandleStandardArgs)\nhandle the QUIETLY and REQUIRED arguments and set benchmark_FOUND to TRUE\nif all listed variables are TRUE\nfind_package_handle_standard_args(benchmark FOUND_VAR benchmark_FOUND\n  REQUIRED_VARS benchmark_LIBRARY\n  benchmark_INCLUDE_DIR)\nif(benchmark_FOUND)\n  set(benchmark_LIBRARIES ${benchmark_LIBRARY})\n  set(benchmark_INCLUDE_DIRS ${benchmark_INCLUDE_DIR})\nendif()\nmark_as_advanced(benchmark_INCLUDE_DIR benchmark_LIBRARY)\n```\n. ",
    "insertinterestingnamehere": "CLA signed.\n. I'm not sure what's going on with the coveralls build. The Travis build looks good though.\n. ",
    "AustinDeric": "I would also like to see this repo work with cmake's find_package function:\nhttps://cmake.org/cmake/help/v3.0/command/find_package.html\nThis way we can reference this package in other cmake packages.  If this is something y'all are interested in, i can submit a PR for it. I can do something similar to this:\nhttps://cmake.org/Wiki/CMake/Tutorials/How_to_create_a_ProjectConfig.cmake_file\nThanks and great work!. This is our initial implementation of the find_package() feature for google/benchmark:\nhttps://github.com/AustinDeric/benchmark/tree/feature/cmake-find\ncurrently we can successfully perform find_package(benchmark REQUIRED) in our ubuntu cmake project.\nit needs:\n- implementation version so we can find_package(benchmark 1.1.0 REQUIRED)\n- more testing!\nFYI, this is the first step so that the ROS community can use this wonderfull tool: http://www.ros.org/\n*Google's ceres solver is ROS compatible: https://github.com/ceres-solver/ceres-solver. ",
    "bryan-lunt": "@amin-jabri Can you add a license header to the google benchmark finder that you wrote?\nThanks.. I signed it! (Signed CLA.). @dominichamon \nI created a project that demonstrates the need for this patch:\nhttps://github.com/bryan-lunt-supercomputing/gbench_threads_patch_need_demo\nThe reason you see nothing different is that you are benchmarking things that are threaded by Google Benchmark itself. But google Benchmark does not give correct results when you benchmark code that spins up and shuts down its own threads, for example, any OpenMP codes.\n. If you tried to use the Current GoogleBenchmark built-in threads to benchmark an OpenMP code, you'd get something much worse than just nonsense CPU times, you'd get that each GB thread calls the function which internally starts up its own threadpool.\nSo you'd have N_OMP_THREADS*N_GB_THREADS threads at once.\nI've been using my own parameter \"num_threads\" but if wishes were fishes, it would be nice to be able to ask the total number of threads from within a benchmark, and to do something like \"set_user_managed_threads\" which results in only one GoogleBenchmark thread, but then inside we can ask the number of threads that were requested.\nThat way, we could have one and only one \"threads\" parameter, which I think would make interpreting the final output easier.. > No. Nothing like that should be done.\n\nIf user code uses it's own threads, there is no sane way to generalize that and display\nin report. There is simply can be no guarantee of anything, unless you want to put arbitrary\nrestrictions on the user threading. E.g. part of the benchmark function may be limited 4 threads,\nand then second part is not limited by the thread count.\n\nGood point.. Now I'm confused, does UseRealTime() get wallclock time into the \"Time\" column, or not?\nIn general, for parallel codes (by which I mean user-managed parallelism within the one function, as in my example) we are primarily interested in the wallclock time and secondarily interested in the CPU time.. It seems like there is a choice and a choice of reduction here.\n\nChoice: Does an individual GB thread get its own CPU time, or the CPU time of itself and all subthreads? (Or something else?)\nReduction choice: When putting together the final report, does the CPU time show the smallest, largest, mean, or sum of all the individual GB thread CPU values?. So who has to sign off on the CLA now?. \n",
    "LepelTsmok": "I thought of something simpler, like in my Result.zip. It's a single file embedding the data, javascript, and css. So there is no need for an HTTPServer.\n. Another possible chart could be like this Result.zip. The x-axis displays the 'range_x' value, which was passed to the benchmark. Sadly, there is no good way to get these value.\nI can think of three possible ways:\n1. You could change the structure Run to contain these values\n2. You could extract the value from the name, but at some point the value changes to a human readable string instead of a number (e.g. 64k)\n3. You could set the value via the label and extract it form there - this is what I did\nWhat solution would you prefer?\n. I used the json output as a basis for a new HTML reporter. But I wanted to generate the HTML output directly, without an additional program. \nI wrote an HTML reporter, which generated the examples I showed you. When the reporter runs, the range_x variable isn't available any more. So actually I don't have full access to the raw data.\n. In my branch I changed the structure Run to contain the arg1 and arg2 values, thanks for that.\nWhen I'm plotting a line chart, the benchmark name contains a different name per point. The reason is the x-value, which is encoded into the name.\nI noticed that the structure Benchmark::Instance contains the values, which are put into the name of the benchmark. With these values the HTML-reporter could generate the name (i.e. without the x-value). \nI see three options:\n1. Process the name string to remove the x-value (fragile).\n2. Store all individual components of the name as members of Run. Keep the full name around (redundant).\n3. Store all individual components of the name as members of Run. Generate the full name in each reporter (large change)\n. I just red the CLA and noticed, that the .js files are not my work. Can those be part of the pull request at all? If not, I would have to implement a download mechanism.\n. I signed it!\n. All fixes are done with this commit. I hope everything is okay.\n. Could you please restart Travis? It seems to fail in the STL.\n. Is everything okay now?\n. Now it is better! So I hope, that it is okay now.\n. The 'IsZero' function is now in benchmark_util.cc.\nI hope this is okay.\n. Line 14 makes the download optional. The default is off, which should be what you want.\n. ",
    "WilliamTambellini": "Any plan to reabse and merge this nice feature ?. up ?. Hi, Any plan to rebase and merge ?. Hi @mattkretz I cannot help on that. \nAre we sure that it would be even accepted/merged if someone rebase/finish the PR ?. Hi all, I m also interested by such feature specially for GPU computation with ArrayFire.\n. Hi Dominic\nThanks. Indeed I was thinking about enhancing the \"compare_bench.py\" but was then remembering the drawback to deal with python 2 vs python 3 thing and the dependency upon python globally. \nAnyway fair/good enough. \nSo would you prefer me to : \n1- enhance compare_bench.py inorder to accept a new CLI option (\"--check_threshold\" or ....) to do the threshold check comparison and return 0 or 1 if any timing in input1 is higher than timings in input2 (per bench of course) ?\nor\n2- create a new python script, as instance \"check_benchs.py\", or whatever that would basically do very similar to compare_bench.py but a little more ?\n  . ",
    "LebedevRI": "With CSV reporter being deprecated, i'm not quite certain in the future of this PR.. (I mean, it can totally be, if it is like the tools/, and simply consumes the JSON.). >If you specify 'repetitions' we will calculate the mean and std dev across each repetition. Ie, we take the average per-iteration time, and then find the mean/std dev of those averages.\nFor this use-case, my pr will fix the issue.\n\nit sounds like you're suggesting we should have the std dev, max, and min, of the iterations over a single run.\n\nBut not for this one...\nI too think it may be nice to have, but wouldn't it be a little bit memory-prohibitive to keep all the measurements for each iteration? There can be a lot of iterations, and we'd need 8 bytes (sizeof(double)) for each one.... Question: is there any status here?\nI would like to have a median in addition to the currently-available mean.\nIf i just add it, would that be upstreamable, or is the full rework like suggested here is the only way to go?. Edit: i didn't know Stat1 was optimized to not keep the values themselves...\nI did implement median, but ATM at the cost of adding std::vector<VType> values_;\nIt is probably too intrusive?. Ok, so if i refactor Stat1 to store std::vector<VType>, that would be fine? (as in upstreamable). @baryluk i think this is resolved, now that we have custom user counters?\nThe nicer support for custom units may be nice to have, but not too impactful.. I think this can be closed, https://github.com/google/benchmark#user-defined-statistics-for-repeated-benchmarks\nBENCHMARK(BM_spin_empty)\n  ->ComputeStatistics(\"min\", [](const std::vector<double>& v) -> double {\n    return *(std::min_element(std::begin(v), std::end(v)));\n  })\n  ->Arg(512);\nshould do it.\nTo be noted, it won't be the true minimal time of of the while(keeprunning) loop iterations, but the min time of the repetitions.. To be noted, there isn't any way (that i'm aware of) right now to actually get the actual time of one iteration. It will always be somehow averaged, because the timer restart is not free, so it tries to minimize them.... I'm not sure it is related to #291, i'm somewhat sure i'm using release 1.1.0, which does not have that yet.. I think this should also contain the unittest, so that it does not resurface.. Better yet, why not use GNUInstallDirs cmake module?. Thank you.. Those builds seem to be flaky, and if you look into history, i'd say it was like that even before this PR... FWIW the build for this PR succeeded, as you can see in https://github.com/google/benchmark/pull/426#issuecomment-317555267. Thank you!\n@dominichamon  Is there any chance i could get some feedback on https://github.com/google/benchmark/issues/324#issuecomment-317276303 ?\nI'd like to also have median output in addition to mean, and that is impossible since Stat1 does not store the actual values. I suppose if it is extended to store the values, then the interface could be extended to also support any user-provided metric over all the measurements.. (Do not merge yet, still want to at least attempt to look into test coverage...). I did manually check several stdev results, and they appear to match.\nI'm not sure what else to do here, any review comments?\n. Just want to note that i did fix most of the review notes, except a few, and i'm not sure about those...\nI'd love to have some feedback, please :). Friendly ping :). Friendly weekly ping :). Maybe the last ping, pretty please? ^^\nI would gladly change the code, i just need to know what is the vision/what to change.... Thank you very much!. Can you please not push every one-line change as a new commit?\nReally spams notifications.. As requested, ping :). Oh, it must then be \nset(CMAKE_CXX_FLAGS_COVERAGE \"${CMAKE_CXX_FLAGS_DEBUG}\" CACHE STRING\n    \"Flags used by the C++ compiler during coverage builds.\"\n    FORCE)\nThat is so bad :/. Alright, i think it is better now.\nIt builds, ci's are happy, and appears to fix the Problem i was hitting.\nPR description updated :). > Yes, it does. It's called ExternalProject. \nYes...\n\nYou should be using that, not configuring it directly as a subdirectory.\n\nYou are wrong.\nIf i use ExternalProject only, then the entire library is built as part of the cmake ../ execution. This is rather suboptimal. But if i first use ExternalProject to prepare cmake, and then do add_subdirectory(), then it is built as part of my buildsystem.\nIt just works for googletest, and is the recommended approach: https://github.com/google/googletest/tree/master/googletest#incorporating-into-an-existing-cmake-project\nThis is done by making the GoogleTest source code available to the main build and adding it using CMake's add_subdirectory() command.\nIt just worked for googlebenchmark until the last release.\nThis is a bug.. @dominichamon thank you!. @dominichamon thank you!. Well, all i can say right away is that it certainly wasn't intentional :)\nLet's see... @dorafmon are you absolutely sure that bundled stuff in test_include and test_lib is not outdated/out-of-sync?. Since you are already tracking googletest/googlebench as submodules, you probably might want to purge test_include and test_lib, and let cmake handle that... @dominichamon did i forgot to bump some API/ABI version that would have prevented this?. > i can only imagine it's some quirk of osx cmake that is causing a feature\nto be mislabeled, or maybe something specific about Bob Fang's\nconfiguration?\nNot first time i'm encountering such an issue. If a library is built for API/ABI version n, while the headers are for API/ABI version m (n != m), the behavior is unspecified.\n(though the last time was with a released version of some other library, and in that case reading from one field was only resulting in a wrong value). While i totally understand the reasoning - less duplication, avoidance of addition of new reporters, less code - i'm not sure i like the idea of having to use tools just to get the partially readable tabular report...\n\nOne option is to keep the tabular and json reporters in the library, remove the CSV reporter, and then have tooling to support the \n\nSo I'd personally prefer to consider this middle-ground. I suspect this should be closed.\nWithout having at least the source sample, not much to be done... I'm reasonably sure that it also was a version mismatch just like in https://github.com/google/benchmark/issues/751\nRegardless, without repro steps, this is unactionable.. To answer a few more yet-unasked questions:\n Yes, i should look into adding support for the second optional benchmark into the tool, so that there are three possible usage scenarios:\n   * One benchmark, two (different!) filters - compare two families of one benchmark (current)\n   * Two (different!) benchmarks\n      * two (different!) filters -  compare two families of two benchmarks\n      * one filter -  compare one family of two benchmarks\n No, sadly i do not think it can be deduplicated into the original tool, compare_bench.py, because that will most likely break the existing command-lines, muscle memory.\nWill need to add tests.. Added docs, too. Not sure what is left to do.. @dominichamon thank you!. > or additionally passing -fuse-ld=gold in CMAKE_EXE_LINKER_FLAGS\nWhy CMAKE_EXE_LINKER_FLAGS only?. You currently could use SetLabel(), though that is not really a proper solution.. @dominichamon I think this can be closed now?. @BaaMeow \nCan you attach the json produced by the following code please?\nc++\nstatic void NOP(benchmark::State& state) {\n  for (auto _ : state) {}\n}\nBENCHMARK(NOP);\nBENCHMARK(NOP)->Repetitions(3);\nBENCHMARK(NOP)->Range(8, 8<<10);\nBENCHMARK(NOP)->Range(8, 8<<10)->Repetitions(3);. Thank you for showing the output!\n\nDoes this answer your questions?\n\nYes, it does. You confirmed my concerns.\nThere are 4 different benchmarks there, but they all have one, exactly the same `basename``.\nUnless i misunderstand the basic idea of this diff, this is bad.\nI'd expect for basename to be equal for name with removed suffix of _mean, _median, _stdev so to speak.\nDo you disagree?. Thank you for replying!\n\nWith the base name you can still accurately determine things like extraneous modifiers (everything after NOP, now), which means you can -- with some effort -- still isolate which benchmark group each of those belongs to and their associated statistics (modifiers like range, threads, etc. all use / and : -- these characters don't show up in regular C function names).\nwith some effort\n\nLet me rephrase. Why can't we have the following:\nname,iterations,real_time,cpu_time,time_unit,bytes_per_second,items_per_second,label,error_occurred,error_message,base_name\n\"NOP\",20363636,32.0716,32.2266,ns,,,,,,\"NOP\"\n\"NOP/repeats:3\",26352941,27.8607,27.8669,ns,,,,,,\"NOP/repeats:3\"\n\"NOP/repeats:3\",26352941,26.3785,25.4953,ns,,,,,,\"NOP/repeats:3\"\n\"NOP/repeats:3\",26352941,27.601,27.8669,ns,,,,,,\"NOP/repeats:3\"\n\"NOP/repeats:3_mean\",26352941,27.28,27.0764,ns,,,,,,\"NOP/repeats:3\"\n\"NOP/repeats:3_median\",26352941,27.601,27.8669,ns,,,,,,\"NOP/repeats:3\"\n\"NOP/repeats:3_stddev\",26352941,0.791506,1.36927,ns,,,,,,\"NOP/repeats:3\"\n\"NOP/8\",26352941,26.7635,26.6811,ns,,,,,,\"NOP/8\"\n\"NOP/64\",26352941,28.6893,28.4598,ns,,,,,,\"NOP/64\"\n\"NOP/512\",26352941,26.8772,27.274,ns,,,,,,\"NOP/512\"\n\"NOP/4096\",26352941,25.9611,26.0882,ns,,,,,,\"NOP/4096\"\n\"NOP/8192\",26352941,26.6772,26.6811,ns,,,,,,\"NOP/8192\"\n\"NOP/8/repeats:3\",28000000,26.5917,26.7857,ns,,,,,,\"NOP/8/repeats:3\"\n\"NOP/8/repeats:3\",28000000,25.7512,25.6696,ns,,,,,,\"NOP/8/repeats:3\"\n\"NOP/8/repeats:3\",28000000,25.507,25.6696,ns,,,,,,\"NOP/8/repeats:3\"\n\"NOP/8/repeats:3_mean\",28000000,25.95,26.0417,ns,,,,,,\"NOP/8/repeats:3\"\n\"NOP/8/repeats:3_median\",28000000,25.7512,25.6696,ns,,,,,,\"NOP/8/repeats:3\"\n\"NOP/8/repeats:3_stddev\",28000000,0.569019,0.644364,ns,,,,,,\"NOP/8/repeats:3\"\n\"NOP/64/repeats:3\",28000000,27.2579,26.7857,ns,,,,,,\"NOP/64/repeats:\"\n\"NOP/64/repeats:3\",28000000,28.7635,29.0179,ns,,,,,,\"NOP/64/repeats:\"\n\"NOP/64/repeats:3\",28000000,27.7135,27.9018,ns,,,,,,\"NOP/64/repeats:\"\n\"NOP/64/repeats:3_mean\",28000000,27.9116,27.9018,ns,,,,,,\"NOP/64/repeats:\"\n\"NOP/64/repeats:3_median\",28000000,27.7135,27.9018,ns,,,,,,\"NOP/64/repeats:\"\n\"NOP/64/repeats:3_stddev\",28000000,0.772122,1.11607,ns,,,,,,\"NOP/64/repeats:\"\n\"NOP/512/repeats:3\",26352941,26.2543,26.0882,ns,,,,,,\"NOP/512/repeats:3\"\n\"NOP/512/repeats:3\",26352941,29.0715,29.0527,ns,,,,,,\"NOP/512/repeats:3\"\n\"NOP/512/repeats:3\",26352941,25.8172,26.0882,ns,,,,,,\"NOP/512/repeats:3\"\n\"NOP/512/repeats:3_mean\",26352941,27.0477,27.0764,ns,,,,,,\"NOP/512/repeats:3\"\n\"NOP/512/repeats:3_median\",26352941,26.2543,26.0882,ns,,,,,,\"NOP/512/repeats:3\"\n\"NOP/512/repeats:3_stddev\",26352941,1.76627,1.71159,ns,,,,,,\"NOP/512/repeats:3\"\n\"NOP/4096/repeats:3\",26352941,26.1249,26.0882,ns,,,,,,\"NOP/4096/repeats:3\"\n\"NOP/4096/repeats:3\",26352941,26.5308,26.6811,ns,,,,,,\"NOP/4096/repeats:3\"\n\"NOP/4096/repeats:3\",26352941,26.593,26.6811,ns,,,,,,\"NOP/4096/repeats:3\"\n\"NOP/4096/repeats:3_mean\",26352941,26.4162,26.4834,ns,,,,,,\"NOP/4096/repeats:3\"\n\"NOP/4096/repeats:3_median\",26352941,26.5308,26.6811,ns,,,,,,\"NOP/4096/repeats:3\"\n\"NOP/4096/repeats:3_stddev\",26352941,0.254198,0.342318,ns,,,,,,\"NOP/4096/repeats:3\"\n\"NOP/8192/repeats:3\",26352941,26.2092,26.0882,ns,,,,,,\"NOP/8192/repeats:3\"\n\"NOP/8192/repeats:3\",26352941,26.4208,26.0882,ns,,,,,,\"NOP/8192/repeats:3\"\n\"NOP/8192/repeats:3\",26352941,25.7008,26.0882,ns,,,,,,\"NOP/8192/repeats:3\"\n\"NOP/8192/repeats:3_mean\",26352941,26.1103,26.0882,ns,,,,,,\"NOP/8192/repeats:3\"\n\"NOP/8192/repeats:3_median\",26352941,26.2092,26.0882,ns,,,,,,\"NOP/8192/repeats:3\"\n\"NOP/8192/repeats:3_stddev\",26352941,0.370045,0,ns,,,,,,\"NOP/8192/repeats:3\"\nThis way, there are more than one distinct benchmark groups, and no effort required to distinguish between them.\nEDIT: I admit, i did not read through everything in this PR, and did not follow this PR from the beginning, so i may be missing some obvious precondition that mandates the current implementation with just one basename... > From my understanding of the code base, that's exceptionally more complicated and might break in small ways when someone forgets to accumulate the \"right\" options (which ones?) into the base_name.\nI'm pretty sure that's not how things work.\nHere, that full name is stored:\nhttps://github.com/google/benchmark/blob/c4858d8012acd54d8ef89053c74e9cdcf0fa6649/src/benchmark.cc#L123\nHere, the statistic's suffix is appended:\nhttps://github.com/google/benchmark/blob/50ffc781b1df6d582d6a453b9942cc8cb512db69/src/statistics.cc#L154\nSo i'm 99.9% sure you just need to change from\nhttps://github.com/BaaMeow/benchmark/blob/bf2f5a5f8618220f4a5e64583bb5564ddadb3931/src/benchmark.cc#L124\ndiff\n  report.benchmark_name = b.name;\n- report.base_name = b.base_name;\n+ report.base_name = b.name;\n(and drop b.base_name field, NOT report.base_name field)\nTry it, please :). > From what you presented and stated before: you have 4 benchmarks, and you want 4 base names to identify each of the 4.\nEeeh, i fooled myself there :)\nI was talking about the each group for which we print statistics.\nSo 18 unique strings is indeed the expected result.\n\nLook at the CSV you posted where you just cut off the statistics name: there's 18 unique strings for the base name.\n\nYep!. > @LebedevRI does that get us to where you can use the json for tooling?\nIn the case of the bundled tools, the current (git master) status is already sufficient.\nSo i don't need this.\nHaving more structured json would be nice, but it needs to be either a non-breaking change,\nor it needs to be really thought-through first, with json scheme written beforehand.. > To be clear, I'm not against what @LebedevRI wants or suggests\nI'm just not sure what problem exactly this pr is trying to solve.\nIf it is as written in https://github.com/google/benchmark/pull/567#issue-181073435:\n\nNow consider that someone runs this code with --benchmark_repetitions=2. We get the report for BM_StringCreation and BM_StringCreation_Append, giving us stddev, mean, and median for the benchmarks. The problem then becomes...\nHow do you associate a the name of the benchmark statistic with the benchmark it belongs to?\nNormally, this wouldn't be a problem... but in the presence of custom statistics allowed by ComputeStatistics, we can run into problems. For example, if a statistic called my_stat is added, we run into issues where a tool cannot reliably determine which stats belong to which benchmark:\n\nthen i really don't see why the current solution is the appropriate one.\nI'm biased, but i'd argue that what i'm suggesting matches the stated problem much more closely.. That sounds like a major json scheme break.\nI'm of no power here, but first i would really want to hear concrete\ndescription of the problem you are trying to solve.\nIf it is what you have described in the https://github.com/google/benchmark/pull/567#issue-181073435\nBut since i have asked that before, it seems we are going in circles here.. > Adding fields to JSON shouldn't break anything, and it should make tooling easier to have the name broken apart.\nYep, adding - absolutely.\nIt's just that in your comment https://github.com/google/benchmark/pull/567#issuecomment-387363969 the full re-design of json scheme is suggested,\nand while that will/should happen eventually, it is probably best to refrain to\njust addition of new fields, not removal of old fields in this PR.\nJust thought i'd mention it. Thank you for replying!\n\nI'm also agree\nthat the number of iterations in a statistic should likely follow the\nstatistic: the mean should be the mean across repetitions, the std dev\nshould be the std dev across repetitions, etc, to give more information\nabout what's going on with the benchmark.\n\nSo what is the \"TLDR\": we should use repetition count?\nIf not, please feel free to close the issue :). After looking a bit more into reporting separate iterations, i now believe that for aggregates,\nthe actual count of rows used for the computation of said aggregate should be displayed.\nBecause if you have one iteration per repetition, all the aggregates will say that they are over one iteration, but they are over n repetitions.\nSimilarly, this in-repetition averaging dramatically cripples all those measurements\n(up to the point that it is completely wrong to compute anything other than median on those \"results\"),\nso again, claiming that those aggregates are for I iterations is misleading. They are for N repetitions.\nI'll write a patch.. (wow, a lot of nop commits, maybe a rebase is needed?)\nIs there a bugreport or some textual description of the problem this is solving?. Thank you for taking a look!\n\nSo this means you're comparing the repetitions, which are already averages of iteration times. \n\nYep, obviously.\n\nIdle thought: should we have a mode that outputs every iteration into a JSON file? :D\n\nThis comes up every so often..\nThere are obvious downsides:\n1. the timer will need to be stopped each time. i don't think we currently stop it unless the code tells to.\n2. this will increase memory (and on-disk) footprint of the measurements\nThe last one is expected.\nBut i do expect that it may be useful sometimes.\nSo if we can somehow not penalize the default case (the current way), i would be interested in this.\n\nbut I wonder if you might want to require more than two repetitions to define a distribution.\n\nYes, clearly, but i'm also not sure what would be the sane choice.\nMaybe state it in help, and print a warning if repetition count is too small? (less than 5?)\n\nDo you have any reason to assume a priori that the distributions of benchmark repetition results are normally distributed?\n\nNo. Only factual (which means - probably invalid) evidence of t-test being used for this purpose.\n\nA more general approach is the Mann-Whitney U test. \n\nOk, great, will take a look!. Ping.. Is there any preferences re enabling this by default?\nI have finally got around to actually using all these my changes, and it took me a moment to realize why u test results aren't printed :). SGTM!. @dominichamon thank you!. I think it may be interesting to define which versions Android suffer from this issue.\nIs it the last one? Is it the first one? All of them?. Since as you have linked, the benchmark library pass-throughs the unknown params,\nyour code will receive them. So your code is fully free to do whatever it wants with them.\nSo i'm not quite sure what the issue is. This isn't specific to the benchmark library at all.. @dominichamon I think this can be closed?. Why so many formatting changes.. Are you using msvs, which always formats the entire files?. Documenting (and sticking with a) dependency versioning policy would be nice[r].\nPersonally i think trying to support 3-years old versions of the deps\n(+ debian stable + last two ubuntu lts) is the correct solution.\nThere can be exceptions of course.. cmake v3.5.1 was released Mar 24, 2016, which is 2 years 2 months. v3.2.3 is the newest that would fit into that sliding window.\nThat sliding window seems to be somewhat reasonable.\nI don't mean to halt forward progress, i'm just trying to hint that cost-benefit analysis,\nand ideally effect on (not just one) users is a good thing.. Let me summarize my views, after brief irc chat:\n1. Forward momentum is good.\n2. Sticking with old-er versions just because is not good.\n3. Whatever the policy is, it's best to document, follow it.\n4. It'd be great if the 3-year sliding window would be the [new] policy.\n    (more specifically, https://github.com/google/benchmark/issues/603#issuecomment-394858872)\n5. 2 years could work too.\n6. Any other more aggressive approach (no written rules, or not following them) is worse, but not my hill to fight on.. Documentation first :). LLVM bundles/builds this library as of https://reviews.llvm.org/D50894,\nbut LLVM has a required cmake version of 3.4.3:\nhttps://github.com/llvm-mirror/llvm/blob/0d21b888881a26b0b50065669163afbe0c9a5812/CMakeLists.txt#L3\nSo this won't be as simple going forward :/. (To be noted, i'm still onboard with https://github.com/google/benchmark/pull/613 in it's current form). PR tips:\n Fetch the remotes (the google/benchmark) repo\n Rebase your local git master branch on top of git master of google/benchmark repo\n Push that to your github repo clone\n Rebase your WIP branch ontop of your new git master\n* [force]Push that to your github repo clone\nElse there is a lot of unrelated noise in the PR.. > I think I managed to clean it up.\nThanks, this is indeed better.\nIt would be best if you could split the \\\\\\\\ change (+ tests!) into a separate pr,\nbecause it can be merged right away, separately from the rest of this.. Ok, that is great, some progress. Now, how about let's try to split this a bit more.\nI think it would be completely uncontroversial to introduce that stats field.\n@dominichamon, @EricWF ?\nCan you split it into yet another PR? :). I had another look, and i still stand with the same opinion :/\nI think this does the right thing, and does move in the right direction.\nBut. It implements loads of different (but related, yes) things at once:\n Store existing data in the JSON:\n  1. threads\n  2. repetitions\n  3. aggregate name (statistic, rms, bigo)\nThese three are great, and completely uncontroversial on their own. I'd merge them.\n Add newish data:\n  * ID's.\n  * Store more name variants.\nAgain, both of these most likely make sense.\nBut It's really hard to tell (due to the diff's size) if they do the sane thing or not.\nAlso, they really should have separate tests, that actually check the names/ids.\nSo i would still suggest to split this up further...\n. @dominichamon thank you!. There is also a slight correlation with #620 - i'd also like to have something like kIsTimer, but the more general fix #620 would avoid that.. I'm currently working on a patch.\nRight now i only have:\nc++\n    // Mark the counter as a constant value, valid/same for *every* iteration.\n    // When reporting, it will be multiplied by the iteration count.\n    kIsIterationInvariant = 1U << 2U,\n    // Mark the counter as a constant rate.\n    // When reporting, it will be multiplied by the iteration count\n    // and then divided by the duration of the benchmark.\n    kIsPerIterationRate = kIsRate | kIsIterationInvariant. > I think it's pointless trying to add an enum entry for each possible combination. So kAvgThreadsRate could also be deprecated.\n\nBut anyway, this is a problem because currently the Counter ctor takes a Flags argument. We can just ignore this and make the flags parameter to the ctor take an int. OTOH, if this interface is to be retained for compatibility reasons, it probably makes sense to make Flags a typedef int, and turn the enumeration into a different type.\n\nI'm not sure i follow.\n1. Just deprecating it won't hide the conbinatorial explosion.\n2. There is indeed no need to \"to add an enum entry for each possible combination\".\n    There is simply no need. Even if it would be an enum class.\n     One can perfectly fine pass some garbage casted to an enum, and it won't be an UB\n3. Similarly, i see absolutely no point in making it an int.. Aha.\nBut then i still think that is the wrong solution.\nhttps://godbolt.org/g/1qwa24. > I'd like to participate in this a little bit, albeit my needs might be a bit different and maybe not exactly aligned.\n\nMany of my company's measurements are tiny enough that I want to count things like cycles (rtdsc, for example) rather than nanoseconds,\n\nThat is basically the same thing.\nProper generalized \"custom 'times'\" should be oblivious to what they count, they are just infrastructure.\n\nand so I wanted the ability to have a custom TimeUnit alongside the ManualTime. Would the work here allow for something like that, or is this orthogonal?\n\nHmm yeah. Even the existing user counters lack the \"unit\" customizability.\nThe current ManualTime does imply that it counts time, so hardcoding of time units makes sense,\nbut indeed, that will be non-ideal for custom timers. I'm not sure how that would/will look though.\nI had some code for custom timers that almost worked,\nbut v2 branch with all it's json infra changes for some reason derailed that work.\nSince v2 is still there where it was back then, i guess maybe i should take another stab at it.\nI'm not sure what will it take to extend everything to support custom \"time\" units,\n(what needs changing? just the \"s\" suffix?), so i think that is orthogonal to this issue... To be fair, these aren't needed.\nAs it is seen from the commits i linked, it can be solved in user code.\nBut having these will make users think, whether maybe they need them?\n(In other words, this may prevent bugs.). @dominichamon thank you for the review!. Is it intentional that an external project (googletest) is being built with warnings?. It might be simpler to go the other way around, and simply rebase the v2 branch on top of git master.\n. Hmm, so should i submit a pr (to the master branch) to drop them?\nI do see v2 branch, but i wonder if that one should be called json-wip instead... Removed == api break == major release ==  2.0. Hmm, yes, i suppose it makes sense to update the docs right now, not when they are actually gone..\nAs for not getting rid of them, i'm not sure. I had a really hard time grasping them. It was very unclear what they were supposed to be doing. Having just one interface (user counters) seems best.. https://github.com/google/benchmark/pull/676. @dbabokin i hope that addressed it.. I'm honestly not sure how useful this is to be automated, some thoughts:\n\nThis has only one measurement, right before the benchmarking begins.\nNothing is stopping some other cpu-hungry process to start (or awake) during the benchmarking, and cause cpu contention.\nAnd adding a second load average report afterwards is of not much use, since it is rather expected that benchmarking would cause some load on the system.\nSimilarly, if you run two benchmarks back-to-back, the load average would not be very useful... That is only a problem when cmake/cmake-generated install files are not used for linking, right?. > https://github.com/google/benchmark/pull/638#issuecomment-405209029\n@aJetHorn Can you confirm that you're ok with this PR being merged? Thanks.\n\n\n\nThis looks good to me, but that ^ was never resolved. Do we wait, or should i merge?. I would suggest looking how googletest handles this issue.\nThey too have gtest_main.. I must say that my personal opinion is that this is kinda ugly.\nI wonder if it would be better to simply do\ncmake\nif(BUILD_SHARED_LIBS AND WIN32)\n  message(SEND_ERROR \"Building libbenchmark as shared library is not supported\")\nendif()\n. This should have been fixed by #652 (although the commit msg did not include Fixes #641 :D). @monamimani did that fix the issue you were seeing?\nIf yes, could you please close the issue.. Perhaps these ascii escape sequences are simply too advanced for that windows console?\nIs there an unicode alternative for them?\nIf not, simply disabling formatting for incompliant output devices may be the solution.. This diff seems rather messed up.\nHalf of the changes has already happened before (in #600).\nAnd i have to say, some of those changes are questionable.. Ping?. > use committer rights ;)\nUhm, i'm not sure this is how it works :)\nIt [normally] never is a pass to do whatever one wants.. Hmm wait, there is no 3rd party quality control of these packages?\nAnd there is just one 'official' prebuilt version of the library (via travis) that is just assumed to be good enough for everybody?. Can you append __builtin_unreachable() after the switch ?\n. ```cpp\nenum E {\n    X,\n    Y\n};\nint test(E e) {\n    switch(e) {\n        case X:\n            return 0;\n        case Y:\n            return 1;\n    }\n    __builtin_unreachable();\n}\nhttps://godbolt.org/g/GjXwwY. Won't work with msvc though IIRC, so you'll need to create\n`benchmark_unreachable()` wrapper function, which is either `__builtin_unreachable()` normally,\nor `__assume(0)` if the former is unavailable.. github really does not like such merges.\nThe diff not only contains the actual your changes,\nbut the changes that were merged from the master branch into this feature branch.. I think you created PR from wrong branch.. I think we can just close this.. (For context, that commit already exists in the master branch: 4fbfa2f3368cb8d8a0cba48edda584c7dd9f0a14). Ah right, need to properly update the docs, too.. Aha, and now i know why it is not _this_ simple :P\n`$ clang++ -fsanitize=address,undefined,integer test.cpp -Lbuild-Clang-SANITIZE/src/ -lbenchmark_main -lbenchmark -Iinclude`c++\ninclude \ninclude \nstatic void BM_memcpy(benchmark::State& state) {\n  char src = new char[state.range(0)];\n  char dst = new char[state.range(0)];\n  memset(src, 'x', state.range(0));\n  for (auto _ : state)\n    memcpy(dst, src, state.range(0));\n  state.SetBytesProcessed(int64_t(state.iterations()) *\n                          int64_t(state.range(0)));\n  state.counters[\"Bytes\"] = benchmark::Counter(\n                                    state.range(0),\n                                    benchmark::Counter::kIsIterationInvariant |\n                                    benchmark::Counter::kIsRate);\n  delete[] src;\n  delete[] dst;\n}\nBENCHMARK(BM_memcpy)->Arg(8)->Arg(64)->Arg(512)->Arg(1<<10)->Arg(8<<10);\n\n\nBenchmark               Time           CPU Iterations UserCounters...\nBM_memcpy/8            63 ns         63 ns   11066720 Bytes=127.185M/s   121.293MB/s\nBM_memcpy/64           70 ns         70 ns   10015445 Bytes=912.276M/s   870.014MB/s\nBM_memcpy/512         123 ns        123 ns    5682800 Bytes=4.1711G/s   3.88464GB/s\nBM_memcpy/1024        157 ns        157 ns    4791239 Bytes=6.50403G/s   6.05735GB/s\nBM_memcpy/8192        626 ns        626 ns    1101562 Bytes=13.0766G/s   12.1785GB/s\n``SetBytesProcessed(), not unexpectedly, uses1024multiplier, whileUserCountersuses1000.\nSo some more preparatory work is needed... Up for review :). Alrighty then, thank you for the review!. With which compiler? I think that was recently broken for some time in clang trunk.. Actually, that issue is from googletest sources, so _maybe_ you want to report it there.. And which googletest version?\nI don't seem to get that warning withgoogletest-1.8.0.\n(Or maybe it is not enabled, not sure.). To address the obvious question - yes, i suppose i could turn this into a bitfield, and squeeze 'iteration report mode' too. But i will also need to add '*display* aggregates only' intoAggregationReportMode..\nSo this would be more messy with bitfield. I think?. Ping.. Thank you for the review.. I'm not sure how that would look. Just takestd::pair` ?. Ping.. Thank you for the review!\nAwaiting CI completion... Ping.. > I'm a little fuzzy on the usefulness\nI hoped i explained it in the description..\nBasically, i want a --benchmark_report_aggregates_only={true|false},\nbut for the tools/compare.py, and this is the step in that direction.\n\nbut it also doesn't affect much, so LGTM.\n\nThank you for the review!\nCare to actually 'Review changes'  -> 'Approve'? Or textual form is sufficient?. It can already be done via tools/compare.py filters ./a.put BM_PushBackStd BM_PushBackCustom. > Am I missing something or can I not compare multiple benchmarks to a single baseline?\nIn a single go - correct.. I would say this should wait for 'proper' json support (https://github.com/google/benchmark/pull/499, v2 branch),\nand then be implemented ontop of that, in on caller's side.. Strange that i never see this.\nDoes it work if you manually place the googletest sources into googletest directory?\n. Thank you for the review!. After digging a bit more, this is the problematic bit:\nhttps://github.com/google/benchmark/blob/5159967520f45d2bb31b226b810f4f8dc03df1c4/src/benchmark.cc#L566-L567\nIf we wanted to test that, we'd need to add some internal debug option to\ndisable this destination rebinding. Reading back from file seems rather fragile... Any further thoughts here? :)\nThe testing issue is unresolved, but i'm not seeing any clear/clean solution for it... Makes sense..\nWould it be unacceptably ugly to simply read back that file and check that?\nI guess the troubling part is coming up with the tmp filename.. Hmm, actually now that i think about it std::tmpnam() might work, too,\nand be more cross-platform. (ignoring the fact that std::filesystem is C++17). This is so horrible ^^. @dominichamon  Thank you for taking a look!. @dominichamon thank you for the review! I will merge this soon-ish.. The appveyor build is clearly failing regardless of this change, so i think this is good to go.. There are no 32-bit builds for OSX as far as i can tell.\nThough i'm unfamiliar with that target so i'm unsure i can help adding them... @dominichamon \n\nWe must be missing a configuration.\n\nYep, it turned out to be trivial https://github.com/google/benchmark/pull/669\nLooks good now?. I wanted to rebase this to double-check that it actually is fully sufficient, but i suppose a merge works, too :)\nThanks!. This apparently is sufficient to expose the @TNorthover's issue that https://github.com/google/benchmark/pull/667 fixes.. Yep, seems to be broken.\nIt works if i use llvm lld as a linker, but does not work if i use ld.gold.\nI suspect this is upstream issue.. As a follow-up, i think i need to somehow rework the generate_difference_report(),\nto better handle differences in the count of runs between benchmarks, at least for the U test needs.. Thank you for the review!. And this is why two build systems is a horrible idea.. Thank you for the review!. Thank you for the review!. Further thoughts here? Moar tests?. Alright, thank you for the review!. How does this affect non-ARM MinGW builds?. At least on the surface, with cmake before 3.4.0-rc1, the FindThreads.cmake completely doesn't work if there isn't C language enabled.\nNot sure if this is applicable here, or whether there are other failure points.. See https://github.com/google/benchmark/pull/613, and more importantly, https://github.com/google/benchmark/issues/603#issuecomment-416928007.. Looks like the travis failure is relevant.. I think you want to try to drop set(THREADS_PREFER_PTHREAD_FLAG TRUE).\nhttps://cmake.org/cmake/help/v3.1/module/FindThreads.html\n\nIf the use of the -pthread compiler and linker flag is prefered then the caller can set\n\nTHREADS_PREFER_PTHREAD_FLAG\n\nPlease note that the compiler flag can only be used with the imported target. Use of both the imported target as well as this switch is highly recommended for new code.\n. @dominichamon thanks!\n@dtoubelis do you feel like bumping required cmake version up to v3.0.0 here, and porting straight to GNUInstallDirs? :). I meant in the scope of this pr, since you are making such a change, why not go for the Right solution (c)\nElse someone else will have to basically repeat all this work afterwards.. > Something tells me that GNUInstallDirs may not work on Windows ;-)\n\nI kind of doubt it. I think it is used elsewhere, with no special-handling for other special os'es. My sample count is small, but i literally never seen any issues like that.\nI expect the dumb approach of simply using those GNUInstallDirs-listed variables to simply work.. You can submit a PR to make those casts explicit if that is better.\nI intentionally left them in the current state, since i don't think we can change the return type of those functions.. I think d8c0f27448dfad95b94285e612bba1f7c55c9dd0 fixed it?. The Appveyor build is red since https://ci.appveyor.com/project/google/benchmark/build/1401/job/2u8kwhb363sxvg7j anyway.\nThat failure looked rather cryptic to me, so i did nothing.\nI suspect that is why those msvc builds are not warning-as-error in the first place.. > This seems to work really well. \nWell, it should be the same algo, just less, convoluted?\n\nI wonder if it's worth extracting the class into a benchmark_runner.cc implementation file to help reduce the overwhelming benchmark.cc file size.\n\nDone, better?. Anything more to do here?\nFrom my side, all the follow-up work in benchmark_runner.cc will be in follow-up pr,\nonce i'm aware what it is... Thank you for the review!. > I.e. I do not want to have the time taken to allocate the vectors to be counted,\nhttps://github.com/google/benchmark#basic-usage\n\nThe benchmark library will measure and report the timing for code within the for(...) loop.\n\nAnything before the loop will not be counted\n\nI also want to be able to reuse some of the vectors.\n\nhttps://github.com/google/benchmark#fixtures ?. Great, i will close this then.\nIf there are further questions, feel free to open a new issue.. It only hides the issue though.\nIt should be fixed upstream in google test.. Well, you were able to write this patch, right?\nSurely, you could submit the proper fix for google test too?. > What is the proper fix? Something like llvm-mirror/libcxx@362353a? Annotating googletest locks properly is non-trivial.\nNo idea what they will consider the proper fix is, it's in gtest, not this repo.\nThis just feels super weird.\nFreeBSD went all that trouble to annotate their pthread headers,\nthat is paying off in terms of that failure you stated in the commit msg,\nand instead of fixing the source of that failure, you will want to silence every\nproject out there that uses google test this way?\nIf you don't care about the warning, either un-annotate your pthreads,\nor pass -Wno-error=thread-safety,\nor apply this patch in the googlebenchmark package in freebsd.\n. > Why do you want googletest (dependency) warnings to affect benchmark (this project)? SYSTEM converts -I into -isystem which makes the include path act like /usr/include where fixing warnings is not always a viable option.\nNow that is indeed a much better question.\nI do know what SYSTEM does, and i personally do think it should be used here.\nI think (i hope so? can't find the discussion), i did suggest that, but clearly that wasn't the outcome.. :/. Maybe in 2.0 we could change the signatures of those functions to avoid casts.. As far i'm aware that is nothing more than just short for [B]ench[M]ark, just to visually be able to tell that it is a benchmark target.. It's just an example, as you can see it will work without the prefix too.\nBut maybe when looking through the list of functions in the IDE?. @ahsabali did that answer your question?. \"the results are not as expected.\" is a very generic statement.\nYou haven't show any code. What is the issue, exactly?. 113200 ms (400000) is ~= 10 * 11414 ms (40000), correct?\nI think you are reading this wrong, not accounting for Iterations column.\nMaybe use some custom counters.. I'd highly recomment to look into https://github.com/google/benchmark#user-defined-counters, https://github.com/google/benchmark/blob/d8c0f27448dfad95b94285e612bba1f7c55c9dd0/include/benchmark/benchmark.h#L349-L393. @saikishor Were you able to understand what you are misunderstanding?. See https://github.com/google/benchmark#runtime-and-reporting-considerations. Great, i'll close this then?. Looks reasonable, but is there any reasonable way to test this?. I wonder if it would be good to introduce something like a scientific notation \"time unit\" (i.e. autodetect each time). I like this, but that padding/alignment is really bothering me :/\n\n\nHow important is it to do that alignment of . separator? Can it be avoided?. > > How important is it to do that alignment of . separator? Can it be avoided?\n\nI think it's pretty important.\nIt's allows you to understand the magnitude of a benchmark by glancing.\n\nTrue.\n\nWhat's you're objection exactly? To readability? Do you think the space between the value and ns is ugly?\n\nReadability is good; but yes, i don't like that wasted space, and the yet-increased width of the user-counter-less line.. > Do you think this issue should block the revision?\nThere are two things here as far as i can see:\n1. Always print at least 3 significant digits\n2. Align/pad .\nI like 1., but i'm not quite sure about 2. yet.\nSo i guess i'll defer to @dominichamon.. > A couple of alternative solutions:\n\n\nAdd a flag to change the alignment to always be right aligned to the unit, which should waste less space. But IMO getting the old behavior back isn't worth the cost of another flag.\n\n\nI agree, a flag for this does not sound good.\n\n\nUse the \"wasted space\" explicitly so it seems \"less wasted\". That is, fill the wasted space with three trailing decimal digits for all outputs. I don't like this either because those digits aren't significant and make it harder to see the information that is.\n\n\nAnd what is even worse, those 'zeros' are likely straight-up a lie.\n\n\nJust right align everything all the time. This means it's a lot harder for the user to determine the magnitude of a benchmark, and ever harder yet to compare two benchmarks next to each other.\n\n\nI would personally go with 3., but it is also possible that i'm simply too picky here.\n(Though it is undeniable that the new layout is more width-consuming.)\nAs i said, i'll defer to @dominichamon... > (Though it is undeniable that the new layout is more width-consuming.)\n(I.e. if one day i/someone else \"finishes\" support for custom timers, and a new time column can be arbitrarily added, the width will be in short supply). If the time is 52321 ms, does one care if it is 52321.000 ms or 52321.444 ms ?\nI'd think that goes against the reason for setting the time units in the first place... > oh you updated that from .666 so i wouldn't complain about rounding :P\n:P\n\nHigh-level answer: I'm not a fan of the extra whitespace between the numbers and the units.\n\nAny opinion on simply not having those whitespaces, will that also be confusing?. That is not what i meant. I was literally talking about the current code, but without that padding with spaces.\nI.e.\n...\n100 ms\n0.1 ms\n  1 ms\n.... if the manual 'time' is actually measuring time, in the same scale/units as the min_time\nThat isn't a requirement.. Actually, more reasonable question: why can't you just adjust the min_time for your benchmark?\nhttps://github.com/google/benchmark/blob/d8c0f27448dfad95b94285e612bba1f7c55c9dd0/include/benchmark/benchmark.h#L850-L853. I don't have a specific concern, i'm only noting that this PR is a direct revert of 46afd8e69339b546526706056da9dd5009fa01f1.\nThat change did not state which actual problem it solved, but since it's a direct revert,\ni'm guessing that whatever that problem was, it will regress back.\nSo either 46afd8e69339b546526706056da9dd5009fa01f1 was an incorrect fix, or this is...\n. @olzhabay hm, thinking about it a bit more, you placed googletest into the benchmark/googletest?\nAnd it is looking in yourproject'sroot/googletest.\nI think the solution here is slightly incorrect.\nI think there should be an option that will specify the path to the googletest directory.. > If it would be place outside of benchmark directory, then it cannot execute add_subdirectory.\nI'm not sure i follow. add_subdirectory() works with both the absolute and relative paths.\nCurrently; it expects googletest to be placed in the root of top-level project.\nYou expect it to look for googletest in it's own subdirectory.\nThose are two expectations, and both of them might be the expected thing to happen.\nI think the location where it looks for googletest should be a config option, thus any expectation can be valid. Also, i think you need to pass ${CMAKE_CURRENT_BINRY_DIR}/googletest as binary_dir into that add_subdirectory() call.. I think i'm failing to convey my thoughts here.\nThe patch is correct if you, who use googlebenchmark as a subproject, expect for the googlebenchmark to look for the googletest in yourproject/googlebenchmark/googletest\nThe patch is not correct if you, who use googlebenchmark as a subproject, expect for the googlebenchmark to look for the googletest in yourproject/googletest.\nI can see both cases being the expected behavior, thus i'm suggesting a config option.. > This patch is \"correct\" because it brings the implementation inline with the currently documented behavior, which says:\n\n\nThis dependency can be provided two ways:\n\nCheckout the Google Test sources into benchmark/googletest as above.\nOtherwise, if -DBENCHMARK_DOWNLOAD_DEPENDENCIES=ON is specified during configuration, > the library will automatically download and build any required dependencies.\n\n\nI'm merging this patch to make the behavior inline with the documentation. Lets continue the broader discussion elsewhere.\n\nOh, i forgot about those docs. Ok, then that patch as it was is ok to go.\n\n@LebedevRI Totally got your point. I added this option. So, now it can look to both myproject's dir and googlebenchmark's dir, if one of them has an googletest\n\nNot really what i meant.\nI was really talking about a config options, not hardcoding looking in two places.\nBut as @EricWF points out, that is not the scope of this PR.. The tests convey that i think..\n\n\n. Hm, that is clearly broken.. Ok, that was a fun issue :) Test added.\nProper comparison:\n\n\n. Yay, thank you for the review!. Thank you for the review!. It's subtle indeed.\nPreviously, BenchmarkRunner() always said that \"if there are no repetitions,\nthen you should never output only the repetitions\". And the report() simply assumed\nthat the report_aggregates_only bool it received makes sense, and simply used it.\nNow, the logic is the same, but the blame has shifted.\nBenchmarkRunner() always propagates what those benchmarks would have wanted\nto happen wrt the aggregates. And the report() lambda has to actually consider\nboth the report_aggregates_only bool, and it's meaningfulness.\nTo put it in the context of the patch series - if the repetition count was 1,\nbut *_report_aggregates_only was set to true, and we capture each iteration separately,\nthen we will compute the aggregates, but then output everything, both the iteration,\nand aggregates, despite *_report_aggregates_only being set to true.\nDid that make sense?. It's basically the same problem as the cause for the #707 rename.. Thank you for the review!. Yeah, i expected that (:\nGuess i need to post the actual dependent [WIP] PR that builds ontop of this, too.. Soft ping.. ping. Updated. Unless we (i) have completely mis-understood the disscussion, this should be better.\nNo virtual calls from constructor, no \"do-it-yourself CRTP-based virtual calls\".. Kinda ping. Would love to get any actionable feedback there.\nThough i got the hint that there is no time, and it is kinda complicated.. > I finally had the time to code up my version of this.\nYay!\n\nI think it's possible to do with less magic:\n\nThat would be good.\n\nno templating, no *this assignment, by removing the iters and seconds fields from the IterationResults. Instead, these become methods that are overridden. Then, to avoid the memory overhead, if we're going to be averaging we can essentially throw away the individual results.\nYou end up with methods for \"has_error\" \"iterations\" \"seconds\" and \"real_time_used\".\nmakes sense?\n\nI don't know. I guess as long as the base idea (don't do 1 repetition with N iterations,\nbut do N repetitons with 1 iterations, basically) is preserved/achievable, it should be ok.\n. Soft ping.. I guess #481 didn't quite get fixed (:. I'm going to guess #596 is related.. No, as in, i would have guessed, that refactoring might have broken it?\nIt would be good to get that backtrace with -O0.. My personal guess is that that value is bad, thus it throws, and we don't catch it.. It is looking for the pattern like\nprocessor       : 0<newline>\n...\nprocessor       : 1<newline>\n...\n...\nprocessor       : n-1<newline>\nSo for that arch, it should either simply use # processors    : 2,\nor use the number right after processor and before :.. Is -Wformat not enabled? Why isn't compiler complaining about that... On which compiler is nvcc based? Does it really not have __builtin_unreachable() or some analog of it?. As i have posted in #613, unfortunately, now there is a small roadblock here:\nhttps://github.com/llvm-mirror/llvm/blob/c1873bf60aa20b3873d859c99ecaf5967c20ba3f/CMakeLists.txt#L3\ncmake\ncmake_minimum_required(VERSION 3.4.3). Looks like CLA bot is still unhappy?. https://github.com/google/benchmark/commit/5d4ad0a277e1761463a227cf50d7748fcf5f1ef4.patch\nFrom: Tobias Ulvgard <tobias@ulvgard.se>\n^ i guess it is checking this email. Is that the one with signed CLA?. cc @dominichamon . Thank you for the review!. Someone using android will have to make it build, and submit a PR.. By default, obviously everything should be run.\nBut i think the question is about explicitly passing --benchmark_filter=\"\".\n. ~~(Actually, i'm not seeing any contradiction)~~\nYeah, i think the doc should be updated.\nI'm not sure what is the expected outcome of not running any benchmarks.\nYou can turn the execution into no-op with --benchmark_list_tests=true e.g.. > TL;DR Be able to dump diff report of two benchmarks into the JSON file and not only on stdout as it is right now. E.g. compare.py -d bench_diff.json benchmarks bench1.json bench2.json\n\nRationale behind this functionality is to be able to manipulate the performance data directly which is much more powerful than simply printing the data onto the stdout. Some other higher-level scripts could then consume this API to make use of the data in whatever way it wants to.\n\nI don't dislike this in general, but right now it is a just simple script.\ni'm really uneasy about making any API/json stability guarantees.\n\nOne example could be detection of performance regressions integrated tightly into the CI/CD pipeline. Another one would be a plotter which would be displaying how performance data has been changing through different commits/releases/etc. You get the idea ...\n\nThis really should be done by something much more specialized, like llvm lnt..\n\nCurrent implementation of compare.py does not have this functionality so I've prepared a small patch which I will send shortly afterwards.\nLet me know what you think about this feature.\n. > > I don't dislike this in general, but right now it is a just simple script.\nYes it is super simple right now but why wouldn't it provide functionality that basically anyone can have benefit of? And the change is a really simple one ...\n\ni'm really uneasy about making any API/json stability guarantees.\n\nBut we already have a JSON format output defined when dumping the benchmark results. Outputting the diff report to JSON would be clearly built upon it.\n\nEeh, these two phrases are just one phrase, i meant to use comma, not full stop.\nIt should have been:\n\nI don't dislike this in general, but right now it is a just simple script,\ni'm really uneasy about making any API/json stability guarantees.\nBut we already have a JSON format output defined when dumping the benchmark results. Outputting the diff report to JSON would be clearly built upon it.\n\nSo can that output be fed back into the compare.py script?\n\n\nThis really should be done by something much more specialized, like llvm lnt..\n\nSorry but I fail to see what does LLVM has to do with this topic? Proposed functionality is a generic one that any project could make use of it without any other dependencies.\n\nThe usual CI is unfit to do any meaningful performance measurements.\nIt will be extremely noisy, because the hardware will be shared.\nThus i'm completely questioning the reasoning here.. > > So can that output be fed back into the compare.py script?\n\nNo, I have not envisioned that use-case. Not sure what compare.py could do in that particular case but fail.\n\nOk, good.\n\n\nThe usual CI is unfit to do any meaningful performance measurements.\n\nCI is just one example that I gave. Different projects will have different needs or means of post-processing the performance data. The most simple one is a plotter.\n\nTrue.\n\n\nIt will be extremely noisy, because the hardware will be shared.\n\nYes, this is a CI challenge but can be tackled by having a dedicated no-shared node which will do only performance tests one at a time. \n\nSure, just want to point out that then it will no longer be advertisable as \"reliable to use by any project with no special setup.\".\n\nHaving reproducible and continuous performance regressions detection is a very important aspect in developing high-quality software, IMO. Or in other words, being able to detect regressions automatically and early during the development cycle.\n\nYes.\n. https://github.com/google/benchmark#basic-usage\n\nThe benchmark library will measure and report the timing for code within the for(...) loop.\n\nSo just put it outside (before) of the for (auto _ : state) loop.\nDoes this answer your question?. Ah i see.\n\nIf you put it in Fixture::Setup it will be called on every benchmark method invocation, including the repeated invocations as the framework searches for the right number of iterations. This means the same data may be repeatedly generated, which also adds noise to the benchmark (e.g., because memory is repeatedly being allocated and de-allocated).\n\nTo be noted, the time outside of the loop isn't counted, thus, there will be no direct noise.\nBut yes, that is not possible. I'm not sure how of a disruptive change it would be to support that.. True.\nBut then you hit the exact same problem with repetitions.. > My issue is that googlebench (imho, incorrectly - but I'll explain further) calculates that fewer iterations are required when N and M are large, as the time taken by F seems more stable.\nWhat it does is it calculates how many iterations are needed so that the time it takes to run these iterations is statistically significant (min_time, 0.5s by default i think)\n\nThis, unfortunately, isn't always the case, as we sometimes work with quite temperamental hardware that can have quite wide performance ranges for F (even when N and M are large).\n\nI'm not sure what temperamental hardware means here.. 1. Have you seen State::KeepRunningBatch()?\n2. Have you seen Benchmark::MinTime()?\n. > > I wonder if an alternative solution might be to implement alternative strategies for limiting the iterations? e.g. keep running the benchmark until the standard deviation is 5% of the mean?\n\nIt's possible, and I thought i'd opened an issue about this at some point in the past but i can't find it.\n\nSeparate iterations is a step in that direction :)\n\nAs a workaround you could split the ranges into multiple registration calls with MinTime called on each individually. \n\nThat is actually what i meant, i should have spelled it out explicitly :/. Would be good to provide more complete error message.\nCan you link to the line where _ReadWriteFence is used?\nI do not find anything:\ngooglebenchmark$ grep -r _ReadWriteFence\n$. > Other than that, I have no idea where I would stick \"repetition\"!\nYes, i agree, this seems to be the only choice right now.. >   while(state.KeepRunning())\nThat should now be for (auto _ : state)\n\nint main()\n{ \n    BENCHMARK(bench_helloWorld);\n    return 0;\n}\n\nThat is very clearly incorrect.\nSee https://github.com/google/benchmark#basic-usage\n. > I made the required changes and the issue still persists.\n\nCode\n...\n\nYes, that looks about right.\n\nValgrind Output\n\nHm, people are still using valgrind.\nCan you compile with -fsanitize=address,undefined and post the results?\n\nFurthermore, It seems to happen only on the above mentioned system, The Same code runs fine on other systems.\n\nWhere do you get the libbenchmark?\nLooks like some abi issue to be honest,\neither due to version mismatch between the headers and the library,\nor due to some difference in how the libbenchmark and the actual benchmark are being compiled.\n\nThank you for your help.\nSorry for not being more thorough.\n\n. > Where do you get the libbenchmark?\n\nLooks like some abi issue to be honest,\neither due to version mismatch between the headers and the library,\nor due to some difference in how the libbenchmark and the actual benchmark are being compiled.\n. That doesn't really answer the question.\nThis repo only contains source codes.\nHow do you build them? How do you build your benchmark program itself?. And you don't specify where to find the headers?\n\nPlease make sure there isn't any other libbenchmark installed in the system.\nIn particular\n$ dpkg -l | grep benchmark\nand\n$ locate benchmark.h\n. Please fully remove everything google benchmark related, including the build directory, and the repository checkout, triple check that there are no other benchmark/benchmark.h files elsewhere in the file system, do a fresh clone of the repository, build the library, and try again.\nThis very strongly looks like a header version mismatch.\n. Finally!. Where can the problem be observed?\nCan you attach screenshots before/after?. > pthread is required at least for GCC (according to the documentation), \nLink?\nI suspect the problem is on cmake's side.. 1. https://github.com/google/benchmark/blame/master/README.md#L128 (unless you're using libc++)\n2. That will still be an issue if one does not use that .pc file, no?\n\nIt's not clear to me what this changes. Does it mean you don't need to remember to add -lpthread yourself when linking with the library?\n\nYes.\nThe thing is, https://github.com/google/benchmark/blob/d8584bda67320937334b038e736f5bffcd438875/src/CMakeLists.txt#L30 theads is already added as the dep of benchmark library, so that hardcoding should not be needed, there should be some way to simply propagate it into the .pc.. I don't want to block the bugfix since the fix seems so trivial, so LG.\nBut i really don't think this is the correct solution.. No further comments from anyone, let's see if merging will cause them then :). Can you show the console output of that test case before and after this change,\nso it is easier to understand what the problem is?\n(Thought yes, i think this is likely going in the right direction..). To be noted, i do believe that the problem is real, i reported that bug afterall.. Fixed by #761.. > LGTM. I'll leave @LebedevRI to do the merge honours if they're happy.\nHm, this has fallen off my radar completely. Sorry.\nThis looks reasonable to me.. @danielharvey458 thank you for the fix!. I think 0 should be included.. > But before going any further, i have a question:\n\n@dominichamon @EricWF do we agree that for the normal case of google benchmark threads,\nThreadCPUUsage() would be equivalent of the sum of per-thread ThreadCPUUsage()?\n\nAs in, are we afraid some other user-created thread will mess the measurements up?\nOr that the results won't be equivalent due to some implementation detail of timers themselves?\n(in kernel/etc). > If you tried to use the Current GoogleBenchmark built-in threads to benchmark an OpenMP code, you'd get something much worse than just nonsense CPU times, you'd get that each GB thread calls the function which internally starts up its own threadpool.\nYep, that would make no sense in general case.\nThe current question remains (i have not heard back from @dominichamon / @EricWF) - what\n//should// CPU timer be reporting, the whole time spent (i.e. by all the threads) by the benchmark\nfunction, or what it currently does - the time spent by the \"main\" thread. It might be that there is no\nThe Right solution, and a switch like ->UseRealTime() (->UseProcessorTime()?) should be added.\n\n...\nThat way, we could have one and only one \"threads\" parameter, which I think would make interpreting the final output easier.\n\nNo. Nothing like that should be done.\nIf user code uses it's own threads, there is no sane way to generalize that and display\nin report. There is simply can be no guarantee of anything, unless you want to put arbitrary\nrestrictions on the user threading. E.g. part of the benchmark function may be limited 4 threads,\nand then second part is not limited by the thread count.. > Your motivation, and the more general user thread issue, isn't something we've considered. We already have a 'manual timer' that tells the library to get out of the way, which sets a precedent for having a 'this is user threaded' setting, but that's clunky.\nTo give an example: (from IRC)\n<LebedevRI> here, and example in the commit msg: https://github.com/darktable-org/rawspeed/commit/eda7972db1b469548b9bfdd054b5c58f5830e230#diff-e03d58ca0e0ab7c09683945bc5c6e825\n<dhamon> in benchmark-threaded cases they also wouldn't be the same.\n<LebedevRI> \"Time\" is wallclock, \"CPU\" is the time spent by main thread, \"CPUTime,s\" is the actual cpu time spent by all threads\n<LebedevRI> iirc\n<dhamon> and cpu/wallclock is, as expected with 8 threads, ~8. \n\nI think @LebedevRI is on to something already with #671 and other discussions about sorting out our timing so we are explicit about per-(benchmark)-thread time vs total process time.\n\nI currently personally think it would be possible to solve this by simply measuring this third timer,\nand using it instead of the \"time of main thread\" for the \"CPU\" column, much like the same way\nas with ->UseRealTime(), ->UseManualTime() - if user requested so.\nI do not think it is worth it adding a third column to the console reporter.. > I think @LebedevRI is on to something already with #671 and other discussions about sorting out our timing so we are explicit about per-(benchmark)-thread time vs total process time.\nI have now looked, and while we could rename the \"Time\" column in reports to \"Wall\", \ni do not think this would universally improve things, because the \"manual time\"\nis displayed in that \"Time\" column, not \"CPU\" column.\nSo i think this will, again, have to wait for custom timers.\n\nNow I'm confused, does UseRealTime() get wallclock time into the \"Time\" column, or not?\n\nUseRealTime() does not change anything about how which timer is displayed in which column.\nUseRealTime() only tells the library to use the wallclock (instead of the \"CPU\",\nthe time of the main thread) to decide for how long to run the benchmark.\n\nIn general, for parallel codes (by which I mean user-managed parallelism within the one function, as in my example) we are primarily interested in the wallclock time and secondarily interested in the CPU time.\n\nYes, i understand the needs, i have one such case myself.\n\nIt seems like there is a choice and a choice of reduction here.\n...\n\nIt looks like i will need to help with this code.  . Updated with all the stuff!!1 Looks like cla bot loves such pushes :S\nNow here is an interesting question, now that i have worked on this code, can i still review the PR? :D\nI'd guess not really, perhaps @dominichamon might have a moment.. > So who has to sign off on the CLA now?\nMine's signed, it's a 'known' glitch i believe.. > It looks like this satisfies the original request,\nYes.\n\nbut at the increase in complexity of the API.\n\nYes :(\n\nIs there a way we can change the default behaviour to be something sensible and expected without adding another boolean option and benchmark method? (the answer might be no, but i should ask).\n\nThe alternative would be to always use whole-process cpu clock instead of the\nmain-thread-only cpu clock when the ->Threads() wasn't called.\nPros: \n no need for all that extra API\n no /process_time suffix in benchmark name.\nCons:\n* Such change will change the timer used by benchmarks that do not use ->Threads().\n  If any of these benchmarks used threads internally, the reported time will naturally be different.\n  Do we know that it is what everyone affected actually wanted?\n  Maybe they actually used the current timing method.\n  Also, there will be no way to switch back to the previous (current) timer method.\nThe other alternative is likely again the old good 'custom timers'. But that feature is nowhere near.. > > > It looks like this satisfies the original request,\n\n\nYes.\n\nHooray!\n\n\nbut at the increase in complexity of the API.\n\nYes :(\n\nAwww.\n\n\nIs there a way we can change the default behaviour to be something sensible and expected without adding another boolean option and benchmark method? (the answer might be no, but i should ask).\n\nThe alternative would be to always use whole-process cpu clock instead of the\nmain-thread-only cpu clock when the ->Threads() wasn't called.\n\nok...\n\nPros:\n\nno need for all that extra API\nno /process_time suffix in benchmark name.\n\n\nI like these...\n\nCons:\n\nSuch change will change the timer used by benchmarks that do not use ->Threads().\n\n\nYes, but will it actually be different for the majority of users?\n\n... i have no clue, i have no statistical info on how which features of the library are used.\nIt won't be 100%, maybe around 10% of BENCHMARK_MAIN()'s will be affected.\n\n\nIf any of these benchmarks used threads internally, the reported time will naturally be different.\n\nIt will, and more accurate (if we believe the premise of this PR, which i think we do).\n\nDo we know that it is what everyone affected actually wanted?\n\nMaybe not, but what it's doing today is a bit silly.\n\nMaybe they actually used the current timing method.\nAlso, there will be no way to switch back to the previous (current) timer method.\n\nI might be ok with this as long as only people who are using custom threads inside their benchmark are affected and we can make the case this PR does that this is a better measurement.\n\nI'm sure this is better for those who want to measure the total toll some computation inflicts on\nthe CPU. I'm just saying that if someone wants to measure how much overhead there is\n(i.e. if all work is being done on worker threads, how much of the work is not being offloaded\nfrom main thread), how much CPU time is only spent on the main thread, then those measurements\nwill no longer be possible. I'm unable to guesstimate how many of the current internally-threaded\nbenchmarks out there (as previously guesstimated 10% out of all benchmarks) want the current\nexisting timing, and how many consider it to be a bug.\nBut okay, let's consider this as a bug and avoid API for now. (Will update PR.)\nIt can be added later if someone complains.. As advised by @dominichamon, posted https://groups.google.com/forum/#!topic/benchmark-discuss/97hVnMfGYgE\nPlease raise concerns NOW.. > I didn't see any concerns. \nThere was one reply, and it did raise a concern, as it can be seen from that link.\nThe cheap way would be to now always append /wholeprocess_time, but that is ugly.\nThe simple choice is still the same, only add a switch, don't change the default.\nI'm not seeing any non-ugly alternatives for console reporter.\nFor json reporter, we could add an always-there field telling which timer was used for which time.\nBut i'm not sure what would be the most forward-compatible way.\nTLDR: @dominichamon do tell which path should be taken :). TBN not a clang regression, and gcc does the same\nhttps://godbolt.org/z/LM_5wz. You want to be rebasing your master branch, and rebasing the feature branches ontop of your master branch. That way the branch history is clean.. Is this actually correct/sufficient though?\nhttps://cmake.org/cmake/help/v3.1/module/FindThreads.html\nIf the use of the -pthread compiler and linker flag is prefered then the caller can set\n  THREADS_PREFER_PTHREAD_FLAG\nPlease note that the compiler flag can only be used with the imported target. Use of both the imported target as well as this switch is highly recommended for new code.\nAnd yet benchmark uses CMAKE_THREAD_LIBS_INIT, not the Threads::Threads import target,\nas far as i can see:\nhttps://github.com/google/benchmark/blob/505be96ab23056580a3a2315abba048f4428b04e/src/CMakeLists.txt#L30. > Each benchmark has a preference for real time or CPU time, depending on what's being tested, but gbenchmark always stores both values in JSON output files.\nIt displays both in the console, too. This is very intentional.\n\nThis means it's not possible to programmatically determine, when reading a JSON file, whether CPU time or real time is relevant in a particular benchmark.\nIt seems that when the benchmark declares a preference for real time, I could parse the benchmark name for the string /real_time. But that seems hackish...\n\nYeah, i guess that is the (current) way.\nWhat is the use case, why do you want to know that after the benchmark?\n. How do they fail? (see one of the CMake*.txt files it mentions at the end)\nMaybe some -dev package is missing?. Yes but does it work in cmake 2.8.12?\nCan you perhaps try to track down when that comment was added, was there any details given which cmake issue had the problem?. Did that, it was #564 / 2844167ff99edb63f629c9768ea1f3f94ebfb8fe\nwith\n~/git/google/benchmark/build$ cmake --version\ncmake version 2.8.12.2\nSo the problem is still there.. > I reproduced and confirmed #564 originally. I remember it being specific to 2.8.\n\nLet's remove the workaround, but lets also bump the CMake required version. Abseil requires 3.1, LLVM requires 3.4.3.\n\n3.4.3 sounds great, #717 did that, but it got stuck with some CI issues.. Aha, that seems to be already possible. https://godbolt.org/z/PhcBV4\nSo this is only a documentation issue.. I agree with @EricWF, i'd say this is working as expected.\nIt's very common pattern for such static const global objects.\nWhile sure, this is a leak in the sense that the memory is not freed by the program before exit,\nit will be immediately freed by the system the very next moment. So it's rather harmless... Hurray.. nagging: \"fix for \", \"output is broken\" and other things people write as commit messages are useless.\nThe commit title needs to explain the fix/change, not just reference some bug somewhere.. Hmm, if i think really hard about this, i'm not 100% sure this run_iterations trickery is never ever needed?\nSo maybe run_iterations should be provided as a parameter into the lambda?. > expected results when we run the tests we have\nI'm not actually seeing any test that fails if i cripple mean/stddev calculation, so i'm not sure it is actually tested.\nBut yes, so far i have not seen any wrong output.. Yep, double indeed. I was actually biten by that while experimenting, but did not incorporate the fix.\nAs for const, it was a leftover from when i copied the lambda from .h, where it was needed for win builds.. Removing behind-the-back magic is generally a good idea.\nHowever i'm not sure how this could be implemented without regressing all the current users?\nIf any of these is called, then some bool is set, and defaults (mean,median,stddev) are not added?\nThat reminds me of more thing i'd like to add to the compare_bench.py - make it print some blurb about statistical significance of the measurements/changes based on comparison of the confidence intervals.. But there is a std::vector<Statistics> usages in this very header, just like with UserCounters.\nI could put it into statistics.h, but it is no better than here, and then that header would have to be included here.. Should there be getters for them?\nCall on a function pointer returned by a function call will look funny (:. No, why?\nThis is one of these functions which are called on a pointer returned by BENCHMARK() to further setup the benchmark, and all the other such functions are defined in this file.. Sounds great!. Makes sense :). Done.. Hmm, you're right, this is not right, likely too greedy s///\nhttps://github.com/google/benchmark/pull/431. This change seems wrong, probably rebase defect.. Seems unnessesary. Not commenting on the typedef itself, but the placement seems not ideal, maybe move down to getters/setters?. reports is const std::vector<BenchmarkReporter::Run>&\nhttps://github.com/google/benchmark/blob/d70417994a3c845c49c4443e92b26a52b320a759/include/benchmark/benchmark.h#L1039\nnamespace benchmark {\n...\n  struct Run {\n    Run() : ...\n...\n    int64_t iterations;\n...\nSo int64_t is the type that is stored.. I though having a variable with name median would help readability, but sure :). I believe i simply kept those double() from the original code in src/stat.h.\nThere they made sense because of templates. Here, changed to 0.0.. Hmm. I'm using -fsanitize=address -fsanitize=undefined, and at least with all the current tests, there are no complaints.\n. This one would break too:\nhttps://github.com/google/benchmark/blob/d70417994a3c845c49c4443e92b26a52b320a759/src/benchmark_register.cc#L150. But then looking at https://github.com/google/benchmark/blob/d70417994a3c845c49c4443e92b26a52b320a759/src/benchmark.cc#L619-L633\n(which is seemingly the only consumer of FindBenchmarksInternal() -> BenchmarkFamilies::FindBenchmarks() that we are talking about here) i think the current answer should be no.\nUnless of course the consumer calls ClearRegisteredBenchmarks() in the middle of benchmarking, but then that probably won't the only thing that will break. I'm not sure there are any current locks that could prevent that.. I'm not sure about this copyright header. It was copied from src/complexity.cc\nBut i'll just add myself here.... Hm, correction, https://github.com/google/benchmark/blob/d70417994a3c845c49c4443e92b26a52b320a759/src/benchmark_register.cc#L86\nBenchmarkFamilies::mutex_ does partially help (but no thread safety analysis annotations are used), but it will still not help against the call to ClearRegisteredBenchmarks() right after FindBenchmarks(). I'm not sure it can happen in reality though.\nBut i guess Benchmark::Instance::benchmark could be a std::shared_ptr, and then it would all be fine, with some performance penalty.. Yes, probably. Will change.. > i think this is where we're ending up with /8 etc.\nYes.\n\ni wonder if it would look better as BM_copy|BM_memcpy/8 or something similar.\n\nIt will become rather convoluted with larger, more complex names.\n\nthis would also help if the report is output somewhere later.\n\nYes, something better than now would be nice.. diff\n+ INTERFACE_SYSTEM_INCLUDE_DIRECTORIES ${CMAKE_BINARY_DIR}/googletest/include. diff\n+ INTERFACE_SYSTEM_INCLUDE_DIRECTORIES ${CMAKE_BINARY_DIR}/googletest/include. diff\n- set(GTEST_INCLUDE_DIRS ${CMAKE_BINARY_DIR}/googletest/include). diff\n- include_directories(SYSTEM ${GTEST_INCLUDE_DIRS}). so now ${GTEST_BOTH_LIBRARIES} will properly add gtest include dirs.. This should be two options:\n1. build gtest\n2. if build gtest, is downloading allowed? this really really should default to false.\n    build systems downloading stuff from internet is such a disgrace :/. I'd highly recommend\n1. providing an option specifying the path to the gtest sources\n2. defaulting to /usr/src/googletest/, since that is the natural location, and is where debian puts it.. Since all this is just to propagate the current params, maybe it would be simpler to just use 2-step approach?\nHere is how it looks in practice: [0] [1]. /usr/share/doc/googletest/README.Debian:\n```\n    Use of precompiled libgtest Not Recommended\n    -------------------------------------------\nThe Google C++ Testing Framework uses conditional compilation for some\nthings.  Because of the C++ \"One Definition Rule\", gtest and gmock\nmust be compiled with exactly the same flags as your C++ code under\ntest.  Because this is hard to manage, upstream no longer recommends\nusing precompiled libraries [1].\nUsing GTest with your project\n-----------------------------\n\nSee the upstream README for instructions on using gtest with your\nproject.  The sources for libgtest are installed into\n/usr/src/googletest/googletest along with CMakeLists.txt for use with\ncmake.\nIf your build system uses CMake, the ExternalProject command can be\nused to build gtest, then FindGTest can be used to find the built\nlibrary.\nUsing gmock with your project\n-----------------------------\n\nSee the upstream README for instructions on using gmock with your\nproject.  The sources for libgmock are installed into\n/usr/src/googletest/googlemock along with CMakeLists.txt for use with\ncmake.\nWith this Debian package something like the following should be enough to build \na static library (which also includes gtest):\ng++ -I/usr/src/googletest/googlemock -c /usr/src/googletest/googlemock/src/gmock-all.cc\ng++ -I/usr/src/googletest/googletest -c /usr/src/googletest/googletest/src/gtest-all.cc\nar -rv libgmock.a gmock-all.o gtest-all.o\n[1] http://groups.google.com/group/googletestframework/browse_thread/thread/668eff1cebf5309d\n-- Steve M. Robbins smr@debian.org, Sat, 19 Nov 2016 21:58:04 -0600\n``. True.\nI'm just following pre-existing practice e.g. withcolor_format()doing this.. Right.. While this may be a valid markdown, it isn't displayed correctly on github,\nA space is missing.. The unittests later in this file not always pass them. Does that actually work though?GLOBshould result in absolute filenames... Nit: reorder :). Hm, this looks like a non-NFC change, but i don't see anything in test changes?. I'm not seeing this change reflected in tests.. This is still bothering me. It just looks completely wrong.. This is correct, but the indent is indeed wrong.. Ah, good idea..\nHow do you want it spelled,_utest_pvalue?. Funny,autopep8actually insists on this weird indent.. I wanted to specify for *which* test the value is.\nBut i suppose i can do that next to theRepetitions: .. But now that i have actually looked, having such a long suffix as_utest_pvalue, causes\nthe need to realign the columns (since the first column may now be wider).\nSo_utest_pvalueis _better_, but more troublesome.. So i have thought, and ok, let's just use_pvalue.\nAvoids the headache with column width readjusting _for now_.. Pushed the_pvaluevariant.. https://github.com/darktable-org/rawspeed/commit/a1ebe07bea5738f8607b48a7596c172be249590e - here,kIsIterationInvariantRatewould be nice\nhttps://github.com/darktable-org/rawspeed/commit/0891555be56b24f9f4af716604cedfa0da1efc6b -kAvgIterations`\nBasically, kAvgIterations means that the value is collected over all the iterations, e.g. a timer, thus for it to be meaningful, it needs to be divided by iteration count.\nAnd kIsIterationInvariant means that every iteration produces this very counter value.\nYou could use it as is, but if you use anything else other than kDefaults, the produced value may be incorrect.. @biojppm FTFY. This isn't true, i think right now, c++11 is still not required for consumers. This is quite messy.\nI'm thinking that maybe you either want to specify a custom install prefix, or not install at all.\nIn either case, you'll need to specify the include/link paths.\nOr maybe just use the packaged version?. Should i follow up with kIsThreadInvariant, for symmetry?. After thinking about it a bit more, it would make no sense, since that is already what happens.\nhttps://github.com/google/benchmark/blob/master/README.md#user-defined-counters\nIn multithreaded benchmarks, each counter is set on the calling thread only.\nWhen the benchmark finishes, the counters from each thread will be summed;\nthe resulting sum is the value which will be shown for the benchmark.\nBUT i will have to follow-up with *= cpu_time;\nNow, if the benchmark processes N Things (not items, more fine-grained),\nkIsRate would tell you how many Things it processes per second.\nBut what about how many seconds per Thing?\nIn the same thought, i wonder how Set{Bytes,Items}Processed() are useful\ngiven the existance of custom counters. I wonder whether they should be\ndeprecated v2. One very obvious downside is that they have very generic names,\nand it is impossible, unlike custom counters, to customize them.\n. Why float? Should be int.. It would be great to have a comment explaining what this is.\nIs this the total amount of memory allocated, ever, or the peak memory usage?\nFor reference, it is possible to get the former natively - e.g. llvm::sys::Process::GetMallocUsage(). Nit: this does not actually check the values, only their presence. MemoryManager has virtual functions but non-virtual destructor :). Relevant: https://github.com/google/benchmark/issues/620\nI wonder if timers should be generalized into metrics,\nand you'd just add a second benchmark with use timer = memory.\nI don't think counters should be taught about that.\n. > The problem with Intel is that it struggles to be a drop-in replacement for GCC, so if you pass any -W* flag it never complains even if it doesn't know about the flag itself.\nhttps://godbolt.org/g/7CNvU7. Point was, it looks like there is a way to make it complain.\nSo sounds like you just need to fix add_cxx_compiler_flag().. It's a very bad idea to write outside of the CMAKE_CURRENT_BINARY_DIR. This could/should be simplified down to\ncmake\nif(NOT MSVC)\n  option(BENCHMARK_BUILD_32_BITS \"Build a 32 bit version of the library.\" OFF)\nelse()\n  set(BENCHMARK_BUILD_32_BITS OFF CACHE BOOL \"Build a 32 bit version of the library - unsupported when using MSVC)\" FORCE)\nendif(). And this change will no longer be needed. This looks like a general cleanup. No longer needed, set(BENCHMARK_BUILD_32_BITS OFF CACHE <...> FORCE) will force-set it to false.. git clone https://github.com/google/googletest.git # If you want to build the tests. These a problems, not intended behaviour.\nI think this should stay as is in the section at the end.. But this is not based on that. They just happen to have a similar approach. As far as i know.. Why are you removing this?. Please linewrap and use proper lists. Uhm, no?\nCan you quote the test you think says that?. Uhm, if that is part of the README.md as of current git master, then why/how it is in this diff?\nSince that line was added by #600 by You, the question remains unanswered.. Maybe i just can't read. Which one?\n* https://github.com/google/benchmark/pull/600/commits/53786ab39e68da9f9c4aeba0f9177fd921b37f6a\n\nThird option under this  line: \"Note that Google Benchmark requires Google Test to build and run the tests. This dependency can be provided three ways:\"\nWas not true (did not occur). If there is a further option that needs to be specified in order for that functionality to work it needs to be specified.\n\nThat is not correct, it's clearly occurring - https://github.com/google/benchmark/blob/6d74c0625b8e88c1afce72b4f383c91b9a99dbe6/cmake/HandleGTest.cmake#L102\n* https://github.com/google/benchmark/pull/600/commits/cf9e96815b298643c99831554e17a1ad6a25bd54 commit msg doesn't actually answer it, it just states it as a fact.\n. > library will reporting the timing\nSomething is missing between will and reporting.. Does \".\" mean that it will be built in the source tree?\nIt should be built in some new subdirectory, e.g. build.. Is this version info needed always, or for releases only?\nHaving it always is actually actively misleading, as this is 1.4.1+git...\nCan it be present for releases only, and otherwise be version = \"unknown\" or something ?\n. How will this work?\nThis has been moved from src/internal_macros.h,\nwhich (at the top of the file) has (*)\n```\nifndef __has_feature\ndefine __has_feature(x) 0\nendif\nifndef __has_builtin\ndefine __has_builtin(x) 0\nendif\nBut it includes *this* header before that:\ninclude \"benchmark/benchmark.h\"\n```\nSo i think you also need to get (move?) the (*) block here.\n. Can you submit just this change in a new PR?\nI think it can be merged safely.\nI suspect it might resolve https://github.com/google/benchmark/issues/641\n\nThe tests do not explicitly check the full executable path, just the resulting endname.\n\nCan you add one windows-specific test (#ifdef _WIN ?)\nthat does check that there is \\\\\\\\ before the  resulting endname?. In principle, all json strings should be properly escaped.. I suspect when the proper json support is here, possibly it will be able to handle all this transparently.. Since we support custom statistics, and custom counters,\nit is totally possible that users could be passing strings that need escaping,\nbut i think it can be done separately later.. I was thinking of something more like:\n```c++\nifndef _WIN\n{\"\\\"executable\\\": \\\".*/reporter_output_test\\\",\", MR_Next},\nelse\n{\"\\\"executable\\\": \\\".*\\\\reporter_output_test\\.exe\\\",\", MR_Next},\nendif\n. @dominichamon why was this added here, and not at the end of the body of per-repetition loop?\n     }\n     // Here\n   }\n   // Calculate additional statistics\n. Oh i see. It's fed directly into `CreateRunReport()`. This is kinda ugly o_O. I would personally think just one value is enough (which is what i had in the first commit, https://github.com/google/benchmark/pull/657/commits/d4a5af5a0e02d9399d9ecf9570551a44e6f5f51f),\nbut https://github.com/google/benchmark/pull/657#issuecomment-414133467 may or may not suggest that something more advanced is needed.\n@EricWF ?. Right now i don't have/is not aware of any real use for the one_k not being 1000 or 1024, so enum _could_ work.\nIt is just that storing enum will take just as much space as storing an int, or a pair of shorts:\nhttps://godbolt.org/z/9Q1gVS\nSo it isn't obvious to me why we want to avoid doing/allowing the more general thing at the **same** cost.\n. Hmm, if it to be an enum, should i just add value `k1024` to the `Flags` enum?\n. Ok, sounds good.. There is no _real_ reason not to do it.\nI didn't do that because `0` and `1` (which is most likely the current values of these values)\nonly need one bit, while `1024` needs 11 bits. So _if_ we wanted to pack the size of `Counter`,\nit would _likely_ be simpler with smaller enum values.\nI don't think we care about that here, so let's avoid this premature optimization :). Hmm, actually it might be.\nI didn't do so initially because i thought it was important to print the whole benchmark at once\n(for header/context pre-print), but maybe it is too pessimistic... Makes sense.. Hmm, kinda.\nIt gets a bit messier due to the command-line flags.. Oh duh, I've meant to clean this but overlooked :)\nThanks!. Then i don't understand the previous suggestion, https://github.com/google/benchmark/pull/665#discussion_r214663892.. But to attempt to answer, `display_report_aggregates_only` is needed,\nbecause i need to somehow convey back which result (all the reports, or aggregates only)\nto report, for both of the reporters. I need to do so because i actually look at the\n`benchmark::internal::Benchmark::Instance::aggregation_report_mode`,\nwhich is per-instance, not just `FLAGS_benchmark_report_aggregates_only` /\n`FLAGS_benchmark_display_aggregates_only`, which is global.. [`std::move()`](https://en.cppreference.com/w/cpp/utility/move) is C++11 or newer, while public api is C++03 and newer.. Ah, could work, won't even need preprocessor stuff :). I personally somehow don't like this.. Well, i should have commented here i guess.\nBecause it's a memory allocation.\nI don't really expect this to matter in the *normal* usage, but *if* it can be avoided... Hm, missed this one... Yes, they are necessary.\nhttps://godbolt.org/z/V6ASzJ. Hmm, maybe i can wrap some of this support code into RAII class... Yep, looks better now.. Just a thought :) I haven't benchmarked this, so i'm likely just paranoid :). by static method you mean just an standalone function?. I was initially thinking about deleting the tmp file in the destructor here, but it does not appear to be worthwhile.. Why not.. And this is why two build systems is a *horrible* idea.. I think it could work in theory, but i have no clue whatc++\n   std::string benchmark_name =\n       reports[0].benchmark_name.substr(0, reports[0].benchmark_name.find('/'));\nis supposed to be doing.. In other words, if we can drop that, then we can create `Run::benchmark_name()`. Ah i see\n\nBenchmark                                      Time           CPU Iterations\n...\nBM_Complexity_O1/1                            14 ns         14 ns    1006981\nBM_Complexity_O1/8                            14 ns         14 ns    1032428\nBM_Complexity_O1/64                           14 ns         14 ns    1032418\nBM_Complexity_O1/512                          14 ns         14 ns     998075\nBM_Complexity_O1/4096                         13 ns         13 ns    1029608\nBM_Complexity_O1/32768                        14 ns         14 ns    1022146\nBM_Complexity_O1/262144                       14 ns         14 ns    1008279\nBM_Complexity_O1/1_BigO                    13.62 (1)      13.62 (1) \nBM_Complexity_O1/1_RMS                         1 %          1 % \n\nBM../test/output_test_helper.cc:108: CheckCase: Check remaining_output.eof() == false' failed. End of output reached before match for regex \"^BM_Complexity_O1_BigO %bigOStr %bigOStr[ ]*$\" was found\n    actual regex string \"^BM_Complexity_O1_BigO [ ]* [0-9]*[.]?[0-9]+([eE][-+][0-9]+)? \\([0-9]+\\) [ ]* [0-9]*[.]?[0-9]+([eE][-+][0-9]+)? \\([0-9]+\\)[ ]*$\"\n    started matching near: BM_Complexity_O1/1                            14 ns         14 ns    1006981\n```\nSo i'm not sure we can do that simplification.. Hm no, i was thinking about something else there it seems... Hmm, right, sorry.\nI think thisrun_namewill be kinda broken for these range-based complexity measurements... Thanks!\nI think i was thinking along the lines that looking up inset()would be better,\nbut i guess we don't really care here.. I *think* you simply need toinclude(GNUInstallDirs)And do s/BENCHMARK/CMAKE/ here\n(and drop those oldBENCHMARK_INSTALL_*variables). I thinkBENCHMARK_INSTALL_PKGCONFIGis${CMAKE_INSTALL_LIBDIR}/lib/pkgconfig,\nsame withBENCHMARK_INSTALL_CMAKEDIR. The deeper problem here is that one more abstraction layer is missing.\nThisnot_in_the_first_repetition/first_repetitionshould simply behas_explicit_iteration_count.. Yes, i don't like this, too.\nThe current approach (complex constructor) seems better,\nbecause we really wouldn't want to re-run re-use\nthe sameBenchmarkRunnerinstance more than once.. Ah, putting them intobenchmark_runner.hworks too.\nBut specifically into header, *not* the.cc!. I think even the current non-constvariant is not what i wanted..\nhttps://godbolt.org/z/7ALmcF. Nit:1isint,timeisdouble.. Hm, why change theiterationprint format?. Have you considered letting the format string figure out the necessary padding at least?\n(seeman 3 vsprintf). I'll double-check if this can be achieved with the format specifiers,\nbut that new extra width looks worrying :(\nI don't think we can do anything about it though; i like this in general.. Does this put a limit on the length of cpu number?\n. That is why there is--benchmark_report_aggregates_only=, and i have added--benchmark_display_aggregates_only=:). They don't interact **directly**. They are separate.\n*benchmark_report_aggregates_only- for all the reporters, only output the aggregates\n*benchmark_display_aggregates_only- for display reporter, only print the aggregates; file reporter will still contain both the non-aggregates, and aggregates.\n*benchmark_report_separate_iterations- when benchmarking, don't do one huge measurement (spanningniterations), and then divide it by iteration count. instead, do all theseniterations separately. I.e. do no implicit averaging of measurements.. I can squash it intoget_results(). You wouldn't be able to return data from ctor..\nIf we really don't like virtual functions here, i guess i could _try_ CRTP.. No.\nIf there were repetitions, the stddev aggregate may (will) be shockingly different between--benchmark_report_aggregates_only --benchmark_report_separate_iterationsand--benchmark_report_aggregates_only`.\nThinking about it, maybe --benchmark_report_separate_iterations isn't such a great name.\nreport looks out of place.. I can clearly see that it confuses you here.\nIt has different meaning as compared to to benchmark_report_aggregates_only / benchmark_display_aggregates_only.. Certainly not --benchmark_max_iterations=1.\nIt looks way too similar to benchmark_min_time, while having radically different meaning.\n--benchmark_report_every_iteration still looks quite similar to the current --benchmark_report_separate_iterations.\nSo i guess --benchmark_average_over_iterations / --benchmark_one_iteration_per_run\nis the closest hit.\nMaybe --benchmark_average_over_iterations.. CRTP it is.. I'm having a hard time imagining it..\nHow will it work in the target, more complex, case of #710:\nc++\nstruct SeparateIterationResults final : public AbstractIterationResults {\n  // *Append* the new data.\n  void new_result(internal::ThreadManager::Result result,\n                  const benchmark::internal::BenchmarkInstance& b) final {\n    append(result, b);\n  }\n};. > ugh. really?\nThe previous review note was about the comment \"do not call me from constructor,\nsince i call virtual method\". Now that problem is gone, since there are no more virtual calls.\n\ni think the issue i have is that i don't really understand the problem that you're trying to solve (still).\n\nI agree.\n\nwhich is completely my fault for not being able to focus on this with you.\n\nNo, probably largely a my fault, failing to explain what this is doing.\n\nI think you're trying to get to the point where every iteration is used when we have repetitions to avoid having statistics at the repetition level that apply to averages across iterations. Is that correct?\n\nLet me try to highlight the issue i'm trying to solve:\nhttps://docs.google.com/spreadsheets/d/11aEs8L8c9SciHAfs30XMNyea2LEBCil8enFRAVVlvzg/edit?usp=sharing\nI.e. i want to avoid this implicit averaging within the each repetition,\nand keep all the measurements separate, and thus e.g. get the correct stddev,\nNow, this is slightly more problematic to do than that, because we don't have those\nN separate measurements, we just keep the timer running over all the N iterations.\n\nIt also seems like you're trying to do this in a way that is optional. Is that correct?\n\nCorrect. I really really really don't want to pessimize the existing BenchmarkRunner.\nThere are very good reasons for it's current behavior.\nThe new alternative variant (#710) by-design has more overhead.. This is the line that is supposed to fix it.\nI think you should instead make sure it expands to some this is unreachable your compiler understands.. Have you tried changing that to\n```\nif defined(GNUC) || __has_builtin(__builtin_unreachable) || defined(WHATEVER_NVCC_MACRO)\n#define BENCHMARK_UNREACHABLE() __builtin_unreachable()\nelif defined(_MSC_VER)\n#define BENCHMARK_UNREACHABLE() __assume(false)\nelse\n#define BENCHMARK_UNREACHABLE() ((void)0)\nendif\n?. Only if `__has_builtin` wasn't expanded to `false` itself, since it is also non-standard.. Can you please show the compiler output if you change that to:   ?diff\n-#if defined(GNUC) || has_builtin(__builtin_unreachable)\n+#if defined(__GNUC) || has_builtin(__builtin_unreachable) || defined(__NVCC)\n  #define BENCHMARK_UNREACHABLE() __builtin_unreachable()\nelif defined(_MSC_VER)\n#define BENCHMARK_UNREACHABLE() __assume(false)\nelse\n#define BENCHMARK_UNREACHABLE() ((void)0)\nendif\n. I said *show*.\nAnyway this current patch will cause compiler warnings for other compilers,\nthat is why the current approach with `BENCHMARK_UNREACHABLE()` was chosen.\nI don't know/have nvcc, and it does not exist on godbolt.org,\nso i don't know whether or not it has a similar construct.. Comment outdated. Should the zero be treated differently?. I'm not sure this is equivalent\nhttps://www.wolframalpha.com/input/?i=(t0+*+n0+%2B+t1+*+n1)+%2F+(n0*n0+%2B+n1*n1)+%3D%3D+(t0+%2B+t1)+%2F+(n0+%2B+n1). I'm not sure, is this still needed now that no alignment happens?. FWIW i'm somewhat sure that this is the correct (other than fp errors) implementation of [\"Least squares fitting\"](https://en.wikipedia.org/wiki/Least_squares#Least_squares,_regression_analysis_and_statistics)\nThis function is named `MinimalLeastSq`, and has a square in the denominator; in the RHS of the diff, there is no more square... Linewrap. Is this printed only for the json dumper, or for all of them?\nPut differently, i don't think it should be in the console output, it is getting rather crowded already.. Just use `HOST_NAME_MAX`. Again, why invent a magic number.\n`MAX_COMPUTERNAME_LENGTH+1` ?\nAnd that being said, isn't that a rather too big to have as an array on stack?\nMaybe use resized `std::string`?. What is \"system name\"?\nHow is this different from \"host name\"?\nIf it is exactly the \"host name\", just use that term?. Please format the changes..\n$ autopep8 -ia tools/compare.py tools/gbench/report.py tools/gbench/util.py\n```. This is bad.\nNow there are two different functions that should only differ with the output format.\nMeaning, every change will need to happen in two places.\nAnd extra attention will need to be paid to prevent them from diverging.\nInstead, get_json_difference_report() should be the only one,\nand generate_difference_report() should be fed the output of get_json_difference_report(),\nand it should just pretty-print it.. Can you please run the autopep on git master and submit it as a pr, so this pr will be formatting-change-only clean?. I agree that the old hardcoding of 0.05 is wrong, but this is not correct either.\nThe specified utest_alpha (passed to generate_difference_report()) should be propagated/used.\nIt would be really great if you could submit this as a preparatory PR, to unclutter this PR :). Hmm, weird, that didn't linewrap (80 col) these lines.. What does stable_results mean here? Is this about the pvalue? Or repetition count?\nPlease s/stable_results/sufficient_repetitions/\nThat is still not great, but slightly less so... I think these print_utest() changes can count as a separate preparatory NFC refactoring,\ncould you please split this into separate PR?. Magic numbers are fun, aren't they? :)\nAlthough this is also 0.05, this is not UTEST_DEFAULT_ALPHA,\nthese are used to color the change of the timers, note the elif.. Ok, i think this can be changed.. Ah wait, I'm not sure it is a good idea to change this.\nThis is a test, and it is very specifically testing what happens with utest_alpha=0.05.\nIf you pass UTEST_DEFAULT_ALPHA, and then change it, the test will need updating.. Same.. timings_time and timings_cpu are not used in this function, only the len(timings_cpu[0]), len(timings_cpu[1]).\nHow about you pass partition into the calc_utest(), and make it additionally return those lenghts?. Hm, given that there already is UTEST_OPTIMAL_REPETITIONS, i guess have_optimal_repetitions would be slightly better.. I have re-read the diff, and i'm not sure what is cleaner.\nOut of all the places where 0.05 is used, only one can be replaced with UTEST_DEFAULT_ALPHA.\nSo yes, the situation will not be great either way. I'd undo these UTEST_DEFAULT_ALPHA changes. :/. I'm not sure stable is correct word here.\nIt should be sufficient_observation_count.\nIt looks like they say the same thing, but the meaning is slightly different.. Does this work? Is this already done elsewhere in the library?\nwchar is bigger than char, what makes sure that this isn't lossy?. Can't you just use HOST_NAME_MAX directly, why do you need COUNT variable?. And others? https://github.com/google/benchmark/blob/439d6b1c2a6da5cb6adc4c4dfc555af235722396/src/internal_macros.h#L39-L73. Are you 100.0% sure this is safe?\nhttps://docs.microsoft.com/en-us/windows/desktop/api/winbase/nf-winbase-getcomputernamew\n\nOn input, specifies the size of the buffer, in TCHARs. On output, the number of TCHARs copied to the destination buffer, not including the terminating null character.\n\nI'd recommend to use 'returned' DWCOUNT.. Also, same comment about DWCOUNT.. ```suggestion\nelse // defined(BENCHMARK_OS_WINDOWS)\n// Catch-all POSIX block.\n.suggestion\nendif // Catch-all POSIX block.\n``. 1. We are sure that this won't contain\\? (seeexecutable_name)\n2. Do we want to print\"Unable to Get Host Name\"?2\n3. I wonder if this should be after theexecutable_name..sufficient` has a very direct meaning based on https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html\n\nUse only when the number of observation in each sample is > 20\n\nWhat is the definition of optimal, stable in this case?\nWe don't know that having less/more repetitions would have degraded the u test meaningfulness... Aren't we still always printing it?. Looks about right other than this.. Here and elsewhere - threads is never(?) auto-detected, thus it is not some %int, but should instead be explicitly checked.\n. Thanks.\nSame for repetitions, i 'forgot' to mention it here.. @dominichamon @EricWF any better ideas for the name of this field?\n(see my previous comments in this PR). That doesn't look right, it will print 132414 cycles instead of 132 kcycles e.g?. Here and elsewhere:\nThis \"if custom units exist - use it, else use some hardcoded default\" is kinda ugly,\nand is pretty much guaranteed to be got wrong somewhere.\nInstead, it should always use custom time units, which is by default just the current seconds.\nNow that you put it like that, i guess i'm no longer fully convinced that it is separate from the more custom timers work.. \n. What i meant is, there should not be customized_units field.\nThe units should always be customized, with the default matching the current output.. Thanks. Need to try to review this... I do not think it is correct to call project() before cmake_policy().\nCan you instead swap these two around, and add CMP0048 into this loop?. I'm not sure we want to spell Manual here.\nI understand that now it only makes sense for manual time,\nbut if i do proceed into custom timers, it will be needed for non-manual time,\nand i think we'll end up with either two identical functions with and without Manual,\nor forever stuck with this function.. Same here, i'd highly prefer to avoid spelling Manual.. But that is not really what the Cost Function does, isn't it?\nIt converts the given custom measurement to the seconds.. zero-init at least maybe?. CHECK(to_time_cost_in_seconds != nullptr);. @dominichamon isn't BenchmarkTime a little bit too vague?\nHow do i know if it is not the actual time, but actually a BenchmarkTimerDsc?\nHow about using the latter name?. (i'll be ok with either BenchmarkTimerDsc or BenchmarkTimeDsc, just not with BenchmarkTime.). Hmm, i think i mis-commented.\nThe Cost Function returns the \"cost\" of a single\n benchmark iteration's in seconds, to be used to determine the cost\n of a single run of the benchmark max iteration estimation and so on.\nMy question was about the word single.\nWe don't have any such guarantees, do we?\nI think it will always end up being called for whatever the timer has counted,\nwithout division by iteration count, i think.. Oh yeah, this is C++03 header :/\ntime(/*nullptr*/0), then. That is better, thanks.\nI really would prefer ...Dsc, but if @dominichamon does not share this view... Description\nBenchmarkTime (and maaaybe BenchmarkTimeInfo) to me read as too vague,\ni can't tell whether it's not supposed to be holding the actual time of the benchmark,\nbut the description of the measurements taken, how to interpret them, print them.\nHopefully that makes sense?. Yep, any of these will be fine by me.. To be noted, other than the fact that that it would be outside of google benchmark,\nfor such measurements of code snippets that lack branching (in the final asm),\nyou are MUCH better off using \nhttps://llvm.org/docs/CommandGuide/llvm-mca.html#using-markers-to-analyze-specific-code-blocks\nor\nhttps://llvm.org/docs/CommandGuide/llvm-exegesis.html + -snippets-file=\nThough you won't really be able to integrate that in any way into the report\nsince, well, yeah, you want measurements in cycles, and library wants seconds.\nSo i suspect there is some merit to this pr.. Please use clang-format. I don't think this is the right place for these tests.\nMaybe it should go to test/complexity_test.cc.. Hmm yeah, i think now tests do what they should.\n. I'm pretty sure C++11-capable compiler is already required to build the google benchmark, so this isn't needed.. That's ... interesting.\nHonestly, i'm not sure this will always work.\nCan you instead stick the declaration into some header?\nsrc/complexity.h perhaps?. I've looked, and i'm not sure this is the right thing to do, assuming i'm understanding the problem being solved here.\nYou want to show the run name with everything EXCEPT the substring that was 'added' by ->Range(1, 2);, correct?\nI think, this is where all those substrings are being added to the run name:\nhttps://github.com/google/benchmark/blob/c9f2693ea97e94c8afcefb57d3074c6a6236ca23/src/benchmark_register.cc#L172-L210\nIn particular, args (range) are being added here: https://github.com/google/benchmark/blob/c9f2693ea97e94c8afcefb57d3074c6a6236ca23/src/benchmark_register.cc#L174-L189\nAnd after that, more stuff is being added: https://github.com/google/benchmark/blob/c9f2693ea97e94c8afcefb57d3074c6a6236ca23/src/benchmark_register.cc#L191-L210\nSo in other words, i'm not sure it is given that the args/range is \"after the last /\".\nAm i wrong?. It probably will work. I'm just conflicted about the fact that the function you want to call\nin the test, and the actual implementation of the function don't have the same declaration.\nMaybe that is ok/allowed, don't know. Looks like a code smell at least.. Yeah, i was afraid to be right yet again :(\nNot sure what is the best path forward for that.\nI would refactor that as a class (putting this code into a function),\nadd an enum with all the substring types (arg, min_time, repeats, ...),\nmake said function take a parameter (with that enum type)\nand only append the substring types specified in that param.\nAnd call this function from the original place with type ALL,\nand from here with ALL & ~arg.\nThat would probably be least fragile.. field_name should probably be field_content. (You are looking at an old diff). Does this have to be in the public header?\nI think this is an internal functionality.. ",
    "jupp0r": "As a starting point, this is what I'm using for prometheus-cpp (hard coded to C++11s regex engine, because I require c++11 anyway):\ncc_library(\n    name = \"googlebenchmark\",\n    srcs = glob([\"src/*.cc\"],\n                exclude = [\"src/re_posix.cc\", \"src/gnuregex.cc\"]),\n    hdrs = glob([\"src/*.h\", \"include/benchmark/*.h\"],\n                exclude = [\"src/re_posix.h\", \"src/gnuregex.h\"]),\n    includes = [\n         \"include\",\n    ],\n    visibility = [\"//visibility:public\"],\n    copts = [\n          \"-DHAVE_STD_REGEX\"\n    ],\n)\n. ",
    "ChrisCummins": "Thanks @jupp0r !. ",
    "ffreling": "I think this issue has been fixed in version v1.4.0\nhttps://github.com/google/benchmark/releases/tag/v1.4.0. ",
    "NewProggie": "I signed it!\n. Gathering all times for all the runs before setting a time unit and reporting it would be more work I guess. Furthermore one would not see any measured times before the last run for a benchmark suite has finished (which might take a longer time). This is unfortunate.\n. By the way: I've noticed that the code formatting is inconsistent in several areas, but didn't want to pollute  this pull request with reformatting. I could provide an appropriate clang-format configuration (basically: use Style: Google) and open another pull request for this?\n. Thanks as well. I've learned some new things along the way ;-)\n. Ah, interesting. Say, for example the measured values are above a certain number the unit should switch to the next greater one (from ns -> us -> ms -> s)? The unit in the header row would be removed then. I would propose a time unit range from nanoseconds up to seconds (we can safely ditch the planck time here, for sure :-) )\n. Yes, indeed.\nIn order to stay backwards compatible, I'd suggest to keep the default behaviour the way it is (all values in ns). If there shouldn't be another commandline flag, we could just add more rows for ms and seconds besides the already existing ns row. In this case at-a-glance comparisons would still be possible for one unit. Otherwise I could change the commandline flag (as well as the implementation) to move units directly to the output.\n. This comment is unclear to me, since csv and json reporter currently don't display the time unit. Nevertheless I moved the function to BenchmarkReporter as you suggested.\n. Actually, yes. I was using SleepForMilliseconds in my test in order to get appropriate timings for each time unit.\n. Yes, this is unfortunate. Maybe we should use a different approach here, such as a bad implemented fibonacci function which generates the work load and ditch the SleepForMilliseconds inside the test alltogether?\n. ",
    "mattkretz": "\nit'd be nice to have some output other than warnings while the benchmarks are running\n\nI agree. However, if this is an issue, don't the JSON and CSV reporters have the same issue? (Assuming the use in all cases is via redirection to file.)\nMy opinion: do one of\n1. Create a general progress report output on stderr\n2. Have json, csv, and html write to a specified file and do progress reporting on stdout via the console reporter\nBoth solutions seem like a separate issue to solve.\n\nit doesn't really make sense to combine the graphs for all the benchmarks given the wide range of times and arguments, and the different meanings of the arguments, across benchmarks.\n\nThis seems hard / impossible to solve in a general way. The best presentation depends a lot on the benchmark itself. Consequently, one would need many configuration options to influence the way the data is presented. The motivation behind the current HTML reporter was to have a quick and easy path to some kind of charting. If you want fully flexibility use CSV or JSON and a dedicated charting program. This extra step is too much for the quick and easy charting use case.\nWhat is really handy is to use the HTML reporter together with the regex filter to generate separate charts of the results you want to compare.\nThat said, there sure may be a better one size fits all default than the current one.\n- Given e.g. three benchmarks without x/y ranges, should those really have their own graph? They would each have a single bar, no?\n- So it seems the HTML reporter must combine multiple benchmarks with different names into the same chart. Can you think of any discriminator that the reporter could use to split benchmarks into different charts? Currently, it only splits via existence of an x range into line chart vs. bar chart.\n. Hi @WilliamTambellini I'd love to get this merged. Currently, my priorities don't leave room for finalizing the PR. I'd be happy to lend a helping hand for anyone willing to see this through.. I hope you can count this as not being \"original source code\", wrt. the CLA. I can get a CLA signed by my  company. But I expect at least 3 months, likely longer delay for this trivial change.. > checking with our CLA experts to see what's possible.\nI guess it's faster/easier if I report an issue with a suggestion how to fix \nit? ;-)\n. ",
    "OwenArnold": "@dominichamon Thanks. I think this'll do the trick.\n. Excellent. I think this explains things well.\n. ",
    "romanlarionov": "I second this. It would be cool if I could test compute kernels for either OpenCL or CUDA with this. \n. ",
    "jknuuttila": "I suppose there is no need to disable the default timing measurement as such, it's just that replacing  the value of seconds in RunBenchmark with the manual time seems to be the easiest way to integrate properly with the bytes/items processed functionality and the iteration count functionality, which are both useful for GPU workloads. Otherwise, some of that code would need to be duplicated or made more complicated somehow.\n. I have created a pull request that implements this feature at https://github.com/google/benchmark/pull/199\nUsage currently looks like this\n``` c++\nstatic void BM_ManualTiming(benchmark::State& state) {\n  int microseconds = state.range_x();\n  std::chrono::duration sleep_duration {\n    static_cast(microseconds)\n  };\nwhile (state.KeepRunning()) {\n    auto start = std::chrono::high_resolution_clock::now();\n    // Simulate some useful workload with a sleep\n    std::this_thread::sleep_for(sleep_duration);\n    auto end   = std::chrono::high_resolution_clock::now();\nauto elapsed_seconds =\n  std::chrono::duration_cast<std::chrono::duration<double>>(\n    end - start);\n\nstate.SetIterationTime(elapsed_seconds.count());\n\n}\n}\nBENCHMARK(BM_ManualTiming)->Range(1, 1<<17)->UseManualTime();\n```\n. I have implemented support for separate tracking of manual and real times.\nHowever, this patch turned out to be very invasive and hairy, because it affects output formatting and stat computation significantly, and as a result it is very possible that bugs may have been introduced. Indeed, it seems that some automatic check failed (which I noticed when writing this comment), so there is certainly some bug.\nAfter implementing this and seeing how many changes were required, I would now personally strongly prefer the original simpler approach, which is to just use the manual time instead of real-time (requiring no modifications to output and stat computation) when UseManualTime has been specified. Enabling both real time and manual time simultaneously seems like a very niche use case to begin with, and I don't believe it is worth the additional code complexity.\nI would suggest handling the combination of UseManualTime and UseRealTime either with an error message (i.e. the combination is forbidden), or by executing the benchmark once in both modes and reporting them completely separately. If you agree, I can implement this functionality and submit it as a separate pull request. If you would still prefer tracking both separately, I can fix the bugs remaining in this pull request.\n. It seems that I also made some mistake while rebasing the pull request, and as a result commits of other people incorrectly show up as parts of this pull request. Perhaps it would be easiest if I just made a new pull request after it is decided which approach is the best one.\n. Sure! I'll try to revert the branch.\n. I updated the PR again with the simpler version, and this time I hopefully did the rebasing better, too.\nI added a couple of CHECKs to warn against using real time and manual time simultaneously.\n. The comment should now be fixed.\n. I'm not sure. Usually, for my use cases (GPU workloads), the real time doesn't make sense when manual time is required, and I would be interested in having the throughput measured against the manual time.\nHowever, it's possible that there are cases where it could be useful to have both. The console output might get a little cluttered in tabular mode, but I guess that's not a big problem.\n. I can make a new version of the pull request that supports this.\n. You mean the SetItemsProcessed? I put it there to test that the item throughput is calculated according to the manual time, as expected. I can remove it if you think it's a problem.\n. ",
    "StephanDollberg": "Yep, makes sense. \nWhat are those attributes actually used for?\n. Ah, that's cool. Didn't know about them.\n. Awesome thanks, that was quick.\n. ",
    "BillyONeal": "If you have unresolved symbols you probably need to add shlwapi.lib to your linker inputs.\n. @EricWF: Don't know if it is possible to ask cmake to do that. See here:\nhttps://github.com/google/benchmark/blob/6e259170011cd2fbac069b9d4ff83b42445a96cc/src/CMakeLists.txt#L36\nThe dependency on shlwapi.lib gets injected through a .obj that is packaged inside benchmark.lib (compiled from here), so it needs to be present on the linker command line for anyone who links against benchmark.lib.\n. (MSVC++ has #pragma comment(lib, \"shlwapi.lib\"), but I don't believe MinGW supports anything like that; so if folks are using CMake they're better off manually specifying it in their CMakeLists.txt)\n. @EricWF: You can just stick that pragma in sysinfo.cc, and that lib will be added as a dependency for anyone who links against the resulting .obj. Putting it after #include <Shlwapi.h> should work: https://github.com/google/benchmark/blob/95dee3c699e0c972fca9ad414f80e451e0bf4d7f/src/sysinfo.cc#L19 -- but as I said I don't think that will work for MinGW targets.\n. @winstondu In the same target_link_libraries where you add google test, just add shlwapi. I've been doing something like this:\nfunction(add_benchmark BENCHNAME)\n  add_executable(\"${BENCHNAME}\" \"${PROJECT_SOURCE_DIR}/benchmarks/${BENCHNAME}.cpp\")\n  target_link_libraries(\"${BENCHNAME}\" benchmark)\n  if (MSVC)\n    target_link_libraries(\"${BENCHNAME}\" Shlwapi.lib)\n  endif()\n  add_test(\"run_${BENCHNAME}\" \"${BENCHNAME}\" \"${BENCHMARK_OUTPUT_FORMAT}\")\nendfunction(add_benchmark). Sorry, I messed this up somehow. Please ignore for now.\n. @dominichamon I set up Appveyor in my fork and it built successfully here: https://ci.appveyor.com/project/BillyONeal/benchmark/build/4\n. I don't have super easy access to a VS2013 machine, and VS2015 supports conditional noexcept (so it works unmodified with the code in question), so it'll take a while for me to try. The build is already broken so if you don't want to wait for me to setup a VS2013 machine you may as well just merge it :)\n. @dominichamon You may need separate macros; I know in the STL we have _NOEXCEPT for unconditional noexcept and _NOEXCEPT_OP(x) for conditional noexcept\n. Yeah; it's a real shame that AppVeyor is so much slower here than travis-ci :(\n. OK, I got a VS2013 machine to test with. How much do you guys really care about 2013? :)\nFAILED: C:\\PROGRA~2\\MICROS~1.0\\VC\\bin\\cl.exe   /nologo /TP -DHAVE_STD_REGEX -DHAVE_STEADY_CLOCK -D_CRT_SECURE_NO_WARNINGS -I..\\include -I..\\src /DWIN32 /D_WINDOWS  /GR /EHsc /W4 /D_DEBUG /MDd /Zi /Ob0 /Od /RTC1 /showIncludes /Fosrc\\CMakeFiles\\benchmark.dir\\benchmark.cc.obj /Fdsrc\\CMakeFiles\\benchmark.dir\\ /FS -c ..\\src\\benchmark.cc\nC:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\INCLUDE\\xxatomic(229) : error C2664: 'std::_Atomic_address &std::_Atomic_address::operator =(const std::_Atomic_address &)' : cannot convert argument 1 from 'const char *' to 'void *'\n        Conversion loses qualifiers\n        C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\INCLUDE\\xxatomic(227) : while compiling class template member function 'std::atomic<const char *>::atomic(_Ty *) throw()'\n        with\n        [\n            _Ty=const char\n        ]\n        ..\\src\\benchmark.cc(118) : see reference to function template instantiation 'std::atomic<const char *>::atomic(_Ty *) throw()' being compiled\n        with\n        [\n            _Ty=const char\n        ]\n        ..\\src\\benchmark.cc(118) : see reference to class template instantiation 'std::atomic<const char *>' being compiled\n[14/37] Building CXX object src\\CMakeFiles\\benchmark.dir\\colorprint.cc.obj\nninja: build stopped: subcommand failed.\n. If you want I can make a separate PR with changes to fix VS2013\n. @dominichamon How about something like https://github.com/BillyONeal/benchmark/commit/9e0c515c93fada906c3bfe6eb190f20932a393d6 ?\n. Yeah; I have to figure out how to fix the author on those commits first -- I had forgotten to setup my name correctly on the throwaway VS2013 VM where I made these changes :/\n. @dominichamon Do you want me to turn that commit into a PR or are you still pursuing an alternate solution?\n. <atomic> workaround not included here; just the noexcept parts. Note that this turns noexcept on for VS2015.\n. I'm not sure why Clang is unhappy in Travis; it looks like its blowing up in <chrono> and is not a file changed here. Any ideas?\n. OK, we'll see if Travis says the other compilers are happy\n. https://ci.appveyor.com/project/BillyONeal/benchmark/build/45 AppVeyor shows MSVC-12 is happy so far\n. Guess I set that up in my own fork for giggles then :)\n. If there are better ways to do this rather than spamming this out I'm all ears :/\n. This should test _MSC_VER so that it uses conditional noexcept in 2015 or noexcept(false) cases will go to terminate() there.\n. I was concerned about exposing other compilers to the _MSC_VER check; will\n```\nif defined(_MSC_VER) && _MSC_VER <= 1800\n```\nwork?\n. ",
    "winstondu": "@BillyONeal , can you explain to Nooglers who aren't used to using Cmake how I can manually link in Shlwapi into my projects CMakeLists?\nEdit: figured it out. Just added \"shlwapi.lib\" to target_link_library command in my project. (Do add an example line of code, as I went on a wild goose chase to find the physical location of that file not knowing minGW had it on its regular path). I believe I did sign the CLA already. I think I deleted the repo i forked\nwith since I thought this went through.\nI do consent to close this PR and make your own version to make it easier.\nWD\nOn Jan 4, 2018 6:24 PM, \"Eric\" notifications@github.com wrote:\n@winstondu https://github.com/winstondu Can you sign the CLA so I can\nmerge this?\nOr, could you give me permission to close this PR and make my own version?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/pull/503#issuecomment-355428356, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AHy65sgXB8xDRJn49L0tQC_AVU8yg_48ks5tHV2ugaJpZM4RHsfv\n.\n. ",
    "mdavezac": "for example, can we define a custom main in whatever binary you link the benchmarks in\nYou mean MPI users would have to define their own main calling ::benchmark::initialize and ::benchmark::RunspecifiedBenchmarks? That would be easy to do. The con is that it changes the API for MPI users with respect to non-MPI users.\nCan you use a fixture or a custom reporter to calculate the world max time for reporting\nNot so sure about this. The times are used to compute the next number of iterations. This has to match across all processes (more explicitly, state.max_iterations has to be the same across all processes) if the processes are doing any sort of work in concert. Even adding a barrier before calling KeepRunning does not necessarily ensure all the times are the same and hence that the number of iterations is rigorously the same. One could add just sync the number of iterations, but that would lead to less accurate timings (not sure max and mean are commutative). Another option is to use #199, if that is going anywhere. But, frankly, not having to understand the intricacies of timing is one reason I'm using a benchmarking library in the first place. \n. Since #199 has been merged, it is now possible to support benchmarking of mpi code without adding any mpi to benchmark itself.\nAll that would be required is the ability to use a null reporter.\nWould that be a preferred route? \n. It looks like it is already possible to use your own reporter by passing it to RunSpecifiedBenchmarks in a custom main. So everything is already there!\n. @dominichamon, here is a gist with a noddy MPI benchmark. I'm happy to reformat it into the README and do a pull request, but it seems a bit of a long example.\n. That would make it difficult to create a custom main where MPI_Initialize (MPI_Finalize) are called by outside libraries, whenever those libraries do not separate mpi initialization from other initializations. For instance, it would be difficult to benchmark Trilinos.\n. Okay. I'll make changes in that direction.\n. Okay\n. ",
    "Dllieu": "I forgot <functional> won't work for cxx03 users, so another solution will be needed if you think it's worth the implementation\n. That's right, it's a way to do it, but the benefit of accepting a functor would be to use template aliasing for reusability\nIt's a very minor enhancement, regarding cxx03 support, I believe there's not clean way to do it to keep the API simple and user friendly as it is today, so I guess it's more sane to close this request and focus on more important enhancement\n. ",
    "CCob": "Hmm, good question.  My preference would be the entire benchmark but I can think of use cases for an iteration to be dropped too.\n. I think having two functions within state would be beneficial,  \nvoid AbortBenchmark(const char* reason);\nvoid IgnoreIteration();\nOne can be used for terminal errors and abort the benchmark the other can be used for ignoring that particular iteration.\n. Excellent, look forward to testing this out.  Thanks both\n. ",
    "zabereer": "In hindsight I realise it is wrong to try to force a memory constraint on all non-pointer types. \nMany thanks for filing the gcc bug, that is kind of you. \n. I have a CLA for pretoalb@gmail.com.\nMany thanks.\n. There was a rookie error in the pull request I submitted as you pointed out, and the pointer should not be const either (sorry about that). I will fix it tomorrow morning and re-submit if ok with you. I guess this is an interim solution until the compiler issue is resolved? \n. Yes please and thanks very much!\n. Sure, that would be good.\n. Sure I am doing this now.\n. ",
    "ismaelJimenez": "@EricWF Yes, I think the solution you propose should work. The reason why I implemented it like that was to keep consistent with the current implementation of \"ComputeStats\" that was already in place, and I took it as a guideline. As ComputeStats is done inside the reporter I also did the same with \"ComputeBigO\". I would suggest that any refactoring of the code done to BigO calculations is done also to ComputeStats in the same way because I think they are quite similar from the logical point of view (both give a summary report).\nI also thought about doing the complexity report inside \"ReportRuns\", but finaly I decided for a separate function because \"ReportComplexity\" shall only be done at the end of a family of benchmarks and \"ReportRuns\" is called at the end of each benchmark. I didn't want add the logic of complexity reporting into \"ReportRuns\" and kept it as separate function in order to not mess too much with existing code. \nYou said that you are already working on the changes, so I propose that once you have the changes I can have a look at them and check whether everything works OK. Also if I can help you with the changes please just let me know. \nThanks\n. I have just check it (regarding the complexity report) and everything looks fine. I have also taken a look at the refactoring and I really like having ComputeBigO in complexity.cc so it is easier to maintain, because all the logic is at the same place. \nAlso I have just thought, that a future update could be moving ComputeStats into complexity.cc, because the logic of computing stats is an small subset of what ComputeBigO can do (maybe I can also merge both of them). It is like calculating the benchmark executions time with O(1), just getting the mean and stddev. What do you think?\n. I just launched a pull request \"fix issue #235\" that shall fix this issue. Could you please check it? I compiled against gcc 6.1.1 but I could not reproduce the problem, I suppose it might also depend on other configurations.\n. I just launched a pull request that shall fix the hang you are experiencing. The problem you are having is not related to non-integral, as the interface to RangeMultiplier only accepts integers. So, your call to range multiplier with parameter 1.6 is morally equivalent to call it with parameter 1. And the problem was that we did not have a check for this extreme case, that represents an invalid input. With the check I added your call shall fail but not hang.\n. yes, that's right. I have already moved the assert to the AddRange function\n. I wasn't aware about that guideline. It is already fixed.\n. Yes, good catch. I have moved it after the filter to avoid that problem. \n. fixed\n. I changed the indentation, the function names to camel case and the variables to small case separated by \"\". If you find something else please let me know.\n. Yes, on the function ComputeBigO() on the file reporter.cc to populate the vector \"n\". I need arg1 and arg2 to know the length of the test. currently I am only using arg1, because I only use one variable, but in the near future I will use also arg2. This will let me calculate, for example, the time of string concatenation as a function of the length of both strings (N+M or in this case arg1 and arg2), but currently the bigO can only be calculated as a function of arg1.\n. I have changed the color to blue, but I do not have any particular preference. So if you have any preference I can change it to any other color. With respect to the output, I actually only use the CPU, but as on the reports the real time is given I supposed that somebody might be interested on it, maybe for some test where CPU value is not reliable, I don\u00b4t know... I think it does not heart to have it there but you know better than me the users of the tool, so if you think is not useful, I could just remove it.\n. I have suppressed  iteration and time unit for RMS report but on BigO report i kept the time unit, because it is important. The multiplication factor given for the high order term is with respect to the time unit. So, if we change from nano to micro, this factor shall be scaled.\n. Also fixed. Same comment as for json reporter.\n. I have extracted BigO enum and GetBigO in a file. But I am not sure about extracting FittingCurve because this is a local method of the leastSq calculation that nobody else should know about.\n. Hello dominic. I have already added the field SetComplexityN. If there is something else please let me know.\n. fixed\n. Yes, that's right.  \"complexity.h\" is included via \"benchmark_api.h\" (because of the BigO enum definition) and then via \"reporter.h\". As I found that the header was already included via the previous path, I just removed if from the reporter.cc. But, if you think that it will increase readability, I could add it again.\n. Yes, makes perfect sense. I have also added the include to console_reporter.cc, because function \"GetBigOString\" comes also from complexity.h.\nOn a different topic, I am not sure why after changing the doubles to 0.0, coveralls started to complain about coverage decreased.\n. Ok. Please have a look at the last check-in\n. Right. Added it with last check-in.\n. Hello dominic. I removed the space because JSONReporter and CSVReporter classes had no space before \"public:\". So I though that we either add an space on those other to classes or remove the space in this one, to keep consistency. Is there a preferred style?\n. No, it doesn't compile on the c++0x compiler because it generates the error \"function\u2019 in namespace \u2018std\u2019 does not name a template type_ I investigated the problem, and I found out that this happens with versions of g++ less that 4.7 and we have target compilers that are g++4.6. \nIn fact, the first implementation I did was with std::function but I had to change it by the function definition to make it compile on target compilers.\n. This is just a copy paste from reporter_output_test.cc. Then we have the same problem over there.\n. Yes, I think it would be a good idea to abstract the implementation of the logic of reporter_output_test.cc, because it is very useful for testing the output of any benchmark on console, json and csv and I think future developments will benefit from it.\nAt first it seems to me that some things could be moved to a .h file (like a couple of classes and macros), but probably there is a better way to reuse it. I think @EricWF did the reporter_output_test.cc, so may be he can help us on the proper way to restructure it to be reused more efficiently.\n@dominichamon : All the other comments are already fixed. Please check if I missed something. \n. fixed\n. fixed\n. ",
    "nbkolchin": "../external/benchmark/src/console_reporter.cc: In member function 'virtual bool benchmark::ConsoleReporter::ReportContext(const benchmark::BenchmarkReporter::Context&)':\n../external/benchmark/src/console_reporter.cc:41:7: error: 'FLAGS_color_print' was not declared in this scope\n   if (FLAGS_color_print && &std::cout != &GetOutputStream()) {\n. Everything OK. Thank you.\n. Confirmed.\n. ",
    "rtzcoder": "I am not 100% sure this is the same issue as what is reported here but it sounds very similar. In my statically linked application static initialization of a test fixture (constructor) in one of the translation units was executed before libstl was ready, as a result a bunch of normal, well tested code (for example boost path normalization and std altorithms were crashing. For example here is one valgrind output:\n==13359== Invalid read of size 4\n==13359==    at 0x4C8E9D0: std::locale::locale(std::locale const&) (atomicity.h:53)\n==13359==    by 0x479DAB: boost::filesystem::path::codecvt() (path.cpp:918)\n==13359==    by 0x47A257: boost::filesystem::path::begin() const (path.hpp:202)\n==13359==    by 0x47BFBC: boost::filesystem::path::m_normalize() (path.cpp:393)\n...\nI fixed the code by moving my fixture constructor code out and make fixture very plain and then use boolean latch to execute initialization code from the body of the test itself.\nOne way to fix this issue is to delay fixture instantiation until main is entered. At static initialization time static registrar can register a templated factory and fixture can be instantiated with this factory after main is entered.\n. ",
    "Bleizo": "Your pull request compiles like a charm with both gcc 6.1.1 and clang 3.8.0 on my end.\n. ",
    "tahsinH": "perhaps on my part , this was jumping the gun. Stead of suggesting a solution, I should elaborate on the issue I am facing more eloquently.  I understand , the need for the framework not to call the setup code due to expensive setup code overhead. In my real case(not shown here), I actually have an expensive setup. \nBut, on the other side, I also want to ensure that the thing I am measuring is measured correctly. Lets change the pseudocode slightly more.\nst.PauseTiming();\n  ...\n  std::set<std::string> stringSet;\n  for (int int i=1: i< 100000 < i++) {\n    stringSet.insert(getSomePrecalculatedString());\n   }\n  st.ResumeTiming();    \n    for (auto const & id: stringSet) {      \n      m_Table.insert(id.c_str(), 1); // contrived example, all keys have same value;\n    }\nThe first time the  gets called , I am measuring insertion of 100k elements in the unordered map.\nThe second time this is called inside the loop, chances of collision for the hash increases , because the unordered map already has 100k elements in it. So I am actually not getting to measure the correct piece of code, which was to time the insertion of 100k elements on a fresh unordered_map. \nThe only correct way to measure it would be to clear the resource before the next run happens. In my case, this maps out to be the piece of code in teardown.\nIn some x-unit frameworks (granted they were never geared towards performance testing/measurement), I have seen that they have both a Setup() and SetupTestCase(); \nWhere SetUp usually involves the very expensive resource creation; but SetupTestCase is run before every testCase run to still hold on to the resource , but clear it in a way that the the next run would still provide a viable solution. \nPerhaps, adding another hook, where the fixture SetupTestCase is called automatically would create more maintainable, clearer code. Otherwise I am duplicating code in my next testpoint and calling the same code again. \nI am not sure what the correct solution for this would be. But , I hope the issue I am facing is a bit clear in this followup post. \n. sorry for the super late reply. I usually end up using another github account. In anycase,\n@EricWF and @baryluk that's exactly what I am doing right now... \nHowever, me and my colleagues were trying to use this google benchmark tooling as sort of a similar tooling to googletest/cppunit (other sort of x-unit) frameworks. And for the longest time , we were looking a builtin framework support in doing the  same. At the very least we should document this non-feature ; so that other's don't trip over it. Coming from various x-unit testing frameworks, it's a bit of a stumble to realize that this feature isn't there. Although I do agree , making a call (could be expensive). But surely a templated code can be written such that if someone does not call a per-testcase-setup code , the call should be omitted by the compiler/optimizer. \nPlease, close the issue , if it does not seem highly desirable. But please consider adding a note regarding the run-sequence behaviour, since it will stumble people used to general x-unit framework testing. \n  . ",
    "baryluk": "Just create a method in MyFixtureTest to do re-setup that is adequate for your use case?\n. BTW. SetLabel would remain unchanged, and still be printed after all key rates.\n. Yes indeed. I was not aware of that PR, or issue #240 \nThere are few differences in my proposal, i.e. ability to report a rates (not just counters, which doesn't play well when using with automatic iteration count computation or time based iterations), and ability to set options and suffixes for example\n. Honestly, I think this is still not really solved. This should be a default behavior (printing minimum time of the repetitions run), or should require at most one simple flag. Making it few lines of code (plus includes), defeats the purpose, and will make people not use it, and stick to default average (which is almost always bad idea in benchmarks).. ",
    "rychale": "I think I'm missing something. I want to terminate all tests (with some custom message) because of missing file, for example. What is the right way to do that?\n. ",
    "biojppm": "Well, time or manualtime would still be the main counter just as before. Nothing would change in that regard. The idea is adding more counters to each benchmark, with user-defined names.\nI'm putting up a patch for this. There is one question I'd like to raise. Forgive me for some context first. \nI've started by creating a UserCounters class (abbreviated):\n```\nclass UserCounters {\npublic:\ntypedef std::vector< std::pair > container_type;\n  typedef container_type::value_type::first_type    name_type;\n  typedef container_type::const_iterator            const_iterator;\npublic:\nUserCounters(size_t initial_capacity = 24);\nsize_t Add(const char name);\n  void   Set(size_t id, float value);\n  void   Set(const char name,  float value);\nconst_iterator cbegin() const { return counters_.begin(); }\n  const_iterator cend  () const { return counters_.end(); }\nprivate:\ncontainer_type counters_;\n};\n```\nNext I've added to State a UserCounters member and corresponding getter/setters. So the usage on the client side is\nstatic void BM_UserCounter(benchmark::State& state) {\n  static const int depth = 1024;\n  while (state.KeepRunning()) {\n    benchmark::DoNotOptimize(CalculatePi(depth));\n  }\n  state.SetCounter(\"Foo\", 1);\n  state.SetCounter(\"Bar\", 2);\n  state.SetCounter(\"Baz\", 3);\n}\nTo give some context, what is currently shown by the reporter is\n```\nBenchmark                     Time       CPU Iterations\n\nBM_UserCounter/threads:8   3271 ns  10492 ns      51848\nBM_UserCounter/threads:1  10310 ns  10343 ns      70000\n```\nAnd I would like the reporter to show this instead:\n```\nBenchmark                     Time       CPU Iterations   Foo  Bar  Baz\n\nBM_UserCounter/threads:8   3271 ns  10492 ns      51848     1    2    3\nBM_UserCounter/threads:1  10310 ns  10343 ns      70000     1    2    3\n```\nNow I need to relay the counters data in the State object to the BenchmarkReporter::Run report object such that the Reporter has access to them. But the problem is that State and reporter never coexist. Currently, State exists only in the function RunInThread() in benchmark.cc:\n```\nvoid RunInThread(const benchmark::internal::Benchmark::Instance b,\n                 size_t iters, int thread_id,\n                 ThreadStats total) EXCLUDES(GetBenchmarkLock()) {\n  State st(iters, b->has_arg1, b->arg1, b->has_arg2, b->arg2, thread_id, b->threads);\n  b->benchmark->Run(st);\n  //... function continues\n  // st gets destroyed here\n}\nvoid RunBenchmark(...) {\n  //...\n  RunInThread(&b, iters, 0, &total);\n}\n```\nSince RunInThread() is called by RunBenchmark() which is where the BenchmarkReporter::Run report only ever exists, I'm thinking I could change RunInThread() to receive a State object:\n```\nvoid RunInThread(const benchmark::internal::Benchmark::Instance b,\n                 State st, // <--- NOTE\n                 size_t iters, int thread_id,\n                 ThreadStats total) EXCLUDES(GetBenchmarkLock()) {\n  b->benchmark->Run(st);\n  //... function continues\n}\nvoid RunBenchmark(...) {\n  //...\n  // create state here instead\n  State st(iters, b.has_arg1, b.arg1, b.has_arg2, b.arg2, thread_id, b.threads);\n  RunInThread(&b, &st, iters, 0, &total);\n  // ...\n  // now we can use the counters stored in st to report them to Run\n  BenchmarkReporter::Run report;\n  report.counters = st.counters; // <--- here \n}\n```\nSo the question is: would you be fine with this change to RunInThread()? If not, how would you go about this?\n\nSome comments regarding the UserCounters class:\nI went down with plain char for storing the counter names to minimize cache misses when adding counters by name. I used float for storing the counter in order to allow for bigger names, making for a per-counter size of 32 bytes which aligns nicely with the cache. I did this to allow fast lookup/insertion when names are given, as some counters may need to be set in the hot loop. Of course, there's also the even more efficient Set() receiving a size_t handle based of course, but that requires you to add the name to get the handle ahead of running the benchmark.\n. Thanks for the ThreadStats tip, that seems to be what I was looking for. Of course, you're right about passing state to RunInThread(). I started down that avenue and turns out it's just a dead end because of the threads.\nAs for the std::string/double combo, you also have a point, specially regarding accumulation in the KeepRunning loop. I'll change from char array/float to std::string/double.\nRegarding items_processed/bytes_processed: are you suggesting moving these to UserCounters?\n. re items_processed: maybe actually moving all the contents of the UserCounters class into ThreadStats? That would make a lot of sense, in my view.\n. Hi Dominic,\nI'm sorry for the off-time --- work got in the way!\nI'm back and trying to wrap this up, but there's one question and I would like your input, as this intrudes on how the benchmarks run and show their data.\nSince it is now possible to add per-benchmark user counters, we have to consider the fact that different benchmarks have different counters. Up until now this was not a problem as all the existing counter names were known and thus all possibilities could be accounted for, but now anything might go. So the presence of user counters is incompatible with printing the headers before running the benchmarks.\nWhat do you think about fixing this? I could see any of the following possibilities, in order of preference:\n1. Show the current output as the tests are run, without user counters. Then after all tests are complete, find out all the necessary user data columns and print again the results, this time with user counters. So this would print data twice: the first time incomplete (no user data), the second time complete.\n2. Wait for completion of the last benchmark before showing anything.  Then print everything at once. This would essentially show a black screen until all the results are available.\n3. Constrain user counters to be the same for every benchmark, and provide a mechanism to register user data counters before the benchmarks are run. This would allow knowing beforehand what columns should be made available. But is still a usage constraint.\nPersonally I'm inclined to 1, but I would like your input.\nThanks,\nJo\u00e3o\n. gcc 4.6 is failing in the use of override. That is easy to fix. \n(It's also failing in previously existing tests that call std::this_thread::sleep_for().) \nOTOH, there's also PR #261 which apparently will stop requiring compilation with gcc 4.6.\n. > Thank you for sticking with us on this. I understand you've gone back and forth on the API, but it's a big thing to introduce and I want to make sure it's going in the right way.\nNo problem. It's a big addition in a mature project, so I guess it's natural for this back and forth to occur.\n. Eric,\nRegarding your changes in the fixture interface:\n\nThe library will call the new non-const functions which can be overridden by the user, otherwise the default implementation will forward to the old const API.\n\nGreat! I would have gone for this too.\n. Please see whether you agree with the new map-like interface. I will change the C arrays to std after we narrow this API down.\nMeanwhile, I added some benchmarks for accessing counters inside the KeepRunning() loop by id and by name, with the latter being done both through the current API and accelerated with several alternative data structures. These tests lookup two counters out of n:\nc++\n  while (st.KeepRunning()) {\n    st.counters[do_a_lookup(\"Foo\")] += 1.;\n    st.counters[do_a_lookup(\"Bar\")] += 1.;\n  }\nI've put counter sizes from 2 to 256, although in general we will always have small counter numbers (say, less than 16). The tests are the following:\n- (ById) Accessing by id with the current data structure. O(1) in number of counters.\n- (ByName) Accessing by name with the current data structure. O(n).\n- (VsMap) Simulates the counters being stored in an std::map< std::string, Counter >. Note that this prevents access by id. O(log(n)).\n- (NameIdMS) Accessing by name with the current data structure, but the lookup is accelerated with an auxiliary std::map< std::string, size_t >. O(log(n)).\n- (NameIdFS) Accelerated as above, but the accelerator is a sorted std::vector< std::pair< std::string, size_t > > (ie, a flat map) which should provide better cache behaviour than the map. O(log(n)).\n- (NameIdFC) Accelerated with a flat map as above, but the vector uses char[] instead of std::string which should provide even better cache behaviour. O(log(n)).\nThe results are expressive:\n```\nRun on (4 X 3728.84 MHz CPU s)\n2016-08-05 14:47:19\n\nBenchmark                      Time      CPU Iterations\nById_/2        3ns     4 ns  196629213   538.851M items/s\nByName_/2       17ns    18 ns   38043478   104.859M items/s\nVsMap_/2       39ns    34 ns   20348837   55.7648M items/s\nNameIdFC/2       19ns    19 ns   37234043   102.038M items/s\nNameIdFS/2       42ns    40 ns   17500000   47.4128M items/s\nNameIdMS/2       32ns    35 ns   19662921   54.1966M items/s\n\nById_/8        3ns     4 ns  198863636    541.86M items/s\nByName_/8       36ns    40 ns   17676768   48.1654M items/s\nVsMap_/8       55ns    50 ns   13461538   38.2081M items/s\nNameIdFC/8       29ns    28 ns   25362319   68.7142M items/s\nNameIdFS/8       60ns    66 ns   10294118   29.0451M items/s\nNameIdMS/8       45ns    50 ns   14112903   38.2361M items/s\n\nById_/16       4ns     3 ns  201149425   548.089M items/s\nByName_/16      63ns    62 ns   11666667   30.5665M items/s\nVsMap_/16      60ns    65 ns   10937500   29.1363M items/s\nNameIdFC/16      33ns    37 ns   19230769    52.102M items/s\nNameIdFS/16      91ns    83 ns    8333333    22.969M items/s\nNameIdMS/16      61ns    67 ns   10937500   28.4995M items/s\n\nById_/32       3ns     3 ns  198863636   548.125M items/s\nByName_/32     131ns   119 ns    5833333   16.0783M items/s\nVsMap_/32      76ns    76 ns    9210526   25.2409M items/s\nNameIdFC/32      36ns    40 ns   17500000    48.235M items/s\nNameIdFS/32      91ns    99 ns    7000000   19.1831M items/s\nNameIdMS/32      84ns    76 ns    9210526   24.9541M items/s\n\nById_/64       4ns     3 ns  203488372   548.197M items/s\nByName_/64     208ns   228 ns    3070175   8.36556M items/s\nVsMap_/64      97ns    86 ns    7954545   22.1814M items/s\nNameIdFC/64      50ns    49 ns   13461538   38.6684M items/s\nNameIdFS/64     107ns   117 ns    5833333   16.3621M items/s\nNameIdMS/64      97ns    87 ns    7954545    21.799M items/s\n\nById_/128      4ns     3 ns  203488372   551.311M items/s\nByName_/128    491ns   463 ns    1521739   4.12285M items/s\nVsMap_/128     98ns   107 ns    6481481   17.7621M items/s\nNameIdFC/128     60ns    54 ns   12500000    35.269M items/s\nNameIdFS/128    137ns   134 ns    5147059   14.1868M items/s\nNameIdMS/128    101ns   110 ns    6481481    17.363M items/s\n\nById_/256      3ns     4 ns  196629213   538.851M items/s\nByName_/256   1038ns   944 ns     729167   2.02148M items/s\nVsMap_/256    105ns   114 ns    6034483   16.7295M items/s\nNameIdFC/256     72ns    65 ns   10937500      29.3M items/s\nNameIdFS/256    169ns   165 ns    4268293   11.5641M items/s\nNameIdMS/256    106ns   115 ns    6250000   16.5568M items/s\n```\nSome conclusions:\n- (ById) is always fastest, standing head and shoulders above all the other ones. This is expected.\nCompared to (ById) the performance of access by name is always very bad, irrespective of which approach is used. But there are some notable differences among them.\nThere's a change in regime precisely at 16 counters. Up until then (ByName) is best for very small numbers of counters, but is then topped by the flat char array accelerator (NameIdFC). This is always best from here on.\nFor more than 16 counters, surprisingly, the map accelerator (NameIdMS) performs better than the flat std::string accelerator (NameIdFS). But the flat char array NameIdFC) always takes half the time of the other name accelerators; this is an impressive speedup. The map accelerator (NameIdMS) has the same performance as directly storing the counters in a map (VsMap).\nSo all in all, given these results, I would essentially keep the current structure, with both access by id and by name, as it provides greater freedom. Of course, it should be clearly stated that access by name is slow and should be made outside of the KeepRunning() loop. Even this may be possible and be of low impact when the benchmarked code is heavy in comparison.\n. @dominichamon @EricWF \nping...\nJust let me know if you are fine with the current iteration of the interface, or what I should change in it. I'll change back to std after we narrow this down.\n. @EricWF @dominichamon @pleroy \nI finally changed to std::map as you requested. Please tell me now whether this implementation is ok. To reduce API surface I placed some helper functions (previously members of BenchmarkCounters) inside the internal namespace; these are all still defined in counters.cc.\nFor the record, and to make this clear (sometimes it seemed as it wasn't), rolling out custom built containers was not my first choice - at the time, policy apparently was to have no std:: includes in the main benchmark_api.h file - and so that's why I rolled out these containers. Neither was I proposing to keep them in the last iteration - I just wanted to establish whether std::map or std::vector should be used before I proceed to rewrite the code to use std:: containers.\nAs for the array vs map lookup, you made good points. Yes, iterators can be used just the same as size_t handles, and the mental overhead is smaller. So map it is!\n@EricWF I saw your PR #276 as I was finishing this rewrite. When you mention you do not want to hijack my PR, I gather you still want this to go ahead. So that's why I'm still pushing this. If you want me to merge from yours or something, let me know.\n. Glad you're happier with the current iteration. I'm on vacation away from the computer so it'll take me a couple of days to get down to finish this.\nAs for the output format, I don't see what you mean. I assume you're talking about the tabular reporter. I changed this reporter so that the header is printed once, or when the counter set changes from one test to another. So the header should be printed only when needed, and if it isn't then that's a bug.\nDo you mean you want the counter names out of the header? Ie, printed like bytes processed? I did do this in the first implementation, but the counters become unaligned when their set changes and the output is confusing to read (eg, where do the names go? printed on every counter value? at the top they can't go because the set may be different).\n. @EricWF @dominichamon @pleroy I finally got around to make the changes you requested. Output is now all at the end, before items_processed and bytes_processed.\nI would like to further discuss the tabular printing issue, as I'm considering a future PR for this.\nFirst, you certainly have a point in that the tabular format will make runs with few lines more difficult to read. But the situation tends to reverse as the number of lines increases. So I still think that there's a place for tabular output of user counters, specially when a run will contain more than a few lines. Here's an example comparison which I hope shows how much easier it is in the eye. (Note that I removed some whitespace from both examples). \nFirst the current, named-at-the-end version:\n```\nBenchmark                        Time           CPU Iterations UserCounters...\nBM_UserCounter/threads:8      2248 ns      10277 ns      68808 Bar=16 Bat=40 Baz=24 Foo=8\nBM_UserCounter/threads:1      9797 ns       9788 ns      71523 Bar=2 Bat=5 Baz=3 Foo=1024m\nBM_UserCounter/threads:2      4924 ns       9842 ns      71036 Bar=4 Bat=10 Baz=6 Foo=2\nBM_UserCounter/threads:4      2589 ns      10284 ns      68012 Bar=8 Bat=20 Baz=12 Foo=4\nBM_UserCounter/threads:8      2212 ns      10287 ns      68040 Bar=16 Bat=40 Baz=24 Foo=8\nBM_UserCounter/threads:16     1782 ns      10278 ns      68144 Bar=32 Bat=80 Baz=48 Foo=16\nBM_UserCounter/threads:32     1291 ns      10296 ns      68256 Bar=64 Bat=160 Baz=96 Foo=32\nBM_UserCounter/threads:4      2615 ns      10307 ns      68040 Bar=8 Bat=20 Baz=12 Foo=4\nBM_Factorial                    26 ns         26 ns   26608979 40320\nBM_Factorial/real_time          26 ns         26 ns   26587936 40320\nBM_CalculatePiRange/1           16 ns         16 ns   45704255 0\nBM_CalculatePiRange/8           73 ns         73 ns    9520927 3.28374\nBM_CalculatePiRange/64         609 ns        609 ns    1140647 3.15746\nBM_CalculatePiRange/512       4900 ns       4901 ns     142696 3.14355\n```\n... compared to the tabular version:\n```\nBenchmark                        Time           CPU Iterations    Bar   Bat   Baz   Foo\nBM_UserCounter/threads:8      2198 ns       9953 ns      70688     16    40    24     8\nBM_UserCounter/threads:1      9504 ns       9504 ns      73787      2     5     3 1024m\nBM_UserCounter/threads:2      4775 ns       9550 ns      72606      4    10     6     2\nBM_UserCounter/threads:4      2508 ns       9951 ns      70332      8    20    12     4\nBM_UserCounter/threads:8      2055 ns       9933 ns      70344     16    40    24     8\nBM_UserCounter/threads:16     1610 ns       9946 ns      70720     32    80    48    16\nBM_UserCounter/threads:32     1192 ns       9948 ns      70496     64   160    96    32\nBM_UserCounter/threads:4      2506 ns       9949 ns      70332      8    20    12     4\n\nBenchmark                        Time           CPU Iterations\nBM_Factorial                    26 ns         26 ns   26392245 40320\nBM_Factorial/real_time          26 ns         26 ns   26494107 40320\nBM_CalculatePiRange/1           15 ns         15 ns   45571597 0\nBM_CalculatePiRange/8           74 ns         74 ns    9450212 3.28374\nBM_CalculatePiRange/64         595 ns        595 ns    1173901 3.15746\nBM_CalculatePiRange/512       4752 ns       4752 ns     147380 3.14355\nBM_CalculatePiRange/4k       37970 ns      37972 ns      18453 3.14184\nBM_CalculatePiRange/32k     303733 ns     303744 ns       2305 3.14162\nBM_CalculatePiRange/256k   2434095 ns    2434186 ns        288 3.1416\nBM_CalculatePiRange/1024k  9721140 ns    9721413 ns         71 3.14159\nBM_CalculatePi/threads:8      2255 ns       9943 ns      70936\n```\nSo I'd like to ask here if you would consider a future PR giving an option to the user. Default behaviour would still be to print named-at-the-end as you want, but providing a command option --benchmark--counters-tabular or somesuch named option would give us the tabular printing. What do you say to this?\n. @dominichamon @EricWF @pleroy \nLet's finish this.\nI didn't notice till yesterday that there was a conflict. I've merged upstream master into this PR. In the process, I discovered a bug of mine in CsvReporter whereby the header was being printed on every call to ReportRuns(). I fixed it, so be sure to check that out too.\n. @dominichamon yes, I'll do it ASAP. @dominichamon @EricWF I was not able to reproduce the failed tests. currently in Linux. tried in Debug with g++5.4, g++6.2, clang3.8, clang3.9. Tried also repeating the tests to see if it was something intermittent. The tests always passed. Can you give more data on how they failed?\n. This PR tries to make simple numbers more readable, not to remove the m for milli.\nTo be clear, I'll highlight some examples for the current output of HumanReadableNumber:\n0.9  -> 921.6m\n1    -> 1024m\n1.05 -> 1075.2m\nThis is arguably less human readable than the original. Hence this PR, which makes HumanReadableNumber() output 1, 0.9 and 1.05 for the examples above.\nNow, above (line 49), I've used 0.001 as a threshold. This covers the \"m\" range as you point out. I chose this as the 1024 base will also arguably end up making the result less readable. But I accept that here the advantage is less clear. Let's compare the current state vs thresholds of 0.001 and 0.01:\n```\n                         current   |thresh=0.001|thresh=0.01\n\n\nHumanReadableNumber(    0.0)=      0   |        0   |        0 \nHumanReadableNumber(    0.5)=   512m   |      0.5   |      0.5 \nHumanReadableNumber(    0.9)= 921.6m   |      0.9   |      0.9 \nHumanReadableNumber(    1.0)=  1024m   |        1   |        1 \nHumanReadableNumber(   1.05)=1075.2m   |     1.05   |     1.05 \nHumanReadableNumber(    1.1)=    1.1   |      1.1   |      1.1 \nHumanReadableNumber(    1.2)=    1.2   |      1.2   |      1.2 \nHumanReadableNumber( 0.5e-1)=  51.2m   |     0.05   |     0.05 \nHumanReadableNumber( 0.9e-1)= 92.16m   |     0.09   |     0.09 \nHumanReadableNumber( 1.0e-1)= 102.4m   |      0.1   |      0.1 \nHumanReadableNumber(1.05e-1)=107.52m   |    0.105   |    0.105 \nHumanReadableNumber( 1.1e-1)=112.64m   |     0.11   |     0.11 \nHumanReadableNumber( 1.2e-1)=122.88m   |     0.12   |     0.12 \nHumanReadableNumber( 0.5e-3)=524.288u  |  524.288u  |  524.288u\nHumanReadableNumber( 0.9e-3)=943.718u  |  943.718u  |  943.718u\nHumanReadableNumber( 1.0e-3)=1048.58u  |  1048.58u  |  1048.58u\nHumanReadableNumber(1.05e-3)=  1101u   |  0.00105   |    1101u \nHumanReadableNumber( 1.1e-3)=1.1264m   |   0.0011   |  1.1264m <--------\nHumanReadableNumber( 1.2e-3)=1.2288m   |   0.0012   |  1.2288m <--------\n```\nWould you be fine with threshold=0.01? You would still get the millis for values lower than 0.01.\n. FWIW, this PR was motivated by the following situation:\n```\nBenchmark                     Time      CPU Iterations    Bar    Foo\nBM_UserCounter/threads:8   2198 ns  9953 ns      70688     16      8\nBM_UserCounter/threads:1   9504 ns  9504 ns      73787      2  1024m <------- was expecting 1!\nBM_UserCounter/threads:2   4775 ns  9550 ns      72606      4      2\nBM_UserCounter/threads:4   2508 ns  9951 ns      70332      8      4\nBM_UserCounter/threads:8   2055 ns  9933 ns      70344     16      8\nBM_UserCounter/threads:16  1610 ns  9946 ns      70720     32     16\nBM_UserCounter/threads:32  1192 ns  9948 ns      70496     64     32\nBM_UserCounter/threads:4   2506 ns  9949 ns      70332      8      4\n```\nIn the second row, I was expecting the value of Foo to be 1 but I get 1024m...\n. @dominichamon if you think there's no merit to this PR, just let me know and I'll close it. I'm not insisting here -- just trying to see whether you've been short on time to revisit this or just have no interest.\n. Yes I'd say it is usable. Even the csv output will mostly work. As always, bug reports are welcome :-).. Sorry -- please ignore, I will submit again after a proper merge with the new master.. ?? I don't know why the commits from #262 are shown here... really, this PR is only two commits or so from the tip. Anyway.. @EricWF can you give me some pointers in how to do that? I was under the impression that pulling the upstream was enough.. Maybe this is because when I did these two commits, 262 was not yet merged into the upstream? I'm gonna try to make it so that it looks like I did a branch off of today's upstream. If it doesn't work, I'll do the reset as you suggest.. @EricWF It did not work.. Moved this to #350. ok.. Exactly. Just realized I pushed the close button.. @dominichamon @EricWF I finally got down and started doing this. It required adding facilities for getting at actual results from the benchmark. The commit above (6452883027b6642157a54cb5ec61c55c34f4948e) allows the following usage:\nc++\nvoid BM_Counters_Simple(benchmark::State& state) {\n  while (state.KeepRunning()) {\n  }\n  state.counters[\"foo\"] = 1;\n  state.counters[\"bar\"] = 2;\n}\nBENCHMARK(BM_Counters_Simple);\n// make the results of BM_Counters_Simple be parsed, and call the given lambda.\nCHECK_BENCHMARK_RESULTS(\"BM_Counters_Simple\", [](ResultsCheckerEntry const& e) {\n  // Check that \"foo\" read as an int is equal to 1\n  // The EQ is appended to CHECK_ so it turns into a call to CHECK_EQ().\n  CHECK_COUNTER_VALUE(e, int, \"foo\", EQ, 1);\n  CHECK_COUNTER_VALUE(e, int, \"bar\", EQ, 2);\n});\nAre you OK with this approach? I would like to hear your thoughts on 6452883027b6642157a54cb5ec61c55c34f4948e before proceeding.\n. Ok, I've gone ahead and completed it. Submitting a PR now.. ok, closing this. Maybe this is better addressed by #350 ?. @dominichamon Sorry for the late reply. I did see that MinGW fail when I first pushed; I thought it was a spurious fail which I've sometimes seen with the output test.\nLooking more closely, it is indeed something in the output test, and AFAICT it is unrelated to any counter logic; here's the message:\nC:\\projects\\benchmark\\test\\output_test_helper.cc:93: CheckCase: Check `remaining_output.eof() == false' failed. End of output reached before match for regex \"^BM_Repeat/repeats:3 %console_report$\" was found\n    actual regex string \"^BM_Repeat/repeats:3 [ ]*[0-9]{1,5} ns [ ]*[0-9]{1,5} ns [ ]*[0-9]+$\"\n    started matching near: BM_Repeat/repeats:3                            4 ns         -0 ns    1000000\nThis application has requested the Runtime to terminate it in an unusual way.\nPlease contact the application's support team for more information\n. @dominichamon Indeed, the (same) PR is now passing Appveyor's MinGW test.. @dominichamon The previous push happened by accident. This push (or more precisely this push sequence) now completes the PR with unit tests. There may be some details which are so-so (the pragma ignore for -Wunused-function comes to mind, or the // clang format off), but other than that I think that's it.. Hmmm. This thing with the pragma ignore is getting out of hand: now MinGW is complaining. I took the liberty of moving the definition of IsZero() out of the anonymous namespace into benchmark.cc.. Yes, maybe that has something to do with #291. I'll have a look.. ok I stand corrected.. Got it. The RMS time unit was not being forwarded. Commit follows.. @LebedevRI @dominichamon If the test is to be done right, it's a bit complicated as RMS is an indirect result: its value is not set directly anywhere. I have to think and explore the code about how to fake RMS to a known non-zero value (maybe using custom time? ideas are welcome here) so that it can be verified with respect to the time unit. This will take me some time, and I can't do this now as my plate is full. If the correct unit test is really required, just leave this PR hanging and I'll revisit in a couple of weeks. For now @LebedevRI you have the fix (but please confirm).. @vladoovtcharov Thanks for reporting. I will take a look at this.. @vladoovtcharov I think I caught the problem. I'm taking this opportunity to finally get down and write the unit tests for the counters; in the meantime please confirm if this fixes the problem for you.. Good catch, thanks. If you don't mind I'll pull from your repo before submitting my PR. . @vladoovtcharov I think you need to add a CLA for this PR to be accepted, as it contains your commit: https://cla.developers.google.com/. I signed it!. @dominichamon The Appveyor fail is again with MinGW in the reporter output test. This fail is intermittent, and is the same as the one discussed in #350. Apparently, BM_Repeat in MinGW sometimes returns a negative time. I've submitted the issue #382 for this.. Hey @EricWF and @dominichamon \nThe unit tests I wrote for the counters are currently reading their data from the CSV output. This will need to be changed as well. FWIW, if it helps, I can change that logic to JSON.. @LebedevRI Makes sense. I was thinking of the following names for the flags. What do you say?\n```c++\nclass Counter {\n public:\n  enum Flags {\n    kDefaults = 0,\n// Mark the counter as a rate. It will be presented divided\n// by the duration of the benchmark.\nkIsRate = 1,\n\n// Mark the counter as a thread-average quantity. It will be\n// presented divided by the number of threads.\nkAvgThreads = 2,\n\n// Mark the counter as an iteration-average quantity.\n// It will be presented divided by the iteration count.\n// This is incompatible with kMultIters.\nkAvgIters = 4,\n\n// Mark the counter as an iteration-extensive quantity,\n// It will be presented multiplied by the iteration count.\n// This is incompatible with kAvgIters.\nkMultIters = 8,\n\n// Mark the counter as a thread-average rate. See above.\nkAvgThreadsRate = kIsRate | kAvgThreads,\n\n};\n  //...\n};\n```\nOf course, owing to their combinatorial nature, adding more flags raises the problem that it becomes difficult to cover all possible combinations (as the current implementation does). I think it's pointless trying to add an enum entry for each possible combination. So kAvgThreadsRate could also be deprecated.\nBut anyway, this is a problem because currently the Counter ctor takes a Flags argument. We can just ignore this and make the flags parameter to the ctor take an int. OTOH, if this interface is to be retained for compatibility reasons, it probably makes sense to make Flags a typedef int, and turn the enumeration into a different type. Something like this:\nc++\nclass Counter {\n public:\n  using Flags = int;\n  enum { /*...*/ }; // maybe give some name here; open to suggestions.\n  Counter(double v = 0., Flags f=kDefaults) : value(v), flags(f) {}\n};. Yes, you can bit-or the flags and cast the result to the enum type. In fact, if you use the enum type in the ctor, you have to do this to avoid compiler warnings/errors.\nBut I'm sure you will agree that this makes the ctor cumbersome to use when a combination can only be obtained from bit-oring other flags. Making the second argument an int cleanly bypasses this problem.\nHence the int type, which lets you combine flags at will without any need for casting.. OK, the operator works too.. ok, will do.\n. I used std::string as per your suggestion in #240 . I acknowledge that does not mean that it should be exposed in the public header. Would you be OK with a PIMPL idiom here? That would hide the need for including both <vector> and <string> .\n. I went with vector first precisely to minimize API lookup cost: when the user wants, he can store the handle and get O(1) lookup with the handle. Also, when the user does not bother doing that, for small numbers of counters (low dozens) as is likely here, even a linear lookup through the vector will probably be better here because of cache behavior. For example, data here: eg http://scottmeyers.blogspot.pt/2015/09/should-you-be-using-something-instead.html .\nNot that lookup cost is paramount here. But anyway, if despite this you still want a map, say so and I'll change this.\n. ok.\n. These were needed to be able to change the state before and after the benchmark, AND outside of the benchmark. It wouldn't be possible to start or stop the counters in SetUp() and TearDown() because they receive a const reference to State, which is where the counters are. The solutions could be:\n- Remove const from the State argument to the SetUp() and TearDown() of Fixture. I chose not to do this because it was more disruptive to how the public API was. But if this was my own project, I would have gone for this.\n- Make the counters interface mutable in the State class, ie add const to the methods adding counters. I chose not to do this because it would change the const semantics of a significant part of the class.\n- Finally, adding these two methods. This has the advantage of being merely additive to the existing API and does not have the constness problems above.\nIf you prefer a different solution let me know!\n. cool. ok.\n. Good suggestion. I'll move them there. (This happened because in my initial implementation there was no ThreadStats, then when I rolled back it was just quicker).\n. As a usage example, here's a PAPI fixture class. Note that the counters are started and stopped in these new Fixture methods, and not inside the actual benchmark.\n```\ninclude \"benchmark/benchmark_api.h\"\ninclude \"papi.h\"\ninclude \ninclude \ninclude \nclass PAPIFixture : public ::benchmark::Fixture {                                                                      \npublic:                                                                                                                  \nvoid SetUp(const benchmark::State& st) {                                                                             \n    values.resize(st.range_x());                                                                                       \n    sequence.resize(st.range_x());                                                                                     \n    int count = 0;                                                                                                     \n    for(int &i : sequence) i = count++;                                                                                \n    std::random_device rng;                                                                                            \n    std::mt19937 urng(rng());                                                                                          \n    std::shuffle(sequence.begin(), sequence.end(), urng);\n  }\nvoid InitState(benchmark::State &) {\n    if(PAPI_start_counters(&event_ids[0], (int)event_ids.size()) != PAPI_OK) {\n        std::cerr << \"could not start PAPI counters\\n\";\n    }\n  }\n  void TerminateState(benchmark::State &st) {\n    if(PAPI_stop_counters(&events[0], (int)events.size()) != PAPI_OK) {\n        std::cerr << \"could not get PAPI counters\\n\";\n    }\n    for(int i = 0, s = (int)events.size(); i < s; i++) {\n      st.SetCounter(event_names[i], (double)events[i]);\n    }\n  }\nstd::vector< int          > event_ids   = { PAPI_L1_DCM ,  PAPI_L2_DCM ,/  PAPI_L3_DCM ,  PAPI_MEM_SCY ,/  PAPI_FP_INS };\n  std::vector< const char * > event_names = {\"PAPI_L1_DCM\", \"PAPI_L2_DCM\",/ \"PAPI_L3_DCM\", \"PAPI_MEM_SCY\",/ \"PAPI_FP_INS\"};\n  std::vector< long_long    > events      = { 0           ,  0           ,/  0           ,  0            ,/  0           };\n//--------------------------------------------------------------\nfloat reduce_linear() const {\n    float sum = 0.;\n    for(size_t i = 0, s = values.size(); i < s; ++i) {\n      sum += values[i];\n    }\n    return sum;\n  }\n  float reduce_jumps() const {\n    float sum = 0.;\n    for(size_t i = 0, s = values.size(); i < s; ++i) {\n        sum += values[sequence[i]];\n    }\n    return sum;\n  }\nstd::vector< int > values;\n  std::vector< int > sequence;\n};\nBENCHMARK_DEFINE_F(PAPIFixture, ReduceLinear)(benchmark::State& st) {\n  float sum = 0.;\n  while (st.KeepRunning()) {\n    benchmark::DoNotOptimize(sum);\n    sum = reduce_linear();\n  }\n}\nBENCHMARK_DEFINE_F(PAPIFixture, ReduceJumps)(benchmark::State& st) {\n  float sum = 0.;\n  while (st.KeepRunning()) {\n    benchmark::DoNotOptimize(sum);\n    sum = reduce_jumps();\n  }\n}\nBENCHMARK_REGISTER_F(PAPIFixture, ReduceLinear)->Arg(1<<6 );\nBENCHMARK_REGISTER_F(PAPIFixture, ReduceJumps )->Arg(1<<6 );\nBENCHMARK_REGISTER_F(PAPIFixture, ReduceLinear)->Arg(1<<8 );\nBENCHMARK_REGISTER_F(PAPIFixture, ReduceJumps )->Arg(1<<8 );\nBENCHMARK_REGISTER_F(PAPIFixture, ReduceLinear)->Arg(1<<10);\nBENCHMARK_REGISTER_F(PAPIFixture, ReduceJumps )->Arg(1<<10);\nBENCHMARK_REGISTER_F(PAPIFixture, ReduceLinear)->Arg(1<<12);\nBENCHMARK_REGISTER_F(PAPIFixture, ReduceJumps )->Arg(1<<12);\nBENCHMARK_REGISTER_F(PAPIFixture, ReduceLinear)->Arg(1<<20);\nBENCHMARK_REGISTER_F(PAPIFixture, ReduceJumps )->Arg(1<<20);\nBENCHMARK_REGISTER_F(PAPIFixture, ReduceLinear)->Arg(1<<24);\nBENCHMARK_REGISTER_F(PAPIFixture, ReduceJumps )->Arg(1<<24);\nBENCHMARK_MAIN()\n``\n. cool.\n. ok, sorry for that.\n. FWIW, in README.md I added some lines explaining this too.\n. Please confirm :-).\n. ok pimpl it is.\n. I wanted to give choice to clients: id maybe the best way in some cases, but OTOH, it does require the client to store the handle. This is cumbersome for vanilla cases where lookup cost is negligible. So to make things agreeable in both cases, I always return the handle and the user can use or not via the handle overloads,Get(size_t)`. \nSo I think the benefit of efficiently dealing with both these usage scenarios outweighs the API cost of exposing the different Get() overloads. But it's fine if you disagree and prefer one approach only. In that case, I would remove the handle overload instead of forcing it on the user.\n. Just to confirm: are you fine with this?\n. Are you OK with this solution here? \n. It's a bit of a hack.\n. I forgot to mention one other argument in favor of vector that weighed in my initial implementation:\nstd::vector retains insertion order. This way, the counters are shown in the same order as they are added (with the exception of the csv reporter, which has to gather and order them all first).\n. NOTE: this fixes the <vector> include. I ended up using raw pointers instead of PIMPL. This was validated in valgrind and passed all tests so far.\n. Good point.\nThis needs clarification: SetCounter() always changes the value of the counter, whether it is new or existing. The only thing it does not change is the value of its flags (kRate, etc) after the first call for that particular counter. This is intended as changing the type of counter through the flags would not be typical use. \nI'll clarify this in the docs. Maybe this:\n\nNote that setting the counter flags only works on the first call to state.SetCounter(); any subsequent flag values will be silently ignored.\n\nWhat do you think?\n. Nothing really, just what I'm used to. Changing in comming commit.\n. D'oh, of course - sorry. Changing in a comming commit.\n. yes, but as we've talked before (at least before these commits), std::string was not to be included in the public header. Even with PIMPL, these operators could not be defaulted.\n. ok\n. ok\n. ok\n. I put this example here precisely to alert to the cost. Maybe adding a BAD would make it more clear?\n. Fair point. As you were writing this, I was finishing up a commit changing the interface to something map-like. Maybe this way the users will be more alerted to potential costs. Please inspect that and if you still consider the access by name confusing I'll remove it.\nFWIW, I still think that offering access by name is practical for the user, provided he is alert to the cost. For instance, accessing by name inside the KeepRunning loop is not a problem if the code being benchmarked is heavy in comparison and will result in few loop iterations. \n. Yesss :-).  I changed it back and forth a few times already, but ended up removing it to make for a smaller API. Anyway, check the new map-like interface which maybe makes this question moot.\n. Oh well. I just changed it from std::string... And what about the array in BenchmarkCounters - do I change that too and then #include <vector>?\n. ok\n. Actually, I went and changed the API to map-like. With that change, this question no longer exists. Though, of course, new questions may appear. See what you think.\n. ok.\n. D'oh! Indeed, this was meant to go as a separate PR. It got accidentally included as a blanket git add.\n. Would it be ok if I added a counter.h to be included where needed?\n. ok, good point. Indeed you had it like that in your example PR #276 .\n. @EricWF It's currently Flags instead of Flags_e. Do you mean something else?\n. ok.\n. Actually the fixture I wrote (which was removed in the meantime) to benchmark the counter api depended on the non-const state. And counter setup is the main driver behind this particular change. Do you still want it as a separate PR?\n. ok\n. D'oh, this was also accidentally put in. BTW, I take this opportunity to ask: would you be interested in an user-defined counter example with PAPI counters? If so, I'll make a separate PR with this. Otherwise I'll just remove this.\n. Indeed.\n. Actually, it already has an implicit operator=(double): via the implicit conversion to double& below.\n. sure.\n. quite. I admit I was on the fence there.\n. Woops, missed this. Commit coming.\n. @EricWF @dominichamon note the use of the ellipsis, as an indication that the output below is not vertically aligned. Example results:\n```\nBenchmark                                         Time           CPU Iterations UserCounters...\nBM_UserCounter/threads:8                       2266 ns      10283 ns      68080 Bar=16 Bat=40 Baz=24 Foo=8\nBM_UserCounter/threads:1                       9791 ns       9785 ns      71503 Bar=2 Bat=5 Baz=3 Foo=1024m\nBM_UserCounter/threads:2                       4915 ns       9829 ns      71076 Bar=4 Bat=10 Baz=6 Foo=2\nBM_UserCounter/threads:4                       2603 ns      10283 ns      68256 Bar=8 Bat=20 Baz=12 Foo=4\nBM_UserCounter/threads:8                       2152 ns      10263 ns      68440 Bar=16 Bat=40 Baz=24 Foo=8\nBM_UserCounter/threads:16                      1484 ns      10283 ns      68224 Bar=32 Bat=80 Baz=48 Foo=16\nBM_UserCounter/threads:32                       906 ns      10285 ns      68096 Bar=64 Bat=160 Baz=96 Foo=32\nBM_UserCounter/threads:4                       2605 ns      10279 ns      68140 Bar=8 Bat=20 Baz=12 Foo=4\n``\n. Yes, I guess so. Anyway, apparently this diff here is my editor's fault - it adds a newline at the last character if there isn't one.\n. Yes and no. I added the counters (line 27), it sticked out like a sore thumb, and to make it aligned I added one space to the second column.\n. To be clear, what I wanted to say here is that if you find the use of the ellipsis awkward, say so and I'll remove it.\n. @EricWF I can't get this to fire. Can you give more info?. Yes, I too found it strange that you were not using googletest!. Good catch! The magic number here is mainly a hack to prevent overflow when printing the percentage when the check fails. In general, I expect eps_factor to be generally two orders of magnitude larger than this, so that's why I just fixed it here. OTOH, the multiplication by 100 does bring this into the same region as eps_factor. But I still see them as unrelated.. The reporter output tests are expecting the regexIterations$`, so adding a space in the format string would make those tests fail, or the tests would have to be adjusted. For that reason, I chose using the space in \" UserCounters\" as a good balance,. yes! :-P. I've fixed by pragma-ignoring it, is it OK?. Done.. ",
    "ryanvo": "I signed it!\n. ",
    "NAThompson": "So if I understand you correctly, then if std::pow is constexpr, then it could be evaluated at compile time, and hence my benchmark::DoNotOptimize(y = std::pow(1.2, 3.8)); would just compile down to mov xmm0, 1.9993495762998474. But if I didn't put in benchmark::DoNotOptimize, it could throw away to move operation as well?\n. My apologies for alarmism, I forgot to state.SetComplexityN(state.range_x());.\n. I signed it!\n. @dominichamon : Thanks for such great software! This tool has really improved my code!\n. Duplicate\n. I signed it!. Looks like the threads are passed to the AddRange function:\nbenchmark/src/benchmark_register.cc:432:12: error: cannot initialize a parameter of type 'std::vector<int64_t> *'\n      (aka 'vector<long long> *') with an rvalue of type 'std::vector<int> *'\n  AddRange(&thread_counts_, min_threads, max_threads, 2);\n           ^~~~~~~~~~~~~~~\nbenchmark/src/benchmark_register.cc:247:48: note: passing argument to parameter 'dst' here\nvoid Benchmark::AddRange(std::vector<int64_t>* dst, int64_t lo, int64_t hi, int64_t mult)\nSo the type of the threads looks like it must be the same as the type as the argument to the AddRange, or the AddRange function must be templated on an integer. Do you have a preference?\n(The other alternative-that my mental model of this code is unsophisticated and I've totally missed the point-is of course the most likely possibility.)\n. I don't think anyone cares what sort of regex the filter takes; seems a little detailed for a README. But nonetheless, if the analogy is confusing, why don't we just write:\n\nTo run a subset of the benchmarks, use the --benchmark-filter option:\n\nand forget about googletest?\n. I got a bunch of compiler warnings while only changing the Range arguments. But yes it is ridiculous to have 64-bit threadcounts.. I figured the iteration count wouldn't go past 4 billion. It reverts the previous change, which is admittedly annoying.. This fixed a build error. Now either type will work.. J/K, that error went away once I made your other suggested changes.\n. ",
    "steve-downey": "Signed the CLA\n. ",
    "SvenJo": "I signed it! (the CLA)\n. ",
    "loganek": "@dominichamon @EricWF ping\n. @dominichamon I've just rebased to master and resolved conflicts. Please test my changes and let me know if I should do some improvements.\nThanks a lot\n. @dominichamon I agree, think your API is more intuitive. I've amended my patch \n. @dominichamon Do we have any tag for documentation, like doxygen's https://www.stack.nl/~dimitri/doxygen/manual/commands.html#cmddeprecated ?\n. Added assertions\n. Yeah, I agree. Fixed it.\n. I've removed range_x() and range_y().\nAlso, ArgPair() and RangePair() have been removed. \n. ",
    "drozdvadym": "I signed it!\n. ",
    "schorsch1976": "To make it clear: The problem is the assertion at the end.\n. ",
    "agauniyal": "ah, I forgot to say include too. I'll be building for linux atm and will report if there are any errors. Thankyou for quick support \ud83d\ude04 .\n. ",
    "OlegJakushkin": "Solved on renderer side using @dominichamon proposal - multiple file names based solution. Renderer code and documentation, display multiple results section is available on github. Html based (for local .json benchmarks visualisation FireFox is your friend)\nIt can render:\n\u2022 1D data + run-time = 2d lines plot\n\u2022 2D data + run-time = 3d scatter plot\n\u2022 3D+ data + run-time = Parallel Coordinates + results table\n. ",
    "AlexanderSidorenko": "I have created pull request:\nhttps://github.com/google/benchmark/pull/275\nPlease let me know if any additional changes are required.\n. As discussed in pull request (see above), team doesn't see the need to retrofit functionality in question at this stage due to added complexity.\n. I signed CLA. Should go through the system shortly.\n. Will look into build failure due to test failure in the evening when I get my hands on Windows machine.\n. Thanks for clarification.\nIs there anything else that I could help with to make this go through?\n. The difference is subtle - no need to have BENCHMARK_MAIN() in your code, but it has added benefit of being consistent with Google Test. Google test supplies gtest and gtest_main libraries, with latter taking care of main for you.\n. Pascal,\nThe reasoning you provided makes sense to me. Agree let's not introduce it.\nI will close pull request, then.\nThanks!\n. Fixed, thanks.\n. BENCHMARK_MAIN() is currently defined in publicly exposed interface (include/ folder), and it is now required in src/ folder.\nUnfortunately, there is no common place that is shared between src/ and include/: they are decoupled. Otherwise, I would move BENCHMARK_MAIN() there.\nI added comment explaining why it's now needed.\n. ",
    "vmrob": "Thanks! This is exactly the kind of start I needed.\n. For future visitor's reference, here is the open source implementation I arrived at: https://github.com/bittorrent/scraps/commit/94e12b77ccbdfba02790d6b7f38a5a1770600807\n. ",
    "hydroo": "I signed it!\n. Thanks for the info.\n. Is there a clang-format file for this project? I just tried clang-format -style=Google on benchmark_api.h and it changed a lot lot. So that's probably not what you do.\n. When I make test, benchark_test.cc is not executed. Is there a non-manual way to run it? Or how is this intended?\nI add the test to the end of benchark_test.cc, ok?\n. This clang format file seems to be for a clang version newer than 3.8 (Newest Ubuntu).\nError: error: unknown key 'IncludeIsMainRegex'\n. I force pushed my master, since I had troubles with git. I used this opportunity to clean up my commits as well. It's now only two.\n. Thanks!\nLast Saturday I had big troubles getting my code to work in the benchmark harness, again.\nI liked having the barrier there.\nI am not sure what was going on.\nIt would help, if I could acquire shared resources before spawning any threads and pass them.\nIf all threads start the benchmark at the same time, I have to make sure everyone has the correct data.\nStarting the benchmarks via my own main seems to not help with that, since I can only start all benchmarks at the same time, and not individually (I can only acquire all resources before running any benchmark, and release them afterwards).\nCan threads overtake each other in KeepRunning() or is there a barrier between iterations?\nAs for SetIterationTime, I would also like to have the maximum of all threads and the time from the earliest before to the latest after (kind of time maximum time collectively spent in the code).\nSince I benchmark synchronization primitives I want to make sure (1) threads are not migrated during benchmark execution, (2) threads are placed in a sensible way (not 1, 3 on 2 cpus with 2 threads each, e.g.), perhaps I want to even place them individually myself because distances between CPUs affect performance.\nLast time I used pthread_getaffinity_np, pthread_setaffinity_np. Haven't looked into it yet. Maybe there is a newer, nicer way. Also sched_setaffinity exists on linux.\n. ",
    "PSIAlt": "I signed it!\n. Same, 10.11.6. It compiles, but does not pass tests. If I pick one test, it does not work also:\n```\nThe following tests FAILED:\n      1 - benchmark (OTHER_FAULT)\n      2 - filter_simple (OTHER_FAULT)\n      4 - filter_suffix (OTHER_FAULT)\n      6 - filter_regex_all (OTHER_FAULT)\n      8 - filter_regex_blank (OTHER_FAULT)\n     12 - filter_regex_wildcard (OTHER_FAULT)\n     14 - filter_regex_begin (OTHER_FAULT)\n     16 - filter_regex_begin2 (OTHER_FAULT)\n     18 - filter_regex_end (OTHER_FAULT)\n     20 - options_benchmarks (OTHER_FAULT)\n     21 - basic_benchmark (OTHER_FAULT)\n     22 - diagnostics_test (OTHER_FAULT)\n     23 - skip_with_error_test (OTHER_FAULT)\n     25 - fixture_test (OTHER_FAULT)\n     26 - register_benchmark_test (OTHER_FAULT)\n     27 - map_test (OTHER_FAULT)\n     28 - multiple_ranges_test (OTHER_FAULT)\n     29 - reporter_output_test (OTHER_FAULT)\n     30 - cxx03 (OTHER_FAULT)\n     31 - complexity_benchmark (OTHER_FAULT)\nErrors while running CTest\nmake:  [test] Error 8\nAlt ~/repos/google-benchmark $ ./test/benchmark_test \nRun on (4 X 1000 MHz CPU s)\n2016-09-26 23:45:01\nWARNING*** Library was built as DEBUG. Timings may be affected.\nBenchmark                                         Time           CPU Iterations\n-------------------------------------------------------------------------------\ndyld: lazy symbol binding failed: Symbol not found: _clock_gettime\n  Referenced from: /Users/Alt/repos/google-benchmark/./test/benchmark_test\n  Expected in: /usr/lib/libSystem.B.dylib\ndyld: Symbol not found: _clock_gettime\n  Referenced from: /Users/Alt/repos/google-benchmark/./test/benchmark_test\n  Expected in: /usr/lib/libSystem.B.dylib\nTrace/BPT trap: 5\n```\n. ",
    "xaxxon": "My CPU is a 3.1 GHz Core i7 (I7-5557U)\n. maybe steal some of this code?   https://chromium.googlesource.com/chromium/src/+/master/base/process/process_metrics_mac.cc\nSpecifically https://chromium.googlesource.com/chromium/src/+/master/base/process/process_metrics_mac.cc#254\nPersonally, I don't trust google's code, though.  I'd never use anything from them.\n. ",
    "EricBackus": "First, thank you for fixing this and #295 that I submitted.  I really appreciate it!  And I verified that google benchmark builds cleanly and passes its tests on Cygwin.\nI think what's happening is that your CMakeLists.txt explicitly sets -std=c++11.  My understanding is that this disables anything that is not in the standard, and then you have to explicitly turn on anything extra that you want to use.  An alternative would be -std=gnu++11, which turns on lots of stuff by default.  What I don't understand is why this isn't a problem on Linux also.  Or maybe it is and it's just that nobody has complained yet?\nBut anyway, here's the requested output of c++ on Cygwin:\n> c++.exe -dM -E -xc++ /dev/null\ndefine DBL_MIN_EXP (-1021)\ndefine UINT_LEAST16_MAX 0xffff\ndefine **ATOMIC_ACQUIRE 2\ndefine __FLT_MIN** 1.17549435082228750797e-38F\ndefine **GCC_IEC_559_COMPLEX 2\ndefine __UINT_LEAST8_TYPE** unsigned char\ndefine SIZEOF_FLOAT80 16\ndefine **INTMAX_C(c) c ## L\ndefine __CHAR_BIT** 8\ndefine UINT8_MAX 0xff\ndefine WINT_MAX 0xffffffffU\ndefine ORDER_LITTLE_ENDIAN 1234\ndefine SIZE_MAX 0xffffffffffffffffUL\ndefine WCHAR_MAX 0xffff\ndefine **GCC_HAVE_SYNC_COMPARE_AND_SWAP_1 1\ndefine __GCC_HAVE_SYNC_COMPARE_AND_SWAP_2 1\ndefine __GCC_HAVE_SYNC_COMPARE_AND_SWAP_4 1\ndefine __DBL_DENORM_MIN** double(4.94065645841246544177e-324L)\ndefine **GCC_HAVE_SYNC_COMPARE_AND_SWAP_8 1\ndefine __GCC_ATOMIC_CHAR_LOCK_FREE 2\ndefine __GCC_IEC_559 2\ndefine __FLT_EVAL_METHOD** 0\ndefine unix 1\ndefine **cpp_binary_literals 201304\ndefine __GCC_ATOMIC_CHAR32_T_LOCK_FREE 2\ndefine __x86_64 1\ndefine __UINT_FAST64_MAX** 0xffffffffffffffffUL\ndefine SIG_ATOMIC_TYPE int\ndefine DBL_MIN_10_EXP (-307)\ndefine FINITE_MATH_ONLY 0\ndefine GNUC_PATCHLEVEL 0\ndefine UINT_FAST8_MAX 0xff\ndefine has_include(STR) __has_include(STR)\ndefine _stdcall attribute((stdcall))\ndefine DEC64_MAX_EXP 385\ndefine **INT8_C(c) c\ndefine __UINT_LEAST64_MAX** 0xffffffffffffffffUL\ndefine SHRT_MAX 0x7fff\ndefine LDBL_MAX 1.18973149535723176502e+4932L\ndefine UINT_LEAST8_MAX 0xff\ndefine **GCC_ATOMIC_BOOL_LOCK_FREE 2\ndefine __UINTMAX_TYPE** long unsigned int\ndefine DEC32_EPSILON 1E-6DF\ndefine **unix 1\ndefine __UINT32_MAX** 0xffffffffU\ndefine LDBL_MAX_EXP 16384\ndefine WINT_MIN 0U\ndefine SCHAR_MAX 0x7f\ndefine WCHAR_MIN 0\ndefine **INT64_C(c) c ## L\ndefine __DBL_DIG** 15\ndefine **GCC_ATOMIC_POINTER_LOCK_FREE 2\ndefine __SIZEOF_INT** 4\ndefine SIZEOF_POINTER 8\ndefine **GCC_ATOMIC_CHAR16_T_LOCK_FREE 2\ndefine __USER_LABEL_PREFIX**\ndefine STDC_HOSTED 1\ndefine LDBL_HAS_INFINITY 1\ndefine FLT_EPSILON 1.19209289550781250000e-7F\ndefine GXX_WEAK 1\ndefine LDBL_MIN 3.36210314311209350626e-4932L\ndefine DEC32_MAX 9.999999E96DF\ndefine INT32_MAX 0x7fffffff\ndefine SIZEOF_LONG 8\ndefine **UINT16_C(c) c\ndefine __DECIMAL_DIG** 21\ndefine has_include_next(STR) __has_include_next(STR)\ndefine LDBL_HAS_QUIET_NAN 1\ndefine GNUC 5\ndefine _cdecl attribute((cdecl))\ndefine **GXX_RTTI 1\ndefine __MMX** 1\ndefine FLT_HAS_DENORM 1\ndefine SIZEOF_LONG_DOUBLE 16\ndefine BIGGEST_ALIGNMENT 16\ndefine DBL_MAX double(1.79769313486231570815e+308L)\ndefine _thiscall attribute((thiscall))\ndefine INT_FAST32_MAX 0x7fffffffffffffffL\ndefine DBL_HAS_INFINITY 1\ndefine INT64_MAX 0x7fffffffffffffffL\ndefine DEC32_MIN_EXP (-94)\ndefine INT_FAST16_TYPE long int\ndefine _fastcall attribute((fastcall))\ndefine LDBL_HAS_DENORM 1\ndefine **cplusplus 199711L\ndefine __DEC128_MAX** 9.999999999999999999999999999999999E6144DL\ndefine INT_LEAST32_MAX 0x7fffffff\ndefine DEC32_MIN 1E-95DF\ndefine **DEPRECATED 1\ndefine __DBL_MAX_EXP** 1024\ndefine DEC128_EPSILON 1E-33DL\ndefine SSE2_MATH 1\ndefine **ATOMIC_HLE_RELEASE 131072\ndefine __PTRDIFF_MAX** 0x7fffffffffffffffL\ndefine **amd64 1\ndefine __ATOMIC_HLE_ACQUIRE 65536\ndefine __GNUG** 5\ndefine LONG_LONG_MAX 0x7fffffffffffffffLL\ndefine SIZEOF_SIZE_T 8\ndefine SIZEOF_WINT_T 4\ndefine **GXX_ABI_VERSION 1009\ndefine __FLT_MIN_EXP** (-125)\ndefine INT_FAST64_TYPE long int\ndefine DBL_MIN double(2.22507385850720138309e-308L)\ndefine LP64 1\ndefine DECIMAL_BID_FORMAT 1\ndefine **GXX_TYPEINFO_EQUALITY_INLINE 0\ndefine __DEC128_MIN** 1E-6143DL\ndefine REGISTER_PREFIX\ndefine UINT16_MAX 0xffff\ndefine DBL_HAS_DENORM 1\ndefine cdecl __attribute((cdecl))\ndefine UINT8_TYPE unsigned char\ndefine NO_INLINE 1\ndefine FLT_MANT_DIG 24\ndefine VERSION \"5.4.0\"\ndefine **UINT64_C(c) c ## UL\ndefine __GCC_ATOMIC_INT_LOCK_FREE 2\ndefine __FLOAT_WORD_ORDER ORDER_LITTLE_ENDIAN**\ndefine **INT32_C(c) c\ndefine __DEC64_EPSILON** 1E-15DD\ndefine ORDER_PDP_ENDIAN 3412\ndefine DEC128_MIN_EXP (-6142)\ndefine INT_FAST32_TYPE long int\ndefine UINT_LEAST16_TYPE short unsigned int\ndefine unix 1\ndefine INT16_MAX 0x7fff\ndefine **cpp_rtti 199711\ndefine __SIZE_TYPE** long unsigned int\ndefine UINT64_MAX 0xffffffffffffffffUL\ndefine INT8_TYPE signed char\ndefine FLT_RADIX 2\ndefine INT_LEAST16_TYPE short int\ndefine LDBL_EPSILON 1.08420217248550443401e-19L\ndefine **UINTMAX_C(c) c ## UL\ndefine __GLIBCXX_BITSIZE_INT_N_0 128\ndefine __k8 1\ndefine __SEH** 1\ndefine SIG_ATOMIC_MAX 0x7fffffff\ndefine **GCC_ATOMIC_WCHAR_T_LOCK_FREE 2\ndefine __SIZEOF_PTRDIFF_T** 8\ndefine CYGWIN 1\ndefine x86_64 1\ndefine DEC32_SUBNORMAL_MIN 0.000001E-95DF\ndefine INT_FAST16_MAX 0x7fffffffffffffffL\ndefine UINT_FAST32_MAX 0xffffffffffffffffUL\ndefine UINT_LEAST64_TYPE long unsigned int\ndefine FLT_HAS_QUIET_NAN 1\ndefine FLT_MAX_10_EXP 38\ndefine LONG_MAX 0x7fffffffffffffffL\ndefine DEC128_SUBNORMAL_MIN 0.000000000000000000000000000000001E-6143DL\ndefine FLT_HAS_INFINITY 1\ndefine UINT_FAST16_TYPE long unsigned int\ndefine DEC64_MAX 9.999999999999999E384DD\ndefine CHAR16_TYPE short unsigned int\ndefine **PRAGMA_REDEFINE_EXTNAME 1\ndefine __INT_LEAST16_MAX** 0x7fff\ndefine DEC64_MANT_DIG 16\ndefine UINT_LEAST32_MAX 0xffffffffU\ndefine **GCC_ATOMIC_LONG_LOCK_FREE 2\ndefine __INT_LEAST64_TYPE** long int\ndefine INT16_TYPE short int\ndefine INT_LEAST8_TYPE signed char\ndefine DEC32_MAX_EXP 97\ndefine INT_FAST8_MAX 0x7f\ndefine INTPTR_MAX 0x7fffffffffffffffL\ndefine **GXX_MERGED_TYPEINFO_NAMES 0\ndefine __stdcall __attribute((stdcall**))\ndefine SSE2 1\ndefine **EXCEPTIONS 1\ndefine __LDBL_MANT_DIG** 64\ndefine DBL_HAS_QUIET_NAN 1\ndefine SIG_ATOMIC_MIN (-SIG_ATOMIC_MAX - 1)\ndefine k8 1\ndefine INTPTR_TYPE long int\ndefine UINT16_TYPE short unsigned int\ndefine WCHAR_TYPE short unsigned int\ndefine SIZEOF_FLOAT 4\ndefine pic 1\ndefine UINTPTR_MAX 0xffffffffffffffffUL\ndefine DEC64_MIN_EXP (-382)\ndefine INT_FAST64_MAX 0x7fffffffffffffffL\ndefine **GCC_ATOMIC_TEST_AND_SET_TRUEVAL 1\ndefine __FLT_DIG** 6\ndefine UINT_FAST64_TYPE long unsigned int\ndefine INT_MAX 0x7fffffff\ndefine amd64 1\ndefine code_model_medium 1\ndefine INT64_TYPE long int\ndefine FLT_MAX_EXP 128\ndefine ORDER_BIG_ENDIAN 4321\ndefine DBL_MANT_DIG 53\ndefine SIZEOF_FLOAT128 16\ndefine INT_LEAST64_MAX 0x7fffffffffffffffL\ndefine DEC64_MIN 1E-383DD\ndefine WINT_TYPE unsigned int\ndefine UINT_LEAST32_TYPE unsigned int\ndefine SIZEOF_SHORT 2\ndefine SSE 1\ndefine LDBL_MIN_EXP (-16381)\ndefine INT_LEAST8_MAX 0x7f\ndefine SIZEOF_INT128 16\ndefine WCHAR_UNSIGNED 1\ndefine LDBL_MAX_10_EXP 4932\ndefine **ATOMIC_RELAXED 0\ndefine __DBL_EPSILON** double(2.22044604925031308085e-16L)\ndefine thiscall __attribute((thiscall))\ndefine _LP64 1\ndefine __UINT8_C(c) c\ndefine INT_LEAST32_TYPE int\ndefine SIZEOF_WCHAR_T 2\ndefine UINT64_TYPE long unsigned int\ndefine INT_FAST8_TYPE signed char\ndefine fastcall __attribute((fastcall))\ndefine DBL_DECIMAL_DIG 17\ndefine FXSR 1\ndefine DEC_EVAL_METHOD 2\ndefine **cpp_runtime_arrays 198712\ndefine __UINT32_C(c) c ## U\ndefine __INTMAX_MAX** 0x7fffffffffffffffL\ndefine BYTE_ORDER ORDER_LITTLE_ENDIAN\ndefine FLT_DENORM_MIN 1.40129846432481707092e-45F\ndefine INT8_MAX 0x7f\ndefine PIC 1\ndefine UINT_FAST32_TYPE long unsigned int\ndefine CHAR32_TYPE unsigned int\ndefine FLT_MAX 3.40282346638528859812e+38F\ndefine INT32_TYPE int\ndefine SIZEOF_DOUBLE 8\ndefine **cpp_exceptions 199711\ndefine __INTMAX_TYPE** long int\ndefine DEC128_MAX_EXP 6145\ndefine **ATOMIC_CONSUME 1\ndefine __GNUC_MINOR** 4\ndefine **GLIBCXX_TYPE_INT_N_0 __int128\ndefine __UINTMAX_MAX** 0xffffffffffffffffUL\ndefine DEC32_MANT_DIG 7\ndefine DBL_MAX_10_EXP 308\ndefine LDBL_DENORM_MIN 3.64519953188247460253e-4951L\ndefine **INT16_C(c) c\ndefine __STDC** 1\ndefine PTRDIFF_TYPE long int\ndefine **ATOMIC_SEQ_CST 5\ndefine __UINT32_TYPE** unsigned int\ndefine UINTPTR_TYPE long unsigned int\ndefine DEC64_SUBNORMAL_MIN 0.000000000000001E-383DD\ndefine DEC128_MANT_DIG 34\ndefine LDBL_MIN_10_EXP (-4931)\ndefine SSE_MATH 1\ndefine SIZEOF_LONG_LONG 8\ndefine **GCC_ATOMIC_LLONG_LOCK_FREE 2\ndefine __LDBL_DIG** 18\ndefine FLT_DECIMAL_DIG 9\ndefine UINT_FAST16_MAX 0xffffffffffffffffUL\ndefine GNUC_GNU_INLINE 1\ndefine FLT_MIN_10_EXP (-37)\ndefine **GCC_ATOMIC_SHORT_LOCK_FREE 2\ndefine __UINT_FAST8_TYPE** unsigned char\ndefine **ATOMIC_ACQ_REL 4\ndefine __ATOMIC_RELEASE 3\ndefine __declspec(x) __attribute**((x))\n. I'd really like to see this merged into the official release, as it fixes a build issue that we've run into several times within Keysight. Any chance this can be merged in?. ",
    "JoostvanPinxten": "The example that I cooked up is the following:\n``` C++\ninclude \ninclude \nusing vertex_descriptor = long long;\nstruct Item {\n    vertex_descriptor id1, id2;\n};\nstruct Vertex {\n    unsigned int id;\n    Item item;\n    Vertex(int id, Item item) : id(id), item(item) {\n    }\n};\nstatic void BM_graph_minimal(benchmark::State& state) {\n    while (state.KeepRunning()) {\n        vertex_descriptor lastId = 0;\n        std::vector> vertices;\n        for(int i = 0; i < state.range(0); i++) {\n        vertex_descriptor id = lastId++;\n        std::shared_ptr<Vertex> v = std::make_shared<Vertex>(id, Item{1,1});\n        vertices.push_back(v);\n    }\n}\n\n}\nBENCHMARK(BM_graph_minimal)->RangeMultiplier(2)->Range(1<<0, 1<<12)->Complexity();\nBENCHMARK_MAIN()\n```\nThe ID part is not necessary, and the internal Item is also not necessary to reproduce the issue on my machine:\n``` C++\nstruct Vertex {\n    unsigned int id;\n};\nstatic void BM_graph_minimal(benchmark::State& state) {\n    while (state.KeepRunning()) {\n        std::vector> vertices;\n        for(int i = 0; i < state.range(0); i++) {\n        std::shared_ptr<Vertex> v = std::make_shared<Vertex>();\n        vertices.push_back(v);\n    }\n}\n\n}\n```\nNote: I do get the warning:\n***WARNING*** Library was built as DEBUG. Timings may be affected. Could be due to not omitting the frame-pointer?\n. Wow, I completely missed that. Might be worth pointing out that for some unknown reason I was expecting that call to be added (implicitly) to: BENCHMARK(BM_graph_minimal)->RangeMultiplier(2)->Range(1<<0, 1<<12)->Complexity();. \nI expected the first element in the state Range to default to N actually, and wasn't looking for the call inside the benchmark itself. \n. ",
    "ktnyt": "I think I've figured out the problem.\n'check_cxx_compiler_flag' called from line 28 inside 'cmake/AddCXXCompilerFlag.cmake' passes an empty string as the first argument which will probably always result to be successful.\nPassing '${FLAG}' as the first argument should properly invoke 'check_cxx_compiler_flag' as intended and include the flag only when the compiler is capable of handling it.\n. Hello, the PR should have solved the problem and as far as I've checked caffe2 seems to be using the commit that includes this patch. Can you provide me with more information on the status of the build?. ",
    "teleger": "i am chinese,i have a problem ,and the same as your ; you're compiling that project?  i am build caffe2 ...can you provide a detailed solution?  .. thank  you .!!. ",
    "jjones646": "signed CLA\n. Ahh, I was looking at the latest release of the source. Thank you for pointing this out.. ",
    "asuhl-formlabs": "I updated to latest and can't reproduce the problem --- but then again, my times aren't coming out to exact integers anymore, so it's hard to tell.\nThere does still seem to be a regex that requires a decimal point on line 64 of test/reporter_output_test.cc:\nSET_SUBSTITUTIONS({{\"%bigOStr\", \"[ ]*[0-9]+\\\\.[0-9]+ \\\\([0-9]+\\\\)\"},\nI understand this isn't the regex that was causing the error above, but it should probably be changed to not require a decimal point anyway.\n. ",
    "Dekken": "The headers print showing time and iterations, but there's no results given for any tests, and there's no errors, at least printed errors.\n. `static void BM_StringCreation(benchmark::State& state) {\n  while (state.KeepRunning())\n    std::string empty_string;\n}\n// Register the function as a benchmark\nBENCHMARK(BM_StringCreation);\n// Define another benchmark\nstatic void BM_StringCopy(benchmark::State& state) {\n  std::string x = \"hello\";\n  while (state.KeepRunning())\n    std::string copy(x);\n}\nBENCHMARK(BM_StringCopy);\nBENCHMARK_MAIN();`\n. It doesn't happen with GCC 4.9.2 / 5.2 / 6.2 or 7, only 4.8.2 with the RPMs noted.\nNo benchmarks run and it exits immediately after printing the headers.\n. cat /opt/rh/devtoolset-2/enable\n0 exit code, no coredumps.\n. Each time a different compiler is used libbenchmark is recompiled with all the same values as the main binary.\n. Switching to -DHAVE_POSIX_REGEX does indeed resolve this.\n. Is it possible that we could have a warning/error if the regex fails?\n. I support that, I had also hardcoded \"-DHAVE_STD_REGEX\" into my build mechanism, not ever expecting it to be an issue.\n. ",
    "ctiller": "I'm seeing this too with https://github.com/grpc/grpc/pull/8500.\n. --benchmark_list_tests=true also outputs an empty list.\n. ",
    "martijncoenen": "(Relying on BogoMIPS at all is probably not a good idea, since they are bogus MIPS ;))\n. I found this when testing on a Pixel phone; the estimation yields 19.2MHz system clock (which is correct), bogoMIPS says 38MHz, which produces results that are off by a factor of two! Maybe if you only have bogo, run the estimation as well and print a big fat warning if they are significantly different?\nAnyway this seems less important in more recent versions, since clockrate appears to be no longer used for measurements directly.\n. ",
    "vser1": "@EricWF Thanks for the fix!\nI'll test it on my side. In case it works, would it be possible to increase google benchmark version (based on semver, it should be 1.1.1 I guess). I'm allowed to use only tagged versions and I'd really love using the compare_bench. If wanted, I'll be happy to share the integration I'm working on with Gitlab CI!. ",
    "Abai": "Tried it with clang-3.8 on the same system. That works.. ",
    "NiklasRosenstein": "Maybe it is <regex> that is behaving not  as expected. I currently force it to use the std backend with HAVE_STD_REGEX. Will do some tests to check if that could actually be the case.\nAnyway, I figured I might be better off asking this on StackOverflow if testing <regex> doesn't yield any solutions. It's more likely to be just a configuration error on my side rather than a ~~bug in~~ problem with benchmark itself.. I guessed right, it's just a matter of using the right regex backend. I will need update my build script to detect the right backend like the CMake script does, but for now using HAVE_POSIX_REGEX works in this scenario!. I'm writing my own build system, and using it in this project. Plus I don't like CMake all that much.. Very well. Would you agree with me when I'm saying that BENCHMARK_MAIN() should report unrecognized flags? Since it is expanded to a full main function, there's no way you're going to handle argc/argv any further.. It does seem like using gflags for both libraries would make things a bit simpler, especially in the case of using the libraries in a single executable. As far as I can tell they both use their own derivative implementation/interface of gflags, GTEST_DEFINE_... in googletest, and DEFINE_... in benchmark.\n@EricWF Unless these flags would then also be consumed by ParseCommandLineFlags(), it could lead to problems when you implement your own reporting of unrecognised flags (i.e. double reports). My initial comment to this issue has the same problem.\nFor now I'm content with BENCHMARK_MAIN() reporting unrecognised flags, which I think is sensible behaviour. Let me know if I should create a PR for NiklasRosenstein@c8b2f2e. @EricWF Please check out PR #332 . It's necessary if we want to treat unrecognized arguments as fatal errors and exit the program (see return 1; two lines below). If the benchmark should still be executed (which IMHO should not be the case, but other people might have a different opinion about that), we can remove the if and return 1;.\nI also considered adding exit(1); to ReportUnrecognizedArguments() when argc > 1, but I figured it would not be obvious that the function can actually cause the program to exit.. > I don't really understand why this was made into a separate function in the first place as i think it > was much clearer inline (albeit in a macro).\nI agree on that.\n\nbut then again what if someone doesn't want to printf but wants to do something else with the > extra args?\n\nThat's when they don't use BENCHMARK_MAIN().. \u2714\ufe0f . > You could have ReportUnrecognizedArguments(...) return true if there are unrecognized arguments and false otherwise. Then change this to if (ReportUnrecognizedArguments(...)) exit(1);\n\u2714\ufe0f . ",
    "pcampr": "I signed it!. I run the tests using python report.py, which failed even in the master version. I propose a fix for the test (two new testing benchmarks with larger numbers added), and updated formatting string.\nBefore (master version with updated test, but not changed formatting string):\n```\n$ python report.py\nBenchmark                   Time           CPU           Old           New\n\nBM_SameTimes               +0.00         +0.00           10           10\nBM_2xFaster                -0.50         -0.50           50           25\nBM_2xSlower                +1.00         +1.00           50          100\nBM_10PercentFaster         -0.10         -0.10          100           90\nBM_10PercentSlower         +0.10         +0.10          100          110\nBM_100xSlower              +99.00         +99.00          100         10000\nBM_100xFaster              -0.99         -0.99         10000          100\n.\n\nRan 1 test in 0.002s\nOK\n```\nAfter (this PR version):\n```\n$ python report.py\nBenchmark                   Time           CPU           Old           New\n\nBM_SameTimes               +0.00         +0.00            10            10\nBM_2xFaster                -0.50         -0.50            50            25\nBM_2xSlower                +1.00         +1.00            50           100\nBM_10PercentFaster         -0.10         -0.10           100            90\nBM_10PercentSlower         +0.10         +0.10           100           110\nBM_100xSlower             +99.00        +99.00           100         10000\nBM_100xFaster              -0.99         -0.99         10000           100\n.\n\nRan 1 test in 0.002s\nOK\n```. @EricWF thank you for maintaining this great tool!. ",
    "ajpower": "Nevermind, I'm a fool: the benchmark was re-running on already sorted data, which my quicksort algorithm does poorly with.. ",
    "ensonic": "I've tried to run my benchmark as below:\nsudo cpufreq-set -g performance\n./my-bench\nsudo cpufreq-set -g powersave\nbut still get the warning. Any idea?\nIdeally we had a benchmark runner that could change the freq-scaling, but I could not find any.. @dominichamon thanks, yours works, so +1 for adding this to the docs. I've sent a PR.. ",
    "BRevzin": "Submitted a pull request changing that line. . I signed it!. ",
    "kunitoki": "\ud83d\udc4d any idea when this will be pushed upstream ?. I signed it!. already fixed here https://github.com/google/benchmark/pull/364. already reported in #364 and #365 . ",
    "shakeel": "Abseil.io mandates using Bazel for building code, it would be so nice if benchmark would have Bazel BUILD files, it would make the task of using Benchmark http_archive so much easier.. ",
    "g-easy": "cctz inlines a BUILD file using new_http_archive: https://github.com/google/cctz/blob/master/WORKSPACE. ",
    "keir": "Just wanted to add to this bug: Having a supported BUILD would be great, so that you can transitively include Google Benchmark easily in other projects. It seems strictly worse to have a multitude of projects copying and pasting their own forked version of a Google Benchmark BUILD file.\nI am working to integrate Google Benchmark with Ceres Solver, and will end up copying and pasting a BUILD into the WORKSPACE file. This feels unsatisfying, especially since Bazel makes it natural and easy to automatically download and build dependencies.\nWould a patch to add a BUILD file back be accepted?. Now landed: https://github.com/ceres-solver/ceres-solver/commit/22fa21c6f824ebefae12a9a2fa459714907acc57. ",
    "sztomi": "I see. It seems though that it is running the benchmark for a rather long time (I've been running it for around 5 minutes now, which takes a lot longer than the 100 repetitions times 5000 iterations that I used with another benchmarking framework). Is there a way to specify a maximum time for a benchmark?. Thanks, I was a bit confused by what \"minimum time\" meant. However, I'm observing some weird behavior (to me at least): While trying to find a reasonable MinTime value, I found that 0.025 runs in an instant (producing around 5800 iterations), but 0.03 never seems to stop. Shouldn't increasing the mintime increase the run time roughly linearly?. It's very much a work in progress, but hopefully it can be built on other systems: https://github.com/rpclib/benchmarks/tree/gbenchmark\nmkdir build && cd build\nconan install .. --build missing\ncmake ..\nmake\nMost conan packages were only tested on Linux, so I suggest sticking to that. Conan can be installed via pip install conan.. I can't produce a smaller repro right now, unfortunately, but it's not a very complicated code. \nFurthermore, it seems that adding a sleep (to get_answer in target_code.cc) will make the problem disappear. That's a good enough workaround for me. Since I'll likely commit more on this branch, here is the commit that exhibits the behavior:  https://github.com/rpclib/benchmarks/tree/c6a29189fa7290d3d288eba3150176ed01a0cb3a. Thanks for the feedback. All paths should be in /package/include BTW. I can\nget back to you in a few days with a more minimal repro.\nOn Dec 29, 2016 10:16 PM, \"Eric\" notifications@github.com wrote:\n\nActually it seems like all of the generated conan include paths are wrong.\nI'm giving up trying to reproduce this myself.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/330#issuecomment-269694762,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAOr0oX2Ab6nCLFuUMhfIH85zl_38YW0ks5rNCMzgaJpZM4LXy0T\n.\n. Thanks. What I'm going to do is remove the other other libraries from the benchmark, because they are not needed for the repro which will hopefully make the benchmark trivial to build. rpclib is self-contained so it should build easily.. well, I have some doubts about the reproducability of this issue. I tried building the stripped version in a VM with little installed (to make sure it builds elsewhere) and I wasn't able to reproduce it in the VM nor on my physical machine later. If you still wish to take a look, you can find the intended repro here: https://github.com/rpclib/benchmarks/tree/getting-stuck-repro. \n",
    "zhangzhimin": "Thanks, It works well, I didn't  call gpu synchronize before.. ",
    "sieren": "Thanks. Filed a PR at https://github.com/google/benchmark/pull/335. I'll go ahead and close this as the PR got merged. Thanks everyone!. Yeah, I think so. Apple see's TARGET_OS_MAC as a superset of all variants (as documented in targetconditionals.h):\nTARGET_OS_MAC             - Generated code will run under Mac OS X variant\n           TARGET_OS_OSX          - Generated code will run under OS X devices\n           TARGET_OS_IPHONE          - Generated code for firmware, devices, or simulator\n              TARGET_OS_IOS             - Generated code will run under iOS \n              TARGET_OS_TV              - Generated code will run under Apple TV OS\n              TARGET_OS_WATCH           - Generated code will run under Apple Watch OS\n                 TARGET_OS_BRIDGE          - Generated code will run under Bridge devices\n           TARGET_OS_SIMULATOR      - Generated code will run under a simulator\n           TARGET_OS_EMBEDDED       - Generated code for firmware. Unless you think it makes more sense to check for OSX and iOS separately, but I suppose the benchmark codebase will have a bunch of more complex/longer ifdefs then.. ",
    "btorpey": "OK, thanks!. ",
    "grypp": "I signed it!. ",
    "dontsovcmc": "remark: if the library wasn't found, check: CMAKE_FIND_ROOT_PATH_MODE_LIBRARY. ",
    "Maratyszcza": "Fixed #ifdef structure. Also made a minor change to ensure benchmark::DoNotOptimize does not use inline assembly on PNaCl and Emscripten platforms (these platforms do not support inline assembly, and using it results in either link-time errors, or runtime errors).. DoNotOptimize() is different. Here is my use-case:\nstatic void quotient_uint32_t(benchmark::State& state) {\n    const uint32_t divisor = benchmark::MakeUnpredictable(3);\n    uint32_t x = 0;\n    while (state.KeepRunning()) {\n        const uint32_t quotient = x++ / divisor;\n        benchmark::DoNotOptimize(quotient);\n    }\n}\nBENCHMARK(quotient_uint32_t);\nNormally compilers would replace division by an integer known at compile-time with a multiplication. I want to hide the value of the variable divisor from the optimizing compiler, so that it generates a generic hardware division instruction.. I see in the assembly code that it gets compiled into division-via-multiplication sequence, with both Apple Clang 8.0.0 and gcc 6.3.0. Removing const doesn't make a difference.. The difference is I pass divisor, not &divisor no DoNotOptimize. However, the README suggests syntax without ampersand, is it wrong?. Ok, thanks for the solution! Would be good to document it in the README.. Exactly the same problem on an ARMv7 system. g++ -v:\nUsing built-in specs.\nCOLLECT_GCC=g++\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/arm-linux-gnueabihf/4.8/lto-wrapper\nTarget: arm-linux-gnueabihf\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu/Linaro 4.8.4-2ubuntu1~14.04' --with-bugurl=file:///usr/share/doc/gcc-4.8/README.Bugs --enable-languages=c,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-4.8 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --with-gxx-include-dir=/usr/include/c++/4.8 --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --enable-gnu-unique-object --disable-libmudflap --disable-libitm --disable-libquadmath --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-armhf/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-armhf --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-armhf --with-arch-directory=arm --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --enable-multilib --disable-sjlj-exceptions --with-arch=armv7-a --with-fpu=vfpv3-d16 --with-float=hard --with-mode=thumb --disable-werror --enable-checking=release --build=arm-linux-gnueabihf --host=arm-linux-gnueabihf --target=arm-linux-gnueabihf\nThread model: posix\ngcc version 4.8.4 (Ubuntu/Linaro 4.8.4-2ubuntu1~14.04)\n. Here is the preprocessed source: \nThe error shows up when building init-bench target in github.com/Maratyszcza/FXdiv.  Probably it is due to a POD structure being passed to DoNotOptimize: if I pass it by pointer instead, it compiles fine. However, there is another benchmark in this project (divide-bench) where I pass a POD structure to DoNotOptimize, and it compiles fine.. Same error when cross-compiling for Android (using android.toolchain.cmake from Android NDK).. @EricWF android.toolchain.cmake from Android NDK sets this variable (and CMAKE_SYSTEM_PROCESSOR too) automatically.. I changed interface for State.iterations() to return uint64_t, so code that relied on iterations being 64-bit on x86-64 will keep working, and will also work on 32-bit ABIs. In the private classes, I left iterations as uint32_t. It is upper bounded by 1e+9 iterations (kMaxIterations), so this wouldn't break anything on either 32-bit or 64-bit systems.. @dominichamon I changed types in the interface to int64_t, can we merge now? Internally, I left iterations as uint32_t.\n@EricWF Using int would hurt portability. We need a type that can hold values up to 1e+9. uint32_t/int32_t guarantees that, int does not (in fact, INT_MAX is usually just 32767 on 16-bit systems).. I think it is Android-specific problem: its GNU STL library mostly, but not fully C++11-compliant, and I haven't seen similar issues on other platforms. That said, Android is a non-negligible platform.. Android STL lives in Android NDK, apps include pre-compiled STL along with their own code. The problem fixed by this patch is present since ancient times (since C++ got support in Android NDK) and up to and including Android NDK r17.. @dominichamon Please take another look. @dominichamon Added BENCHMARK_STL_ANDROID_GNUSTL macro, and moved implementation into .cc file. PR #612 for a fix. @EricWF CMake 3.5.1 is the latest available in Ubuntu 14.04 and 16.04 (LTS releases).. @dominichamon Ubuntu 14.04 also has cmake3 package, which installs CMake 3.5.1. Both PNaCl and Emscripten use Clang, which defines __GNUC__. However, as they compile to virtual ISA (PNaCl bytecode, Asm.js, or WebAssembly bytecode), they don't support inline assembly.. In case of PNaCl it fails to link, with an explicit message blaming inline assembly:\n[21/29] LINK build/init-bench.bc\nFAILED: /Users/marat/Projects/fxdiv-confu/build/init-bench.bc\n/Users/marat/Tools/nacl_sdk/pepper_49/toolchain/mac_pnacl/bin/pnacl-clang++   -o /Users/marat/Projects/fxdiv-confu/build/init-bench.bc /Users/marat/Projects/fxdiv-confu/build/bench/init.cc.bc /Users/marat/Projects/fxdiv-confu/lib/libgooglebenchmark.a\nFunction _ZL13init_uint32_tRN9benchmark5StateE disallowed: bad result type: %struct.fxdiv_divisor_uint32_t*   %divisor.bc2 = bitcast i8* %divisor to %struct.fxdiv_divisor_uint32_t*\nFunction _ZL13init_uint32_tRN9benchmark5StateE disallowed: inline assembly:    call void asm sideeffect \"\", \"*imr,~{memory}\"(%struct.fxdiv_divisor_uint32_t* %divisor.bc2), !dbg !100433\nIn case of Emscripten/Asm.js, it links, but fails with a strange error at runtime:\n```\nRun on (1 X 1000.15 MHz CPU )\n2017-02-10 21:33:11\nBenchmark                           Time           CPU Iterations\n\nexception thrown: ReferenceError: $5 is not defined,ReferenceError: $5 is not defined\n    at Array.Rb (/Users/marat/Projects/fxdiv-confu/bin/multiply-bench.js:8:434)\n    at Array.Pc (/Users/marat/Projects/fxdiv-confu/bin/multiply-bench.js:8:36040)\n    at mc (/Users/marat/Projects/fxdiv-confu/bin/multiply-bench.js:8:17877)\n    at ic (/Users/marat/Projects/fxdiv-confu/bin/multiply-bench.js:8:10462)\n    at hc (/Users/marat/Projects/fxdiv-confu/bin/multiply-bench.js:8:5620)\n    at Object.Wb [as _main] (/Users/marat/Projects/fxdiv-confu/bin/multiply-bench.js:8:1543)\n    at Object.callMain (/Users/marat/Projects/fxdiv-confu/bin/multiply-bench.js:18:5036)\n    at doRun (/Users/marat/Projects/fxdiv-confu/bin/multiply-bench.js:18:5742)\n    at run (/Users/marat/Projects/fxdiv-confu/bin/multiply-bench.js:18:5910)\n    at Object. (/Users/marat/Projects/fxdiv-confu/bin/multiply-bench.js:18:7050)\n/Users/marat/Projects/fxdiv-confu/bin/multiply-bench.js:1\n(function (exports, require, module, __filename, __dirname) { var Module;if(!Module)Module=(typeof Module!==\"undefined\"?Module:null)||{};var moduleOverrides={};for(var key in Module){if(Module.hasOwnProperty(key)){moduleOverrides[key]=Module[key]}}var ENVIRONMENT_IS_WEB=false;var ENVIRONMENT_IS_WORKER=false;var ENVIRONMENT_IS_NODE=false;var ENVIRONMENT_IS_SHELL=false;if(Module[\"ENVIRONMENT\"]){if(Module[\"ENVIRONMENT\"]===\"WEB\"){ENVIRONMENT_IS_WEB=true}else if(Module[\"ENVIRONMENT\"]===\"WORKER\"){ENVIRONMENT_IS_WORKER=true}else if(Module[\"ENVIRONMENT\"]===\"NODE\"){ENVIRONMENT_IS_NODE=true}else if(Module[\"ENVIRONMENT\"]===\"SHELL\"){ENVIRONMENT_IS_SHELL=true}else{throw new Error(\"The provided Module['ENVIRONMENT'] value is not valid. It must be one of: WEB|WORKER|NODE|SHELL.\")}}else{ENVIRONMENT_IS_WEB=typeof window===\"object\";ENVIRONMENT_IS_WORKER=typeof importScripts===\"function\";ENVIRONMENT_IS_NODE=typeof process===\"object\"&&typeof require===\"function\"&&!ENVIR\nReferenceError: $5 is not defined\n    at Array.Rb (/Users/marat/Projects/fxdiv-confu/bin/multiply-bench.js:8:434)\n    at Array.Pc (/Users/marat/Projects/fxdiv-confu/bin/multiply-bench.js:8:36040)\n    at mc (/Users/marat/Projects/fxdiv-confu/bin/multiply-bench.js:8:17877)\n    at ic (/Users/marat/Projects/fxdiv-confu/bin/multiply-bench.js:8:10462)\n    at hc (/Users/marat/Projects/fxdiv-confu/bin/multiply-bench.js:8:5620)\n    at Object.Wb [as _main] (/Users/marat/Projects/fxdiv-confu/bin/multiply-bench.js:8:1543)\n    at Object.callMain (/Users/marat/Projects/fxdiv-confu/bin/multiply-bench.js:18:5036)\n    at doRun (/Users/marat/Projects/fxdiv-confu/bin/multiply-bench.js:18:5742)\n    at run (/Users/marat/Projects/fxdiv-confu/bin/multiply-bench.js:18:5910)\n    at Object. (/Users/marat/Projects/fxdiv-confu/bin/multiply-bench.js:18:7050)\n```\nBoth Native Client SDK and Emscripten are based on forks of Clang, not upstream Clang. The lack of inline assembly support is documented (for PNaCl, for Emscripten) so I don't think they will fix it.. x86_64-unknown-nacl is (non-Portable) Native Client target, it compiles to regular x86-64 instructions (with some limitations due to sandboxing), and supports inline assembly. You need PNaCl toolchain from Native Client SDK or emcc toolchain from Emscripten SDK to try it.. I checked that the current implementation of benchmark::DoNotOptimize results in seemingly reasonable numbers on PNaCl and Asm.js, but it would certainly help to have documented built-in support for this idiom in compilers.. With this change, if a benchmark has an overflow bug on 32-bit systems, it will also manifest on a 64-bit system. E.g. state.SetItemsProcessed(1000000 * state.iterations()) would have identical overflow behavior on both 32-bit and 64-bit systems.. Added tests and replaced log2(n) with 1.44269504088896340736 * log(n) as Android GNUSTL lacks that too.. @dominichamon done. Right, fix coming. Switched to solution suggested by @kfitch in #603. ",
    "meastman": "Below is a standalone test case that fails for the following configurations on godbolt.org (all with -Wall -Werror -std=c++0x -O3):\n- PowerPC gcc 4.8\n- PowerPC 64 6.3.0\n- ARM gcc 4.6.4\n- x86-64 gcc 4.6.4\n- x86-64 gcc 4.7.4\nBut works with the following:\n- x86-64 gcc 4.8.1 (and above)\n- x86-64 clang 3.4.1 (and above)\n- MIPS64 gcc 5.4\n```c++\ninclude \ntemplate \ninline void DoNotOptimize(Tp const& value) {\n  asm volatile(\"\" : : \"g\"(value) : \"memory\");\n}\nint main() {\n  std::bitset<1> bitset;\n  DoNotOptimize(bitset[0]);\n}\n```. ",
    "pgera": "The last comment in the PR looks like other than csv output, everything else works as intended. Is it in a usable state ? . The use case is scaling studies where the amount of work done by the benchmark isn't constant. For example, I have some OpenCL benchmarks where I vary the number of work groups and work items, which also affects the amount of actual work the benchmark does (i.e., it is variable work). My argument passing is something like:\nb->Args({platform_id, device_id, num_wgs, num_wi_per_wg,region_size});\nThere are a couple of issues with -benchmark_min_time. One is that it's some combination of CPU time/ wall time, whereas it would be nice to have a manual timer, especially for OpenCL code. The second more important issue is that I want precise control over the number of iterations because a benchmark that takes 100x longer will end up running for 1/100 iterations (in some cases only once).. Thanks for adding support for this. This greatly simplifies my workflow.\n\nAlso the library already supports using user provided/manual timers. Take a look at UseManualTime and SetIterationTime in benchmark_api.h\n\nI am using those functions for manual timers, but --benchmark_min_time doesn't tie into the manual timer, right ?\nEdit: Never mind. The manual timer is used. It's just that in my case, the manual time is much smaller than real time, and the iteration count is always determined by (results.real_time_used >= 5 * min_time). . >Fixed in master. If the manual timing feature had worked all along would have you needed the explicit iteration count change at all?\nThanks! Yes, I think manual iteration count control is still a great feature. I'm not sure if I explained the use case very well. I have a single OpenCL kernel, but I have created several benchmarks out of it based on how the kernel is called. When I run the application with a certain value of min_time, my current output looks like this:\n```\nBenchmark                                                                                                                                   Time           CPU Iterations\n\nOclSuite/StridedInterleavedIndependentTest/PlatformId:0/DeviceId:0/NumWGs:1/NumWIperWG:16/NumClsPerWI:256/Stride:1/manual_time          23978 ns     902190 ns       1000\nOclSuite/StridedInterleavedIndependentTest/PlatformId:0/DeviceId:0/NumWGs:1/NumWIperWG:16/NumClsPerWI:256/Stride:2/manual_time          23660 ns     864061 ns       1000\n...\n...\n...\nOclSuite/StridedInterleavedIndependentTest/PlatformId:0/DeviceId:0/NumWGs:512/NumWIperWG:256/NumClsPerWI:256/Stride:8/manual_time    53616176 ns  170292709 ns          3\nOclSuite/StridedInterleavedIndependentTest/PlatformId:0/DeviceId:0/NumWGs:512/NumWIperWG:256/NumClsPerWI:256/Stride:16/manual_time  108993235 ns  344253808 ns          1\n```\nThis would remain similar even with the manual timer's min_time. There is very high difference between the time taken per iteration between the first benchmark and the last one. So it's difficult to pick a good value for min_time, and I would like to ensure that every benchmark is run for at least an acceptable number of iterations. I am also interested in seeing any cache/tlb/other microarchitectural warm-up artifacts. So ideally, I would like to see how the the iteration time varies with number of iterations. Is it possible to get individual iteration's times or aggregate stats ? I know that the documentation mentions --benchmark_repetitions, but that's for inter-benchmark stats and not intra-benchmark stats, right ? . >You can set different values of min_time for different benchmarks usingBenchmark::MinTime(...)\nThat is not very convenient though. I don't know before-hand what the MinTime for each benchmark should be, and even if I did, it is not very feasible to fine tune that per benchmark when I am generating a few thousand benchmarks. It makes much more sense for me to say, \"Run each benchmark for a million iterations\".\n\nIf you're trying to expose warm-up artifacts then you likely want to use --benchmark_repetitions=N or Benchmark::Repetitions(...) instead of specifying a manual iteration count. Each repetitions will run the same amount of iterations, so you can compare the different repetitions by seeing how long they each took.\n\nI think part of the confusion is arising from the definition of an iteration v/s the definition of a benchmark. Maybe I am using the library wrong, but let's work through this example. Let's say that the benchmark does a random walk in some region of memory (like traversing a link list for instance). In my case, one iteration of this benchmark does one traversal. min_time or num_iterations will dictate the number of times I'll loop over the link list. Let's say that based on min_time that happens to be 10 iterations. The first traversal may be slower than the rest due to cache misses, and I would like to know how much slower that is. If I use --benchmark_repetitions=k, it'll do this exercise k times, but each time it'll run the same 10 iterations, and I won't know what the time delta from iteration 0 to iteration 1 was. The benchmark runs themselves will show relatively little variation. I'm assuming that each benchmark starts with a clean slate. Nothing is carried over between benchmarks, and before each benchmark, hardware data structures are flushed. Do you have any suggestion for getting this data ? You could modify it such that each benchmark only does 1 traversal, and you do --benchmark_repetitions=k, but that is not correct either if you are clearing all state between benchmark runs. You are just getting the timing of the first iteration k times with little to no variation. If you do retain state, you can make it work, but that seems hacky.\nAnother use case for having precise iteration count control is generating traces (that you later play in a simulator). The most common case is to have just 1 iteration, which can be achieved with min_time=0, but it is not hard to imagine a need for k iterations. . >Now that you have the feature do you think you can provide an example that demonstrates the behavior you described?\nWhich behaviour in particular would you like to see ?. Yes, I was just reading that section of the README. I think I can make it work. It's a bit roundabout for simple cases though. As a simple fix though, is it possible to increase the arg's size from 32 bits (int) to 64 bits ? Or would it break the library for 32 bit architectures.  Would long make sense ? It would be 32 bits on 32 bit architectures and 64 bits on 64 bit arch. . ",
    "diegohavenstein": "Isn't that what the following code achieves in the StartKeepRunning and FinishKeepRunning implementations?\nmanager_->StartStopBarrier();\nThe way I understood it, there is an implicit barrier before and also after the KeepRunning loop. . I will use an external barrier then.... I guess it would suffice to make StartStopBarrier public and maybe change its name (unless it messes with the amount of iterations or so...). But if it is not a common scenario to need this (like my case), using an external barrier will also do. Doing this of course slows down execution very noticeable, but I do not really have a choice if I am to use the framework\nBut that is something that could be written explicitly in the documentation to avoid misunderstandings, i.e. that thread iterations are not synchronized. In which case it might be useful to have a KeepSyncRunning or so when the opposite is desired\nRegards\nDiego. ",
    "ilanbiala": "I think it's quite common to have the threads synchronize at the start of the iteration. Otherwise the throughput is much higher than it should be since some threads can progress without waiting for others.\nIs there any way to get the behavior of synchronizing at the start of every iteration?. ",
    "a-krebs": "The feature tests use cmake's try_run to compile and then run some binaries to determine if the compiler supports a feature.\nThe documentation for try_run has a section on cross-compiling:\n\nWhen cross compiling, the executable compiled in the first step usually cannot be run on the build host. The try_run command checks the CMAKE_CROSSCOMPILING variable to detect whether CMake is in cross-compiling mode. If that is the case, it will still try to compile the executable, but it will not try to run the executable unless the CMAKE_CROSSCOMPILING_EMULATOR variable is set. Instead it will create cache variables which must be filled by the user or by presetting them in some CMake script file to the values the executable would have produced if it had been run on its actual target platform.\n\nSpecifically, you'll need run cmake with the -DCMAKE_CROSSCOMPILING option. Then, depending on what your target platform supports, provide correct values for the feature test variables listed below. The feature test executables are expected to return 0 on success.\n\n-DRUN_HAVE_STD_REGEX\n-DRUN_HAVE_GNU_POSIX_REGEX\n-DRUN_HAVE_POSIX_REGEX\n-DRUN_HAVE_STEADY_CLOCK\n\nFor example:\ncmake -DCMAKE_TOOLCHAIN_FILE=../toolchain.aarch64.cmake -DCMAKE_CROSSCOMPILING -DRUN_HAVE_STD_REGEX=0 .. ",
    "zlqhem": "Great! Thank you very much. It works.\n-DCMAKE_CROSSCOMPILING should be set to 1.\ncmake -DCMAKE_TOOLCHAIN_FILE=../toolchain.aarch64.cmake -DCMAKE_CROSSCOMPILING=1 -DRUN_HAVE_STD_REGEX=0 -DRUN_HAVE_POSIX_REGEX=0 ... ",
    "roberti42": "I signed it!. You're right.. ",
    "horttanainen": "I signed it!. Closing because there is 2 similar pull requests. . ",
    "lysevi": "\n\nAre you setting this value to ON?\n\n\nno, i'm no set this variable on build script or manually.. yes, it helps. thank you. ",
    "frankist": "I have a benchmark where I am trying to measure how fast is a reader at dequeuing all elements from a queue. However, the queue has limited size, so in order not to account for dequeue() calls to an empty queue, I have to call dequeue() a limited number of times. I thought of two possible approaches and both seem to require me having control over the number of iterations:\nApproach 1 - For each iteration, I could start from a full queue and keep dequeuing until it gets empty, and when that happens move to the next iteration. However, this approach would require me to, before entering the benchmark loop, make several copies of the full queue (each to be used in each iteration). The number of copies would be equal to the number of iterations, which I do not know in advance. I could make that copy inside the loop but this would affect the benchmark timing.\nApproach 2 - each iteration corresponds to a single dequeue() call. However, I could not let the number of iterations go beyond the queue size. Not having control over the number of iterations poses a problem once again.\nMaybe there is an alternative to these approaches that I am not thinking of. I tried to play with stopTiming() to discount for copies inside the main loop, however, those functions also seem to affect the benchmark timings.. \"it is consistent so comparisons between implementations are still going to be useful\"\nThat's great! Continuing on that approach, would it be possible in practice to compute its overhead separately and subtract it in the final results? My wild guess would be yes, but computers are weird and a million different variables can affect the speed of your code.. Yes, you are right. The last question was just a personal curiosity.\nOne more question, I think that if the code between \"state.PauseTiming()\" and \"state.ResumeTiming()\" (non-measured) takes much more time than the code I am measuring, google benchmark becomes very slow. My guess is that google benchmark is using the \"measured\" part of the iteration to compute the number of iterations. If it is fast, the number of iterations will be high. This is causing the slow non-measured part to be computed way too many times. Would this be the expected behavior or a bug?. ",
    "yixuan": "Hi Dominic, thanks for the quick response. According to your explanation, I think it is exactly where the sample standard deviation can be used. The logic is as follows:\nWe are interested in the overall elapsed time of one benchmark, whose value is mu but unknown. To estimate mu, we run the benchmark n times and collect a sample of n data x = [x_1, x_2, ..., x_n]. The sample mean of x is x_mean and the sample standard deviation is x_sd. With these two statistics, we can construct a 95% confidence interval of mu as x_mean \u00b1 1.96 * x_sd / sqrt(n) (1.96 is obtained from the normal distribution for convenience, a better one is to use the t-distribution). To use this equation, x_sd has to be the sample version.\nIf this makes sense to you, I'm glad to submit a PR later. Thanks.. Hi @dominichamon, when I was creating the patch I noticed one line in the code: https://github.com/google/benchmark/blob/master/src/complexity.cc#L197. Basically it adds a data point to the sample with weight run.iterations. May I ask why this weight is set to run.iterations instead of 1? I'm asking this since it may cause problems in the calculation of sample variance.\nConsider the hypothetical example below. Assume that we have two runs, each with 1000 iterations. The first run takes 2 seconds and the second takes 3 seconds. The first run has the effect of\nreal_accumulated_time_stat += Stat1_d(0.002, 1000)\nso that inside real_accumulated_time_stat, the member variable numsamples_ = 1000. Similarly, after adding the second run, the number of samples becomes 2000.\nHowever this is not the sample size used in the calculation of sample standard deviation. What we should have is numsamples_ = 2. A quick fix of this is to add the data without a weight (which means weight = 1), so the code looks like this one: https://github.com/yixuan/benchmark/blob/master/src/complexity.cc#L196-L201.\nI want to know if the weights have any other use in the code that I have missed. If not, I think I can go and submit the PR. Thanks!\n(For the confidence interval, currently there is some difficulty since we need the quantile function of t-distribution, which is not provided by standard C++ libraries.). Sure. I'll post the result here later.\nThe reason why I think the sample size should be 2 instead of 2000 (as in the example I gave above) can be explained as follows:\nAs you have discovered, the mean column is doing means of means, and similarly, stddev should be doing standard deviations of means. Sample size characterizes how many data you we really observed. If we have the measurements of each iteration, then yes, our sample size would be 2000, but the reality is that we have aggregated the data in each run and lost individual measurements per iteration. As a result, what we have really observed is the time for each run, so the sample size is equal to the number of runs in this case.\nA tiny example can illustrate this idea. Imagine that we have two runs. The first run has two iterations, with time X1 and X2. The second run also has two iterations with time Y1 and Y2. If we have all these four data points, then the standard deviation can be calculated based on a sample size of 4. However, the real process is, we first measure the time for run 1, X1+X2, divide it by 2, Xbar = (X1+X2)/2, and then add it to the stat class; and then, we similarly measure Ybar = (Y1+Y2)/2, and then add it to the stat class. What we really \"observe\" are Xbar and Ybar, which are aggregations of the raw data. In this case, we cannot use 4 as the sample size.. This is the result of a simple test based on the current google/benchmark:\n```\n\nBenchmark                      Time           CPU Iterations\nBM_SORT                     7792 ns       7782 ns      72674\nBM_SORT                     7775 ns       7766 ns      72674\nBM_SORT                     7753 ns       7745 ns      72674\nBM_SORT_mean                7774 ns       7764 ns      72674\nBM_SORT_stddev                16 ns         15 ns          0\nBM_STABLE_SORT              8179 ns       8169 ns      84570\nBM_STABLE_SORT              8249 ns       8238 ns      84570\nBM_STABLE_SORT              8252 ns       8242 ns      84570\nBM_STABLE_SORT_mean         8226 ns       8217 ns      84570\nBM_STABLE_SORT_stddev         34 ns         34 ns          0\n```\nWe can manually verify that sqrt(((7792 - 7774)^2 + (7775 - 7774)^2 + (7753 - 7774)^2) / 3) = 15.98 ~= 16.\nBelow is my patched version (https://github.com/yixuan/benchmark/commit/9657621b748001386db2c8338fc3975ea9dc5748, https://github.com/yixuan/benchmark/commit/9f6434da82defb0bdebd3aeb8512258646e861a5):\n```\n\nBenchmark                      Time           CPU Iterations\nBM_SORT                     7807 ns       7799 ns      83800\nBM_SORT                     7782 ns       7775 ns      83800\nBM_SORT                     7770 ns       7762 ns      83800\nBM_SORT_mean                7786 ns       7779 ns      83800\nBM_SORT_stddev                19 ns         19 ns          0\nBM_STABLE_SORT              8354 ns       8345 ns      79040\nBM_STABLE_SORT              8255 ns       8246 ns      79040\nBM_STABLE_SORT              8288 ns       8279 ns      79040\nBM_STABLE_SORT_mean         8299 ns       8290 ns      79040\nBM_STABLE_SORT_stddev         50 ns         50 ns          0\n```\nTake the first stddev value as example, it can be verified that sqrt(((7807 - 7786)^2 + (7782 - 7786)^2 + (7770 - 7786)^2) / 2) = 18.88 ~= 19\nThe difference is that one uses 3 as denominator and the other uses 2.. I think so. I used an example derived from the test suite:\n```cpp\ninclude \ninclude \"benchmark/benchmark_api.h\"\nstd::set ConstructRandomSet(int size) {\n  std::set s;\n  for (int i = 0; i < size; ++i) s.insert(i);\n  return s;\n}\nstatic void BM_SetInsert(benchmark::State& state) {\n  while (state.KeepRunning()) {\n    state.PauseTiming();\n    std::set data = ConstructRandomSet(state.range(0));\n    state.ResumeTiming();\n    for (int j = 0; j < state.range(1); ++j) data.insert(rand());\n  }\n  state.SetItemsProcessed(state.iterations() * state.range(1));\n  state.SetBytesProcessed(state.iterations() * state.range(1) * sizeof(int));\n}\nBENCHMARK(BM_SetInsert)->Args({1024, 1});\nBENCHMARK_MAIN();\n```\nOfficial version:\n```\nBenchmark                           Time           CPU Iterations\nBM_SetInsert/1024/1             48752 ns      48713 ns      14356   80.1897kB/s   20.0474k items/s\nBM_SetInsert/1024/1             48759 ns      48722 ns      14356   80.1743kB/s   20.0436k items/s\nBM_SetInsert/1024/1             48773 ns      48733 ns      14356   80.1566kB/s   20.0392k items/s\nBM_SetInsert/1024/1_mean        48761 ns      48722 ns      14356   80.1735kB/s   20.0434k items/s\nBM_SetInsert/1024/1_stddev          9 ns          8 ns          0    13.8566B/s    3.46415 items/s\n```\nPatched version:\n```\nBenchmark                           Time           CPU Iterations\nBM_SetInsert/1024/1             48622 ns      48581 ns      14407   80.4065kB/s   20.1016k items/s\nBM_SetInsert/1024/1             48764 ns      48722 ns      14407   80.1736kB/s   20.0434k items/s\nBM_SetInsert/1024/1             48850 ns      48810 ns      14407   80.0297kB/s   20.0074k items/s\nBM_SetInsert/1024/1_mean        48745 ns      48705 ns      14407   80.2033kB/s   20.0508k items/s\nBM_SetInsert/1024/1_stddev        116 ns        115 ns          0    194.749B/s    48.6872 items/s\n```. Did you notice the different units?. But still, I would say that the current algorithm to calculate standard deviation has large rounding errors, since the numbers passed to the stats class are usually very large (they are in the units of nanoseconds or bytes), and moreover the stats class computes their sum of squares that makes the total even larger.\nA better algorithm is to compute the mean first and then sum up the square of the difference between each data point and the mean, but it requires storing the whole array.. Sure. I'll submit the PR later.\nI also have some ideas on reducing the rounding error and may open a new issue.. Hi @pleroy , thanks for your nice correction, especially for the second point.\nWhat I wanted to say is that the numbers are too large compared to their variations, so instead of adding [10024, 10025, 10021] to the sum and sum of squares, it's better to first choose an offset, e.g. 10000, and then use their difference with the offset, [24, 45, 21], for calculation. Ideally the offset is chosen to be the mean, but other choices such as the first value in the sequence is also fine.. Those failures seem to be due to abnormal running time (e.g. 195313 ns) that goes beyond the regular expression limit (five digits), usually occurring in multi-threaded programs. Is it possible to re-launch those tests and then see the results?. Looks great. Thank you @dominichamon !. Sounds good.. And probably we need to use Mean(&stddev); for the second line, since mean is an unused variable and the Makefile converts warnings into errors.. ",
    "navia1991": "@EricWF  Thanks! Yes, it is probably something wrong with cmakelist... I will take a look at the examples... . ",
    "vladoovtcharov": "That almost fixed it, it was also erasing the flag when coercing it to a double in the update and finish functions. Probably more robust to overload the operators but I just did a quick hack for now.. ok, I added the CLA, hopefully it will pick it up.... I signed the CLA. [clabot:check] . ",
    "felixduvallet": "Sorry about the delay @dominichamon and @googlebot , I filled out the CLA.. ",
    "andersfylling": "target_link_libraries(chess_ann_benchmarks benchmark -lpthread)\nthis worked!\nBut now I get this error:\nFailed to match any benchmarks against regex: .\nThats the only error I get, however if I add a benchmark, it dissapears. (Y). ",
    "tilsche": "\nAs a simple fix though, is it possible to increase the arg's size from 32 bits (int) to 64 bits ?\n\nI have to second that request. It is extremely inconvenient to use int rather than long or ssize_t for range. Even the README example use case uses range for a data size so you can't run BM_memcpy for > 2 GiB of data. Also the ugly int64_t(state.range(0)).\nSo please. Whenever you make an ABI-braking version jump anyway. Change the type.. ",
    "tusharpm": "I signed it!. To make it consistent in the file and with appveyor.yml.\nDo you want me to revert the whitespace changes?. ",
    "schoetbi": "@EricWF Thanks for the review.I agree with all off your suggestions but I will be offline the next weeks so expect some delay. I will try to create some threads then in the benchmark setup. The mutexes are already in place since I plan to use shared memory.\nThank you.\n. The problem for me arose because I used the cpplint.py script to check my complete codebase and got errors for every benchmark I wrote.\nI do not suggest to change anything in the API.\nThat it is not worth the trouble. Honestly I would rather change the rule of not using non-const references as parameters in the coding guideline. The reason for this is that a reference has the property of never being null. That alone makes it superior to pointers. \nI saw a posibility in the cpplint.py script to disable this rule. That solves the issue for me. Thanks.. Yes true. I will try to deduce the version number from the latest git tag. Would this work out?. ",
    "advaitraut": "Thanks, very true.\nHow can I print Standard Deviation ?\nOn Tue, 23 May 2017 18:13 Pascal Leroy, notifications@github.com wrote:\n\nWhat makes you think that the two benchmarks are going to loop the same\nnumber of times? If BM_deleteInt went into more iterations than BM_newInt\nyou'd index into the boonies.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/391#issuecomment-303385897,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABClPgLl1rMfJ_B_2tnIyr6SQ2FAqiDpks5r8tSOgaJpZM4Njns5\n.\n. Can I do something like\n\n```\nstatic void BM_deleteInt(benchmark::State& state){\n while (state.KeepRunning() && c > 0)\n  {\n    delete int_pp[--c];\n  }\n// Post processing ? \n// How ?\n// Stop benchmarks early ?\n}\n```\n. ",
    "dkruger": "I signed it!. ",
    "chen3feng": "Is it a feasible way to change the API to accpet both benchmark::State* and benchmark::State&?\nor just change cpplint.py to allow non-const reference in benchmark case as an exception (function name starts with \"BM_\")?. ",
    "yangguangxu": "@dominichamon Thanks, link pthread can solve the problem.. @EricWF   yes, problem has been solved.. ",
    "GeorgeARM": "I signed it!. @EricWF can I filter tests based on their Label? . Makes sense @dominichamon, will add the appropriate methods.\nThanks.. ",
    "zack-snyder": "1)\nI do some initialization before the loop.\nWill this also be included in the measurement?\nSomething like this:\nstatic void BM_somefunc(benchmark::State& state)\n{\n    auto foo = do_some_init();\n    while (state.KeepRunning())\n    {\n        do_some_calculation(foo);\n    }\n}\n2)\nI get sometimes following results\n```\n\nBenchmark                  Time           CPU Iterations\nBM_fastFuncReturn      16327 ns      16392 ns      44800\nBM_slowFuncReturn      17499 ns      16881 ns      40727\n```\nYou see that the CPU time is in one case higher. How can this be?\n. How can the CPU timer higher, then the wall clock time?\nHow is the CPU time measured, with ticks?. no, there are no multiple threads involved. ",
    "werdna87": "I believe there might be a few ways. The most obvious is if your test uses multiple threads. ",
    "mattreecebentley": "I feel the basic documentation is incredibly sparse and not very detailed about how any of the functions and macros work, their methodology or reasons why they are doing what they're doing. Coming to it afresh having not studied it before, I find it almost impenetrable. I would recommend a full review of the documentation as it stands.. I signed. Thanks - I think it needs a lot more work, it's just that I don't actually understand what's going on enough at this point to re-write it. There's no details about what methodology the benchmarks is using, and I can only pick up anecdotal stuff elsewhere. . It's more that there's just no detail there. No information about what the individual macro/functions are doing, nothing. I don't feel like this is something that even requires discussion, it's that bizarre, to me.. @dominichamon: I'll consider it. I didn't bother because I have low time. \n@LebedevRI: I don't think you're worth listening to, based on your comments.. Well, if that's your attitude, consider your bias first. You've been fine, but he's been obstructive, unhelpful, arrogant, refusing to reply to requests for clarification. I don't have time for this as mentioned and your project can continue on without my unpaid help. Bye. Don't @ me.. If I can't call a spade a spade - and I believe my previous statement describes his behaviour accurately - then I quit.. They may be problems, but the user is not going to see the bottom of the document when they are trying to compile the first time, following the guidelines. When corrected in Gbenchmark, they can be corrected here also.. May have been a mistake. The previous line should've been removed (Otherwise, etc) as it is incorrect in my experience - something is broken there, at least under ubuntu.. When you say 'linewrap', what do you mean exactly.. This is actually part of a previous commit (see current readme.md). Google benchmark is indeed based on the gtest framework - that's why you need it to compile it.. BTW, I spent the better part of an hour googling to find the answer above the first time I tried to compile google benchmark code on windows. So I'm speaking from experience, not theory. You can't expect the user to read the whole document in one go, or assume that parts which relate to linking are somewhere else for some previously unknown reason.. What?\nHave you even compiled google benchmark yourself before?\nAt any rate, it's part of a previous commit which has Already Been Accepted.. I'm not changing this commit. . Why is this important when the HTML will wrap anyway?\nHappy to split up the lining to space it out a bit.. It was answered by the commit itself. Look it up.. You're not being helpful, or posting under the right heading, and I'm going to ignore you.. ",
    "elderRex": "maybe something as simple as:\nc++\nvoid BenchmarkFamilies::ClearBenchmark() {\n  MutexLock l(mutex_);\n  families_.clear();\n}\nTho not sure about the potential complications this might introduce in other parts of the code.... Hi dominic, the goal is to embed the benchmark in a class so that it can be evoked repeatedly with ease. 'cause the function i'm benchmarking is constantly changing... Tho there could be a better design, probably need to figure this out.\nalso, thanks Eric : D. @EricWF Thanks for the clarification! Though the broken pipe problem is probably 'caused by other code in my program & I'm trying to pinpoint the 'cause\nmy question is more about multiple initialization as dominic said (also removing registered benchmarks)\n. yup that's the plan. @EricWF Thanks for the update! : D. ",
    "jernkuan": "I have signed the CLA, somehow it's not picking it up.\nI have checked my email address of the signed CLA and with the github.com account. Any other clues?\nFor example, if you are cross compiling you might do something like this.\ncmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_TOOLCHAIN_FILE=../../Toolchain.cmake -DHAVE_STD_REGEX=true\nYou will get an error, as the \nCMake Error at CMakeLists.txt:183 (message):\n  Failed to determine the source files for the regular expression backend\nAs the CXXFeatureCheck.cmake will do an early exit if the variable is define,  it does not add the definition to be used in the generation of the Makefile. The adding of the definition and setting of cache is executed only when RUN_HAVE_STD_REGEX is true.\n. Ah yes, i have a mixed up author email address. I have made the changes, and pushed again to my branch. But seems that this is not picking up the changes to the author email. Should i close this and reissue a new pull request?. ",
    "fuji246": "The docker env:\n```\nFROM centos:6\nRUN yum -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm\nRUN curl https://people.centos.org/tru/devtools-1.1/devtools-1.1.repo -o /etc/yum.repos.d/devtools-1.1.repo\nRUN yum -y install devtoolset-1.1\nRUN yum -y install cmake\n```\nThe compile command and output:\n[root@d9306eee282a build]# scl enable devtoolset-1.1 \"cmake ../\"\n-- The C compiler identification is GNU 4.7.2\n-- The CXX compiler identification is GNU 4.7.2\n-- Check for working C compiler: /opt/centos/devtoolset-1.1/root/usr/bin/cc\n-- Check for working C compiler: /opt/centos/devtoolset-1.1/root/usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working CXX compiler: /opt/centos/devtoolset-1.1/root/usr/bin/c++\n-- Check for working CXX compiler: /opt/centos/devtoolset-1.1/root/usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Could NOT find Git (missing:  GIT_EXECUTABLE) \n-- git Version: v0.0.0\n-- Version: 0.0.0\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11 - Success\n-- Performing Test HAVE_CXX_FLAG_WALL\n-- Performing Test HAVE_CXX_FLAG_WALL - Success\n-- Performing Test HAVE_CXX_FLAG_WEXTRA\n-- Performing Test HAVE_CXX_FLAG_WEXTRA - Success\n-- Performing Test HAVE_CXX_FLAG_WSHADOW\n-- Performing Test HAVE_CXX_FLAG_WSHADOW - Success\n-- Performing Test HAVE_CXX_FLAG_WERROR\n-- Performing Test HAVE_CXX_FLAG_WERROR - Success\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC - Success\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS - Success\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32 - Failed\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL - Success\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING - Success\n-- Performing Test HAVE_CXX_FLAG_WZERO_AS_NULL_POINTER_CONSTANT\n-- Performing Test HAVE_CXX_FLAG_WZERO_AS_NULL_POINTER_CONSTANT - Success\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING - Success\n-- Performing Test HAVE_CXX_FLAG_WD654\n-- Performing Test HAVE_CXX_FLAG_WD654 - Failed\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY - Failed\n-- Performing Test HAVE_CXX_FLAG_COVERAGE\n-- Performing Test HAVE_CXX_FLAG_COVERAGE - Success\n-- Performing Test HAVE_STD_REGEX\n-- Performing Test HAVE_STD_REGEX -- compiled but failed to run\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile\n-- Performing Test HAVE_POSIX_REGEX\n-- Performing Test HAVE_POSIX_REGEX -- success\n-- Performing Test HAVE_STEADY_CLOCK\n-- Performing Test HAVE_STEADY_CLOCK -- success\n-- Looking for include file pthread.h\n-- Looking for include file pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE  \n-- Performing Test BENCHMARK_HAS_CXX03_FLAG\n-- Performing Test BENCHMARK_HAS_CXX03_FLAG - Success\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /adaptation_resilience/code/external/benchmark-master/build\n[root@d9306eee282a build]# scl enable devtoolset-1.1 \"make\"\nScanning dependencies of target benchmark\n[  3%] Building CXX object src/CMakeFiles/benchmark.dir/benchmark.cc.o\n[  6%] Building CXX object src/CMakeFiles/benchmark.dir/benchmark_register.cc.o\n[  9%] Building CXX object src/CMakeFiles/benchmark.dir/colorprint.cc.o\n[ 12%] Building CXX object src/CMakeFiles/benchmark.dir/commandlineflags.cc.o\n[ 16%] Building CXX object src/CMakeFiles/benchmark.dir/complexity.cc.o\n[ 19%] Building CXX object src/CMakeFiles/benchmark.dir/console_reporter.cc.o\n[ 22%] Building CXX object src/CMakeFiles/benchmark.dir/counter.cc.o\n[ 25%] Building CXX object src/CMakeFiles/benchmark.dir/csv_reporter.cc.o\n[ 29%] Building CXX object src/CMakeFiles/benchmark.dir/json_reporter.cc.o\n[ 32%] Building CXX object src/CMakeFiles/benchmark.dir/reporter.cc.o\n[ 35%] Building CXX object src/CMakeFiles/benchmark.dir/sleep.cc.o\n[ 38%] Building CXX object src/CMakeFiles/benchmark.dir/string_util.cc.o\n[ 41%] Building CXX object src/CMakeFiles/benchmark.dir/sysinfo.cc.o\n[ 45%] Building CXX object src/CMakeFiles/benchmark.dir/timers.cc.o\nLinking CXX static library libbenchmark.a\n[ 45%] Built target benchmark\nScanning dependencies of target basic_test\n[ 48%] Building CXX object test/CMakeFiles/basic_test.dir/basic_test.cc.o\nLinking CXX executable basic_test\n[ 48%] Built target basic_test\nScanning dependencies of target benchmark_test\n[ 51%] Building CXX object test/CMakeFiles/benchmark_test.dir/benchmark_test.cc.o\n/adaptation_resilience/code/external/benchmark-master/test/benchmark_test.cc: In function \u2018void BM_ManualTiming(benchmark::State&)\u2019:\n/adaptation_resilience/code/external/benchmark-master/test/benchmark_test.cc:185:5: error: \u2018sleep_for\u2019 is not a member of \u2018std::this_thread\u2019\nmake[2]: *** [test/CMakeFiles/benchmark_test.dir/benchmark_test.cc.o] Error 1\nmake[1]: *** [test/CMakeFiles/benchmark_test.dir/all] Error 2\nmake: *** [all] Error 2. ",
    "astrelni": "@dominichamon . @EricWF Thanks for having a look. I'm not too familiar with the pull request workflow: is there any con to adding new commits to remove the range-based for from this pull request? Or do you prefer to start with a new pull request for both?. @dominichamon Since @EricWF hasn't had a chance to respond, what is your standing on splitting this up into two pull requests? And also, quoting my previous question to him:\n\nI'm not too familiar with the pull request workflow: is there any con to adding new commits to remove the range-based for from this pull request? Or do you prefer to start with a new pull request for both?. @dominichamon @EricWF I've updated to remove the range-based for from this pull request, please take a look.. Hi @EricWF, I just realized @dominichamon is likely on vacation right now. I have a sneaking suspicion you may be out as well. But in case otherwise, I'm pinging this thread. Please let me know if I accidentally missed addressing something.. @EricWF Yes, very sorry for the long hiatus. I am going on parental leave any day now and some priorities had to be juggled around. I am working on updates in response to your comments now and hope to have something for you to look at soon. (I don't plan on dropping off entirely and want to finish this out even if it has to be while on leave). @EricWF thanks a lot for getting the range-for in. Sorry that you had to do it yourself. I will come back later to add the batch iteration. I hope very much not to have you looking at it for as much time as you worked on this PR.. @EricWF Not at all, you're helping me out. No need to credit me, it was actually a fleshed out implementation of something that was originally Dmitry Vyukov's idea. I probably should have credited him in the original description of this PR, but this was my first attempt at an OSS contribution. Is that regular etiquette for PRs? If so, maybe I should go add his name to your PR.. For context, I am currently failing to build a benchmark with gcc because it is triggering Werror=invalid-offsetof on that line.. Would you be in favor of turning it into a runtime CHECK on &error_occured_ - this with reinterpret_casts to char* to keep standard compliant?. Ping @EricWF @dominichamon . So if IIUC we just need to add -Winvalid-offsetof to the bazel copts?. Great, thanks a lot.. I'm not sure that Abseil would want to take sysinfo.h/.cc out of being an internal implementation detail, and we should probably set a good example and not pull the source out here. Good cmake support is coming really soontm :). //absl/base:base is tricky in that it has some internal circular dependencies. There are headers in absl/base/internal that are needed in other parts of abseil, but we would like to keep them as internal implementation details at least for now. However, we could not split them into their own target with non-public visibility. \n\nThe result right now is that they are in hdrs, but we explicitly document that people should not be using anything internal to abseil. It's not going to stop them all, but it's there to give us flexibility in refactoring that internal code.\nNow, all that said. We started out with a lot of this code as internal, because it is much easier to take something from internal to public than vice versa. There would need to be a conversation, but we are open to discussing making certain internal details public. Whether or not that applies to sysinfo, I cannot say.\nFor the time being, I would prefer to work this out in parallel.\nOn the other hand, I see that this library is tested with MinGW, so I may need to find another way to link against shlwapi.lib. I saw in the comments that added the issue to the README discussing that the pragma would not work on MinGW, but I did not see MinGW as supported on the README either.. Closing this one in favor of https://github.com/google/benchmark/pull/581 which only attempts to solve the issue for bazel. There we only need to worry about msvc on windows and not mingw.. Recommended linking against libraries with msvc and bazel taken from\nhttps://github.com/bazelbuild/bazel/issues/3425#issuecomment-316998146. @dominichamon My understanding of @pleroy's comment (please correct me) is that this project would continue to have the same test targets, but each target would use benchmark_main instead of BENCHMARK_MAIN();. That would allow users who want to keep a local copy of the repository to use a custom provided build configuration not generated by CMake. That could allow them to combine multiple benchmark source files into one target without first having to strip out the BENCHMARK_MAIN();s every time they sync their copy.. @pleroy @dominichamon I took a quick look this morning and about half of the benchmark tests in the repository use custom a custom main(). I don't know if the follow up would be very practical at this point.. Yeah, moving to cc saw a small performance hit (~1ns). Though we could move the slow path of the last ten or so lines into a separate method if that makes things a little cleaner.. Was following along with all the other tests which use asserts. Tried it and we don't have visibility of check.h in test/. My reasoning was that the range based for hurts readability a little bit with the unused opaque variable, and the vast majority of cases are not affected by KeepRunning. If that's a nonissue, I can reword the documentation to put the range-based for front and center.\nThe only other thing is that this library supports C++03 as far as I can tell. So I'm not sure where that leaves things. Add a separate section in the documentation for \"old API + C++03\"?. One other thought. This may not be the time to resolve it since we're likely separating the range-based for addition from the first batch support, but I'd like to not forget to mention this.\nI think there needs to be some clear documentation on State::iterations(). I may be mistaken, but I cannot find a place saying don't bother calling this inside of the benchmarking loop. The current implementation would return the number of actual iterations + 1 which is only slightly wrong. But, range-based for would be way off and for no apparent (to the user) reason. If checking iterations inside the benchmarking loop is something that should be supported, then I don't see how the range-based for can replace KeepRunning(). Please let me know if I'm misunderstanding this.. Fixed. Fixed. Fixed. Fixed. Fixed. Fixed. Fixed. Had to wrap in a struct because of C++03 support.. Done.. Thanks for bringing this up. It does sound like probably a cleaner solution. There is one situation where I'm seeing it complicate things: \nAs much as I'd like it not to be the case, I think the range-based for loop API will depend on this behavior somewhat. Especially if we want to document it as the API of choice. The range-based for uses 1000 as the batch size, but the API could easily be used by a benchmark which is slow enough to only run in the single digits to tens of iterations in a reasonable amount of time. So we want to easily be able to cut the batch size short when that feature comes.\nI guess thinking more, we could give a bool flag for State for whether KeepRunningBatch was called or not and use that when calculating the number of iterations. (Also would add the batch size as a State attribute and update the contract that the batch size must remain the same)\nThanks again for bringing this up. I will work on this tomorrow, but I'll leave this comment here already in case you have some thoughts to add in the meantime.. size_t is the natural type to provide in the API, but I think int64_t is more appropriate for the internals because we do arithmetic with these values. In 32-bit applications, size_t could overflow and wrap*, giving wrong but potentially plausible benchmark readouts. I think it's the same reason bytes_processed and items_processed are stored as int64_t's.\n\nkMaxIterations seems to be 1 billion now, so > 4 threads could do it. Or in case kMaxIterations is increased in the future.. I figured that diamond dependency isn't an issue here because the benchmark library doesn't actually depend on gtest, only the benchmark tests do. So whatever version of gtest is brought in here does not get forwarded to a project that uses benchmark. Let me know if I'm confused on that and I'll look into adding a specific commit back.. Done. \n",
    "vgvassilev": "It looks like this is fixing this: https://github.com/google/benchmark/commit/8ae6448cc7ec6353e3491a2a15f972f9735f124b\nCould we have a new release of google benchmark then?. I signed it!. Thanks for the feedback. Are you saying that such a service should be outside of the project?. I am not sure I can follow, this seems to be true for any other reporter. I'd think of this as adding 'colored' console output. It could be done by a separate service but it is very useful to have this in the library, available to everybody. It seems that the html reports are popular user request.\nI'd expect that we will not change much of this code. We want to be able to plot benchmark families and counters. The customization of what color the bars and other details could be can be done from the outside, based on the default. . ",
    "Skabunkel": "The primary reason it is 0 is because of a copy paste fail on my part, from the section Calculate asymptotic complexity (Big O).\nI do however consider 0 to a possible value. A possible solution would be to check if n amount of them satisfy the condition complexity_n > 0 where n is a significant subset of the total amount of benchmarks.\ni'm unsure if i can come with any better suggestions since i have little reference regarding the code flow of complexity_n.. ",
    "verbit": "\nare you sure you're on the latest?\n$ git log\ncommit 9d4b719daeda35acf3a3d81b9ac1f38fc13333d1\nhave you pulled tags?\n\nDoes a fresh git clone pull the tags? I also tried with git fetch --tags but still get the same output:\n$ git describe\nv0.1.0-462-g9d4b719\n$ git describe --tags\nv1.1.0-112-g9d4b719\n. git pull --tags does not help and git cat-file -t v1.1.0 shows commit.\nAre you testing this on a fresh git clone? Maybe the tag is only annotated in your local repository and not pushed to github?\nOtherwise, I'm out of ideas.. Works now, thanks. ",
    "dalg24": "Just ran into the same issue when attempting to pass the version argument to find_package.  Any update?\nEDIT  This is fixed in master (c.f. #611) but latest tagged release (v1.4.1) is broken.  I guess this issue can be closed now.  It would be good to have another release with the fix soon.... ",
    "gnarlie": "The error appears to be valid ODR violation - the test cxx03_test (built with std=c++03) is linking libbenchmark.a, built with C++11. At least for this version of gcc, the tree nodes in a std::map appear to have a different layout based on the value of __cplusplus.\nDisabling LTO works around the problem.\n```c++\n  template\n    struct _Rb_tree_node : public _Rb_tree_node_base\n    {\n      typedef _Rb_tree_node<_Val>* _Link_type;\nif __cplusplus < 201103L\n  _Val _M_value_field;\n\n  _Val*\n  _M_valptr()\n  { return std::__addressof(_M_value_field); }\n\n  const _Val*\n  _M_valptr() const\n  { return std::__addressof(_M_value_field); }\n\nelse\n  __gnu_cxx::__aligned_membuf<_Val> _M_storage;\n\n  _Val*\n  _M_valptr()\n  { return _M_storage._M_ptr(); }\n\n  const _Val*\n  _M_valptr() const\n  { return _M_storage._M_ptr(); }\n\nendif\n};\n\n```. Moving the implementation of UserCounters to the .cc file (using pImpl or similar) would likely fix #420 as well. . @EricWF I'm not sure that the diagnostic can be suppressed - its an error, not a warning. Perhaps we can just disallow LTO for that test?\n@dominichamon - to be valid, the C++03 binary would need to be compiled with the same ABI settings introduced in GCC 5 to support C++11. It can be done, but I can't think of a use case.. @EricWF looks good - thanks for fixing. ",
    "oshadura": "I signed it!. ",
    "ax3l": "uh, we just realized the master branch is already installing the proper files! #363\n(any releases planned, soon? :)) #419 . ",
    "DerThorsten": "I signed it!. I think an std::any implementation is overkill.  Since we store the benchmarks in a string-ish fashion one would need to convert all metadata to a string anyhow.\n. I'll implement the missing CSV reporter these days .\nEdit:\n\nsee work in progress  commit . Sure, sry for that. @EricWF  I agree. \n",
    "egpbos": "With a label and/or name you still have only one metadata field. Being able to specify multiple will create far cleaner output in case you want to reuse the metadata, e.g. for aggregating over certain metadata fields. This would significantly ease analysis of large amounts of benchmarks over parameter matrices. I would really like to see this feature included!. ",
    "vincecr0ft": "Whilst I'm appreciative of the time I've just spent brushing up on my data wrangling. Making it easier to plot the outputs of these parameter matrices would be nice.... ",
    "henrik-jensen": "Thanks @aJetHorn for the pull request #522.. ",
    "pwnall": "@dominichamon Thank you very much for the quick review!. The Travis build here doesn't appear to run GCC on OSX, so I tested this PR by using it in a project where Travis is configured to include GCC on OSX.\nWith PR: https://travis-ci.org/google/crc32c/builds/309377910\nWithout PR: https://travis-ci.org/google/crc32c/builds/309350965. My changes are contained in a block that is guarded by an #ifdef BENCHMARK_HAS_SYSCTL, which shouldn't be defined on Windows... so I think the AppVeyor failure is unrelated.. @dominichamon Thank you very much for the quick turnaround time! Will submit another PR for GCC on Travis.. @dominichamon PTAL?. And thank you for responding to my PRs so quickly!. @dominichamon Thank you very much for the quick review!. Thank you very much for the quick review!. Done. Thank you very much for the feedback!. ",
    "Fluxie": "I take it that a small Python application that does the trick would be preferred then?. ",
    "ghost": "For some reason which I don't know yet in this line  the thread_count size is very large.. I am pretty sure this is caused by the commits on Aug 29, 2017.. Hi @dominichamon  the code is located here. I don't think there is anything unusual with it. I tried to check out commit d70417994a3c845c49c4443e92b26a52b320a759 of benchmark and then build the benchmark it is fine but if I check out the latest commit (of benchmark) and test it I got the above error, so I am pretty sure it is the last 3 commits that caused the issue here.. Okay, so I built the latest commit of benchmark and then use the libbenchmark.a produced to build my own benchmarks. The CMake log for my own benchmarks is here:\n```\nOn branch master\n    Feature record: CXX_FEATURE:1cxx_inline_namespaces\n    Feature record: CXX_FEATURE:1cxx_lambdas\n    Feature record: CXX_FEATURE:0cxx_lambda_init_captures\n    Feature record: CXX_FEATURE:1cxx_local_type_template_args\n    Feature record: CXX_FEATURE:1cxx_long_long_type\n    Feature record: CXX_FEATURE:1cxx_noexcept\n    Feature record: CXX_FEATURE:1cxx_nonstatic_member_init\n    Feature record: CXX_FEATURE:1cxx_nullptr\n    Feature record: CXX_FEATURE:1cxx_override\n    Feature record: CXX_FEATURE:1cxx_range_for\n    Feature record: CXX_FEATURE:1cxx_raw_string_literals\n    Feature record: CXX_FEATURE:1cxx_reference_qualified_functions\n    Feature record: CXX_FEATURE:0cxx_relaxed_constexpr\n    Feature record: CXX_FEATURE:0cxx_return_type_deduction\n    Feature record: CXX_FEATURE:1cxx_right_angle_brackets\n    Feature record: CXX_FEATURE:1cxx_rvalue_references\n    Feature record: CXX_FEATURE:1cxx_sizeof_member\n    Feature record: CXX_FEATURE:1cxx_static_assert\n    Feature record: CXX_FEATURE:1cxx_strong_enums\n    Feature record: CXX_FEATURE:1cxx_template_template_parameters\n    Feature record: CXX_FEATURE:1cxx_thread_local\n    Feature record: CXX_FEATURE:1cxx_trailing_return_types\n    Feature record: CXX_FEATURE:1cxx_unicode_literals\n    Feature record: CXX_FEATURE:1cxx_uniform_initialization\n    Feature record: CXX_FEATURE:1cxx_unrestricted_unions\n    Feature record: CXX_FEATURE:1cxx_user_literals\n    Feature record: CXX_FEATURE:0cxx_variable_templates\n    Feature record: CXX_FEATURE:1cxx_variadic_macros\n    Feature record: CXX_FEATURE:1cxx_variadic_templates\nDetecting CXX [-std=c++98] compiler features compiled with the following output:\nChange Dir: /Users/bobfang/Documents/cf/build/CMakeFiles/CMakeTmp\nRun Build Command:\"/usr/bin/make\" \"cmTC_9afa7/fast\"\n/Library/Developer/CommandLineTools/usr/bin/make -f CMakeFiles/cmTC_9afa7.dir/build.make CMakeFiles/cmTC_9afa7.dir/build\nBuilding CXX object CMakeFiles/cmTC_9afa7.dir/feature_tests.cxx.o\n/Library/Developer/CommandLineTools/usr/bin/c++    -std=c++98 -o CMakeFiles/cmTC_9afa7.dir/feature_tests.cxx.o -c /Users/bobfang/Documents/cf/build/CMakeFiles/feature_tests.cxx\nLinking CXX executable cmTC_9afa7\n/usr/local/Cellar/cmake/3.9.0/bin/cmake -E cmake_link_script CMakeFiles/cmTC_9afa7.dir/link.txt --verbose=1\n/Library/Developer/CommandLineTools/usr/bin/c++    -Wl,-search_paths_first -Wl,-headerpad_max_install_names   CMakeFiles/cmTC_9afa7.dir/feature_tests.cxx.o  -o cmTC_9afa7\nFeature record: CXX_FEATURE:0cxx_aggregate_default_initializers\nFeature record: CXX_FEATURE:0cxx_alias_templates\nFeature record: CXX_FEATURE:0cxx_alignas\nFeature record: CXX_FEATURE:0cxx_alignof\nFeature record: CXX_FEATURE:0cxx_attributes\nFeature record: CXX_FEATURE:0cxx_attribute_deprecated\nFeature record: CXX_FEATURE:0cxx_auto_type\nFeature record: CXX_FEATURE:0cxx_binary_literals\nFeature record: CXX_FEATURE:0cxx_constexpr\nFeature record: CXX_FEATURE:0cxx_contextual_conversions\nFeature record: CXX_FEATURE:0cxx_decltype\nFeature record: CXX_FEATURE:0cxx_decltype_auto\nFeature record: CXX_FEATURE:0cxx_decltype_incomplete_return_types\nFeature record: CXX_FEATURE:0cxx_default_function_template_args\nFeature record: CXX_FEATURE:0cxx_defaulted_functions\nFeature record: CXX_FEATURE:0cxx_defaulted_move_initializers\nFeature record: CXX_FEATURE:0cxx_delegating_constructors\nFeature record: CXX_FEATURE:0cxx_deleted_functions\nFeature record: CXX_FEATURE:0cxx_digit_separators\nFeature record: CXX_FEATURE:0cxx_enum_forward_declarations\nFeature record: CXX_FEATURE:0cxx_explicit_conversions\nFeature record: CXX_FEATURE:0cxx_extended_friend_declarations\nFeature record: CXX_FEATURE:0cxx_extern_templates\nFeature record: CXX_FEATURE:0cxx_final\nFeature record: CXX_FEATURE:0cxx_func_identifier\nFeature record: CXX_FEATURE:0cxx_generalized_initializers\nFeature record: CXX_FEATURE:0cxx_generic_lambdas\nFeature record: CXX_FEATURE:0cxx_inheriting_constructors\nFeature record: CXX_FEATURE:0cxx_inline_namespaces\nFeature record: CXX_FEATURE:0cxx_lambdas\nFeature record: CXX_FEATURE:0cxx_lambda_init_captures\nFeature record: CXX_FEATURE:0cxx_local_type_template_args\nFeature record: CXX_FEATURE:0cxx_long_long_type\nFeature record: CXX_FEATURE:0cxx_noexcept\nFeature record: CXX_FEATURE:0cxx_nonstatic_member_init\nFeature record: CXX_FEATURE:0cxx_nullptr\nFeature record: CXX_FEATURE:0cxx_override\nFeature record: CXX_FEATURE:0cxx_range_for\nFeature record: CXX_FEATURE:0cxx_raw_string_literals\nFeature record: CXX_FEATURE:0cxx_reference_qualified_functions\nFeature record: CXX_FEATURE:0cxx_relaxed_constexpr\nFeature record: CXX_FEATURE:0cxx_return_type_deduction\nFeature record: CXX_FEATURE:0cxx_right_angle_brackets\nFeature record: CXX_FEATURE:0cxx_rvalue_references\nFeature record: CXX_FEATURE:0cxx_sizeof_member\nFeature record: CXX_FEATURE:0cxx_static_assert\nFeature record: CXX_FEATURE:0cxx_strong_enums\nFeature record: CXX_FEATURE:1cxx_template_template_parameters\nFeature record: CXX_FEATURE:0cxx_thread_local\nFeature record: CXX_FEATURE:0cxx_trailing_return_types\nFeature record: CXX_FEATURE:0cxx_unicode_literals\nFeature record: CXX_FEATURE:0cxx_uniform_initialization\nFeature record: CXX_FEATURE:0cxx_unrestricted_unions\nFeature record: CXX_FEATURE:0cxx_user_literals\nFeature record: CXX_FEATURE:0cxx_variable_templates\nFeature record: CXX_FEATURE:0cxx_variadic_macros\nFeature record: CXX_FEATURE:0cxx_variadic_templates\n\n``. I tested commit d70417994a3c845c49c4443e92b26a52b320a759 and it worked fine, then I moved to tested commit a271c36af93c7a3b19dfeb2aefa9ca77a58e52e4 and it stopped working. So I would guess commit a271c36af93c7a3b19dfeb2aefa9ca77a58e52e4 caused the issue.. In another word, it is #428 . @LebedevRI testing.... @LebedevRI okay I updated the .h file intest_include` and it worked. Thanks!. @LebedevRI yeah that's a good idea. ",
    "ldionne": "I really, really want the ability to easily generate charts from GoogleBenchmarks output (that is how I found this PR in the first place). However, I don't see at all how removing native support for useful reporters is going to help that goal. I'd personally like to conserve the console output by default, as it is super useful. Am I missing something?. We can already have JSON output, and so we should be able to generate charts from those right now (given scripts to generate charts from JSON). I just don't see what removing the default reporters we currently have will bring to the table, and I'll be very sad if I need to use an additional command-line tool just to translate that to tabular output.. > everything is simpler both in terms of implementation and conceptually, if reporting in the library is done one way\nI fully agree. On the other hand, not having the tabular format be the default is a huge usability regression for users IMO, and that should weight more than internal code clarity. My use case is registering benchmarks as CMake targets and running them as part of my unit tests. This includes on Travis CI. Having the tabular output is very convenient because I can see the result of my benchmarks without any post-processing, wherever I run the benchmarks. For anything fancier, of course, I'd use JSON and post-process it myself. I personally don't have a use case for CSV and I don't care about it.\nPerhaps one way you could go is to have only the JSON reporter, but then implement the tabular reporting natively on top of JSON. This way, you may get some of the simplifications of having the library pretend it only supports JSON, while still being able to do tabular reports natively. I haven't looked at the code much, so perhaps this makes no sense.. > Not having it at all would be a usability regression. I'm not removing it, just removing it from the library.\nWhich is a usability regression just the same. For example, what if I don't have Python available on my system? It also means that GoogleBenchmark would somehow need to install those Python scripts somewhere when you install the library; you can't assume that we're using the library checked out and built from source.\n\nI'm looking into the compromise of having the tabular and JSON reporters, but not the totally custom reporters, instead supporting that through python tooling.\n\nThe only reason why I'm okay with this is that I don't care about other reports ATM. However, I could easily see someone oppose to that with the same argument I use to oppose the removal of the console reporter, and that would be valid.\n. I signed the CLA.. Looking at the appveyor log, it's not clear to me that the failure is related to this PR.. ",
    "Neumann-A": "Build output (ALL_BUILD using default CMAKE settings) from your fix-cxx11 branch:\nVS2017: \nbuild_output_VS2017.txt\nclang-cl (LLVM 6.0, fails to build 1 project due to exception settings (bug in cmakelists compiler settings?)):\nbuild_output_clang-cl_LLVM_6.txt\nSince RUN_TESTS did not work from the VS IDE and also the test adapter for gtest did not work so I ran the test manually using a batch file:\nVS2017:\nVS2017_out.txt\nVS2017_err.txt\nuser_counters_tabular_tests.exe calls abort()\nclang-cl(LLVM 6):\nLLVM_out.txt\nLLVM_err.txt\nuser_counters_tabular_tests.exe also calls abort()\nFor me the fix seems to work and all other problems seem to be related to other issues\n. ",
    "jayfurmanek": "Oh I guess I was working on the commit level that is a sub-module of protobuf.\nYea, there is the that int64/int64_t bug in the powerpc section, but that whole section is old and generally aimed at 32bit systems.  The new mfspr instruction in the 64bit ISA is better and more appropriate for powerpc64 systems.\nI'll work on the CLA. ",
    "nasica88": "@jayfurmanek \nI'm trying to build caffe2 on ppc64le (POWER8) and it got stuck in this error.    It would be appreciated if you can get started on the patch for this.\n/home/u0017649/caffe2/third_party/benchmark/src/cycleclock.h:82:23: error: \u2018int64\u2019 does not name a type\n   tbl &= -static_cast<int64>(tbu0 == tbu1);\n                       ^\nthird_party/benchmark/src/CMakeFiles/benchmark.dir/build.make:230: recipe for target 'third_party/benchmark/src/CMakeFiles/benchmark.dir/sysinfo.cc.o' failed. ",
    "renjie130": "Hi MichaelMauderer, have you solved this problem, i.e.,malloc assertion failure on ARM64 . Thank you. ",
    "MichaelMauderer": "No sorry, I never had a chance to further look into this.\nOn Thu, 19 Apr 2018, 13:57 renjie130, notifications@github.com wrote:\n\nHi MichaelMauderer, have you solved this problem, i.e.,malloc assertion\nfailure on ARM64\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/google/benchmark/issues/450#issuecomment-382724319,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABXNwu8uHUo3-ovH4iUAtwJLHFHexCQXks5tqIm2gaJpZM4PokwN\n.\n. \n",
    "Kadle11": "Hello MichaelMauderer, Any luck or insights with respect to this issue?. Thank you, LebedevRI.. I made the required changes and the issue still persists.\nCode\n```\ninclude\nstatic void BM_StringCreation(benchmark::State& state) {\n  for (auto _ : state)\n    std::string empty_string;\n}\n// Register the function as a benchmark\nBENCHMARK(BM_StringCreation);\n// Define another benchmark\nstatic void BM_StringCopy(benchmark::State& state) {\n  std::string x = \"hello\";\n  for (auto _ : state)\n    std::string copy(x);\n}\nBENCHMARK(BM_StringCopy);\nBENCHMARK_MAIN();\n```\nValgrind Output\n```\n==25776== Memcheck, a memory error detector\n==25776== Copyright (C) 2002-2015, and GNU GPL'd, by Julian Seward et al.\n==25776== Using Valgrind-3.12.0.SVN and LibVEX; rerun with -h for copyright info\n==25776== Command: ./Test.out\n==25776== \n==25776== Invalid write of size 8\n==25776==    at 0x40DB68: benchmark::internal::Benchmark::Benchmark(char const) (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x405120: benchmark::internal::FunctionBenchmark::FunctionBenchmark(char const, void ()(benchmark::State&)) (benchmark.h:989)\n==25776==    by 0x405070: __static_initialization_and_destruction_0(int, int) (BuggyCode.cpp:8)\n==25776==    by 0x4050F7: _GLOBAL__sub_I_main (BuggyCode.cpp:17)\n==25776==    by 0x43729C: __libc_csu_init (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x5FBF26F: (below main) (libc-start.c:247)\n==25776==  Address 0x6553d30 is 0 bytes after a block of size 176 alloc'd\n==25776==    at 0x4C2C21F: operator new(unsigned long) (vg_replace_malloc.c:334)\n==25776==    by 0x40505B: __static_initialization_and_destruction_0(int, int) (BuggyCode.cpp:8)\n==25776==    by 0x4050F7: _GLOBAL__sub_I_main (BuggyCode.cpp:17)\n==25776==    by 0x43729C: __libc_csu_init (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x5FBF26F: (below main) (libc-start.c:247)\n==25776== \n==25776== Invalid write of size 8\n==25776==    at 0x40DB73: benchmark::internal::Benchmark::Benchmark(char const) (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x405120: benchmark::internal::FunctionBenchmark::FunctionBenchmark(char const, void ()(benchmark::State&)) (benchmark.h:989)\n==25776==    by 0x405070: __static_initialization_and_destruction_0(int, int) (BuggyCode.cpp:8)\n==25776==    by 0x4050F7: _GLOBAL__sub_I_main (BuggyCode.cpp:17)\n==25776==    by 0x43729C: __libc_csu_init (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x5FBF26F: (below main) (libc-start.c:247)\n==25776==  Address 0x6553d38 is 8 bytes after a block of size 176 alloc'd\n==25776==    at 0x4C2C21F: operator new(unsigned long) (vg_replace_malloc.c:334)\n==25776==    by 0x40505B: __static_initialization_and_destruction_0(int, int) (BuggyCode.cpp:8)\n==25776==    by 0x4050F7: _GLOBAL__sub_I_main (BuggyCode.cpp:17)\n==25776==    by 0x43729C: __libc_csu_init (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x5FBF26F: (below main) (libc-start.c:247)\n==25776== \n==25776== Invalid write of size 8\n==25776==    at 0x40DB68: benchmark::internal::Benchmark::Benchmark(char const) (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x405120: benchmark::internal::FunctionBenchmark::FunctionBenchmark(char const, void ()(benchmark::State&)) (benchmark.h:989)\n==25776==    by 0x40509E: __static_initialization_and_destruction_0(int, int) (BuggyCode.cpp:16)\n==25776==    by 0x4050F7: _GLOBAL__sub_I_main (BuggyCode.cpp:17)\n==25776==    by 0x43729C: __libc_csu_init (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x5FBF26F: (below main) (libc-start.c:247)\n==25776==  Address 0x65540b0 is 0 bytes after a block of size 176 alloc'd\n==25776==    at 0x4C2C21F: operator new(unsigned long) (vg_replace_malloc.c:334)\n==25776==    by 0x405089: __static_initialization_and_destruction_0(int, int) (BuggyCode.cpp:16)\n==25776==    by 0x4050F7: _GLOBAL__sub_I_main (BuggyCode.cpp:17)\n==25776==    by 0x43729C: __libc_csu_init (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x5FBF26F: (below main) (libc-start.c:247)\n==25776== \n==25776== Invalid write of size 8\n==25776==    at 0x40DB73: benchmark::internal::Benchmark::Benchmark(char const) (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x405120: benchmark::internal::FunctionBenchmark::FunctionBenchmark(char const, void ()(benchmark::State&)) (benchmark.h:989)\n==25776==    by 0x40509E: __static_initialization_and_destruction_0(int, int) (BuggyCode.cpp:16)\n==25776==    by 0x4050F7: _GLOBAL__sub_I_main (BuggyCode.cpp:17)\n==25776==    by 0x43729C: __libc_csu_init (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x5FBF26F: (below main) (libc-start.c:247)\n==25776==  Address 0x65540b8 is 8 bytes after a block of size 176 alloc'd\n==25776==    at 0x4C2C21F: operator new(unsigned long) (vg_replace_malloc.c:334)\n==25776==    by 0x405089: __static_initialization_and_destruction_0(int, int) (BuggyCode.cpp:16)\n==25776==    by 0x4050F7: _GLOBAL__sub_I_main (BuggyCode.cpp:17)\n==25776==    by 0x43729C: __libc_csu_init (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x5FBF26F: (below main) (libc-start.c:247)\n==25776== \n==25776== Invalid read of size 8\n==25776==    at 0x40F88C: benchmark::internal::BenchmarkFamilies::FindBenchmarks(std::__cxx11::basic_string, std::allocator >, std::vector >, std::ostream) (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x410C02: benchmark::internal::FindBenchmarksInternal(std::__cxx11::basic_string, std::allocator > const&, std::vector >, std::ostream) (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x4069CD: benchmark::RunSpecifiedBenchmarks(benchmark::BenchmarkReporter, benchmark::BenchmarkReporter) (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x405017: main (BuggyCode.cpp:17)\n==25776==  Address 0x6553d30 is 0 bytes after a block of size 176 alloc'd\n==25776==    at 0x4C2C21F: operator new(unsigned long) (vg_replace_malloc.c:334)\n==25776==    by 0x40505B: __static_initialization_and_destruction_0(int, int) (BuggyCode.cpp:8)\n==25776==    by 0x4050F7: _GLOBAL__sub_I_main (BuggyCode.cpp:17)\n==25776==    by 0x43729C: __libc_csu_init (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x5FBF26F: (below main) (libc-start.c:247)\n==25776== \nThe number of inputs is very large. BM_StringCreation will be repeated at least 18446744073708498102 times.\nterminate called after throwing an instance of 'std::length_error'\n  what():  vector::reserve\n==25776== \n==25776== Process terminating with default action of signal 6 (SIGABRT)\n==25776==    at 0x5FD1FFF: raise (raise.c:51)\n==25776==    by 0x5FD3429: abort (abort.c:89)\n==25776==    by 0x57930AC: __gnu_cxx::__verbose_terminate_handler() (in /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.22)\n==25776==    by 0x5791065: ??? (in /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.22)\n==25776==    by 0x57910B0: std::terminate() (in /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.22)\n==25776==    by 0x57912C8: __cxa_throw (in /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.22)\n==25776==    by 0x57B9A0E: std::__throw_length_error(char const) (in /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.22)\n==25776==    by 0x411FB4: std::vector >::reserve(unsigned long) (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x41062C: benchmark::internal::BenchmarkFamilies::FindBenchmarks(std::__cxx11::basic_string, std::allocator >, std::vector >, std::ostream) (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x410C02: benchmark::internal::FindBenchmarksInternal(std::__cxx11::basic_string, std::allocator > const&, std::vector >, std::ostream) (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x4069CD: benchmark::RunSpecifiedBenchmarks(benchmark::BenchmarkReporter, benchmark::BenchmarkReporter) (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x405017: main (BuggyCode.cpp:17)\n==25776== \n==25776== HEAP SUMMARY:\n==25776==     in use at exit: 1,992 bytes in 17 blocks\n==25776==   total heap usage: 33 allocs, 16 frees, 76,594 bytes allocated\n==25776== \n==25776== 144 bytes in 1 blocks are possibly lost in loss record 10 of 16\n==25776==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)\n==25776==    by 0x578FE5F: __cxa_allocate_exception (in /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.22)\n==25776==    by 0x57B99E2: std::__throw_length_error(char const) (in /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.22)\n==25776==    by 0x411FB4: std::vector >::reserve(unsigned long) (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x41062C: benchmark::internal::BenchmarkFamilies::FindBenchmarks(std::__cxx11::basic_string, std::allocator >, std::vector >, std::ostream) (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x410C02: benchmark::internal::FindBenchmarksInternal(std::__cxx11::basic_string, std::allocator > const&, std::vector >, std::ostream) (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x4069CD: benchmark::RunSpecifiedBenchmarks(benchmark::BenchmarkReporter, benchmark::BenchmarkReporter) (in /home/s2group/MKL_Perf/Test.out)\n==25776==    by 0x405017: main (BuggyCode.cpp:17)\n==25776== \n==25776== LEAK SUMMARY:\n==25776==    definitely lost: 0 bytes in 0 blocks\n==25776==    indirectly lost: 0 bytes in 0 blocks\n==25776==      possibly lost: 144 bytes in 1 blocks\n==25776==    still reachable: 1,848 bytes in 16 blocks\n==25776==                       of which reachable via heuristic:\n==25776==                         stdstring          : 40 bytes in 1 blocks\n==25776==         suppressed: 0 bytes in 0 blocks\n==25776== Reachable blocks (those to which a pointer was found) are not shown.\n==25776== To see them, rerun with: --leak-check=full --show-leak-kinds=all\n==25776== \n==25776== For counts of detected and suppressed errors, rerun with: -v\n==25776== ERROR SUMMARY: 6 errors from 6 contexts (suppressed: 0 from 0)\n./NewSh.sh: line 17: 25776 Aborted                 valgrind --leak-check=full ./Test.out\nFurthermore, It seems to happen only on the above mentioned system, The Same code runs fine on other systems.\nThank you for your help.\nSorry for not being more thorough.. Here is the output when compiled with -fsanitize=address,undefined \nThe number of inputs is very large. BM_StringCreation will be repeated at least 18446744073708497794 times.\nterminate called after throwing an instance of 'std::length_error'\n  what():  vector::reserve\n./NewSh.sh: line 17: 24703 Aborted                 ./Test.out\n```\nI'll check if there is an version mismatch \nThank You. . I've copied the 6 required static library archives for libbenchmark into a folder which I am dynamically Linking with the program using the -L flag .The libbenchmark I'm using is the one from this repository. . I've followed the instructions on the Readme of the Repository to build the library. It seems to be working on other systems so I'm assuming that I've followed the instructions correctly. \nTo build the program. I used the two specified flags \n-lbenchmark and -lbenchmark_main\nIs that what you were asking? I will rebuild again and check.. The header file is specified with the -I flag\nThere is no other libbenchmark installed on the system.. Thank You So Much, LebedevRI and dominichamon. It was a version mismatch issue as LebedevRI suggested. Sorry for bothering you.. ",
    "onto": "I signed it!. Tests added, documentation updated. ",
    "mwinterb": "Well, this is bothersome.. Thank you.. ",
    "FredTingaud": "(just to be clear, I am the one who wrote the site).\nYes, the fact that the first bar goes to 1 is expected, both having exactly the same code.\nBut the second one runs in 0.66 this time, which I clearly didn't expect.\nOn your machine, the results you get make total sense to me. Which compiler / flags did you use?\nOn the server, I have the following result for this benchmark:\n\"benchmarks\": [\n    {\n      \"name\": \"DoNothing\",\n      \"iterations\": 261491884,\n      \"real_time\": 11,\n      \"cpu_time\": 3,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"name\": \"DoSomething\",\n      \"iterations\": 305503085,\n      \"real_time\": 9,\n      \"cpu_time\": 2,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"name\": \"Noop\",\n      \"iterations\": 276862199,\n      \"real_time\": 10,\n      \"cpu_time\": 3,\n      \"time_unit\": \"ns\"\n    }\n  ]\n}. (Thanks! I was aiming at something godbolt like.)\nThe NOOP time is exactly the same code as DoNothing, which is why it gives the same time of 3ns.\nBut I wonder why with -03 I get a \"DoSomething\" of 2ns, when it is creating a string on top of running the benchmark.\nAlso, for the raw time, I wrote an explanation in the \"About\" section, but the idea is that there is not enough control on the hardware to have a meaningful raw time. I want people to use the site only to compare benchmarks run on the same hardware at the same time, as it is the only way to get meaningful results.. I updated my first message with the raw output from google benchmark (in json mode). I will try to add a way to get that output in the website, but it might take me a bit more time.. The new ranged-for loop is indeed way faster and solves the issue.\nThanks. I signed it!. CLA signing website refuses to use the correct Google account, so I guess we'll have to wait for it to be corrected before I can sign for this pull request \u00af_(\u30c4)_/\u00af. Thanks @dominichamon! I was so focused on the CLA displaying the wrong address (it only displays the \n\"main\" when I wanted to sign with the secondary), that I didn't see I had the wrong address on my side too.. @dominichamon done for the other typo too.. The url I put in the commit message compares the previous and the new implementations (and it uses google benchmark).\nDo you prefer a local comparison?. The memory footprint is indeed twice the previous one, as partial_sort works with no overhead.\n\nOff topic question: I have a bigger changelist I have been working on, that is not finished and that might not be doable at all. Is there a channel through which I could ask you questions about it?. ",
    "DenisYaroshevskiy": "Upd: Seems like the issue goes away if I use BENCHMARK instead of BENCHMARK_TEMPLATE.\nIs this even possible?. That makes sence. Didn't reproduce since though.\nHere is gist, I didn't know about it, don't use github that often: https://gist.github.com/DenisYaroshevskiy/c1c28e3fd95f8a7eb53ddab40dc8464f. This is mine compilation line. \nclang++ --std=c++14 -O3 -Werror -Wall $1   -I /space/google_benchmarks/include/ /space/google_benchmarks/build/src/libbenchmark.a  set_union_bench.cc\nReproduced the bug again, no problem.\nI use https://github.com/google/benchmark/releases/tag/v1.2.0\nBut previously it manifested itself on master too.. I can answer faster in Telegram, if it helps: @dyaroshev \nAnd the problem still goes away, if you do functions instead.. Yes, libc++, out of the box, no hand waving! I even have the Wall on!\nDo you use c++14?. Ok, seems like the older version of clang is the problem - I reproduced your compilation error on quick-bench.\nHere you go, a fixed gist: https://gist.github.com/DenisYaroshevskiy/604f1e331b384579b2a4fb5eac3ac8ef\nI even made it single file\nThe problem is still here.. Hi!\nI found a repro, that kinda works on quick-bench too.\nHere is the code: http://quick-bench.com/pFHnrUKpCRJMgTPBH6pcZImXYIw\nEnabling/Disabling CurrentSetUnion makes LinearSetUnion slower by a millisecond.\nOn my machine, things are much worse:\n```\n\nBenchmark                         Time           CPU Iterations\nbaseline/560/1440               152 ns        152 ns    4313718\nLinearSetUnion/560/1440        4705 ns       4703 ns     149426\nCurrentSetUnion/560/1440       3130 ns       3130 ns     222490\n\nBenchmark                        Time           CPU Iterations\nbaseline/560/1440              153 ns        153 ns    4512810\nLinearSetUnion/560/1440       1909 ns       1909 ns     370949\n```. Great work.\nI'm not 100% sure that function alignment is exactly the reason - since I've seen examples where changing from BENCHMARK to BENCHMARK_TEMPLATE causes the bug to occur, which shouldn't affect the code gen (I think).\nHere is what I can do:\nI have about 10 benchmarks, that consecutively compare: previous version/current version.\n(v1, v2) (v2, v3) ...\nAt one of the comparisons either at -O2 or -O3 the issue would occur.\nIf you can tell me what I should do, I can redo the benchmarks (with new clang and updated google benchmark) and see if the issue would reoccur.\nIf it does not - we can call it a day. If it does than it's not it.. @dominichamon \nThank you, I'll try that. . I should really be reading comments better:\n'\n// Defines and statically (i.e. at link time) initializes a static mutex.\n// The initialization list here does not explicitly initialize each field,\n// instead relying on default initialization for the unspecified fields. In\n// particular, the owner_ field (a pthread_t) is not explicitly initialized.\n// This allows initialization to work whether pthread_t is a scalar or struct.\n// The flag -Wmissing-field-initializers must not be specified for this to work.\n`\nAnyways, seems like you force gtests to compile with extra warnings. . ",
    "Yangqing": "@dominichamon - thanks for the comment. The PR is actually in order to enable one to not install it - see \nhttps://github.com/google/googletest/blob/b7e8a993b4125d1083cb431d91407d8ee4dba2ad/CMakeLists.txt#L26\nhttps://github.com/google/googletest/blob/deace2546ef4ff8f3e6349ec4f26d414753818c3/googletest/CMakeLists.txt#L113. Ah, so basically, it is similar to the use case of googletest - for some projects (github.com/caffe2/caffe2 to be specific), we want to use test and benchmark in our build when we do USE_TEST, and\n(1) we do not want to burden the user to install googletest and benchmark.\n(2) we don't really want to install googletest and benchmark as part of our library - to avoid duplicated installation problems - so we will just link them statically.\nIn these cases having an option INSTALL_{GTEST,BENCHMARK} comes really handy.\nBy the way, I am a bit confused by the Appveyor failure - this should have introduced any behavior change with default settings, should I ignore it?\nThanks - YQ. Done - PTAL.. Thanks! Basically, the cxx_feature_check should largely stay the same unless one changes the compiler stack on the machine - in which case, it would probably mandate a cmake clean anyway.. Thanks - will do.. ",
    "leokoppel": "Just curious, is there some way this is better than using EXCLUDE_FROM_ALL in the enclosing project, as suggested here? https://github.com/google/googletest/issues/868. Yes, the slowdown occurs without the two \"optimizations\" I list. Running the BM_SetInsert variant in the README:\nbefore: 40s\nafter: 10 minutes later it's on the third item, 8192/1\nShould I edit the ranges in the README as well? I don't want to make it over-complicated but it should be possible to run the readme code.. OK, I made them consistent. Runtime of benchmark_test --benchmark_filter=BM_SetInsert:\nbefore: 0m33.991s\nafter: 0m46.226s. ",
    "apmanol": "Indeed that was my second attempt:\n```\n$cmake -DCMAKE_TOOLCHAIN_FILE=/home/manap/Projects/xpu/nmsngv3/tools/share/cmake/toolchain-mips-wibasip.cmake -DBENCHMARK_ENABLE_TESTING=OFF -DHAVE_POSIX_REGEX=0  .. \n...\n-- Performing Test HAVE_STD_REGEX\nCMake Warning at cmake/CXXFeatureCheck.cmake:37 (message):\n  If you see build failures due to cross compilation, try setting\n  HAVE_STD_REGEX to 0\nCall Stack (most recent call first):\n  CMakeLists.txt:180 (cxx_feature_check)\n-- Performing Test HAVE_STD_REGEX -- success\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile\n-- Performing Test HAVE_STEADY_CLOCK\nCMake Warning at cmake/CXXFeatureCheck.cmake:37 (message):\n  If you see build failures due to cross compilation, try setting\n  HAVE_STEADY_CLOCK to 0\nCall Stack (most recent call first):\n  CMakeLists.txt:190 (cxx_feature_check)\n-- Performing Test HAVE_STEADY_CLOCK -- success\n[  6%] Building CXX object src/CMakeFiles/benchmark.dir/benchmark.cc.o\ncd /projects/Thirdparty/benchmark/builds_mips2/src && /opt/cross-mips-wibasip/cross.mips/bin/mips-wibasip-linux-gnu-g++  -DHAVE_POSIX_REGEX -DHAVE_STD_REGEX -DHAVE_STEADY_CLOCK -I/projects/Thirdparty/benchmark/include -I/projects/Thirdparty/benchmark/src -I/projects/Thirdparty/benchmark/src/../include  -std=c++11  -Wall  -Wextra  -Wshadow  -pedantic  -pedantic-errors  -Wfloat-equal  -fstrict-aliasing  -Wzero-as-null-pointer-constant  -Wstrict-aliasing   -o CMakeFiles/benchmark.dir/benchmark.cc.o -c /projects/Thirdparty/benchmark/src/benchmark.cc\n``. No, I am not aware of this option. Whatfoo` should be?. The trick and explanation is here , I think:\n```\nCMAKE_AR=/usr/bin/ld.gold\nCMAKE_RANLIB=/usr/bin/ld.gold\nadd_cxx_compiler_flag(-flto)\nadd_cxx_compiler_flag(-Wodr)\nSET(CMAKE_AR  \"gcc-ar\")\nSET(CMAKE_C_ARCHIVE_CREATE \" qcs   \")\nSET(CMAKE_C_ARCHIVE_FINISH   true)\nset(CMAKE_CXX_ARCHIVE_CREATE \" qcs   \")\nset(CMAKE_CXX_ARCHIVE_FINISH true)\n```. ",
    "lordalcol": "I am currently trying to build for Android using:\ncmake ../ -G Ninja -DCMAKE_SYSTEM_NAME=Android -DANDROID_ABI=arm64-v8a -DCMAKE_TOOLCHAIN_FILE=%ANDROID_NDK%/build/cmake/android.toolchain.cmake -DANDROID_TOOLCHAIN=clang -DCMAKE_BUILD_TYPE=Release -DBENCHMARK_ENABLE_TESTING=false -DANDROID_STL=gnustl_static -DANDROID_NATIVE_API_LEVEL=android-22 -DBENCHMARK_USE_LIBCXX=ON -DBENCHMARK_ENABLE_EXCEPTIONS=Off\nninja\nBut I get the error:\n../src/sysinfo.cc:213:23: error: no member named 'stoul' in namespace 'std'\n    CPUMask Mask(std::stoul(Part, nullptr, 16));\n                 ~~~~~^\n../src/sysinfo.cc:219:11: error: invalid operands to binary expression ('int' and 'void')\n    total += CountBits(Val.substr(0, Pos));\n    ~~~~~ ^  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n../src/sysinfo.cc:223:11: error: invalid operands to binary expression ('int' and 'void')\n    total += CountBits(Val);\n    ~~~~~ ^  ~~~~~~~~~~~~~~\n../src/sysinfo.cc:387:26: error: no member named 'stoi' in namespace 'std'\n        int CurID = std::stoi(value);\n                    ~~~~~^\n../src/sysinfo.cc:460:41: error: no member named 'stod' in namespace 'std'\n        double cycles_per_second = std::stod(value) * 1000000.0;\n                                   ~~~~~^\n../src/sysinfo.cc:465:27: error: no member named 'stod' in namespace 'std'\n        bogo_clock = std::stod(value) * 1000000.0;\n                     ~~~~~^\n6 errors generated.\nIf I use c++_static instead it compiles but when I try to link it against my native lib it throws hundreds of errors like: Error:(24) undefined reference tostd::__ndk1::clog'orError:(538) undefined reference to std::__ndk1::cerr'. ",
    "mkwork": "\nI am currently trying to build for Android using:\ncmake ../ -G Ninja -DCMAKE_SYSTEM_NAME=Android -DANDROID_ABI=arm64-v8a -DCMAKE_TOOLCHAIN_FILE=%ANDROID_NDK%/build/cmake/android.toolchain.cmake -DANDROID_TOOLCHAIN=clang -DCMAKE_BUILD_TYPE=Release -DBENCHMARK_ENABLE_TESTING=false -DANDROID_STL=gnustl_static -DANDROID_NATIVE_API_LEVEL=android-22 -DBENCHMARK_USE_LIBCXX=ON -DBENCHMARK_ENABLE_EXCEPTIONS=Off\nninja\nBut I get the error:\n../src/sysinfo.cc:213:23: error: no member named 'stoul' in namespace 'std'\n    CPUMask Mask(std::stoul(Part, nullptr, 16));\n                 ~~~~~^\n../src/sysinfo.cc:219:11: error: invalid operands to binary expression ('int' and 'void')\n    total += CountBits(Val.substr(0, Pos));\n    ~~~~~ ^  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n../src/sysinfo.cc:223:11: error: invalid operands to binary expression ('int' and 'void')\n    total += CountBits(Val);\n    ~~~~~ ^  ~~~~~~~~~~~~~~\n../src/sysinfo.cc:387:26: error: no member named 'stoi' in namespace 'std'\n        int CurID = std::stoi(value);\n                    ~~~~~^\n../src/sysinfo.cc:460:41: error: no member named 'stod' in namespace 'std'\n        double cycles_per_second = std::stod(value) * 1000000.0;\n                                   ~~~~~^\n../src/sysinfo.cc:465:27: error: no member named 'stod' in namespace 'std'\n        bogo_clock = std::stod(value) * 1000000.0;\n                     ~~~~~^\n6 errors generated.\nIf I use c++_static instead it compiles but when I try to link it against my native lib it throws hundreds of errors like: Error:(24) undefined reference tostd::__ndk1::clog'orError:(538) undefined reference to std::__ndk1::cerr'\n\nIt's not seems to be related to this concrete issue. Also it looks like this issue is  partially fixed in 1.4.1. At least while CMAKE_CROSSCOMPILING=ON manually defined.. ",
    "furkanusta": "I think instead of putting README in the same place with headers, you can use doc folder. I have lots of documentations installed by third-party libraries in both /usr/share/doc/ and /usr/local/share/doc. ",
    "madars": "Very interesting! CMake FAQ suggests two ways of specifying the compiler; first is setting CXX environment variable as above. Second is using the -DCMAKE_CXX_COMPILER command line argument. I tried this second way now, but unfortunately with the same results.. By the way, I just tried the update-alternatives approach on a fully-updated Ubuntu 17.10 and it didn't work:\n$ c++ --version\nclang version 4.0.1-6 (tags/RELEASE_401/final)\nTarget: x86_64-pc-linux-gnu\nThread model: posix\nInstalledDir: /usr/bin\n$ ninja -j1 -v\n[1/18] : && /usr/bin/c++  -std=c++11  -Wall  -Wextra  -Wshadow  -pedantic  -pedantic-errors  -Wshorten-64-to-32  -Wfloat-equal  -fstrict-aliasing  -Wstrict-aliasing  -Wthread-safety  -flto  -rdynamic -Wno-odr test/CMakeFiles/cxx03_test.dir/cxx03_test.cc.o  -o test/cxx03_test  src/libbenchmark.a -lpthread -lrt && :\nFAILED: test/cxx03_test \n: && /usr/bin/c++  -std=c++11  -Wall  -Wextra  -Wshadow  -pedantic  -pedantic-errors  -Wshorten-64-to-32  -Wfloat-equal  -fstrict-aliasing  -Wstrict-aliasing  -Wthread-safety  -flto  -rdynamic -Wno-odr test/CMakeFiles/cxx03_test.dir/cxx03_test.cc.o  -o test/cxx03_test  src/libbenchmark.a -lpthread -lrt && :\nsrc/libbenchmark.a: error adding symbols: Archive has no index; run ranlib to add one\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\nninja: build stopped: subcommand failed.. As another data point, I tried updating clang (more precisely, installing clang-5.0 package for which clang++ -v gives clang version 5.0.0-3 (tags/RELEASE_500/final)) and the results here are the same.. ",
    "gladk": "Yes, thanks! I have cherry-picked the commit and now it builds on s390x also [1].\n[1] https://buildd.debian.org/status/package.php?p=benchmark. No it was fixed for the previous version. But is broken in 1.4.1. OK, I will check with the latest version in git and report.. gdb.txt\nI have attached the backtrace with -O0 and the current git version. As you pointed,  the problem is in frame 9 probably:\nvalue = \" version = FF,  identification = 01F467,  machine = 2964\". Just some more info about the processors on this machine. \n```\ncat /proc/cpuinfo\nvendor_id       : IBM/S390\nprocessors    : 2\nbogomips per cpu: 9057.00\nmax thread id   : 0\nfeatures        : esan3 zarch stfle msa ldisp eimm dfp edat etf3eh highgprs te vx sie\ncache0          : level=1 type=Data scope=Private size=128K line_size=256 associativity=8\ncache1          : level=1 type=Instruction scope=Private size=96K line_size=256 associativity=6\ncache2          : level=2 type=Data scope=Private size=2048K line_size=256 associativity=8\ncache3          : level=2 type=Instruction scope=Private size=2048K line_size=256 associativity=8\ncache4          : level=3 type=Unified scope=Shared size=65536K line_size=256 associativity=16\ncache5          : level=4 type=Unified scope=Shared size=491520K line_size=256 associativity=30\nprocessor 0: version = FF,  identification = 01F467,  machine = 2964\nprocessor 1: version = FF,  identification = 01F467,  machine = 2964\ncpu number      : 0\ncpu MHz dynamic : 5000\ncpu MHz static  : 5000\ncpu number      : 1\ncpu MHz dynamic : 5000\ncpu MHz static  : 5000\n```. If I hardocode the line 411 in src/sysinfo.cc, tests are passing. So the problem is that s390x machines are having another cpuinfo-output.. OK, @LebedevRI I will try to provide a patch. Thanks.. @LebedevRI thanks for the valuable comment. I improved the line which extracts the processor number. Now it works for any number of CPUs.. ",
    "eliaskosunen": "@EricWF I'm fairly certain that I'm not mistaken here, but I may as well be. Reading Microsoft documentation, it seems to me that __forceinline, in some cases, simply overrides compiler analysis regarding the effectiveness of function inlining and subtituting one for the other should not have any behavioral effect or influence on linkage.\nNonetheless, without this PR, inline __forceinline gives the warning C141 (Inline used more than once), while __forceinline doesn't.\nAppveyor build (of my personal project) with patch, without patch. Appveyor bot seems to agree. @EricWF @dominichamon Ping?. Is there any sort of plan to resolve this issue?. Looking more into this issue, I agree.. ",
    "spinosae": "Hi @EricWF, thanks for your prompt reply. I am a new user of Google Benchmark. Therefore it took me a while to notice the problem and figure out where the trap was. It is not so obvious that initialization of fixtures happens before main. Wouldn't it be better if benchmarks are registered in main function by default (i.e. in BENCHMARK_MAIN)? \nRegisterBenchmark doesn't seem to support fixture benchmark, unless I missed something.\nAnyway, this is probably not a common problem. Feel free to close it if you feel it's not relevant. . ",
    "qzmfranklin": "Hi @dominichamon ,\nThanks for the reply!\nI saw the one you listed before I created this PR.\nI think these two PRs have the same spirit of bazeling the build. Here are some differences:\n1. This PR does not modify the existing source code at all, thus minimizing the impact on the existing build system. The other one needs to modify the source code and has thus broken a few unit tests.\n2. The BUILD file of this PR is only 69 lines of code. The other one was 392 lines of code. They achieved the same effects.\nGiven the above, I believed there was still value in creating this PR.\nHow do you think?\nThanks,\nZhongming\n. Will update with a Bazel build test for Travis. Though, at this very moment, user_counters_tabular_test fails. I will add it and hopefully get some guidance from you as to what can be done to fix that test.. OK, before that, the Travis bot got a case-sensitivity issue. This run failed to create the build directory:\n```\n$ mkdir -p build && cd build\nmkdir: build: Not a directory\n```\nThis is because the filesystem it uses is case-insensitive and this PR is adding a file named BUILD. Is there a way to make the filesystem used in the Travis bot case-sensitive?\nI have found a few things on Travis:\nhttps://github.com/travis-ci/docs-travis-ci-com/issues/209\nLooks like they just do not have case-sensitivity be default on some bots.\nI am going to rename BUILD to BUILD.bazel for now. There is no downside,  AFAIK, to using BUILD.bazel other than the fact that the file name is longer.\n. @dominichamon \nUrrhh, not successful with adding the bazel test yet. Somehow it fails on TravisCI bot.\nI need to go to sleep soon. Will continue tomorrow.\nThanks for all your advice so far. Really appreciate them.. Have not got time to work on this one tonight. Will try tomorrow.. The former.\nHere is an explanation of how the new_git_repository rule registers a git repository as a dependency:\nhttps://docs.bazel.build/versions/master/be/workspace.html#git_repository. But I believe you might have had question regarding the comment of copying the content of the BUILD file. The in-source comment is, to the best of my knowledge, stating the facts and rationale. Hopefully with a look at the git_repository rule that I pasted above, it clears out for you and potentially other reviewers.. Yes. But we do not need to specify that dependency here because output_test_helper already depends on :benchmark.. Thanks! Removed. Will update with the next update.. Thanks! Will remove with the next update.. ",
    "mstankus": "You are right. I made an assumption that I should not have. Here is the command which worked in the end. I am on a MAC, I used home-brew to install gcc and it was necessary to include the -lc++ part. \ng++-7   junk.cpp -L /usr/local/lib -I /usr/local/include -lbenchmark -lc++\nShould something like \"add the -lc++\" be added to the documentation\nas appropriate?\nMark Stankus. ",
    "lijinpei": "The current commit has race condition when FixtureBenchmark.Run() is called from multiple threads, please don't merge it.. I will rewrite a commit to just handle the fixtures, the class hierarchy will be like this commit, and the Benchmark base class will have a virtual member named like \"private: virtual void prepare();\" to give sub-class a chance to construct fixtures before benchmark is run.\nApply() won't be changed.\nPlus multiple coding style changes.. I can't reproduce the compiler error on my local machines, that's the command line you use to compile the code ?. Also, I think Benchmark::SetName() can be removed, because it's a protected method, and currently no sub-class uses it. But I didn't remove it in this pr.. Travis CI's failure is caused by apt https://travis-ci.org/google/benchmark/jobs/325788181 , it succeeded in my repo https://travis-ci.org/lijinpei/benchmark/builds/325782844 .. I will fix those problems, add the test and re-pr against the main branch.. I will add the new test https://gist.github.com/EricWF/e033d66c024fe65f9e15705b653bd3f7 as fixture_init_test.cc, is this OK?. I can revert to the version which doesn't change the header file very much if you don't want to merge the newest change.. I can re-start to work on this.\nBTW, on my machine(gcc 8.1.1), state_assembly_test.cc compiles to the following state_assembly_test.s output, I think it is just a reorder of the loop header/body of what FileCheck means to check, but FileCheck won't accept it. . I will change the code.. This method should be private.. I think it is good practice to add a  private virtual member function named setUp/prepare or something like this.. I will change the code.. I will change the code.. I will change the code.. I will change the code.. I will change the code.. This function is in the public api because there is this function https://github.com/google/benchmark/blob/master/include/benchmark/benchmark.h#L856 in the public api, I just keep them similar.  If users decide to write their own main function, and register benchmarks themselves, they may need this function. But class Benchmark itself is in the internal namespace, and I think it's wired for a function in the public api to return or accept parameters of types defined in the internal namespace.. Inline virtual function?  The compiler has to be able to determine the pointee's type at compile time to truly inline the function, as the benchmarks are run()ed through a std::vector>, I don't think the compiler much of the function call?. OK, I will do this.. Yes, the template function is a good idea.. I will change it to upper case, and use \"Init()\" if no others object. @dominichamon . I will move the three RegisterBenchmark() functions(one for function, one for fixture, and if CXX11, one for lambda) to internal namespace.. Because this is only needed if you use lambda benchmarks, function and fixture benchmarks are registered using the non-internal RegisterBenchmark().. All Benchmakr:: member functions are already in benchmark_register.cc, maybe we shall use a seperate commit to move them to benchmark.cc. OK, maybe I shall also move benchmark's member function definitions to benchmark.cc?. OK, I will do it.. OK. OK. just to mention it, all those comments are 81 characters width(not including the newline). Of the existing 35 testcases, only one(statistics_test) uses gtest. I add two testcases which does essentially the same thing, fixture_init_test.cpp doesn't use gtest while fixture_init_gtest.cpp does.. ",
    "f9rocket": "@dominichamon yep, that seems to be the case: https://github.com/bazelbuild/bazel/blob/master/tools/cpp/CROSSTOOL#L626. ",
    "manparvesh": "Sorry, it seems that there are some things that I missed. I'll close this and create another PR when I'm done.. ",
    "sam-panzer": "I appreciate the quick review! This is my first time using Github in years, so I apologize in advance for any procedural mistakes.. My goal with this PR is to unify the open-source Google Benchmark API with the internal version. KeepRunningBatch() is missing from the open-source version.\nI've taken a cue from the internal version to create a fast path in the common case that the benchmark has already started and will continue running. What do you think?. Indeed. I tried to mention the worse codegen above, but it looks like the comment didn't go through. As you suggested, a separate is_batch parameter restored nicer codegen.. @EricWF Done. The build bots seem happy now. Thanks for fixing them!. Looks like this PR isn't useful after updating to upstream. Closing.. Done.. Done.. Good question. total_iterations_ is now set to 0 when State is initialized. StartKeepRunning() can set total_iterations_ to any positive number of iterations. For example, if max_iterations is 10 and KeepRunningBatch() is given a batch size of 100, we should hit this path. I see that I can fold it into the  if block just above the call to FinishKeepRunning(), though I wouldn't be surprised if it doesn't change generated code with optimizations enabled. Does this address your comment, or were you looking for something else?\nOn another front, how much does code generation matter this far down? The fast path should be hit every time within a benchmark except for up to two calls:\n 1. The first call, which starts the benchmark\n 2. The last call, either due to error or because it's the last iteration/batch.. I'll send a PR to implement that.. I'll send a PR to assert and note it.. Fixes are in https://github.com/google/benchmark/pull/526.. I'm not sure what you mean. Were you looking for BENCHMARK_BUILTIN_EXPECT or something else?. ",
    "Croydon": "@dominichamon @chfast Is simplifying this condition the only reason why this didn't get merged yet?\nIf so I would be happy to open a new pull request.. I'm working on something non-official here, but would be nice if we can get official support https://github.com/Croydon/conan-google_benchmark. This would be also required for https://github.com/grpc/grpc/issues/15363. That was indeed my goal with this recipe :). Linux, macOS and Windows static builds seem to be fine, Windows shared fails and I don't understand why yet \nIdeas would be appreciated \nhttps://ci.appveyor.com/project/Croydon/conan-google-benchmark-83vi6/build/job/q9nm3togdwumnct8#L1609. @raulbocanegra This is a general problem, not a Conan recipe specific one. See #640 . I am preparing a pull request here: https://github.com/Croydon/benchmark\n// @p-groarke Could you please test if the recipe is fulfilling your needs?. CLA is fine as can be seen in #523 \nI just wanted to give more credits to the author of the original pull request by adding aJetHorn as the co-author.. This PR has nothing to do anymore with @aJetHorn. I took @LebedevRI's code suggesting from here: https://github.com/google/benchmark/pull/638#discussion_r202648802 and that is literally the entire change left within this PR.\nSo if anyone I would need to add @LebedevRI as the co-auther, but then the Google Bot would likely say again cla unlear (it can't handle co-authers from my understanding).\nAnd again: @aJetHorn did agree to the cla in this PR: #523 - but again nothing left from the other PR here :). The failure of Travis seems to be unrelated since I have neither touched source code nor the previous existing Travis jobs.. ## Tl;dr:\n  * we can get rid of the version string \n  * but then I even more strongly advocate for setting up a Conan repository on Bintray than I would have done anyway :)\n\nSome thoughts..\nA conan reference has the following elements:\n<name> / <version> @ <user> / <channel>\nSo the variables for Benchmark are version & channel:\nbenchmark / <version> @ google / <channel>\nThe version is in most cases the common version number like 1.4.1\nThe channel is freely selectable, but common practice within the Conan ecosystem is to use either stable or testing.\nThe full package reference is decided at the Conan export step. So the reference, including the version and channel can't be changed at a later step (without a re-export(, or some form of alias)).\nThe channel is not set within the Conan recipe, but rather directly at export, together with the user, like this:\nconan export . <user>/<channel>\nconan export . google/testing\nSo common practice is: Name + version from the recipe; user + channel from the export step.\nHowever, I fully understand that it is annoying to change the version via commit for each release and maybe even change it again directly after the release to something like \"1.4.2-dev\".\nIt is possible to remove the version string from the recipe, which I have done in my latest commits. This requires then to specify the full reference at the export step like:\nconan export . benchmark/1.4.1@google/stable\nBy this way we got rid of the annoyance for the people doing the releases, but now got an very uncomfortable situation for all consumers of the recipe, which is a strong contradiction to one of the main goals of a package manager: Making it easier to use and handle the library. Consumers would now be required to clone the repository, then doing the export step with a reference which hopefully doesn't bring in some misunderstanding about what Benchmark version is currently in use; also two projects may use completely different references, even though both use the very same version of Benchmark.\nSolution\nThe annoyance of manually maintaining the version string and the mess and complication for consumers can be best avoided by setting up a Conan repository and let Travis upload new recipe versions automatically.\nI have already written the code for this. I have done it in a way that every tag gets a \nbenchmark/<version>@google/stable release and there is an evergreen benchmark/master@google/testing version.\nThe things you would now have to do would include:\n\nuse/create a free account on http://bintray.com\nthere is already a Google account, not sure if you would like to use this or setup a separate Benchmark one \n\n\ncreate a new Conan repository (\"Add New Repository\" -> Type: Conan)\nGo to https://travis-ci.org/google/benchmark/settings and create the environment variables CONAN_UPLOAD, CONAN_LOGIN_USERNAME and CONAN_PASSWORD\nas I have documented directly in the .travis.yml file \n\n\nthe package will be created automatically by the first upload from Travis\nthis will look similar to this https://bintray.com/croydon/conan/benchmark%3Agoogle\n\n\nmerge this pull request, so that the first upload is happing \nfill out the meta information for the package on Bintray (website, License etc.)\non the package overview page you can see a \"Add to Conan Center\" button; follow the steps \nconan-center is the official, central Conan repository which is pre-added for every Conan client\nsince you are the maintainers themselves of the library you will get accepted for sure \n\n\n\nResult\n\nno manually version string maintenance \neasiest way to install Benchmark for consumers\nno git cloning\nno manual exporting with self-defining of user/channel/version \njust conan install benchmark/1.4.1@google/stable\n\n\nonly people who are working on Benchmark itself and do install/test it via Conan, will need to specify the full reference manually\nbonus points: you are helping conan-center to grow :). > Hmm wait, there is no 3rd party quality control of these packages?\n\nThere is a strict quality control for conan-center when you are packaging third-party libraries, it's much more relaxed when you are the maintainer of the library itself, for I guess obvious reasons.\n\nAnd there is just one 'official' prebuilt version of the library (via travis) that is just assumed to be good enough for everybody?\n\nThe Travis script I have scripted right now is not uploading prebuilt packages, only the recipe. Conan is automatically building the package if there isn't a prebuilt package available for the current os/version/arch/configuration/options...\nWe can surely add this as well, but the CI is then going to be a lot bigger. I wanted to start small and expend afterwards based on feedback.. @dominichamon Almost. I need a little bit more time to get details right. This pull request did stale for some time because I kinda wished for more early feedback \ud83d\ude05\nAnyway, did you setup already a Bintray repository which can be used?. @dominichamon This is now ready for review. Please don't forget about setting up Bintray as it will be almost pointless without \ud83d\ude04 . The CI is looking like this on normal pushes:\nhttps://travis-ci.org/Croydon/benchmark/builds/440290923\nhttps://ci.appveyor.com/project/Croydon/benchmark/builds/19441354\nLike this on tag pushes (in that example is 11.10.2018 the tag name):\nhttps://travis-ci.org/Croydon/benchmark/builds/440291199\nhttps://ci.appveyor.com/project/Croydon/benchmark/builds/19441379\nAnd this builds the following matrix on tag pushes (= releases):\n\nEdit: And I just noticed that I missed to set the environment variables for my personal AppVeyor setup, so obviously there will be Windows builds as well.\nEdit 2: This is the full build matrix:\n\n. If the Bintray organisation is created and all necessary environment variables are set in both Travis UI and AppVeyor UI (as documented) then it should be good to go. The package will be automatically created by the CI via the first upload.\n#   CONAN_UPLOAD - a reference to the Conan repository e.g. https://api.bintray.com/google/conan\n        #   CONAN_LOGIN_USERNAME - user for the upload e.g. upload_bot_username\n        #   CONAN_PASSWORD - password for the CONAN_LOGIN_USERNAME; for Bintray this is the API key. @dominichamon Is there anything I can help with to get this over the finish line?. > I haven't been able to get access to the google conan project (yet) to set up a benchmark one.\nI have seen that you set up https://bintray.com/dominichamon/benchmark which would be already enough. The name of the package will be benchmark/<version>@google/stable no matter if you decide to use https://bintray.com/dominichamon/benchmark or https://bintray.com/google or another Bintray account. So I would say it's rather a question of personal taste which Bintray account you want to use.\nAs a reference we have done something similar with Flatbuffers https://github.com/google/flatbuffers/pull/4594\naardappel did set it up at https://bintray.com/aardappel/flatbuffers, the CI is building and upload automatically packages and Flatbuffers is included in conan-center: https://bintray.com/conan/conan-center?filterByPkgName=flatbuffers%3Agoogle. > Regarding point 2, I think it can be safely merged as it is not uploading anything to Bintray right now. \nTrue, it is safe to merge this, but without Bintray upload it hasn't much value right now.. > Help me out: Is anything else necessary for this to be checked in if we use dominichamon/benchmark (and maybe update it later)?\nNo, it's good to go once the Bintray access variables are set.\n\nI'm not completely content with having API keys in the travis config in github so an alternative to that would be useful.\n\nFully agreed, that it why I would recommend to set them via Travis and AppVeyor UI\nFor Travis: https://travis-ci.org/google/benchmark/settings\nFor AppVeyor: on the project page -> settings -> environment -> add environment variable\nPlease note that while Travis is encrypted via default, AppVeyor needs an explicit click on the lock symbol at the end of the line.\n#   CONAN_UPLOAD - a reference to the Conan repository e.g. https://api.bintray.com/conan/dominichamon/benchmark \n        #   CONAN_LOGIN_USERNAME - user for the upload e.g. dominichamon\n        #   CONAN_PASSWORD - password for the CONAN_LOGIN_USERNAME; for Bintray this is the API key. See: #728 - Minimal Conan support\nAfter #728 is merged, I'm going to rebase this pull request and it will add CI for Conan, including building of Conan packages and publishing.. I have actually reconsidered and removed CONAN_RUN_TESTS and enable_gtest_tests for the sake of simplification. The tests for Benchmark should be continued to run via plain CMake as it is done so far.\nPlease review this @dominichamon. I believe you have been incredible fast in checking out the last commit, which I have dropped within a few seconds, because I commited too much :). Done. Benchmark seems to dynamically get the version from git tags.\nI don't think it's possible to use this here as Conan runs prior to CMake.\nAlso thinking ahead, there might not be a git repository to get the version from, then the version would always fall to 0.0.0.. Conan is creating working and package directories in it's own Conan directory. The original source origin will never be modified.. \n. Since my reply got a bit longer I answered here: https://github.com/google/benchmark/pull/647#issuecomment-408663311 \ud83d\ude04 . Both is possible, but actually for options raising an exception is recommended\nhttps://docs.conan.io/en/latest/uploading_packages/bintray/conan_center_guide.html#recipe-quality\nConan 1.8 (released today) also introduced the ConanInvalidConfiguration exception. Since this is an in-source recipe there is no additional LICENSE file for the Conan recipe. Since we are copying all source files, we also copying the LICENSE file already.. Are you seeing a major advantage in the method from the documentation? If not I would like to keep it this way.. The enable_gtest_tests is only available when CONAN_RUN_TESTS env variable is set. So this should be fine.. Done please review again. Updated. I think this should be fine. Thanks!. ",
    "bansalnvn": "I can produce it with the sample code given in this page as well. \nOne more thing I did compile the libbenchmark from the master branch and the last commit is \ncommit bc83262f9d687ac7e31509b3f5177ff391a7452a (HEAD -> master, origin/master, origin/HEAD)\nAuthor: Tim timothy.joseph.ohearn@gmail.com\nDate:   Sat Feb 3 23:04:36 2018 -0600\n.vs/ and CmakeSettings.json to gitignore (#522)\n```\n[nbansal@lt-hkg1-dws13 codes]$ g++ -std=c++17 bench.cpp -lbenchmark\n[nbansal@lt-hkg1-dws13 codes]$ ./a.out \n2018-02-07 09:27:47\nRun on (16 X 3000 MHz CPU s)\nCPU Caches:\n  L1 Data 32K (x8)\n  L1 Instruction 64K (x8)\n  L2 Unified 512K (x8)\n  L3 Unified 8192K (x2)\nWARNING CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead.\nSegmentation fault (core dumped)\n[nbansal@lt-hkg1-dws13 codes]$ uname -a\nLinux lt-hkg1-dws13 4.14.5-1.el7.elrepo.x86_64 #1 SMP Sun Dec 10 09:54:56 EST 2017 x86_64 x86_64 x86_64 GNU/Linux\n[nbansal@lt-hkg1-dws13 codes]$ less bench.cpp ^C\n[nbansal@lt-hkg1-dws13 codes]$ cat bench.cpp \ninclude \nstatic void BM_StringCreation(benchmark::State& state) {\n    for (auto _ : state)\n        std::string empty_string;\n}\n// Register the function as a benchmark\nBENCHMARK(BM_StringCreation);\n// Define another benchmark\nstatic void BM_StringCopy(benchmark::State& state) {\n    std::string x = \"hello\";\n    for (auto _ : state)\n        std::string copy(x);\n}\nBENCHMARK(BM_StringCopy);\nBENCHMARK_MAIN();\n. I can see, if I use the default system compiler which is 4.8 then it works fine. But if I use gcc 6.4 which I installed later on, then I see this crash.\nFor testing it simpally I am not using cmake, just the command line to compile and run.\n```\n[nbansal@lt-hkg1-dws13 codes]$ /usr/bin/g++ --version\ng++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n[nbansal@lt-hkg1-dws13 codes]$ /usr/bin/g++ -std=c++11 -O3 bench.cpp -lbenchmark\n[nbansal@lt-hkg1-dws13 codes]$ ./a.\nbash: ./a.: No such file or directory\n[nbansal@lt-hkg1-dws13 codes]$ ./a.out \n2018-02-07 10:51:18\nRun on (16 X 3000 MHz CPU s)\nCPU Caches:\n  L1 Data 32K (x8)\n  L1 Instruction 64K (x8)\n  L2 Unified 512K (x8)\n  L3 Unified 8192K (x2)\nWARNING CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead.\n---------------------------------------------------------\nBenchmark                  Time           CPU Iterations\n---------------------------------------------------------\nBM_StringCreation          0 ns          0 ns 1000000000\nBM_StringCopy              5 ns          5 ns  132871934\n[nbansal@lt-hkg1-dws13 codes]$ /opt/lt/rel/x86_64-centos7-linux/gcc/6.4.0/bin/g++ -std=c++17 -O3 bench.cpp -lbenchmark\n[nbansal@lt-hkg1-dws13 codes]$ ./a.out \n2018-02-07 10:51:27\nRun on (16 X 3000 MHz CPU s)\nCPU Caches:\n  L1 Data 32K (x8)\n  L1 Instruction 64K (x8)\n  L2 Unified 512K (x8)\n  L3 Unified 8192K (x2)\nWARNING CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead.\nSegmentation fault (core dumped)\n. Now I have it working. I think it was compiler version mismatch between how I compiled libbenchmark and my benchmark itself.\nNow I can see if I am compiling both of them with the same compiler version, everything works. I am closing the ticket.\n. ",
    "mmcloughlin": "Thanks for the quick reply.\nI would have thought it made sense to leave the BENCHMARK_MAIN macro as it is, since adding context fields doesn't seem like the most common use case. But a method that could be called from a custom main function would be ideal.. Just thought I'd bump this, since I find myself wishing I had this feature a second time :) Right now I have a benchmark that uses a GPU and I would like to be able to output GPU information to the context.. ",
    "SiteshSahoo": "Thanks, it fixed the issue. BTW I have question, is there any specific reason why static library was preferred to a dynamic library ? . ",
    "jmillikin-stripe": "cc @dominichamon\nDominic, will you have time to review this PR, or should I find someone else?. That AppVeyor failure looks unrelated to this PR, since it occurred in the cmake build.. I'd like to leave this as git_repository, because the auto-generated tarballs created by GitHub don't have stable checksums. See https://github.com/tensorflow/tensorflow/issues/12979 and https://github.com/bazelbuild/bazel/issues/3737 for some examples of GitHub breaking builds by tweaking their tarball builder.\nhttp_repository does work well for GitHub \"release\" tarballs uploaded by maintainers, such as the Protobuf releases. Googletest doesn't have any of those, though.. ",
    "brenoguim": "All fair points, although the last one is a bit controversial. Benchmarks could be used to compare implementations and if one implementation heavily relies on the allocator to be warmed up, then it's not a fair comparison.\nStill, I fully agree with the conclusion.\nThank you for the answer!. By the way, I implemented the process launching (linux only of course), and it seems to work.\nBut I was not happy with the precision of getting the resident memory, so I ended up reverting everything and using malloc hooks :) Works like a charm (though its deprecated).. It could be seen as harmful in the sense that it hides potential bugs. If no one should be using the object when shutdown starts, then having the proper destructor would point bugs.\nNow, if it is intended by design that users would/could continue to use this object even during shutdown, then seems to be a fine solution. I don't see how that feature could be useful though.. Some options:\n1 - Find the construct in nvcc that represents the unreachable, and update BENCHMARK_UNREACHABLE\n2 - Make it:\nBENCHMARK_UNREACHABLE();\nreturn \"ns\";\nBut that may trigger warnings on some compiler (doesn't seem to trigger for GCC/clang though).\n3 - Create a BENCHMARK_UNREACHABLE_OR_RETURN(\"ns\") ?  Not great either but should be better than adding the default.. Cool! But for the record, it was not my intention to sound as the responsible for this code. I was just passing by and decided to leave my 2 cents on ways to solve this. I should've made that clearer.. @EricWF  the warning is actually complaining about lack of return in end of non-void function, not about the lack of default. Disabling that warning on the whole code can be harmful.. Option #2 introduces warnings in MSVC :( https://ci.appveyor.com/project/google/benchmark/builds/20198554/job/ub5ulj85wb67ym3o\n. Yeah, adding the default should not be an option.\nI like the std::abort suggestion. Writing the BENCHMARK_UNREACHABLE last fallback as std::abort would keep the good code generation for the targets that support it, and silence the warning for the others.. ",
    "rzuckerm": "$ cmake .. -DCMAKE_BUILD_TYPE=RELEASE -DBENCHMARK_DOWNLOAD_DEPENDENCIES=ON\n-- git Version: v1.2.0-56f52ee2\n-- Version: 1.2.0\n-- Performing Test HAVE_STD_REGEX\n-- Performing Test HAVE_STD_REGEX\n-- Performing Test HAVE_STD_REGEX -- compiled but failed to run\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile\n-- Performing Test HAVE_POSIX_REGEX\n-- Performing Test HAVE_POSIX_REGEX\n-- Performing Test HAVE_POSIX_REGEX -- success\n-- Performing Test HAVE_STEADY_CLOCK\n-- Performing Test HAVE_STEADY_CLOCK\n-- Performing Test HAVE_STEADY_CLOCK -- success\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/developer/git/benchmark/build. ~/git/benchmark/build$ cmake .. -DCMAKE_BUILD_TYPE=RELEASE -DBENCHMARK_DOWNLOAD_DEPENDENCIES=ON\n-- The C compiler identification is GNU 4.8.4\n-- The CXX compiler identification is GNU 4.8.4\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Found Git: /usr/bin/git (found version \"1.9.1\") \n-- git Version: v1.2.0-61497236\n-- Version: 1.2.0\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11 - Success\n-- Performing Test HAVE_CXX_FLAG_WALL\n-- Performing Test HAVE_CXX_FLAG_WALL - Success\n-- Performing Test HAVE_CXX_FLAG_WEXTRA\n-- Performing Test HAVE_CXX_FLAG_WEXTRA - Success\n-- Performing Test HAVE_CXX_FLAG_WSHADOW\n-- Performing Test HAVE_CXX_FLAG_WSHADOW - Success\n-- Performing Test HAVE_CXX_FLAG_WERROR\n-- Performing Test HAVE_CXX_FLAG_WERROR - Success\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC - Success\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS - Success\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32 - Failed\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL - Success\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING - Success\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING - Success\n-- Performing Test HAVE_CXX_FLAG_WD654\n-- Performing Test HAVE_CXX_FLAG_WD654 - Failed\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY - Failed\n-- Performing Test HAVE_CXX_FLAG_WINVALID_OFFSETOF\n-- Performing Test HAVE_CXX_FLAG_WINVALID_OFFSETOF - Success\n-- Performing Test HAVE_CXX_FLAG_WNO_INVALID_OFFSETOF\n-- Performing Test HAVE_CXX_FLAG_WNO_INVALID_OFFSETOF - Success\n-- Performing Test HAVE_CXX_FLAG_COVERAGE\n-- Performing Test HAVE_CXX_FLAG_COVERAGE - Success\n-- Performing Test HAVE_STD_REGEX\n-- Performing Test HAVE_STD_REGEX\n-- Performing Test HAVE_STD_REGEX -- compiled but failed to run\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile\n-- Performing Test HAVE_POSIX_REGEX\n-- Performing Test HAVE_POSIX_REGEX\n-- Performing Test HAVE_POSIX_REGEX -- success\n-- Performing Test HAVE_STEADY_CLOCK\n-- Performing Test HAVE_STEADY_CLOCK\n-- Performing Test HAVE_STEADY_CLOCK -- success\n-- Looking for include file pthread.h\n-- Looking for include file pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\nCMake Warning (dev) at /usr/share/cmake-2.8/Modules/ExternalProject.cmake:242 (message):\n  value 'EXCLUDE_FROM_ALL' with no previous keyword in ExternalProject_Add\nCall Stack (most recent call first):\n  /usr/share/cmake-2.8/Modules/ExternalProject.cmake:1762 (_ep_parse_arguments)\n  cmake/HandleGTest.cmake:34 (ExternalProject_Add)\n  cmake/HandleGTest.cmake:82 (build_external_gtest)\n  CMakeLists.txt:217 (include)\nThis warning is for project developers.  Use -Wno-dev to suppress it.\nCMake Warning (dev) at /usr/share/cmake-2.8/Modules/ExternalProject.cmake:242 (message):\n  value 'ON' with no previous keyword in ExternalProject_Add\nCall Stack (most recent call first):\n  /usr/share/cmake-2.8/Modules/ExternalProject.cmake:1762 (_ep_parse_arguments)\n  cmake/HandleGTest.cmake:34 (ExternalProject_Add)\n  cmake/HandleGTest.cmake:82 (build_external_gtest)\n  CMakeLists.txt:217 (include)\nThis warning is for project developers.  Use -Wno-dev to suppress it.\n-- Performing Test BENCHMARK_HAS_O3_FLAG\n-- Performing Test BENCHMARK_HAS_O3_FLAG - Success\n-- Performing Test BENCHMARK_HAS_CXX03_FLAG\n-- Performing Test BENCHMARK_HAS_CXX03_FLAG - Success\n-- Performing Test BENCHMARK_HAS_WNO_ODR\n-- Performing Test BENCHMARK_HAS_WNO_ODR - Success\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/developer/git/benchmark/build\n~/git/benchmark/build$ make\nScanning dependencies of target googletest\n[  2%] Creating directories for 'googletest'\n[  4%] Performing download step (git clone) for 'googletest'\nCloning into 'googletest'...\nremote: Counting objects: 10660, done.\nremote: Compressing objects: 100% (45/45), done.\nremote: Total 10660 (delta 37), reused 58 (delta 30), pack-reused 10580\nReceiving objects: 100% (10660/10660), 3.25 MiB | 0 bytes/s, done.\nResolving deltas: 100% (7839/7839), done.\nChecking connectivity... done.\nAlready on 'master'\nYour branch is up-to-date with 'origin/master'.\n[  7%] No patch step for 'googletest'\n[  9%] Performing update step for 'googletest'\nAlready on 'master'\nYour branch is up-to-date with 'origin/master'.\n[ 11%] Performing configure step for 'googletest'\nloading initial cache file /home/developer/git/benchmark/build/googletest/tmp/googletest-cache.cmake\n-- The C compiler identification is GNU 4.8.4\n-- The CXX compiler identification is GNU 4.8.4\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Found PythonInterp: /usr/bin/python (found version \"2.7.6\") \n-- Looking for include file pthread.h\n-- Looking for include file pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/developer/git/benchmark/build/googletest/src/googletest-build\n[ 14%] Performing build step for 'googletest'\nScanning dependencies of target gtest\n[ 25%] Building CXX object googlemock/gtest/CMakeFiles/gtest.dir/src/gtest-all.cc.o\nLinking CXX static library libgtest.a\n[ 25%] Built target gtest\nScanning dependencies of target gmock\n[ 50%] Building CXX object googlemock/CMakeFiles/gmock.dir/src/gmock-all.cc.o\nLinking CXX static library libgmock.a\n[ 50%] Built target gmock\nScanning dependencies of target gmock_main\n[ 75%] Building CXX object googlemock/CMakeFiles/gmock_main.dir/src/gmock_main.cc.o\nLinking CXX static library libgmock_main.a\n[ 75%] Built target gmock_main\nScanning dependencies of target gtest_main\n[100%] Building CXX object googlemock/gtest/CMakeFiles/gtest_main.dir/src/gtest_main.cc.o\nLinking CXX static library libgtest_main.a\n[100%] Built target gtest_main\n[ 16%] Performing install step for 'googletest'\n[ 25%] Built target gtest\n[ 50%] Built target gmock\n[ 75%] Built target gmock_main\n[100%] Built target gtest_main\nInstall the project...\n-- Install configuration: \"RELEASE\"\n-- Installing: /home/developer/git/benchmark/build/googletest/lib/x86_64-linux-gnu/libgmock.a\n-- Installing: /home/developer/git/benchmark/build/googletest/lib/x86_64-linux-gnu/libgmock_main.a\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/gmock.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/gmock-generated-function-mockers.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/gmock-generated-actions.h.pump\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/gmock-more-actions.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/gmock-generated-actions.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/gmock-spec-builders.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/gmock-more-matchers.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/gmock-generated-matchers.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/gmock-matchers.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/gmock-generated-function-mockers.h.pump\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/gmock-actions.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/gmock-cardinalities.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/gmock-generated-nice-strict.h.pump\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/gmock-generated-nice-strict.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/internal\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/internal/gmock-port.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/internal/custom\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/internal/custom/gmock-generated-actions.h.pump\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/internal/custom/gmock-generated-actions.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/internal/custom/gmock-port.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/internal/custom/gmock-matchers.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/internal/gmock-internal-utils.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/internal/gmock-generated-internal-utils.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/internal/gmock-generated-internal-utils.h.pump\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gmock/gmock-generated-matchers.h.pump\n-- Installing: /home/developer/git/benchmark/build/googletest/lib/x86_64-linux-gnu/pkgconfig/gmock.pc\n-- Installing: /home/developer/git/benchmark/build/googletest/lib/x86_64-linux-gnu/pkgconfig/gmock_main.pc\n-- Installing: /home/developer/git/benchmark/build/googletest/lib/x86_64-linux-gnu/libgtest.a\n-- Installing: /home/developer/git/benchmark/build/googletest/lib/x86_64-linux-gnu/libgtest_main.a\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/gtest_pred_impl.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/gtest-param-test.h.pump\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/gtest-death-test.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/gtest-message.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/gtest-spi.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/gtest-param-test.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/gtest_prod.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/gtest-test-part.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/internal\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/internal/gtest-filepath.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/internal/gtest-tuple.h.pump\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/internal/gtest-type-util.h.pump\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/internal/gtest-internal.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/internal/gtest-param-util.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/internal/gtest-death-test-internal.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/internal/gtest-type-util.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/internal/custom\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/internal/custom/gtest-port.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/internal/custom/gtest-printers.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/internal/custom/gtest.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/internal/gtest-string.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/internal/gtest-port.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/internal/gtest-param-util-generated.h.pump\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/internal/gtest-tuple.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/internal/gtest-linked_ptr.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/internal/gtest-port-arch.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/internal/gtest-param-util-generated.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/gtest-printers.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/gtest.h\n-- Installing: /home/developer/git/benchmark/build/googletest/include/gtest/gtest-typed-test.h\n-- Installing: /home/developer/git/benchmark/build/googletest/lib/x86_64-linux-gnu/pkgconfig/gtest.pc\n-- Installing: /home/developer/git/benchmark/build/googletest/lib/x86_64-linux-gnu/pkgconfig/gtest_main.pc\n[ 19%] Completed 'googletest'\n[ 19%] Built target googletest\nScanning dependencies of target benchmark\n[ 21%] Building CXX object src/CMakeFiles/benchmark.dir/string_util.cc.o\n[ 23%] Building CXX object src/CMakeFiles/benchmark.dir/reporter.cc.o\n[ 26%] Building CXX object src/CMakeFiles/benchmark.dir/statistics.cc.o\n[ 28%] Building CXX object src/CMakeFiles/benchmark.dir/json_reporter.cc.o\n[ 30%] Building CXX object src/CMakeFiles/benchmark.dir/colorprint.cc.o\n[ 33%] Building CXX object src/CMakeFiles/benchmark.dir/counter.cc.o\n[ 35%] Building CXX object src/CMakeFiles/benchmark.dir/benchmark_register.cc.o\n[ 38%] Building CXX object src/CMakeFiles/benchmark.dir/commandlineflags.cc.o\n[ 40%] Building CXX object src/CMakeFiles/benchmark.dir/csv_reporter.cc.o\n[ 42%] Building CXX object src/CMakeFiles/benchmark.dir/console_reporter.cc.o\n[ 45%] Building CXX object src/CMakeFiles/benchmark.dir/sleep.cc.o\n[ 47%] Building CXX object src/CMakeFiles/benchmark.dir/timers.cc.o\n[ 50%] Building CXX object src/CMakeFiles/benchmark.dir/complexity.cc.o\n[ 52%] Building CXX object src/CMakeFiles/benchmark.dir/benchmark.cc.o\n[ 54%] Building CXX object src/CMakeFiles/benchmark.dir/sysinfo.cc.o\nLinking CXX static library libbenchmark.a\n[ 54%] Built target benchmark\nScanning dependencies of target basic_test\n[ 57%] Building CXX object test/CMakeFiles/basic_test.dir/basic_test.cc.o\nLinking CXX executable basic_test\n[ 57%] Built target basic_test\nScanning dependencies of target benchmark_test\n[ 59%] Building CXX object test/CMakeFiles/benchmark_test.dir/benchmark_test.cc.o\nLinking CXX executable benchmark_test\n[ 59%] Built target benchmark_test\nScanning dependencies of target output_test_helper\n[ 61%] Building CXX object test/CMakeFiles/output_test_helper.dir/output_test_helper.cc.o\nLinking CXX static library liboutput_test_helper.a\n[ 61%] Built target output_test_helper\nScanning dependencies of target complexity_test\n[ 64%] Building CXX object test/CMakeFiles/complexity_test.dir/complexity_test.cc.o\nLinking CXX executable complexity_test\n[ 64%] Built target complexity_test\nScanning dependencies of target cxx03_test\n[ 66%] Building CXX object test/CMakeFiles/cxx03_test.dir/cxx03_test.cc.o\nLinking CXX executable cxx03_test\n[ 66%] Built target cxx03_test\nScanning dependencies of target diagnostics_test\n[ 69%] Building CXX object test/CMakeFiles/diagnostics_test.dir/diagnostics_test.cc.o\nLinking CXX executable diagnostics_test\n[ 69%] Built target diagnostics_test\nScanning dependencies of target donotoptimize_test\n[ 71%] Building CXX object test/CMakeFiles/donotoptimize_test.dir/donotoptimize_test.cc.o\nLinking CXX executable donotoptimize_test\n[ 71%] Built target donotoptimize_test\nScanning dependencies of target filter_test\n[ 73%] Building CXX object test/CMakeFiles/filter_test.dir/filter_test.cc.o\nLinking CXX executable filter_test\n[ 73%] Built target filter_test\nScanning dependencies of target fixture_test\n[ 76%] Building CXX object test/CMakeFiles/fixture_test.dir/fixture_test.cc.o\nLinking CXX executable fixture_test\n[ 76%] Built target fixture_test\nScanning dependencies of target map_test\n[ 78%] Building CXX object test/CMakeFiles/map_test.dir/map_test.cc.o\nLinking CXX executable map_test\n[ 78%] Built target map_test\nScanning dependencies of target multiple_ranges_test\n[ 80%] Building CXX object test/CMakeFiles/multiple_ranges_test.dir/multiple_ranges_test.cc.o\nLinking CXX executable multiple_ranges_test\n[ 80%] Built target multiple_ranges_test\nScanning dependencies of target options_test\n[ 83%] Building CXX object test/CMakeFiles/options_test.dir/options_test.cc.o\nLinking CXX executable options_test\n[ 83%] Built target options_test\nScanning dependencies of target register_benchmark_test\n[ 85%] Building CXX object test/CMakeFiles/register_benchmark_test.dir/register_benchmark_test.cc.o\nLinking CXX executable register_benchmark_test\n[ 85%] Built target register_benchmark_test\nScanning dependencies of target reporter_output_test\n[ 88%] Building CXX object test/CMakeFiles/reporter_output_test.dir/reporter_output_test.cc.o\nLinking CXX executable reporter_output_test\n[ 88%] Built target reporter_output_test\nScanning dependencies of target skip_with_error_test\n[ 90%] Building CXX object test/CMakeFiles/skip_with_error_test.dir/skip_with_error_test.cc.o\nLinking CXX executable skip_with_error_test\n[ 90%] Built target skip_with_error_test\nScanning dependencies of target statistics_test\n[ 92%] Building CXX object test/CMakeFiles/statistics_test.dir/statistics_test.cc.o\n/home/developer/git/benchmark/test/statistics_test.cc:6:25: fatal error: gtest/gtest.h: No such file or directory\n #include \"gtest/gtest.h\"\n                         ^\ncompilation terminated.\nmake[2]:  [test/CMakeFiles/statistics_test.dir/statistics_test.cc.o] Error 1\nmake[1]:  [test/CMakeFiles/statistics_test.dir/all] Error 2\nmake: *** [all] Error 2. Any word on this?. $ cmake --version\ncmake version 2.8.12.2. @EricWF I pulled the latest code, and it fixes my issue. Thanks!. I'm also interested in this as well. I would like to start using an official release. I'm currently using the code that is based on the 68e2289 commit that fixed my issue #538 . I don't know the specific version, but I did the \"git clone\" around March 7, 2018. I just did a git pull, and that test is still failing in the same way. Here's the last commit that I see:\n$ git log\ncommit a9beffda0b89a6995372100456a4ad894d29b93b\nAuthor: jmillikin-stripe <jmillikin@stripe.com>\n.... I'm running the tests using \"make test\" now, and everything is passing. Thanks @dominichamon !. I'm using the latest 1.4.0 release. Yes, it works if I do -DBENCHMARK_DOWNLOAD_DEPENDENCIES=ON. Here's the output:\n```\n~/git/google/benchmark$ rm -rf googletest/ build/\n~/git/google/benchmark$ mkdir build\n~/git/google/benchmark$ cd build\n~/git/google/benchmark/build$ cmake .. -DCMAKE_BUILD_TYPE=RELEASE -DBENCHMARK_DOWNLOAD_DEPENDENCIES=ON\n-- The C compiler identification is GNU 4.8.4\n-- The CXX compiler identification is GNU 4.8.4\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Failed to find LLVM FileCheck\n-- Found Git: /usr/bin/git (found version \"1.9.1\") \n-- git Version: v1.4.0\n-- Version: 1.4.0\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11 - Success\n-- Performing Test HAVE_CXX_FLAG_WALL\n-- Performing Test HAVE_CXX_FLAG_WALL - Success\n-- Performing Test HAVE_CXX_FLAG_WEXTRA\n-- Performing Test HAVE_CXX_FLAG_WEXTRA - Success\n-- Performing Test HAVE_CXX_FLAG_WSHADOW\n-- Performing Test HAVE_CXX_FLAG_WSHADOW - Success\n-- Performing Test HAVE_CXX_FLAG_WERROR\n-- Performing Test HAVE_CXX_FLAG_WERROR - Success\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC - Success\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS - Success\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32 - Failed\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL - Success\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING - Success\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING - Success\n-- Performing Test HAVE_CXX_FLAG_WD654\n-- Performing Test HAVE_CXX_FLAG_WD654 - Failed\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY - Failed\n-- Performing Test HAVE_CXX_FLAG_COVERAGE\n-- Performing Test HAVE_CXX_FLAG_COVERAGE - Success\n-- Performing Test HAVE_STD_REGEX\n-- Performing Test HAVE_STD_REGEX\n-- Performing Test HAVE_STD_REGEX -- compiled but failed to run\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile\n-- Performing Test HAVE_POSIX_REGEX\n-- Performing Test HAVE_POSIX_REGEX\n-- Performing Test HAVE_POSIX_REGEX -- success\n-- Performing Test HAVE_STEADY_CLOCK\n-- Performing Test HAVE_STEADY_CLOCK\n-- Performing Test HAVE_STEADY_CLOCK -- success\n-- Looking for include file pthread.h\n-- Looking for include file pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Performing Test BENCHMARK_HAS_O3_FLAG\n-- Performing Test BENCHMARK_HAS_O3_FLAG - Success\n-- Performing Test BENCHMARK_HAS_CXX03_FLAG\n-- Performing Test BENCHMARK_HAS_CXX03_FLAG - Success\n-- Performing Test BENCHMARK_HAS_WNO_ODR\n-- Performing Test BENCHMARK_HAS_WNO_ODR - Success\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/developer/git/google/benchmark/build\n~/git/google/benchmark/build$ make\nScanning dependencies of target googletest\n[  2%] Creating directories for 'googletest'\n[  4%] Performing download step (git clone) for 'googletest'\nCloning into 'googletest'...\nremote: Counting objects: 11103, done.\nremote: Compressing objects: 100% (70/70), done.\nremote: Total 11103 (delta 110), reused 164 (delta 108), pack-reused 10912\nReceiving objects: 100% (11103/11103), 3.48 MiB | 5.30 MiB/s, done.\nResolving deltas: 100% (8149/8149), done.\nChecking connectivity... done.\nAlready on 'master'\nYour branch is up-to-date with 'origin/master'.\n[  6%] No patch step for 'googletest'\n[  9%] Performing update step for 'googletest'\nAlready on 'master'\nYour branch is up-to-date with 'origin/master'.\n[ 11%] Performing configure step for 'googletest'\nloading initial cache file /home/developer/git/google/benchmark/build/googletest/tmp/googletest-cache.cmake\n-- The C compiler identification is GNU 4.8.4\n-- The CXX compiler identification is GNU 4.8.4\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Found PythonInterp: /usr/bin/python (found version \"2.7.6\") \n-- Looking for include file pthread.h\n-- Looking for include file pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/developer/git/google/benchmark/build/googletest/src/googletest-build\n[ 13%] Performing build step for 'googletest'\nScanning dependencies of target gtest\n[ 25%] Building CXX object googlemock/gtest/CMakeFiles/gtest.dir/src/gtest-all.cc.o\nLinking CXX static library libgtest.a\n[ 25%] Built target gtest\nScanning dependencies of target gmock\n[ 50%] Building CXX object googlemock/CMakeFiles/gmock.dir/src/gmock-all.cc.o\nLinking CXX static library libgmock.a\n[ 50%] Built target gmock\nScanning dependencies of target gmock_main\n[ 75%] Building CXX object googlemock/CMakeFiles/gmock_main.dir/src/gmock_main.cc.o\nLinking CXX static library libgmock_main.a\n[ 75%] Built target gmock_main\nScanning dependencies of target gtest_main\n[100%] Building CXX object googlemock/gtest/CMakeFiles/gtest_main.dir/src/gtest_main.cc.o\nLinking CXX static library libgtest_main.a\n[100%] Built target gtest_main\n[ 16%] Performing install step for 'googletest'\n[ 25%] Built target gtest\n[ 50%] Built target gmock\n[ 75%] Built target gmock_main\n[100%] Built target gtest_main\nInstall the project...\n-- Install configuration: \"RELEASE\"\n-- Installing: /home/developer/git/google/benchmark/build/googletest/lib/libgmock.a\n-- Installing: /home/developer/git/google/benchmark/build/googletest/lib/libgmock_main.a\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-generated-function-mockers.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-generated-actions.h.pump\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-more-actions.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-generated-actions.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-spec-builders.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-more-matchers.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-generated-matchers.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-matchers.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-generated-function-mockers.h.pump\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-actions.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-cardinalities.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-generated-nice-strict.h.pump\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-generated-nice-strict.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/internal\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/internal/gmock-port.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/internal/custom\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/internal/custom/gmock-generated-actions.h.pump\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/internal/custom/gmock-generated-actions.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/internal/custom/gmock-port.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/internal/custom/gmock-matchers.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/internal/gmock-internal-utils.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/internal/gmock-generated-internal-utils.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/internal/gmock-generated-internal-utils.h.pump\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-generated-matchers.h.pump\n-- Installing: /home/developer/git/google/benchmark/build/googletest/lib/pkgconfig/gmock.pc\n-- Installing: /home/developer/git/google/benchmark/build/googletest/lib/pkgconfig/gmock_main.pc\n-- Installing: /home/developer/git/google/benchmark/build/googletest/lib/libgtest.a\n-- Installing: /home/developer/git/google/benchmark/build/googletest/lib/libgtest_main.a\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/gtest_pred_impl.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/gtest-param-test.h.pump\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/gtest-death-test.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/gtest-message.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/gtest-spi.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/gtest-param-test.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/gtest_prod.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/gtest-test-part.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-filepath.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-tuple.h.pump\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-type-util.h.pump\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-internal.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-param-util.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-death-test-internal.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-type-util.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/custom\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/custom/gtest-port.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/custom/gtest-printers.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/custom/gtest.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-string.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-port.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-param-util-generated.h.pump\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-tuple.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-linked_ptr.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-port-arch.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-param-util-generated.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/gtest-printers.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/gtest.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/gtest-typed-test.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/lib/pkgconfig/gtest.pc\n-- Installing: /home/developer/git/google/benchmark/build/googletest/lib/pkgconfig/gtest_main.pc\n[ 18%] Completed 'googletest'\n[ 18%] Built target googletest\nScanning dependencies of target benchmark\n[ 20%] Building CXX object src/CMakeFiles/benchmark.dir/string_util.cc.o\n[ 23%] Building CXX object src/CMakeFiles/benchmark.dir/reporter.cc.o\n[ 25%] Building CXX object src/CMakeFiles/benchmark.dir/statistics.cc.o\n[ 27%] Building CXX object src/CMakeFiles/benchmark.dir/json_reporter.cc.o\n[ 30%] Building CXX object src/CMakeFiles/benchmark.dir/colorprint.cc.o\n[ 32%] Building CXX object src/CMakeFiles/benchmark.dir/counter.cc.o\n[ 34%] Building CXX object src/CMakeFiles/benchmark.dir/benchmark_register.cc.o\n[ 37%] Building CXX object src/CMakeFiles/benchmark.dir/commandlineflags.cc.o\n[ 39%] Building CXX object src/CMakeFiles/benchmark.dir/csv_reporter.cc.o\n[ 41%] Building CXX object src/CMakeFiles/benchmark.dir/console_reporter.cc.o\n[ 44%] Building CXX object src/CMakeFiles/benchmark.dir/sleep.cc.o\n[ 46%] Building CXX object src/CMakeFiles/benchmark.dir/timers.cc.o\n[ 48%] Building CXX object src/CMakeFiles/benchmark.dir/complexity.cc.o\n[ 51%] Building CXX object src/CMakeFiles/benchmark.dir/benchmark.cc.o\n[ 53%] Building CXX object src/CMakeFiles/benchmark.dir/sysinfo.cc.o\nLinking CXX static library libbenchmark.a\n[ 53%] Built target benchmark\nScanning dependencies of target basic_test\n[ 55%] Building CXX object test/CMakeFiles/basic_test.dir/basic_test.cc.o\nLinking CXX executable basic_test\n[ 55%] Built target basic_test\nScanning dependencies of target benchmark_gtest\n[ 58%] Building CXX object test/CMakeFiles/benchmark_gtest.dir/benchmark_gtest.cc.o\nLinking CXX executable benchmark_gtest\n[ 58%] Built target benchmark_gtest\nScanning dependencies of target benchmark_test\n[ 60%] Building CXX object test/CMakeFiles/benchmark_test.dir/benchmark_test.cc.o\nLinking CXX executable benchmark_test\n[ 60%] Built target benchmark_test\nScanning dependencies of target output_test_helper\n[ 62%] Building CXX object test/CMakeFiles/output_test_helper.dir/output_test_helper.cc.o\nLinking CXX static library liboutput_test_helper.a\n[ 62%] Built target output_test_helper\nScanning dependencies of target complexity_test\n[ 65%] Building CXX object test/CMakeFiles/complexity_test.dir/complexity_test.cc.o\nLinking CXX executable complexity_test\n[ 65%] Built target complexity_test\nScanning dependencies of target cxx03_test\n[ 67%] Building CXX object test/CMakeFiles/cxx03_test.dir/cxx03_test.cc.o\nLinking CXX executable cxx03_test\n[ 67%] Built target cxx03_test\nScanning dependencies of target diagnostics_test\n[ 69%] Building CXX object test/CMakeFiles/diagnostics_test.dir/diagnostics_test.cc.o\nLinking CXX executable diagnostics_test\n[ 69%] Built target diagnostics_test\nScanning dependencies of target donotoptimize_test\n[ 72%] Building CXX object test/CMakeFiles/donotoptimize_test.dir/donotoptimize_test.cc.o\nLinking CXX executable donotoptimize_test\n[ 72%] Built target donotoptimize_test\nScanning dependencies of target filter_test\n[ 74%] Building CXX object test/CMakeFiles/filter_test.dir/filter_test.cc.o\nLinking CXX executable filter_test\n[ 74%] Built target filter_test\nScanning dependencies of target fixture_test\n[ 76%] Building CXX object test/CMakeFiles/fixture_test.dir/fixture_test.cc.o\nLinking CXX executable fixture_test\n[ 76%] Built target fixture_test\nScanning dependencies of target map_test\n[ 79%] Building CXX object test/CMakeFiles/map_test.dir/map_test.cc.o\nLinking CXX executable map_test\n[ 79%] Built target map_test\nScanning dependencies of target multiple_ranges_test\n[ 81%] Building CXX object test/CMakeFiles/multiple_ranges_test.dir/multiple_ranges_test.cc.o\nLinking CXX executable multiple_ranges_test\n[ 81%] Built target multiple_ranges_test\nScanning dependencies of target options_test\n[ 83%] Building CXX object test/CMakeFiles/options_test.dir/options_test.cc.o\nLinking CXX executable options_test\n[ 83%] Built target options_test\nScanning dependencies of target register_benchmark_test\n[ 86%] Building CXX object test/CMakeFiles/register_benchmark_test.dir/register_benchmark_test.cc.o\nLinking CXX executable register_benchmark_test\n[ 86%] Built target register_benchmark_test\nScanning dependencies of target reporter_output_test\n[ 88%] Building CXX object test/CMakeFiles/reporter_output_test.dir/reporter_output_test.cc.o\nLinking CXX executable reporter_output_test\n[ 88%] Built target reporter_output_test\nScanning dependencies of target skip_with_error_test\n[ 90%] Building CXX object test/CMakeFiles/skip_with_error_test.dir/skip_with_error_test.cc.o\nLinking CXX executable skip_with_error_test\n[ 90%] Built target skip_with_error_test\nScanning dependencies of target statistics_gtest\n[ 93%] Building CXX object test/CMakeFiles/statistics_gtest.dir/statistics_gtest.cc.o\nLinking CXX executable statistics_gtest\n[ 93%] Built target statistics_gtest\nScanning dependencies of target templated_fixture_test\n[ 95%] Building CXX object test/CMakeFiles/templated_fixture_test.dir/templated_fixture_test.cc.o\nLinking CXX executable templated_fixture_test\n[ 95%] Built target templated_fixture_test\nScanning dependencies of target user_counters_tabular_test\n[ 97%] Building CXX object test/CMakeFiles/user_counters_tabular_test.dir/user_counters_tabular_test.cc.o\nLinking CXX executable user_counters_tabular_test\n[ 97%] Built target user_counters_tabular_test\nScanning dependencies of target user_counters_test\n[100%] Building CXX object test/CMakeFiles/user_counters_test.dir/user_counters_test.cc.o\nLinking CXX executable user_counters_test\n[100%] Built target user_counters_test\n. Here's the output of cmake and make:\n~/git/google/benchmark/build$ cmake --version\ncmake version 2.8.12.2\n~/git/google/benchmark/build$ cmake .. -DCMAKE_BUILD_TYPE=RELEASE -DBENCHMARK_DOWNLOAD_DEPENDENCIES=ON\n-- The C compiler identification is GNU 4.8.4\n-- The CXX compiler identification is GNU 4.8.4\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Failed to find LLVM FileCheck\n-- Found Git: /usr/bin/git (found version \"1.9.1\") \n-- git Version: v1.4.0-7d03f2df\n-- Version: 1.4.0\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11\n-- Performing Test HAVE_CXX_FLAG_STD_CXX11 - Success\n-- Performing Test HAVE_CXX_FLAG_WALL\n-- Performing Test HAVE_CXX_FLAG_WALL - Success\n-- Performing Test HAVE_CXX_FLAG_WEXTRA\n-- Performing Test HAVE_CXX_FLAG_WEXTRA - Success\n-- Performing Test HAVE_CXX_FLAG_WSHADOW\n-- Performing Test HAVE_CXX_FLAG_WSHADOW - Success\n-- Performing Test HAVE_CXX_FLAG_WERROR\n-- Performing Test HAVE_CXX_FLAG_WERROR - Success\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC - Success\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS\n-- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS - Success\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32\n-- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32 - Failed\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL\n-- Performing Test HAVE_CXX_FLAG_WFLOAT_EQUAL - Success\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING\n-- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING - Success\n-- Performing Test HAVE_CXX_FLAG_WNO_DEPRECATED_DECLARATIONS\n-- Performing Test HAVE_CXX_FLAG_WNO_DEPRECATED_DECLARATIONS - Success\n-- Performing Test HAVE_CXX_FLAG_WNO_DEPRECATED\n-- Performing Test HAVE_CXX_FLAG_WNO_DEPRECATED - Success\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING\n-- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING - Success\n-- Performing Test HAVE_CXX_FLAG_WD654\n-- Performing Test HAVE_CXX_FLAG_WD654 - Failed\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY\n-- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY - Failed\n-- Performing Test HAVE_CXX_FLAG_COVERAGE\n-- Performing Test HAVE_CXX_FLAG_COVERAGE - Success\n-- Performing Test HAVE_STD_REGEX\n-- Performing Test HAVE_STD_REGEX\n-- Performing Test HAVE_STD_REGEX -- compiled but failed to run\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX\n-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile\n-- Performing Test HAVE_POSIX_REGEX\n-- Performing Test HAVE_POSIX_REGEX\n-- Performing Test HAVE_POSIX_REGEX -- success\n-- Performing Test HAVE_STEADY_CLOCK\n-- Performing Test HAVE_STEADY_CLOCK\n-- Performing Test HAVE_STEADY_CLOCK -- success\n-- Looking for include file pthread.h\n-- Looking for include file pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Performing Test BENCHMARK_HAS_O3_FLAG\n-- Performing Test BENCHMARK_HAS_O3_FLAG - Success\n-- Performing Test BENCHMARK_HAS_CXX03_FLAG\n-- Performing Test BENCHMARK_HAS_CXX03_FLAG - Success\n-- Performing Test BENCHMARK_HAS_WNO_ODR\n-- Performing Test BENCHMARK_HAS_WNO_ODR - Success\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/developer/git/google/benchmark/build\n~/git/google/benchmark/build$ make\nScanning dependencies of target googletest\n[  2%] Creating directories for 'googletest'\n[  4%] Performing download step (git clone) for 'googletest'\nCloning into 'googletest'...\nremote: Counting objects: 12065, done.\nremote: Compressing objects: 100% (3/3), done.\nremote: Total 12065 (delta 0), reused 2 (delta 0), pack-reused 12062\nReceiving objects: 100% (12065/12065), 3.53 MiB | 0 bytes/s, done.\nResolving deltas: 100% (8872/8872), done.\nChecking connectivity... done.\nAlready on 'master'\nYour branch is up-to-date with 'origin/master'.\n[  6%] No patch step for 'googletest'\n[  8%] Performing update step for 'googletest'\nAlready on 'master'\nYour branch is up-to-date with 'origin/master'.\n[ 10%] Performing configure step for 'googletest'\nloading initial cache file /home/developer/git/google/benchmark/build/googletest/tmp/googletest-cache.cmake\n-- The C compiler identification is GNU 4.8.4\n-- The CXX compiler identification is GNU 4.8.4\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Found PythonInterp: /usr/bin/python (found version \"2.7.6\") \n-- Looking for include file pthread.h\n-- Looking for include file pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/developer/git/google/benchmark/build/googletest/src/googletest-build\n[ 13%] Performing build step for 'googletest'\nScanning dependencies of target gtest\n[ 25%] Building CXX object googlemock/gtest/CMakeFiles/gtest.dir/src/gtest-all.cc.o\nLinking CXX static library libgtest.a\n[ 25%] Built target gtest\nScanning dependencies of target gmock\n[ 50%] Building CXX object googlemock/CMakeFiles/gmock.dir/src/gmock-all.cc.o\nLinking CXX static library libgmock.a\n[ 50%] Built target gmock\nScanning dependencies of target gmock_main\n[ 75%] Building CXX object googlemock/CMakeFiles/gmock_main.dir/src/gmock_main.cc.o\nLinking CXX static library libgmock_main.a\n[ 75%] Built target gmock_main\nScanning dependencies of target gtest_main\n[100%] Building CXX object googlemock/gtest/CMakeFiles/gtest_main.dir/src/gtest_main.cc.o\nLinking CXX static library libgtest_main.a\n[100%] Built target gtest_main\n[ 15%] Performing install step for 'googletest'\n[ 25%] Built target gtest\n[ 50%] Built target gmock\n[ 75%] Built target gmock_main\n[100%] Built target gtest_main\nInstall the project...\n-- Install configuration: \"RELEASE\"\n-- Installing: /home/developer/git/google/benchmark/build/googletest/lib/libgmock.a\n-- Installing: /home/developer/git/google/benchmark/build/googletest/lib/libgmock_main.a\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-generated-function-mockers.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-generated-actions.h.pump\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-more-actions.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-generated-actions.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-spec-builders.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-more-matchers.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-generated-matchers.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-matchers.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-generated-function-mockers.h.pump\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-actions.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-cardinalities.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-generated-nice-strict.h.pump\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-generated-nice-strict.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/internal\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/internal/gmock-port.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/internal/custom\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/internal/custom/gmock-generated-actions.h.pump\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/internal/custom/gmock-generated-actions.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/internal/custom/gmock-port.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/internal/custom/gmock-matchers.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/internal/gmock-internal-utils.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/internal/gmock-generated-internal-utils.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/internal/gmock-generated-internal-utils.h.pump\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gmock/gmock-generated-matchers.h.pump\n-- Installing: /home/developer/git/google/benchmark/build/googletest/lib/pkgconfig/gmock.pc\n-- Installing: /home/developer/git/google/benchmark/build/googletest/lib/pkgconfig/gmock_main.pc\n-- Installing: /home/developer/git/google/benchmark/build/googletest/lib/libgtest.a\n-- Installing: /home/developer/git/google/benchmark/build/googletest/lib/libgtest_main.a\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/gtest_pred_impl.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/gtest-param-test.h.pump\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/gtest-death-test.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/gtest-message.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/gtest-spi.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/gtest-param-test.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/gtest_prod.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/gtest-test-part.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-filepath.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-tuple.h.pump\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-type-util.h.pump\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-internal.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-param-util.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-death-test-internal.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-type-util.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/custom\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/custom/gtest-port.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/custom/gtest-printers.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/custom/gtest.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-string.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-port.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-param-util-generated.h.pump\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-tuple.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-linked_ptr.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-port-arch.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/internal/gtest-param-util-generated.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/gtest-printers.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/gtest.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/include/gtest/gtest-typed-test.h\n-- Installing: /home/developer/git/google/benchmark/build/googletest/lib/pkgconfig/gtest.pc\n-- Installing: /home/developer/git/google/benchmark/build/googletest/lib/pkgconfig/gtest_main.pc\n[ 17%] Completed 'googletest'\n[ 17%] Built target googletest\nScanning dependencies of target benchmark\n[ 19%] Building CXX object src/CMakeFiles/benchmark.dir/string_util.cc.o\n[ 21%] Building CXX object src/CMakeFiles/benchmark.dir/reporter.cc.o\n[ 23%] Building CXX object src/CMakeFiles/benchmark.dir/statistics.cc.o\n[ 26%] Building CXX object src/CMakeFiles/benchmark.dir/json_reporter.cc.o\n[ 28%] Building CXX object src/CMakeFiles/benchmark.dir/colorprint.cc.o\n[ 30%] Building CXX object src/CMakeFiles/benchmark.dir/counter.cc.o\n[ 32%] Building CXX object src/CMakeFiles/benchmark.dir/benchmark_register.cc.o\n[ 34%] Building CXX object src/CMakeFiles/benchmark.dir/commandlineflags.cc.o\n[ 36%] Building CXX object src/CMakeFiles/benchmark.dir/csv_reporter.cc.o\n[ 39%] Building CXX object src/CMakeFiles/benchmark.dir/console_reporter.cc.o\n[ 41%] Building CXX object src/CMakeFiles/benchmark.dir/sleep.cc.o\n[ 43%] Building CXX object src/CMakeFiles/benchmark.dir/timers.cc.o\n[ 45%] Building CXX object src/CMakeFiles/benchmark.dir/complexity.cc.o\n[ 47%] Building CXX object src/CMakeFiles/benchmark.dir/benchmark.cc.o\n[ 50%] Building CXX object src/CMakeFiles/benchmark.dir/sysinfo.cc.o\nLinking CXX static library libbenchmark.a\n[ 50%] Built target benchmark\nScanning dependencies of target benchmark_main\n[ 52%] Building CXX object src/CMakeFiles/benchmark_main.dir/benchmark_main.cc.o\nLinking CXX static library libbenchmark_main.a\n[ 52%] Built target benchmark_main\nScanning dependencies of target basic_test\n[ 54%] Building CXX object test/CMakeFiles/basic_test.dir/basic_test.cc.o\nLinking CXX executable basic_test\n[ 54%] Built target basic_test\nScanning dependencies of target benchmark_gtest\n[ 56%] Building CXX object test/CMakeFiles/benchmark_gtest.dir/benchmark_gtest.cc.o\nLinking CXX executable benchmark_gtest\n[ 56%] Built target benchmark_gtest\nScanning dependencies of target benchmark_test\n[ 58%] Building CXX object test/CMakeFiles/benchmark_test.dir/benchmark_test.cc.o\nLinking CXX executable benchmark_test\n[ 58%] Built target benchmark_test\nScanning dependencies of target output_test_helper\n[ 60%] Building CXX object test/CMakeFiles/output_test_helper.dir/output_test_helper.cc.o\nLinking CXX static library liboutput_test_helper.a\n[ 60%] Built target output_test_helper\nScanning dependencies of target complexity_test\n[ 63%] Building CXX object test/CMakeFiles/complexity_test.dir/complexity_test.cc.o\nLinking CXX executable complexity_test\n[ 63%] Built target complexity_test\nScanning dependencies of target cxx03_test\n[ 65%] Building CXX object test/CMakeFiles/cxx03_test.dir/cxx03_test.cc.o\nLinking CXX executable cxx03_test\n[ 65%] Built target cxx03_test\nScanning dependencies of target diagnostics_test\n[ 67%] Building CXX object test/CMakeFiles/diagnostics_test.dir/diagnostics_test.cc.o\nLinking CXX executable diagnostics_test\n[ 67%] Built target diagnostics_test\nScanning dependencies of target donotoptimize_test\n[ 69%] Building CXX object test/CMakeFiles/donotoptimize_test.dir/donotoptimize_test.cc.o\nLinking CXX executable donotoptimize_test\n[ 69%] Built target donotoptimize_test\nScanning dependencies of target filter_test\n[ 71%] Building CXX object test/CMakeFiles/filter_test.dir/filter_test.cc.o\nLinking CXX executable filter_test\n[ 71%] Built target filter_test\nScanning dependencies of target fixture_test\n[ 73%] Building CXX object test/CMakeFiles/fixture_test.dir/fixture_test.cc.o\nLinking CXX executable fixture_test\n[ 73%] Built target fixture_test\nScanning dependencies of target link_main_test\n[ 76%] Building CXX object test/CMakeFiles/link_main_test.dir/link_main_test.cc.o\nLinking CXX executable link_main_test\n[ 76%] Built target link_main_test\nScanning dependencies of target map_test\n[ 78%] Building CXX object test/CMakeFiles/map_test.dir/map_test.cc.o\nLinking CXX executable map_test\n[ 78%] Built target map_test\nScanning dependencies of target multiple_ranges_test\n[ 80%] Building CXX object test/CMakeFiles/multiple_ranges_test.dir/multiple_ranges_test.cc.o\nLinking CXX executable multiple_ranges_test\n[ 80%] Built target multiple_ranges_test\nScanning dependencies of target options_test\n[ 82%] Building CXX object test/CMakeFiles/options_test.dir/options_test.cc.o\nLinking CXX executable options_test\n[ 82%] Built target options_test\nScanning dependencies of target register_benchmark_test\n[ 84%] Building CXX object test/CMakeFiles/register_benchmark_test.dir/register_benchmark_test.cc.o\nLinking CXX executable register_benchmark_test\n[ 84%] Built target register_benchmark_test\nScanning dependencies of target reporter_output_test\n[ 86%] Building CXX object test/CMakeFiles/reporter_output_test.dir/reporter_output_test.cc.o\nLinking CXX executable reporter_output_test\n[ 86%] Built target reporter_output_test\nScanning dependencies of target skip_with_error_test\n[ 89%] Building CXX object test/CMakeFiles/skip_with_error_test.dir/skip_with_error_test.cc.o\nLinking CXX executable skip_with_error_test\n[ 89%] Built target skip_with_error_test\nScanning dependencies of target statistics_gtest\n[ 91%] Building CXX object test/CMakeFiles/statistics_gtest.dir/statistics_gtest.cc.o\nLinking CXX executable statistics_gtest\n[ 91%] Built target statistics_gtest\nScanning dependencies of target string_util_gtest\n[ 93%] Building CXX object test/CMakeFiles/string_util_gtest.dir/string_util_gtest.cc.o\nIn file included from /home/developer/git/google/benchmark/test/string_util_gtest.cc:6:0:\n/home/developer/git/google/benchmark/build/googletest/include/gtest/gtest.h: In instantiation of \u2018testing::AssertionResult testing::internal::CmpHelperEQ(const char, const char, const T1&, const T2&) [with T1 = double; T2 = double]\u2019:\n/home/developer/git/google/benchmark/build/googletest/include/gtest/gtest.h:1460:64:   required from \u2018static testing::AssertionResult testing::internal::EqHelper::Compare(const char, const char, const T1&, const T2&) [with T1 = double; T2 = double; bool lhs_is_null_literal = false]\u2019\n/home/developer/git/google/benchmark/test/string_util_gtest.cc:117:5:   required from here\n/home/developer/git/google/benchmark/build/googletest/include/gtest/gtest.h:1432:11: error: comparing floating point with == or != is unsafe [-Werror=float-equal]\n   if (lhs == rhs) {\n           ^\ncc1plus: all warnings being treated as errors\nmake[2]:  [test/CMakeFiles/string_util_gtest.dir/string_util_gtest.cc.o] Error 1\nmake[1]:  [test/CMakeFiles/string_util_gtest.dir/all] Error 2\nmake: *** [all] Error 2\n```. @dominichamon I built the fixes623 branch, and it fixed my problem, thanks. When will the PR be merged?. ",
    "guoyr": "I signed it. ",
    "winksaville": "Created PR #543 . Closing, fixed with merge of #543. @pleroy, if it\u2019s decided that to use the undef solution, should we change other StringXxx names to StrXxx? With this change there is consistency, which seems a good thing.. Created PR #546 and #547, closing this PR.. Closing, fixed with #546 . I submitted 3 PR's in the \"minor fixes and documentation\" category from pony and I'd like to have an official release sooner rather than later, which is why I asked. If they went in next week and then a release that would be perfect.\nSo do you think a release next week is possible?. Thanks. @dominichamon THANKS!!!. ",
    "andan342": "Some progress here. We are compiling whole solution with the flag /std:c++latest on MSVC, and get an error, as in the OP.\nProblem reproduced online,l:'5',n:'0',o:'C%2B%2B+source+%231',t:'0')),k:50,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:cl19_64,filters:(b:'0',binary:'1',commentOnly:'0',demangle:'0',directives:'0',execute:'1',intel:'0',trim:'0'),lang:c%2B%2B,libs:!(),options:'/std:c%2B%2Blatest',source:1),l:'5',n:'0',o:'x86-64+MSVC+19+2017+RTW+(Editor+%231,+Compiler+%231)+C%2B%2B',t:'0')),k:50,l:'4',n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4). If you remove the /std:c++latest, it compiles fine.\nSo is it a deprecated STL feature used in Google Benchmark?\nOtherwise, I guess we'll need to isolate Google Benchmark-dependent projects, so they don't inherit /std:c++latest option from solutions CMake script - I wonder if that's possible.... ",
    "qis": "Can confirm the warnings. Is it a good idea to remove the additional inline from BENCHMARK_ALWAYS_INLINE inline functions and declare #define BENCHMARK_ALWAYS_INLINE inline for __clang__?. @dominichamon I had no problems so far using Google Benchmark with vcpkg. All warnings are gone.. ",
    "traceon": "Submitted a PR https://github.com/google/benchmark/pull/721 for this (which still exists when compiling in C++17 mode, at least for clang that comes with XCode 10.1):\n... \\test\\output_test_helper.cc(206): error C2039: 'streampos': is not a member of 'std::basic_ios<char,std::char_traits<char>>'. I signed it!. ",
    "milladgit": "Hi everybody,\nWhat is the equivalent of this feature for PGI compiler? \nDoes anybody know about this? \nI am trying to compile the benchmark package with PGI. But, it gives me this error:\n\"/home/m/benchmark/src/benchmark.cc\", line 158: error: expression must\n          have a constant value\n    static_assert(offsetof(State, error_occurred_) <=\nAny ideas?. ",
    "AndrewGaspar": "Hm... I think my question is malformed. I don't really want ProcessCPUUsage. Give me a second and I'll see if I can ask a better question.. Here's my actual issue:\nMy problem is that my root thread is not balanced with the rest of the threads in the group. Presumably the library is scheduling all actual work on other threads. So benchmark is running the benchmark for too many iterations. I think what I really want is a toggle to always use \"Real Time\" to decide how many iterations to run, without having to tag every benchmark with UseRealTime(). Does that seem like a reasonable request?. I've decided to just mark all of my tests that use threads to \"UseRealTime\". Thanks!. ",
    "chfast": "Thanks, let me find the warnings.. In https://github.com/google/benchmark/blob/v1.3.0/include/benchmark/benchmark.h#L645\n\nc:.hunter_base\\5e7b471\\cbd9067\\2860324\\install\\include\\benchmark\\benchmark.h(645): warning C4141: 'inline': used more than once [c:\\projects\\ethash\\build\\test\\benchmarks\\bench.vcxproj]\n\nIt looks it's caused by inline being after __forceinline.\nThis was swapped in v1.4.0, so I will check this version. Sorry for not checking it earlier. \n. No warning in v1.4.0, and already in Hunter: https://github.com/ruslo/hunter/pull/1395.. Was there a way to achieve similar results before this PR?. You can simplify it to NOT MSVC.. You can use std::memory_order_relaxed here.. ",
    "ThePhD": "So I did a bit more digging and this is really, really unfortunate:\nhttps://cmake.org/cmake/help/v2.8.12/cmake.html#command:project\nCmake 2.8 doesn't HAVE a VERSION field in the project command. ... But CMake itself warns that it's relying on the old behavior. So this is essentially a case of \"Damned if you do, Damned if you don't\".\nMaybe this issue can be reopened if the CMake version is ever bumped to 3.0... Sorry for making my own noise.. ",
    "BaaMeow": "@dominichamon Thank you! Albeit, I had this discussion with someone earlier: because names can conceptually be anything (because of the more generic RegisterBenchmark function), I believe it would only be a temporary fix and tooling would run into issues later if they accidentally chose what we use as the \"reserved\" delimiter (not that it's actually reserved).\nI chose this solution because it doesn't rely on the semantics of the name. Not relying on the name means it can be chosen for purely aesthetic and descriptive reasons, without affecting classification and grouping.. What do you mean by \"base name\"? As in, prefixed to the benchmark \"name\" output somehow, or something else?. As a side note, if we're concerned about breaking things, should I move the CSV field \"id\" to the last column, rather than the first? People may expect a certain ordering from the CSV (or is it not not really that much of a big deal, because CSV reporting is frowned upon and soon to be deprecated / removed like other issues indicate?).\n(To be clear, I did the step you asked for. But, I did leave an option behind if someone in the future decides that ID printing is a good thing, or should be controllable by an option for the console reporter or something.). Oh, and I forgot screenshots of what things look like.\nConsole output:\n\nJSON Output:\n\nCSV Output:\n. @pleroy You don't have to! @dominichamon suggested I do not touch the console output: I already put in the code to keep console output displaying as it always has. My concern is with my tooling and working with the JSON, so I only updated the CSV and JSON output as per @dominichamon's earlier comment! I really don't want to break existing tools that do things like parse the console output, so his suggestion made sense to me and I rolled back changing console output.\nWith respect to Names. The function RegisterBenchmark allows you to pick an arbitrary name to register a benchmark with that does not have to be the name of a C++ identifier, and there's nothing to enforce that it is. This is what I was referring to before. It looks like this:\n```\ninclude \nauto BM_test =  {\n  std::string x = \"hello\";\n  for (auto _ : state)\n    std::string copy(x);\n};\nint main(int argc, char** argv) {\n::benchmark::RegisterBenchmark(\"A. Quirky?! Benchmark$$ name!\", BM_test);\n::benchmark::Initialize(&argc, argv);\n  if (::benchmark::ReportUnrecognizedArguments(argc, argv)) {\n      return 1;\n  }\n  ::benchmark::RunSpecifiedBenchmarks();\n}\n```\nAnd shows up like this:\n````\n2018-04-13 07:40:48\nRunning ./x64/bin/ptrptr_benchmarks\nRun on (2 X 2592 MHz CPU s)\nCPU Caches:\n  L1 Data 32K (x2)\n  L1 Instruction 32K (x2)\n  L2 Unified 256K (x2)\n  L3 Unified 6144K (x2)\n\nBenchmark                              Time           CPU Iterations\nA. Quirky?! Benchmark$$ name!          4 ns          4 ns  167304859\n````\nTo confirm, here's the actual visual output from the code:\n. @dominichamon That's a great idea, using base_name. You are right that I don't do anything in particular to stabilize the integer ID, just use it as a way of making each benchmark set uniquely identifiable. However, since this is for the purposes of just making sure I can properly group Benchmarks with their corresponding statistics, base_name sounds like a much better idea. I will implement that right away.. @dominichamon I was able to implement the base_name attribute and it works very well as an identifier, for both csv and json for my use cases! I also checked over the tests, and made sure the console ones were left alone but the JSON and CSV ones were updated to reflect the addition of 'base_name' in the fields. This should keep @pleroy happy, and anyone else who had to depend on console output in a time before csv/json were supported.\nIf there's anything I did wrong in the code, please do let me know!. So I looked into the build failure on the appveyor bot. It expects the CPU time to be a single-digit nanosecond... which is why it always passes on my machine. But Appveyor (and travis) are virtualized and share their CPU with others: chances are the test just got unlucky and failed us because we got the short end of the processor's share for that specific benchmark. Otherwise, all the tests are fine...!. @dominichamon I fixed up the regex in as many places as I saw I could be more explicit about how many numbers were allowed for things like console_report, but I'm not sure if it was expansive. It seems to cover the cases okay, though, so that might solve that problem!\nIf you'd still like me to get rid of the e.g. base_name option (off by default) for console printing, I can do that too. Just still leaving it in there in case somebody decides it would be a good idea in the future, should be gated behind a command line option, or wants to poke at the reporter directly.. @LebedevRI The formatting changes are not really something I can control: I make an edit and save, and according to the clang-format rules the entire thing gets changed. I would have hoped that somebody before me made the fixes and formatted the files before they committed: they did not, it happened here. I'm sorry! But, well. The formatting fixes would have to happen at some point, right?\nThe screenshots above are perhaps misleading because they were done before the final changes. Here it what it looks like in JSON for the final changes:\n\nThe base_name is the name of the benchmark exactly as you specified it.\nAll of the tests were updated to properly test for base_name, so you can run those if you'd like to see it in the JSON and CSV reporters. Note that we don't include the printout in the console reporter because this breaks people who parse the console report, as @pleroy stated earlier.\nDoes that make sense?. Console:\n\nJSON:\njson\n{\n  \"context\": {\n    \"date\": \"05/06/18 00:57:53\",\n    \"executable\": \"build\\\\x64-Debug\\\\x64\\\\bin\\\\nop_benchmarks.exe\",\n    \"num_cpus\": 8,\n    \"mhz_per_cpu\": 2592,\n    \"cpu_scaling_enabled\": false,\n    \"caches\": [\n      {\n        \"type\": \"Data\",\n        \"level\": 1,\n        \"size\": 32768000,\n        \"num_sharing\": 2\n      },\n      {\n        \"type\": \"Instruction\",\n        \"level\": 1,\n        \"size\": 32768000,\n        \"num_sharing\": 2\n      },\n      {\n        \"type\": \"Unified\",\n        \"level\": 2,\n        \"size\": 262144000,\n        \"num_sharing\": 2\n      },\n      {\n        \"type\": \"Unified\",\n        \"level\": 3,\n        \"size\": 6291456000,\n        \"num_sharing\": 8\n      }\n    ],\n    \"library_build_type\": \"debug\"\n  },\n  \"benchmarks\": [\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP\",\n      \"iterations\": 22400000,\n      \"real_time\": 2.9738841874080077e+01,\n      \"cpu_time\": 2.9994419642857142e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/repeats:3\",\n      \"iterations\": 22400000,\n      \"real_time\": 2.7985229106499382e+01,\n      \"cpu_time\": 2.7901785714285715e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/repeats:3\",\n      \"iterations\": 22400000,\n      \"real_time\": 2.6264596921724401e+01,\n      \"cpu_time\": 2.6506696428571427e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/repeats:3\",\n      \"iterations\": 22400000,\n      \"real_time\": 2.7667434242175659e+01,\n      \"cpu_time\": 2.7204241071428573e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/repeats:3_mean\",\n      \"iterations\": 22400000,\n      \"real_time\": 2.7305753423466481e+01,\n      \"cpu_time\": 2.7204241071428573e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/repeats:3_median\",\n      \"iterations\": 22400000,\n      \"real_time\": 2.7667434242175659e+01,\n      \"cpu_time\": 2.7204241071428573e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/repeats:3_stddev\",\n      \"iterations\": 22400000,\n      \"real_time\": 9.1556187108647824e-01,\n      \"cpu_time\": 6.9754464285706352e-01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/8\",\n      \"iterations\": 26352941,\n      \"real_time\": 2.7724797660724654e+01,\n      \"cpu_time\": 2.7866908668751620e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/64\",\n      \"iterations\": 24888889,\n      \"real_time\": 2.6776640090242402e+01,\n      \"cpu_time\": 2.6367187382289341e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/512\",\n      \"iterations\": 26352941,\n      \"real_time\": 2.5701620931957933e+01,\n      \"cpu_time\": 2.5495256867155739e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/4096\",\n      \"iterations\": 26352941,\n      \"real_time\": 2.7658431784598143e+01,\n      \"cpu_time\": 2.7273995718352651e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/8192\",\n      \"iterations\": 26352941,\n      \"real_time\": 2.5998235983796267e+01,\n      \"cpu_time\": 2.6088169817554707e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/8/repeats:3\",\n      \"iterations\": 28000000,\n      \"real_time\": 2.5851613463601094e+01,\n      \"cpu_time\": 2.6227678571428573e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/8/repeats:3\",\n      \"iterations\": 28000000,\n      \"real_time\": 2.5843542927759700e+01,\n      \"cpu_time\": 2.5669642857142858e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/8/repeats:3\",\n      \"iterations\": 28000000,\n      \"real_time\": 2.5681342143798251e+01,\n      \"cpu_time\": 2.5669642857142858e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/8/repeats:3_mean\",\n      \"iterations\": 28000000,\n      \"real_time\": 2.5792166178386349e+01,\n      \"cpu_time\": 2.5855654761904759e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/8/repeats:3_median\",\n      \"iterations\": 28000000,\n      \"real_time\": 2.5843542927759700e+01,\n      \"cpu_time\": 2.5669642857142858e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/8/repeats:3_stddev\",\n      \"iterations\": 28000000,\n      \"real_time\": 9.6061221984072728e-02,\n      \"cpu_time\": 3.2218206986057524e-01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/64/repeats:3\",\n      \"iterations\": 28000000,\n      \"real_time\": 2.6447238641724521e+01,\n      \"cpu_time\": 2.6785714285714285e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/64/repeats:3\",\n      \"iterations\": 28000000,\n      \"real_time\": 2.6957008571896171e+01,\n      \"cpu_time\": 2.6227678571428573e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/64/repeats:3\",\n      \"iterations\": 28000000,\n      \"real_time\": 2.6219090821021901e+01,\n      \"cpu_time\": 2.5669642857142858e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/64/repeats:3_mean\",\n      \"iterations\": 28000000,\n      \"real_time\": 2.6541112678214198e+01,\n      \"cpu_time\": 2.6227678571428573e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/64/repeats:3_median\",\n      \"iterations\": 28000000,\n      \"real_time\": 2.6447238641724521e+01,\n      \"cpu_time\": 2.6227678571428573e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/64/repeats:3_stddev\",\n      \"iterations\": 28000000,\n      \"real_time\": 3.7780934716984538e-01,\n      \"cpu_time\": 5.5803571428565080e-01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/512/repeats:3\",\n      \"iterations\": 28000000,\n      \"real_time\": 2.7084712072142533e+01,\n      \"cpu_time\": 2.7343750000000000e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/512/repeats:3\",\n      \"iterations\": 28000000,\n      \"real_time\": 2.5702844750152767e+01,\n      \"cpu_time\": 2.5111607142857142e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/512/repeats:3\",\n      \"iterations\": 28000000,\n      \"real_time\": 2.6121764714064607e+01,\n      \"cpu_time\": 2.5669642857142858e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/512/repeats:3_mean\",\n      \"iterations\": 28000000,\n      \"real_time\": 2.6303107178786636e+01,\n      \"cpu_time\": 2.6041666666666664e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/512/repeats:3_median\",\n      \"iterations\": 28000000,\n      \"real_time\": 2.6121764714064607e+01,\n      \"cpu_time\": 2.5669642857142858e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/512/repeats:3_stddev\",\n      \"iterations\": 28000000,\n      \"real_time\": 7.0855708381853810e-01,\n      \"cpu_time\": 1.1616439729164194e+00,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/4096/repeats:3\",\n      \"iterations\": 28000000,\n      \"real_time\": 2.6202540605611699e+01,\n      \"cpu_time\": 2.6227678571428573e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/4096/repeats:3\",\n      \"iterations\": 28000000,\n      \"real_time\": 2.5716149857284368e+01,\n      \"cpu_time\": 2.5111607142857142e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/4096/repeats:3\",\n      \"iterations\": 28000000,\n      \"real_time\": 2.6736733751022257e+01,\n      \"cpu_time\": 2.6785714285714285e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/4096/repeats:3_mean\",\n      \"iterations\": 28000000,\n      \"real_time\": 2.6218474737972773e+01,\n      \"cpu_time\": 2.6041666666666664e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/4096/repeats:3_median\",\n      \"iterations\": 28000000,\n      \"real_time\": 2.6202540605611699e+01,\n      \"cpu_time\": 2.6227678571428573e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/4096/repeats:3_stddev\",\n      \"iterations\": 28000000,\n      \"real_time\": 5.1047849462049211e-01,\n      \"cpu_time\": 8.5241363373437296e-01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/8192/repeats:3\",\n      \"iterations\": 26352941,\n      \"real_time\": 2.6258227648907159e+01,\n      \"cpu_time\": 2.6088169817554707e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/8192/repeats:3\",\n      \"iterations\": 26352941,\n      \"real_time\": 2.7611554437120592e+01,\n      \"cpu_time\": 2.7273995718352651e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/8192/repeats:3\",\n      \"iterations\": 26352941,\n      \"real_time\": 2.5462437000549215e+01,\n      \"cpu_time\": 2.5495256867155739e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/8192/repeats:3_mean\",\n      \"iterations\": 26352941,\n      \"real_time\": 2.6444073028858988e+01,\n      \"cpu_time\": 2.6285807467687697e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/8192/repeats:3_median\",\n      \"iterations\": 26352941,\n      \"real_time\": 2.6258227648907159e+01,\n      \"cpu_time\": 2.6088169817554707e+01,\n      \"time_unit\": \"ns\"\n    },\n    {\n      \"base_name\": \"NOP\",\n      \"name\": \"NOP/8192/repeats:3_stddev\",\n      \"iterations\": 26352941,\n      \"real_time\": 1.0865451292885608e+00,\n      \"cpu_time\": 9.0568949190772241e-01,\n      \"time_unit\": \"ns\"\n    }\n  ]\n}\nCSV:\nname,iterations,real_time,cpu_time,time_unit,bytes_per_second,items_per_second,label,error_occurred,error_message,base_name\n\"NOP\",20363636,32.0716,32.2266,ns,,,,,,\"NOP\"\n\"NOP/repeats:3\",26352941,27.8607,27.8669,ns,,,,,,\"NOP\"\n\"NOP/repeats:3\",26352941,26.3785,25.4953,ns,,,,,,\"NOP\"\n\"NOP/repeats:3\",26352941,27.601,27.8669,ns,,,,,,\"NOP\"\n\"NOP/repeats:3_mean\",26352941,27.28,27.0764,ns,,,,,,\"NOP\"\n\"NOP/repeats:3_median\",26352941,27.601,27.8669,ns,,,,,,\"NOP\"\n\"NOP/repeats:3_stddev\",26352941,0.791506,1.36927,ns,,,,,,\"NOP\"\n\"NOP/8\",26352941,26.7635,26.6811,ns,,,,,,\"NOP\"\n\"NOP/64\",26352941,28.6893,28.4598,ns,,,,,,\"NOP\"\n\"NOP/512\",26352941,26.8772,27.274,ns,,,,,,\"NOP\"\n\"NOP/4096\",26352941,25.9611,26.0882,ns,,,,,,\"NOP\"\n\"NOP/8192\",26352941,26.6772,26.6811,ns,,,,,,\"NOP\"\n\"NOP/8/repeats:3\",28000000,26.5917,26.7857,ns,,,,,,\"NOP\"\n\"NOP/8/repeats:3\",28000000,25.7512,25.6696,ns,,,,,,\"NOP\"\n\"NOP/8/repeats:3\",28000000,25.507,25.6696,ns,,,,,,\"NOP\"\n\"NOP/8/repeats:3_mean\",28000000,25.95,26.0417,ns,,,,,,\"NOP\"\n\"NOP/8/repeats:3_median\",28000000,25.7512,25.6696,ns,,,,,,\"NOP\"\n\"NOP/8/repeats:3_stddev\",28000000,0.569019,0.644364,ns,,,,,,\"NOP\"\n\"NOP/64/repeats:3\",28000000,27.2579,26.7857,ns,,,,,,\"NOP\"\n\"NOP/64/repeats:3\",28000000,28.7635,29.0179,ns,,,,,,\"NOP\"\n\"NOP/64/repeats:3\",28000000,27.7135,27.9018,ns,,,,,,\"NOP\"\n\"NOP/64/repeats:3_mean\",28000000,27.9116,27.9018,ns,,,,,,\"NOP\"\n\"NOP/64/repeats:3_median\",28000000,27.7135,27.9018,ns,,,,,,\"NOP\"\n\"NOP/64/repeats:3_stddev\",28000000,0.772122,1.11607,ns,,,,,,\"NOP\"\n\"NOP/512/repeats:3\",26352941,26.2543,26.0882,ns,,,,,,\"NOP\"\n\"NOP/512/repeats:3\",26352941,29.0715,29.0527,ns,,,,,,\"NOP\"\n\"NOP/512/repeats:3\",26352941,25.8172,26.0882,ns,,,,,,\"NOP\"\n\"NOP/512/repeats:3_mean\",26352941,27.0477,27.0764,ns,,,,,,\"NOP\"\n\"NOP/512/repeats:3_median\",26352941,26.2543,26.0882,ns,,,,,,\"NOP\"\n\"NOP/512/repeats:3_stddev\",26352941,1.76627,1.71159,ns,,,,,,\"NOP\"\n\"NOP/4096/repeats:3\",26352941,26.1249,26.0882,ns,,,,,,\"NOP\"\n\"NOP/4096/repeats:3\",26352941,26.5308,26.6811,ns,,,,,,\"NOP\"\n\"NOP/4096/repeats:3\",26352941,26.593,26.6811,ns,,,,,,\"NOP\"\n\"NOP/4096/repeats:3_mean\",26352941,26.4162,26.4834,ns,,,,,,\"NOP\"\n\"NOP/4096/repeats:3_median\",26352941,26.5308,26.6811,ns,,,,,,\"NOP\"\n\"NOP/4096/repeats:3_stddev\",26352941,0.254198,0.342318,ns,,,,,,\"NOP\"\n\"NOP/8192/repeats:3\",26352941,26.2092,26.0882,ns,,,,,,\"NOP\"\n\"NOP/8192/repeats:3\",26352941,26.4208,26.0882,ns,,,,,,\"NOP\"\n\"NOP/8192/repeats:3\",26352941,25.7008,26.0882,ns,,,,,,\"NOP\"\n\"NOP/8192/repeats:3_mean\",26352941,26.1103,26.0882,ns,,,,,,\"NOP\"\n\"NOP/8192/repeats:3_median\",26352941,26.2092,26.0882,ns,,,,,,\"NOP\"\n\"NOP/8192/repeats:3_stddev\",26352941,0.370045,0,ns,,,,,,\"NOP\"\nNote that, as talked about in earlier bikeshedding, the CSV output has the base_name as a parameter at the end of the formal list, primarily to avoid breaking people who use CSV and hard-code their use of the CSV indices.\nDoes this answer your questions?. Well, when I first started this pull request I used a id marker instead of base_name. That would have been unique per-benchmark, even the test you mentioned above. However, that was deemed less user friendly, and the base_name idea was encouraged instead.\nAs it stands, the benchmark example you have shown me @LebedevRI seems something that's a bit easier to work around. With the base name you can still accurately determine things like extraneous modifiers (everything after NOP, now), which means you can -- with some effort -- still isolate which benchmark group each of those belongs to and their associated statistics (modifiers like range, threads, etc. all use /  and : -- these characters don't show up in regular C function names).\nThere is still a problem with individually hand-registering benchmark functions with RegisterBenchmark, where a user can make your life as the tools user / creator hard.\nI still like having base_name, but if we would also want to add id back in to handle the case you have, then I would suggest another PR. This one's been sort of dragging and I don't want to keep flipping the implementation back and forth. base_name is still useful information, it should stay.\nI'll be happy to open a PR for an ID, citing your use case and putting forth an argument. But base_name is still good, especially for those of us that don't have more involved needs. Does that make sense?. Because that would require every relevant separating counter, option, and repetition added VIA the BENCHMARK(...) API to be accumulated in the base name. And, someone would have to remember to make sure to tag each one appropriately. From my understanding of the code base, that's exceptionally more complicated and might break in small ways when someone forgets to accumulate the \"right\" options (which ones?) into the base_name.\nContinually extending base_name to encompass all of these is likely more engineering work and less tenable. The succinct solution to your problem is an id marker, because each benchmark registered with BENCHMARK(...) or RegisterBenchmark( Name, ... ) will get tagged. It requires no fiddling with the reporters beyond printing the ID, and no identification of which options deserved to be accumulated in the base name or not.. That would not bring the desired results. From what you presented and stated before: you have 4 benchmarks, and you want 4 base names to identify each of the 4.\nLook at the CSV you posted where you just cut off the statistics name: there's 18 unique strings for the base name. You've only moved the problem around, and it still requires post-processing to handle correctly. This is what I mean by \"accumulating certain fields\". Do we drop the range specifier from the base name, but keep the threads? The repetitions? How do we reduce the name field to a suitable base_name that gets you to the 4 distinct identifiers you want?\nThis is the cognitive overhead of how the reporters are defined I am talking about.\nThe answer is not in this PR. I would be happy to make a new PR for an ID number that matches your use case and would give you 4 distinct identifiers for your 4 benchmark registrations, but I still maintain that mangling the base name is not the answer.. @dominichamon I would be more than happy to take up putting threads, repetitions and other information in the CSV and JSON in a well-defined manner, but I still think that base_name as it is would still be useful (in conjunction with adding the rest).\nTo be clear, I'm not against what @LebedevRI wants or suggests, I'm just wanting to not turn this PR into a spiraling monster of features. I want to close this one out and then open a new one that tackles these other issues.. So to solve the problems I brought up and the ones @LebedevRI has, I'm going to implement base_name, id, and also include all of the modifiers @dominichamon has mentioned (adding \"Threads\", \"repetitions\", \"iterations\" to the JSON and CSV output).\nWould it be preferred I close this and open a new PR so this is less messy?. @LebedevRI There already is a discrete description of the problems we're trying to solve here. 1 is that associating a statistic with the benchmark it came from is a bit harder than it should be (the original problem), and the other is that there is a desire for a conceptual base_name grouping that comes from base_name + thread + repetition count.\nProblem 1 is solved by a numeric ID on each benchmark. Repeated runs and statistics with the same ID can be successfully mapped to a single registered benchmark, regardless of if the user has the same name applied 2 or 3 times with separate repetitions / threading numbers. This is the finest granularity I can think of offering.\nProblem 2 is solved by producing more fields on the JSON that accurately describe a single benchmark group (or, as the code calls it, a single BenchmarkFamily in the BenchmarkFamilies). This means adding (not subtracting) JSON fields, and leaving the current ones alone. To do this, you need a combination of base_name (the name the benchmark was registered with), repetitions, and threads.\nSo, there will be 4 total new fields:\nbase_name\nid\nthreads\nrepetitions\nAll of this information will allow as fine or as coarse comparisons as one would like in their tooling, plus arbitrary grouping. I think this satisfies all of the necessary use cases present.. This pull request is being closed in favor of #599.. @dominichamon I went through and made the format PR (#610). I'll remake / rebase this once that PR goes through, with the splits you asked for (CSV in another PR, command line parameters in another PR, etc...). Some test cases for the edge / adversarial cases:\n````\ninclude \nstatic void adversarial(benchmark::State &state)\n{\n    for (auto _ : state)\n    {\n    }\n}\nBENCHMARK(adversarial);\nBENCHMARK(adversarial);\nstatic void many_range_rep(benchmark::State &state)\n{\n    for (auto _ : state)\n    {\n    }\n}\nBENCHMARK(many_range_rep);\nBENCHMARK(many_range_rep)->Repetitions(3);\nBENCHMARK(many_range_rep)->Range(8, 8 << 10);\nBENCHMARK(many_range_rep)->Range(8, 8 << 10)->Repetitions(3);\nstatic void common_prefix(benchmark::State &state)\n{\n    for (auto _ : state)\n    {\n    }\n}\nstatic void common_prefix_mean(benchmark::State &state)\n{\n    for (auto _ : state)\n    {\n    }\n}\nBENCHMARK(common_prefix);\nBENCHMARK(common_prefix_mean);\n````\nJSON output from above test-case benchmarks. I'm still working on this, just as an FYI. I've just been terribly busy!\nI guess I'm not very good at rebasing / merges (or PGP and commit signing). Sorry about that; it looked okay on my local box but I guess I have some more work to do!. @dominichamon I added the name \"stats\" with the name of the statistic (including for RMS and BigO). That mostly solves the problem, since there's a distinct way to tell runs apart now.\nI still think \"family\" is useful as an ID, since I would want to group things without having to hobble together the name. There are also cases where someone can register things of the exact same name. For example, below is a collection of degenerately named and used benchmarks. This is what the output looks like.\nWith the name \"stats\", we can maybe get rid of id. I'm still hesitant to do so, though: you would need to parse multiple fields (base_name, stats, repetitions/iterations/threads/etc.) to make sure you had one unique grouping, especially in the case of degenerate names as illustrated below.\n```c++\ninclude \nstatic void adversarial(benchmark::State& state) {\n    int x = 0;\n    for (auto _ : state) {\n        x += 2;\n        benchmark::DoNotOptimize(x);\n    }\n}\nBENCHMARK(adversarial);\nBENCHMARK(adversarial);\nstatic void common_prefix(benchmark::State& state) {\n    for (auto _ : state) {\n    }\n}\nstatic void common_prefix_mean(benchmark::State& state) {\n    for (auto _ : state) {\n    }\n}\nstatic void using_the_underscore_poorly_(benchmark::State& state) {\n    for (auto _ : state) {\n    }\n}\nBENCHMARK(common_prefix)->Repetitions(3);\nBENCHMARK(common_prefix_mean)->Repetitions(3);\nBENCHMARK(using_the_underscore_poorly_)->Repetitions(3);\nvoid complexity(benchmark::State& state) {\n    for (auto _ : state) {\n        for (int i = 0; i < 1024; ++i) {\n            benchmark::DoNotOptimize(&i);\n        }\n    }\n    state.SetComplexityN(state.range(0));\n}\nBENCHMARK(complexity)->Range(1, 1 << 18)->Complexity(benchmark::o1);\nBENCHMARK(complexity)->Range(1, 1 << 18)->Complexity();\nBENCHMARK(complexity)->Range(1, 1 << 18)->Complexity( {\n    return 1.0;\n});\nstd::vector random_vector(int64_t size) {\n    std::vector v;\n    v.reserve(static_cast(size));\n    for (int i = 0; i < size; ++i) {\n        v.push_back(static_cast(std::rand() % size));\n    }\n    return v;\n}\nvoid complexity_rms(benchmark::State& state) {\n    auto v = random_vector(state.range(0));\n    const int64_t item_not_in_vector = state.range(0) * 2;\n    for (auto _ : state) {\n        benchmark::DoNotOptimize(std::find(v.begin(), v.end(), item_not_in_vector));\n    }\n    state.SetComplexityN(state.range(0));\n}\nBENCHMARK(complexity_rms)\n    ->RangeMultiplier(2)\n    ->Range(1 << 10, 1 << 16)\n    ->Complexity(benchmark::oN);\nBENCHMARK(complexity_rms)\n    ->RangeMultiplier(2)\n    ->Range(1 << 10, 1 << 16)\n    ->Complexity( -> double { return static_cast(n); });\nBENCHMARK(complexity_rms)\n    ->RangeMultiplier(2)\n    ->Range(1 << 10, 1 << 16)\n    ->Complexity();\nstatic void many_range_rep(benchmark::State& state) {\n    for (auto _ : state) {\n    }\n}\nBENCHMARK(many_range_rep);\nBENCHMARK(many_range_rep)->Repetitions(3);\nBENCHMARK(many_range_rep)->Range(8, 8 << 10);\nBENCHMARK(many_range_rep)->Range(8, 8 << 10)->Repetitions(3);\n````. @LebedevRI Thanks for the tips; I think I managed to clean it up. The googlebot seems to agree; I just need to check over the diffs to make sure they're no longer super messy...\nThere's a few places where the clang-format er had fun, but it didn't seem to drastically alter the presentation of anything. If something looks wrong do let me know: otherwise, everything looks to be okay (just need to wait for Appveyor and Travis-CI to report that everything is fine).. @LebedevRI Done! #652 . @LebedevRI Sure, if anyone else chimes in and says just the stats part is wanted I will be more than happy to break it off into another PR and leave the ID bits for later.\nAs a small update, I changed id in the JSON to be the much more descriptive \"stats_group\" label. It just makes a bit more sense than \u00ecd.. @LebedevRI Ping pong: sorry for the quiet! I've been way too busy for my own good, but I'm back now and I have quite a bit of free time!\nI wanted to get up to speed again and help move forward some of the wanted things. Judging from #675, it looks like a stats_name (aggregate name) was pulled into a separate PR by you and handled nicely, yay!\nShould I go ahead and add a new PR for threads and repetition counts, since those are uncontroversial? I can probably get those done by Monday, and then we can figure out if additional identifiers are still worth it!. With #748 going live, if its accepted then the only thing that might be needed are some of the family IDs for the case of clashing and degenerate benchmark names. I'll make a new pull request with the last of that after #748 is reviewed.\nHappy Holidays to everyone! \ud83c\udf84 . I'd like to participate in this a little bit, albeit my needs might be a bit different and maybe not exactly aligned.\nMany of my company's measurements are tiny enough that I want to count things like cycles (rtdsc, for example) rather than nanoseconds, and so I wanted the ability to have a custom TimeUnit alongside the ManualTime. Would the work here allow for something like that, or is this orthogonal?. Maybe having ManualTime count (whatever) and then allowing to have a TimeUnit::Manual, which will call a user-provided function to serialize the stored value might be a good first start? You could trigger it by using a 1-argument UseManualTime() function, like\nBENCHMARK(BM_ManualTiming)->Range(1, 1<<17)->UseManualTime(time_unit_serialization_function);\nWhere the serialization function just takes a std::ostream& and the double seconds of the iteration time:\nvoid time_unit_serialization_function( std::ostream& ostr, double time ) {\n     ostr << time << \" cycles\";\n}\nMaybe this is a good start? It doesn't allow us to customize the storage of SetIterationTime, but maybe Double is good enough?. I didn't even realize other people were having this problem, aha. Well, it's fixed in #652!. Well, there's no real way to correlate some top-level \"this is a family of runs\" block in the JSON, and the various reported runs / samples in the output. Though, if we go with the \"family\" id in #616 then we can create a few top-level blocks in the JSON describing each \"family\" along with the parameters (threads, etc.) pass to a BENCHMARK(...) call or a manual benchmark registration.\nOther than that, I have no idea where I would stick \"repetition\"!. Alright, I think I changed all the tests to be directly reflective (except for the Complexity Tests. Since it's added by a function and not linked exactly to a specific BENCHMARK() creation instance, %int seems more appropriate since someone can add a Complexity Benchmark with repetitions, call that function, and suddenly end up with failures unrelated to any features they were adding!). I fixed this to report repetition_index on every non-aggregate, and both repetition_index and repetitions on every iteration. It's a bit coarse but until the v2 of JSON I don't think I can make a sweeping breaking change to the actual JSON structure, where I would have a dedicated \"benchmark_registrations\": { ... } block that describes every registered benchmark in a single invocation of the benchmark executable so that someone could get good top-level data about each benchmark registered (from name to # of aggregate stats and other pertinent per-benchmark things).\nI think this is a good compromise until that bright, beautiful future.... Sorry about the CLA stuff: I'm really terrible at rebasing and merging, so I think I made the bot trip over itself several times.. I think I resolved all the issues and questions! Thanks for reviewing it, @LebedevRI and @dominichamon. I think this will really help people with custom units make a lot of headway with benchmark!\nPlease do let me know if anything else needs changing. (Also thanks for taking care of Statistics so I wouldn't have to do it here, @dominichamon \ud83d\udc96!). Thanks! I specified that memory order.. It's not strictly necessary, but I wanted to leave this as a way for someone to (easily) enable printing the base name for the console reporter, maybe gated by an option or something. I didn't add any such option here, it's just in case somebody wants it.. Sorry! Clang format kicked in when I was doing the latest commits and, uh. Changed things.... Gotcha: will remove!. Yeah.... I had to move the comment to the next line, otherwise clang-format just kind of mangles the : on the private.. Nope, removed!. I removed the code for it: I suppose somebody else can implement the option for console reporting and the like!. I didn't introduce the FLAG macro; this was present in the source already. It was defined as \"the way you are supposed to use it to refer to flags\". I simply noticed that it was defined as the way to do things and we didn't use it anywhere. I will revert the changes, but does that mean the FLAG macro present here should not be around...?. The base name can be similar between several benchmarks. See the example posted on the last PR, where a NOP function with only slight variations in repetitions, threads, etc. were applied: https://github.com/google/benchmark/pull/567#issuecomment-386820365 . Base name is equivalent to everything leading up to the first /, so there's no reason to run a substr.. The tests do not explicitly check the full executable path, just the resulting endname. Properly escaped names show up in the appveyor Windows tests now.. The full name is {base_name}\\threads:{thread}\\{repetitions}. This PR provides access to the parts, and lets you re-assemble them and group them arbitrarily to your liking. We provide ID numbers for instances, so you can pick statistics, and provide family ID numbers, so you can differentiate between benchmarks which are -- for whatever reason -- registered with the same name.. I changed it to check for either // or an escaped \\\\\\\\ in the reporter output tests.. This is true, but I think it doesn't matter for everything else since we know we're not dumping out escape-necessary characters in any other place.\nOf course, @LebedevRI is right: proper json support with some library would likely take care of it for us.. Alright, done!. This is mostly because I'm not sure of if the nano/micro/milli prefixes will apply to whatever the type is, so it's passed through directly (and the printed time_unit value has no prefix). I don't know of a way to signal \"and nano / micro / milli / centi / etc. apply here\".. I could make it so that by-default we serialize \"seconds\", \"s\", pass_through_function into ManualTime with a customized_units == false. That way, we just always go through manual_time->cost_in_seconds rather than checking the difference. It will also make printing easier too, so maybe that's the direction I go in?. I made some changes! Mostly, the defaults are the new BenchmarkTime struct defaults. There are less conditionals now too, making it easier to reason about the code and not make future mistakes.. I made some changes to allow for custom units to be applied to be applied here without checking for customized units or manual time.. b.time is used with regular time unit measurements too, as per @LebedevRI's advice: it is used for all time measurements, so maybe I should specify ManualCost to be more explicit just for this one member (as that is what it is used for).. I was trying to make it so if something special was done in CpuUnit or changes were made later, someone would've have to update 2 places, but I will change it!. Probably not, no.. But neither does Statistics, strictly speaking, but it's still \"public\". Is it important to move this into the internal namespace?. While I'm fixing this, should I drive-by move Statistics too?. I kept the last argument default, but with a better defaulted function name. The other 2 should always be specified though, I think.. I added a comment!. I guess I'll remove the Manual then...!. The perception internally is that its in Seconds, but there's nothing really stopping the user from setting arbitrary values that more appropriately fits their idea of what's necessary and setting arbitrary \"seconds\" measurements for MinTime.. Pointer init with this syntax is the nullptr, but I'll put a 0 in there. (I can't use nullptr because this is supposed to be consumable from C++03 code).. I went with Info instead of Dsc, because it's a bit easier to read.. I'm a bit confused. The Readme specifically calls out using the for loop syntax as a better KeepRunning loop. Maybe I just lack understanding of how KeepRunningBatch is meant to be used to make this scenario better? Batch and KeepRunningBatch don't even exist on the readme... am I missing something? Is this a v2 feature?\nWe specify UseManualTime here; this means we have to call SetIterationTime() each iteration, that's why I'm using the manual timing here. I wish there was a way to batch-measure, but there is no SetRunTime(total) that divides by the # of iterations internally like it does for the non-manual measure.. I can change the description to talk about it more appropriately.... I think that's fundamentally cross from what this PR is trying to do, which is allow custom measurements (e.g, with rdpmc and other instruction / cycle performance counters) to be printed out in benchmarking data.\nWhile I generally agree with std::chrono::time_point and std::chrono::duration being the right choice, they're not possible to expose to the user because this is a C++03-interface library. But lifting the internals to do that is entirely possible.. Changed to BenchmarkTimeDescription.. I changed the benchmark to be what I think is more appropriate for a rotl measurement, using KeepRunningBatch. This serves to benchmark both KeepRunningBatch and the advanced UseManualTime, but it probably doesn't address your other concerns.. I don't really know what to do about this, really. If the PR isn't considered well-motivated enough (providing custom units to be used and reported by the benchmark) then I don't really have a way forward.. I am building this example based on my understanding of KeepRunningBatch, which is probably very poor right now.\nMy impression was I needed to do batch_size number of iterations and, when I set UseManualTime (even without this PR), I am required to submit 1 measurement per iteration. If that's not the case I can submit times for batch_size iterations, or maybe just surrounding the whole thing?\nI am unsure how KeepRunningBatch works in the presence of UseManualTime, I could probably spend a few more days checking to be sure..... ",
    "UCLGuichard": "Great, thanks a lot, I'll see if that works.. It works for the first although I still get the second. I'm using the Intel compiler for which this fix is not applicable ?. Yes, I'm using CMake and Intel compiler (2017). . So far we moved to the GNU compiler for which it works. . ",
    "NTimmons": "That's fine - but the dependency fetcher works on Linux and not Windows?\nShouldn't the behaviour across OS be the same where it is possible? (And if not, documented?). ",
    "jerryzh168": "@Yangqing this breaks for android, currently I'm adding a workaround from: https://github.com/google/benchmark/issues/351\n\"-DCMAKE_CROSSCOMPILING=1 -DRUN_HAVE_STD_REGEX=0 -DRUN_HAVE_POSIX_REGEX=0\".. ",
    "Tim020": "I signed it!. Implements #571 . @dominichamon done :) Added negative variants for all the existing unit tests . Ah not anymore, this was left over from some test code, will remove :). Done :). ",
    "JAkutenshi": "Also, seems the same with #185 but more informative with solution approach. ",
    "CarloWood": "The paper that you refer to is kinda old. Currently the Intel manual prefers lfence before the first rdtsc, as opposed to cpuid, and rdtscp if that is supported. Personally I prefer lfence; rdtsc anyway and use rdtscp; cpuid to measure the closing timestamp (as in that paper). But all of that seems rather futile unless you do your measurement in a kernel module on a cpu that isn't used (turned off during boot) and with a tweaked bios; or else you get so much noise from all kinds of things that you might as well not bother with the serializing.. ",
    "degski": "@CarloWood\n\n... else you get so much noise from all kinds of things that you might as well not bother with the serializing.\n\nWould booting the OS in 'safe mode (minimal)' help in mitigating that problem in your opinion?. @dominichamon \nOn the README.md page it says:\n\nDon't forget to inform your linker to add benchmark library e.g. through -lbenchmark compilation flag. Alternatively, you may leave out the BENCHMARK_MAIN(); at the end of the source file and link against -lbenchmark_main to get the same default behavior.\n\nthere somewhere, I would say, doing that does not work (on it's own) and with vcpkg, that bit is automatic anyway, the linking to that library is not though.. Or add to benchmark.h (no need for any documentation):\n#ifdef _WIN32\n#pragma comment ( lib, \"Shlwapi.lib\" )\n#ifdef _DEBUG\n#pragma comment ( lib, \"benchmarkd.lib\" )\n#pragma comment ( lib, \"benchmark_maind.lib\" )\n#else \n#pragma comment ( lib, \"benchmark.lib\" )\n#pragma comment ( lib, \"benchmark_main.lib\" )\n#endif\n#endif\n\nIt called auto-linking (this is an old feature, so no worries about backward compatibility), handy (therefor it's not gonna go away either, it's not all bad on windows). Link order b.t.w. does not matter on windows (except gcc/MinGW(64)), as VC-link (and lld-link with Clang/LLVM) builds a list of all functions to be linked (instead of taking the first one that pops up) and complains profusely about double definitions, while gcc will just carry on with the first one silently, which might be the wrong one to link to, if you made an error.. @LebedevRI \n@dominichamon \nIt is, because Shlwapi.lib library does not get linked automatically, ever. The headers are not in Windows.h either, the header is pulled in by ../src/sysinfo.cc, which hides the fact that the linking is actually required, you won't be any the wiser until  such time you are going to link to benchmark.lib.. ",
    "sbc100": "Its possible, but you would need to pull down the emscripten SDK as part of your build.\nThat might well be do-able, but even then it likely would not have found this error unless you were testing with strict option and/or with the upstream LLVM backend.  I guess its up to you to decide how much you care about the emscripten target.. ",
    "php1ic": "Fixed and merged with PR #590\n. ",
    "NanXiao": "@dominichamon IMHO, no matter set std::tm to 0 or not, the following statement will fill std::tm correct value:  \n::gmtime_r(&now, &timeinfo);\n\nWhether std::tm is zero-initialized or not, its value is meaningless unless filled by functions like localtime_r or gmtime_r. So it is no need to call memset() before gmtime_r.  \nJust my personal opinion, don't mind, thanks!. ",
    "ahsabali": "Thank you for replying. It is very helpful. Let me look into this further, and shall let you know if I have any question.. Thank you Dominic. It was very helpful.. It seems it adds no significant value to the output (everything is a benchmark in the output), and everything in the C++ file must be registered with a BENCHMARK call. Any example where it can be helpful to use \"BM_\"?. Yes, that answered my question. Thank you.. ",
    "enh": "(note that gnustl is being removed from NDK r18.). ",
    "zhihuiFan": "Thanks for your quick response!  Due to the timezone,  I will use google group for future questions. Thank you!. ",
    "fengggli": "I got the same one!. ",
    "kfitch": "I can't get to a computer where I can fork/edit/test code at the moment, but I think the proper solution (that should work on cmake 2.8.12)  is something like:\ncmake\nfile(GLOB BENCHMARK_MAIN \"benchmark_main.cc\")\nforeach(item ${BENCHMARK_MAIN})\n    list(REMOVE_ITEM SOURCE_FILES ${item})\nendforeach(). ",
    "SJL----": "\nthis the code where call RegisterBenchmark.. @dominichamon hi, thank you for help me, I downloaded source code of benchmark, and see benchmark.h, and i see three func in header file, so i upgrade my g++, to support the grammar.\nresult is solved.\n\n. ",
    "kanak": "I'm happy to contribute either of those. Another option is to start a new document in docs/ called \"Getting Started\" or \"Compiling your first benchmark\". . I'm new to the library and also not a native speaker, so I'm open to all feedback.\nI'm also happy to add Ubuntu Linux instructions or additional instructions using Clang if necessary. I also did not remove or otherwise refactor any docs in the Readme, I can help with that work as well.. @dominichamon : I would like to abandon this effort. I understand the concerns laid out here, that this documentation would be too specific and overly prescriptive and unsuitable in its form as a piece of official documentation. However, I wrote this when I was running into some really basic problems (in retrospect), such as passing the -lbenchmark flag in the wrong place, and want a very rigid set of steps that was guaranteed to work. I think I will probably just publish this as a blog post instead. \nThank you for your time and for the wonderful library. . Fixed.. Sadly, google benchmark is not available in the Fedora 28 repository right now, so I have to compile it from source.\nI'll prepare instructions for the no install use case. The \"install\" related bits can then be an \"appendix\" in this document or just removed.. ",
    "dbabokin": "Would be nice to update the documentation as well. I've just started looking at the project and I'm trying examples from README. Deprecated warnings are highly confusing.. Also, from user prospective SetBytesProcessed() and other deprecated functions are way more convenient, than verbose and generic Custom Counters. Keeping them around as aliases / special case of Custom Counters seems reasonable from usability point of view.. As I see, current implementation is really special-casing them. My point is that they probably might be just aliases for Custom Counters. To me they make perfect sense from 1000 feet view and are convenient functions to start with, before diving into verbose Custom Counter.\nNote, I'm playing with benchmarks measuring load/store performance (i.e. memory bandwidth in different scenarios), so \"byte processed\" is organically what I need.. Yes, with manually checked-out googletest directory in the source tree, it works like a charm.. ",
    "mumumu": "I signed it!. ",
    "atdt": "@LebedevRI, valid points. However: while load average won't always be pertinent, neither will the count of CPU cores, or the cache hierarchy. You are also right that reporting load averages prior to running the benchmarks does nothing to alert the user to spikes in demand from other processes that occur while the benchmark is in flight. But persistent high demand on the system is important to report for two reasons: first, it can cause a consistent depression of results across multiple benchmark runs, which is a subtler issue to spot than a sporadic spike that manifests in a few outlier results. Second, while a low/zero load average does not guarantee that load won't spike, a high load average is almost always a sign that the person performing the benchmark has overlooked something.. ",
    "p-groarke": "Bincrafters may be interested in your package once it's ready. Thx for taking the time.. ",
    "raulbocanegra": "Hi! \nAs @Croydon referenced I was asked to include my repo a couple of weeks ago.\nI am happy to help and also open to receive any suggestions.\nOne of the issues I found on Windows is building a shared version of the library. After having a look at the code it seems that the symbols are not being exported, so shared builds are not supported on Windows.\n. ",
    "SSE4": "I signed it!. it seems to be dual build-system project (Bazel + CMake), so CMake's generate_export_header doesn't seem to be good fit here (I don't want to re-implement the same in Bazel, especially as I have no experience with mentioned build system). probably, the best thing I can do for now is to just copy generated export definitions into benchmark/benchmark.h, does it sound like an acceptable solution?. they do not use generate_export_header and have their own macro definitions (GTEST_API_) :\nhttps://github.com/google/googletest/blob/41f0e24335d8874cc87420a6349bb1844e2a15de/googletest/include/gtest/internal/gtest-port.h#L969. I've asked @Croydon and he is okay with not supporting Windows shared built at all, works for me as well.. yes. thanks, I'll try to change to use CMAKE_CURRENT_BINARY_DIR. initial attempt produced some CMake errors for me. also, build fails hard on tests, I'll take a look.. ",
    "xorz57": "Take a look at these source files\njson_reporter.cc\nreporter_output_test.cc\nstring_util.cc. Nice!. ",
    "monamimani": "Thanks yes it is fixed!!. I created an other issue for the json, what is wrong is the backslashes they should be escaped, or slashes or when the field is read to be canonalized.)\nThe problem that i am reporting here is the output to the console.\nThose characters \ufffd and  [92. Either it is reading bad memory or there is a locale issue, or something else.. I believe the issue is about Unicode. I looked at the python code and in report.py in the function generate_difference_report the var first_line is an ASCII string but in the JSON object, the strings are Unicode and then later in the function color_format the strings are formatted with an  ASCII fmt_str.\nI believe this is the culprit for the bad display of the characters.\nI guess the solution is to go all ASCII or all Unicode, I am just not sure with the one you want.\n. ",
    "markgalpin": "Importing a repository primarily just sets the URL and brings in users/groups, it is not necessary to publish to bintray. You may need to set a few extra parameters manually and add the users and teams manually though.  If this is a large enough to make that impractical, let me know, and we'll work with you further.. ",
    "danimtb": "Regarding point 2, I think it can be safely merged as it is not uploading anything to Bintray right now. However, the final goal is to have the packages there, so adding the variables in Travis/Appveyor that @Croydon indicated above will be the best. Thanks!. The version populated from other files such as cmakefiles or headers. Here is the howto: https://docs.conan.io/en/latest/howtos/capture_version.html. This line looks kind of weird to me but having a look at BENCHMARK_ENABLE_TESTING and BENCHMARK_ENABLE_GTEST_TESTS it could make sense to have it separated. Anyway both the option and env var for tests are well managed. ",
    "memsharded": "Hi!\nConan maintainer here.\nI think this PR is very big, and I understand there are concerns about changing CI.\nI would approach it step by step. The most important thing, which is not intrusive (won't change absolutely nothing in the build or in the CI) is the conanfile.py recipe and the \"test_package\" folder. I think that should go first, by its own.\nCI is a different beast, so better wait until later. We are also working on some ideas that could help in greatly reducing or completely removing the required CI changes.\nOnce the conanfile.py is there, in the meantime (while we wait until the above ideas are released) we could try to setup a dedicated repo (in bincrafters or conan-community) for conan CI, without requiring to modify this repo CI scripts.\nWhat do you think?. ",
    "vtkacenko": "@Croydon Is there anything one can help with to get this moving?. @dominichamon Not waiting for anything. Just wondering if there is anything I can help with to get the https://github.com/google/benchmark/pull/647 merged; in order to have the package available in the conan-center.. ",
    "kirillbobyrev": "@LebedevRI sure. Are you suggesting to replace the outer return VALUE; statement (which was added in the latest commit so that old GCC doesn't complain about control reaching end of non-void function) or just before that, e.g.:\nC++\ninline double GetTimeUnitMultiplier(TimeUnit unit) {\n  switch (unit) {\n    case kMillisecond:\n      return 1e3;\n    case kMicrosecond:\n      return 1e6;\n    case kNanosecond:\n      return 1e9;\n  }\n  __builtin_unreachable();\n  return 1e9;\n}\n. @LebedevRI Ah, okay, sounds great! Thank you for the suggestion!. Aww, I guess I totally messed this up, should get back to this tomorrow.\nI assumed __has_builtin(x) is supported by all host compilers and missed this macro in the corresponding files.\nNow the problem is also that #if defined(COMPILER_GCC) and similar are also defined in the header which I have operated on. This is a mess, I have to refine everything.. @LebedevRI these two are the only cases with default: placed in the wrong place. I was misguided by the compiler messages in the first place, thinking there were more, but all of the error messages were actually caused by these two pieces of code.. @dominichamon thank you very much for taking a look!\nIt's way better to use the same macros the top of the file utilizes, I didn't notice them in the first place.. I signed it!. ",
    "bmwiedemann": "I don't care. I could even live with a program telling me that I have \"1 CPUs\". Keeping code simple is always a plus for future maintainability.. ",
    "yurivict": "Compiler is clang-6.. googletest-1.8.0.712\n. ",
    "Leandros": "Sweet. Am I missing something or can I not compare multiple benchmarks to a single baseline? . ",
    "nicolastagliani": "To be honest I have a forked version that adds exactly that functionality (plus the possibility of comparing multiple benchmarks on a single baseline).\n@LebedevRI Would you suggest to try to make the pull request anyways?. ",
    "TNorthover": "Thanks Roman! I checked on my local build and this did indeed fix the issue.. ",
    "rnk": "Looks good, thanks!. I think that's not true in general, but it's specifically true for this policy, because it affects the way project() behaves. Either way, doing with the loop for consistency seems fine.. ",
    "mstorsjo": "This change was originally written for LLVM, at https://reviews.llvm.org/D52181, but it's not approved and committed there yet.. > How does this affect non-ARM MinGW builds?\nAll MinGW environments have time.h and sys/time.h, so it shouldn't matter if we include these headers there, just like it is done on all non-windows platforms.. Also just for the record; I realize this isn't the ideal full solution. I'm not extremely familiar with the lowlevel Windows timing APIs, but I think something like QueryPerformanceCounter() could be used as fallback instead of gettimeofday.. ",
    "janisozaur": "Amended with suggested changes. In about half a year when Trusty's support ends, will you consider switching CMake base version to that of Xenial's, 3.5, and removing the workaround included here?. This looks like a fun one. When project() is set to CXX, the compiler seems to enable pthreads by default(?)[0][1] and doesn't require -pthread option. When I add C, like in case of pre-3.4 CMake, the C compiler is interrogated and it requires -pthread[2] which then gets set appropriately.\n[0]\n```diff\n@@ -257,7 +257,10 @@ if (NOT BENCHMARK_ENABLE_EXCEPTIONS AND HAVE_STD_REGEX\n endif()\n cxx_feature_check(STEADY_CLOCK)\n # Ensure we have pthreads\n+set(CMAKE_THREAD_PREFER_PTHREAD TRUE)\n+set(THREADS_PREFER_PTHREAD_FLAG TRUE)\n find_package(Threads REQUIRED)\n+message(\"Threads: ${CMAKE_THREAD_LIBS_INIT}\")\n# Set up directories\n include_directories(${PROJECT_SOURCE_DIR}/include)\n```\n[1]\n-- Looking for C++ include pthread.h\n-- Looking for C++ include pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - found\n-- Found Threads: TRUE  \nThreads:\n[2]\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Check if compiler accepts -pthread\n-- Check if compiler accepts -pthread - yes\n-- Found Threads: TRUE  \nThreads: -pthread\nI haven't completely figured out why pthread_create is available in C++, but not in C or how to address this.. I tried it locally before and it didn't work. I'm still looking for ways to not hardcode -pthread and why it happens in the first place.. ",
    "dtoubelis": "GNUInstallDirs would be the ideal approach. This is a quick workaround made out of despair. It gets the problem solved and you can replace it with proper implementation at your convenience.\n. I'm perfectly fine. It would be my preferred approach because yocto supports GNUInstallDirs out of the box and we use cmake 3.6.x anyway. So, feel free to close this PR if you have a better solution.. Oh, I see :-) I only have few days of experience with cmake but I can give it a try.. It looks pretty straightforward but I also don't want to break you windows build. Something tells me that GNUInstallDirs may not work on Windows ;-). The easiest would be to keep BENCHMARK_ variables but populate them depending on the environment. But I'll figure it out.. ",
    "staffantj": "It looks as if the build break (as opposed to the warnings) comes out of gmock, with an \"no idea what warning 5046 is\" error message. \nIt doesn't really make sense that it suddenly appears at the point in the build history where it does, unless there's something about the build environment changing that Appveyor isn't showing me.\nCertainly it looks as if there has pretty much always been warnings in the Windows build (at least going back some time).\nI might try attacking some of those once/if I get the attribution paperwork in order.\nI've traced the C4512 warnings down to the State const parent_ member on the StateIterator. Should that by any chance be a State const ?. Apologies, I typo'd the subject, hopefully fixed now.\nIt occurs at, for instance, in master: https://github.com/google/benchmark/blob/4528c76b718acc9b57956f63069c699ae21edcab/include/benchmark/benchmark.h#L337\nThere's no compiler error / warning message that I've seen, merely the documentation on the microsoft site, I came across the issue whilst looking at gcc / msvc differences for an in-house documentation effort.. ",
    "Wheest": "Your quick concise response has got me on the right track, thank you. ",
    "jbeich": "Not so simple:\n- Users with old system googletest would be still affected\n- -Wthread-safety isn't used/supported by googletest\n- googletest upstream doesn't care much about FreeBSD\n. @yurivict, I'm putting this patch under public domain. Can you resubmit it and sign CLA (if not already)? CC'ing @FreeBSDFoundation in case it can act as a proxy for FreeBSD contributors that cannot sign up for Google Account due to privacy concerns.\n. > Surely, you could submit the proper fix for google test too?\nWhat is the proper fix? Something like llvm-mirror/libcxx@362353aa2d01? Annotating googletest locks properly is non-trivial.\nBesides, CLA is still an issue for Google (not me). I had no trouble signing Facebook CLA because it didn't require something as stupid as registering a Facebook Account... just to submit a one-off patch.\n. > will want to silence every project out there that uses google test this way?\nWhy do you want googletest (dependency) warnings to affect benchmark (this project)? SYSTEM converts -I into -isystem which makes the include path act like /usr/include where fixing warnings is not always a viable option. Let googletest handle -Wthread-safety issue at their own pace.\nOther projects don't force -Werror for release builds because there's no guarantee newer compiler versions won't introduce more warnings or if users won't try to compile on not officially supported platforms. Not to mention, -Werror=thread-safety only became an issue on FreeBSD because it cannot make up its mind about whether /usr/local is a system path or not. Most Linux distributions implicitly mark the path to where googletest is installed as SYSTEM.\n. ",
    "saikishor": "If you see for the first two iteration count the results are merely 10 times of the earlier one, but for a loop of 4000 the results are not nearly 10 times of the earliest one. I guess the issue exists when we start the benchmarking.. > 113200 ms (400000) is ~= 10 * 11414 ms (40000), correct?\nYes right. Isn't it?\n\nI think you are reading this wrong, not accounting for Iterations column.\n\nMay be, In what way should I consider the Iterations column to evaluate results?. @LebedevRI Thank you. I will take a look into it.. I am not sure, but I guess benchmarking is taking sometime at start. It's just my inference based on the following results.\n```\nBenchmark                                          Time           CPU Iterations\nBM_distance_check/400000                        158453 ms     158483 ms          1\nBM_distance_check/40000                          11254 ms      11258 ms          1\nBM_distance_check/4000                            1124 ms       1124 ms          1\nBM_distance_check/400                              113 ms        113 ms          6\nBM_distance_check/40                                11 ms         11 ms         63\nBM_distance_check/40                                11 ms         11 ms         62\nBM_distance_check/400                              117 ms        117 ms          6\nBM_distance_check/4000                            1113 ms       1114 ms          1\nBM_distance_check/40000                          11284 ms      11288 ms          1\nBM_distance_check/400000                        114894 ms     114928 ms          1\n```\n```\nBenchmark                                           Time           CPU Iterations\nBM_distance_check/40                                17 ms         17 ms         60\nBM_distance_check/400                              189 ms        189 ms          6\nBM_distance_check/4000                            1114 ms       1114 ms          1\nBM_distance_check/40000                          11332 ms      11335 ms          1\nBM_distance_check/400000                        114069 ms     114104 ms          1\nBM_distance_check/400000                        114251 ms     114280 ms          1\nBM_distance_check/40000                          11326 ms      11323 ms          1\nBM_distance_check/4000                            1135 ms       1135 ms          1\nBM_distance_check/400                              115 ms        115 ms          6\nBM_distance_check/40                                12 ms         12 ms         59\n```. @LebedevRI  With respect to iterations, Yes for the faster calls, benchmark calls the function many times to produce stable result. I have one more question, in my last comment I have shared more result here, where the first run benchmark accumulates some time, is there any reason for that? or is it just my intuition?. \nThank you,. @LebedevRI  Thanks a lot for the clarifications. Clearly understood!!. ",
    "Lord-Kamina": "CLA signed.. ",
    "marehr": "Is this a duplicate of #541?. ",
    "dscole": "yeah duplicate. ",
    "iillyyaa": "Roman, I'm sure there is a way, but my cmake-fu is not up to snuff to come up with it. I'd appreciate if you accepted the fix as is. If tests must be provided, I will do my best, but can't make promises on the timing of the update.. ",
    "peterjc123": "@googlebot I signed it!. @LebedevRI Yes, you are right. I forgot to update the code to the current master.. ",
    "timshen91": "\nif the manual 'time' is actually measuring time, in the same scale/units as the min_time\nThat isn't a requirement.\n\nAh yes, one shouldn't always compare real_time with min_time. What I mean is to compare real_time with min_real_time. There isn't a min_real_time, but I hope that there is a way to get it.\n\nActually, more reasonable question: why can't you just adjust the min_time for your benchmark?\n\nFor a single benchmark, I can do that. I can take a look at --v=3, observe that in order to run for roughly 0.5 seconds real time, I need to set MinTime(0.00194). But it doesn't scale, as for every benchmark the MinTime() argument is going to be different, and the argument may not be stable across library upgrades.. @EricWF , do you still remember the motivation behind 46afd8e? Do you have any solutions on solving the problem described in this patch description?. ",
    "olzhabay": "CLA signed. > @olzhabay hm, thinking about it a bit more, you placed googletest into the benchmark/googletest?\n\nAnd it is looking in yourproject'sroot/googletest.\nI think the solution here is slightly incorrect.\nI think there should be an option that will specify the path to the googletest directory.\n\nYes, googletest is place in benchmark/googletest. \nIf it would be place outside of benchmark directory, then it cannot execute add_subdirectory.\nElif it fails to find googletest in benchmark subdir, it will simply download gtest or find it in among system installs.\nWhat you think @LebedevRI ?\n. @LebedevRI Totally got your point. I added this option. So, now it can look to both myproject's dir and googlebenchmark's dir, if one of them has an googletest. @EricWF sure. @EricWF my pleasure :)\n. ",
    "amallia": "I signed it!. The problem is that now all the compilers understand BENCHMARK_UNREACHABLE();. https://github.com/google/benchmark/blob/d8c0f27448dfad95b94285e612bba1f7c55c9dd0/include/benchmark/benchmark.h#L253\n. I believe since __has_builtin(__builtin_unreachable) is false, it should not make any difference.... This is what I am saying..... Don't you want to support compilers that are not MSC, GNU and do not have __builtin_unreachable macro?. Tried already! Same stuff.... Well, what can I show? It is exactly the same error.... why is default giving a warning? . These options sound good. I will try them and update the pr. I have removed the default cases. Opted for solution #2 from @brenoguim \nI hope this one gets accepted. Yes @brenoguim you are correct. ",
    "RuABraun": "So how does one fix this? I do not understand @dominichamon 's answer.. ",
    "jatinx": "I signed it!. I signed it!. @LebedevRI  did the asked changes, can you please have a look at it?. @LebedevRI its been a week, is something wrong with my design, would like some comments if thats the case.. Sometimes, on travis-ci, the last build on MacOS fails in configure.\nIt gets resolved if I restart the build with a minor comment merge.\nThis has happened twice now.\n@dominichamon @LebedevRI . Removed from console and added only to JSON. HOST_NAME_MAX was not present on OSx, added local initialization.\nThe Job on Travis failed in config, can you please have a look at it?. The only segregation needed here is POSIX vs non POSIX Compliance.\nPOSIX has gethostname. Using Standard wstring_convert to fix this. Removing unnecessary COUNT variable references, using macro directly. Initialising TCHAR to null and only copying DWCOUNT into string. . we can use something like this\nhttps://github.com/google/benchmark/blob/master/src/benchmark.cc#L430-L431. Can be done, let me know if you want me proceed with this. merging requested changes. 1. Yes, I checked the output of the function, it just outputs the hostname for windows without the \\\\\n2. Addressed\n3. I am not sure about this one, it can be done, but since the output being in json only, is it desirable?. Done. are you talking about the 2nd point?\nI have placed an empty string incase it fails to fetch the host name. ",
    "hal0": "That's fair enough. Thank you for your response.. ",
    "derekmauro": "The short story is that I had a benchmark that I thought had to be declared cc_test (in Bazel) to get around a dependency check, but I didn't want the benchmark to actually run when bazel test was invoked because it would take too long. I found a solution for the dependency problem (cc_binary with testonly = 1), so for now simply updating the documentation is fine.. ",
    "JBakamovic": "\nI don't dislike this in general, but right now it is a just simple script.\n\nYes it is super simple right now but why wouldn't it provide functionality that basically anyone can have benefit of? And the change is a really simple one ...\n\ni'm really uneasy about making any API/json stability guarantees.\n\nBut we already have a JSON format output defined when dumping the benchmark results. Outputting the diff report to JSON would be clearly built upon it.\n\nThis really should be done by something much more specialized, like llvm lnt..\n\nSorry but I fail to see what does LLVM has to do with this topic? Proposed functionality is a generic one that any project could make use of it without any other dependencies.\n. > So can that output be fed back into the compare.py script?\nNo, I have not envisioned that use-case. Not sure what compare.py could do in that particular case but fail.\n\nThe usual CI is unfit to do any meaningful performance measurements.\n\nCI is just one example that I gave. Different projects will have different needs or means of post-processing the performance data. The most simple one is a plotter.\n\nIt will be extremely noisy, because the hardware will be shared.\n\nYes, this is a CI challenge but can be tackled by having a dedicated no-shared node which will do only performance tests one at a time. Having reproducible and continuous performance regressions detection is a very important aspect in developing high-quality software, IMO. Or in other words, being able to detect regressions automatically and early during the development cycle.. > Sure, just want to point out that then it will no longer be advertisable as \"reliable to use by any project with no special setup.\".\nAgreed, but understanding performance implications has never been a trivial task. Even with this framework you still have to know what you're doing regardless of dump-to-json functionality. If you accept the PR, we can make an emphasis in documentation about correctness of interpreting the results.\nP.S. I somehow managed to close the issue unintentionally. Ignore that :D. Can you please trigger Travis again? I can't seem to find a way to do it. It looks like a flakiness not related to this change because apt-get install seem to timeout.. > good to merge?\nWhile I was working on it, focus was actually on #738 which includes this change among others, but that pull-request kinda stalled because reviewer seem not to care to reply anymore.\nIIRC I think this change has a subtle bug. I'll check it.\nEDIT: from the quick look I can see that calc_utest() was called with cpu and real time mixed up. That was fixed in #738.. Will do so but I'm not sure what you're thinking on particularly? I tried to format these as much as I could so that the unit tests do not become too much spread out because of the test data. Can you please give an example of how you would like it to be?. Thanks, I was unaware of this tool. New commit is up.. Done. #739 . I had it named optimal_repetitions previously but changed it to stable_results because I thought that communicating that results (both pvalues) not being stable was sufficient (generic enough) to indicate the erroneous state. That way we are avoiding leaking details of how this error condition is actually evaluated by the underlying function call: calc_utest.\nLet me know what you think about that.. Done. #740 . > I agree that the old hardcoding of 0.05 is wrong, but this is not correct either.\n\nThe specified utest_alpha (passed to generate_difference_report()) should be propagated/used.\n\nYou're actually referring to this line? If so, I will make a change and send it another PR if that's what you would like.. Yes, meaning that the original code was not abstracted away enough to be complemented/extended with new features because it only ever assumed that data will be printed to stdout and not consumed in any other way. I didn't want to go there unless asked by someone because that will require quite a few changes in the whole script.. Ok, I misunderstood this one then.. Alright, it makes sense. Then I think reverting the whole change makes most sense because most occurrences of it are in unit tests. Do you agree?. Agreed. I will revert the change.. Done in the 622d96c commit.. Hmmm, I'm not sure about this one. calc_utest(), as it's name implies, return a u-test result no less no more. Narrowing the scope of the functions usually results in less bugs because there are less things that you can do wrong. So, tbh, I am not sure if this change would improve on the code.. Alright, I think I'm on my way finishing this change. Please see 9021718. generate_difference_report is now a consumer of data fed by get_json_difference_report.\nOne more thing I've yet to be done is to re-implement the unit-tests to complement this change.. One more thing I would like to note and which is related to your comment in #740 is that print_utest now became much less involved and looks more natural to what is supposed to actually do.. I believe that this change now became sort of obsolete (or irrelevant) because IMO is that now we have much better solution with changes being done in #737. See 9021718 to get a feeling how it looks like.. sufficient_observation_count seems a bit overly wordy IMO. Perhaps, optimal_repetitions if stable is not satisfying?\nNewest changes are up so please have a look at them when you find it appropriate.. > What is the definition of optimal, stable in this case?\nI was following the wording of existing code (e.g. UTEST_OPTIMAL_REPETITIONS) so if we decide that optimal is not good enough changes should be done to that parts of the source code as well.\n\nsufficient has a very direct meaning based on https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html\n\nAlright, makes sense. So sufficient_observation_count it is?\n. @LebedevRI ping :). ",
    "travisdowns": "@LebedevRI - no, that is essentially the same as putting it in the Setup method (plus some extra duplication) - it will be called not once per benchmark, but once every time the benchmark method is invoked (which is 10+ times as the benchmark hunts for right number of iterations).\nConsider this simple example: you want to test std::find on vectors of size 100, 10,000 and 1,000,000 (using say ->Arg(100)->Arg(10000)->Arg(1000000)). How can I write this so that it only creates 3 vectors total of the correct size?. > To be noted, the time outside of the loop isn't counted, thus, there will be no direct noise.\nYes, I didn't mean \"direct\" noise. What I mean is that constantly allocating and deallocating the per-benchmark data adds an unnecessary source of variance. For example, the new allocations may have a different alignment, or no longer be backed by huge pages, happen to alias differently in the L2 cache, end up coming from a different NUMA node, whatever.\nSimilarly, any setup which uses say a random number generator has to be careful to ensure the generator if freshly seeded with a fixed seed or else you'll be testing different input each time.. > But then you hit the exact same problem with repetitions.\nYes, but at least then the variance is in the \"right place\" I think - since GB will track the variance and tell you about it, right?\nIf there is variance in the \"iteration hunting search\" or whatever it is called that GB does before it has run long enough, it seems worse, because it will affect the search algorithm and only the final result will eventually be used, so the variance will be kind of hidden.\nI agree it is not necessarily a super serious issue though - depends on your use case.\nPerhaps a more obvious reason to fix it is to avoid doing the setup 10x when the setup is slow, which slows down the whole benchmark.. ",
    "AdamHarries": "I think I might have miscommunication somewhat - I meant to say that both \"minimum time\", and \"iterations\" are insufficient for what I'm trying to do. I'll try and give an example. \nI have a number of benchmarks where we want to evaluate them across a range of values. For example, an operation F on N*M matrices, where we want to measure performance across various values of N and M, where they might range between [64, 2048] (say). It should be unsurprising that F is fairly fast when N and M are small, and quite slow when N and M are large. \nMy issue is that googlebench (imho, incorrectly - but I'll explain further) calculates that fewer iterations are required when N and M are large, as the time taken by F seems more stable. This, unfortunately, isn't always the case, as we sometimes work with quite temperamental hardware that can have quite wide performance ranges for F (even when N and M are large). \nThe two existing solutions benchmark_min_time and Iterations() don't really address this case I don't think: \n- Iterations() sets an absolute number of iterations that all configurations of parameters must run for. This number might be too large for longer running configurations of F (e.g. when N and M are large), causing our benchmarks to run for a long time, and might also be too small for faster configurations, causing us to gather insufficiently statistically significant data. \n- benchmark_min_time sets a minimum length of time that all configurations must run for. This would solve the problem for large values of N and M, but when N and M are small, this imposes a somewhat unacceptable overhead, as the faster benchmarks currently finish quite quickly. \nAs far as I can tell, having a benchmark_min_iterations parameter is the only possible way to get around these limitations: \n- For small values of N and M, the number of iterations is unaffected, as they would already perform more iterations in order to get statistically significant data. \n- For larger values of N and M, we increase the number of iterations without reducing the number of iterations for other configurations, and without making other configurations take just as long to run. \nI hope I've explained my use case a bit more clearly? If you have any other thoughts or potential solutions, they would also be very welcome. \nCheers, \nAdam . For the F that I'm specifically interested in benchmarking, the \"tempermental hardware\" are various customer GPUs that have slightly unpredictable throttling, and warmup times. Additionally, we do a certain amount of JIT compilation that may cause unexpected changes in performance at unpredictable times. Warming up the benchmark might be a potential solution, but also has other problems (especially with long running benchmarks), so I'd much rather ask the benchmarking framework to take more measurements than it thinks it needs. . As I (think) I explained above, Benchmark::MinTime doesn't suit my requirements, as it affects all configurations, not just long running ones. If I understand correctly, State::KeepRunningBatch() essentially implements the same behaviour as Iterations, which (as I said above) doesn't suit my requirements. That said, a way to \"request\" that the current state keeps running iterations (while in a for (auto _ : state) loop) would also be suitable, but I haven't managed to find a way to do that yet. . Good to know about the statistical stability of the benchmarks - and what controls the number of iterations. I wonder if an alternative solution might be to implement alternative strategies for deciding the number of iterations? e.g. keep running the benchmark until the standard deviation is 5% of the mean? \n\nIt will mean the smaller runs will run for more iterations, but that would be the case if you increased minimum iterations too :)\n\nThat wouldn't be the case - my faster configurations run for (say) 20,000 iterations, while my slower ones run for (say) 5 iterations. Let's say both take about 10 seconds total. If I could set a minimum number of iterations (say, 30), the former set would still run for 20,000 iterations, and the second set would run for 30. \nIf I can only set the minimum time, however, then I need to set it to the equivalent of 30 iterations for the slower configuration (60s). In this case both the faster and slower configurations will run for that long, meaning that the whole set takes 2 minutes, rather than 1m10s, and the faster benchmarks will run for 120,000 iterations which slows down the whole suite of benchmarks. \nI hope this makes my use case a bit clearer?. ",
    "berestovskyy": "\nWhere can the problem be observed?\n\nIt's every horizontal line in the header of the console reporter.\nBefore:\n\nAfter:\n\n. ",
    "domob1812": "\n\npthread is required at least for GCC (according to the documentation),\n\nLink?\nI suspect the problem is on cmake's side.\n\nhttps://github.com/google/benchmark/blame/master/README.md#L126 says so and I get linker errors without that change.  I'm not a cmake expert, though, so perhaps the change should be different - it is similar to what gflags does, though.\nAlso signed the CLA.. @dominichamon:  Yes exactly.  pkg-config is supposed to figure out all the flags you have to use for using some library, so you shouldn't need to manually add -pthread when you use benchmark through that.\n@LebedevRI:  As I said, I'm not an expert with cmake.  So if there is some other way to pass the flags through to pkg-config, that is of course even better.  But it seems that other projects (e.g. gflags) are doing it that way, and it solves the issue.. ",
    "andreprager": "cmake-gui is just a convient tool for setting all the cache-values etc.\nI don't download the dependencies, I use prebuilt binaries for gtest (same setup as for gbenchmark).\nFrom a technical point of view the appropriate test project (executables) need a main-entry point which is actually missing if I do not link against the benchmark_main target. So my question would be, how is that entry point added if not via the target?. ",
    "NikolaMr": "Thank you, Dominic. I will surely give it a try!. In the end, I have acomplished this by calling a native method that invokes the performTests() method that is visible in the code. The method redirects the output of the ConsoleReporter to the BenchmarkStream as it can be seen in the code below. Hope it will be of some use to others.\n```\nstruct BenchmarkStream : std::ostream, std::streambuf\n{\n    BenchmarkStream() : std::ostream(this) {}\nstd::vector<char> buffer;\n\nint overflow(int c)\n{\n    buffer.push_back(c);\n    return 0;\n}\n\n};\nstd::string benchmarkResults = \"\";\nvoid performTests() {\n    int argc = 1;\n    std::string argument = \"\";\n    char* argv[1] = {\"test\"};\n    benchmark::ConsoleReporter reporter;\n    BenchmarkStream outStream;\n    reporter.SetOutputStream(&outStream);\n    benchmark::Initialize(&argc, argv);\n    if (benchmark::ReportUnrecognizedArguments(argc, argv)) return;\n    benchmark::RunSpecifiedBenchmarks(&reporter, nullptr);\nbenchmarkResults = \"\";\nfor (auto c : outStream.buffer) {\n    benchmarkResults += c;\n}\n\nLOGI(\"%s\", benchmarkResults.c_str());\n\n}\n```. ",
    "danielharvey458": "\nCan you show the console output of that test case before and after this change,\nso it is easier to understand what the problem is?\n(Thought yes, i think this is likely going in the right direction..)\n\nDo you mean in this conversation, or somehow in the PR? Example of console output differences:\n```\n< BM_ComplexityCaptureArgs_BigO                 0.00 N          0.00 N  \n< BM_ComplexityCaptureArgs_RMS                     1 %            33 %    \n\nBM_ComplexityCaptureArgs/capture_test_BigO       0.00 N          0.00 N  \nBM_ComplexityCaptureArgs/capture_test_RMS          0 %            20 %  \n```\n\nSo test_case_name (in this case capture_test) is included in the output.. I'm happy to take a look at this unless anyone has a patch ready to go.. Should we definitely include 0 in the range if lo and hi straddle zero? The current behaviour is to include powers of the range multiplier between lo and hi, which would naturally extend to e.g. {-8, -4, -2, -1, 1, 2, 4, 8}. I can see that special-casing 0 might be desirable, though.. Sure thing. I wanted to avoid cluttering the headers with declarations purely for the purpose of writing tests.\nFor my education - why do you think it won't work?. You're absolutely right - and I thought I had an easy fix!. Thanks - that sounds good to me. I'll give it a try, perhaps I'll submit a separate PR since it will be pretty different to this solution!. I'm fairly confident that const T and T for parameters are interchangeable. I personally mark them const in the definition to document that I won't modify the argument in the function body, and not const in the header since the caller needn't care for by-value parameters.\nAll that said, I can't find anything in the standard from a quick glance ensuring that this is OK, and anyway I'm happy to comply with the local conventions.. True, although get(Field::kAll) would, currently, re-build the string component-wise whereas .str() just does a string copy. Could add a fast-path into the implementation of get though.. I agree this is fragile. With modern c++ we could compute at compile time the number of set bits in kAll, which would do the trick, but it has to be done in the header so wouldn't work for <= c++03 clients.\nI think your suggestion would make it tricky to chose a subset of name fields - composing the bits of enumerator values seems natural for that.\nOf course, as you point out, only kAll and kAll | ~kArgs are currently used, so we could just add an additional enumerator for 'name without the args' request (or, as you suggest below, drop the enumeration entirely and just have two accessors for the currently-used cases).. That certainly makes the code simpler, I didn't consider it.\nI guess the downsides of that are that in the common case (accessing the whole name) you have to re-build the full name every time. The current approach keeps the full name and can just return it. It also means adding a new field to the name requires a little bit more work than just adding an enumeration value (and, er, updating enumerator_count).\nWhat do you think? Happy to go for the simpler option if preferred.. It's a member of BenchmarkReporter, which is the only reason it's in the header. I could store a pointer to it in BenchmarkReporter so that it only requires a forward-declaration?\nEDIT: BenchmarkReporter::benchmark_name also requires knowing about BenchmarkName::Field enumeration, which cannot be forward-declared.. ",
    "jilinzhou": "I signed it!. I signed it!\nGet Outlook for Androidhttps://aka.ms/ghei36\n\nFrom: googlebot notifications@github.com\nSent: Thursday, February 14, 2019 2:37:08 PM\nTo: google/benchmark\nCc: Jilin Zhou; Author\nSubject: Re: [google/benchmark] [#766] Add x-compile support for QNX (#767)\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n\nWhat to do if you already signed the CLA\nIndividual signers\n\nIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA datahttps://cla.developers.google.com/clas and verify that your email is set on your git commitshttps://help.github.com/articles/setting-your-email-in-git/.\n\nCorporate signers\n\nYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoothttp://go/cla#troubleshoot (Public versionhttps://opensource.google.com/docs/cla/#troubleshoot).\nThe email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA datahttps://cla.developers.google.com/clas and verify that your email is set on your git commitshttps://help.github.com/articles/setting-your-email-in-git/.\nThe email used to register you as an authorized contributor must also be attached to your GitHub accounthttps://github.com/settings/emails.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/google/benchmark/pull/767#issuecomment-463762548, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AGjDPU6WuzH-3JKUzB1OSWzwi9cM9LU3ks5vNbrkgaJpZM4a8S_O.\n. I signed it!. What does this mean:\nunzip:  cannot find zipfile directory in one of bazel-installer.sh or\n        bazel-installer.sh.zip, and cannot find bazel-installer.sh.ZIP, period.\n/home/travis/.travis/functions: line 524:  7496 Terminated              travis_jigger \"${!}\" \"${timeout}\" \"${cmd[@]}\"\nThe command \"if [ \"${TRAVIS_OS_NAME}\" == \"linux\" ]; then sudo apt-get update -qq; sudo apt-get install -qq unzip; wget https://github.com/bazelbuild/bazel/releases/download/0.10.1/bazel-0.10.1-installer-linux-x86_64.sh --output-document bazel-installer.sh; travis_wait sudo bash bazel-installer.sh; fi\" failed and exited with 9 during .. Here is what in QNX SDP 7:\n```\n\n\ngrep -r \"HOST_NAME_MAX\" ./usr/include\n./usr/include/confname.h:#define _SC_HOST_NAME_MAX                  154\n./usr/include/limits.h:#undef   _POSIX_HOST_NAME_MAX\n./usr/include/limits.h:#define  _POSIX_HOST_NAME_MAX    255\n```. \n\n",
    "wesm": "@LebedevRI if you use Threads::Threads then this is what appears in benchmarkTargets.cmake instead of -pthread. So consumers of the installed library would need to also use FindThreads in order to use the CMake config. I haven't tested this extensively on other platforms so would be interested to get feedback from experts who understand the nuances more deeply. Seems reasonable. @jcfr are you able to submit a PR for this?. ",
    "jcfr": "(Background: @wesm I am commenting here following up you last tweet.)\n\nSo consumers of the installed library would need to also use FindThreads in order to use the CMake config\n\nThe key here is to update cmake/Config.cmake.in so that it has:\ninclude(CMakeFindDependencyMacro)\nset(THREADS_PREFER_PTHREAD_FLAG 1)\nfind_dependency(Threads)\nDoing so would ensure that consumer do not have to worry about the private dependencies of the benchmark  project and would allow to use Thread::Thread target.\nNote that CMakeLists.txt would have to be updated to also set THREADS_PREFER_PTHREAD_FLAG\n. ",
    "pitrou": "\nWhat is the use case, why do you want to know that after the benchmark?\n\nBecause we're going to inject benchmark results in a database and then track results across time, try to detect regressions, etc. For that we need to know which number is authoritative for each benchmark.. ",
    "assassin5615": "I assume that you mean CMake*.log? I didn't find anything useful. contents at the end of CMakeOutput.log is as below, it's only for HAVE_CXX_FLAG_COVERAGE which is before the REGEX check.\n```\nSource file was:\nint main() { return 0; }\nPerforming C++ SOURCE FILE Test HAVE_CXX_FLAG_COVERAGE succeded with the following output:\nChange Dir: /home/ylei/development/third_party/benchmark/build/CMakeFiles/CMakeTmp\nRun Build Command:\"/usr/bin/gmake\" \"cmTC_a413e/fast\"\n/usr/bin/gmake -f CMakeFiles/cmTC_a413e.dir/build.make CMakeFiles/cmTC_a413e.dir/build\ngmake[1]: Entering directory /home/ylei/development/third_party/benchmark/build/CMakeFiles/CMakeTmp'\nBuilding CXX object CMakeFiles/cmTC_a413e.dir/src.cxx.o\n/home/ylei/usr/local/gcc-7.3.0/bin/g++     -std=c++11  -Wall  -Wextra  -Wshadow  -pedantic  -pedantic-errors  -fstrict-aliasing  -Wno-deprecated-declarations  -Wstrict-aliasing -DHAVE_CXX_FLAG_COVERAGE  --coverage   --coverage -o CMakeFiles/cmTC_a413e.dir/src.cxx.o -c /home/ylei/development/third_party/benchmark/build/CMakeFiles/CMakeTmp/src.cxx\nLinking CXX executable cmTC_a413e\n/usr/local/bin/cmake -E cmake_link_script CMakeFiles/cmTC_a413e.dir/link.txt --verbose=1\n/home/ylei/usr/local/gcc-7.3.0/bin/g++     -std=c++11  -Wall  -Wextra  -Wshadow  -pedantic  -pedantic-errors  -fstrict-aliasing  -Wno-deprecated-declarations  -Wstrict-aliasing -DHAVE_CXX_FLAG_COVERAGE  --coverage    CMakeFiles/cmTC_a413e.dir/src.cxx.o  -o cmTC_a413e -rdynamic\ngmake[1]: Leaving directory/home/ylei/development/third_party/benchmark/build/CMakeFiles/CMakeTmp'\nSource file was:\nint main() { return 0; }\n```\nContents at the end of CMakeError.log as below. it's for THREAD_SAFETY which is also before REGEX check.\n```\nSource file was:\nint main() { return 0; }\nPerforming C++ SOURCE FILE Test HAVE_CXX_FLAG_WTHREAD_SAFETY failed with the following output:\nChange Dir: /home/ylei/development/third_party/benchmark/build/CMakeFiles/CMakeTmp\nRun Build Command:\"/usr/bin/gmake\" \"cmTC_a5ac9/fast\"\n/usr/bin/gmake -f CMakeFiles/cmTC_a5ac9.dir/build.make CMakeFiles/cmTC_a5ac9.dir/build\ngmake[1]: Entering directory /home/ylei/development/third_party/benchmark/build/CMakeFiles/CMakeTmp'\nBuilding CXX object CMakeFiles/cmTC_a5ac9.dir/src.cxx.o\n/home/ylei/usr/local/gcc-7.3.0/bin/g++     -std=c++11  -Wall  -Wextra  -Wshadow  -pedantic  -pedantic-errors  -fstrict-aliasing  -Wno-deprecated-declarations  -Wstrict-aliasing -DHAVE_CXX_FLAG_WTHREAD_SAFETY  -Wthread-safety   -Wthread-safety -o CMakeFiles/cmTC_a5ac9.dir/src.cxx.o -c /home/ylei/development/third_party/benchmark/build/CMakeFiles/CMakeTmp/src.cxx\ng++: error: unrecognized command line option '-Wthread-safety'; did you mean '-fthread-jumps'?\ng++: error: unrecognized command line option '-Wthread-safety'; did you mean '-fthread-jumps'?\ngmake[1]: *** [CMakeFiles/cmTC_a5ac9.dir/src.cxx.o] Error 1\ngmake[1]: Leaving directory/home/ylei/development/third_party/benchmark/build/CMakeFiles/CMakeTmp'\ngmake: *** [cmTC_a5ac9/fast] Error 2\nSource file was:\nint main() { return 0; }\n```\nit does not look like that -dev package is missing. as with -DCMAKE_CROSSCOMPILING=1 -DRUN_HAVE_STD_REGEX=0 -DRUN_HAVE_POSIX_REGEX=0 added, I got following output that says REGEX is successful.\n-- Performing Test HAVE_STD_REGEX -- success\n-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile\n-- Performing Test HAVE_POSIX_REGEX -- success\n-- Performing Test HAVE_STEADY_CLOCK\n-- Performing Test HAVE_STEADY_CLOCK\n-- Performing Test HAVE_STEADY_CLOCK -- success\n. ",
    "haquocviet": "I use it in MSVC. \nRegardless which tool is used, I think the leak indeed exists because the CPUInfo object is allocated but never be freed anywhere.\nThough that leak is not a disaster in the library but any careful programmer could be irritated when look at the leak report.. If the situation in the benchmark is similar to the example you cited then there should be a design flaw since it lets tasks still continuing after the program's main function goes out of scope. . ",
    "tesch1": "I signed it!. @LebedevRI the new title is a work of unmitigated beauty ;o)  I will try harder next time. ",
    "zzz6519003": "\u6c42\u95ee\u8fd9\u53e5\u80fd\u89e3\u91ca\u4e0b\u4e48XD\n. \u591a\u8c22\u54c8 \u60a8\u5728akuna\u4e48  \u6211\u770b\u5230\u4f60\u7684\u9898\u89e3\u4e86XD \u90a3\u91cc\u611f\u89c9\u5982\u4f55\u54c7\n. ",
    "mkurdej": "Well, to preserve current behaviour, the output will be exactly as in the current version if you do not specify ArgName. If you have more arguments than names, than only the first will be named.\nOne can find it a possible source of incoherence, but that's similar to the behaviour of Args, imagine calling it as Args({1, 2, 3})->Args({1, 2})->Args({1,2,3,4,5}) - that's user's responsibility to define all the arguments its benchmark will need.\nFor your suggestion, I find it a bit \"heavy\" to set argument names each time, I would certainly not want to have to write this: Args{{\"first\", 1}, {\"second\", 3}}->Args{{\"first\", 2}, {\"second\", 5}} and so on. And it is disturbing that the same argument can be named differently each time (or isn't it?)\nAnyway, your remarks made me think of a case that I didn't anticipate: the user might want to name only few arguments, e.g. ArgNames({\"first\", \"\", \"third\"}), my current PR will write BM_benchmark/first:1/:2/third:3 (note the semicolon before 2). I'll correct it ASAP.\n. Oh, I see now that there are checks in Arg/Args/Range/Ranges/DenseRange that use ArgsCnt. Previously I thought that you could call Args with different number of elements each time.\nI'll add these checks then, it seems to be the way to go.\n. I've added the constraint for the size. I've also modified ArgsCnt, so that it could be possible to call Args and ArgNames in any order. If you see a simpler/more elegant way to do this, I'm all ears.\n. Did you mean removing the duplicated private:? Cf. the last commit.\n. ",
    "nlkm": "Is it possible to use cc_test instead?\nShouldn't users be able to do \"bazel test :all\" to validate that all tests are passing?. ",
    "mvafin": "I have no problems in changing it to \"> 0\", but in case of an error read returns negative number as error. Non-negative number means that read succsesfully finished. The case when nothing is read is covered on line 80.. ",
    "rolandschulz": "Otherwise ICC produces: non-pointer conversion from \"long\" to \"int\" may lose significant bits. I could suppress the warning. But I assumed that a cast would be better. Also it seemed that the static_cast as it was made no sense. Given that milliseconds is already int.. Otherwise ICC produces: declaration hides variable \"::dec_re\" (declared at line 67 of \"../test/output_test.h\"). I understood Dominic on #354 that it is better to fix the warning then suppress it.\n. No.. ",
    "rayglover-ibm": "actually, this is a minor bugfix... ",
    "Slonegg": "Tested this approach, doesn't work. This is quirk of Android GNU STL library. This problem doesn't appear if compiling with  -DCMAKE_ANDROID_STL_TYPE=c++_static which uses clang stl library. But gnustl is default for Android now. I can replace it with #if defined(__ANDROID__) && defined(__GLIBCXX__) - it works, if you think it is better.. Fair, didn't find FormatString, but there is StringPrintF.. ",
    "eggrobin": "@EricWF, you're right, something is afoot with the floating point arithmetic. Unfortunately, while it explains a nonzero difference, it does not explain its negative sign.\nThe issue is not that d1 and d2 (from your traces) are actually equal, even though, when you print their bit representation as IEEE 754 binary64 (double), you get the same thing (yes, this is terrible). If they were equal, any remotely sane floating-point arithmetic would give you a difference of 0.\nWhat is instead happening is that you are seeing x87's 80-bit extended precision format hiding beneath those doubles.\nThe following analysis was performed using my soft-float Mathematica package, for easy inspection of representations and selection of floating point formats.\nRepresentations below are shown in the form\nsign bit|exponent bits|significand bits;,\nnon-representable numbers are shown extending the significand,\nsign bit|exponent bits|significand bits;more bits.\nA caveat on the x87 representations, I do not print the infamous redundant bit 63; those will thus be 79 bits long.\nThe number printed for d1 and d2 (which I shall call d, to avoid confusion with the actual underlying values of d1 and d2) is 1/64, binary64 representation\n0|01111111001|0000000000000000000000000000000000000000000000000000;\nThe number printed for d3 is 835/2048, binary64 representation\n1|01111000010|1010000110000000000000000000000000000000000000000000;\nThe ULP distance between d and d+d3 is 835/4096, which less 1, i.e., it was not the result of a subtraction of two nearby binary64 floating-point numbers. Since this distance is not an integer, d+d3 is not representable in binary64:\n0|01111111000|1111111111111111111111111111111111111111111111111111;10010111101\nNote that we expect the difference to be exact by Sterbenz's lemma, as @pleroy said; indeed, d3 uses very few of its significand bits. It is likely that its value is actually a double. \nLet us look at d+d3 in x87 extended precision: it is \n0|011111111111000|111111111111111111111111111111111111111111111111111110010111101;\nexactly.\nThis sum takes the entire significand to represent, which is an unmistakable clue that we are indeed dealing with x87 extended format.\nThat is all very well and good, and explains why the difference of numbers that print equal is nonzero: printing goes through memory rather than staying on the x87 stack, so extended format gets rounded to binary64.\nThe question remains of why it is negative: even in x87 arithmetic, if d2-d1 < 0, d2 < d1.\nThis means that the output of ProcessCPUUsage is not monotonic. Further, since we are getting extended format woes, it means that the output of ProcessCPUUsage is not binary64.\nThe latter is easily explained by the presence of arithmetic in MakeTime:\ncpp\n  return (static_cast<double>(kernel.QuadPart) +\n          static_cast<double>(user.QuadPart)) *\n         1e-7;\nwhile the casts will probably really return double, the rest of the arithmetic gets done in x87.\nWhat about the nonmonotonicity?\nComposition of monotonic functions is monotonic;\nin any rounding mode, rounding is monotonic;\na sum is correctly rounded;\ntherefore if the exact sum is monotonic, the rounded sum will be monotonic;\nmultiplication by a constant is monotonic;\na product is correctly rounded;\ntherefore if the exact sum of static_cast<double>(kernel.QuadPart) and static_cast<double>(user.QuadPart) is monotonic, the result of MakeTime will be monotonic.\nSince both are positive, if both kernel.QuadPart and  user.QuadPart are monotonic, their casts to double and their sum will be too.\nIt follows that the root issue is nonmonotonicity of either kernel time or user time returned by GetProcessTimes, in sufficiently insignificant bits that it only makes it out of the significance-killing MakeTime when using x87 arithmetic.\nIt should be possible to verify that (isolating the issue from the hard-to-trace x87 floating-point) by looking at the actual integers in MakeTime.. > One of the two operands to the subtraction has been written to memory truncating it, while the other has not\nGood point, I had overlooked the possibility of additional roundings to binary64 in my analysis. You found the bug.\n\n\nI believe that the fix is to force flushing to double at the end of MakeTime using a volatile variable.\n\nI think the real fix here is to store the integer representation instead of converting to double, at least until as late as possible.\n\nSwitching to integers makes things clumsier, since it prevents using the SI base unit everywhere, which means that identifiers need to be peppered with SI prefixes to avoid mistaking nanoseconds for seconds, microseconds or Windows ticks.\nA more proper alternative would be to switch to a wrapper class, such as absl::Duration.\nAs discussed, this would be a much larger scale change (especially if it involves introducing a dependency on absl).\nIn the meantime I would join @pleroy in favouring forcing a rounding (which solves the root issue of inconsistent rounding introducing nonmonotonicity) rather than maxing this difference with 0, since this would allow ThreadCPUUsage to be usable safely anywhere (as well as ProcessCPUUsage, which would suffer from the same issue). In their current state, these functions seem to be waiting to cause more hard-to-diagnose bugs.\nAdding a volatile has, unsurprisingly, the right effect both on gcc 5.3 (which is what you seem to be using) and on a more modern version.. ",
    "Kumar-Kishan": "It's 3 in the morning here. Will recommit the fix tomorrow. Added the changes, as per told. ",
    "monkeynova": "It's not perfect agreed, but it's also not unprecedented. gflags has a similar hardcoding (https://github.com/gflags/gflags/blob/master/bazel/gflags.bzl). I started a patch for moving some of the defines there to run off of a config but haven't gotten to a PR for that yet (googletest does something similar for linkopts: https://github.com/google/googletest/blob/master/BUILD.bazel).\nBut I know I don't have complete access to all the build modes I might need. Is there an automated build tool I can use to ensure coverage?. A fair number of the tests #include \"../src/\" so the encapsulation is actually pretty bad. Since this was tests as part of the package I wasn't particularly concerned with the model here, but it's a reasonable concern.\nAs far as the visibility:private, isn't that the default in the build file? When I was trying to make use of this from an external dependency, I had to add the explicit visibility:public to the one rule I wanted to make visible.. ",
    "ianloic": "Ok, done.. ",
    "alekseyshl": "Yes, it is, thank you for catching it. Removed.. ",
    "ghemawat": "IIUC, the memory manager is also responsible for recording allocation data.  So perhaps reword to:\n.. will be used to collect and report allocation measurements for benchmark runs.\n. How about:\n   SetMemoryManager\n   GetMemoryManager\nGetters are often useful to have; e.g., if somebody wants to wrap the existing memory manager. . See note on comment above.\n. From what I see in the .cc file, iterations is only used to convey info from one internal function to another.  So maybe this field should not be in the API (header file) and should be just passed as an extra arg internally.  You will end up with two new args in the internal call:\n    int64 memory_iterations, const MemoryManager::Result& result\n\"memory_iterations > 0\" can be used to determine whether or not memory usage was recorded, and therefore as a side-benefit, Result will no longer need to be heap-allocated.\n. 1. I  think it is fine to leave out the accesor.  But maybe rename the setter to SetMemoryManager just in case we add the getter later?\n\nThe main benefit of registering the memory manager outside the confines of a benchmarks is that then all benchmarks get the benefit of memory measurement automatically.   But your comment does make me think that this API is perhaps too focused one type of measurement: it is only treating one type of measurement somebody might want to add to all benchmarks.  What if somebody wants a different type of measurement; maybe they want to report contention observed in the benchmark.\n\nSo an alternative more general API might be something like:\n```\nvoid RegisterMetric(const string& name, Metric* metric);\nclass Metric {\n public:\n   virtual void Start(Counter) = 0;  // Empties counter\n   virtual void Stop(Counter) = 0;  // Fills counter\n};\n```\nmain (or something called at initialization time) would call RegisterMetric for all relevant metrics that should be measured across all benchmarks.  The benchmark library would call Start/Stop around benchmark execution for every registered metric.\nWhat do you think?\n. One thing I just realized is that unlike Counters, the memory measurement is collected in a separate run (because it slows down regular benchmark performance too much).  Something to keep in mind if we go with the Metric approach I proposed.  Perhaps Counter flags can be extended to indicate that the particular metric must be collected via a separate run.  Or maybe we could just do separate runs for each metric registered like this?\n. ",
    "nazavode": "Firstly thanks for looking into it. The problem with Intel is that it struggles to be a drop-in replacement for GCC, so if you pass any -W* flag it never complains even if it doesn't know about the flag itself. Just tried with the approach you suggested but, unfortunately, the compiler just accepts whatever flag you throw at it and HAVE_CXX_FLAG_WNO_DEPRECATED_DECLARATIONS ends up being always true. A maybe cleaner approach would be to just rely on the fact that a GCC flag is ignored when not supported:\ncmake\nadd_cxx_compiler_flag(-Wno-deprecated-declarations)\nif (CMAKE_CXX_COMPILER_ID STREQUAL \"Intel\")\n  # Intel silently ignores '-Wno-deprecated-declarations',\n  # warning no. 1786 must be explicitly disabled.\n  add_cxx_compiler_flag(-wd1786)\nendif()\nLet me know which one you prefer!. @LebedevRI yep, their frontend can get quite weird. A probably safer choice would be to tell the compiler to treat unrecognized flags as errors:\nhttps://godbolt.org/g/SFSuFb. @LebedevRI I was trying to fix add_cxx_compiler_flag() for icpc but this looks like an Intel regression to me:\nhttps://godbolt.org/g/tfxWvW\nIf you're using icc >= 17.0.4 (the 17.0.0 godbolt is using still works), the -Wno-deprecated-declarations option is considered recognized (-diag-error=10006 is happy) but still ignored.. @dominichamon I like obvious and explicit :+1: \nOk, I will add that change and I'm going to try to open a call to Intel to fix the regression, let's just hope for a better future :). ",
    "Mikayex": "Why not simply remove the option in config_options since it's not supported (as in my conanfile) ? This is how it should be handled according to Conan documentation.... ",
    "uilianries": "there is another option to check compiler version: https://docs.conan.io/en/latest/howtos/check_conan_version_in_conanfile.html#how-to-check-the-version-of-the-conan-client-inside-a-conanfile. where is exports LICENSE?. string or boolean? https://docs.conan.io/en/latest/reference/env_vars.html#conan-run-tests. the latest version is 1.8.1, but I dont know if 1.8.0 is a requirement for benchmark library. nope, keep float. "
}